  
  <row AcceptedAnswerId="76871" AnswerCount="3" Body="&lt;p&gt;I've got some temporal data taken from a data logger that I'm trying to plot in a graphical form (as a line graph). Because it's a large amount of data, plotting it one one big graph (e.g. in Excel) makes it difficult to explore the visualised data as you can't really zoom in and scroll through the data. What I'm looking for is some standalone software that can plot the data as a line graph, but also allow the user to easily scroll through the graph along the horizontal (time) axis and be able to zoom that axis in and out. Ideally, the software would be free and be GUI driven. Does anyone know of any such software?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-28T11:29:09.030" Id="73939" LastActivityDate="2015-01-05T19:48:19.133" LastEditDate="2013-10-28T16:16:42.387" LastEditorUserId="31989" OwnerUserId="31989" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;" Title="(Standalone) Software for plotting graphs of large amounts of data and allowing you to scroll/zoom" ViewCount="867" />
  
  <row Body="&lt;p&gt;I think Bayesian model comparison might be what you are looking for. See for example &lt;a href=&quot;http://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;amp;qid=1382965736&amp;amp;sr=8-1&amp;amp;keywords=bishop&quot; rel=&quot;nofollow&quot;&gt;Bishop&lt;/a&gt;, Chapter 3.4.&#10;Generally speaking, given a set of $N$ models, you choose your weights to correspond to the posterior probability of each model.&#10;$$ p(M_i | D) \propto p(M_i)p(D | M_i) $$&#10;where $p(M_i)$ is the prior of model importance, you can assume this to be uniform, and $D$ is your data. Hence $p(D | M_i)$ is simply the likelihood of model $i$ given data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The predictive probability for a new value $y^*$ and explanatory values $\mathbf{x}$ is then:&#10;$$ p(y^* | \mathbf{x}, D) = \sum_{i=1}^N p(y^*|\mathbf{x},D,M_i)p(M_i | D)$$&#10;where we use the posteriors as weighting between models.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's the theory. Now in practice, unless you happen to be dealing with conjugated probabilities, you won't get a closed form solution for those posteriors and the above is pretty much useless.&lt;/p&gt;&#10;&#10;&lt;p&gt;When people fit a mixture of distributions, they usually use a mixture of Gaussians and in rare occasions a mixture of t distributions. I think the reason is simply that the mixture is fit using the EM algorithm (again, see Bishop) and Gaussians are particularly useful since their posterior is again a Gaussian and you can get all the required updates for the EM algorithm in closed form solution. And when I say &quot;fit&quot;, they don't fit them individually, but learn the best parameters for all mixture components from the data, which is not what you are doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Don't worry about that for now and simply check whether you can get the posteriors for your model, or whether you don't want to fit your mixture using the EM and some well known distribution, such as the Gaussian, or t distribution in case of many outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT (to your comment):&lt;/p&gt;&#10;&#10;&lt;p&gt;So first of all, my notation: $y$ is the quantity you are trying to model, $x$ is a vector of data used to model $y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data $D$ is just a tuple of vectors $x$, used to model a tuple of $y$'s. (basically think any normal dataset with a dependent variable and multiple independent ones)&lt;/p&gt;&#10;&#10;&lt;p&gt;OK. now, if I understand correctly you are trying to fit a complex distribution and you somehow already know the models that explain the data (perhaps because you know the generating mechanism) so you only need the mixing proportions.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could try the following, which is guaranteed to work for Gaussians, and I don't see why it shouldn't work for other distributions too. (though there is a big caveat here!)&lt;/p&gt;&#10;&#10;&lt;p&gt;Calculate for each point x the likelihood that the point was generated by model $f1$ and $f2$, where the parameters are known. You end up with a matrix of likelihoods $2 x N$ where N is the number of your points. sum each row and divide by N. You should get the responsibility of each of the models for generating the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Use that to weight your mixture. You should also check if the resulting density integrates to 1, and if not normalize appropriately.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-28T13:20:37.797" Id="73948" LastActivityDate="2013-10-29T19:07:47.983" LastEditDate="2013-10-29T19:07:47.983" LastEditorUserId="12436" OwnerUserId="12436" ParentId="73941" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Yes, logistic regression would work, but also classification trees. I don't think you need to worry about false positives.  It seems that the &quot;confusion matrix&quot; the model produces will tell you what you are looking for in terms of false positives and false negatives&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-28T21:18:20.893" Id="73984" LastActivityDate="2013-10-28T21:18:20.893" OwnerUserId="32015" ParentId="68945" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Since correlation is a measure of the degree of linear dependence, first differences should tease this out. Now, I am assuming you check cointegration across multiple lags, not just contemporaneous values, since there could be something like $y_t = a + b x_{t-1} + \varepsilon_t$ going on, which may complicate matters. Alecos' observation that there may be not &lt;em&gt;detectible&lt;/em&gt; cointegration is also important.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-29T13:24:54.120" Id="74035" LastActivityDate="2013-10-29T13:24:54.120" OwnerDisplayName="user31668" ParentId="74012" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I know that the statement in question is wrong because estimators cannot have asymptotic variances that are lower than the Cramer-Rao bound.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if asymptotic consistence means that an estimator converges in probability to a value, then doesn't this also mean that its variance becomes 0?&lt;/p&gt;&#10;&#10;&lt;p&gt;Where in this train of thought am I wrong?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-29T15:57:09.100" Id="74047" LastActivityDate="2013-10-29T15:57:09.100" OwnerUserId="20148" PostTypeId="1" Score="2" Tags="&lt;asymptotics&gt;&lt;estimators&gt;&lt;consistency&gt;" Title="Why don't asymptotically consistent estimators have zero variance at infinity?" ViewCount="226" />
  <row Body="&lt;p&gt;I think I may have found the problem with my argument, i.e. how what i called &quot;the frequentist interpretation of the bayesian approach&quot; and viceversa don't really make sense:&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;frequentist interpretation of the bayesian approach&quot;, as described in my quesion, doesn't make much sense because it says that it assumes the likelihood function (and the prior), but then says &quot;no matter which data we get [in the hypothetical large set of experiments]&quot; which is incompatible with the frequentist interpretation of the likelihood function.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;bayesian interpretation of the frequentist approach&quot;, is also wrong because it doesn't ensure what I say below. For example, in the frequentist approach, I may well make a measurement and have an emtpy confidence interval for it, which clearly means that it doesn't ensure an a% probability of being right.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I think I understand it better now. And so it seems that if you want to be as cautious as possible, frequentist is the way to go. But if you don't have much statistical significance, or have good reasons for your prior, bayesian is the way to go.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-29T16:13:50.840" Id="74048" LastActivityDate="2013-10-29T16:46:35.257" LastEditDate="2013-10-29T16:46:35.257" LastEditorUserId="32021" OwnerUserId="32021" ParentId="74000" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="74067" AnswerCount="1" Body="&lt;p&gt;I am trying to understand the difference between:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Expectation_propagation&quot; rel=&quot;nofollow&quot;&gt;Expectation Propagation&lt;/a&gt; (EP)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Variational_Bayesian_methods&quot; rel=&quot;nofollow&quot;&gt;Variational Bayes&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Wikipedia says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Expectation Propagation differs from other Bayesian approximation approaches such as&#10;  Variational Bayesian methods.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Why isn't &lt;strong&gt;EP&lt;/strong&gt; considered a &lt;strong&gt;Variational Bayes&lt;/strong&gt; method? Isn't EP Bayesian, and relies on message-passing to approximate the posterior? What makes a method Variational Bayes?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, what about the following methods. Can they be considered Variational Bayes?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Sum-product&lt;/li&gt;&#10;&lt;li&gt;Mean-field methods&lt;/li&gt;&#10;&lt;li&gt;Bethe-Kikuchi approximations&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-10-29T16:30:49.660" Id="74050" LastActivityDate="2013-10-29T19:42:46.853" OwnerUserId="27838" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;markov-process&gt;" Title="Variational Bayes vs EP and other message-passing methods" ViewCount="130" />
  <row AcceptedAnswerId="74205" AnswerCount="1" Body="&lt;p&gt;I have the following data for 8 runners in a 100 meter dash:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;runner 1 88 91 87 82&#10;runner 2 81 85 78 91&#10;runner 3 75 77 83 81&#10;runner 4 92 89 84 82&#10;runner 5 78 79 84 92&#10;runner 6 89 75 79 83&#10;runner 7 91 89 92 91&#10;runner 8 87 86 88 91&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The ratings represent a performance rating and are normally distributed with unknown mean and unknown variance. Each runner can be considered as a sub-group with a mean and variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any guidance will be highly appreciated&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-29T17:21:38.730" FavoriteCount="1" Id="74054" LastActivityDate="2013-11-18T07:36:33.623" LastEditDate="2013-10-31T00:34:03.273" LastEditorUserId="32050" OwnerUserId="32050" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;self-study&gt;" Title="Hierarchical Bayes Normal-Normal Model" ViewCount="461" />
  
  <row AcceptedAnswerId="74070" AnswerCount="1" Body="&lt;p&gt;So, with the help of someone here in stats i could solve  most of my problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now i'm stuck at this part:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the below code, which worked very well if this values were fixed.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;immediate.amput.below.EV&#10;watchful.wait.EV&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now what they are is vectors. They represent a vector with [n_psa] size.&#10;What i need to do is, i need iterate for n_psa values, i.e for 100 values and find the difference between each  immediate.amput.below.EV[1] - watchful.wait.EV[1] and so on, save the results only as positive numbers, even if the output is a negative number in a new vector, and find the mean of the new vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what i used to do.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DELTA_COST &amp;lt;- max(immediate.amput.below.EV,watchful.wait.EV) - min(immediate.amput.below.EV,watchful.wait.EV)&#10;mean(DELTA_COST)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-10-29T19:39:47.533" Id="74066" LastActivityDate="2013-10-29T20:06:53.583" OwnerUserId="31627" PostTypeId="1" Score="0" Tags="&lt;r&gt;" Title="Find the difference between two vectors and the mean of the result" ViewCount="669" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set of N people, T items. Let's say N=100, and T=10. &lt;/p&gt;&#10;&#10;&lt;p&gt;Each person goes through the following exercise. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;She is shown 2 random items from the set of T=10, and ranks them as rank 1 and 2. &lt;/li&gt;&#10;&lt;li&gt;She is next shown 2 more random items from the remaining 8 out of 10 items, and ranks as rank 1 and 2. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;At the end, the data set is of size 100x10, where each row has 4 numeric entries (two of which will be 1, and the other two will be 2) and 6 empty entries. &lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to compare the 10 items against one another, and come up with an estimated rank value for a given item. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best way to analyze such data ? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-30T04:27:36.030" FavoriteCount="0" Id="74090" LastActivityDate="2013-10-30T04:27:36.030" OwnerUserId="32081" PostTypeId="1" Score="1" Tags="&lt;ranking&gt;" Title="Analyzing repeated rank data." ViewCount="66" />
  
  
  
  
  <row Body="&lt;p&gt;This has the potential to be an interesting question. Clustering algorithms perform 'well' or 'not-well' depending of the topology of your data and what you are looking for in that data. ¿What do you want the clusters to represent? I attach a diagram which sadly does not include kernel k-means or SOM but I think it is of great value for understanding the grave differences between the techniques. You probably need to ask and respond this to yourself before you dig in to measuring the &quot;pros&quot; and &quot;cons&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ssjyq.png&quot; alt=&quot;clustering techniques&quot;&gt; &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html&quot; rel=&quot;nofollow&quot;&gt;This&lt;/a&gt; is the source of the image.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-30T15:49:08.193" Id="74131" LastActivityDate="2013-10-30T15:56:28.930" LastEditDate="2013-10-30T15:56:28.930" LastEditorUserId="11748" OwnerUserId="11748" ParentId="74104" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a data set with highly positively correlated features that I'm classifying with LR. AFAIK correlated weights are not a problem in the same way they are in Naive Bayes - overcounting will not occur with LR. &lt;/p&gt;&#10;&#10;&lt;p&gt;The strange things that I'm seeing is that some of the highly correlated features assume opposite weights: feature A might be highly positive and feature B highly negative, though not as much. Is this a symptom of something going wrong with optimization or is this expected (a priori I expect A and B to be positive class indicators) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-30T16:29:34.377" FavoriteCount="1" Id="74138" LastActivityDate="2013-10-30T19:01:50.803" LastEditDate="2013-10-30T17:52:49.863" LastEditorUserId="32097" OwnerUserId="32097" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;logistic&gt;" Title="Correlated features produce strange weights in Logistic Regression" ViewCount="302" />
  
  
  <row AcceptedAnswerId="74173" AnswerCount="1" Body="&lt;p&gt;Increasing the flexibility of models makes it prone to overfitting. On the other hand, it looks to me that, if the space function classes $\mathcal{F}$ is too big, it is hard to prove bounds on empirical risks bounds and that sort of stuff. That's why I am questioning the necessity/importance/applicability of non-parametric models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here by nonparametrics I mostly mean, Dirichlet Processes and Beta Processes (and related family). &lt;/p&gt;&#10;&#10;&lt;p&gt;Any comments?  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-30T19:13:32.363" Id="74164" LastActivityDate="2013-10-30T20:30:32.093" LastEditDate="2013-10-30T20:26:48.407" LastEditorUserId="17812" OwnerUserId="17812" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;nonparametric&gt;&lt;hierarchical-bayesian&gt;&lt;nonparametric-bayes&gt;" Title="Why semi/nonparametric models?" ViewCount="141" />
  <row AnswerCount="2" Body="&lt;p&gt;I understand intuitively why this is a horrible idea - you assume your model is correct and then increase your number of observations which will likely result in a poor fit on future data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering if there is some mathematical/statistical property to describe this, or if there is any rare case where this may not be as fatal as I am thinking?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-30T23:29:33.807" Id="74190" LastActivityDate="2013-12-30T09:45:44.623" LastEditDate="2013-10-31T00:24:29.863" LastEditorUserId="22047" OwnerUserId="31841" PostTypeId="1" Score="2" Tags="&lt;modeling&gt;&lt;predictive-models&gt;&lt;train&gt;" Title="Is it always bad to retrain your model to include predicted data?" ViewCount="72" />
  
  <row AnswerCount="1" Body="&lt;p&gt;You go gambling with a pair of loaded dice. Because of this, your odds of winning are 53% on every throw. Assuming the game pays 2:1 and you keep betting the same amount, how many games do you need to play to ensure an 80% likelihood of winning money?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am lost with where to start on this problem. I would appreciate some help so I can figure it out. Thanks.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-31T05:32:00.743" Id="74210" LastActivityDate="2013-10-31T07:05:57.080" LastEditDate="2013-10-31T07:05:57.080" LastEditorUserId="805" OwnerUserId="32121" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;binomial&gt;" Title="Probability of winning money with a pair of loaded dice" ViewCount="46" />
  <row Body="&lt;p&gt;You can apply a chi-squared Test of independence test, but you will face an issue with the fact that only 1 respondent has chosen Hua Hin post receiving a stimulus.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your alternative would be to run a Fisher's exact test to accommodate less than a frequency of 5 in each cell.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: Since your samples are paired, you would want to use a McNemar test instead. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-31T07:28:29.130" Id="74217" LastActivityDate="2013-10-31T07:28:29.130" OwnerUserId="30417" ParentId="74203" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In the statistical practice of experimental design, you separate your tests by 'blocks' if you can control the factors (say routing to one page or another, or categorizing by browser) or through randomization of trials to counteract variables you don't control.&lt;/p&gt;&#10;&#10;&lt;p&gt;When doing A/B testing, you obviously can't control who comes to your site when. How can good statistical practice be done?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-31T13:24:46.067" Id="74239" LastActivityDate="2013-10-31T15:47:34.343" LastEditDate="2013-10-31T15:47:34.343" LastEditorUserId="264" OwnerUserId="12678" PostTypeId="1" Score="2" Tags="&lt;experiment-design&gt;&lt;randomization&gt;&lt;ab-test&gt;" Title="How to get proper randomization in website A/B testing?" ViewCount="112" />
  
  
  
  
  
  <row AcceptedAnswerId="74344" AnswerCount="1" Body="&lt;p&gt;A while back I asked a question about &lt;a href=&quot;http://stats.stackexchange.com/questions/17462/correlating-time-stamps&quot;&gt;correlating times between time stamps&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/a/22333/7482&quot;&gt;received a response&lt;/a&gt; from Peter Ellis that said I could calculate mean distances between codes...&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;This already will give you some sense of which behaviours are clustered together, but you also should check that this isn't plausibly due just to chance.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;To check that, I would create simulated data generated by a model under the null hypothesis of no relation. Doing this would require generating data for each behaviour's time from a plausible null model, probably based on resampling the times between each event (eg between each yawn) to create a new set of time stamps for hypothetical null model events. Then calculate the same indicator statistic for this null model and compare to the indicator from your genuine data. By repeating this simulation a number of times, you could find out whether the indicator from your data is sufficiently different from the null model's simulated data (smaller average time from each yawn to the nearest stretch, for example) to count as statistically significant evidence against your null hypothesis.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I finally possess the skill set to do this and have done so in R but I don't know what this method or technique is called so that I can (a) learn more about it (b) speak intelligently about the theory behind what I'm doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some people have suggested this is called a permutation test, others say similar to but not the same as bootstrapping and some have told me it's related to Monte Carlo re sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is this method of resampling, given the NULL is TRUE, called?  If you have a reference or two to back up your response that may be helpful but not necessary.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-01T01:12:07.763" FavoriteCount="3" Id="74286" LastActivityDate="2013-11-01T22:02:21.007" LastEditDate="2013-11-01T03:23:59.940" LastEditorUserId="7482" OwnerUserId="7482" PostTypeId="1" Score="7" Tags="&lt;bootstrap&gt;&lt;monte-carlo&gt;&lt;resampling&gt;&lt;quasi-monte-carlo&gt;" Title="What method is simulating pvalues from re sampling from the data" ViewCount="145" />
  <row AcceptedAnswerId="74598" AnswerCount="1" Body="&lt;p&gt;I'm testing throttle position sensors (TPS) my business sells and I print the plot of voltage response to the throttle shaft's rotation. A TPS is a rotational sensor with $\approx$ 90° of range and the output is like a potentiometer with full open being 5V (or sensor's input value) and initial opening being some value between 0 and 0.5V. I built a &lt;a href=&quot;http://forums.adafruit.com/viewtopic.php?f=22&amp;amp;t=44710&quot; rel=&quot;nofollow&quot;&gt;test bench with a PIC32 controller&lt;/a&gt; to take a voltage measurement every 0.75° and the black line connects these measurements. &lt;/p&gt;&#10;&#10;&lt;p&gt;One of my products has a tendency to make localized, low amplitude variations away from (and under) the ideal line. &lt;strong&gt;This question is about my algorithm for quantifying these localized &quot;dips&quot;; what is a good name or description&lt;/strong&gt; for the process of measuring the dips? (full explanation follows) In the below picture, the dip occurs at the left third of the plot and is a marginal case whether I would pass or fail this part:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gfx2w.jpg&quot; alt=&quot;Print out of a suspect part&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So I built a dip detector (&lt;a href=&quot;http://stackoverflow.com/questions/19269638&quot;&gt;stackoverflow qa about the algorithm&lt;/a&gt;) to quantify my gut feeling. I initially thought I was measuring &quot;area&quot;. This graph is based on the printout above and my attempt to explain the algorithm graphically. There is a dip lasting for 13 samples between 17 and 31:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/W5oXL.png&quot; alt=&quot;Sampled data shown with the &amp;quot;dip&amp;quot; magnified&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Test data goes in an array and I make another array for &quot;rise&quot; from one data point to the next, which I call $deltas$. I use a library to get the average and standard deviation for $deltas$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Analyzing the $deltas$ array is represented in the graph below, where the slope is removed from the above graph. Originally, I thought of this as &quot;normalizing&quot; or &quot;unitizing&quot; the data as the x axis are equal steps and I'm now solely working with the rise between data points. When researching this question, I recalled this is the derivative, $\frac {dy}{dx}$ of the original data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FjzSS.png&quot; alt=&quot;Analysis of the derivative...?&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I walk through $deltas$ to find sequences where there are 5 or more adjacent negative values. The blue bars are a series of data points who are below the average of all $deltas$. The values of the blue bars are:&lt;/p&gt;&#10;&#10;&lt;p&gt;$0.7 + 1.2 + 1.3 + 1.4 + 1.8 + 2.5 + 2.9 + 3.0 + 2.5 + 2.0 + 1.5 + 1.0 + 1.2$&lt;/p&gt;&#10;&#10;&lt;p&gt;They sum to $23$, which represents the area (or the integral). My first thought is &quot;I just integrated the derivative&quot; which should mean I get back the original data, though I'm certain there's a term for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;The green line is the average of these &quot;below average values&quot; found via dividing the area by the length of the dip:  &lt;/p&gt;&#10;&#10;&lt;p&gt;$23 \div 13 = 1.77$&lt;/p&gt;&#10;&#10;&lt;p&gt;During the testing of 100+ parts, I came to decide that dips with my green line average less than $2.6$ are acceptable. Standard deviation calculated across the entire data set wasn't a strict enough test for these dips, as without enough total area, they still fell within the limit I established for good parts. I observationally chose standard deviation of $3.0$ to be the highest I would allow.&lt;/p&gt;&#10;&#10;&lt;p&gt;Setting a cutoff for standard deviation strict enough to fail this part would then be so strict as to fail parts which otherwise appear to have a great plot. I do also have a spike detector which fails the part if any $|deltas - avg| &amp;gt; avg+std dev$. &lt;/p&gt;&#10;&#10;&lt;p&gt;It's been almost 20 years since Calc 1, so please go easy on me, but this &lt;em&gt;feels&lt;/em&gt; a lot like when a professor used calculus and the displacement equation to explain how in racing, a competitor with less acceleration who maintains higher corner speed can beat another competitor having greater acceleration to the next turn: going through the previous turn faster, the higher initial speed means the area under his velocity (displacement) is greater.&lt;/p&gt;&#10;&#10;&lt;p&gt;To translate that to my question, I feel like my green line would be like acceleration, the 2nd derivative of the original data. &lt;/p&gt;&#10;&#10;&lt;p&gt;I visited wikipedia to re-read the fundamentals of calculus and the definitions of derivative and &lt;a href=&quot;http://en.wikipedia.org/wiki/Integral&quot; rel=&quot;nofollow&quot;&gt;integral&lt;/a&gt;, learned the proper term for adding up the area under a curve via discreet measurements as &lt;a href=&quot;http://en.wikipedia.org/wiki/Numerical_integration&quot; rel=&quot;nofollow&quot;&gt;Numerical Integration&lt;/a&gt;. Much more googling on &lt;em&gt;average of the integral&lt;/em&gt; and I'm lead to the topic of nonlinearity and digital signal processing. &lt;a href=&quot;https://www.google.com/search?q=%22average%20of%20the%20integral%22%20quantify%20deviation&amp;amp;num=20&amp;amp;filter=0&amp;amp;biw=1447&amp;amp;bih=792&quot; rel=&quot;nofollow&quot;&gt;Averaging the integral seems to be a popular metric for quantifying data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there a term for the Average of the Integral? ($1.77$, the green line)?&lt;/strong&gt; &lt;br&gt;&#10;... or for the process of using it to evaluate data?&lt;/p&gt;&#10;" CommentCount="19" CreationDate="2013-10-18T08:27:04.943" Id="74306" LastActivityDate="2013-11-04T23:37:18.850" LastEditDate="2013-11-04T19:52:28.763" LastEditorUserId="32124" OwnerDisplayName="Chris K" OwnerUserId="32124" PostTypeId="1" Score="12" Tags="&lt;terminology&gt;" Title="Is there a better name than &quot;average of the integral&quot;?" ViewCount="139" />
  
  
  
  <row Body="&lt;p&gt;Because of your sample size, I would recommend using &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#Leave-one-out_cross-validation&quot; rel=&quot;nofollow&quot;&gt;leave one out cross&lt;/a&gt; validation to estimate model fit.  The algorithm goes like this: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Take out an observation from the data set.&lt;/li&gt;&#10;&lt;li&gt;Fit the model.&lt;/li&gt;&#10;&lt;li&gt;Estimate the class label of the held out observation.&lt;/li&gt;&#10;&lt;li&gt;Repeat steps 1-3 until all observations have been held out.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;To see the algorithms performance, look at how well it fit the held out data. &lt;/p&gt;&#10;&#10;&lt;p&gt;You may find, as Simone suggested, that your algorithm is overfitting the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing that I have found is that you need much more than 16 observations to fit a good classification model.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Check out the &lt;strong&gt;cvTools&lt;/strong&gt; package to perform the cross validation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-01T14:36:27.640" Id="74323" LastActivityDate="2013-11-01T14:36:27.640" OwnerUserId="32186" ParentId="74221" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Jerome Cornfield has written:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One of the finest fruits of the Fisherian revolution was the idea of&#10;  randomization, and statisticians who agree on few other things have at&#10;  least agreed on this. But despite this agreement and despite the&#10;  widespread use of randomized allocation procedures in clinical and in&#10;  other forms of experimentation, its logical status, i.e., the exact&#10;  function it performs, is still obscure.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Cornfield, Jerome (1976). &lt;a href=&quot;http://www.epidemiology.ch/history/PDF%20bg/Cornfield%20J%201976%20recent%20methodological%20contributions.pdf&quot;&gt;&quot;Recent Methodological Contributions to Clinical Trials&quot;&lt;/a&gt;. American Journal of Epidemiology 104 (4): 408–421.&lt;/p&gt;&#10;&#10;&lt;p&gt;Throughout this site and in a variety of literature I consistently see confident claims about the powers of randomization. Strong terminology such as &quot;it &lt;strong&gt;eliminates&lt;/strong&gt; the issue of confounding variables&quot; are common. See &lt;a href=&quot;http://stats.stackexchange.com/questions/32941/examples-of-lurking-variable-and-influential-observation&quot;&gt;here&lt;/a&gt;, for example. However, many times experiments are run with small samples (3-10 samples per group) for practical/ethical reasons. This is very common in preclinical research using animals and cell cultures and the researchers commonly report p values in support of their conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;This got me wondering, how good is randomization at balancing confounds. For this plot I modeled a situation comparing treatment and control groups with one confound that could take on two values with 50/50 chance (eg type1/type2, male/female). It shows the distribution of &quot;% Unbalanced&quot; (Difference in # of type1 between treatment and control samples divided by sample size) for studies of a variety of small sample sizes. The red lines and right side axes show the ecdf.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Probability of various degrees of balance under randomization for small sample sizes:&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/H3n20.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Two things are clear from this plot (unless I messed up somewhere).&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The probability of getting exactly balanced samples decreases as sample size is increased.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The probability of getting a very unbalanced sample decreases as sample size increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) In the case of n=3 for both groups, there is a 3% chance of getting a completely unbalanced set of groups (all type1 in the control, all type2 in the treatment). N=3 is common for molecular biology experiments (eg  measure mRNA with PCR, or proteins with western blot)&lt;/p&gt;&#10;&#10;&lt;p&gt;When I examined the n=3 case further, I observed strange behaviour of the p values under these conditions. The left side shows the overall distribution of pvalues calculating using t-tests under conditions of different means for the type2 subgroup. The mean for type1 was 0, and sd=1 for both groups. The right panels show the corresponding false positive rates for nominal &quot;significance cutoffs&quot; from .05 to.0001.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Distribution of p-values for n=3 with two subgroups and different means of the second subgroup when compared via t test (10000 monte carlo runs):&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/PBxSw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here are the results for n=4 for both groups:&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/yAxux.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For n=5 for both groups:&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/MbM4w.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For n=10 for both groups:&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/9Dt0R.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As can be seen from the charts above there appears to be an interaction between sample size and difference between subgroups that results in a variety of p-value distributions under the null hypothesis that are not uniform.&lt;/p&gt;&#10;&#10;&lt;p&gt;So can we conclude that p-values are not reliable for properly randomized and controlled experiments with small sample size?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R code for first plot&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(gtools)&#10;&#10;#pdf(&quot;sim.pdf&quot;)&#10;par(mfrow=c(4,2))&#10;for(n in c(3,4,5,6,7,8,9,10)){&#10;  #n&amp;lt;-3&#10;  p&amp;lt;-permutations(2, n, repeats.allowed=T)&#10;&#10;  #a&amp;lt;-p[-which(duplicated(rowSums(p))==T),]&#10;  #b&amp;lt;-p[-which(duplicated(rowSums(p))==T),]&#10;&#10;  a&amp;lt;-p&#10;  b&amp;lt;-p&#10;&#10;  cnts=matrix(nrow=nrow(a))&#10;  for(i in 1:nrow(a)){&#10;    cnts[i]&amp;lt;-length(which(a[i,]==1))&#10;  }&#10;&#10;&#10;  d=matrix(nrow=nrow(cnts)^2)&#10;  c&amp;lt;-1&#10;  for(j in 1:nrow(cnts)){&#10;    for(i in 1:nrow(cnts)){&#10;      d[c]&amp;lt;-cnts[j]-cnts[i]&#10;      c&amp;lt;-c+1&#10;    }&#10;  }&#10;  d&amp;lt;-100*abs(d)/n&#10;&#10;  perc&amp;lt;-round(100*length(which(d&amp;lt;=50))/length(d),2)&#10;&#10;  hist(d, freq=F, col=&quot;Grey&quot;, breaks=seq(0,100,by=1), xlab=&quot;% Unbalanced&quot;,&#10;       ylim=c(0,.4), main=c(paste(&quot;n=&quot;,n))&#10;  )&#10;  axis(side=4, at=seq(0,.4,by=.4*.25),labels=seq(0,1,,by=.25), pos=101)&#10;  segments(0,seq(0,.4,by=.1),100,seq(0,.4,by=.1))&#10;  lines(seq(1,100,by=1),.4*cumsum(hist(d, plot=F, breaks=seq(0,100,by=1))$density),&#10;        col=&quot;Red&quot;, lwd=2)&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R code for plots 2-5&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for(samp.size in c(6,8,10,20)){&#10;  dev.new()&#10;  par(mfrow=c(4,2))&#10;  for(mean2 in c(2,3,10,100)){&#10;    p.out=matrix(nrow=10000)&#10;&#10;    for(i in 1:10000){&#10;&#10;      d=NULL&#10;      #samp.size&amp;lt;-20&#10;      for(n in 1:samp.size){&#10;        s&amp;lt;-rbinom(1,1,.5)&#10;        if(s==1){&#10;          d&amp;lt;-rbind(d,rnorm(1,0,1))&#10;        }else{&#10;          d&amp;lt;-rbind(d,rnorm(1,mean2,1))&#10;        }&#10;      }&#10;&#10;      p&amp;lt;-t.test(d[1:(samp.size/2)],d[(1+ samp.size/2):samp.size], var.equal=T)$p.value&#10;&#10;      p.out[i]&amp;lt;-p&#10;    }&#10;&#10;&#10;    hist(p.out, main=c(paste(&quot;Sample Size=&quot;,samp.size/2),&#10;                       paste( &quot;% &amp;lt;0.05 =&quot;, round(100*length(which(p.out&amp;lt;0.05))/length(p.out),2)),&#10;                       paste(&quot;Mean2=&quot;,mean2)&#10;    ), breaks=seq(0,1,by=.05), col=&quot;Grey&quot;, freq=F&#10;    )&#10;&#10;    out=NULL&#10;    alpha&amp;lt;-.05&#10;    while(alpha &amp;gt;.0001){&#10;&#10;      out&amp;lt;-rbind(out,cbind(alpha,length(which(p.out&amp;lt;alpha))/length(p.out)))&#10;      alpha&amp;lt;-alpha-.0001&#10;    }&#10;&#10;    par(mar=c(5.1,4.1,1.1,2.1))&#10;    plot(out, ylim=c(0,max(.05,out[,2])),&#10;         xlab=&quot;Nominal alpha&quot;, ylab=&quot;False Postive Rate&quot;&#10;    )&#10;    par(mar=c(5.1,4.1,4.1,2.1))&#10;  }&#10;&#10;}&#10;#dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-11-01T23:24:19.330" FavoriteCount="2" Id="74350" LastActivityDate="2014-01-03T18:38:45.973" OwnerUserId="31334" PostTypeId="1" Score="9" Tags="&lt;small-sample&gt;&lt;randomization&gt;" Title="Is randomization reliable with small samples?" ViewCount="391" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a data monthly as well as daily data of no of patients and I want to employ time series model .How can I estimate missing counts.Any one would please guide me&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-02T07:49:31.200" Id="74365" LastActivityDate="2013-11-02T10:07:38.983" OwnerUserId="32208" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;" Title="Estimation of Missing Observations" ViewCount="53" />
  
  
  
  <row Body="&lt;p&gt;Here's a quote from Andrew Gilpin (1993) advocating  Maurice Kendall's $τ$ over Spearman's $ρ$ for theoretical reasons: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;[Kendall's $τ$] approaches a normal distribution more rapidly than $ρ$, as $N$, the sample size, increases; and $τ$ is also more tractable mathematically, particularly when ties are present.  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I can't add much about Goodman-Kruskal $γ$, other than that it seems to produce ever-so-slightly larger estimates than Kendall's $τ$ in a sample of survey data I've been working with lately... and of course, noticeably lower estimates than Spearman's $ρ$.  However, I also tried calculating a couple partial $γ$ estimates (Foraita &amp;amp; Sobotka, 2012), and those came out closer to the partial $ρ$ than the partial $τ$... It took a fair amount of processing time though, so I'll leave the simulation tests or mathematical comparisons to someone else... (who would know how to do them...)&lt;/p&gt;&#10;&#10;&lt;p&gt;As &lt;a href=&quot;http://stats.stackexchange.com/users/3277/ttnphns&quot;&gt;ttnphns&lt;/a&gt; implies, you can't conclude that your $ρ$ estimates are better than your $τ$ estimates by magnitude alone, because their scales differ (even though the limits don't).  Gilpin cites Kendall (1962) as describing the ratio of $ρ$ to $τ$ to be roughly 1.5 over most of the range of values.  They get closer gradually as their magnitudes increase, so as both approach 1 (or -1), the difference becomes infinitesimal.  Gilpin gives a nice big table of equivalent values of $ρ$, $r$, $r^2$, &lt;em&gt;d&lt;/em&gt;, and $Z_r$ out to the third digit for $τ$ at every increment of .01 across its range, just like you'd expect to see inside the cover of an intro stats textbook.  He based those values on Kendall's specific formulas, which are as follows:&#10;$$&#10;\begin{aligned}&#10;r &amp;amp;= \sin\bigg(\tau\cdot\frac \pi 2 \bigg)  \\&#10;\rho &amp;amp;= \frac 6 \pi \bigg(\tau\cdot\arcsin \bigg(\frac{\sin(\tau\cdot\frac \pi 2)} 2 \bigg)\bigg)&#10;\end{aligned}&#10;$$&#10;(I simplified this formula for $ρ$ from the form in which Gilpin wrote, which was in terms of Pearson's $r$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe it would make sense &lt;strong&gt;to convert your $τ$ into a $ρ$&lt;/strong&gt; and see how the computational change affects your effect size estimate.  Seems that comparison would give some indication of the extent to which the problems that Spearman's $ρ$ is more sensitive to are present in your data, if at all.  More direct methods surely exist for identifying each specific problem individually; my suggestion would produce more of a quick-and-dirty omnibus effect size for those problems.  If there's no difference (after correcting for the difference in scale), then one might argue there's no need to look further for problems that only apply to $ρ$.  If there's a substantial difference, then it's probably time to break out the magnifying lens to determine what's responsible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure how people usually report effect sizes when using Kendall's $τ$ (to the unfortunately limited extent that people worry about reporting effect sizes in general), but since it seems likely that unfamiliar readers would try to interpret it on the scale of Pearson's $r$, it &lt;strong&gt;might be wise to report both&lt;/strong&gt; your $τ$ statistic and its effect size on the scale of $r$ using the above conversion formula...or at least point out the difference in scale and give a shout out to Gilpin for his handy conversion table.&lt;/p&gt;&#10;&#10;&lt;h3&gt;References&lt;/h3&gt;&#10;&#10;&lt;p&gt;Foraita, R., &amp;amp; Sobotka, F. (2012).  Validation of graphical models.  &lt;em&gt;gmvalid Package, v1.23.&lt;/em&gt;  The Comprehensive R Archive Network.  URL: &lt;a href=&quot;http://cran.r-project.org/web/packages/gmvalid/gmvalid.pdf&quot;&gt;http://cran.r-project.org/web/packages/gmvalid/gmvalid.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Gilpin, A. R. (1993).  Table for conversion of Kendall's Tau to Spearman's Rho within the context measures of magnitude of effect for meta-analysis.  &lt;em&gt;Educational and Psychological Measurement, 53&lt;/em&gt;(1), 87-92.&lt;/p&gt;&#10;&#10;&lt;p&gt;Kendall, M. G. (1962).  &lt;em&gt;Rank correlation methods&lt;/em&gt; (3rd ed.).  London: Griffin.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-02T15:42:21.763" Id="74390" LastActivityDate="2013-11-03T14:56:57.887" LastEditDate="2013-11-03T14:56:57.887" LastEditorUserId="3277" OwnerUserId="32036" ParentId="18112" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;A few points:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The log of a ratio is simply the difference of the logs. Is this really what you want? &lt;/p&gt;&#10;&#10;&lt;p&gt;2) As to your main point; although it is not completely clear, the two methods that come to mind here are multiple imputation and propensity scores. There is a huge amount of literature on both of these. You could start by looking up both those terms right here on CrossValidated. That should get you to places where you can access the wider literature.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/propensity-scores&quot;&gt;Here&lt;/a&gt; are 28 posts about propensity scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/multiple-imputation&quot;&gt;Here&lt;/a&gt; are 53 posts about multiple imputation. &lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;a href=&quot;http://stats.stackexchange.com/questions/35955/propensity-score-matching-after-multiple-imputation&quot;&gt;here&lt;/a&gt; is one post about using both (with links to several articles)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-02T18:22:39.377" Id="74403" LastActivityDate="2013-11-02T18:28:53.100" LastEditDate="2013-11-02T18:28:53.100" LastEditorUserId="22047" OwnerUserId="686" ParentId="74391" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The population variance is the average squared distance from the mean. You use it when you have the &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_population&quot; rel=&quot;nofollow&quot;&gt;population&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, if you have every member of the population of interest, you compute $\sigma^2 = \bar v = \frac{\sum_i v_i}{n}$ where $v_i = (x_i - \mu)^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you're asking about variances of random variables, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for the details of the relevant formulas)&lt;/p&gt;&#10;&#10;&lt;p&gt;The sample variance, $s^2$ uses the same formula, but usually the denominator of the average is taken to be one smaller because observations are closer to the sample mean than they are to the population mean, which makes the squared deviations too small on average; replacing $\frac{}{n}$ with $\frac{}{n-1}$ makes them right on average. The $n-1$ denominator version is sometimes called $s^2_{n-1}$ by contrast from the version with the $n$ denominator, $s^2_n$. You use sample variance when you have a &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_sample&quot; rel=&quot;nofollow&quot;&gt;sample&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The relevant standard deviations are simply the square roots of the corresponding variances. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-02T22:49:02.983" Id="74422" LastActivityDate="2013-11-02T22:54:07.267" LastEditDate="2013-11-02T22:54:07.267" LastEditorUserId="805" OwnerUserId="805" ParentId="74420" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The formula for the sample mean is $\bar{x} = \frac{x_i}{n}$ where $x_i$ are the individual values and n is the number of values. &lt;/p&gt;&#10;&#10;&lt;p&gt;There could also be sample means for other means, but without further details, the mean here refers to the arithmetic mean&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-03T00:16:49.967" Id="74427" LastActivityDate="2013-11-03T00:16:49.967" LastEditDate="2013-11-03T00:16:49.967" LastEditorUserId="686" OwnerUserId="686" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;The difficulty of the problem is highly dependent on how erroneous the uncertain labels can be. If the the uncertain labels are right, say, 90% of the time, you can probably get away with just using logistic regression. On the other hand, if the labels are wrong almost half the time, you may need to resort to some special techniques. &lt;a href=&quot;http://arxiv.org/abs/1310.1363&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt;'s one stab I took at a very similar problem. (We had multiple observations per label, but otherwise the setup is quite similar.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-03T00:58:09.247" Id="74433" LastActivityDate="2013-11-03T00:58:09.247" OwnerUserId="24498" ParentId="74042" PostTypeId="2" Score="2" />
  
  
  <row Body="" CommentCount="0" CreationDate="2013-11-03T11:27:30.657" Id="74454" LastActivityDate="2013-11-03T11:27:30.657" LastEditDate="2013-11-03T11:27:30.657" LastEditorUserId="686" OwnerUserId="686" PostTypeId="5" Score="0" />
  
  
  <row Body="&lt;p&gt;It is certainly adds value to a single test because you get a stronger justification that your estimated accuracy is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Large dataset certainly helps in making robust, accurate models though it won't bias the cross-validation on its own. The only possible problem you should check for is whether the set contains significant fraction of duplicated objects -- this may happen if the number of attributes is very small in comparison.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-03T18:09:20.110" Id="74473" LastActivityDate="2013-11-03T18:09:20.110" OwnerUserId="88" ParentId="74466" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a symbolic representation of time series obtained from &lt;a href=&quot;http://www.cs.ucr.edu/~eamonn/SAX.htm&quot; rel=&quot;nofollow&quot;&gt;SAX toolbox&lt;/a&gt;. I was wondering if it is possible to construct a graph where each node represents a unique symbol and the edges represent the transition providing that there is no transition to itself. For ex, let the time series T of n=20 data points be represented as&#10; &lt;code&gt;T=[1 1 2 1 2 1 3 1 1 1 2 2 3 3 3 1 1 2 3 3 1]'&lt;/code&gt; where number of alphabets used to symbolize = 3 and they are (1,2,3). I have combined co-occuring symbols together so, the compressed time series becomes &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;T' = [1 2 1 2 1 3 1 2 3 1 2 3 1]'&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In order to construct a graph (Esp. &lt;a href=&quot;http://en.wikipedia.org/wiki/Fuzzy_cognitive_map&quot; rel=&quot;nofollow&quot;&gt;fuzzy cognitive map&lt;/a&gt; ) with fuzzy membership values from T' where the nodes will be (1,2,3), there will be an edge from $Node_i$ to $Node_j$ and it will have a weight $W_{ji}$. How do I find the weights ? I do not know which theory to search for this kind of problem and so if there are any ideas as to what can form the weights. Thank you&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-03T20:04:07.333" FavoriteCount="0" Id="74479" LastActivityDate="2015-02-26T18:06:51.057" LastEditDate="2013-11-04T18:30:23.420" LastEditorUserId="21160" OwnerUserId="21160" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;matlab&gt;&lt;stochastic-processes&gt;&lt;markov-process&gt;&lt;fuzzy-set&gt;" Title="Issue in graph construction" ViewCount="119" />
  
  
  
  
  <row AcceptedAnswerId="74523" AnswerCount="1" Body="&lt;p&gt;I have read that the update equation for stochastic gradient descent is as shown below, for each iteration, k. Does one iteration correspond to one training example? So for each example is there only one update to $\theta$? &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \theta^{k+1} = \theta^k - \epsilon_k    \frac{\partial L(\theta^k,z)}{\partial \theta^k} $&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: Is it different for Online learning?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-04T07:14:44.243" FavoriteCount="2" Id="74518" LastActivityDate="2013-11-06T02:18:08.330" LastEditDate="2013-11-06T02:18:08.330" LastEditorUserId="16596" OwnerUserId="16596" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;maximum-likelihood&gt;&lt;optimization&gt;&lt;gradient-descent&gt;" Title="In stochastic gradient descent, is there only one update to $\theta$ for each iteration?" ViewCount="155" />
  
  
  <row Body="&lt;p&gt;There is one book dedicated to the problem of products of random variables:&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0824754026&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Products-Random-Variables-Applications-Arithmetical/dp/0824754026/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1383564424&amp;amp;sr=1-1&amp;amp;keywords=product+of+random+variables&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you can find it in a library. (Or search google scholar  with the author names)&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a connection between products of independent random variables and the Mellin transform, see the paper: &quot;Some Applications of the Mellin Transform in Statistics&quot; by Benjamin Epstein, which is on JSTOR. There is a Wikipedia article on the Mellin Transform, and search google scholar for &quot;Mellin transform product of random variables&quot; gives some relevant papers.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-04T12:06:40.070" Id="74535" LastActivityDate="2013-11-04T12:15:12.123" LastEditDate="2013-11-04T12:15:12.123" LastEditorUserId="11887" OwnerUserId="11887" ParentId="74511" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Propensity score matching:&#10;First you'd model the probability of students' going to an advisor, based on available predictors&amp;mdash;perhaps course marks, financial situation, living arrangements, medical records, &amp;amp;c. For each student the predicted probability of going to an advisor is the propensity score. Next you'd match each student who &lt;em&gt;did&lt;/em&gt; go to an advisor with one who &lt;em&gt;didn't&lt;/em&gt; but has the same, or as near as possible, propensity score. So you'd now proceed with an analysis of retention based on matched treatment-control pairs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, this approach doesn't provide as compelling evidence as an experiment in which students are randomly assigned to go to an advisor or not&amp;mdash;you'd need to consider whether predictors you hadn't accounted for could be significantly contributing to the effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rosenbaum &amp;amp; Rubin (1983), &quot;The Central Role of the Propensity Score in Observational Studies for Causal Effects&quot;, &lt;em&gt;Biometrika&lt;/em&gt;, &lt;strong&gt;70&lt;/strong&gt;, 1&lt;/p&gt;&#10;&#10;&lt;p&gt;Rosenbaum &amp;amp; Rubin (1985), &quot;Constructing a Control Group Using Multivariate Matched Sampling Methods that Incorporate the Propensity Score&quot;, &lt;em&gt;The American Statistician&lt;/em&gt;, &lt;strong&gt;39&lt;/strong&gt;, 1&lt;/p&gt;&#10;&#10;&lt;p&gt;And the &lt;a href=&quot;http://cran.r-project.org/web/packages/Matching/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Matching&lt;/code&gt;&lt;/a&gt; package for R might be useful.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-04T13:52:25.530" Id="74540" LastActivityDate="2013-11-06T10:22:43.047" LastEditDate="2013-11-06T10:22:43.047" LastEditorUserId="17230" OwnerUserId="17230" ParentId="74297" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;What i am suggesting is not the right answer and i agree with the comments that it cannot be calculated. However if you still had to do it, and nothing else is given, then i would suggest that you can fit a regression model with the CPI as the dependent variable and the food price index as the independent variable. if you get a good fit, then you know how to remove the effect of the food price index simply by subtracting the food price index multiplied by the coefficient you get. and once again, this is just an attempt if nothing else is provided and you get a good fit.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-04T15:10:23.797" Id="74548" LastActivityDate="2013-11-04T15:10:23.797" OwnerUserId="25692" ParentId="74522" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You should investigate ARMAX/Dynamic Regression/Transfer Functions making sure that you deal with any Outliers/Level Shifts/Seasonal Pulses/Local Time Trends while testing for constancy of parameters and constancy of error variance. If you wish to post your data, do so and I will send you some results illustrating these ideas. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-04T16:44:17.273" Id="74554" LastActivityDate="2013-11-04T16:44:17.273" OwnerUserId="3382" ParentId="74545" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;From kinect depth images, I have collected the following time series that represent the features = 3D joint positions, quarternion angles, difference between hip joint and centroid of the right arm (composed of right shoulder, right elbow joint and right wrist). These are the feature sets at each time frame. But these are noisy and I need to eliminate those frames where there is no data, remove outlier and smooth the feature set. My data set contains frames where the values are zero since the recording does not start immediately.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen some methods like moving average filters and it is applied on each dimension for smoothing so the output contains the same data points. However, for outlier removal, the length of the dataset may get altered. As a result, I need to apply on the entire dataset instead of considering each time series individually. &lt;/p&gt;&#10;&#10;&lt;p&gt;Q1 : What are the ways by which I can detect and remove outliers for multi dimensional timeseries so that I can remove bad frames which do not contribute from the good frames?&lt;/p&gt;&#10;&#10;&lt;p&gt;Q2 : Are there any implementations or packages for achieving this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-04T18:00:42.670" Id="74565" LastActivityDate="2013-11-04T18:00:42.670" OwnerUserId="28545" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;matlab&gt;&lt;outliers&gt;" Title="Outlier detection and smoothing for multi dimensional time series" ViewCount="480" />
  
  <row Body="&lt;p&gt;Here's the situation as I understand it:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) you want to test for equality of spread with heavy tailed distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) you assume they will only differ in spread.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) you have a measure of spread already in mind (median absolute deviation); what you need is a single statistic that identifies how much those spreads are unalike (since it's a measure of scale, their ratio is a reasonable choice, but in fact for what I am going to suggest we're going to be able to do something simpler).&lt;/p&gt;&#10;&#10;&lt;p&gt;4) the usual assumptions like independence etc apply.&lt;/p&gt;&#10;&#10;&lt;p&gt;(5) it sounds like you are using R; if I give any example code I'll use R also)&lt;/p&gt;&#10;&#10;&lt;p&gt;With these assumptions, under the null, the distributions of the two samples are the same; the null is a test of identical distributions, the test statistic is a measure that's sensitive to a particular kind of deviation from identical distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want a test where dispersion is measured by median absolute deviation, then in these circumstances you could just do a permutation test (or randomization test if the samples are too large to evaluate all permutations).&lt;/p&gt;&#10;&#10;&lt;p&gt;Because it's a permutation test, we don't have to take the ratio of mads, anything whose size is sensitive to deviations from the null would also work. In this case I'm going to discuss looking at the mad of the smaller-sized sample, in the hopes of calculating a bit faster (though calculation speed shouldn't be the only consideration, it allows me to make some useful points about what we're doing along the way).&lt;/p&gt;&#10;&#10;&lt;p&gt;If the null were true, the labels for the two groups are 'arbitrary' - we could combine the samples and randomly relabel the observations without altering their underlying distribution. However, under the alternative, the labels signify something important - that we are sampling from distributions that differ in spread. In that case, the sample test statistic won't be 'typical' of the permutation distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basic outline of how to do a test of equality of distributions using a specified statistic&lt;/p&gt;&#10;&#10;&lt;p&gt;Permutation test:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Combine both samples and allocate the set of group labels in all possible combinations to find the distribution of the test statistic under the null of identical distributions, calculating the test statistic each time.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;find how far 'in from the end(s)' the sample value occurs, and calculate the proportion of null values from the permutation distribution that are at least as extreme as it. (the p-value)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If the sample statistic is sufficiently extreme in the null distribution (if that p-value is low enough), reject the null.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In this case I assume you want a two-tailed test and then (with the mad of the smaller group as the statistic) we have the problem of what to count up to in the other tail (this would be easy with the ratio, since we just take the reciprocal). However we'll take the ratio to the combined mad (which doesn't change so we don't have to sample that), and use the reciprocal from that to get the cutoff in the other tail.&lt;/p&gt;&#10;&#10;&lt;p&gt;The randomization test is basically the same as the above but instead of evaluating all possible permutations it simply samples (with replacement) from the set of permutations.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Another alternative would be to use bootstrapping; in that case you're seeing whether a ratio of median absolution deviations of 1 could plausibly have come from the bootstrap distribution of the ratio. In this case you would resample with replacement &lt;em&gt;within groups&lt;/em&gt; and compute the ratio each time, and see whether the null value is consistent with it (would be inside a confidence interval for the ratio). But bootstrapping tends not to work so well on medians, and you would require fairly large samples. A smoother (but similarly robust) statistic might work better.]&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; (here I've chosen the sample sizes to be just a little bit too large to do a permutation test; there's 3.2 billion combinations - you &lt;em&gt;could&lt;/em&gt; do it if you were determined to get the exact permutation distribution, but we won't)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# create some data with heavy tail and different spread&#10;set.seed(3498732)&#10;nrand &amp;lt;- 100000&#10;x &amp;lt;- rt(20,df=3)*5+50&#10;y &amp;lt;- rt(15,df=3)*20+50&#10;print(x,d=4)&#10;print(y,d=4)&#10;xy &amp;lt;- c(x,y)&#10;&#10;# do the randomization&#10;madrand &amp;lt;- replicate(nrand,mad(sample(xy,15),constant=1))&#10;mm &amp;lt;- mad(xy,constant=1)&#10;t &amp;lt;- mad(x,constant=1)/mm&#10;tr &amp;lt;- range(t,1/t)&#10;madrand &amp;lt;- madrand/mm&#10;hist(madrand,n=200)&#10;abline(v=mad(x,constant=1)/mm,col=3)&#10;abline(v=1/(mad(x,constant=1)/mm),col=3)&#10;&#10;(pvalue &amp;lt;- (sum(madrand &amp;lt;= tr[1])+sum(madrand &amp;gt;= tr[2]))/nrand)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This produces a p-value of 0.114 (with a binomial standard error of 0.001); the histogram looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/J1yA3.png&quot; alt=&quot;histogram of scaled mad of smaller sample&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[This looks 'rough' like I haven't sampled enough. Not so - the exact permutation distribution will look like this too; it's a result of using a median (of the absolute deviations from the median) on small samples -- the resulting statistic has a somewhat &quot;clumpy&quot; distribution even though the original data was continuous.]&lt;/p&gt;&#10;&#10;&lt;p&gt;If I'd been clever I'd have used a constant in the calls to &lt;code&gt;mad&lt;/code&gt; of &lt;code&gt;1/mm&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we wanted to retain the scale of the mad in our results, we could still do the other tail by computing &lt;code&gt;mm^2/mad(x,constant=1)&lt;/code&gt; as the cutoff and get the same resulting p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully that's clear enough, but I can elaborate if needed.&lt;/p&gt;&#10;&#10;&lt;p&gt;The ratio of mads that I originally discussed can be done almost as easily - the sampling will be slower (it will take maybe twice as long to run) but the fiddling around at the end would be simpler.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edit: If you're not wedded to the mad, the package exactRankTests has some alternative robust permutation tests of equality of spread.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-11-04T22:39:54.083" Id="74591" LastActivityDate="2013-11-05T03:16:42.860" LastEditDate="2013-11-05T03:16:42.860" LastEditorUserId="805" OwnerUserId="805" ParentId="74507" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The cost of compliance for the company is $c_{ij}$ and the penalty for not complying is $\Lambda$. Since $c_{ij}$ is a random variable with cdf $F$, we have that $F(\Lambda)$ is another way of writing $P(c_{ij} &amp;lt; \Lambda)$, or in other words the probability that the compliance cost will be less than the cost of not complying. Or in other words, the probability that the company will comply.&lt;/p&gt;&#10;&#10;&lt;p&gt;So saying that $F(\Lambda) &amp;lt;1$ is just saying that the probability that the company will comply is less than $1$. Similarly, $F(\Lambda) &amp;lt; 0.4$ means that there is less than a $40\%$ chance that the company will comply.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your questions in the comments about how to derive (3) and (4), to get (4) you observe that the only way in which a company can do more damage under dealing is if it's in category $\beta$ on page 365 of the paper. This is the same as $\mathrm{max}\{c_{i1}, c_{i2}\} &amp;lt; \Lambda$. Since $c_{i1}$ and $c_{i2}$ are independent, the probability of both of them being less than $\Lambda$ is&#10;$$P(c_{i1}, c_{i2} &amp;lt; \Lambda) = P(c_{i1} &amp;lt;\Lambda)P(c_{i2} &amp;lt; \Lambda) = F(\Lambda)^2$$&#10;which gives (4).&lt;/p&gt;&#10;&#10;&lt;p&gt;To get (3), the company needs to be in category $\alpha$ on page 364, which means that one $c_{ij}$ has to be between $\Lambda$ and $2\Lambda$ and the other $c_{ij}$ has to be greater than $\Lambda$. The desired probability is&#10;$$\alpha(\Lambda) = P((\Lambda &amp;lt; c_{i1} &amp;lt; 2\Lambda \text{ and } c_{i2} &amp;gt; \Lambda) \text{ OR } (\Lambda &amp;lt; c_{i2} &amp;lt; 2\Lambda \text{ and } c_{i1} &amp;gt; \Lambda))$$&#10;but when you have the ``OR&quot; of two events you have to take into account that they might have outcomes in common, so you need to use the formula $P(X \text{ or } Y)=P(X) + P(Y) - P(X \text{ and } Y)$. Here, this gives you&#10;$$\alpha(\Lambda) = P(\Lambda &amp;lt; c_{i1} &amp;lt; 2\Lambda \text{ and } c_{i2} &amp;gt; \Lambda) + P(\Lambda &amp;lt; c_{i2} &amp;lt; 2\Lambda \text{ and } c_{i1} &amp;gt; \Lambda) - P(\Lambda &amp;lt; c_{i1} &amp;lt; 2\Lambda \text{ and } c_{i2} &amp;gt; \Lambda \text{ and } \Lambda &amp;lt; c_{i2} &amp;lt; 2\Lambda \text{ and } c_{i1} &amp;gt; \Lambda)$$&#10;which using independence reduces to&#10;$$P(\Lambda &amp;lt; c_{i1} &amp;lt; 2\Lambda)P(c_{i2} &amp;gt; \Lambda) + P(\Lambda &amp;lt; c_{i2} &amp;lt; 2\Lambda)P(c_{i1} &amp;gt; \Lambda) - P(\Lambda &amp;lt; c_{i1} &amp;lt; 2\Lambda)P(\Lambda &amp;lt; c_{i2} &amp;lt; 2\Lambda)$$&#10;which gives&#10;$$\alpha(\Lambda) = 2(1-F(\Lambda))(F(2\Lambda)-F(\Lambda))-(F(2\Lambda)-F(\Lambda))^2$$&#10;which is (3).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-05T01:46:04.520" Id="74603" LastActivityDate="2013-11-05T01:46:04.520" OwnerUserId="13818" ParentId="57711" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="74633" AnswerCount="1" Body="&lt;p&gt;The Wikipedia article on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;two-sample Kolmogorov-Smirnov test&lt;/a&gt; states that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The Kolmogorov–Smirnov test may also be used to test whether two&#10;  underlying one-dimensional probability distributions differ. In this&#10;  case, the Kolmogorov–Smirnov statistic is&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$D_{n,n'}=\sup_x |F_{1,n}(x)-F_{2,n'}(x)|$$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;where $F_{1,n}$ and $F_{2,n'}$ are the empirical distribution&#10;  functions of the first and the second sample respectively. The null&#10;  hypothesis is rejected at level $\alpha$ if&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$D_{n,n'}&amp;gt;c(\alpha)\sqrt{\frac{n + n'}{n n'}}.$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It is not clear to me the meaning of the &lt;em&gt;$\alpha$ level&lt;/em&gt;. Where does it come from and what does it mean statistically?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-05T13:03:04.053" Id="74631" LastActivityDate="2013-11-05T13:56:50.450" LastEditDate="2013-11-05T13:11:25.560" LastEditorUserId="22311" OwnerUserId="10416" PostTypeId="1" Score="2" Tags="&lt;kolmogorov-smirnov&gt;" Title="Explain the meaning of $\alpha$ in a Kolmogorov-Smirnov two-sample test" ViewCount="173" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let $Z_1$ and $Z_2$ be categorical random variables with $3$ and $2$ categories, respectively. Let $Y_1$ and $Y_2$ be $2$ continuous random variables. Define completely the GLOM (general location model) for the joint distribution of $Z=(Z_1,Z_2)^T$ and $Y=(Y_1,Y_2)^T$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I couldn't solve this problem. Can anyone help me?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-05T13:33:17.720" Id="74635" LastActivityDate="2013-11-05T13:57:03.917" LastEditDate="2013-11-05T13:57:03.917" LastEditorUserId="7290" OwnerUserId="32335" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;categorical-data&gt;&lt;continuous-data&gt;&lt;joint-distribution&gt;" Title="General location model" ViewCount="42" />
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Notation.&lt;/strong&gt; Let $y$, $a$, and $b$ be $n\times 1$, $p\times 1$, and $q\times1$ real vectors. Let also $X$ and $Z$ be $n\times p$ and $n \times q$ real matrices. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that there is no solution, $a$, to $y = X a$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;. What are the conditions on $Z$ such that $y = Xa + Zb$ has no solution for each choice of $b$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;. I came across this problem in the context of linear regression. The fact that $y=Xa$ has no solution can be interpreted as &quot;no hyperplane can perfectly fit the data&quot;. I am analysing an extension of this problem which has lead me to the need for finding something similar for &quot;$y = Xa + Zb$ has no solution&quot;, but in this case $b$ is not fixed and can actually take any value in ${\mathbb R}^q$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-05T15:16:51.700" FavoriteCount="0" Id="74648" LastActivityDate="2014-08-26T01:59:13.653" LastEditDate="2013-11-05T15:42:46.343" LastEditorUserId="32344" OwnerUserId="32344" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;mixed-effect&gt;" Title="Equations from linear regression" ViewCount="81" />
  <row AcceptedAnswerId="74660" AnswerCount="1" Body="&lt;p&gt;This is a theoretical question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that I have a sample s1 coming from distribution K, and a sample s2 coming from distribution M. But I don't know what K or M are. I hypothesize that s1 and s2 are coming from distribution T. I plug in s1 and s2 to pdf of T and calculate their likelihoods, say L(s1), L(s2).  If I know differential entropies of K and M, being H(K) and H(M) respectively, can I find a relation between L(s1) and L(s2) in terms of H(K) and H(M). Basically what I am trying to show is something along the lines of this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac {L(s1)}{L(s2)} \sim \frac {e^{H(K)}} {e^{H(M)}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this even meaningful?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-05T17:08:48.903" Id="74654" LastActivityDate="2013-11-05T17:38:49.307" LastEditDate="2013-11-05T17:37:48.373" LastEditorUserId="20980" OwnerUserId="20980" PostTypeId="1" Score="0" Tags="&lt;entropy&gt;" Title="Entropy and Likelihood Relationship" ViewCount="68" />
  <row Body="&lt;p&gt;Jim Berger's review articles: &lt;a href=&quot;http://www.stat.duke.edu/~berger/papers.html&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.duke.edu/~berger/papers.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You might start with &lt;a href=&quot;http://www.stat.duke.edu/~berger/papers/02-01.html&quot; rel=&quot;nofollow&quot;&gt;Could Fisher, Jeffreys and Neyman have agreed upon testing?&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-11-05T17:52:09.723" CreationDate="2013-11-05T17:52:09.723" Id="74662" LastActivityDate="2013-11-05T17:52:09.723" OwnerUserId="319" ParentId="9365" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I just started taking survival analysis class and I'm stumped on this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;We need to show that when there are no censored observations $\hat{S}(t)=\prod_t_{(i)\le t}(n_i-d_i)/n_i$ equals to the empirical survival function $S_n(t)=\# \{ t_{(i)} \ge t \}/n  $&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks pretty obvious to me so I'm not sure how to approach this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-05T19:38:25.167" Id="74674" LastActivityDate="2013-11-05T20:57:19.250" LastEditDate="2013-11-05T20:57:19.250" LastEditorUserId="2970" OwnerUserId="23956" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;survival&gt;&lt;kaplan-meier&gt;" Title="Kaplan-Meier statistic with no censoring reduces to empirical survival function" ViewCount="76" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to make a model that predicts the survival rate of wildlife based on the severity of an entire winter. The idea is that colder weather in combination with lots of snow has a negative effect (the colder it is the longer the snow lasts). It might be cold winter with lots of snow, but if the cold and snow did not occur in the same month it would have less of an effect. In order to preserve the interaction of snow and temp, we want to use monthly data, but our survival will always be for the entire winter. It doesn't really matter which month the bad weather occurs in so I am hesitant to use month as a variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there some technique that I can use where it preserves the interaction of snow and temp each month but uses the cumulative effect of all months in a winter to predict survival?&lt;/p&gt;&#10;&#10;&lt;p&gt;example of what the data looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Year1 survival = 0.9&#10;month      temp     snow&#10; m1         -5       20&#10; m2        -15       20&#10; m3        -20      100&#10; m4          2      100&#10;&#10;Year2 survival = 0.7&#10;month     temp     snow&#10; m1        -5       20&#10; m2       -18      110&#10; m3       -20      100&#10; m4       -11       20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Temp was the average temperature for the month, and snow was the sum of all snow in a month.  I'm not sure where the comment about order statistics is going but if it makes a difference daily data is available.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-11-05T22:36:04.513" Id="74687" LastActivityDate="2013-11-06T02:03:31.753" LastEditDate="2013-11-06T02:03:31.753" LastEditorUserId="21793" OwnerUserId="21793" PostTypeId="1" Score="2" Tags="&lt;modeling&gt;" Title="modeling cumulative effect of a winter" ViewCount="46" />
  <row AnswerCount="4" Body="&lt;p&gt;I'm in an introductory statistics class in which the probability density function for continuous random variables has been defined as $P\left\{X\in B\right\}=\int_B f\left(x\right)dx$.  I understand that the integral of $\int\limits_a^af(x)dx=0$ but I can't rectify this with my intuition of a continuous random variable.  Say X is the random variable equal to the number of minutes from time t that the train arrives.  How do I calculate the probability that the train arrives exactly 5 minutes from now?  How can this probability be zero?  Is it not possible?  What if the train &lt;em&gt;does&lt;/em&gt; arrive exactly 5 minutes from now, how could it occur if it had probability 0?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-05T23:21:57.263" FavoriteCount="1" Id="74692" LastActivityDate="2014-12-18T01:32:57.467" LastEditDate="2014-12-18T01:32:57.467" LastEditorUserId="805" OwnerUserId="32371" PostTypeId="1" Score="9" Tags="&lt;probability&gt;&lt;random-variable&gt;&lt;pdf&gt;&lt;continuous-data&gt;" Title="Probability that a continuous random variable assumes a fixed point" ViewCount="223" />
  <row Body="&lt;p&gt;@digdeep, as usual @whuber provided an excellent and comprehensive answer from a statistical view point. I'm not trained in statistics, so take this response with a grain of salt. I have used the response below in my real world practice data, so I hope this is helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll try to provide a non statistician view of transformation of time series data for Arima modeling. There is no straightforward answer. Since you are interested in knowing which transformation to use, it might be helpful to review why we do transformation.We do transformation for 3 main reasons and there might be ton of other reasons:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Transformation makes the data's linear structure more usable for&lt;br&gt;&#10;ARIMA modeling. &lt;/li&gt;&#10;&lt;li&gt;If variance in the data is increasing or changing then transformation of data might be helpful to stabilize the variance in data.&lt;/li&gt;&#10;&lt;li&gt;Transformation also makes the errors/residuals in &#10;    ARIMA model normally distributed which is a requirement in ARIMA modeling proposed by Box-Jenkins.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There are several data transformations including Box-Cox, Log, square root, quartic and inverse and other transformations mentioned @irishstat. As with all the statistical methods there is no good guidance/answer on which transformation to select for a particular dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;As the famous statistician G.E.P Box said &quot;All models are wrong but some are useful&quot;, this would apply to the transformations as well &quot;All &lt;em&gt;transformations&lt;/em&gt; are wrong but some are useful&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to choose a transformation is to experiment. Since you have a long time series, I would hold out the last 12 - 24 months, and build a model using all the transformation and see if a particular transformation is helpful at predicting your out of sample data accurately. Also examine the residuals for normality assumption of your model. Hopefully, this would guide you in choosing an appropriate transformation. You might also want to compare this with non-transformed data and see if the transformation helped your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;@whuber's excellent graphical representation of your data motivated me to explore this data graphically using a decomposition method. I might  add, R has an excellent decomposition method called STL which would be helpful in identifying patterns that you would normally not notice. For a dataset like this, STL decomposition is helpful in not only selecting an appropriate method for analyzing your data, it might also  be helpful in identifying anomalies such as outliers/level shift/change in seasonality etc., See below. Notice that the remainder (irregular) component of the data, looks like there is stochastic seasonality and the variation is not random, there appears to be a pattern. See also change in level of trend component after 2004/2005 that @whuber is refrencing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully this is helpful.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;g &amp;lt;- stl(y,s.window = &quot;periodic&quot;)&#10;plot(g)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/EMDl6.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-05T23:49:58.747" Id="74695" LastActivityDate="2013-11-06T00:07:22.410" LastEditDate="2013-11-06T00:07:22.410" LastEditorUserId="29137" OwnerUserId="29137" ParentId="74537" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Yes, that's the necessary and sufficient condition for independence not only for two random variables but also for a (finite) sequence of random variables. Check out for example P.2 on page 242 of &lt;a href=&quot;http://books.google.ca/books?id=zF1kkWU_fmQC&amp;amp;pg=PA242&amp;amp;dq=moment%20generating%20function%20independent%20if%20and%20only%20if&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=h7J5UoaRNuGfyQGQtIGACw&amp;amp;ved=0CEIQ6AEwAw#v=onepage&amp;amp;q=moment%20generating%20function%20independent%20if%20and%20only%20if&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Probability with Statistical Applications&lt;/a&gt;, By Rinaldo B. Schinazi. Or page 259 of&#10;&lt;a href=&quot;http://books.google.ca/books?id=mFi05v3OVE0C&amp;amp;pg=PA259&amp;amp;dq=moment%20generating%20function%20independent%20if%20and%20only&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=PrF5UvLMNu3iyAHMmgE&amp;amp;ved=0CEEQ6AEwAw#v=onepage&amp;amp;q=moment%20generating%20function%20independent%20if%20and%20only&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Econometric Analysis of Count Data&lt;/a&gt; which is based on probability generating function. Just note that &quot;the moment-generating function does not always exist&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-06T03:14:02.577" Id="74717" LastActivityDate="2013-11-06T03:21:53.083" LastEditDate="2013-11-06T03:21:53.083" LastEditorUserId="13138" OwnerUserId="13138" ParentId="74711" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Your question is quite general (specific cases may offer shortcuts), so I'll limit myself to suggesting strategies. Typically such a question involves constructing appropriate limits on integrals and trying to evaluate them by some means. Usually there will be a bivariate integral where the limits in the inner integral involve the variable in the outer integral. Sometimes the hardest part is simply writing the correct limits down; the general case will involve &quot;min&quot; and &quot;max&quot; functions on a problem like this. &lt;/p&gt;&#10;&#10;&lt;p&gt;To make progress I strongly suggest you get into the habit of making diagrams of the region you're trying to integrate.&lt;/p&gt;&#10;&#10;&lt;p&gt;A couple of suggested strategies for approaching such a problem, by making slightly simpler problems you might see how to write integrals for. &lt;/p&gt;&#10;&#10;&lt;p&gt;One approach: Let $Z = Y-aX$ and work out the joint probability in terms of $X$ and $Z$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach: First, replace your $x$ and $y$ with $x_0$ and $y_0$ so I can use $x$ and $y$ as dummy variables in the integration. If $a\leq y_0/x_0\leq b$ then you have a region like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tdNn1.png&quot; alt=&quot;region&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;While you can actually write the integral by splitting it up into pieces (move a vertical line along the $x$-axis and split the integral where your line hits any 'corners' on the dark green region), you might otherwise evaluate it by working out the probability of being in the rectangle $0&amp;lt;X&amp;lt;x_0; 0&amp;lt;Y&amp;lt;y_0$ and then subtract the two triangles.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other cases (the other arrangements of $x_0$ and $y_0$ relative to $a$ and $b$) might be worked out by drawing the relevant diagram in order to obtain the right limits on the integrals; in each case you'll do something similar, but sometimes you might not hit a corner.&lt;/p&gt;&#10;&#10;&lt;p&gt;With more details, more specific responses might be possible.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-06T07:12:01.233" Id="74726" LastActivityDate="2013-11-06T22:29:20.360" LastEditDate="2013-11-06T22:29:20.360" LastEditorUserId="805" OwnerUserId="805" ParentId="74724" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I think the answer to your question is yes. What you have is two columns, one is a long list of combinations and one with a 1 or a 0. 1 if there is a video at that combination and 0 if there is not a video. Were you to sum this second column, then this will give you the number of videos. For the whole population this works. &lt;/p&gt;&#10;&#10;&lt;p&gt;The only problem I see is in drawing a random sample as I have no idea how youtube generates its urls. You could of course send them an email and ask. I would just give it a shot with the script, try different ways to generate random urls take a large sample (n&gt;1,000) and see whether the estimates are close enough.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-06T12:40:09.360" Id="74744" LastActivityDate="2013-11-06T12:40:09.360" OwnerUserId="31837" ParentId="74731" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I am implementing an EEG classifier with 15 subjects (patients), specifically a support vector machine classifier.&lt;/p&gt;&#10;&#10;&lt;p&gt;I randomly choose the training and testing sets, but I was faced by a question &quot;how did you choose subjects in each set?&quot;. I looked for the response but I couldn't find a good one (cross validation wouldn't be the best solution in my case). &lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please help me with this problem?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-11-06T13:44:37.607" Id="74751" LastActivityDate="2014-03-09T06:52:31.660" LastEditDate="2013-11-07T10:20:52.797" LastEditorUserId="805" OwnerUserId="32400" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;cross-validation&gt;&lt;train&gt;" Title="Data split into training and test" ViewCount="436" />
  <row AnswerCount="2" Body="&lt;p&gt;Imagine an experiment where you roll two fair, six-sided dice. Someone peeks at the dice, and (truthfully) tells you that &quot;at least one of the dice is a 4&quot;. What is the probability that the total of the dice is 7?&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems straightforward to calculate that the probability the total is 7 is 2/11.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the person who peeked at the dice could equally well have said &quot;at least one of the dice is a 1&quot; and you would come to the same conclusion - 2/11. Or they could have said &quot;at least one of the dice is a 2&quot; or &quot;at least one of the dice is a 3&quot;, or indeed any number from 1 to 6, and you would still conclude that the probability that the total is 7 is 2/11.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you will always conclude that the probability that the total is 7 is 2/11, you could block your ears as they speak, and you'd still come up with 2/11. From there it's a short hop to conclude that &lt;em&gt;even if they don't say anything&lt;/em&gt; the probability that the total is 7 is 2/11.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, clearly if they don't say anything, the probability that the total is 7 is not 2/11, but rather 1/6.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where is the flaw in the argument?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-06T15:07:02.440" FavoriteCount="2" Id="74755" LastActivityDate="2013-11-06T20:56:39.623" OwnerUserId="2425" PostTypeId="1" Score="6" Tags="&lt;probability&gt;&lt;conditional-probability&gt;&lt;bayes&gt;" Title="Flaw in a conditional probability argument" ViewCount="172" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;The maximum by which X1 can increase is 0.30-0.001. Following your&#10;  explanation, if I want to compare the max. value with the lowest&#10;  value, does this mean that the average log(y) increases by 1.5/100 *&#10;  0.3-0.001, i.e. around 45%?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No, because the simple way of multiplying the coefficient by 100 and then interpreting as a factorial change only works when both the coefficient and the change in the independent variable are relatively small.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the equation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln(y) = \alpha + 1.5X_1 + 0.8 X_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;With one percent point increase:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln(y') = \alpha + 1.5(X_1+0.01) + 0.8 X_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Subtracting for the difference:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln(y') - \ln(y) = 1.5 \times 0.01$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln(y'/y) = 0.015$&lt;/p&gt;&#10;&#10;&lt;p&gt;The factorial change is then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y'/y = 1.015113$,&lt;/p&gt;&#10;&#10;&lt;p&gt;which is about 1.5%. So, that approximation shortcut works.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;When the change is large, like your 0.299, the approximation can deviate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln(y'/y) = 1.5 \times 0.299$&lt;/p&gt;&#10;&#10;&lt;p&gt;The factorial change is then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y'/y = 1.565961$,&lt;/p&gt;&#10;&#10;&lt;p&gt;about 56.6% larger instead of the approximated 45%.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-06T15:13:40.960" Id="74757" LastActivityDate="2013-11-06T15:28:38.413" LastEditDate="2013-11-06T15:28:38.413" LastEditorUserId="22047" OwnerUserId="13047" ParentId="74734" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Bill has given a great answer. But maybe some practical things are worth adding.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;code&gt;nls&lt;/code&gt; is sensitive to starting values. Depending on your staring values you're going to get different answers or your model might not converge. That's life. No reason to be surprised. An important part of &lt;code&gt;nls&lt;/code&gt; is working out a methodology of deriving your starting values. &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;nls&lt;/code&gt; is particularly sensitive. If you're having troubles you should always &lt;code&gt;minpack.lm&lt;/code&gt; before reassessing your approach. Using an optimizing function that is more robust to starting values.&lt;/li&gt;&#10;&lt;li&gt;Try what Bill recommends. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-11-06T20:47:27.883" Id="74785" LastActivityDate="2013-11-06T21:00:10.827" LastEditDate="2013-11-06T21:00:10.827" LastEditorUserId="7290" OwnerUserId="32379" ParentId="74752" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have the following data&lt;/p&gt;&#10;&#10;&lt;p&gt;X 1 2 3 4 5…&lt;/p&gt;&#10;&#10;&lt;p&gt;Y 10 12 13 14 15…&lt;/p&gt;&#10;&#10;&lt;p&gt;X/Y 10% 16% 23% etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I find the standard deviation of percentage (last line)? Can I treat the ratio as a normal distribution and apply regular SD formula?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-06T22:36:03.583" Id="74797" LastActivityDate="2014-01-06T12:21:15.313" OwnerUserId="32424" PostTypeId="1" Score="0" Tags="&lt;standard-deviation&gt;" Title="Calculating Standard deviation of percentages?" ViewCount="6937" />
  
  
  <row Body="&lt;p&gt;I would NOT recommend the $R^2$ as this measure increases as the number of variables increases. In other words, the $R^2$ does not account for overfitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;Among the options you mentioned the adjusted $R^2$ would be the best. If you take a look at the formula:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R^2_{adj} = 1 - \frac{(1-R^2)\cdot(n-1)}{n-p-1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the number of parameters $p$ is in the denominator of the formula, the addition of variables that do not increase significantly the $R^2$ will penalize the $R^2_{adj}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;A better approach to compare your models would be to use the Akaike Information Criterion:&lt;/p&gt;&#10;&#10;&lt;p&gt;$AIC_i = -2\cdot log(\mathcal{L}_i) + 2\cdot p_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathcal{L}_i$ is the likelihood of model $i$&lt;/p&gt;&#10;&#10;&lt;p&gt;You could obtain this very easy in R by using the AIC function:&#10;&lt;code&gt; AIC(model1, model2) &lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-07T00:45:53.630" Id="74811" LastActivityDate="2013-11-07T00:45:53.630" OwnerUserId="17045" ParentId="74796" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I seeking help in understanding specific values underlying the formula's for the MA(p) model &amp;amp; the AR(q) model. I am attempting to implement the models (building up to the combined ARIMA model) in the programming language Java.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not come from an overly mathematical (I'm fairly new to statistics at least) background so be gentle.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the formula I am using for the AR(p) model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_t - μ = β_1(X_{t-1} - μ) + ... +  β_p(X_{t-p} - μ) + Z_t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $X$ is the time series, $μ$ is the mean of the time series, $β$ is the auto-correlation coefficient at a specific lag, $p$ is the order of the model and $Z$ is white noise of mean $0$ and variance $σ^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm fairly certain I have the above figured out, however the term &quot;$Z_t$&quot; confuses me. How would I implement this in code? I understand it is &quot;random&quot; however what are its ranges? Surely there must be a maximum and minimum of the term $Z_t$. Is it somehow based on the variance of the overall dataset? How is the &quot;$Z$&quot; value calculated on implementation exactly?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the formula I am using for the MA(q) model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_t - μ = Z_t - θ_1(Z_{t-1}) - ... - θ_q(Z_{t-q})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $X$ is again the time series dataset, $μ$ is the mean of the dataset, $Z$ is white noise with mean 0 and variance $σ^2$, $θ$ is the correlation coefficient at a specific lag and $q$ is the order of the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Again I the same issue as the above model in regards to the &quot;$Z$&quot; term. Also $θ$ is also the correlation coefficient of the dataset at various different lags, correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help on this matter would be extremely welcomed and if you have any questions I would more than happy to answer them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any use of examples alongside a full dataset i.e., X = (1,2,3,4,5,6,7) would be also extremely welcomed as it helps me understand the concept much more easily. Also please try to keep the explaination as idiot proof and contained as possible.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-07T02:27:04.507" Id="74817" LastActivityDate="2014-03-09T23:52:34.157" LastEditDate="2013-11-07T03:22:56.243" LastEditorUserId="31264" OwnerUserId="32433" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;predictive-models&gt;&lt;arima&gt;&lt;autoregressive&gt;" Title="Auto-Regressional &amp; Moving Average Model Formula Properties" ViewCount="274" />
  <row AcceptedAnswerId="74834" AnswerCount="1" Body="&lt;p&gt;First time question on this site, so please bear with me, thank you:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 6 coin-flip-type experiments for which I can calculate 6 binomial p-values. I would now like to calculate the significance of observing at least 4 p-values &amp;lt; 0.05 in six experiments total.&lt;/p&gt;&#10;&#10;&lt;p&gt;In one approach, I used Fisher's method (&lt;a href=&quot;http://en.wikipedia.org/wiki/Fishers_method&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Fishers_method&lt;/a&gt;) to compound the p-values, but I wanted to add an additional test based on simulating the original coin-flip data.&lt;/p&gt;&#10;&#10;&lt;p&gt;To this end, I performed random coin-flips (P=0.5) for each of the 6 experiments; the total number of flips differs between these 6 experiments but is irrelevant. I then count how many times (out of 100 simulations) the binomial p-value &amp;lt; 0.05. Simulating the original data 100 times, I get the following number of significant binomial p-values (&quot;false positive hits&quot;) from these 6 experiments:&lt;/p&gt;&#10;&#10;&lt;p&gt;12, 13, 9, 10, 7, 11&lt;/p&gt;&#10;&#10;&lt;p&gt;Or divided by 100 (= frequency of false positive hits in simulated experiments):&lt;/p&gt;&#10;&#10;&lt;p&gt;0.12, 0.13, 0.09, 0.1, 0.07, 0.11&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I calculate the probability that 4 or more out of these 6 would be positive given these frequencies? I realize that for calculating the probability that 6/6 are positive, I would simply multiply 0.12 x 0.13 x 0.09 x 0.1 x 0.07 x 0.11. But for 1-5/6 it's more complicated. I'm leaning towards a hypergeometric test, since I have to draw 6 times and I &lt;em&gt;think&lt;/em&gt; there's no replacement, but I want to double-check with you experts.&#10;Thank you! &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-07T02:40:18.163" Id="74818" LastActivityDate="2013-11-07T20:17:51.090" LastEditDate="2013-11-07T20:17:51.090" LastEditorUserId="32432" OwnerUserId="32432" PostTypeId="1" Score="2" Tags="&lt;binomial&gt;&lt;aggregation&gt;&lt;hypergeometric&gt;" Title="Significance test across multiple simulated experiments" ViewCount="140" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to apply a &lt;em&gt;Granger Causality test&lt;/em&gt; to panel data. I've found enough literature to understand that topic. However, I've been unable to find and &lt;strong&gt;R&lt;/strong&gt; package to carry out that analysis. I'm wondering if anybody know whether there is around any package to deal with that. Thanks!&lt;/p&gt;&#10;&#10;&lt;h3&gt;&lt;em&gt;I'm adding a potential solution, but new questions arose.&lt;/em&gt;&lt;/h3&gt;&#10;&#10;&lt;p&gt;The solution that I found is apply a &lt;em&gt;Granger Non- Causality test&lt;/em&gt; and using &lt;em&gt;Generalized Method of Moments&lt;/em&gt; (&lt;em&gt;GMM&lt;/em&gt;). In the &lt;a href=&quot;http://www.fnu.zmaw.de/fileadmin/fnu-files/publication/working-papers/FNU47.pdf&quot; rel=&quot;nofollow&quot;&gt;Erdil &amp;amp; Yetkiner’s (2004)&lt;/a&gt; paper you can find a description of &lt;em&gt;Granger non-causality test&lt;/em&gt; with panel data. To perform a &lt;em&gt;GMM&lt;/em&gt; I used the &lt;code&gt;plm&lt;/code&gt; package for &lt;strong&gt;R&lt;/strong&gt;. If you have a look at its tutorial (&lt;a href=&quot;http://www.jstatsoft.org/v27/i02/paper&quot; rel=&quot;nofollow&quot;&gt;Croissant &amp;amp; Millo, 2008&lt;/a&gt;), you’ll see that the built-in function &lt;code&gt;pgmm&lt;/code&gt; (page 17) removes the individual effect by the first difference and time dummies are included. The function’s summary also provides some tests to assess the model. For instance, to check serial autocorrelation in the residuals, Wald tests for coefficients and for time dummies and the Sargan test to evaluate if there is correlation between the instrumental variable and the residuals. Then I performed a Wald test (the first one in &lt;a href=&quot;http://www.fnu.zmaw.de/fileadmin/fnu-files/publication/working-papers/FNU47.pdf&quot; rel=&quot;nofollow&quot;&gt;Erdil &amp;amp; Yetkiner, 2004&lt;/a&gt;) with the sum of squared residuals of an unrestricted model (&lt;em&gt;SSRu&lt;/em&gt;) and of a restricted model (&lt;em&gt;SSRr&lt;/em&gt;).&#10;Now, my questions for the audience are:   &lt;/p&gt;&#10;&#10;&lt;p&gt;1) Do the time dummies remove the time effect?  I think so.&lt;/p&gt;&#10;&#10;&lt;p&gt;1.1) What if the time dummies aren't significant?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Therefore, if I got rid of the individuals and time effects, is the Wald test (&lt;em&gt;SSRr-SSRu&lt;/em&gt;) be as a Wald test applied to an &lt;code&gt;OLS&lt;/code&gt; model? I think so.&#10;If so, I’m not sure about the freedom degrees. Let’s see first the test suggested by Erdil &amp;amp; Yetkiner (2004):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$W=\frac{(SSRr-SSRu)/Np}{SSRu/[NT-N(1+p)-p]}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;em&gt;N&lt;/em&gt;= number of individuals, &lt;em&gt;T&lt;/em&gt;=years and &lt;em&gt;p&lt;/em&gt;=number of lags. Note that they didn’t get rid of individuals and time effects (at least that's what I understood).&#10;Now, if I got rid of individuals and time effects the Wald test as applied to OLS models would be:&#10; $$W=\frac{(SSRr-SSRu)/m}{SSRu/ (n-k)}$$ where &lt;em&gt;m&lt;/em&gt;= number of restrictions (number of coefficients that were removed from the unrestricted model to turn it restricted), &lt;em&gt;k&lt;/em&gt;= total number of coefficients in the unrestricted model and &lt;em&gt;n&lt;/em&gt;= number of observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;More questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;3) What is number of observation?&lt;/p&gt;&#10;&#10;&lt;p&gt;3.1) Is it the number of year or number of years*number of individuals? If it is number of years it seems reasonable, but if it is the product between years and individual it doesn’t. For example, in my case I have 328 individual and 13 years, so it is 4264; therefore, the numerator in the Wald test will be very, very small and I’ll be rejecting everything.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, &lt;/p&gt;&#10;&#10;&lt;p&gt;4) Am I right doing as I did?&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, any help will be much appreciated &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-07T06:18:12.660" Id="74832" LastActivityDate="2013-11-29T12:54:22.163" LastEditDate="2013-11-29T12:54:22.163" LastEditorUserId="17017" OwnerUserId="17017" PostTypeId="1" Score="1" Tags="&lt;econometrics&gt;&lt;panel-data&gt;&lt;granger-causality&gt;&lt;generalized-moments&gt;" Title="Granger Causality Testing With Panel Data" ViewCount="1575" />
  
  <row Body="&lt;p&gt;The first takes a value on the &lt;code&gt;x-value&lt;/code&gt; and degrees of freedom and reports the amount of probability to the left of the &lt;code&gt;x-value&lt;/code&gt;. If you're familiar with calculus, this is the equivalent of taking the integral of the Student's &lt;em&gt;t&lt;/em&gt; probability density function over the interval $(-\infty,x]$. If you're familiar with statistics, this is the Student's &lt;em&gt;t&lt;/em&gt; cumulative density function evaluated at $x$ for some degrees of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second does the inverse, taking some probability and returning the corresponding &lt;code&gt;x-value&lt;/code&gt;. In statistics, this is called the quantile function. &lt;/p&gt;&#10;&#10;&lt;p&gt;So the &lt;em&gt;mathematical&lt;/em&gt; relationship between the two is the same as for any function and its inverse, with the &lt;em&gt;substantive&lt;/em&gt; knowledge that they also have probability-based interpretations and statistical applications.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-07T14:43:37.290" Id="74875" LastActivityDate="2013-11-07T14:57:32.813" LastEditDate="2013-11-07T14:57:32.813" LastEditorUserId="22311" OwnerUserId="22311" ParentId="74874" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Let's work out the general answer: we have independent identically distributed random variables $X_i$, and a sample of $n$ instantiations of these RVs. I'll denote the sample maximum by $X_{max}$ and the sample minimum by $X_{min}$. Let the $X_i$ have the cdf $F$. Then we have the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(X_{max} \leq x \cap X_{min}&amp;gt; y) = (F(x) - F(y))^n \textbf{1}_{\{x \geq y\}} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $\textbf{1}$ is the indicator function. This holds because for $x\geq y$, the probability that the sample maximum and minimum are in the interval $(y, x]$ is equal to the probability that each of the $n$ random variables is in this interval.&#10;Then to get the joint density, use:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(X_{max} \leq x \cap X_{min}\leq y) = P(X_{max} \leq x) - P(X_{max} \leq x \cap X_{min}&amp;gt; y) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(X_{max} \leq x \cap X_{min}\leq y) = F(x)^n- (F(x) - F(y))^n \textbf{1}_{\{x \geq y\}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you work out what this is for $n=2$, and look at the cases $x \geq y$ and $x &amp;lt; y$, this is equivalent to the expression you gave.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-07T16:12:12.497" Id="74880" LastActivityDate="2013-11-07T16:12:12.497" OwnerUserId="31485" ParentId="74841" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;This is a problem in which working from first principles is better than specializing from poorly-understood general formulas. If $X$ and $Y$ are&#10;the outcome on the two dice, then their joint mass function $p_{X,Y}(i,j)$ has value&#10;$\frac{1}{36}$ for all $i,j \in \{1, 2, \dots, 6\}$ and so $p_{X_1,X_2}(i,j)$ has value $\frac{2}{36}$ if $1 \leq i &amp;lt; j \leq 6$, and value $\frac{1}{36}$ if $1 \leq i=j \leq 6$. The CDF can be worked out from this, but writing it out explicitly&#10;gives a long multi-case expression that I will leave to the OP to figure out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-07T16:28:36.937" Id="74884" LastActivityDate="2013-11-07T16:28:36.937" OwnerUserId="6633" ParentId="74841" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I'm reluctant to post because I don't quite understand what you're looking for here. Maybe this will jumpstart someone else to respond that has a better grasp of what you want.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;I would like to draw conclusions about whether Z is in fact a significant factor in the outcome variable via interactions b4 and b5, but I understand that in a logistic regression all coefficients and particularly interactions need to be evaluated in the context of specific values of the independent variable x.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The statement above isn't quite correct. Z is significant, the p-value for the interaction term is significant so Z is significant. You don't need to go much further (unless you tested an unreasonable number of interactions/data size small). It does not make a difference that Z is not significant if the higher order interactions are significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;To estimate the effect of Z on the outcome, this needs to be done in the context of specific values of the independent variable x. This is often best done graphically. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-07T17:26:20.267" Id="74890" LastActivityDate="2013-11-07T17:26:20.267" OwnerUserId="32379" ParentId="74882" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="74919" AnswerCount="1" Body="&lt;p&gt;I am sure this already exists but I just don't know the terminology to look for.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have three sets of 10 measurements. Each set corresponds to a different geographic region.&#10;So in total I have 30 measurements of my variable, and I have the factor &quot;region&quot; with 3 levels (west region, middle region, east region).&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I do a simple ANOVA and I get differences between the 3 regions. But I want to play a little with the possibility of this differences being &quot;by chance&quot;. Or, in another scenario, let's say I can't use ANOVA for some reason (eg. strongly inhomogeneous variances) and I use a non-parametric test and I don't find differences&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know if it's possible to do the following (or if the idea is appropriate):&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is really no difference between the 3 regions, then I can assume that any test (eg ANOVA or a non-parametric equivalent) will find approximate the same results even if I randomly mix all data once and again. So I thought I could simulate this, using my own data but just in different grouping. for example:&#10;1- take all the 30 values from my own measurements&#10;2- shuffle them into 3 groups, ie. randomly choose 10 values and assign them to a randomly chosen group; repeat with the next 10 data and then you have again 3 groups of 10 measurements.&#10;3- run the test (eg. ANOVA)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I go back to 1, and repeat this eg 1000 times, and see if there is a convergence towards a &quot;stable&quot; pattern. If there is, then there are actually no differences.&#10;If the convergence deviates a lot from the results I found with my &quot;real&quot; dataset, then I may think there are actually differences between the 3 regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my reasoning correct/sound? I know there is something like this, I just don't remember the name.. I thought it was related to permutations but I'm not sure...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-07T21:31:58.190" Id="74918" LastActivityDate="2014-01-23T14:19:10.207" LastEditDate="2014-01-23T14:19:10.207" LastEditorUserId="805" OwnerUserId="27504" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;statistical-significance&gt;&lt;simulation&gt;" Title="shuffle my data to investigate differences between 3 groups" ViewCount="87" />
  
  
  
  <row Body="&lt;p&gt;The documentary about Andrew Wiles proof of Fermat's Last Theorem is fantastic:&#10;&lt;a href=&quot;http://www.pbs.org/wgbh/nova/proof/&quot; rel=&quot;nofollow&quot;&gt;http://www.pbs.org/wgbh/nova/proof/&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Available on youtube:&#10;&lt;a href=&quot;http://www.youtube.com/watch?v=7FnXgprKgSE&quot; rel=&quot;nofollow&quot;&gt;http://www.youtube.com/watch?v=7FnXgprKgSE&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-11-08T16:12:46.300" CreationDate="2013-11-08T16:12:46.300" Id="74987" LastActivityDate="2013-11-08T16:12:46.300" OwnerUserId="3748" ParentId="10459" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;So I'm doing network simulations, and I'd like to know how long does each simulation run needs to be.&#10;My network is quite simple: its composed of &lt;strong&gt;multiple&lt;/strong&gt; M/M/1/H queues (Markovian processes + finite waiting queues):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;clients arrive at some node(s), are put into waiting queues&lt;/li&gt;&#10;&lt;li&gt;each node process one client at a time&lt;/li&gt;&#10;&lt;li&gt;a processed client is diriged to another node, or goes out of the system&lt;/li&gt;&#10;&lt;li&gt;all random processes follow a Poisson distribution&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The finite waiting queues prevents it of being a Poisson process: the output of a node is no longer Poisson, so neither will be the inputs of the nodes after it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought I could try to plot the variance and mean of the waiting time (inside the queues) and see when it becomes &quot;stable&quot;?&#10;Would that be a good solution? How else could I do that? &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-08T16:15:51.660" Id="74989" LastActivityDate="2013-11-09T13:50:10.063" LastEditDate="2013-11-09T13:50:10.063" LastEditorUserId="88" OwnerUserId="32518" PostTypeId="1" Score="1" Tags="&lt;simulation&gt;&lt;networks&gt;" Title="How to determine a good simulation time?" ViewCount="51" />
  
  
  <row Body="&lt;p&gt;First, I am not a statistician, just a researcher who has looked into it alot the last few years to figure out why the methods I observe being used around me are so lacking and why there is so much confusion about basic concepts like the &quot;what is a p-value?&quot; I will give my perspective.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;First, one clarification question:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The Time magazine wrote,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;A power of 0.8 means that of ten true hypotheses tested, only two will be ruled out &amp;gt; because their effects are not picked up in the&#10;&lt;/code&gt;&lt;/pre&gt;&#10;  &#10;  &lt;p&gt;data;&quot;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;I am not sure how this fits into the definition of the power function&#10;  I found in textbook, which is the probability of rejecting the null as&#10;  a function of parameter θ. With different θ we have different power,&#10;  so I don't quite understand the above quote.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Power is a function of θ, variance, and sample size. I am not sure what the confusion is. Also for many cases in which significance testing is used null hypothesis of mean1=mean2 is always false. In these cases &lt;em&gt;significance&lt;/em&gt; is only a function of sample size. Please read Paul Meehl's &lt;a href=&quot;http://mres.gmu.edu/pmwiki/uploads/Main/Meehl1967.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Theory-Testing in Psychology and Physics: A Methodological Paradox&quot;&lt;/a&gt; it clarified many things for me and I have never seen an adequate response. Paul Meehl has a few other papers on this you can find by searching his name. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In my field of political science / economics, scholars simply use up&#10;  all the country-year data available. Thus, should we not be concerned&#10;  with sample fiddling here?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you read the Simmons 2011 paper this is only one of the &quot;p-hacking&quot; techniques mentioned. If it is true that there is only one data set and no one picks out selective samples from it then I guess there is no room for increasing sample size.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Can the problem of running multiple tests but reporting only one model&#10;  be fixed simply by the fact that someone else in the discipline will&#10;  re-test your paper and strike you down immediately for not having&#10;  robust results? Anticipating this, scholars in my field are more&#10;  likely to include a robustness check section, where they show that&#10;  multiple model specifications does not change the result. Is this&#10;  sufficient?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If replication was occurring without publication bias there would be no need for &quot;journals of the null result&quot;. I would say the robustness check section is good to have but is not sufficient in the presence of researchers failing to publish what they consider null results. Also I would not consider a result robust just because multiple analysis techniques on the same data come to the same conclusion. A robust result is one that makes a correct prediction of effect/correlation/etc on &lt;strong&gt;new data&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;A replication is not getting p&amp;lt;0.05 both times. The theory should be considered more robust if it predicted a different effect/correlation/etc than used in the first study. I do not refer to the presence of an effect or correlation, but the precise value or a small range of values compared to possible range of values. The presence of increased/decreased effect or positive/negative correlation are 100% likely to be true in the case of the null hypothesis being false. Read Meehl.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Andrew Gelman and others raise the point that no matter the data, it&#10;  would be always possible to find and publish some &quot;pattern&quot; that isn't&#10;  really there. But this should not be a concern, given the fact that&#10;  any empirical &quot;pattern&quot; must be supported by a theory, and rival&#10;  theories within a discipline will just engage in an debate / race to&#10;  find which camp is able to find more &quot;patterns&quot; in various places. If&#10;  a pattern is truly spurious, then the theory behind will be quickly&#10;  struck down when there is no similar pattern in other samples /&#10;  settings. Isn't this how science progresses?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Science cannot function properly if researchers are failing to publish null results. Also just because the pattern was not discovered in the second sample/setting does not mean it does not exist under the conditions of the initial study.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Assuming that the current trend of journals for null result will&#10;  actually flourish, is there a way for us to aggregate all the null and&#10;  positive results together and make an inference on the theory that&#10;  they all try to test?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This would be &lt;a href=&quot;https://en.wikipedia.org/wiki/Meta-analysis&quot; rel=&quot;nofollow&quot;&gt;meta-analysis&lt;/a&gt;. There is nothing special about null results in this case other than that researchers do not publish them because the p-values were above the arbitrary threshold. In the presence of publication bias meta-analysis is unreliable as is the entire literature suffering from publication bias. While it can be useful, meta analysis is far inferior for assessing a theory than having that theory make a precise prediction that is then tested. Publication bias does not matter nearly as much as long as new predictions pan out and are replicated by independent groups.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-08T18:55:14.857" Id="75001" LastActivityDate="2013-11-08T18:55:14.857" OwnerUserId="31334" ParentId="74939" PostTypeId="2" Score="3" />
  
  
  
  
  
  
  
  <row AcceptedAnswerId="76080" AnswerCount="4" Body="&lt;p&gt;I am learning R and have been experimenting with analysis of variance.  I have been running both&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;kruskal.test(depVar ~ indepVar, data=df)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova(lm(depVar ~ indepVar, data=dF))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there a practical difference between these two tests?  My understanding is that they both evaluate the null hypothesis that the populations have the same mean.  Thanks in advance&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-09T18:36:42.673" FavoriteCount="4" Id="76059" LastActivityDate="2013-11-10T13:13:52.720" OwnerUserId="34560" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;anova&gt;&lt;kruskal-wallis&gt;" Title="Difference Between ANOVA and Kruskal-Wallis test" ViewCount="5467" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; $MGF_X(t)=E(e^{Xt})$ is the definition of the moment generating function of a random variable, which is a very convenient tool to work with.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; The statistical estimators, like OLS, MLE, etc, are by construction functions of random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3)&lt;/strong&gt; Apart from the above, we consider functions of random variables whenever a theoretical model leads us there, or whenever a non-linear transformation induces (or is thought to induce) desirable properties at the technical estimation level.&#10;Two examples:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You want to estimate the parameters of a &quot;Cobb-Douglas&quot; production function, $Q = A K^aL^{1-a}$. ($Q$= quantity, $K$=capital, $L$=labor). You take the logs to transform the problem into a linear one&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$$\ln Q = \ln A + a\ln K + (1-a)\ln L + u$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you are dealing with functions of random variables. If you want to consider the estimated error term back in levels you will have to consider $E(e^u)$.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You want to estimate a binary-choice model, using the logit specification. $Y$ is the 0/1 binary dependent variable, $X$ is a matrix containing the variables that are thought to affect the probability of $Y$ acquiring the value $1$. Your model is &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$$P(Y=1\mid X) = \Lambda(X) = \Big( 1+ \exp\{-a-X\beta\}\Big)^{-1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and it contains functions of random variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-09T19:09:52.763" Id="76061" LastActivityDate="2013-11-09T19:09:52.763" OwnerUserId="28746" ParentId="76060" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Yes there is. The &lt;code&gt;anova&lt;/code&gt; is a parametric approach while &lt;code&gt;kruskal.test&lt;/code&gt; is a non parametric approach. So  &lt;code&gt;kruskal.test&lt;/code&gt; does not need any distributional assumption.&lt;br&gt;&#10;From practical point of view, when your data is skewed, then &lt;code&gt;anova&lt;/code&gt; would not a be good approach to use. Have a look at &lt;a href=&quot;http://stats.stackexchange.com/questions/31658/contradicting-p-values-for-anova-and-kruskal-wallis-on-same-data-which-is-right&quot;&gt;this question&lt;/a&gt; for example.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2013-11-09T20:12:19.383" Id="76065" LastActivityDate="2013-11-09T20:12:19.383" OwnerUserId="13138" ParentId="76059" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I need tο use 3D data for training and for that I need to find the Gaussian basis function. I know how to find $f(x,y)$ but how can I find $f(x,y,z)$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-10T00:54:53.397" Id="76084" LastActivityDate="2013-11-10T01:26:33.200" LastEditDate="2013-11-10T01:26:33.200" LastEditorUserId="7290" OwnerUserId="34580" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;train&gt;" Title="3D Gaussian basis function" ViewCount="88" />
  <row Body="&lt;p&gt;The difference between the confidence interval for the mean response and the prediction interval is subtle but important. I'll explain it first and then provide you with a graphical intuition which helped me a lot when I learned this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously, there is an error component to our prediction. Under the normal probability model, the prediction is normally distributed about the mean response. The problem is that &lt;em&gt;the mean response is a random variable that also has an error component associated to it&lt;/em&gt;. Given a sample of size $N$ we will have an estimate for the mean response: the confidence interval describes where we would expect to find the mean response if we had built our model using different samples of size $N$ from our population. So there is an upper and lower bound to where we expect to find the mean response, and the distribution of our predictions could be &lt;em&gt;centered&lt;/em&gt; anywhere between these bounds. So the prediction interval contains the confidence interval for the mean response, with a tail added to each end of the interval to encompass the error we would expect for a prediction if the mean response were fixed at that location (which it's not).&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's make this concrete with an example. Let's say for some level of $X$, our model predicts a mean response of 0 with unit variance (i.e. the mean response is distributed according to the standard normal).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qriIE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A given preduction is distributed with mean equal to the mean response, so if we're only considering values within a 95% confidence interval, the distribution of our prediction could be centered as far left as the lower limit of the CI or as far right as the upper limit of the CI.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jR2HD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We can then intuit graphically that the prediction interval is given by the lower limit given when we anticipate the mean response will be located at the lower extreme and the upper limit found when we anticipate the mean response is located at the upper extreme.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JQn5Z.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This example was inspired by Figure 2.5 (p. 58) of &lt;em&gt;Applied Linear Statistical Models&lt;/em&gt; by Kutner, Nachsteim, Neter and Li.&lt;/p&gt;&#10;&#10;&lt;p&gt;Code used for this example, for posterity:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xv=seq(from=-5,to=5, length.out=1e4)&#10;plot(xv, dnorm(xv), type='l', xlim=c(-5,5), main=&quot;95% CI for the Mean Response&quot;)&#10;abline(v=0)&#10;abline(v=qnorm(.975), lty=2)&#10;abline(v=-qnorm(.975), lty=2)&#10;&#10;&#10;plot(xv, dnorm(xv), xlim=c(-5,5), type='n', main=&quot;Distribution of predictions given mean \nresponse is located at extremes of CI&quot;)&#10;abline(v=qnorm(.975), col=&quot;blue&quot;)&#10;abline(v=-qnorm(.975), col=&quot;blue&quot;)&#10;abline(v=0, col=&quot;blue&quot;, lty=2)&#10;lines(xv, dnorm(xv, qnorm(.975)), col='blue')&#10;lines(xv, dnorm(xv, -qnorm(.975)), col='blue')&#10;abline(v=2*(-qnorm(.975)), lty=2, col='blue')&#10;abline(v=2*(qnorm(.975)), lty=2, col='blue')&#10;&#10;plot(xv, dnorm(xv), xlim=c(-5,5), type='l', main=&quot;95% CI vs. 95% Prediction Interval&quot;)&#10;lines(xv, dnorm(xv,0,2), col=&quot;blue&quot;)&#10;abline(v=0)&#10;abline(v=qnorm(.975), lty=2)&#10;abline(v=-qnorm(.975), lty=2)&#10;abline(v=2*(-qnorm(.975)), lty=2, col='blue')&#10;abline(v=2*(qnorm(.975)), lty=2, col='blue')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-10T01:06:20.393" Id="76085" LastActivityDate="2013-11-12T15:52:23.697" LastEditDate="2013-11-12T15:52:23.697" LastEditorUserId="8451" OwnerUserId="8451" ParentId="75022" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In general, you'd use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins&quot; rel=&quot;nofollow&quot;&gt;Box-Jenkins methodology&lt;/a&gt; to figure the proper number of AR terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;Remember, each lagged term has a coefficient, so each term you add is more-or-less diminishing the coefficients of all the other terms. In addition, you're going farther and farther back in time, and more recent terms will tend to affect the future more than ancient terms. So there's a natural tradeoff between more terms and more recent terms.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-10T02:18:55.657" Id="76090" LastActivityDate="2013-11-10T02:18:55.657" OwnerUserId="1764" ParentId="76077" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Wang, Kaijun, Baijie Wang, and Liuqing Peng. &quot;CVAP: Validation for cluster analyses.&quot; Data Science Journal 0 (2009): 0904220071.:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To measure the quality of clustering results, there are two kinds of&#10;  validity indices: external indices and internal indices. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;An external&#10;  index is a measure of agreement between two partitions where the first&#10;  partition is the a priori known clustering structure, and the second&#10;  results from the clustering procedure (Dudoit et al., 2002). &lt;/p&gt;&#10;  &#10;  &lt;p&gt;Internal&#10;  indices are used to measure the goodness of a clustering structure&#10;  without external information (Tseng et al., 2005).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;For internal indices, we evaluate the results using quantities and features inherent in the data set. The optimal number of clusters is usually determined based on an internal validity index.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(Dudoit et al., 2002): Dudoit, S. &amp;amp; Fridlyand, J. (2002) A prediction-based resampling method for estimating the number of clusters in a dataset. Genome Biology, 3(7): 0036.1-21.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Tseng et al., 2005): Thalamuthu, A, Mukhopadhyay, I, Zheng, X, &amp;amp; Tseng, G. C. (2006) Evaluation and comparison of gene clustering methods in microarray analysis. Bioinformatics, 22(19):2405-12.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In your case, you need some internal indices since you have no a priori clustering structure. There exist tens of internal indices, like:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Silhouette index (implementation in &lt;a href=&quot;http://www.mathworks.com/help/stats/silhouette.html&quot;&gt;MATLAB&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;Davies-Bouldin&lt;/li&gt;&#10;&lt;li&gt;Calinski-Harabasz&lt;/li&gt;&#10;&lt;li&gt;Dunn index (implementation in &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/27859-dunns-index&quot;&gt;MATLAB&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;R-squared index&lt;/li&gt;&#10;&lt;li&gt;Hubert-Levin (C-index)&lt;/li&gt;&#10;&lt;li&gt;Krzanowski-Lai index&lt;/li&gt;&#10;&lt;li&gt;Hartigan index&lt;/li&gt;&#10;&lt;li&gt;Root-mean-square standard deviation (RMSSTD) index&lt;/li&gt;&#10;&lt;li&gt;Semi-partial R-squared (SPR) index&lt;/li&gt;&#10;&lt;li&gt;Distance between two clusters (CD) index&lt;/li&gt;&#10;&lt;li&gt;weighted inter-intra index&lt;/li&gt;&#10;&lt;li&gt;Homogeneity index&lt;/li&gt;&#10;&lt;li&gt;Separation index&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Each of them have pros and cons, but at least they'll give you a more formal basis for your comparison. The MATLAB toolbox &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/14620-cvap-cluster-validity-analysis-platform-cluster-analysis-and-validation-tool&quot;&gt;CVAP&lt;/a&gt; might be handy as it contains many internal validity indices.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-10T04:51:34.790" Id="76095" LastActivityDate="2013-11-10T04:51:34.790" OwnerUserId="12359" ParentId="76093" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;A smaller sample size results in a larger confidence interval if the data are from the same distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your case is quite different. You could, of course, just take two CIs and see what happens, but, in general deleting points at one end will make the CI smaller, regardless of whether those points re wrong or not or negative or not. e.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(plotrix)&#10;set.seed(1929191)&#10;x &amp;lt;- rnorm(20)&#10;&#10;std.error(x)&#10;&#10;xsort &amp;lt;- sort(x)&#10;xtrunc &amp;lt;- xsort[1:17]&#10;&#10;std.error(xtrunc)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, this is only true &lt;em&gt;in general&lt;/em&gt;. Michael Mayer raises the point of right skewed distributions and deleting the smallest values, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1929191)&#10;x2 &amp;lt;- rexp(20, 2)&#10;std.error(x2) #0.13&#10;&#10;x2sort &amp;lt;- sort(x2)&#10;x2trunc &amp;lt;- x2sort[3:20]&#10;&#10;std.error(x2trunc) #0.12&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and I am sure more extreme shrinkages of se.mean could occur.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-10T13:02:47.000" Id="76118" LastActivityDate="2013-11-10T13:42:00.523" LastEditDate="2013-11-10T13:42:00.523" LastEditorUserId="686" OwnerUserId="686" ParentId="76108" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Make two columns in Excel: A is CDC data, B is Google Trends data. Then you will make two new columns. In cell C2 put &quot;=A2-A1&quot; and copy/paste this equation down to one row past the data in column A. Similarly put &quot;=B2-B1&quot; in cell D2 and copy/paste it. Columns C and D are your rates of change. Next plot column C vs column D and fit a line to perform linear regression.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-10T19:02:28.827" Id="76147" LastActivityDate="2013-11-10T19:10:55.663" LastEditDate="2013-11-10T19:10:55.663" LastEditorUserId="22047" OwnerUserId="31334" ParentId="76127" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to match the prediction interval of the predict.lm() function in R using the formula found in this discussion :&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model&quot;&gt;Obtaining a formula for prediction limits in a linear model&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using a student's quantile in my interval but in the end it's far larger from the one given by predict().&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any specific calculation in the predict function, I tried to look at the code but couldn't find any answer.&#10;The formula looks ok as I found exactly the same from others source.&lt;/p&gt;&#10;&#10;&lt;p&gt;My R code :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;airquality_clean &amp;lt;- na.omit(airquality)&#10;attach(airquality_clean)&#10;&#10;#Model estimation&#10;model_1 &amp;lt;- lm(Ozone ~., data = airquality_clean)&#10;&#10;#Unbias variance of the residuals&#10;sigma_2 &amp;lt;- sum(model_1$residuals**2)/(dim(airquality_clean)[1]-dim(airquality_clean)[2])&#10;&#10;#New observation&#10;new &amp;lt;- data.frame(Solar.R=200,Wind=10,Temp=70,Day=1,Month=3)&#10;&#10;#Calculated prediction interval&#10;sigma &amp;lt;- sqrt(sigma_2*(1 + as.matrix(new)%*%solve(as.matrix(t(airquality_clean[,-1]))%*%as.matrix(airquality_clean[,-1]))%*%as.matrix(t(new))))&#10;qt &amp;lt;- qt(0.995, df = dim(airquality_clean)[1]-dim(airquality_clean)[2])&#10;int_pred_t &amp;lt;- cbind(predict(model_1, new)-(qt*sigma),predict(model_1, new)+(qt*sigma))&#10;int_pred_t&#10;          [,1]     [,2]&#10;[1,] -22.59931 95.82563&#10;&#10;#R prediction interval&#10;predict(model_1, new, interval=&quot;predict&quot;, level=0.99)}&#10;       fit       lwr      upr&#10;1 36.61316 -21.12916 94.35548&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm not too far but it's not the same results. If I use a p value from a normal distribution and not a student I'm even closer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-11T00:03:26.260" FavoriteCount="1" Id="76165" LastActivityDate="2014-05-04T08:33:59.137" LastEditDate="2014-04-03T00:34:12.163" LastEditorUserId="27412" OwnerUserId="27412" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;prediction&gt;&lt;interval&gt;" Title="R predict with &quot;prediction&quot; option" ViewCount="134" />
  <row AcceptedAnswerId="76185" AnswerCount="2" Body="&lt;p&gt;I'm a beginner in ML, and I want to practice with some algorithms I learned from the book, after searching stats.SE, I found some places where I can get some datasets. But I have this stupid question, how to use them?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Iris&quot; rel=&quot;nofollow&quot;&gt;Iris dataset&lt;/a&gt; from UCI contains 150 examples, 50 instances for each classes. OK, I thought I should take bulk of this dataset as &lt;strong&gt;training&lt;/strong&gt; examples, spare rest of the dataset as &lt;strong&gt;test&lt;/strong&gt; examples, right? And the problem is how should I divide this dataset? Shuffle them randomly and then draw the training examples randomly? &lt;/p&gt;&#10;&#10;&lt;p&gt;I think different training set will affect the model learned from it, right? Please give me some advice on how to use the dataset, thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-11T01:26:10.500" FavoriteCount="1" Id="76172" LastActivityDate="2014-09-06T16:10:56.287" OwnerUserId="30540" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;dataset&gt;" Title="How to use the datasets to practice machine learning algorithms?" ViewCount="235" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;What is the significance of using conditional expectation E(Y|X=x) in regression and not just use the regular expectation E(y). What will be the consequence if we use E(Y) instead? ( can we do this since we know one of the assumptions of regression is that X is a known quantity and it is fixed). So, what is wrong here, &lt;/p&gt;&#10;&#10;&lt;p&gt;Y= a+ b*X + e&lt;/p&gt;&#10;&#10;&lt;p&gt;E(Y)= a + bX   since { E(e)=0}&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sorry if this is a stupid question but I can't find an intuitive answer ( it may well have to do with that I may not have understood (conditional) expectation fully)! So, any comments on the difference between expectation and conditional expectation will definitely help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-11T05:09:57.723" FavoriteCount="1" Id="76191" LastActivityDate="2013-11-11T17:04:02.510" OwnerUserId="30613" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;expected-value&gt;" Title="Why do we use conditional expectation vs regular expectation in regression?" ViewCount="219" />
  <row AnswerCount="0" Body="&lt;p&gt;Using glm, I have fitted a model with a gamma response distribution and tested all the  model diagnostics so everything looks to be a good fit. The only problem is that the model's deviances point to the inverse link function being the most appropriate, however, I not too sure how to interpret the parameters. If I was using a log link, I would use the exponential function on the estimates of beta to get the log odds. For the inverse link function what is the best way to interpret the estimates of beta?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-11T06:31:12.277" Id="76196" LastActivityDate="2013-11-11T06:31:12.277" OwnerUserId="34626" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;&lt;gamma-distribution&gt;" Title="Interpretation of gamma response distribution with inverse link function" ViewCount="249" />
  <row Body="&lt;p&gt;The function responsible for plotting the ACF in the package is called &lt;code&gt;.plot.garchfit.4&lt;/code&gt;. It is not hard to modify it to produce plots for time series. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;acf_rugarch = function(x, ...)&#10;{   &#10;    T = length(x)&#10;    insample = 1:T&#10;    xseries = x&#10;    lag.max = as.integer(10*log10(T))&#10;    acfx    = acf(xseries, lag.max = lag.max, plot = FALSE)&#10;    clim0   = qnorm((1 + 0.95)/2)/sqrt(acfx$n.used)&#10;     ylim  = range(c(-clim0, clim0, as.numeric(acfx$acf)[-1]))&#10;    clx     = vector(mode = &quot;character&quot;, length = lag.max)&#10;    clx[which(as.numeric(acfx$acf)[-1]&amp;gt;=0)] = &quot;steelblue&quot;&#10;     clx[which(as.numeric(acfx$acf)[-1]&amp;lt;0)] = &quot;orange&quot;&#10;    barplot(height = as.numeric(acfx$acf)[-1], names.arg = as.numeric(acfx$lag)[-1], ylim = 1.2*ylim, col = clx,&#10;            ylab = &quot;ACF&quot;, xlab=&quot;lag&quot;, main = &quot;ACF of Observations&quot;, cex.main = 0.8)&#10;    abline(h = c(clim0, -clim0), col = &quot;tomato1&quot;, lty = 2)&#10;    abline(h = 0, col = &quot;black&quot;, lty = 1)&#10;    box()   &#10;    grid()&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This function is basically a wrapper for base R function &lt;code&gt;acf&lt;/code&gt;. Here is the example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(11)&#10;&#10;#Generate simple MA(1) process&#10;z&amp;lt;-rnorm(1000)&#10;y&amp;lt;-z+0.5*c(NA,z[-1000])&#10;acf_rugarch(na.omit(y))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/o7NFe.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-11T07:36:46.127" Id="76198" LastActivityDate="2013-11-11T07:36:46.127" OwnerUserId="2116" ParentId="57567" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="76614" AnswerCount="1" Body="&lt;p&gt;I have a table with observed and estimated values for political districts like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;BEZIRK  EDU_OBS EDU_WEI EDU_CI&#10;101     157     5129    15&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;BEZIRK&lt;/code&gt; is an ID for the district, &lt;code&gt;EDU_OBS&lt;/code&gt; is the number of actually observed people in a sample (lets say, inhabitants), &lt;code&gt;EDU_WEI&lt;/code&gt; is the estimation based on some unknown, black-box, extrapolation, and &lt;code&gt;EDU_CI&lt;/code&gt; is the 95% confidence interval in percentage of the estimated value, e.g. for district &lt;code&gt;101&lt;/code&gt;, the estimation would be 5129 $\pm$ 770 with 95% confidentiality. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, let's say I have another table with the same districts but other values. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;BEZIRK  EDU_OBS EDU_WEI EDU_CI&#10;101     180     5987    14&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to add the values (the estimates) of those with the first table, per district.&lt;/p&gt;&#10;&#10;&lt;p&gt;What happens to the CI? I.e. how do I recalculate the new CI of the combined values without knowing how it was computed in the first place? Can I just average it? I don't think so. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-11T08:58:14.447" Id="76201" LastActivityDate="2013-11-14T22:40:55.427" LastEditDate="2013-11-11T14:14:18.123" LastEditorUserId="805" OwnerUserId="18493" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;sample&gt;" Title="Summing &quot;black-box&quot; confidence" ViewCount="80" />
  <row AcceptedAnswerId="76224" AnswerCount="1" Body="&lt;p&gt;I have a problem about normal distribution and multivariate normal distribution .so I need   a whole books on multivariate normal distributions and normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for help.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-11T13:48:40.097" Id="76219" LastActivityDate="2013-11-17T11:57:11.937" LastEditDate="2013-11-17T11:57:11.937" LastEditorUserId="686" OwnerUserId="32172" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;references&gt;&lt;pdf&gt;" Title="I want a book about Normal distribution and Multivariate normal distribution" ViewCount="178" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to determine association between my independent variables (categorical, nominal) and dependent variables (categorical, nominal). &lt;/p&gt;&#10;&#10;&lt;p&gt;What other statistical analysis I can use aside from chi-square? &lt;/p&gt;&#10;&#10;&lt;p&gt;Almost all my variables have two levels except two independent variables that had more than 2 levels but chi-square analysis didn't show any significance association for those variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, can I determine the magnitude of the significance by using chi-square? &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a few variables that chi-square showed significant associations, $p&amp;lt;.05$, but I don't know if I need to run further tests to determine the magnitude of association or chi-square should be able to tell me that without further analysis.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-11T18:03:52.223" Id="76258" LastActivityDate="2013-11-12T00:24:29.413" LastEditDate="2013-11-12T00:24:29.413" LastEditorUserId="2970" OwnerUserId="34642" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;spss&gt;&lt;chi-squared&gt;" Title="Chi-square analysis" ViewCount="88" />
  
  <row Body="&lt;p&gt;Stephan, I don't think that your question is very clear, perhaps because you are not clear on what estimation and significance represent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Significance tests provide a p-value that is a measure of how extreme the results are relative to the expected distribution of results under the null hypothesis (i.e. the sampling distribution). The p-value is usually spoken of as the 'significance level' of the test. You determine the p-value by calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;The estimation part usually relates to determination of the value of a parameter of interest. For coin tossing that would be the long-run proportion of heads, the probability of the coin turning up heads on each toss. You estimate that parameter using the mean of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The p-values that you calculate serve to tell you how unusual the observed result would be if the null hypothesis (usually Pr(heads)=0.5) was true. The level of significance in that is calculated, not estimated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in your experiment are you setting or assuming that Pr(heads) is the same each time? (Is it the same coin being tossed in each experiment?) If yes, then the best estimate of Pr(heads) is provided by the total fraction of the tosses that came up heads in all of the experiments.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I can assume that the 'experiments' use different coins that may have different Pr(heads) then the probability of obtaining a p-value less than the arbitrary value of 0.05 depends on how far Pr(heads) deviates from 0.5 in how many of the coins.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-11T20:40:51.757" Id="76271" LastActivityDate="2013-11-11T20:40:51.757" OwnerUserId="1679" ParentId="76265" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;This looks to me like a power law curve fit. Using the least squares estimators given &lt;a href=&quot;http://mathworld.wolfram.com/LeastSquaresFittingPowerLaw.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, I think this should work:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y_i = 1-R(t)$. Then&lt;/p&gt;&#10;&#10;&lt;p&gt;$ &#10;\begin{align}&#10;\hat{\alpha} &amp;amp;= \frac{n \sum_{i=1}^{n}(\ln t_i)^2 - (\sum_{i=1}^{n}\ln t_i)^2}{n \sum_{i=1}^{n}(\ln t_i \ln Y_i) - \sum_{i=1}^{n}(\ln t_i) \sum_{i=1}^{n}(\ln Y_i)} \\&#10;\hat{d} &amp;amp;= \frac{\sum_{i=1}^{n}(\ln Y_i) - \alpha \sum_{i=1}^{n}(\ln t_i)}{n}\\&#10;\hat{R(t)} &amp;amp;=1 - \hat{d} t^{1/\hat{\alpha}}&#10;\end{align} &#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I tried prototyping these estimators on some sample data and I think there's something off here. This should get you on the right track, in any event.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-12T14:45:04.787" Id="76339" LastActivityDate="2013-11-12T16:30:46.797" LastEditDate="2013-11-12T16:30:46.797" LastEditorUserId="8451" OwnerUserId="8451" ParentId="76305" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;code&gt;nsprcomp&lt;/code&gt; (as of version 0.4.1) only supports enforcing constraints on the loadings.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are interested in an approximate matrix factorization $\mathbf{X}\approx\mathbf{P}\mathbf{Q}$ where both $\mathbf{P}$ and $\mathbf{Q}$ only contain non-negative elements, I suggest that you study the non-negative matrix factorization (NMF) literature. For example the &lt;a href=&quot;http://arxiv.org/pdf/cs.LG/0408058&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; by Hoyer (2004) is well written.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-12T15:06:21.833" Id="76342" LastActivityDate="2013-11-12T15:06:21.833" OwnerUserId="29181" ParentId="66897" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Fix $\varepsilon\gt 0$. We have $\mathbb P\{|Y^{(1)}_n-a|\geqslant \varepsilon\}=\mathbb P\{Y_n^{(1)}\geqslant a+\varepsilon\}$ (this because the density of $Y^{(1)}_n$ is supported on $[a,\infty)$, hence $\mathbb P\{Y^{(1)}_n\lt a\}=0$). &lt;/p&gt;&#10;&#10;&lt;p&gt;We can compute $\mathbb P\{a\leqslant Y_n^{(1)}\leqslant a+\varepsilon\}$ thanks to the expression of the density:&#10;$$\mathbb P\{Y_n^{(1)}\leqslant a+\varepsilon\}=\int_{a+\varepsilon}^\infty bne^{-nb(y-a)}\mathrm dy=\int_{a+\varepsilon}^\infty bn{e}^{-nbt}\mathrm dt=[-e^{-nbt}]_{a+\varepsilon}^\infty=e^{-nb(a+\varepsilon)},$$&#10;and this goes to $0$ as $n$ goes to infinity.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-12T15:19:49.087" Id="76345" LastActivityDate="2013-11-12T15:19:49.087" OwnerUserId="14675" ParentId="74451" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm wrestling with a question regarding random effects that I haven't been able to figure out with my regular resources. I am examining the effects of two treatments (heat and water) on plant biomass data that I've collected for 4 years in 20 plots (each plot has 4 quadrants that are not independent but are measured separately).&lt;/p&gt;&#10;&#10;&lt;p&gt;My &lt;code&gt;lme&lt;/code&gt; model (using &lt;code&gt;nlme&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;) has two nested categorical random effects: year is nested within plot and quad such that my R script is the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lme(biomass ~ heat*water + ... + elevation, data, random=~1|plot/quad/year)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are no error messages given and the model appears to run smoothly. I only have 4 levels for year (80 for plot/quad) and have read that that a low number of levels will make it hard to get accurate estimates of variance. I tried adding year as a fixed effect and get completely different results (there is a lot of year-to-year variability in my dependent variable). &lt;/p&gt;&#10;&#10;&lt;p&gt;My questions:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is the small size of the second-level random effect much of a problem? Is it only the number of levels of the main random effect that I should worry about?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you think having only 4 levels is a problem, how can I test to see if it is messing up my model? Collecting data for several more years just to correct this issue is not possible.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Below are the summary output of the model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(model)&#10;Linear mixed-effects model fit by REML&#10; Data: data &#10;       AIC      BIC    logLik&#10;  2327.316 2359.081 -1154.658&#10;&#10;Random effects:&#10; Formula: ~1 | plot&#10;        (Intercept)&#10;StdDev:    13.89338&#10;&#10; Formula: ~1 | quad %in% plot&#10;        (Intercept)&#10;StdDev:    11.32595&#10;&#10; Formula: ~1 | year %in% quad %in% plot&#10;        (Intercept) Residual&#10;StdDev:    18.00256  7.39362&#10;&#10;Fixed effects: forbs ~ h * w + elevation &#10;                Value Std.Error  DF    t-value p-value&#10;(Intercept) 1290.2549 2576.2021 183  0.5008360  0.6171&#10;hy           -10.6171   10.5874  15 -1.0028063  0.3319&#10;wy           -13.6224   10.3395  15 -1.3175078  0.2074&#10;elevation     -0.3382    0.7265  15 -0.4654946  0.6483&#10;hy:wy         37.1865   15.0869  15  2.4648273  0.0263&#10;&#10;Correlation: &#10;          (Intr) hy     wy     elevtn&#10;hy        -0.222                     &#10;wy        -0.189  0.503              &#10;elevation -1.000  0.220  0.187       &#10;hy:wy      0.275 -0.728 -0.713 -0.274&#10;&#10;Standardized Within-Group Residuals:&#10;        Min          Q1         Med          Q3         Max &#10;-0.76090000 -0.23913835 -0.02857524  0.19955752  1.39996632 &#10;&#10;Number of Observations: 257&#10;Number of Groups: &#10;                       plot            quad %in% plot   year %in% quad %in% plot &#10;                       20                         74                        257 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks for your help, and please let me know if I can provide any additional info. I'd be happy to send you my data and script if it helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-12T17:53:56.890" Id="76358" LastActivityDate="2013-11-12T18:37:29.870" LastEditDate="2013-11-12T18:37:29.870" LastEditorUserId="22468" OwnerUserId="34715" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;random-variable&gt;&lt;lme&gt;&lt;mixed-effect&gt;&lt;nlme&gt;" Title="Nested random effects group size using lme (in nlme)" ViewCount="1967" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;What throws me off is the NULL Hypothesis. According to my calculation&#10;  using EXCEL its 7.37481E-27 or 0.00000000000000000000000000737 (for a&#10;  paired, 2 tail TTEST). Is this realistic to get?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It depends on the number of students and difference between pre and post. I think there is probably a problem with your calculation however. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;My understanding is that you have to achieve &amp;lt;.05 to reject the null&#10;  hypothesis. And my calculated TTESTS are really small. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The number .05 is arbitrary and based on an example Ronald Fisher used ~80 years ago. He later said that no one should use the same number for every case, it depends on the circumstances. Your p-value is very small so I am sure would be considered &quot;significant&quot; by anyone, however there is probably a problem with your calculation. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Does this mean&#10;  I can reject the null hypothesis and say that what I did to increase&#10;  reading scores was effective?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Rejecting the null hypothesis only means that the pre and post scores were not exactly the same. It could also be significant if the students were worse in later semester. Or the scores could be different for some other reason besides your teaching (a popular tv show ended so they studied more, it could be anything).&lt;/p&gt;&#10;&#10;&lt;p&gt;It is best to plot your results to show the effect. Plot a line for each student between pre and post. Did some students improve while others did not, or even got worse? Perhaps there is something in common regarding who got better and who did not (if this is the case) that could help you improve your teaching strategy even further. Most likely you do not only care about the &quot;average student&quot; but more how to help each individual student.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also, some students who took a test in fall withdrew from my class&#10;  mid-year and didn’t take a spring test. Do I leave those cells blank&#10;  or do I fill them in with zeros? Or do I just pretend that those kids&#10;  never existed? How much would that throw off the results? Thanks.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Do not fill them in with zeros. It would probably be easiest for you to drop them from the significance test analysis (pretend they didnt exist). But you should not ignore the data! For example, if there was a large difference pre and post for all the other students and the ones who dropped the course were the ones with high scores the first semester this may skew your results. Perhaps they withdrew because of the teaching method, etc. There is no statistical test for this, you just have to think about how to explain what happened to generate the data you got.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you post the calculation you did I can help further, still I think it is more important to plot the data and look at the effect for each student rather than calculate a p-value.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-12T19:18:29.333" Id="76366" LastActivityDate="2013-11-12T19:18:29.333" OwnerUserId="31334" ParentId="76364" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Defining the empirical distribution function &#10;$$&#10;  \hat{F}_n(t)=\frac{1}{n}\sum_{i=1}^n I_{[x_i,\infty)}(t) \, ,&#10;$$&#10;it follows that&#10;$$&#10;  \int_{-\infty}^\infty g(t)\,d\hat{F}_n(t) = \frac{1}{n} \sum_{i=1}^n g(x_i) \, .&#10;$$&#10;Hence, you don't need to use &lt;code&gt;integrate()&lt;/code&gt; to solve this problem. This kind of &lt;code&gt;R&lt;/code&gt; code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rnorm(10^6)&#10;g &amp;lt;- function(t) exp(t) # say&#10;mean(g(x))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;should be super fast because it is vectorized.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-12T19:44:05.333" Id="76372" LastActivityDate="2014-06-04T22:42:07.857" LastEditDate="2014-06-04T22:42:07.857" LastEditorUserId="9394" OwnerUserId="9394" ParentId="76359" PostTypeId="2" Score="10" />
  
  <row AcceptedAnswerId="76378" AnswerCount="2" Body="&lt;p&gt;Hyndman's great explanation of proper time series CV is at the bottom of the page in the following link: &lt;a href=&quot;http://robjhyndman.com/hyndsight/crossvalidation/&quot; rel=&quot;nofollow&quot;&gt;http://robjhyndman.com/hyndsight/crossvalidation/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Leave-One-Out illustration in the following link: &lt;a href=&quot;http://i.imgur.com/qrQI4LY.png&quot; rel=&quot;nofollow&quot;&gt;http://i.imgur.com/qrQI4LY.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the 'leave one out' illustration if the dataset in the illustration was time series data and was sorted from past to present from left to right, wouldn't it be identical to Hyndman's explanation on Time Series CV?  If not, how so?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-12T20:18:35.237" Id="76376" LastActivityDate="2013-11-13T10:05:42.053" LastEditDate="2013-11-12T20:44:43.153" LastEditorUserId="32423" OwnerUserId="32423" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;forecasting&gt;&lt;cross-validation&gt;" Title="How is Hyndman's explanation of proper Time Series Cross Validation different from Leave-One-Out?" ViewCount="156" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set showing an (exponential) increase in the size of an animal population over time. I can fit an exponential model to these data and obtain an estimate of the population growth rate (under the assumption of unconstrained exponential growth).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would like to get a handle on how variable this estimate is however.&#10;To do this, I have written an R script which conducts some Bootstrap resampling of the initial observations and harvests the value of the population growth rate for each Bootstrap resample. I am achieving this by fitting a lm of the form log(abundance)~Year.&lt;/p&gt;&#10;&#10;&lt;p&gt;While the initial exponential model was highly significant (p&amp;lt;&amp;lt;0.05), this is not necessarily the case in all Bootstrap resamples. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I construct reliable confidence intervals for the population growth rate, knowing that sometimes (in some runs) it is found to be non-significant at alpha=0.05?&lt;/p&gt;&#10;&#10;&lt;p&gt;I see 3 options here:&#10;1) Generate a distribution of growth rate values based on all resamples, regardless of whether the regression was significant or not;&#10;2) Only base the CI on the subset of resamples where the regression was significant;&#10;3) Assign a value of zero to the regression coefficient whenever its associated p-value is larger than the chosen significance threshold, i.e. alpha=0.05.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have only found one published paper that uses method 3) [Austin (2007). Using the bootstrap to improve estimation and confidence intervals for regression coefficients selected using backwards variable elimination. STATISTICS IN MEDICINE, 27, VOL. 17:3286-3300].&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a valid approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-13T00:03:36.123" Id="76392" LastActivityDate="2013-11-13T00:03:36.123" OwnerUserId="34735" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;statistical-significance&gt;&lt;p-value&gt;&lt;bootstrap&gt;" Title="Bootstrapping regression slopes - p-value and significance level" ViewCount="186" />
  <row AnswerCount="1" Body="&lt;p&gt;I have developed a model for construction waste reduction, but I'm not sure how to validate it to serve the purpose. I have two sets of data: measured waste % from sites and predicted values obtained from model constructed. How do I tell that the model constructed is good for prediction purposes? The data is linearly related.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-13T07:31:15.143" Id="76423" LastActivityDate="2014-02-14T10:35:54.163" LastEditDate="2013-11-13T08:33:55.843" LastEditorUserId="2116" OwnerUserId="31438" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="What is acceptable error in prediction models?" ViewCount="101" />
  
  
  <row Body="&lt;p&gt;OK, scratching my own itch. We want the $1-\alpha$ level curve of $y \sim N_2(\mu,\Sigma)$&lt;/p&gt;&#10;&#10;&lt;p&gt;The $1-\alpha$ level curve of the $\mathcal N_2(0,I)$ distribution function is a circle of radius $\sigma = \sqrt{ \chi^2_{2,1-\alpha} }$ centred at the origin. This holds because if we consider some $x$ drawn from that distribution then $\mathbb P(x^T x \le \chi^2_{2,1-\alpha}) = 1-\alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using this to get the comparable curve for $y$ is a case of applying a standard linear transformation to $x$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Take some points on the unit circle by generating a sequence of angles, $\phi$, over $[0, 2\pi)$ and apply $(cos(\phi),sin(\phi))$, call the matrix formed by stacking these row vectors $R$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tilde Y =  R \sigma \Sigma^{1/2} + \mu$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tilde Y$ is a set of points on the requisite level curve. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;EDIT: Tidied up the notation in response to @whuber's comment, hope it's less incoherent now.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-13T08:45:53.717" Id="76428" LastActivityDate="2013-11-19T17:29:00.470" LastEditDate="2013-11-19T17:29:00.470" LastEditorUserId="16663" OwnerUserId="16663" ParentId="60011" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="76450" AnswerCount="1" Body="&lt;p&gt;Permutation tests (also called a randomization test, re-randomization test, or an exact test) are very useful and come in handy when the assumption of normal distribution required by for instance, &lt;code&gt;t-test&lt;/code&gt; is not met and when transformation of the values by ranking of the non-parametric test like &lt;code&gt;Mann-Whitney-U-test&lt;/code&gt; would lead to more information being lost. However, one and only one assumption should not be overlooked when using this kind of test is the assumption of exchangeability of the samples under the null hypothesis. It is also noteworthy that this kind of approach can also be applied when there are more than two samples like what implemented in &lt;code&gt;coin&lt;/code&gt; R package. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can you please use some figurative language or conceptual intuition in plain English to illustrate this assumption? This would be very useful to clarify this overlooked issue among non-statisticians like me.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br&gt;&#10;It would be very helpful to mention a case where applying a permutation test doesn't hold or invalid under the same assumption.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;br&gt;&#10;Supppose that I have 50 subjects collected from the local clinic in my district at random. They were randomly assigned to received drug or a placebo at 1:1 ratio. They were all measured for paramerter 1 &lt;code&gt;Par1&lt;/code&gt; at V1 (baseline), V2 (3 months later), and V3 (1 year later). All 50 subjects can be subgrouped into 2 groups based on feature A; A positive = 20 and A negative = 30. They can also be subgrouped into another 2 groups based on feature B; B positive = 15 and B negative = 35.&lt;br&gt;&#10;Now, I have values of &lt;code&gt;Par1&lt;/code&gt; from all subjects at all visits. Under the assumption of exchangeability, can I do comparison between levels of &lt;code&gt;Par1&lt;/code&gt; using permutation test if I would:&lt;br&gt;&#10;- Compare subjects with drug with those received placebo at V2?&lt;br&gt;&#10;- Compare subjects with feature A with those having feature B at V2?&lt;br&gt;&#10;- Compare subjects having feature A at V2 with those having feature A but at V3?&lt;br&gt;&#10;- By which situation this comparison would be invalid and would violate the assumption of exchangeability?  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-13T11:39:12.633" Id="76442" LastActivityDate="2013-11-13T17:17:54.893" LastEditDate="2013-11-13T17:17:54.893" LastEditorUserId="22518" OwnerUserId="22518" PostTypeId="1" Score="9" Tags="&lt;hypothesis-testing&gt;&lt;permutation&gt;" Title="What is the intuition behind exchangeable samples under the null hypothesis?" ViewCount="274" />
  
  <row AnswerCount="0" Body="&lt;p&gt;How to get a random variable with a Boltzmann distribution as the probability density distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or more practically: In my Java program, I can easily produce a uniform random variable U(0,1) or a uniform distributed integer from the integer value range. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Random rnd = new Random();&#10;double dvar = rnd.nextDouble(); // U(0,1) [0..1)&#10;int ivar = rnd.nextInt(); // [-2147483648 .. 2147483647]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Starting from this, how can I produce random variables with a Boltzmann distribution?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-13T14:12:21.317" Id="76452" LastActivityDate="2013-11-13T14:12:21.317" OwnerUserId="34759" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;pdf&gt;" Title="Getting a random variable with a Boltzmann distribution" ViewCount="42" />
  <row Body="&lt;p&gt;(1) I don't think you've given an accurate example of bootstrap validation &lt;/p&gt;&#10;&#10;&lt;p&gt;a) it does draw random sample with replacement, but I'm not sure it &quot;ignores&quot; anything&#10;b) it doesn't increase stability/accuracy of the model, but rather of the test error&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) This might be a useful starting point: Steyerberg: Internal validation of predictive models: efficiency of some procedures for logistic regression analysis (J Clin Epidemiol. 2001 Aug;54(8):774-81)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-13T14:51:51.837" Id="76457" LastActivityDate="2013-11-13T15:17:12.740" LastEditDate="2013-11-13T15:17:12.740" LastEditorUserId="34658" OwnerUserId="34658" ParentId="76455" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I believe that the problem is with your guess that the inverse gamma is so easily extended to the multivariate case.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the distributions appendix of Gleman et al, &lt;em&gt;Bayesian Data Analysis&lt;/em&gt; (3rd Edition), 582&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The Inverse-&lt;em&gt;Wishart&lt;/em&gt; distribution is the conjugate prior distribution for the multivariate normal co-variance matrix. ...&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The Wishart distribution is the conjugate prior distribution for the inverse co-variance matrix in a multivariate normal distribution and is a multivariate generalization of the gamma distribution. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm uncertain whether you'd like to proceed in your own investigation with this hint, or if you'd like me to spill the beans and post a full solution. (Though, turning to page 73 of the same text, we find the particular underlying algebra that you're interested in.) &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-13T15:33:30.390" Id="76464" LastActivityDate="2013-11-13T19:03:46.977" LastEditDate="2013-11-13T19:03:46.977" LastEditorUserId="22311" OwnerUserId="22311" ParentId="76460" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I've been working through the HW work in the online book &lt;a href=&quot;http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;A little book of R for time series analysis&lt;/em&gt;&lt;/a&gt;, and have started testing with some &quot;live&quot; customer data. I have a dataset that looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;CustomerName | Sales&#10;123456         $5,000&#10;123456         $3,455&#10;123456         $7,540&#10;123456         $2,300&#10;987654         $5,600&#10;987654         $6,700&#10;987654         $1,300&#10;987654         $690&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where I have &lt;code&gt;Sales&lt;/code&gt; values by customer for the previous 60 months. There are ~200 customers for which I'm looking to generate a forecast. I'm able to generate a forecast for a single customer at a time, but I am having trouble finding guidance on how to run the forecast for the whole group of customers and output the results. &lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, the output would be the regular forecast output but with &lt;code&gt;CustomerID&lt;/code&gt; included like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;CustomerID | Month | Point.Forecast | Lo.80 |Hi.80 | Lo.95 | Hi.95&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-13T15:47:45.787" Id="76466" LastActivityDate="2014-05-30T17:55:23.983" LastEditDate="2014-05-30T17:53:29.663" LastEditorUserId="7290" OwnerUserId="34763" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;" Title="Holt Winters for multiple customers and output with R" ViewCount="191" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working with principal component analysis (PCA) to have a regression model. Let's say we have 3 variables that we get every month, and so far we gathered 10 months of data. We have built the PCA on the raw data matrix.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p = prcomp(na.omit(rdatat), center=TRUE,scale=TRUE)&#10;&#10;loadings=p$rotation[]&#10;&#10;pca=cbind(p$x[,1], p$x[,2], p$x[,3])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now we can put &lt;code&gt;p$x[,1]&lt;/code&gt; as input of the regression and the result will be in the form of $Y=$ intercept$+  K\,$Component1, where $K$ is a coefficient from regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I build Component1 the month after in order to predict $Y$? I have the raw values of the 3 variables I use in PCA, but how can I use them?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-13T17:27:43.160" FavoriteCount="1" Id="76483" LastActivityDate="2013-11-15T18:51:21.253" LastEditDate="2013-11-14T00:10:25.683" LastEditorUserId="805" OwnerUserId="34772" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;pca&gt;" Title="Regression through PCA" ViewCount="209" />
  
  
  <row Body="&lt;p&gt;The sampling difference is a problem.  To compare apples-to-apples, both series need to be based on the same frequency and timing.   In this case, if Series B is based on the last day of each month, then Series A also needs to be based on the same days.  If you can't get daily data for Series A then you may need to interpolate on the weekly data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If seasonality is involved, then using weekly data is an even bigger problem.   The main seasonality issue with weekly data is that there aren't 52 weeks in a year.  Using 365 days per year and 7 days a week gives 52.14 weeks per year.  That's not an integer number, which means that when a year ends, the associated week may or may not end.  As result, all calculations have to be modified to reflect that.  Monthly data has exactly 12 months per year.  Each month may not have the same number of days, but that is typically understood for monthly data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-13T22:09:51.817" Id="76511" LastActivityDate="2013-11-13T22:09:51.817" OwnerUserId="2775" ParentId="76505" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a case and control group, both of which had a blood sugar level tested multiple times over 48 hours. I took the mean of the levels for each person and then calculated the mean of the means (with an SD etc) to compare the two groups. I am not sure if this is a valid method for comparison and whether another method is more appropriate? Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-14T01:29:59.480" FavoriteCount="1" Id="76520" LastActivityDate="2014-08-06T00:33:23.610" OwnerUserId="34789" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;standard-deviation&gt;&lt;mean&gt;" Title="What is the appropriate test for comparing means?" ViewCount="125" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The following question is about an Exercise 14.15 from &quot;The Elements of Statistical Learning&quot; by Hastie, Friedman and Tibshirani.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Generate $200$ observations of three variates $X_1, X_2 , X_3$ according&#10;  to&#10;  \begin{align}X_1 &amp;amp;= Z_1 \\&#10;X_2 &amp;amp;= X_1 + 0.001 \cdot Z_2 \\&#10;X_3 &amp;amp;= 10 \cdot Z_3 \end{align}&#10;  where $ Z_1, Z_2, Z_3 $ are independent standard normal variables. Compute the&#10;  leading principal component and factor analysis directions. Hence show that the leading principal component aligns itself in the maximal variance&#10;  direction $X_3$, while the leading factor essentially ignores the uncorrelated&#10;  component $X_3$, and picks up the correlated component $X_2 + X_1$ (Geoffrey&#10;  Hinton, personal communication).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Why? I thought that they are both &quot;powered by&quot; the same matrix decomposition? What have I missed?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-11-14T05:46:04.140" FavoriteCount="1" Id="76543" LastActivityDate="2015-01-26T21:32:28.600" LastEditDate="2015-01-26T00:15:50.843" LastEditorUserId="28666" OwnerUserId="16596" PostTypeId="1" Score="4" Tags="&lt;pca&gt;&lt;factor-analysis&gt;&lt;dimensionality-reduction&gt;&lt;matrix-decomposition&gt;" Title="Why do PCA and Factor Analysis return different results in this example?" ViewCount="155" />
  <row Body="&lt;p&gt;The answer to your question depends on the question you are trying to answer! The method you described may work well for some questions and not for others. For example, your method assumes&lt;/p&gt;&#10;&#10;&lt;p&gt;That blood sugar levels at different times have equal importance. For example, a high (or low) blood sugar at hour 2 is of the same importance as a high (or low) blood sugar at hour 37. &lt;/p&gt;&#10;&#10;&lt;p&gt;If this is in line with the question you are trying to answer, great. If not you may consider (as Glen suggested) a model for your collection of variables, in particular one that allows you to estimate the effect of time (regression could help you do this).&lt;/p&gt;&#10;&#10;&lt;p&gt;Another aspect that will help you answer your question is how you compared the means. You mentioned that you used a SD, etc. This sounds like a good start, but you might want to do a more in depth check of your method. If you haven't already, I would consider looking up the assumptions and procedure for two-sample t-tests, which will provoke good questions like did you pool the standard deviations or not? Was there good evidence to do so, etc.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-14T06:10:12.027" Id="76544" LastActivityDate="2013-11-14T06:10:12.027" OwnerUserId="34768" ParentId="76520" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="76565" AnswerCount="1" Body="&lt;p&gt;The moment generating function of a random variable $X$ is defined to be the function $$M_{X}(t)=E(e^{tX})=\sum_{n=0}^{\infty}\frac{E(X^n)}{n!}t^n.$$ Let $I=\{t\in\mathbb R:M_{X}(t)&amp;lt;\infty\}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to show that&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$I$ is possibly a degenerate interval and $0\in I$. (Degenerate means the interval includes only one real number.)  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$M_{X}(t)$ is a convex function on $I$.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If $0$ is an interior point of $I$, then $E(X^k)\lt\infty$ for all $k\in \mathbb N$; i.e., $X$ has finite moments of all orders.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" ClosedDate="2013-11-14T15:28:55.397" CommentCount="4" CreationDate="2013-11-14T06:57:45.170" Id="76547" LastActivityDate="2013-11-14T14:00:11.487" LastEditDate="2013-11-14T07:35:30.723" LastEditorUserId="919" OwnerUserId="32172" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;expected-value&gt;&lt;moments&gt;" Title="How to prove three properties of the moment generating function?" ViewCount="125" />
  <row AnswerCount="1" Body="&lt;p&gt;Assume that we have a white noise process. When you try to fit an ARMA(1,1) model on it (clearly wrong model but bear with me):&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_t=ay_{t-1}+b\epsilon_{t-1}+\epsilon_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;you will end up with a variety of different $(a,b)$ where $a+b=0$ depending on where the optimizer will start.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course I rather get $a=b=0$ to slap me in the face to show me this is not the right model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any way to constrain the optimization so that I get the $(0,0)$ solution?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-14T08:30:28.593" Id="76551" LastActivityDate="2014-06-03T11:18:45.040" OwnerUserId="20980" PostTypeId="1" Score="1" Tags="&lt;arma&gt;" Title="ARMA(1,1) Unique Solution" ViewCount="116" />
  <row Body="&lt;p&gt;You are not looking for clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead, you are looking for &lt;strong&gt;frequent itemset mining&lt;/strong&gt;. There are dozens of algorithms for this, the most widely known probably are APRIORI, FP-Growth and Eclat.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-14T09:16:54.057" Id="76553" LastActivityDate="2013-11-14T09:16:54.057" OwnerUserId="7828" ParentId="76373" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="76566" AnswerCount="1" Body="&lt;p&gt;I have the following sample. Now I have to test: &#10;$$H_0\!:\mu=60,\!000&#10;\quad \text{Vs.}\quad&#10;H_a\!:\mu&amp;gt;60,\!000$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have computed it in &lt;code&gt;R Software&lt;/code&gt; But I do not understand the result.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Price  &amp;lt;- c(60,250,400,550,517,380,425,280,389,559)&#10; Number &amp;lt;- c(3,3,4,5,4,3,6,3,4,5)&#10; data   &amp;lt;- data.frame(Price, Number)&#10; price  &amp;lt;- data$Price  #$&#10; num    &amp;lt;- data$Number&#10;&#10; # test statistic&#10; xbar   &amp;lt;- mean(price)&#10; mu0    &amp;lt;- 60000&#10; s      &amp;lt;- sd(price)&#10; n      &amp;lt;- length(price)&#10; t      &amp;lt;- (xbar-mu0)/(s/sqrt(n))&#10; t&#10;&#10;#Reference statistic&#10;alpha   &amp;lt;- 0.05&#10;t.alpha &amp;lt;- qt(1-alpha, df=n-1)&#10;t.alpha&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Do I need to consider the absolute value of t-statistic or not?&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-14T13:42:51.733" Id="76563" LastActivityDate="2013-11-15T13:46:47.360" LastEditDate="2013-11-14T14:08:46.437" LastEditorUserId="7290" OwnerUserId="34809" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;interpretation&gt;" Title="Interpretation of hypothesis testing" ViewCount="92" />
  <row AnswerCount="1" Body="&lt;p&gt;Using Stats models and Pandas (and requests for the data) I'm working on a forecast model.. my 1st step is just getting the Arma function working and understood.&#10;My data is available publically and is highly seasonal residential real estate unit sales data, I'm planning to see how a quarterly survey that we do can help with the forecast as a later step. Hence why I am changing the frequency to quarterly with dates that match the dates I have on the quarterly survey.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the code looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#get the statewide actual data from Sheet 1 parse dates and select just unit sales&#10;act = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak_wF7ZGeMmHdFZtQjI1a1hhUWR2UExCa2E4MFhiWWc&amp;amp;output=csv&amp;amp;gid=1')&#10;dataact = act.content&#10;actdf = pd.read_csv(StringIO(dataact),index_col=0,parse_dates=['date'], thousands=',') #converts to numbers&#10;actdf.rename(columns={'Unit Sales': 'Units'}, inplace=True)&#10;actdf=actdf[['Units']]&#10;actdfq=actdf.resample('Q',sum)&#10;actdfq.index = actdfq.index + pd.DateOffset(days=15) #align the actual data dates to the   survey dates Eg the 15th of the quarter&#10;actdfq=actdfq['2009':] # selcts the time periods for which we have surveys (actual results here) The survey would be shifted back by one&#10;actdfqchg=actdfq['Units']&#10;fig = plt.figure(figsize=(12,8))&#10;ax1 = fig.add_subplot(211)&#10;fig = sm.graphics.tsa.plot_acf(actdfqchg.values.squeeze(), lags=4, ax=ax1)&#10;ax2 = fig.add_subplot(212)&#10;fig = sm.graphics.tsa.plot_pacf(actdfqchg, lags=4, ax=ax2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the data looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2009-01-15     7867&#10;2009-04-15     7483&#10;2009-07-15    10109&#10;2009-10-15    10648&#10;2010-01-15     9678&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The acf graphs look like:&#10;&lt;img src=&quot;http://i.stack.imgur.com/PKuA1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So I don't really know what the ACF is telling me..The graph of auto correlation would tend to tell me that the 4q correlation is the strongest but still only .4? (correct?) and the 2q score of -.4 would indicate that the summer to winter correlation would be the weakest (which makes sense) and how to proceed with 1) a projection based on this actual data just using straight Arma Stats models capabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;and 2) how to best incorporate survey data that attempt to predict the following quarter.. the survey is taken asking for predictions for the following quarter on a 5 point scale with a400+ participants, the straight correlation is not super strong but I think somehow I should be able to find the quarterly correlation to help inform the projection for the subsequent quarter..??&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-13T22:31:58.080" Id="76587" LastActivityDate="2013-11-15T14:55:41.853" LastEditDate="2013-11-14T17:45:31.320" LastEditorUserId="1963" OwnerDisplayName="dartdog" OwnerUserId="1963" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;python&gt;&lt;arima&gt;" Title="Pandas Statsmodels Time series seasonal forecasting" ViewCount="2693" />
  
  <row AcceptedAnswerId="76628" AnswerCount="1" Body="&lt;p&gt;I have been trying to do code this:&#10;For each $\left(  x,z\right)  \in\left(  X,Z\right):$&lt;/p&gt;&#10;&#10;&lt;p&gt;$r(x,z)=\sum_{i=1}^{n}Y_{i}1\{X_{i}&amp;lt;=x\}1\left\{  Z_{i}&amp;lt;=z\right\}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, the best way I came up to do this is by using a loop. Here an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y=rnorm(10)&#10;x=c(1,1,1,2,2,2,3,3,3,4)&#10;z=c(5,5,6,6,7,7,8,8,9,9)&#10;data=data.frame(y,x,z)    &#10;s=rep(NA,length(unique(x))*length(unique(z)))&#10;dim(s)=c(length(unique(x)),length(unique(z)))&#10;for (i in 1:length(unique(x))){&#10;for (j in 1:length(unique(z))){&#10;   s[i,j]=sum(y*as.numeric((x&amp;lt;=unique(x)[i]))*&#10;                as.numeric((z&amp;lt;=unique(z)[j])))&#10;}&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output is OK, but given that once the dimension of x and z grows, this becomes quite slow.Since, for a given z, this looks like a conditional cumulative sum,  I am 100% sure that there is a more efficient way of doing this, without the loop.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would any of you have any suggestion? If I didn't have z, I know I could use data.table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; s=data[order(x)][,lapply(.SD, sum),by=c(&quot;x&quot;), .SDcols=c(&quot;y&quot;)]&#10;  s=s[,lapply(.SD, cumsum), .SDcols=c(&quot;y&quot;)]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but with more than one index (x and z, not just x) I was not able to formulate the program.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-14T23:00:34.377" Id="76616" LastActivityDate="2013-11-15T01:58:49.637" LastEditDate="2013-11-15T00:41:45.073" LastEditorUserId="805" OwnerUserId="17645" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="Inequality based conditional cumulative sum" ViewCount="133" />
  
  
  
  
  
  <row Body="&lt;p&gt;To put the argument of @alecos-papadopoulos in graphical terms:&lt;/p&gt;&#10;&#10;&lt;p&gt;Say his $a$ is -1 and his $b$ is 0.5 and $x$ ranges between 0 and 2. If we were to graph that relationship we could type in Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;twoway function Pr = invlogit(-1+0.5*x), range(0 2) lwidth(medthick)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That would result in the following graph:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L4NgQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;which looks pretty linear. However, if we were to look at this relationship over a larger range of values for $x$, we would see that it is part of a non-linear relationship:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;twoway function Pr = invlogit(-1+0.5*x),           ///&#10;                     range(-10 10) lpattern(solid) ///&#10;                     lcolor(gs12) lwidth(thick) || ///&#10;       function Pr = invlogit(-1+0.5*x),           ///&#10;                     range(0 2) lpattern(solid)    ///&#10;                     lcolor(black) lwidth(thick)   ///&#10;                     legend(off)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hHNme.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As a rule of thumb the relationship between $x$ and the predicted probability is approximately linear between a predicted probability of .2 and .8.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-15T08:51:19.887" Id="76641" LastActivityDate="2013-11-15T09:02:40.710" LastEditDate="2013-11-15T09:02:40.710" LastEditorUserId="22047" OwnerUserId="23853" ParentId="76623" PostTypeId="2" Score="6" />
  
  
  
  
  
  <row AcceptedAnswerId="76679" AnswerCount="1" Body="&lt;p&gt;Let's say I have 7 urns filled with random numbers of colourful marbles. An example dataset is as below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;- matrix (c(5,3,4,4,4,2,1,1,1,1,1,2,2,1,2,2,1,1,2,1,4,1,1,2,4,1,3,1,7,1,1,2,1,3,3), ncol = 5);&#10;rownames(data) &amp;lt;- as.character(seq(1,7));&#10;colnames(data) &amp;lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;pink&quot;);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I assume the count-based colour distribution to be multinomial. I want to test whether the contents of Urn 1 follow the colour distribution of Urns 2-7. &lt;/p&gt;&#10;&#10;&lt;p&gt;I get the MLEs for Urns 2-7 as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p_sample &amp;lt;- colSums(data[2:7,])/ sum(colSums(data[2:7,]))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; When I run a $\chi^2$ test as below, R displays a warning message since the expected counts (EC)&amp;lt;5. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;obs &amp;lt;- data[&quot;1&quot;,]&#10;chisq.test(obs,p=p_sample)&#10;# Chi-squared test for given probabilities&#10;&#10;# data:  obs&#10;# X-squared = 8.0578, df = 4, p-value = 0.08948&#10;#&#10;# Warning message:&#10;# In chisq.test(obs, p = p_sample) :&#10;#  Chi-squared approximation may be incorrect&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;One of the answers to this &lt;a href=&quot;http://stats.stackexchange.com/questions/14226/given-the-power-of-computers-these-days-is-there-ever-a-reason-to-do-a-chi-squa&quot;&gt;question&lt;/a&gt; states that $\chi^2$ test would nevertheless return accurate results as long as ECs exceed 1.0 if a very simple $\frac{N-1}{N}$ correction is applied to the test statistic.  Is the correction implemented simply like this: $\chi^2 = (\sum_i \frac{(O_i - E_i)^2}{E_i} )* \frac{N-1}{N}  $  ?&lt;/p&gt;&#10;&#10;&lt;p&gt;This &lt;a href=&quot;https://sites.google.com/a/lakeheadu.ca/bweaver/Home/statistics/notes/chisqr_assumptions&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt; suggests so but I am not sure about its reliability:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If one has the regular Pearson chi-square (e.g., in the output from&#10;  statistical software), it can be converted to the 'N - 1' chi-square&#10;  as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           'N -1' chi-square = Pearson chi-square x (N -1) / N&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; Alternative to $\chi^2$, since the number of counts is low, I ran a Fisher's exact test and got the results below. Is how I call the fisher.test below correct? The result I get makes me think: &quot;No&quot;. I am confused since the R help document only refers to use cases for contingency tables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fisher.test(obs, sum(obs)*p_sample)&#10;&#10;# Fisher's Exact Test for Count Data&#10;#&#10;# data:  obs and sum(obs) * p_sample&#10;# p-value = 1&#10;# alternative hypothesis: two.sided&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-11-15T16:42:49.343" Id="76669" LastActivityDate="2013-11-17T13:34:03.587" LastEditDate="2013-11-15T18:36:48.360" LastEditorUserId="88" OwnerUserId="28740" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;categorical-data&gt;&lt;chi-squared&gt;&lt;fishersexact&gt;" Title="Using $\chi^2$ or Fisher's exact test with low expected counts" ViewCount="251" />
  <row AnswerCount="2" Body="&lt;p&gt;I fit a mixed effect model to one set of data and now I want to check its performance when applied to another data set. I want to plot a figure of &quot;predicted value&quot; and &quot;observed value&quot; and calculate $R^2$ to evaluate model performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any code or command for this in R software?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-15T17:00:21.477" FavoriteCount="1" Id="76671" LastActivityDate="2014-01-14T21:20:05.140" LastEditDate="2013-11-15T19:31:54.877" LastEditorUserId="22311" OwnerUserId="34566" PostTypeId="1" Score="0" Tags="&lt;mixed-model&gt;" Title="Mixed effect model validation" ViewCount="350" />
  
  <row Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vVziI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume a seller on some e-commerce web-site receives 500 ratings of which 400 are good and 100 are bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;We think of this as the result of a Bernoulli experiment of length 500 which led to 400 successes (1 = good) while the underlying probability $p$ is unknown.&lt;/p&gt;&#10;&#10;&lt;p&gt;The naive quality in terms of ratings of the seller is 80% because 0.8 = 400 / 500. But the &quot;true&quot; quality in terms of ratings we don't know.&lt;/p&gt;&#10;&#10;&lt;p&gt;Theoretically also a seller with &quot;true&quot; quality of $p=77\%$ might have ended up with 400 good of 500 ratings.&lt;/p&gt;&#10;&#10;&lt;p&gt;The pointy bar plot in the picture represents the frequency of how often it happend in a simulation that for a given assumed &quot;true&quot; $p$ 400 of 500 ratings were good. The bar plot is the density of the histogram of the result of the simulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;And as you can see - the density curve of the beta distribution for $\alpha=400+1$ and $\beta=100+1$ (orange) tightly surrounds the bar chart (the density of the histogram for the simulation).&lt;/p&gt;&#10;&#10;&lt;p&gt;So the beta distribution essentially defines the probability that a Bernoulli experiment's success probability is $p$ given the outcome of the experiment.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2)&#10;&#10;# 90% positive of 10 ratings&#10;o1 &amp;lt;- 9&#10;o0 &amp;lt;- 1&#10;M &amp;lt;- 100&#10;N &amp;lt;- 100000&#10;&#10;m &amp;lt;- sapply(0:M/M,function(prob)rbinom(N,o1+o0,prob))&#10;v &amp;lt;- colSums(m==o1)&#10;df_sim1 &amp;lt;- data.frame(p=rep(0:M/M,v))&#10;df_beta1 &amp;lt;- data.frame(p=0:M/M, y=dbeta(0:M/M,o1+1,o0+1))&#10;&#10;# 80% positive of 500 ratings&#10;o1 &amp;lt;- 400&#10;o0 &amp;lt;- 100&#10;M &amp;lt;- 100&#10;N &amp;lt;- 100000&#10;&#10;m &amp;lt;- sapply(0:M/M,function(prob)rbinom(N,o1+o0,prob))&#10;v &amp;lt;- colSums(m==o1)&#10;df_sim2 &amp;lt;- data.frame(p=rep(0:M/M,v))&#10;df_beta2 &amp;lt;- data.frame(p=0:M/M, y=dbeta(0:M/M,o1+1,o0+1))&#10;&#10;ggplot(data=df_sim1,aes(p)) +&#10;    scale_x_continuous(breaks=0:10/10) +&#10;&#10;    geom_histogram(aes(y=..density..,fill=..density..),&#10;        binwidth=0.01, origin=-.005, colour=I(&quot;gray&quot;)) +&#10;    geom_line(data=df_beta1 ,aes(p,y),colour=I(&quot;red&quot;),size=2,alpha=.5) +&#10;&#10;    geom_histogram(data=df_sim2, aes(y=..density..,fill=..density..),&#10;        binwidth=0.01, origin=-.005, colour=I(&quot;gray&quot;)) +&#10;    geom_line(data=df_beta2,aes(p,y),colour=I(&quot;orange&quot;),size=2,alpha=.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.joyofdata.de/blog/an-intuitive-interpretation-of-the-beta-distribution/&quot;&gt;http://www.joyofdata.de/blog/an-intuitive-interpretation-of-the-beta-distribution/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-15T20:17:41.097" Id="76687" LastActivityDate="2013-11-15T21:37:28.277" LastEditDate="2013-11-15T21:37:28.277" LastEditorUserId="3077" OwnerUserId="3077" ParentId="47771" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;A simple way to handle this is to make a &quot;tree&quot; of models. The first model (hopefully) predicts the correct Category. Then for each Category, we train a sub model for each Sub-Category that belongs to the given category. And so on. This can be used for any model building algorithm. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another alternative would be to define your own loss function for which getting the wrong category is a bigger loss than the wrong sub-category, and so on. Then you could use SGD or some other technique to learn directly based on your desire to avoid such errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a similar vein, there is a lot of research on cost sensitive classification. You could use this, where things int he same sub-categories have lower costs for miss classification than getting the wrong category. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-16T01:52:28.470" Id="76704" LastActivityDate="2013-11-16T01:52:28.470" OwnerUserId="34874" ParentId="74669" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to understand the Newman Modularity (doi:10.1073/pnas.0601602103) by investigating its calculation on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Modularity_%28networks%29&quot; rel=&quot;nofollow&quot;&gt;Wikipedia example &lt;/a&gt;&#10;. My question is why are probabilities of self-loops included in the Q formula while the adjacency matrix A has no self-loops? &#10;i.e., given the standard definition of modularity Q...&lt;/p&gt;&#10;&#10;&lt;p&gt;$Q=\frac{1}{2m}\Sigma_{ij} (A-P_{ij})\delta(s_{i},s_{j})$&lt;/p&gt;&#10;&#10;&lt;p&gt;where A is the adjacency matrix and $P_{ij}$ is probability of a link between i and j, and $\delta()=1$ if entries are in the same community. While $A_{ii}=0$, this is not true for P, where $P_{ii}=\frac{k_{i}k_{i}}{2m} \neq0$, $k_i = \Sigma_{j}a_{ij}$ is the degree of i.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would seem to me that probabilities of self loops should NOT be included. A-P leads to negative entries for the community of i. Here is R code to show how self-loop probabilities need to be included for agreement with both wikipedia example and the same calculation in the 'igraph' package function 'modularity'&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# modularity function, input matrix A and membership vector mem&#10;Q &amp;lt;- function(A,mem){ diag(A) &amp;lt;- rep(0,ncol(A)) &#10;m &amp;lt;- sum(A)/2 # number of links&#10;k &amp;lt;- colSums(A) #degree&#10;kk &amp;lt;- outer(k,k,&quot;*&quot;) &#10;memmem &amp;lt;- outer(mem,mem,&quot;==&quot;)*1 # diag is 1!&#10;Pij &amp;lt;- kk/(2*m) # probability of link&#10;1/(2*m)*(sum((A-Pij)*memmem))}&#10;&#10;# Make a graph from Wikipedia entry on Modularity (Q=0.4895)&#10;Aw &amp;lt;- matrix(c(0,1,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,&#10;0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,&#10;0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,0,1,0,&#10;0,1,0,0,1,0,0,0),10,10)&#10;membership &amp;lt;- c(1,1,1,2,2,2,3,3,3,1)&#10;Q(Aw, membership) # = 0.4895833&#10;&#10;library(igraph) # check in igraph package&#10;modularity(graph.adjacency(Aw),membership) # = 0.4895833&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think that the entries $P_{ii}$ are needed to fulfill the constraint $\Sigma_{j} p_{ij} = \Sigma_{j} a_{ij} = k_i$ , i.e, the expectation of i's degree is its degree, but if $p_{ii}\neq 0$, than the probabilities are not distributed across the other nodes $i\neq j$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-16T06:15:58.267" Id="76713" LastActivityDate="2014-01-26T13:55:13.867" LastEditDate="2013-11-24T15:57:41.453" LastEditorUserId="32554" OwnerUserId="9526" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;networks&gt;&lt;partitioning&gt;" Title="Modularity of graph: why are probabilities of self-loop included?" ViewCount="149" />
  
  <row Body="&lt;p&gt;I guess the question arises because of some confusion with the possibilities to parameterise a Weibull survival model. Weibull survival model can be parameterised both as a proportional hazard (PH) model and accelerated failure time (AFT) model. The &lt;code&gt;survreg&lt;/code&gt; function in the &lt;code&gt;survival&lt;/code&gt; package in R uses the AFT, which means that it is not the hazard rate that is being modelled but time to failure. As such, you do not get an estimate in terms of hazard. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, in the case of Weibull survival model, both the PH and AFT result in the same regression model. This means, having the AFT model parameter coefficients one is able to get the coefficients of PH model. This is explained in P. Hougaard, &lt;em&gt;Fundamentals of Survival Data&lt;/em&gt;, Biometrics, March 1999, p.18. If the AFT model is: &lt;/p&gt;&#10;&#10;&lt;p&gt;$logT_i = \eta' z_i + \epsilon_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;the PH coefficient $\beta$ (as in $h(t)=exp(\beta'z)$) can be obtained using:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\eta = - \beta/\gamma$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\gamma$ is the shape parameter which is the inverse of scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps an easier alternative is to use &lt;code&gt;weibreg&lt;/code&gt; in &lt;code&gt;eha&lt;/code&gt; package which is a PH model from the beginning. &lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;delta&lt;/code&gt; in your model formula is supposed to be the vector that indicates failure or censoring. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-16T08:59:19.390" Id="76720" LastActivityDate="2013-11-16T08:59:19.390" OwnerUserId="26080" ParentId="76672" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Has anyone attempted to use Empirical Mode Decomposition(EMD)-Support Vector Regression(SVR) in nonstationary time series forecasting? It seems quite interesting. As I observed it has high performance compared with single SVR. But I didn't find it as a real approach for time series. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-16T09:27:54.807" Id="76721" LastActivityDate="2014-10-14T03:36:02.320" LastEditDate="2013-11-16T09:40:08.560" LastEditorUserId="22047" OwnerUserId="28260" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;svm&gt;&lt;forecasting&gt;" Title="EMD-SVR method for time series" ViewCount="70" />
  <row Body="&lt;p&gt;Actually, the steps you listed are incorrect. Stepwise works in the following way:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Add one variable based on &lt;em&gt;some&lt;/em&gt; criteria. By default for linear models, this is the F-statistic or the lowest associated p-value.&lt;/li&gt;&#10;&lt;li&gt;Try to remove one variable from the new model. This is based on the same criteria as above, but the cut-off is often different. If the p-value of any variables exceeds a certain limit, delete the variable with the largest p-value.&lt;/li&gt;&#10;&lt;li&gt;If no more variables can be added to the model, return to (1).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The easiest way to see this in action is to create an initial linear model where the first variable is weak. Off the top of my head, I took an example from R.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(deug) # French exam data&#10;formula = &quot;Algebra ~ Analysis + Proba + Informatic + Economy &#10;      + Option1 + Option2 + English + Sport&quot;&#10;&#10;model_1 = lm(Algebra ~ Sport, data=deug$tab)&#10;step(model_1, scope=formula, data=deug$tab)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Most of the variables are predictive, but you'll see that Sport is removed in the middle of the run. Once it's removed, another variable is found to be predictive, added in, etc.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-16T11:57:06.357" Id="76725" LastActivityDate="2013-11-16T11:57:06.357" OwnerUserId="29433" ParentId="76712" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I think the two functions are all to split the data, but I really can't get the difference between both of them, even I has read the help manual of them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-16T13:07:27.340" FavoriteCount="1" Id="76730" LastActivityDate="2014-03-20T18:40:34.890" OwnerUserId="34677" PostTypeId="1" Score="2" Tags="&lt;caret&gt;" Title="What is the difference between function createFolds and createDataPartition in caret" ViewCount="465" />
  <row Body="&lt;p&gt;Got too long for a comment, so I guess it has to be an answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is MCMC ever actually &lt;em&gt;required&lt;/em&gt;? I don't think so. There's nearly always some way of proceeding without it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes, one or another MCMC algorithm is quite convenient, where alternatives are not so convenient. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's one example:&lt;/p&gt;&#10;&#10;&lt;p&gt;To my mind a big part of what makes MCMC especially convienient - particularly with things like Gibbs and variations (Metropolis-within-Gibbs, block Gibbs etc) is the ability to construct components of models piece by piece as if everything else were okay. So you might have a tool for variable selection or selection of model order, and you might have a tool for dealing with outliers, and you might have a tool for dealing with estimation and you might have a tool for dealing with changing variance, but such procedures usually rely on everything else being either absent, or known rather than estimated.&lt;/p&gt;&#10;&#10;&lt;p&gt;What makes an outlier an outlier depends on your model, ... but what are the properties of your variable selection if you haven't controlled for outliers? If you do first one and then the other and then some of the first again... what are the properties of that?&lt;/p&gt;&#10;&#10;&lt;p&gt;MCMC often lets you work with those components conditionally on knowing the other things that would be inconvenient if you didn't know them (while still allowing you to integrate out the things it's convenient to integrate out). If you can establish that the particular details of the sampler you use converges to the desired stationary distribution (trivial if you do something standard, usually not difficult otherwise) and you actually iterate to effective convergence to it (rather harder to assess), then you are sampling from the stationary distribution (in a Bayesian context, typically the desired joint distribution of all your unknowns, from which you can extract the marginals).&lt;/p&gt;&#10;&#10;&lt;p&gt;That's by no means even remotely exhaustive - for example, I haven't touched on the quite different kind of convenience that Random Walk Metropolis-Hastings can give.&lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to really begin to understand the value in MCMC is to use it on a few real problems that are otherwise relatively hard.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-11-16T17:37:28.540" Id="76745" LastActivityDate="2013-11-16T18:32:11.507" LastEditDate="2013-11-16T18:32:11.507" LastEditorUserId="805" OwnerUserId="805" ParentId="76723" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The help page for those functions says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;&lt;code&gt;createFolds&lt;/code&gt; splits the data into &lt;code&gt;k&lt;/code&gt; groups&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;A series of test/training partitions are created using &lt;code&gt;createDataPartition&lt;/code&gt;&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So, &lt;code&gt;createDataPartition&lt;/code&gt; does one or more splits of the data (into two groups). You saw that in your other post that it can be used to create a single split. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;createFolds&lt;/code&gt; will split the data into &lt;code&gt;K&lt;/code&gt; groups of roughly equal size (presumably to use with K fold CV)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-16T19:48:01.683" Id="76752" LastActivityDate="2013-11-16T21:21:03.540" LastEditDate="2013-11-16T21:21:03.540" LastEditorUserId="22047" OwnerUserId="3468" ParentId="76730" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="76769" AnswerCount="1" Body="&lt;p&gt;We have a location family, with location parameter $\theta$, and densities defined over the real numbers $\mathbb{R}$ such that $p_\theta(x) = p(x-\theta)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to prove that the expected Fisher information&lt;/p&gt;&#10;&#10;&lt;p&gt;$J(\theta) = \int_\mathbb{R} \frac{(p'(x))^2}{p(x)}\,\mathrm{d}x$&lt;/p&gt;&#10;&#10;&lt;p&gt;implying that $J(\theta)$ is a constant function. Is it just about solving the integral? If yes, please a hint, my weak math background does not allow me to solve this... &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-17T00:01:22.610" Id="76767" LastActivityDate="2013-11-17T01:21:51.887" LastEditDate="2013-11-17T00:44:38.180" LastEditorUserId="17230" OwnerUserId="30623" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;fisher-information&gt;" Title="Show that expected Fisher information for location families is a constant function" ViewCount="223" />
  
  
  <row AcceptedAnswerId="76822" AnswerCount="1" Body="&lt;p&gt;Why should the importance density (or biasing density) $g$ have heavier tails than the original distribution $f$? &lt;/p&gt;&#10;&#10;&lt;p&gt;Equivalently, why should&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{f(x)}{g(x)}&amp;lt;M , \forall x$$ for some $M&amp;gt;0$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-17T13:24:47.800" FavoriteCount="2" Id="76798" LastActivityDate="2013-11-18T12:30:39.657" LastEditDate="2013-11-18T12:30:39.657" LastEditorUserId="9394" OwnerUserId="31718" PostTypeId="1" Score="2" Tags="&lt;importance-sampling&gt;" Title="In importance sampling, why should the importance density have heavier tails?" ViewCount="250" />
  <row Body="&lt;p&gt;I have came across a paper where they stated that: &quot;The product of independent log-normal&#10;quantities also follows a log-normal distribution&quot;, pp-345. It also have very rich understanding of the Lognormal Distribution. You can download the Article from here:&#10;&lt;a href=&quot;http://stat.ethz.ch/~stahel/lognormal/bioscience.pdf&quot; rel=&quot;nofollow&quot;&gt;http://stat.ethz.ch/~stahel/lognormal/bioscience.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the Second question, If I come across any solution, i will let you know. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-17T15:52:52.583" Id="76803" LastActivityDate="2013-11-17T15:52:52.583" OwnerUserId="34916" ParentId="65998" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Heuristically, it's because, for many situations of interest, what happens in the tails of the distribution is important, maybe more important than what happens in the middle, so undersampling the tails results in relatively inaccurate estimates of the target quantity.&lt;/p&gt;&#10;&#10;&lt;p&gt;More formally, consider a known function $h(x)$, along with original distribution $f$ and importance sampling distribution $g$.  Assume we are attempting to estimate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu = \mathbb{E}h = \int h(x)f(x)\text{d}x$$&lt;/p&gt;&#10;&#10;&lt;p&gt;but we are forced by circumstance to resort to importance sampling.  Our importance sampling estimate $\hat{\mu}_h$ is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\mu}_h = \frac{1}{n}\sum_{i=1}^nh(x_i)f(x_i)/g(x_i)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $x_i \sim g$.  The variance of our estimate is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma^2(\hat{\mu}) = {1 \over n}\left[\int {[h(x)f(x)]^2 \over g(x)} \text{d}x - \mu^2\right] = {1 \over n}\left[\int \left({f(x) \over g(x)}\right) h^2(x)f(x)\text{d}x - \mu^2 \right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For comparison, if we had a sample of $h(x)$ with $x$ drawn from $f$, the variance of the sample mean of the $h$ is $\sigma^2(\bar{h}) = {1 \over n}\left[\int h^2(x)f(x)\text{d}x - \mu^2 \right]$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, consider $f,g$ such that $f/g$ is unbounded.  Typically this would happen in the tails of the two distributions and would come about because your sampling density $g$ has thinner tails than $f$, although you could easily construct examples where it happened in the center.  Depending upon $h^2f$, this could result in a very large or even infinite $\sigma^2(\hat{\mu})$.  (If $h = 0$ over the regions where $f/g$ is large, of course, you won't have an issue - but that is a very problem-specific, and I expect rare, situation.)  On the other hand, if $f/g &amp;lt; M$ for some $M &amp;gt; 1$, it is clear that $\sigma^2(\hat{\mu}) \leq M \sigma^2(\bar{h})$.  We'll have not only prevented a possible catastrophe in estimation, we'll have bounded how poorly we do relative to using the sample mean of the $h$ as an estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, importance sampling can be a variance-reduction technique, even relative to the sample mean.  By &quot;over-sampling&quot; those regions which contribute disproportionately heavily to $h^2f$, we can increase the accuracy of our final estimate.  A trivial example of this is when $h = 0$ outside some region; an importance sampling distribution that also equals zero outside that region will prevent us from wasting samples on $x_i$ from a region that contributes $0$ to $\mathbb{E}h$.  A more realistic example is estimating $h = $ the mean absolute deviation of a $t(3)$ variate which we know is centered at $0$; we'll compare the sample mean, an importance sampling estimate based upon the Normal distribution, and an importance sampling estimate based upon the Cauchy distribution.  Our sample size is 100, and we repeat the experiment $N = 10,000$ times to evaluate the performance of the three estimators.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 10000&#10;results &amp;lt;- data.frame(list(Normal=rep(0,N), Cauchy=rep(0,N), t3=rep(0,N)))&#10;for (i in 1:N) {&#10;   x_norm &amp;lt;- rnorm(100)&#10;   x_cauchy &amp;lt;- rt(100, df=1)&#10;   results[i,1] &amp;lt;- mean(abs(x_norm)*dt(x_norm, df=3)/dnorm(x_norm))&#10;   results[i,2] &amp;lt;- mean(abs(x_cauchy)*dt(x_cauchy, df=3)/dt(x_cauchy, df=1))&#10;   results[i,3] &amp;lt;- mean(abs(rt(100, df=3)))&#10;}&#10;apply(results,2,var)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Repeating this five times results in the following estimated variances of the three estimates of MAD:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      Normal      Cauchy          t3 &#10; 3.392863921 0.005228449 0.016933091 &#10; 4.987301438 0.005108166 0.018078036 &#10;21.527506149 0.005078266 0.018151188 &#10; 1.314209463 0.005108059 0.017396005 &#10; 2.829562814 0.005163212 0.017341226 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Clearly the Cauchy-based estimator is the winner, and that &quot;21.52...&quot; result for the Normal-based estimator should make us suspect that the true variance might not be finite. &lt;/p&gt;&#10;&#10;&lt;p&gt;The moral of the story is: use proposal distributions with heavier tails than the original, unless you have a good reason not to.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-17T20:47:31.240" Id="76822" LastActivityDate="2013-11-17T20:47:31.240" OwnerUserId="7555" ParentId="76798" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working with a highly class-skewed three class classification problem. The class percentages are A = 1.8%, B = 17.5% and C = 80.7%. According to &lt;a href=&quot;https://www.google.co.in/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;ved=0CDcQFjAB&amp;amp;url=http://www.hpl.hp.com/techreports/2003/HPL-2003-4.pdf&amp;amp;ei=f0KJUurmNIqR5ASsw4DwCw&amp;amp;usg=AFQjCNHRBGHhI_WPGcnVc8VrTL3LGStuzg&amp;amp;sig2=rTrxpymNf2eSZtufqG2jLQ&amp;amp;bvm=bv.56643336,d.bGE&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; paper, the following definition of multi-class AUC is insensitive to class distributions and therefore, I am using it:&lt;img src=&quot;http://i.stack.imgur.com/oqdEN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I am testing 10 different classfiers on the dataset and only one of them has $AUC_{total} &amp;gt; 0.5$. Am I right in assuming that $AUC_{total} = 0.5$ for a random classifier? If yes, are 9 of the classifiers worse than a random classifier (on this data) just because they have a lower $AUC_{total}$? If yes again, can you suggest possible reasons for such poor performance and what can be done.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.: Even the best performing classifier has $AUC_{total}$ of about 0.6. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-17T22:40:19.573" Id="76830" LastActivityDate="2013-11-17T22:57:15.050" OwnerUserId="34645" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;roc&gt;&lt;multi-class&gt;&lt;supervised-learning&gt;" Title="Area Under ROC Curve for Multiple Classes" ViewCount="199" />
  <row Body="&lt;p&gt;Let $Z \sim N_n(μ_Z, Σ_Z)$ and $Z= \left(\begin{array}\\Y\\X\end{array}\right)$, where $X$ has been observed. What you want is to generate from the distribution of $Y|X$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If $μ_Z$ and $Σ_Z$ are partitioned as follows&lt;/p&gt;&#10;&#10;&lt;p&gt;$$    \mu_Z = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;corresponding to the means of the $Y$ and $X$ components respectively, where the lengths of $\mu_1$ and $\mu_2$ are &#10;$n-k$ and $k$ respectively, and &lt;/p&gt;&#10;&#10;&lt;p&gt;$$    \Sigma_Z = \begin{bmatrix} \Sigma_{11} &amp;amp;\Sigma_{12} \\ \Sigma_{21} &amp;amp; \Sigma_{22} \end{bmatrix}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $\Sigma_{11}$ being $(n-k) \times (n-k)$, $\Sigma_{12}=\Sigma_{21}^T$ being $(n-k) \times k$ and $\Sigma_{22}$ being $k \times k$, then this is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions&quot; rel=&quot;nofollow&quot;&gt;standard result&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution of $Y$ conditional on $X = \mathbf{x}$ is multivariate normal $(Y|X = \mathbf{x}) \sim N(μ, Σ)$ where&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mu = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} \left( \mathbf{x} - \mu_2 \right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and covariance matrix&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Sigma = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;A derivation is outlined &lt;a href=&quot;http://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have $\mu$ and $\Sigma$, you can generate as for any multivariate normal, which if you already know them might, for example, be done via Cholesky decomposition. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you don't have that conditional mean and variance already available, the above computation of $\mu$ and $\Sigma$ is not the most efficient or stable way to approach the problem, though; a good way would probably involve working with decompositions more directly, for example avoiding explicit computation of $\Sigma_{22}^{-1}$ at all. &lt;/p&gt;&#10;&#10;&lt;p&gt;One approach (most likely not the best approach):&lt;/p&gt;&#10;&#10;&lt;p&gt;If $L$ is the lower triangular Cholesky factor of $\Sigma_{22}$ (that is $LL^T=\Sigma_{22}$ for lower triangular $L$), then you can avoid computation of the inverse by computing $\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}$ and $\Sigma_{12} \Sigma_{22}^{-1} \left( \mathbf{x} - \mu_2 \right)$ by solving linear systems involving $L$ or $R=L^T$. Note that&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} = \Sigma_{12} (LL^T)^{-1} \Sigma_{21}= \Sigma_{12} (L^T)^{-1}L^{-1} \Sigma_{21} = A^TA$, say.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you need to solve the system $LA = \Sigma_{21}$ to find $A$ (or equivalently solve $BR=\Sigma_{12}$ for $B=A^T$ where $R=L^T$ is the upper, or right Cholesky factor).&lt;/p&gt;&#10;&#10;&lt;p&gt;With that solved, you could compute $\Sigma = \Sigma_{11}-A^TA$, and if you also solve $L^Tb = \left( \mathbf{x} - \mu_2 \right)$ for $b$, then $\mu=A^Tb$.&lt;/p&gt;&#10;&#10;&lt;p&gt;[However, there's probably a way to compute the decomposition of $\Sigma$ more directly; I'll try to think about how to that.]&lt;/p&gt;&#10;&#10;&lt;p&gt;These calculations are readily done in a number of packages.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, for example, assuming we already have variables &lt;code&gt;muz&lt;/code&gt;, &lt;code&gt;Sigz&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;k&lt;/code&gt;, being the mean and variance-covariance matrix of the original normal $n$-vector, and the observed $k$-vector as well as the values values of those dimensions, you might do this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nk &amp;lt;- n-k&#10;ry &amp;lt;- 1:nk&#10;rx &amp;lt;- (nk+1):n&#10;&#10;mu1 &amp;lt;- muz[ry]&#10;mu2 &amp;lt;- muz[rx]&#10;S11 &amp;lt;- Sigz[ry,ry]&#10;S12 &amp;lt;- Sigz[ry,rx]   # not used, I just get it for completeness&#10;S21 &amp;lt;- t(S12)        &#10;S22 &amp;lt;- Sigz[rx,rx]   # so far just setting up all the partitions&#10;&#10;R &amp;lt;- chol(S22)&#10;L &amp;lt;- t(R)&#10;&#10;A &amp;lt;- solve(L,S21)&#10;b &amp;lt;- solve(R,x-mu2)&#10;mu &amp;lt;- mu1 + crossprod(A,b)&#10;Sig &amp;lt;- S11 - crossprod(A)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The samples could then be generated by computing the Cholesky decomposition of Sig:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R2 &amp;lt;- chol(Sig)&#10;zeta &amp;lt;- matrix(rnorm(nk*nsim),nr=nk)&#10;dev &amp;lt;- crossprod(R2,zeta)&#10;y &amp;lt;- drop(mu)+dev&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;provided one has previously defined &lt;code&gt;nsim&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, the generation part could be done by installing the &lt;code&gt;mvtnorm&lt;/code&gt; package and calling &lt;code&gt;rmvnorm&lt;/code&gt; with &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;Sig&lt;/code&gt; as the mean and variance arguments. &lt;/p&gt;&#10;&#10;&lt;p&gt;The calculations could as easily be done in say Matlab, or any number of other packages with similar ease.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sigz &amp;lt;- matrix(c(5,3,3,4),nr=2)&#10;muz &amp;lt;- c(10,8)&#10;x=9; n=2; k=1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;followed by running the first section of above code to produce &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;Sig&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mu&#10;      [,1]&#10;[1,] 10.75&#10;&amp;gt; Sig&#10;     [,1]&#10;[1,] 2.75&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nsim &amp;lt;- 3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;followed by the second section of code to produce the random values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; y&#10;         [,1]     [,2]     [,3]&#10;[1,] 11.50796 10.76874 13.42943&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If $n-k$ was greater than 1 each column would be a vector of values (of length $n-k$), and there would be &lt;code&gt;nsim&lt;/code&gt; columns.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-18T00:25:29.863" Id="76835" LastActivityDate="2013-11-18T23:03:16.457" LastEditDate="2013-11-18T23:03:16.457" LastEditorUserId="805" OwnerUserId="805" ParentId="76820" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;I'd like to find the min/max boundaries of a sliding window of minimum size that contains a certain fraction of the total number of elements in an array or collection of numbers. &lt;/p&gt;&#10;&#10;&lt;p&gt;Example: taking integers to make it easier to explain, say the proportion of elements that we're looking for is 50% on this array:&lt;/p&gt;&#10;&#10;&lt;p&gt;[1,1,2,3,3,3,3,3,3,4,5,6,7,8,9,10]&lt;/p&gt;&#10;&#10;&lt;p&gt;The mystery function would return something like ( 2, 4 ), meaning that the minimum value is 2 and the max value is 4 (let's assume it's inclusive and note that these are VALUES, not indices). That little window contains 8 of the 16 values and it's only 2 units wide, the narrowest such window that contains half the number of values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: the quartiles are [1,1,2,3], [3,3,3,3], [3,4,5,6], [7,8,9,10] so they are NOT what I'm looking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can code it...may take some time to get it nice. Hopefully somebody has run across it before. Does it have a name?  Is there an existing method that somebody knows about?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in Python implementations. &lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE:&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks to Glen_b who gave the names 'short-half' and 'shortest interval' I was able to come up with this:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/18533/find-probability-density-intervals&quot;&gt;Find probability density intervals&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-18T05:39:17.110" FavoriteCount="3" Id="76848" LastActivityDate="2015-02-26T11:43:21.270" LastEditDate="2013-11-18T11:53:59.500" LastEditorUserId="22047" OwnerUserId="34941" PostTypeId="1" Score="3" Tags="&lt;python&gt;&lt;descriptive-statistics&gt;" Title="Retrieving minimum width that contains specified fraction of all values" ViewCount="171" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have studied algorithms for clustering data (unsupervised learning): EM, and k-means. &#10;I keep reading the following : &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;k-means is a variant of EM, with the assumptions that clusters are&#10;  spherical.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Can somebody explain the above sentence? I do not understand what spherical means, and how kmeans and EM are related, since one does probabilistic assignment and the other does it in a deterministic way. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, in which situation is it better to use k-means clustering? or use EM clustering? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-18T11:47:06.623" FavoriteCount="6" Id="76866" LastActivityDate="2014-11-23T03:16:02.347" LastEditDate="2013-11-18T12:11:14.763" LastEditorUserId="3277" OwnerUserId="34950" PostTypeId="1" Score="6" Tags="&lt;clustering&gt;&lt;k-means&gt;&lt;expectation-maximization&gt;" Title="Clustering with K-Means and EM: how are they related?" ViewCount="1610" />
  <row Body="&lt;p&gt;Thus, with lots of help in the comments of the question, I have worked out the following solution in R. I think the result is correct, but am glad if anybody could review it. &lt;/p&gt;&#10;&#10;&lt;p&gt;The algorithm from @whuber may be less costly to calculate. As soon as the amount of different balls and N of drawn balls rise, the the &lt;code&gt;combn()&lt;/code&gt; function will fail due to too many combinations.&lt;/p&gt;&#10;&#10;&lt;p&gt;The result of the example in the question is p = &lt;code&gt;0.3993262&lt;/code&gt; with Fisher's noncentral hypergeometric destribution and &lt;code&gt;p = 0.3310244&lt;/code&gt; with Wallenius' noncentral hypergeometric distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define the variables&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;lt;!-- language: lang-R --&amp;gt;&#10;weights = c(0.4, 0.3, 0.1, 0.1, 0.1)&#10;names = c(&quot;draw1&quot;,&quot;draw2&quot;,&quot;draw3&quot;)&#10;drawN = c(3, 3, 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Create empty list to store data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; &amp;lt;!-- language: lang-R --&amp;gt;&#10;&#10;  N = length(weights)&#10;  draws = list()&#10;  probs = list()&#10;  groups = list()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Calculate the probabilities for each possible draw of the three draws: &lt;code&gt;3 times 10 possibilities&lt;/code&gt;. As pointed out by @henry in the comments, I am using the Fisher's non-central hypergeometric distribution: &lt;code&gt;dMFNCHypergeo()&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  &amp;lt;!-- language: lang-r --&amp;gt;    &#10;  for (i in seq(1,length(names))) {&#10;    name &amp;lt;- names[i]   &#10;    d &amp;lt;- data.frame(combn(seq(1,N),drawN[i]))&#10;    colnames(d) &amp;lt;- paste(name,colnames(d),sep=&quot;.&quot;)&#10;    draws &amp;lt;- c(draws,d)&#10;    groups[[name]] &amp;lt;- colnames(d)&#10;&#10;&#10;    urn &amp;lt;- rep(1,N)&#10;    for (col in colnames(d)) {&#10;      thisdraw &amp;lt;- rep(0,N)&#10;      thisdraw[d[,col]] &amp;lt;- 1        &#10;      probs[[col]] &amp;lt;- dMFNCHypergeo(thisdraw, urn, sum(thisdraw), weights)&#10;    }  &#10;  }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Combine the different draws and sum the wanted probabilities: &lt;code&gt;10 * 10 * 10 = 1000&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  &amp;lt;!-- language: lang-R --&amp;gt;    &#10;  groupcombs &amp;lt;- as.matrix(expand.grid(groups))&#10;  groupprobs &amp;lt;- unlist(probs)&#10;&#10;&#10;  psum = 0;&#10;  is &amp;lt;- c()&#10;  for (i in seq(1,dim(groupcombs)[1])) {&#10;    d &amp;lt;- draws[groupcombs[i,]]&#10;    if (length(unique(unlist(d))) == N) {&#10;      p &amp;lt;- prod(groupprobs[names(d)])&#10;      psum &amp;lt;- psum + p&#10;    }&#10;  }&#10;  print(psum)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-11-18T12:33:53.640" Id="76870" LastActivityDate="2013-11-18T12:33:53.640" OwnerUserId="2117" ParentId="76662" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Another option is &lt;a href=&quot;http://www.gnu.org/software/glpk/&quot; rel=&quot;nofollow&quot;&gt;GLPK&lt;/a&gt; for which there is also an &lt;a href=&quot;http://cran.r-project.org/web/packages/Rglpk/index.html&quot; rel=&quot;nofollow&quot;&gt;R interface&lt;/a&gt;. But you can also use it within other frameworks or packages, like Octave, Matlab or even the command line (glpsol).&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the nicest things is the MathProg language (a subset of AMPL). So you can write your program in MathProg and run it in any of these frameworks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-18T15:52:23.963" Id="76890" LastActivityDate="2013-11-18T15:52:23.963" OwnerUserId="17908" ParentId="76182" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;My Linear Models professor said that it is possible sometimes to obtain two different linear models for the same data, such that the models cannot be compared to determine which is better. How can this be?&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example he mentioned  models that have different scales. What does it mean for models to have different scales, and why does it make them incomparable?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is scale mismatch the only reason for models to be incomparable?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it only possible to compare a nested model to the full model that it derives from? If not, how does one compare two models that are not hierarchically related?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-11-18T16:06:31.273" FavoriteCount="1" Id="76893" LastActivityDate="2013-11-18T16:06:31.273" OwnerUserId="25386" PostTypeId="1" Score="3" Tags="&lt;linear-model&gt;&lt;model-comparison&gt;" Title="Comparing linear models" ViewCount="114" />
  <row AcceptedAnswerId="76913" AnswerCount="1" Body="&lt;p&gt;This should be a very elementary question, yet I cannot figure out where I am going wrong.&#10;The matrix below contains data on the colour distributions of balls in two urns. I am looking for a formal method that can tell me whether the contents of the two come from the same population distribution. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;freqs = c(25,94,85,47,13,1685)&#10;data = matrix(freqs, nrow=2)&#10;dimnames(data) = list(&quot;treatment&quot;=c(&quot;Urn1&quot;,&quot;Urn2&quot;), &quot;outcome&quot;=c(&quot;Blue&quot;,&quot;Green&quot;,&quot;Red&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plotting the (frequency-based) MLE's per urn, I can qualitatively observe that the colour distributions of Urn1 and Urn2 look pretty dissimilar.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;toplot&amp;lt;- as.matrix(rbind(data[1,],data[2,] ))&#10;barplot(toplot, beside = TRUE, col = c(&quot;green&quot;, &quot;gray&quot;), las=2); &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aSWTG.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen the $\chi^2$ independence test used to check 'association' between two sample sets like mine. When I run the test (below) I get the p_value &amp;lt; 2.2e-16 (below), which accepts (?) the null hypothesis that the colour distribution of sample set Urn1 is independent of the colour distribution of Urn2. I had expected to see a test result that indicates the two sample sets come from independent / different population distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think I am mixing concepts here. Am I trying to use $\chi^2$ test for something that it is not meant for ? If so, which method should I use for my simple comparison?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;result &amp;lt;- chisq.test(data)&#10;&#10;#   Pearson's Chi-squared test&#10;#&#10;#data:  data&#10;#X-squared = 884.9506, df = 2, p-value &amp;lt; 2.2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-11-18T17:01:16.457" Id="76898" LastActivityDate="2014-08-14T07:56:01.460" LastEditDate="2014-08-14T07:56:01.460" LastEditorUserId="28740" OwnerUserId="28740" PostTypeId="1" Score="2" Tags="&lt;categorical-data&gt;&lt;chi-squared&gt;&lt;multinomial&gt;" Title="Test whether two multinomial samples come from the same distribution" ViewCount="796" />
  <row Body="&lt;p&gt;The complexity of SVM regression is similar to the complexity of SVM classification. If problems of that size are feasible for you in a classification context, they are also feasible in regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;When using a nonlinear kernel, training complexity is quadratic in terms of the number of training instances. 100k training instances is quite a lot, so I most definitely recommend trying a linear kernel first. For the linear kernel, you should consider using &lt;a href=&quot;https://www.google.be/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDIQFjAA&amp;amp;url=http://www.csie.ntu.edu.tw/~cjlin/liblinear/&amp;amp;ei=UUuKUqfWAamw0AXX2YGgAw&amp;amp;usg=AFQjCNGoF6RDBPRPnl9JJ1wbv_ZDW9TDPQ&amp;amp;sig2=WDxG4dprhXtgHfKSVHrnmA&amp;amp;bvm=bv.56643336,d.d2k&quot; rel=&quot;nofollow&quot;&gt;LIBLINEAR&lt;/a&gt; instead of LIBSVM (same authors, the former is made specifically for large-scale problems).&lt;/p&gt;&#10;&#10;&lt;p&gt;The impact of the number of dimensions on training time is not very high, this is one of the advantages of kernel methods. Knowing that, you may well go for 4000 dimensions straight away. If you have 4000 dimensions, &lt;a href=&quot;http://stats.stackexchange.com/questions/73032/linear-kernel-and-non-linear-kernel-for-support-vector-machine/73156#73156&quot;&gt;linear models are likely to perform quite well&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is very hard to give a good estimate of the actual run time as it depends on a lot of things, related to the data &lt;em&gt;and&lt;/em&gt; your hardware. That said, you can expect training time to be in the order of &lt;strike&gt;hours&lt;/strike&gt; tens of minutes per model for LIBSVM. If you use LIBLINEAR, it will be a couple of seconds.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-18T17:18:44.600" Id="76902" LastActivityDate="2013-11-19T11:23:52.837" LastEditDate="2013-11-19T11:23:52.837" LastEditorUserId="25433" OwnerUserId="25433" ParentId="76899" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I would recommend using &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot; rel=&quot;nofollow&quot;&gt;LibSVM&lt;/a&gt; which you can &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/#matlab&quot; rel=&quot;nofollow&quot;&gt;call from Matlab&lt;/a&gt;, it is generally much faster and better. As for the parameters: you need to optimize both of these by splitting your data in three pieces and testing different values on a part of the data. Basically you would procede as follows (fractions are just a choice):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Split off 1/3 training data&lt;/li&gt;&#10;&lt;li&gt;Split off 1/3 validation data&lt;/li&gt;&#10;&lt;li&gt;Split off 1/3 testing data&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You would train the SVM using a set of parameters on the training data and test it on the validation data and repeat this procedure for different parameters. Once an optimal parameter set is found, you then train on both the training and validation data (put them together) and evaluate your model on the testing data.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the parameter testing, this is usually done in a 'grid search' with a procedure called 'cross-validation' (see also the part about k-fold cross-validation &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;One more thing: if you were to use a linear kernel, things would probably go much faster. The kernel itself is much faster to evaluate and you would only need to optimize one parameter (the box constraint) instead of two (the box constraint and the sigma bandwidth).  That being said, I have worked with dimensions much larger than yours in a matter of seconds using LibSVM.&lt;/p&gt;&#10;&#10;&lt;p&gt;All of this and more is als explained in &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf&quot; rel=&quot;nofollow&quot;&gt;'A Practical Guide to Support Vector Classication'&lt;/a&gt;, which I strongly recommend you read before starting to use SVMs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-18T17:41:51.110" Id="76905" LastActivityDate="2013-11-18T17:41:51.110" OwnerUserId="12752" ParentId="76896" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Both variables are categorical without natural ordering. A $\chi^2$-test of independence (and its versions) is usually the test of choice in such situation. It tests the null hypothesis that the true proportions of females (and males) are equal across eye color versus the alternative that there are some differences.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Of course, a non-significant result does not mean that the proportions are equal.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If you compare the p value of the $\chi^2$-test with the F-test of the one-way ANOVA with 0-1 coded gender as response, you will get about the same. So your intuition was quite right!&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: The $\chi^2$-test is symmetric in its two variables. So you can as well check the null hypothesis that the color distribution of males equals the color distribution of females.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-18T18:49:22.020" Id="76917" LastActivityDate="2013-11-18T18:49:22.020" OwnerUserId="30351" ParentId="76897" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Once or twice a year, we go through an elaborate and expensive process to measure leakage in a pipe.  We calculate leakage via mass balance, so that:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;    Leak = X + Y - Z&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on how well data was collected for each variable, we select particular assumptions.  For example, we might assume Z to be uniformly distributed, while X &amp;amp; Y are normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;The end goal is to calculate the confidence interval of the estimated leak.&lt;/p&gt;&#10;&#10;&lt;p&gt;After calculating $\hat{\mu}$ and $\hat{\sigma}$ for each variable, it's tempting to assume that $Leak$ is normally distributed, just because deriving the confidence interval is straight forward.  I just need to calculate $t_{\alpha/2}$, and define the confidence interval as $\mu_{Leak} \pm t_{\alpha/2}\hat{\sigma}_{Leak}$.  But this a pretty bad assumption, as $Leak$ is not normally distributed because $Z$ is uniformly distributed.  Any clues on how to calculate this confidence interval?&lt;/p&gt;&#10;&#10;&lt;h1&gt;Update:&lt;/h1&gt;&#10;&#10;&lt;p&gt;I used simulation, as @soakey suggested, to generate 10k-sized samples for each historical event, and calculating the confidence interval was a matter of calculating the $(\alpha/2)^{th}$ and the $(1-\alpha/2)^{th}$ percentiles.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-11-18T21:36:42.797" Id="76936" LastActivityDate="2013-11-19T05:21:12.093" LastEditDate="2013-11-19T05:21:12.093" LastEditorUserId="25697" OwnerUserId="25697" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;" Title="Calculating confidence interval of a combination of variables of different distributions?" ViewCount="60" />
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I'm studying two geographically-isolated populations of the same species. Inspecting the distributions, I see that both are bimodal (there's some seasonality to their occurrence), but the peaks in one population are much higher and much narrower (i.e., the variance of the local peaks is smaller).&lt;/p&gt;&#10;&#10;&lt;p&gt;What sort of statistical test would be appropriate to determine whether these differences are significant?&lt;/p&gt;&#10;&#10;&lt;p&gt;To clarify, my y-axis is the number of individuals identified in a trap on a particular day, and the x-axis is Julian day.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-11-18T18:41:14.833" FavoriteCount="1" Id="76962" LastActivityDate="2014-01-15T13:56:37.637" LastEditDate="2013-11-20T11:03:09.350" LastEditorUserId="25898" OwnerDisplayName="Atticus29" OwnerUserId="7344" PostTypeId="1" Score="7" Tags="&lt;distributions&gt;&lt;statistical-significance&gt;&lt;variance&gt;" Title="How to test whether the variance of two distributions is different if the distributions are not normal" ViewCount="352" />
  <row AcceptedAnswerId="76976" AnswerCount="2" Body="&lt;p&gt;I'm a little confused here. Appreciate if anyone can help me out.&lt;/p&gt;&#10;&#10;&lt;p&gt;During a &lt;em&gt;k-fold&lt;/em&gt; nested CV, I understand for each combination of training fold and testing fold , the training fold will be further split into &lt;em&gt;k&lt;/em&gt; subsets and a small CV will be carried out to determine the optimal hyper-parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: are the models generated by each training fold the same? If not, which model should be used when apply to productive environment.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know there is a similar thread 'Nested cross validation for model selection'. But I still don't quite get it after reading for several times. Please bear with me as I just step into the field.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;[Note]&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After reading some other threads, I can understand the mechanism below:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Create hyper-parameter matrix;&lt;/li&gt;&#10;&lt;li&gt;Use &lt;strong&gt;sub-training set&lt;/strong&gt; to fit model;&lt;/li&gt;&#10;&lt;li&gt;Use fitted model and &lt;strong&gt;validation set&lt;/strong&gt; to select model;&lt;/li&gt;&#10;&lt;li&gt;Use &lt;strong&gt;test fold&lt;/strong&gt; to evaluate the performance of the chosen model.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;However, my question is triggered by the book 'Data Science for Business'. When it describes 'Nested Cross Validation', it says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;... before building the model for each fold, we take the training set and first run an experiment: we run another entire cross-validation on just that training set to find the value of &lt;em&gt;C&lt;/em&gt; estimated to give the best accuracy. The result of that experiment is used only to set the value of &lt;em&gt;C&lt;/em&gt; to build the actual model for &lt;strong&gt;that&lt;/strong&gt; fold of the cross-validation...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In my understanding, each fold may get a different value of &lt;em&gt;C&lt;/em&gt; which leads to different models. Then which model should I use for productive operation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please help point out anything wrong in above understanding or if I have any misunderstanding of the book.&#10;Thanks a lot.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-19T06:46:15.607" Id="76967" LastActivityDate="2014-07-03T14:42:37.837" LastEditDate="2013-11-19T09:01:34.570" LastEditorUserId="34995" OwnerUserId="34995" PostTypeId="1" Score="1" Tags="&lt;cross-validation&gt;&lt;model-selection&gt;" Title="Model generation during nested cross validation" ViewCount="114" />
  <row AnswerCount="1" Body="&lt;p&gt;There is such a problem: we have to process multi-label classification (assignmet of tags) of text articles, using some pre-labeled training set. But for many texts in the training set, should be assigned more tags, that it was done by their authors.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, there is a text about war medicine, and there is assigned only the text &quot;war&quot;, and tag &quot;medicine&quot; is omitted.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any common, conventional methods to deal with such data?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-19T11:52:37.513" Id="76988" LastActivityDate="2013-11-19T12:59:01.567" LastEditDate="2013-11-19T12:46:08.730" LastEditorUserId="264" OwnerUserId="27261" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" Title="Classification of data with incomplete label sets" ViewCount="87" />
  <row Body="&lt;p&gt;Don't drop any variables, but do consider using PCA. Here's why.&lt;/p&gt;&#10;&#10;&lt;p&gt;Firstly, as pointed out by Anony-mousse, k-means is not badly affected by collinearity/correlations. You don't need to throw away information because of that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, if you drop your variables in the wrong way, you'll artificially bring some samples closer together. An example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Customer CatA CatB CatC&#10;1        1    0    0&#10;2        0    1    0&#10;3        0    0    1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(I've removed the % notation and just put values between 0 and 1, constrained so they all sum to 1.) &lt;/p&gt;&#10;&#10;&lt;p&gt;The euclidean distance between each of those customers in their natural 3d space is $\sqrt{(1-0)^2+(0-1)^2+(0-0)^2} = \sqrt{2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's say you drop CatC.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Customer CatA CatB &#10;1        1    0    &#10;2        0    1    &#10;3        0    0    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now the distance between customers 1 and 2 is still $\sqrt{2}$, but between customers 1 and 3, and 2 and 3, it's only $\sqrt{(1-0)^2+(0-0)^2}=1$. You've artificially made customer 3 more similar to 1 and 2, in a way the raw data doesn't support.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thirdly, collinerarity/correlations are not the problem. Your dimensionality is. 100 variables is large enough that even with 10 million datapoints, I worry that k-means may find spurious patterns in the data and fit to that. Instead, think about using PCA to compress it down to a more manageable number of dimensions - say 10 or 12 to start with (maybe much higher, maybe much lower - you'll have to look at the variance along each component, and play around a bit, to find the correct number). You'll artificially bring some samples closer together doing this, yes, but you'll do so in a way that should preserve most of the variance in the data, and which will preferentially remove correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;~~~~~&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;Re, comments below about PCA. Yes, it absolutely does have pathologies. But it's pretty quick and easy to try, so still seems not a bad bet to me if you want to reduce the dimensionality of the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;On that note though, I tried quickly throwing a few sets of 100 dimensional synthetic data into a k-means algorithm to see what they came up with. While the cluster centre position estimates weren't that accurate, the cluster &lt;em&gt;membership&lt;/em&gt; (i.e. whether two samples were assigned to the same cluster or not, which seems to be what the OP is interested in) was much better than I thought it would be. So my gut feeling earlier was quite possibly wrong - k-means migth work just fine on the raw data.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-19T12:08:47.670" Id="76991" LastActivityDate="2013-11-19T17:25:49.573" LastEditDate="2013-11-19T17:25:49.573" LastEditorUserId="16765" OwnerUserId="16765" ParentId="62253" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;How can I check the linear independence of my variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have this system $Ax=b$ where A is a $N\times 4$ matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to check the linear independence between the 4 variables in matrix $A$. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-19T16:01:29.847" Id="77012" LastActivityDate="2013-11-21T01:15:17.203" OwnerUserId="32285" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;mathematical-statistics&gt;&lt;independence&gt;" Title="How to check linear independence" ViewCount="58" />
  
  <row Body="&lt;p&gt;I would say your question is a qualified question not only in &lt;code&gt;cross validated&lt;/code&gt; but also in &lt;code&gt;stack overflow&lt;/code&gt;, where you will be told how to implement dimension reduction in R(..etc.) to effectively help you identify which column/variable contribute the better to the variance of the whole dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;The PCA(Principal Component Analysis) has the same functionality as SVD(Singular Value Decomposition), and they are actually the exact same process after applying &lt;code&gt;scale&lt;/code&gt;/the z-transformation to the dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some resources that you can go through in half an hour to get much better understanding. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not capable to give a vivid coding solution to help you understand how to implement svd and what each component does, but people are awesome, here are some very informative posts that I used to catch up with the application side of SVD even if I know how to hand calculate a 3by3 SVD problem.. :)&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Coursera Data Analysis Class by Jeff Leek: &lt;a href=&quot;https://class.coursera.org/dataanalysis-002/lecture/95&quot; rel=&quot;nofollow&quot;&gt;Video Lecture&lt;/a&gt; / &lt;a href=&quot;https://d396qusza40orc.cloudfront.net/dataanalysis/dimensionReduction.pdf&quot; rel=&quot;nofollow&quot;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://class.coursera.org/dataanalysis-002/forum/thread?thread_id=571&quot; rel=&quot;nofollow&quot;&gt;A Very Informative student post&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.ams.org/samplings/feature-column/fcarc-svd&quot; rel=&quot;nofollow&quot;&gt;A post from American Mathematical Society.&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-11-19T18:28:38.200" Id="77034" LastActivityDate="2013-11-19T18:28:38.200" OwnerUserId="31547" ParentId="76906" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In the bioinformatics literature, this is known as sequence coevolution. Disclaimer: I'm an author. &lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0038114&quot; rel=&quot;nofollow&quot;&gt;See this article.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm familiar with one branch of the literature. The early literature on coevolution formed positional covariance using $\chi^2$ measures. Someone got the idea to compare positional covariance to structural contact maps derived from pdbs. When compared in this way normalized mutual information handedly beats $\chi^2$ metrics. Several other metrics, such as SCA(which means a number of things, but probably means inner product), sum-of-squares-methods et al. have fell by the wayside. Normalized mutual information was improved upon by z-scoring away phylogenetic background for positions. I'm not sure where the literature is currently.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-11-19T18:41:56.083" Id="77036" LastActivityDate="2013-11-19T18:41:56.083" OwnerUserId="9568" ParentId="77029" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I hadn't heard about Kerrich before-- what a bizarre story.  The Google book scan (shared by reftt) of &quot;An Experimental Introduction to the Theory of Probability&quot; doesn't seem to include the body of the text.  Feeling a little old-fashioned, I checked out a copy of the 1950 edition from the library.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/j7c22syk8lkvno6/kerrich_1950.pdf&quot; rel=&quot;nofollow&quot;&gt;I have scanned a few pages&lt;/a&gt; that I found interesting.  The pages describe his test conditions, data from the first 2000 coin flips and data from the first 500 of a series of 5000 equally implausible-sounding urn experiments (with 2 red and 2 green ping pong balls).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Text recognition (and some cleanup) using &lt;em&gt;Mathematica&lt;/em&gt; 9 gives this sequence of 2000 tails (0) and heads (1) from Table 1.  The head count of 1014 is one more than 502+511=1013 in Table 2, so the recognition was imperfect, but it looks pretty good--at least it got the right number of characters!  (Sharp-eyed readers are invited to correct it.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a graphical summary of this random walk, followed by the data themselves.  The accumulated difference between head and tail counts proceeds from left to right, covering all 2000 results.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ozMto.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;00011101001111101000110101111000100111001000001110&#10;00101010100100001001100010000111010100010000101101&#10;01110100001101001010000011111011111001101100101011&#10;01010000011000111001111101101010110100110110110110&#10;01111100001110110001010010000010100111111011101011&#10;10001100011000110001100110100100001000011101111000&#10;11111110000000001101011010011111011110010010101100&#10;11101101110010000010001100101100111110100111100010&#10;00001001101011101010110011111011001000001101011111&#10;11010001111110010111111001110011111111010000100000&#10;00001111100101010111100001110111001000110100001111&#10;11000101001111111101101110110111011010010110110011&#10;01010011011111110010111000111101111111000001001001&#10;01001110111011011011111100000101010101010101001001&#10;11101101110011100000001001101010011001000100001100&#10;10111100010011010110110111001101001010100000010000&#10;00001011001101011011111000101100101000011100110011&#10;11100101011010000110001001100010010001100100001001&#10;01000011100000011101101111001110011010101101001011&#10;01000001110110100010001110010011100001010000000010&#10;10010001011000010010100011111101101111010101010000&#10;01100010100000100000000010000001100100011011101010&#10;11011000110111010110010010111000101101101010110110&#10;00001011011101010101000011100111000110100111011101&#10;10001101110000010011110001110100001010000111110100&#10;00111111111111010101001001100010111100101010001111&#10;11000110101010011010010111110000111011110110011001&#10;11111010000011101010111101101011100001000101101001&#10;10011010000101111101111010110011011110000010110010&#10;00110110101111101011100101001101100100011000011000&#10;01010011000110100111010000011001100011101011100001&#10;11010111011110101101101111001111011100011011010000&#10;01011110100111011001001110001111011000011110011111&#10;01101011101110011011100011001111001011101010010010&#10;10100011010111011000111110000011000000010011101011&#10;10001011101000101111110111000001111111011000000010&#10;10111111011100010000110000110001111101001110110000&#10;00001111011100011101010001011000110111010001110111&#10;10000010000110100000101000010101000101100010111100&#10;00101110010111010010110010110100011000001110000111&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-11-19T20:15:13.717" Id="77044" LastActivityDate="2013-11-20T18:36:39.743" LastEditDate="2013-11-20T18:36:39.743" LastEditorUserId="919" OwnerUserId="27381" ParentId="76663" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;This answer supposes that $X^TY$ (where $X$ and $Y$ are $n\times 1$ vectors) is a $1\times 1$ vector or scalar $\sum_i X_iY_i$ and so we need to consider the&#10;variance of a single random variable that is this sum of products.  Since $X$&#10;and $Y$ are independent random vectors, we note that $X_1, Y_1$ are independent&#10;random variables as are $X_2, Y_2$. Also, $(X_1, X_2)$ is independent of &#10;$(Y_1, Y_2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\operatorname{var}(Z_1+Z_2) = \operatorname{var}(Z_1) + \operatorname{var}(Z_2) + 2\operatorname{cov}(Z_1,Z_2)$, we get that&#10;$$\begin{align}&#10;\operatorname{var}(X_1Y_1+X_2Y_2) &amp;amp;= \operatorname{var}(X_1Y_1)+&#10;\operatorname{var}(X_2Y_2) + 2\operatorname{cov}(X_1Y_1, X_2Y_2)\\&#10;\end{align}$$&#10;The first two terms are the variances of the products of independent&#10;random variables, and the OP knows the formula for handling these.&#10;Turning to the covariance, we have that&#10;$$\begin{align}&#10;\operatorname{cov}(X_1Y_1, X_2Y_2) &amp;amp;=&#10;E[X_1Y_1X_2Y_2] - E[X_1Y_1]E[X_2Y_2]\\&#10;&amp;amp;= E[X_1X_2]E[Y_1Y_2] - E[X_1]E[Y_1]E[X_2]E[Y_2]\\&#10;&amp;amp;= \left(\operatorname{cov}(X_1, X_2)+E[X_1]E[X_2]\right)\left(\operatorname{cov}(Y_1, Y_2)+E[Y_1]E[Y_2]\right)\\&#10;&amp;amp; \qquad \qquad - E[X_1]E[Y_1]E[X_2]E[Y_2]\\&#10;\end{align}$$&#10;which simplifies to&#10;$$&#10;\operatorname{cov}(X_1Y_1, X_2Y_2) = &#10;\operatorname{cov}(X_1, X_2)E[Y_1]E[Y_2]+ \operatorname{cov}(Y_1, Y_2)E[X_1]E[X_2]\\ + \operatorname{cov}(X_1, X_2)\operatorname{cov}(Y_1, Y_2),\tag{1}$$&#10;a result that is eerily similar to the result&#10;$$\operatorname{var}(X_iY_i) = \operatorname{var}(X_i)(E[Y_i])^2 + \operatorname{var}(Y_i)(E[X_i])^2 + \operatorname{var}(X_i)\operatorname{var}(Y_i) \tag{2}$$&#10;quoted by the OP for independent random variables $X$ and $Y$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus we have&#10;$$\operatorname{var}\left(\sum_{i=1}^n X_iY_i\right)&#10;= \sum_{i=1}^n \operatorname{var}(X_iY_i) + 2\sum_{i=1}^{n-1}\sum_{j=i+1}^n&#10;\operatorname{cov}(X_iY_i, X_jY_j)$$&#10;where each term in the first sum on the right is given by $(2)$ and&#10;each term in the double sum on the right is given by $(1)$. Plug and chug and enjoy!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-20T03:59:03.553" Id="77077" LastActivityDate="2013-11-20T03:59:03.553" OwnerUserId="6633" ParentId="76961" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm not sure if I have the right concept of how to perform the chi-square test. I have a variable called &lt;code&gt;race&lt;/code&gt; which is a factor with multiple levels for different races, and I would like to see whether there is a correlation between each race and the response variable (&lt;code&gt;mortality&lt;/code&gt;). If I were to perform chi-square like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;chi.race &amp;lt;- chisq.test(mortality, race)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then it gives me an output like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Pearson's Chi-squared test&#10;data:  mortality and race&#10;X-squared = 4.9626, df = 9, p-value = 0.8376&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but it doesn't tell me anything about the correlation between each level in &lt;code&gt;race&lt;/code&gt; (e.g. White, Black, Asian, Hispanic, etc.) and &lt;code&gt;mortality&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;What am I doing wrong and/or what is wrong with my concept of chi-square? Thanks for the help.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-20T05:09:04.770" Id="77081" LastActivityDate="2013-11-20T16:08:30.293" OwnerUserId="9244" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;chi-squared&gt;" Title="Chi-square using factors with multiple levels in R" ViewCount="298" />
  
  
  <row AcceptedAnswerId="77120" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; I am working on a calibration problem involving a 1D function of parameter $\theta$ for which I derived a Jeffreys prior (in fact a 2D but I have an informative prior for one of the parameters).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Observation:&lt;/strong&gt; Using this prior gives me in practice very bad inference results. Paralelly when using a default choice of the form : $\theta \sim \mathcal{N}(o,1000^2)$ where $o$ is a typical value I get pretty good results. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What to do in such a situation (expect checking formulas and code) ? what does it means about my model/data ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess this question is quite abstract but any suggestion or remark would help me.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-20T13:39:29.393" Id="77115" LastActivityDate="2014-03-28T13:12:43.023" OwnerUserId="14346" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;prior&gt;" Title="When Jeffreys prior &quot;fails&quot;" ViewCount="160" />
  
  
  <row Body="&lt;p&gt;I don't think you want chi-square at all. I think you want some form of count regression (possibly Poisson or negative binomial). You have a model&lt;/p&gt;&#10;&#10;&lt;p&gt;Number of plants ~ environment + species (maybe + interaction)&lt;/p&gt;&#10;&#10;&lt;p&gt;where both of the independent variables are categorical. The above model would allow you to see overall effects of environment and species as well as the effect at each level of each variable. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-20T14:15:08.030" Id="77126" LastActivityDate="2013-11-20T14:15:08.030" OwnerUserId="686" ParentId="77124" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I guess I would like to read or at least browse in that too, but only a polymath or a committee could write it, and the polymath isn't evident and committee books often don't work well. Also, many of the general books on statistics that tend to pop up from (e.g.) searches on Amazon just leave out most of the interesting technical details and/or are written by people not close to any cutting edge. &lt;/p&gt;&#10;&#10;&lt;p&gt;But I would recommend browsing in the &lt;em&gt;Encyclopedia of Statistical Sciences&lt;/em&gt; if a library near you holds a copy: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471150444.html&quot; rel=&quot;nofollow&quot;&gt;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471150444.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and also that you look through what appears in &lt;em&gt;Statistical Science&lt;/em&gt;, which has a good track record of readable review and discussion papers. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would venture an assertion that most specialists in econometrics, psychometrics, machine learning, etc. would have little confidence that people outside their own field really understand what is currently central and most interesting in that field. (So, what else is new?) &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-01-19T17:08:43.427" CreationDate="2013-11-20T14:43:40.717" Id="77134" LastActivityDate="2013-11-20T14:43:40.717" OwnerUserId="22047" ParentId="73281" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a time series of daily measurement data, if you plot the histogram of the data points of the time series, you see long tail and high positive scewness. Also zero values is an important state. &lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to use Markov Chain to model the time series, but the measurement data are integers without upper bound, so in order to model it as markov chain, I need to define the a mapping from the measurement data to state space.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's no obvious definition of different states of the time series data. My first attempt is to use 25 percentile, 50 percentile and 75 percentile as threshold to define the states(0s are a separate state), say if I'm aiming at 5 states.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there other ways to define meaningful states? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-20T16:22:56.457" FavoriteCount="1" Id="77146" LastActivityDate="2013-11-20T16:45:35.053" LastEditDate="2013-11-20T16:45:35.053" LastEditorUserId="31333" OwnerUserId="31333" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;prediction&gt;&lt;markov-chain&gt;" Title="Alternative ways to define markov chain states?" ViewCount="41" />
  
  
  
  <row Body="&lt;p&gt;You can find a closed form (well, if you accept to use special functions) for the density of $X = \max (X_1, \dots, X_n)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $F_i, f_i$ be the cdf and the density of $X_i$, for $i=1, ..., n$. The cdf of $X$ is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{aligned} F(x) &amp;amp;= \mathbb P(X \le x) \\&#10;&amp;amp;= \mathbb P(X_1  \le x, \dots, X_n \le x) \\&#10;&amp;amp;= \mathbb P(X_1  \le x) \cdots \mathbb P( X_n \le x) \\&#10;&amp;amp;= F_1(x) \cdots F_n(x).&#10;\end{aligned}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Its density is then obtained by derivation:&#10;$$f(x) = F(x)\left( {f_1 \over F_1} + \cdots + {f_n \over F_n} \right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using this, you can find expected value and variance to a good accuracy and reasonable computing time with numerical integration procedures (cf &lt;code&gt;integrate&lt;/code&gt; in R). &lt;/p&gt;&#10;&#10;&lt;p&gt;I bet that in this case, you can permute integral and derivation with respect to the parameter, so you can obtain the derivatives in a similar way.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-20T20:41:17.370" Id="77177" LastActivityDate="2013-11-20T20:41:17.370" OwnerUserId="8076" ParentId="77110" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;What i wrote is not correct, it should have been:&#10;$1-((45!/39!)/45^6)$  where the nominator $(45!/39!)/45^6$ represents the probability to get zero equal cars. Having 6 cars, i have 45 choices for the 1st car, 44 for the 2nd, 43 for the 3d, 42 for the 4th, 41 for the 5th and 40 for the 6th. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Question answered by slash^ from ##statistics&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-20T21:48:04.253" Id="77183" LastActivityDate="2013-11-20T21:48:04.253" OwnerUserId="32087" ParentId="77156" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row Body="&lt;p&gt;Your model fits a linear time trend plus a first order Fourier series approximation for the seasonality. Since you don't define &lt;code&gt;cos.t&lt;/code&gt; or &lt;code&gt;sin.t&lt;/code&gt;, it is not possible to tell what seasonal period you have used. You can fit a model with correct seasonal period using&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t &amp;lt;- 1:length(NH3cH6)&#10;cos.t &amp;lt;- cos(2*pi*t/365)&#10;sin.t &amp;lt;- sin(2*pi*t/365)&#10;trend &amp;lt;- lm(NH3cH6 ~ t + cos.t + sin.t)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, you may need a higher order Fourier series approximation, and there are facilities in the &lt;code&gt;forecast&lt;/code&gt; package in R for doing that.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, make sure the data is a time series object of the correct frequency:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;NH3cH6 &amp;lt;- ts(NH3cH6, frequency=365)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Generate the Fourier terms for the regression (using 3 terms here, but choose the best number by minimizing the AIC of the fitted regression model).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(forecast)&#10;X &amp;lt;- fourier(NH3cH6,3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Fit the regression model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit  &amp;lt;- tslm(NH3cH6 ~ X + trend)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;tslm&lt;/code&gt; command will understand what &lt;code&gt;trend&lt;/code&gt; means, and will make sure the residuals and fitted values are &lt;code&gt;ts&lt;/code&gt; objects with time series characteristics.&lt;/p&gt;&#10;&#10;&lt;p&gt;To see the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(NH3cH6)&#10;lines(fitted(fit), col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-21T04:35:48.857" Id="77214" LastActivityDate="2013-11-21T04:35:48.857" OwnerUserId="159" ParentId="77080" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;I assume you are using ordinary least squares (linear) regression of the form&#10;$$y_i=b_0+b_1 x_{1,i}+b_2 x_{2,i}+\ldots+\epsilon_i.$$&#10;You are asking whether you have to include a predictor $\alpha$ if it is a function of another predictor $\beta$, i.e., if $\alpha=f(\beta)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is: it depends on the functional form $f$. If $f$ is linear, i.e., $\alpha=c\beta$, then you should/can only include one (perfect collinearity)&#10;$$y_i=b_0+b_1\alpha_i+b_2\beta_i+\epsilon_i=b_0+(b_2+b_1c)\beta+\epsilon_i.$$&#10;But, as this shows, you can estimate the effect of $\alpha$ and $\beta$ both at once if you just include $\beta$, so this doesn't really hurt the quality of your fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, since this is linear regression, you would lose information by including only one predictor if $f$ is nonlinear. Now, a ratio of two predictors is nonlinear, so you would lose information by not including the ratio as independent predictor.&lt;/p&gt;&#10;&#10;&lt;p&gt;So much on the theory. In practice, you should include &quot;squareness&quot; if you have good reasons to suspect it might affect your dependent variable (and I guess you do, otherwise you wouldn't ask), after controlling for height and width. Not including it in this case would be to introduce omitted variable bias deliberately. The only problem that might arise is that ratios can take very small or very large values, thus making standard errors very large (if you don't do significance testing, then maybe this is not a huge problem). But that could be solved by defining &quot;squareness groups&quot;: any ratio below, say, 0.5 is group 1 (define dummy for that), any ratio $r\in(0.5,2]$ is group 2 (another dummy) and so on. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-20T17:12:29.730" Id="77228" LastActivityDate="2013-11-20T17:12:29.730" OwnerDisplayName="Nameless" OwnerUserId="22543" ParentId="77227" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am attempting to analyse downstream travel speed of fish within two different rivers, however, I am getting lost on which model I should use. Please see working below&lt;/p&gt;&#10;&#10;&lt;p&gt;data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data4&amp;lt;-structure(list(code = structure(1:32, .Label = c(&quot;10888&quot;, &quot;10889&quot;, &#10;&quot;10890&quot;, &quot;10891&quot;, &quot;10892&quot;, &quot;10893&quot;, &quot;10894&quot;, &quot;10896&quot;, &quot;10897&quot;, &#10;&quot;10898&quot;, &quot;10899&quot;, &quot;10900&quot;, &quot;10901&quot;, &quot;10902&quot;, &quot;10903&quot;, &quot;10904&quot;, &#10;&quot;10905&quot;, &quot;10906&quot;, &quot;10907&quot;, &quot;10908&quot;, &quot;10909&quot;, &quot;10910&quot;, &quot;10914&quot;, &#10;&quot;10916&quot;, &quot;10917&quot;, &quot;10919&quot;, &quot;10920&quot;, &quot;10922&quot;, &quot;10923&quot;, &quot;10924&quot;, &#10;&quot;10925&quot;, &quot;10927&quot;), class = &quot;factor&quot;), speed = c(0.0296315046039244, &#10;0.0366986630049636, 0.0294297725505692, 0.048316183511095, 0.0294275666501456, &#10;0.199924957584131, 0.0798850288176711, 0.0445886457047146, 0.0285993712316451, &#10;0.0715158276875623, 1.53078364415863, 1.32386730007184, 0.0879404548120395, &#10;0.0884436446391665, 0.142680147253483, 0.256122292011316, 0.029228002169911, &#10;0.0535706245570686, 0.0268752274519418, 0.238146999654007, 0.424175803745118, &#10;0.021181456637386, 0.0337364380380858, 0.030136111418826, 0.0335130031085068, &#10;0.148793370546863, 0.096737574741185, 0.114746050390462, 0.0993760102817675, &#10;0.0985453244761994, 0.100223001228893, 0.0924775724221547), meanflow =       c(0.657410742496051, &#10;0.608271363339857, 0.663241108786611, 0.538259450171821, 0.666299529534762, &#10;0.507156583629893, 0.762448863636364, 37.6559178370787, 50.8557196935557, &#10;31.6601587837838, 225.109658536585, 230.452020833333, 106.450571428571, &#10;106.51658356546, 26.6998648648649, 22.6653709677419, 0.667544935064935, &#10;0.803912464319696, 0.660643403441683, 58.1045714285714, 164.84368, &#10;63.5970920920921, 50.7789043570669, 47.3644579971523, 45.6948401055409, &#10;0.808431216931217, 0.612270689655172, 46.4646057866184, 0.605024778761062, &#10;0.6091, 0.601483928571429, 0.616563426688633), length = c(136, &#10;157, 132, 140, 135, 134, 144, 149, 139, 165, 155, 143, 144, 143, &#10;156, 147, 133, 131, 129, 143, 140, 125, 140, 140, 147, 120, 115, &#10;139, 123, 120, 125, 136), river = structure(c(2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, &#10;1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L), .Label = c(&quot;c&quot;, &#10;&quot;f&quot;), class = &quot;factor&quot;)), .Names = c(&quot;code&quot;, &quot;speed&quot;, &quot;meanflow&quot;, &#10;&quot;length&quot;, &quot;river&quot;), class = &quot;data.frame&quot;, row.names = c(2L, 4L, &#10;6L, 8L, 10L, 12L, 14L, 16L, 18L, 20L, 22L, 24L, 26L, 28L, 30L, &#10;32L, 34L, 36L, 38L, 40L, 42L, 44L, 48L, 50L, 52L, 54L, 56L, 58L, &#10;60L, 62L, 64L, 68L)) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to look at the effect of specific variables on travel speed, particularly to see if travel speed is different between rivers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fixed effects: river, length&lt;/p&gt;&#10;&#10;&lt;p&gt;Interaction: An interaction between river and length is considered and added as a fixed effect due to fish swimming in slower currents, and body length having a direct impact on swimspeed. currents differ between river and so length will have greater impact on rivers with slower currents&lt;/p&gt;&#10;&#10;&lt;p&gt;So:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model1 &amp;lt;-lm(speed ~ 1 + river + length + river: length, data = data4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However 'meanflow' has a direct impact on travel speed as a higher flow will enable fish to swim quicker downstream. To account for this I believe this should be added as a random effect?!&lt;/p&gt;&#10;&#10;&lt;p&gt;And be like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;model2&amp;lt;- lmer(speed ~ river + length + river:length + (1|meanflow), data = data4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Yet this does not run as 'meanflow' is a continuous variable. would I be better to dis regard this variable, or maybe use a different model all together&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be great&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-11-21T11:28:26.047" Id="77242" LastActivityDate="2013-11-24T15:15:06.280" LastEditDate="2013-11-24T15:15:06.280" LastEditorUserId="32554" OwnerUserId="35111" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;mixed-model&gt;&lt;mixed-effect&gt;&lt;lme4&gt;" Title="Correct model to use in travel speed analysis" ViewCount="73" />
  <row AnswerCount="3" Body="&lt;p&gt;I am conducting research in which employee engagement will be evaluated on approximately 20 participants.  They will complete an 11 question Likert-scale survey, have interventions done on them (read a book, read 3 articles, and sit through a lecture), and complete the same 11 question survey. Is the correlated t-test the appropriate test to conduct on the pre and post data to determine if a statistically significant increase in employee engagement occurred?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-21T14:58:33.677" Id="77254" LastActivityDate="2014-02-21T21:34:52.897" LastEditDate="2013-11-21T18:29:51.437" LastEditorUserId="22047" OwnerUserId="35116" PostTypeId="1" Score="2" Tags="&lt;t-test&gt;" Title="Which test is appropriate for comparing Pre and Post intervention data?" ViewCount="1240" />
  
  
  
  <row AcceptedAnswerId="78487" AnswerCount="2" Body="&lt;p&gt;Almost in all texts which are discussing theorems of statistical learning, they assume analyzing arbitrary unknown distribution (the worst case). But in practice different problems (different data) have different levels of hardness, for example linear separable data is easier to learn than the data that are less (or not) separable by hyperplanes. &#10;Are there any works on formalizing the hardness of data (similar to the stuff done in complexity theory)&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I am not sure if the same as analyzing VC-dimension or not, but I think it is not. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-21T19:15:10.743" FavoriteCount="1" Id="77278" LastActivityDate="2013-12-19T19:46:59.447" LastEditDate="2013-11-21T22:18:17.313" LastEditorUserId="17812" OwnerUserId="17812" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;inference&gt;&lt;learning&gt;&lt;statistical-learning&gt;" Title="On the hardness of data to learn" ViewCount="131" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to run a Canonical Correlation (in &lt;code&gt;R&lt;/code&gt;) but I don't have the original (raw) data. I have only the correlation matrix of all the variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen some questions here about this, but my question continue unsolved. A user gave a parcial solution (&lt;a href=&quot;http://www.stat.wmich.edu/wang/561/egs/Rcancor.html&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.wmich.edu/wang/561/egs/Rcancor.html&lt;/a&gt;), but I need the canonical loadings, the percentual of variance in set Y that was explained from set X, and the variables significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone here could help me?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.: I am a new &lt;code&gt;R&lt;/code&gt; user. I have experience only on Eviews, GRETL and SPSS (also a little bit in Stata).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-21T21:11:46.527" Id="77287" LastActivityDate="2014-01-24T22:13:14.027" LastEditDate="2013-11-22T00:37:01.563" LastEditorUserId="3277" OwnerUserId="35142" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;algorithms&gt;&lt;canonical-correlation&gt;" Title="Canonical Correlation analysis without raw data" ViewCount="290" />
  
  <row Body="&lt;p&gt;It does in the exponent.&lt;br&gt;&#10;exp^(ARIMA(1,0,1)) &lt;/p&gt;&#10;&#10;&lt;p&gt;Where you calculate the arima on the lognormal difference between time points.&#10;ln(x_i / x_i+1) ~ ARIMA(1,0,1)&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, this needs to have uniform intervals or everything breaks, but thats for anything time series related usually. Infinite crossing and all that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Happy simulating.&lt;/p&gt;&#10;&#10;&lt;p&gt;-K&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-21T22:39:20.743" Id="77296" LastActivityDate="2013-11-21T22:39:20.743" OwnerUserId="35145" ParentId="76177" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="77652" AnswerCount="1" Body="&lt;p&gt;&lt;em&gt;Dataset of frequent 3-itemsets before running the algorithm:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{1, 2, 3}, {1, 2, 4}, {1, 2, 5}, {1, 2, 6}, {1, 3, 4}, {1, 3, 5}, &#10;{1, 3, 6}, {2, 3, 4}, {2, 3, 6}, {2, 3, 5}, {3, 4, 5}, {3, 4,6}, {4, 5, 6}.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Assuming there are only six items in the data set. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What is the list of all candidate 4-itemsets obtained by the candidate generation procedure in Apriori algorithm?&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;What are all candidate 4-itemsets that survive the candidate pruning step of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Apriori_algorithm&quot; rel=&quot;nofollow&quot;&gt;Apriori Algorithm&lt;/a&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;After using Apriori I get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\{1,2}\}, {\{1,3}\}, {\{1,2,5}\}, {\{2,3,6}\}$$&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-11-22T04:05:04.963" FavoriteCount="1" Id="77320" LastActivityDate="2013-11-25T18:04:06.713" LastEditDate="2013-11-24T16:24:00.413" LastEditorUserId="2970" OwnerUserId="32554" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;algorithms&gt;" Title="Apriori candidate pruning" ViewCount="324" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have got some small data sets (about 8 to 11 data points for each set), following Normal distribution. I would like to find out the 95% confidence interval of the 0.005 and 0.995 percentile of each set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Firstly, moment estimation method is employed to estimate the Normal distribution parameters, and their confidence interval is built by (mu~Normal, sigma^2~Chi-square) theorem. And find the CI of percentile by simulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, MLE method is also employed and the parameter's CI is built by MLE~asymptotic Normal theorem. Then find the CI of percentile by simulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;As the figure shows, the MLE CI is much narrow than Moment method.&#10;We know that MLE is efficient, leading small variance and narrow CI. This understanding is consistent with our figure.&lt;/p&gt;&#10;&#10;&lt;p&gt;But my MLE CI approach is based on asymptotic assumption, while my amount of data points is quite small. &#10;Would this (too small data amount) leads MLE's CI incorrect and worse than moment method?&#10;or it is still more efficient than moment method?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is the MLE CI too narrow to contain the 95% probability of the true value, if the amount is too small?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Mv3H1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-22T10:39:44.913" FavoriteCount="1" Id="77334" LastActivityDate="2013-12-22T18:44:17.450" LastEditDate="2013-11-22T14:20:54.773" LastEditorUserId="32167" OwnerUserId="32167" PostTypeId="1" Score="4" Tags="&lt;confidence-interval&gt;&lt;maximum-likelihood&gt;&lt;method-of-moments&gt;&lt;efficiency&gt;" Title="Is MLE more efficient than Moment method?" ViewCount="349" />
  <row Body="&lt;p&gt;You could do this by changing the data set, but you should not do it. You are evidently comparing some feature of basketball players. That could be useful (depending on what y is, of course). But if you remove the forwards, then you are creating a model for just guards and centers. This will change the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;If y is, say, height, then you should probably set the reference level to either &quot;guard&quot; or &quot;center&quot; (the shortest or the tallest). You will then be able to compare the other two positions to that one. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-22T11:35:02.667" Id="77338" LastActivityDate="2013-11-22T11:35:02.667" OwnerUserId="686" ParentId="77336" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Firstly, the regression of Y on $$X_1, X_2, X_3, X_4$$ yields:&#10;$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+e  (1)$$ &#10;Then, regress Y on $$X_2, X_3, X_4$$&#10;$$Y=\beta'_0+\beta'_2X_2+\beta'_3X_3+\beta'_4X_4+e'(2) $$&#10;Lastly, regress $$X_1$$ on $$X_2, X_3, X_4$$&#10;$$X_1=\beta''_0+\beta''_2X_2+\beta''_3X_3+\beta''_4X_4+e'' (3)$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I regress e' on e''&#10;$$e'= \alpha_1+\alpha_2e''+\epsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a simple method to prove that $$\alpha_2=\beta_2$$ &#10;Can I plug (3) into (1) (which yields equation (4) ) and then identity the residuals of (2) and (4) ? &lt;/p&gt;&#10;&#10;&lt;p&gt;p/s: I'm sorry but can anyone tell me how to add comment? I try the button &quot;add comment&quot; but I still can't. I'm new here.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-22T12:59:11.850" Id="77346" LastActivityDate="2013-11-22T13:05:54.240" LastEditDate="2013-11-22T13:05:54.240" LastEditorUserId="35125" OwnerUserId="35125" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Is this an application of Frisch-Waugh-Lovell theorem?" ViewCount="43" />
  <row Body="&lt;p&gt;Surely, you could run a logit, probit, or a cloglog model; in fact any appropriate binary model works well. The logit model (and to some extent the cloglog) model tend to be used more often, but this is only because of the familiarity of the users with these models.&lt;/p&gt;&#10;&#10;&lt;p&gt;See the following link for more details:&#10;&lt;a href=&quot;https://files.nyu.edu/mrg217/public/btscs.pdf&quot; rel=&quot;nofollow&quot;&gt;https://files.nyu.edu/mrg217/public/btscs.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-22T13:06:24.783" Id="77347" LastActivityDate="2013-11-22T13:06:24.783" OwnerUserId="34948" ParentId="57665" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="77392" AnswerCount="2" Body="&lt;p&gt;If I have some share of a product in some sector as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2012 2013&#10;50%  60%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and I find share change for 2012-13 then should it be 10% or 0.1?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-22T15:16:58.693" Id="77361" LastActivityDate="2013-11-23T01:31:46.967" LastEditDate="2013-11-23T01:31:46.967" LastEditorUserId="22047" OwnerUserId="32380" PostTypeId="1" Score="1" Tags="&lt;basic-concepts&gt;&lt;arithmetic&gt;" Title="How to express the difference between two percentages?" ViewCount="187" />
  <row AcceptedAnswerId="77371" AnswerCount="1" Body="&lt;p&gt;I found this question and since I'm learning probability I'm not sure how to go about it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Of a company’s employees, 30% are &#10;women and 6% are married women. &#10;Suppose an employee is selected at &#10;random. If the employee selected is a &#10;woman, what is the probability that she is &#10;married?&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have done it like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    W       W'&#10;&#10;M  0.018&#10;&#10;M' 0.282   &#10;  p(W)=0.3  p(W')0.7&#10;&#10;p(W')=1-p(W)=1-0.3=0.7&#10;p(M and W)=p(W)p(M|W)=0.3x0.06=0.018&#10;p(W and M')=p(W)-p(M and W')= 0.3-0.018=0.282&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Well that's where I'm at, how do I go about it next? Can I get this numbers? Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-22T16:22:36.423" Id="77367" LastActivityDate="2013-11-22T16:49:13.063" LastEditDate="2013-11-22T16:45:30.323" LastEditorUserId="31843" OwnerUserId="31843" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;conditional-probability&gt;&lt;contingency-tables&gt;" Title="Conditional table, how to fill it" ViewCount="67" />
  
  <row AnswerCount="1" Body="&lt;p&gt;We are having problems running generalized linear models with proportional data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, we have data like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Species  Trait(Diet)   IndividualsinForest   TotalIndividuals   ProportionForest &#10;X        Insectivore   300.5                 500.7              0.60             &#10;Y        Frugivore      32.3                  47.6              0.67              &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And we want to determine whether trait influences the proportion of individuals in forest. Note that the individual numbers are continuous (have decimals), because the original counts have been DISTANCE-adjusted.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most on the models that we have seen in R deal with count data. For example, see M.J. Crawley &lt;em&gt;Statistics: An Introduction using R&lt;/em&gt;, Chapter 10: Analyzing proportion data (&lt;a href=&quot;http://www3.imperial.ac.uk/pls/portallive/docs/1/1171926.PDF&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;These models use the &lt;code&gt;cbind&lt;/code&gt; statement to bind together the number of successes and the number of failures and then relate that to the predictor:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model.1 &amp;lt;- glm (cbind(IndividualsinForest, (TotalIndividuals-IndividualsinForest))~Diet, &#10;                family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, there seems to be two problems to using this approach for our situation:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;We are using &lt;code&gt;glm&lt;/code&gt; on a &lt;code&gt;cbind&lt;/code&gt; that combined vectors with continuous (decimal point) values; it is not clear whether this is allowed, although it runs in R. We have played around with multiplying everything by 1000 to get rid of the decimals, but find the results change a lot with such a technique (very different results if multiply by 10, by 100, by 1000)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;cbind&lt;/code&gt; runs a weighted regression, as explained by Crawley. This means that the model will weight the data of Species X, above, with 500.7 total individuals more heavily than the data of Species Y, with 47.6 individuals.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;But conceptually, we do not want a weighted regression, because we want to treat all species equally: a rare species’ data is as important to us as an abundant species’ data.  Species is the unit of replication here.&lt;/p&gt;&#10;&#10;&lt;p&gt;We tried to force a non-weighted regression using the same code, by adding columns to the data, for example, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Species   ProportionForest   PInForest   PTotal &#10;X         .60                60          100 &#10;Y         .67                67          100 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And then running:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model.3 &amp;lt;- glm(cbind(PInForest, (PTotal-PInForest))~Diet, family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But the resulting analyses are very overdispersed, and running them with &lt;code&gt;family=quasibinomial&lt;/code&gt; gives strange results (completely non-sensitive tests, with large p-values for fairly clear differences).&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence our current analytical strategy is to fall back on arcsine transforming the proportion and then running a general linear model. But we'd prefer though to run a generalized linear model because the arcsine transformation seems to be a relict of old pre-computer statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-22T17:11:43.923" FavoriteCount="1" Id="77376" LastActivityDate="2013-11-22T18:11:16.073" LastEditDate="2013-11-22T18:11:16.073" LastEditorUserId="22047" OwnerUserId="35180" PostTypeId="1" Score="3" Tags="&lt;generalized-linear-model&gt;&lt;proportion&gt;" Title="Generalized linear models with continuous proportions" ViewCount="446" />
  
  
  <row AcceptedAnswerId="77393" AnswerCount="1" Body="&lt;p&gt;I know that I can perform a stepwise selection on ordinary linear regression model based on the t-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;but what about Negative Binomial regression model - or GLM in general? Does it theoretically make sense to perform a stepwise selection on variables bases on P-value resulting from &quot;z-scores&quot; when I am trying to improve Negative Binomial regression model? Is such method recommended?&lt;/p&gt;&#10;&#10;&lt;p&gt;thank you&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-22T18:17:41.077" Id="77385" LastActivityDate="2013-11-22T19:26:48.413" OwnerUserId="35176" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;generalized-linear-model&gt;" Title="stepwise selection on Negative Binomial regression model" ViewCount="258" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;We are a financial institution which falls under this ‘Big Bank’ category. As a part of our routine analysis on input variables that feed several of our risk models, we would like to setup an input monitoring system that can trigger any drastic changes in the trend/pattern in our input variables over different period of time spanning form 2007 to 2013. We have decent understanding on how to perform this analysis on continuous variables, but we don’t know how to do this on binary variables that has two outcome(yes=1, No=0).I would like to take the same variable at different time period ( for e.g  Var1 from Jan2007 to Jan2008 Vs Var1 from Jan2008 to Jan2009) and see if there is a significant change in the trend/distribution. I would assume my question boil down to a point where we are comparing the binomial distribution of the same variable at two different time period. If I do that I am  afraid my samples wont be independent to each other, that threw away the option of using Chi-Squared test, What’s the right strategy to perform this analysis, how do I carry out this test preferably in SAS, if SAS doesn’t have right procedure or function, then R will be the next choice.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-22T19:10:25.203" Id="77391" LastActivityDate="2013-11-22T19:10:25.203" OwnerUserId="9228" PostTypeId="1" Score="0" Tags="&lt;sas&gt;" Title="Comparing two time period of same binary variables" ViewCount="138" />
  <row AnswerCount="1" Body="&lt;p&gt;Hopefully somebody will be able to shed some light on my SPSS problems! &lt;/p&gt;&#10;&#10;&lt;p&gt;I have been given 65 values. 57 of these data values are quarterly results and 8 are the holdback data to be used. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have to do: &#10;- Regression with Dummy variables with a linear trend cycle component &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know what to do as my results aren't making much sense? &lt;/p&gt;&#10;&#10;&lt;p&gt;For the first part - I obviously split the data into dummy variables for the relevant quarters (Q1-Q4). &lt;/p&gt;&#10;&#10;&lt;p&gt;I then performed regression analysis - linear. But all my values are extremely large and not significant. Also Q2 has been listed as 'excluded variables' in the results? I have followed the steps and I am unsure why this has happened. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then I thought of removing Q4, due to multi-collinearity but again the values are still quite large (&gt;.450). &lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure if I am doing something wrong at the start (especially with the excluded variables aspect) &lt;/p&gt;&#10;&#10;&lt;p&gt;Anybody got any idea? This is driving me nuts&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: It won't let me comment back on the main page for some reason. &lt;/p&gt;&#10;&#10;&lt;p&gt;The data set was given to us:&#10;&quot;It is a quarterly series of total consumer lending. It is not seasonally adjusted.&#10;The first 57 data values for modelling and choose the remaining 8 data values as holdback data to test your models.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The data is: (last 8 are holdback data)&#10;16180&#10;17425&#10;17424&#10;17240&#10;18240&#10;19880&#10;20143&#10;20545&#10;22155&#10;23344&#10;23717&#10;23467&#10;25442&#10;27278&#10;27848&#10;25704&#10;28919&#10;30280&#10;32095&#10;31041&#10;33182&#10;35067&#10;35557&#10;34420&#10;35948&#10;38643&#10;39612&#10;39185&#10;40143&#10;40056&#10;41360&#10;41343&#10;43652&#10;44554&#10;47903&#10;46460&#10;49402&#10;50254&#10;50335&#10;48763&#10;51529&#10;53481&#10;53482&#10;53882&#10;55219&#10;56180&#10;56037&#10;54106&#10;54915&#10;54641&#10;53805&#10;52179&#10;52026&#10;51522&#10;51733&#10;50672&#10;50882&#10;50878&#10;52199&#10;50261&#10;49615&#10;47995&#10;45273&#10;42836&#10;43321&lt;/p&gt;&#10;&#10;&lt;p&gt;It has to be SPSS generated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Email primarybeing12@hotmail.co.uk - not letting me respond to people. Thanks for any help!&lt;/p&gt;&#10;&#10;&lt;p&gt;Doing the ARIMA forecasting is the next step (which I understand). I have to do regression on the linear/non-linear for this question&lt;/p&gt;&#10;&#10;&lt;p&gt;If I was to use time, time^2, Q1, Q2, Q3 + lagged variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would I use lagged variables 1-3? Also, I understand the rest, but what benefit does using lagged variables do? As I said, feel free to e-mail me if you can.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-22T19:47:08.737" Id="77394" LastActivityDate="2013-11-23T13:38:18.720" LastEditDate="2013-11-23T10:24:47.480" LastEditorUserId="35184" OwnerUserId="35184" PostTypeId="1" Score="-1" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;self-study&gt;&lt;spss&gt;&lt;linear-model&gt;" Title="Linear/Non-linear Regression - SPSS" ViewCount="939" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm using the Poisson distribution to predict the number of concurrent users accessing a website. The method is described in a &lt;a href=&quot;http://wenku.baidu.com/view/68f1853a580216fc700afd74.html&quot; rel=&quot;nofollow&quot;&gt;paper by Eric Man Wong&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;This paper provides a formula for calculating the average number of concurrent users as the number of users multiplied by their time on the website divided by length of day. For example, 100 users access a system for 30 minutes during a day of length 480 minutes will have an average of 6.250 concurrent users (100 * 30 / 480).&lt;/p&gt;&#10;&#10;&lt;p&gt;From this I use the POISSON.DIST function in Excel to calculate the probabilities of a range of concurrent users. So for 13 concurrent users I use POISSON.DIST(13, 6.250, false) which gives 0.688%.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is how do I interpret this result. Does it mean that within any one day we should expect to see 13 concurrent users for 0.688% of the time (480*0.00688 = 3.3 minutes) Or in any one day there is 0.688% chance of 13 concurrent users which means that we can expect to see 13 concurrent users once every 145 days (1/0.00688).&lt;/p&gt;&#10;&#10;&lt;p&gt;The table generated in Excel is below&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;Concurrent  Probability&#10;Users &#10; 0           0.19305% &#10; 1           1.20653%&#10; 2           3.77042%&#10; 3           7.85504%&#10; 4          12.27350%&#10; 5          15.34187%&#10; 6          15.98112%&#10; 7          14.26885%&#10; 8          11.14754%&#10; 9           7.74135%&#10;10           4.83834%&#10;11           2.74906%&#10;12           1.43180%&#10;13           0.68837%&#10;14           0.30731%&#10;15           0.12804%&#10;16           0.05002%&#10;17           0.01839%&#10;18           0.00638%&#10;19           0.00210%&#10;20           0.00066%&#10;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-22T22:23:45.867" Id="77404" LastActivityDate="2013-11-22T23:01:55.930" LastEditDate="2013-11-22T23:01:55.930" LastEditorUserId="35192" OwnerUserId="35192" PostTypeId="1" Score="0" Tags="&lt;poisson&gt;&lt;interpretation&gt;" Title="Interpreting Results of Poisson Distribution" ViewCount="163" />
  <row Body="&lt;p&gt;Feature normalization is to make different features in the same scale. The scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest(Without scaling, the cost function that is visualized will show a great asymmetry).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I think it makes sense that use the mean and var from training set when test data come. Yet if the data size is huge, both training and validation sets can be approximately viewed as normal distribution, thus they roughly share the mean and var.                                                                                                     &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-23T02:54:43.147" Id="77415" LastActivityDate="2013-11-23T02:54:43.147" OwnerUserId="35099" ParentId="77350" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;10) Which of the following distributions will have the smallest&#10;  standard deviation, assuming that none contain outliers?&lt;br&gt;&#10;  a) A uniform distribution of integers with a mean of 5 and a range of 10.&lt;br&gt;&#10;  b) A bell-shaped distribution of integers with a mean of 5 and a range of 10.&lt;br&gt;&#10;  c) A right-skewed distribution of integers with a mean of 4 and a range of 10.&lt;br&gt;&#10;  d) All three distributions have equal standard deviation.&lt;br&gt;&#10;  e) The answer cannot be determined from the information given.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The answer is given as (b) but I really can't figure out a simple reason why.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-11-23T14:32:11.310" Id="77444" LastActivityDate="2013-12-07T00:46:37.737" LastEditDate="2013-11-25T17:56:36.240" LastEditorUserId="7290" OwnerUserId="35215" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;standard-deviation&gt;" Title="Must the SD of a 'bell shaped' distribution be less than the SD of a skewed distribution with the same range?" ViewCount="215" />
  <row Body="Error Correction Model" CommentCount="0" CreationDate="2013-11-23T16:02:08.190" Id="77455" LastActivityDate="2013-11-23T16:02:08.190" LastEditDate="2013-11-23T16:02:08.190" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="4" Score="0" />
  
  <row Body="&lt;p&gt;Feature selection should be done before CV.  If you select features during CV, then they will change depending on the training data -- there are techniques that exploit this but at the beginner level, you should first consider selecting features before CV.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Splitting the data into a fixed portion for training and testing is also inefficient. &lt;/p&gt;&#10;&#10;&lt;p&gt;Instead do this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Select features that best predict class membership or best predict the function using the entire dataset.  Note, I always like to use a separate feature $filtering$ method to identify informative features prior to and separately from classification, in order to minimize selection bias.  Recurrent feature selection or $wrapping$ uses the classifier to select features -- and commonly has greater risk for selection bias, so filtering is better (less biased).  Separating the feature selection filtration from the classification step is very beneficial when generalizing results from future data not used for training/testing -- so I always keep the two ``far removed'' from one another.  (that is, I don't want the classifier to select any features).  Use, for example, statistical hypothesis tests (t-test, Mann-Whitney test, F-test, Kruskal-Wallis test), or information gain (entropy), or Gini index for feature filtration (selection). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Divide the objects uniformly into ten folds $\mathcal{D}_1, \mathcal{D}_1,\ldots,\mathcal{D}_{10}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;First, train with objects in the 9 folds $\mathcal{D}_2, \mathcal{D}_3,\ldots,\mathcal{D}_{10}$ and test objects with the trained system in fold $\mathcal{D}_1$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Next, train with objects in the 9 folds $\mathcal{D}_1, \mathcal{D}_3,\ldots,\mathcal{D}_{10}$ and test objects with the trained system in fold $\mathcal{D}_2$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Repeat the above up to a point where objects in fold $\mathcal{D}_{10}$ are tested and &#10;9 folds $\mathcal{D}_1, \mathcal{D}_2,\ldots,\mathcal{D}_{9}$ are used for training.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For each object in each test fold, increment the confusion matrix $\mathbf{C}$ (with dimensions $\Omega \times \Omega$) with a one in element $c_{\omega,\hat{\omega}}$, where $\omega$ is the true class of the object and $\hat{\omega}$ is the predicted class.  Thus &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;After each 10-fold CV, total accuracy for classification is the sum of the diagonal elements of $\mathbf{C}$, divided by the total number of objects, i.e., $Acc=\sum_\omega^\Omega c_{\omega\omega}/n$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the above methods are called a 10-fold CV.  You should next $repartition$ the objects into 10 folds again but this time after randomly shuffling (permuting) the order of all objects, then repeat the above 10-fold CV.  This will ensure that objects assigned to the folds are different.  Repartition ten times, each time performing a 10-fold CV, then calculate total accuracy.  This will then be called a ``ten 10-fold CV''.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you perform ten 10-fold CV, you can select features after, for example, mean-zero standardizing, normalizing into range [0,1], or fuzzifying.  The key point is that, on average, classification accuracy will change with the features used.  But first, get a handle on classification accuracy using the first group of features.  Then, if you want to select features a different way (maybe after transforming their values), then run a complete ten 10-fold CV every time you change the features.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For accuracy determination following ten 10-fold CV, use $Acc=\sum_\omega^\Omega c_{\omega\omega}/ \sum_\omega^\Omega \sum_\hat{\omega}^\Omega c_{\omega,\hat{\omega}}$, which is equal to the sum of the diagonal elements of $\mathbf{C}$ divided by the sum of all elements of $\mathbf{C}$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-23T20:32:08.410" Id="77475" LastActivityDate="2013-11-24T00:58:23.773" LastEditDate="2013-11-24T00:58:23.773" LastEditorUserId="32398" OwnerUserId="32398" ParentId="77471" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;It is not; the whole point of having a test set is that you don't use it to optimize anything, so that the accuracy on this set is a reliable estimate of real accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you need to extract some consensus subset of features over folds you either need some unsupervised method to do it (like a binomial test) or a one more layer of nested validation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-23T20:33:18.867" Id="77476" LastActivityDate="2013-11-23T20:33:18.867" OwnerUserId="88" ParentId="77471" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;genetic algorithms were used to lower the prime gap to 4680 in the recent Zhang twin primes proof breakthrough &amp;amp; associated polymath project. the bound has been lowered by other methods but it shows some potential for machine-learning approaches in this or related areas. they can be used to devise/optimize effective &quot;combs&quot; or basically sieves for analyzing/screening smallest-possible prime gaps.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.wired.com/wiredscience/2013/11/prime/2/&quot; rel=&quot;nofollow&quot;&gt;Sudden Progress on Prime Number Problem Has Mathematicians Buzzing&lt;/a&gt;&#10;BY ERICA KLARREICH, QUANTA MAGAZINE11.22.136:30 AM&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The team eventually came up with the Polymath project’s record-holder — a 632-tooth comb whose width is 4,680 — using a genetic algorithm that “mates” admissible combs with each other to produce new, potentially better combs.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2013-11-23T21:14:03.227" Id="77479" LastActivityDate="2013-11-23T21:14:03.227" OwnerUserId="17493" ParentId="73801" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have some timecourse data which plotted looks like the figure below.&#10;I want to better describe the difference between the two conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;My adviser advised :D me to use a linear model and observe a time effect (interesting!) a main condition effect (probably none) and a condition*time effect (most interesting).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I am wondering what this information would look like - the time course isn't gradual, and I have run similar tests with R's &lt;code&gt;lme4::lmer&lt;/code&gt; and &lt;code&gt;nlme:lme&lt;/code&gt; and for continuous variables (in my case time) they just output one value as if the progression is linear. Obviously in my case it is not :-/ &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CpQ5X.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have previously worked with such data and used PCA (which indeed, does not give me just-linear progressions) - would it be a good substitute for a linear model in my case? Also, the problem with PCA is that I have no idea how to reap the added information one would get from single participants (with mixed models, I can just add the participant IDs as random variables and feed it the raw data) - is there any way to pass the entire (un-meaned) per-participant data to, say Python's &lt;code&gt;mdp.pca()&lt;/code&gt; and get something reasonable out of that?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-23T23:46:22.240" Id="77486" LastActivityDate="2013-11-23T23:46:22.240" OwnerUserId="32504" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;pca&gt;&lt;lmer&gt;&lt;lme&gt;&lt;lme4&gt;" Title="Linear model or component analysis on timecourse data" ViewCount="92" />
  
  <row Body="&lt;p&gt;To expand on my comment:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I am aware that there is a matched-pairs t-test between two lists of counts. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Matched pairs of counts might be tested in other ways. Once issue with using a paired t-test on counts is that the assumption of equal variance for observations (and even of differences) will almost certainly be violated.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;However, I cannot find any matched-pairs t-test of proportions. For example, list a may be [1/2, 24/30, 5/8] and list b may be [51/100, 4/9, 4/8].&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, those are also counts, but the usual assumption would be that they're binomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the null is that within the pairs the proportions are equal (but not necessarily across pairs), then one approach would be to do a chi-square test. Each matched pair gives you a 2x2 table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         Table 1:&#10;        Gp1  Gp2    Total&#10;   S     1   51       52&#10;   F     1   49       50 &#10;&#10;Total    2  100      102&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-24T00:12:40.503" Id="77487" LastActivityDate="2013-11-24T00:12:40.503" OwnerUserId="805" ParentId="76199" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;MSE has several advantages over MAE, but also some disadvantages.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just list some of them, include but not limited to:&lt;/p&gt;&#10;&#10;&lt;p&gt;Decomposition of MSE into Variance and Bias square is one of the most famous advantages.&#10;This property helps us to understand the logic behind error, especially MSE, while MAE has no such mathematical meaning.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vw1W2.png&quot; alt=&quot;MSE decomposition&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;MAE with absolute value calculation is not differentiable globally, while MSE can.&#10;This make it convenient to act as loss function and help algorithm to find the gradient method for optimization.&lt;/p&gt;&#10;&#10;&lt;p&gt;But as you may think, MSE weights a lot for the outliers than MAE and sensitive to outlier. Model by minimizing MSE may be affected by outlier substantially.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-24T01:42:01.857" Id="77492" LastActivityDate="2013-11-24T01:42:01.857" OwnerUserId="29187" ParentId="77489" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I need to create a sample of a given size from a population. However, the population is dynamic, that is, comes as a stream of items, and every item has a &quot;time stamp&quot; based on its location in this stream. I don't know beforehand how long is this stream, but I need that in the chosen sample, the time stamps will look as a random sequence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Practically, I can't &quot;save&quot; the whole population, and then choose a sample. I must somehow retain a sample throughout the process, and end up with the required sample of the gien size.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an idea I had: Suppose you need to sample m items, and the unknown population size total is n.&#10;- choose a number k which is greater than m and smaller than n, and for which you can handle populations of size k&#10;- for the first k elements in the stream, create a random sample of m elements from it and retain it as the &quot;working sample&quot;&#10;- for the next k (that is the second chunk of k items), sample about m/2 elements from it, and replace randomly chosen m/2 elements from your working sample, wit the new sampled items. now you have an m sample from 2k items&#10;- so on and so forth... for the i'th chunk of k items from the stream, choose about m/i elements randomly and substitute them randomly into your working sample...&#10;- do it until the stream is over. your working sample is the result&lt;/p&gt;&#10;&#10;&lt;p&gt;does this algorithm create a good sample?&#10;are there better ways to do it?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-24T14:37:43.777" Id="77513" LastActivityDate="2013-12-25T11:04:14.397" LastEditDate="2013-11-24T15:41:28.740" LastEditorUserId="25986" OwnerUserId="25986" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;sampling&gt;&lt;population&gt;" Title="Sampling from a dynamic population" ViewCount="74" />
  
  
  <row AcceptedAnswerId="77578" AnswerCount="2" Body="&lt;p&gt;I'm trying to understand how multiple regression statistically controls for the effects of other predictor variables when calculating partial regression slopes. In a multiple regression of Y~X1+X2, would the partial regression slope of X1 be given by [Y]~[residuals of X1~X2], or by [residuals of Y~X2] ~ [residuals of X1~X2]? Different pages on the internet tell me different things.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've done some simulations to try and figure this out (see below), and it seems that both methods give the same estimates of slopes as multiple regression, but only the latter method has similar standard errors around those estimates. This makes me think the latter method is the one that multiple regression uses, but it would be nice to know for sure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, if I wanted to plot Y against X1 so that I could visualise how strongly the two were related while also controlling for any confounding with X2, would I plot [Y]~[residuals of X1~X2], or [residuals of Y~X2] ~ [residuals of X1~X2]? These two plots in the code below look very different in terms of the strength of the relationship.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help, &lt;/p&gt;&#10;&#10;&lt;p&gt;Jay&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#1. simulate data, where x1 and x2 are correlated due to lurking variable, &#10;#...and y is explained by both.&#10;lurker &amp;lt;- rnorm(n=100)&#10;x1 &amp;lt;- rnorm(n=100, mean=lurker*2, sd=1)&#10;x2 &amp;lt;- rnorm(n=100, mean=lurker*5, sd=1)&#10;y &amp;lt;- rnorm(n=100, mean=x1*2 + x2*5, sd=1)&#10;&#10;#2. multiple regn model to estimate partial slopes:&#10;summary(lm(y~x1+x2))      #partial slopes pretty close to simulated values&#10;&#10;#3. calculate partial slopes manually, using either &#10;#....(1) Y~[resids of X1~X2] OR (2) [resids of Y~X2]~[resids of X1~X2]&#10;&#10;  #3.a. based on (1) Y~[resids of X1~X2]&#10;  m.x1x2 &amp;lt;- lm(x1~x2)&#10;  resids.x1x2 &amp;lt;- m.x1x2$residuals&#10;  summary(lm(y ~resids.x1x2))    #slope pretty close to true value, but conf intervals MUCH larger than those for MR estimate&#10;&#10;  #3.b. based on (2) [resids of Y ~ X2]~[resids of X1~X2]&#10;  m.y1x2 &amp;lt;- lm(y~x2)&#10;  resids.y1x2 &amp;lt;- m.y1x2$residuals&#10;  summary(lm(resids.y1x2 ~resids.x1x2))  #also very close to true value, but conf intervals now similar scale to those from MR.&#10;&#10;  # plot the relationship between y and x1 after controlling for x2, based on the different methods:&#10;    op &amp;lt;- par(mfrow=c(2,1))&#10;    plot(y ~resids.x1x2)&#10;    plot(resids.y1x2 ~resids.x1x2)&#10;    par(op)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-24T23:57:49.903" FavoriteCount="1" Id="77557" LastActivityDate="2013-11-25T05:33:33.300" OwnerUserId="15949" PostTypeId="1" Score="3" Tags="&lt;multiple-regression&gt;&lt;regression-coefficients&gt;&lt;statistical-control&gt;&lt;partial&gt;" Title="How are partial regression slopes calculated in multiple regression?" ViewCount="312" />
  <row Body="&lt;p&gt;P-values are very specialized probabilities, and my first feeling is that they're not directly helpful. Bayes' rule, on the other hand, is. Let's refer to your two distributions as Class 1 and Class 2, respectively. Let $f_1(y)$ and $f_2(y)$ be the respective normal densities. Then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \Pr(\text{Class 1} | y) = \frac{f(y , \text{Class 1})}{f(y)}  = \frac{\Pr(\text{Class 1}) \cdot f_1(y)}{f(y)}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;By the law of total probability, we can expand $$f(y) = \Pr(\text{Class 1})\cdot f_1(y) + \Pr(\text{Class 2})\cdot f_2(y).$$ You could make the class probabilities unknowns themselves to be estimated, or just plug in .5 to reflect your uncertainty. That would give you $$ \Pr(\text{Class 1} | y) = \frac{f_1(y)}{f_1(y) + f_2(y)}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-25T00:15:09.090" Id="77559" LastActivityDate="2013-11-25T17:27:47.660" LastEditDate="2013-11-25T17:27:47.660" LastEditorUserId="35131" OwnerUserId="35131" ParentId="77556" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="77616" AnswerCount="3" Body="&lt;p&gt;We have data with a binary outcome and some covariates. I used logistic regression to model the data. Just a simple analysis, nothing extraordinary. The final output is supposed to be a dose-response curve where we show how the probability changes for a specific covariate. Something like this: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0I8uo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We received some criticism from an internal reviewer (not a pure statistician) for choosing logistic regression. Logistic regression assumes (or defines) that the inflection point of the S-shaped curve on the probability scale is at probability 0.5. He argued that there would be no reason to assume that the inflection point was indeed at probability 0.5 and we should choose a different regression model that allows the inflection point to vary such that the actual position is data driven.&lt;/p&gt;&#10;&#10;&lt;p&gt;At first I was caught off guard by his argument, since I have never thought about this point. I did not have any arguments to why it would be justified to assume that the inflection point is at 0.5. After doing some research, I still don't have an answer to this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;I came across 5-parameter logistic regression, for which the inflection point is an additional parameter, but it seems that this regression model is usually used when producing dose-response curves with a continuous outcome. I'm not sure if and how it can be extended to binary response variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess my main question is why or when it is OK to assume that the inflection point for a logistic regression is at 0.5? Does it even matter? I have never seen anybody fitting a logistic regression model and explicitly discussing the matter of the inflection point. Are there alternatives for creating a dose response curve where the inflection point is not necessarily at 0.5?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just for completeness, the R code for generating the above picture:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat &amp;lt;- read.csv(&quot;http://www.ats.ucla.edu/stat/data/binary.csv&quot;)&#10;dat$rank &amp;lt;- factor(dat$rank)&#10;logit &amp;lt;- glm(admit ~ gre + gpa + rank, family = binomial(link = &quot;logit&quot;), data = dat)&#10;newdata &amp;lt;- data.frame(gre = seq(-2000,8000,1), gpa = 2.5, rank = factor(1,c(1,2,3,4)))&#10;pp &amp;lt;- predict(logit, newdata, type = &quot;response&quot;, se.fit = TRUE)&#10;plot(newdata$gre, pp$fit, type=&quot;l&quot;, col=&quot;black&quot;, lwd=2,ylab=&quot;Probability&quot;, xlab=&quot;Dose&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to add to what Scortchi said in one of the comments: The reviewer did indeed argue that biologically it might be more likely that the change in curvature occurs earlier than 0.5. Therefore his resistance against assuming that the inflection point is at 0.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As a reaction to the comment by Frank Harrell:&lt;/p&gt;&#10;&#10;&lt;p&gt;As example, I modified my model above to include a quadratic and a cubic term in &lt;code&gt;gre&lt;/code&gt; (which is the &quot;dose&quot; in this example). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logit &amp;lt;- glm(admit ~ gre+I(gre^2)+I(gre^3)+  gpa + rank, family = binomial(link = &quot;logit&quot;), data = dat)&#10;newdata &amp;lt;- data.frame(admit=1, gre = seq(-2000,8000,1), gpa = 2.5, rank = factor(1,c(1,2,3,4)))&#10;pp &amp;lt;- predict(logit, newdata, type = &quot;response&quot;, se.fit = TRUE)&#10;plot(newdata$gre, pp$fit, type=&quot;l&quot;, col=&quot;black&quot;, lwd=2,xlim=c(-2000,4000),ylab=&quot;Probability&quot;, xlab=&quot;Dose&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0JpC3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Despite the fact that it is probably not meaningful to add a quadratic and a cubic &lt;code&gt;gre&lt;/code&gt; term in this case, we see that the form of the dose-response curve has changed. Indeed we now have two inflection points at about 0.25 and near 0.7.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-25T12:32:00.290" FavoriteCount="2" Id="77609" LastActivityDate="2013-11-27T17:21:50.737" LastEditDate="2013-11-26T11:05:13.830" LastEditorUserId="17230" OwnerUserId="35286" PostTypeId="1" Score="10" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;&lt;binary-data&gt;" Title="Logistic Regression and Inflection Point" ViewCount="1212" />
  <row Body="&lt;p&gt;I would recommend P.J. Green and B.W. Silverman - &quot;Nonparametric regression and generalised linear models - a roughness penalty approach&quot;, CHapman &amp;amp; Hall/CRC Monographs on statistics and applied probability, volume 58.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-11-25T13:38:23.607" CreationDate="2013-11-25T13:24:23.963" Id="77614" LastActivityDate="2013-11-25T13:24:23.963" OwnerUserId="887" ParentId="77612" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;The author of a famous online course takes &lt;a href=&quot;https://www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing-two-samples/v/comparing-population-proportions-2&quot; rel=&quot;nofollow&quot;&gt;a problem&lt;/a&gt; where 642 of 1000 polled men and 591 of 1000 polled women responded &lt;em&gt;yes&lt;/em&gt;. The question is if men are more likely to agree or not, using 95% confidence interval. He starts by computing the difference of the means, $\bar p_1 - \bar p_2 = .642 -. 591 = 0.051$ and its standard deviation (i.e. the standard error of the .051), which is a sum of variances $\sigma = \sigma(\bar p_1) + \sigma(\bar p_2) = \sqrt{1/1000 (p_1(1-p_1) + p_2(1-p_2))} = 0.021715$. This difference of means is normally distributed and, according to z-table, 1.96 standard deviations contain 95% of such mean differences. We thus can say that the difference is 0.051 ± 1.96 * 0.021715 = 0.051 ± .042562 or there is 95% confidence that $p_1-p_2$ lies between .008 and .094. Since zero is excluded from the region, the 95% confidence interval is purely positive and we are sure that men are more likely to say &lt;em&gt;yes&lt;/em&gt; than women. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that after this conclusion, author conducts &lt;a href=&quot;https://www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing-two-samples/v/hypothesis-test-comparing-population-proportions&quot; rel=&quot;nofollow&quot;&gt;something similar yet different: he tests the hypothesis that $\bar p_1 = \bar p_2$&lt;/a&gt;! That is, he takes the population average, $p = (\bar p_1 + \bar p_2)/2$, computes its error by summing variances for men and women. BTW, I wonder &lt;a href=&quot;http://stats.stackexchange.com/questions/77654&quot;&gt;why claiming that there are 2000 individuals averaged he uses formula $\sqrt{2p(1-p) \over 1000} = 0.0217$ instead of $\sqrt{p(1-p) \over 2000}$&lt;/a&gt;? The difference is 2x. Anyway, he takes the z-score = $0.051/0.0217=2.35$ deviations afterwards, which is outside the 95% confidence interval (of 1.96 deviations) and, therefore, it is unlikely that means, $\bar p_1$ and $\bar p_2$, are equal. &lt;/p&gt;&#10;&#10;&lt;p&gt;I do not understand: what is the difference between the methods and which of the tests should I use? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-25T15:12:41.400" Id="77629" LastActivityDate="2014-04-10T21:20:29.687" LastEditDate="2014-04-10T20:49:35.357" LastEditorUserId="805" OwnerUserId="26140" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;standard-error&gt;&lt;group-differences&gt;" Title="Comparing population proportions vs hypothesis testing" ViewCount="119" />
  
  <row Body="&lt;pre&gt;&lt;code&gt;Day(t)  p(t)  p(t-1)&#10;2       2.1   1.2&#10;3       4     2.1&#10;4       3.2   4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;$$\text{Cov}(p_t,p_{t-1}) = \text{E}[(p_t - \text{E}[p_t]) \times (p_{t-1} - \text{E}[p_{t-1}])]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Go to: Data ---&gt; Data Analysis ---&gt; Covariance&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bQmiA.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Cov}(p_t,p_{t-1}) = 0.3633$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-25T15:32:00.003" Id="77632" LastActivityDate="2013-11-25T15:32:00.003" OwnerUserId="22468" ParentId="76234" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a response variable of percent cover of vegetation in a quadrat.  I have tried to arcsine square this data as recommended in the Crawley R book but I am not getting good fit. The data is zero-inflated with about half the data points being zero.  Can someone point me in the right direction for how to approach building this model?  I am striking out on Google.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-25T17:14:03.653" Id="77642" LastActivityDate="2014-03-20T09:34:59.997" LastEditDate="2013-11-25T22:18:23.923" LastEditorUserId="88" OwnerUserId="35305" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;quantiles&gt;&lt;zero-inflation&gt;" Title="Modelling zero-inflated percentage data in R" ViewCount="314" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have attempted to view the following experiment as a split plot design:&lt;/p&gt;&#10;&#10;&lt;p&gt;15 undergraduates serve as subjects for an experiment that compares two hypnosis methods. There are two phases to the experiment. In phase one, all subjects take an exam to recall 16 word-number pairs in a normal waking state. The experimenters record how many errors they make in reciting the lists. Their errors are recorded until they either go through the entire list without making an error or if they use up a maximum of 15 attempts. &lt;/p&gt;&#10;&#10;&lt;p&gt;In phase two, the subjects are randomly assigned one of the three conditions to memorize a new 16 word-pair list of equal difficulty: normal waking, hypnosis, or alternative hypnosis state. Next, they are given the second test and their errors are recorded in the same vein as the first test. &lt;/p&gt;&#10;&#10;&lt;p&gt;My argument is that it it's a split plot because the first test serves as a benchmark. The blocks are created by randomly assigning the between-block units, the subjects, a state (between-block factor). Next, all groups, the within-block unit, receive the within-block factor, the second test. &lt;/p&gt;&#10;&#10;&lt;p&gt;The response in this experiment is the # of errors made on the second test. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this reasoning make sense? Is it ok to view the first assessment as a benchmark? Please do not tell me the answer but do tell where my reasoning may have gone astray if that is the case? &lt;/p&gt;&#10;&#10;&lt;p&gt;My other inkling is that if I ignore, the first test, then this is just a completely randomized experiment where the units change to the three groups of five if this were to be balanced. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-25T20:32:36.863" FavoriteCount="1" Id="77666" LastActivityDate="2013-11-25T22:14:19.323" LastEditDate="2013-11-25T22:14:19.323" LastEditorUserId="88" OwnerUserId="35313" PostTypeId="1" Score="1" Tags="&lt;experiment-design&gt;&lt;split-plot&gt;" Title="Is this a split plot?" ViewCount="62" />
  <row Body="&lt;p&gt;$$E(X_{n+1}\mid \mathcal F_{n}) = E(\eta_{n+1}+X_{n}\mid \mathcal F_{n})$$&#10;$$=E(\eta_{n+1}\mid \mathcal F_{n}) +E(X_{n}\mid \mathcal F_{n}) =E(\eta_{n+1}) + X_n$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now look up the definitions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Analogously for $Y_n$ - if you understand its subscript (I don't).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-25T22:51:25.967" Id="77678" LastActivityDate="2013-11-25T22:51:25.967" OwnerUserId="28746" ParentId="77647" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="77685" AnswerCount="1" Body="&lt;p&gt;I've seen this referred to in relation to model selection but haven't come across a definitive description of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What the flat maximum effect?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-26T00:24:29.537" Id="77682" LastActivityDate="2013-11-26T00:53:06.023" OwnerUserId="29070" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;model-selection&gt;&lt;linear-model&gt;" Title="Flat maximum effect?" ViewCount="72" />
  <row AnswerCount="1" Body="&lt;p&gt;Let's say I have a sensor that measures pressure in a range from 10 - 60 mmHg.&lt;/p&gt;&#10;&#10;&lt;p&gt;This sensor has an error of +/- 0.8 mmHg.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to quantify how large this error is with respect to the range of values the sensor can sense? I want to figure out what the relative size of this error is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-26T04:03:04.517" Id="77698" LastActivityDate="2015-03-04T05:49:49.953" LastEditDate="2013-11-26T10:19:57.383" LastEditorUserId="88" OwnerUserId="35327" PostTypeId="1" Score="1" Tags="&lt;error&gt;" Title="How to calculate relative error?" ViewCount="121" />
  <row Body="&lt;p&gt;Under the assumption that your data are continuous, you can use an &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot; rel=&quot;nofollow&quot;&gt;ANOVA&lt;/a&gt; for this situation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-26T04:45:21.470" Id="77703" LastActivityDate="2013-11-26T04:45:21.470" OwnerUserId="7290" ParentId="77700" PostTypeId="2" Score="2" />
  
  
  
  
  <row AcceptedAnswerId="77741" AnswerCount="2" Body="&lt;p&gt;I use simple regressions. My independent variable is a count data (# of a drug used per year), and it has too many zeroes. Depending upon a dependent variable, for some simple regressions, I have 13 observations, and there are only 3 non-zero values out of 13. For other simple regressions, I have 150 observations and only 12 are non-zeros out of 150. Is it appropriate to use simple regressions for these data? Is there a rule about how many zeros should be in independent variable to be able to model the relationship? My dependent variable is continuous (weight). &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-26T15:19:23.797" Id="77735" LastActivityDate="2013-11-26T16:03:52.290" LastEditDate="2013-11-26T15:57:34.947" LastEditorUserId="7290" OwnerUserId="35343" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="How many zeros in an independent variable are too many for regression?" ViewCount="967" />
  <row Body="&lt;p&gt;Seems to me this is a trick question.&lt;/p&gt;&#10;&#10;&lt;p&gt;If he keeps at it, and the probability is less than 1, his winnings are zero. He will eventually lose it all.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-11-26T16:03:57.267" Id="77742" LastActivityDate="2013-11-26T16:03:57.267" OwnerUserId="35346" ParentId="77514" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to understand zero-inflated negative binomial regression.&#10;My impression is that if a zero-inflated negative binomial model does not contain any logit part, the model is identical to the one can obtain with just ordinary negative binomial regression. Is this correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: the logit part I was talking about - well - zero-inflated model assumes that the 0s within the dataset are generated based on two different process: one is negative binomial and the other is, if I remember it correctly, poisson. By &quot;no logit part&quot; I meant what if we take out the effect of the poisson distribution from the zero-inflated model? would it be same as ordinary negative binomial regression?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-26T16:34:09.183" Id="77745" LastActivityDate="2013-11-27T01:52:12.373" LastEditDate="2013-11-27T01:52:12.373" LastEditorUserId="35176" OwnerUserId="35176" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;nonlinear-regression&gt;&lt;zero-inflation&gt;" Title="Zero-inflated negative binomial" ViewCount="107" />
  
  <row AcceptedAnswerId="77781" AnswerCount="1" Body="&lt;p&gt;I have data on about 20000 consumers who were exposed to some form of advertising. The data is in the following form.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Cookie_Id   Observation_Number  Ad_Id   Ad_Id_Lookup    Placement_Id    Placement_Category  Placement_Cpi   Cookie_Lookup&#10;2   1   325 Standard    3722    News    20  0&#10;3   1   325 Standard    3722    News    20  0&#10;4   1   325 Standard    3719    Weather 8   2&#10;4   2   325 Standard    3719    Weather 8   2&#10;5   1   324 Standard    3718    Weather 8   0&#10;5   2   324 Standard    3718    Weather 8   0&#10;6   1   327 Rich-Media  3716    Travel  20  0&#10;6   2   327 Rich-Media  3716    Travel  20  0&#10;6   3   327 Rich-Media  3716    Travel  20  0&#10;6   4   327 Rich-Media  3716    Travel  20  0&#10;7   1   324 Standard    3718    Weather 8   1&#10;7   2   324 Standard    3718    Weather 8   1&#10;8   1   323 Standard    3717    Weather 8   0&#10;8   2   323 Standard    3717    Weather 8   0&#10;9   1   325 Standard    3719    Weather 8   0&#10;9   2   325 Standard    3719    Weather 8   0&#10;11  1   324 Standard    3713    Travel  12  0&#10;11  2   324 Standard    3713    Travel  12  0&#10;11  3   324 Standard    3713    Travel  12  0&#10;11  4   324 Standard    3713    Travel  12  0&#10;12  1   324 Standard    3713    Travel  12  0&#10;12  2   324 Standard    3713    Travel  12  0&#10;12  3   324 Standard    3713    Travel  12  0&#10;12  4   324 Standard    3713    Travel  12  0&#10;13  1   327 Rich-Media  3723    News    28  0&#10;14  1   325 Standard    3722    News    20  0&#10;15  1   325 Standard    3722    News    20  0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm looking to model the data using a linear mixed model, with Ad_Id_Lookup and Placement_Category as input variables and Cookie_Lookup as my output variable (the states 0,1 and 2 correspond to different outcomes, such as whether someone made a purchase).&lt;/p&gt;&#10;&#10;&lt;p&gt;The trouble is that many of the rows are identical to each other, other than the observation number (this ordering is a bit artificial because I don't have time stamps). I want to treat each ad exposure as a new treatment on each individual cookie ID.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I do this using the nlme package in R? If not, is there another package that can deal with data of this type?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-26T20:49:23.347" Id="77773" LastActivityDate="2013-11-26T23:25:48.423" OwnerUserId="7883" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;mixed-effect&gt;&lt;nlme&gt;" Title="How to fit a mixed model to this advertising data?" ViewCount="94" />
  <row AnswerCount="1" Body="&lt;p&gt;I believe one major advantage of Bayesian inference is the intuitiveness of interpretation. This is my primary interest. However, it's not completely clear to me when it's OK to make such an interpretation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I make the potentially false assumption that fitting a probability model in the frequentist way is virtually the same as fitting the same model with a flat prior in a Bayesian way. Please nuance or correct that as interest number one (1).&lt;/p&gt;&#10;&#10;&lt;p&gt;And my main interest (2) is if my assumption is true (and if a flat prior happens to be the best prior I could possibly come up with), does the posterior of the model fitted in a frequentist way, but sampled say with an MCMC sampler, allow for me to make a Bayesian type of interpretation? e.g., the probability that an individual described by some particular configuration of $X$ will have an income ($Y$) greater than $100K is 76%.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've read that what makes an analysis Bayesian is that it involves prior information. Is that really the essence? I've also read that you can't make such interpretations as my example about income from frequentist results. Does sampling from the posterior with MCMC methods move me away from frequentist methods sufficiently to make a Bayesian interpretation?&lt;/p&gt;&#10;&#10;&lt;p&gt;I greatly appreciate your direction. Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-26T21:47:16.197" Id="77774" LastActivityDate="2013-11-26T23:59:58.730" LastEditDate="2013-11-26T21:56:38.427" LastEditorUserId="24000" OwnerUserId="24000" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;interpretation&gt;" Title="When may I make a Bayesian interpretation of the posterior?" ViewCount="66" />
  <row AcceptedAnswerId="77842" AnswerCount="1" Body="&lt;p&gt;I'm learning clustering analysis and one book I read says the clustering model should be applied to a disjoint data set to examine the consistency of the model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think in clustering analysis we don't need to split the data into train and test sets like in supervised learning since without labels there is nothing to &quot;train&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;So what is the possible meaning of this &quot;consistency&quot;? How is it evaluated? Is this disjoint data set really necessary?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: There isn't really a broader context. The text talks about how to select optimal number of clusters and then mentions this. I don't think this consistency is about the number of clusters...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-26T21:48:07.627" Id="77775" LastActivityDate="2013-11-27T12:10:31.177" LastEditDate="2013-11-27T00:18:45.170" LastEditorUserId="23915" OwnerUserId="23915" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;unsupervised-learning&gt;" Title="Is it necessary to split data in clustering like in supervised learning?" ViewCount="68" />
  <row Body="&lt;p&gt;(1) your constant = 2480! Where does that fit on the graph?&lt;br&gt;&#10;(2) Graph doesn't have axis labels /legend. Looks like fitted data.&lt;br&gt;&#10;(3) is time a linear term in this model? why?  Have you tried fitting quadratic terms, cubic terms, quartic terms...&lt;br&gt;&#10;- unless the change in scale is distorting the graph, the time trend is clearly non-linear and using a simple linear term lacks face validity. I'd build a main effects model first (time time2, time time2 time3 etc. where time2=time^2 and time3=time^3), find which one fitted best and then consider adding interaction terms. Your categorical would need to interact with each time term if you wanted to add an interaction. I would also test to see if you need the interactions&lt;br&gt;&#10;- how you represent that in R is not clear to me (I usually use Stata)&lt;br&gt;&#10;- There are other ways of handling non-linearity from piecewise models, to I think there is a mixed-effects GAM package. But this is a solid and traditional approach.&#10;(4) have you considered time as a random slope?&lt;br&gt;&#10;(5) have you done residual diagnostics?&lt;br&gt;&#10;- simplest would be to generate histograms of your level-1 and level-2 standardized residuals. but I think the linear time assumption is the key.&lt;br&gt;&#10;(6) have you plotted predicted vs. actual?&lt;br&gt;&#10;- from the model you should be able to predict a mean/model averaged trajectory over time. If you do this with the above model it should be clear that it doesn't fit the data at all. Or show that it sort of does fit the data.      &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think you're at the &quot;what does this model tell me&quot; stage yet. It's always hard to tell from just a snapshot like this, sorry if I'm way off base and wasting your time, but does seem like you still have a lot of model refinement to do.            &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-11-27T04:30:50.980" Id="77803" LastActivityDate="2013-11-27T17:30:41.797" LastEditDate="2013-11-27T17:30:41.797" LastEditorUserId="34658" OwnerUserId="34658" ParentId="77797" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let $\varepsilon$ be a Gaussian distributed random variable with mean $\mu_0$ and standard deviation $\sigma_0$. Is it possible to compute/approximate the expected value&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{eqnarray}&#10;&amp;amp; &amp;amp;\mathbb{E}\left[\Phi\left(\frac{\varepsilon-\mu_1}{\sigma_1}\right)\mid\varepsilon&amp;lt;c\right]=...\\&#10;&amp;amp; &amp;amp;... =\int_{-\infty}^{c}d\varepsilon \frac{1}{\sqrt{2\,\pi\,\sigma_0^2}}\,\exp\left(-\frac{\left(\varepsilon-\mu_0\right)^2}{2\,\sigma_0^2}\right)\,\int_{-\infty}^{\frac{\varepsilon-\mu_1}{\sigma_1}}dt\,\frac{1}{\sqrt{2\,\pi}}\,\exp\left(-\frac{t^2}{2}\right)  ?&#10;\end{eqnarray}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that while the integral &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int \exp\left(-x^2\right)\,\textrm{erf}\left(x\right)\,dx$$&#10;exists (check on &lt;a href=&quot;http://integrals.wolfram.com/index.jsp&quot; rel=&quot;nofollow&quot;&gt;http://integrals.wolfram.com/index.jsp&lt;/a&gt;), any integral of the kind&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\int \exp\left(-(x-a)^2/b\right)\,\textrm{erf}\left((x-c)/d\right)\,dx&#10;$$&#10;does not exist (at least according to Wolfram integrals).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-27T11:04:34.783" Id="77833" LastActivityDate="2013-11-27T11:15:18.510" LastEditDate="2013-11-27T11:15:18.510" LastEditorUserId="35351" OwnerUserId="35351" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;expected-value&gt;" Title="Expected Value of cumulative distribution function" ViewCount="545" />
  
  <row AcceptedAnswerId="77863" AnswerCount="2" Body="&lt;p&gt;I am from an economics background and usually in the discipline the summary statistics of the variables are reported in a table. However, I wish to plot them. I could modify a box plot to allow it to display the mean,sd and min and maximum but I don't wish to do so as box plots are traditionally used to display medians and Q1 and Q3. All my variables have different scales. It would be great if somebody could suggest a meaningful way by which I could plot these summary statistics? I can work with R or Stata&lt;/p&gt;&#10;&#10;&lt;p&gt;Many Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-27T11:30:45.167" FavoriteCount="2" Id="77836" LastActivityDate="2013-11-28T05:36:43.333" LastEditDate="2013-11-28T05:36:43.333" LastEditorUserId="24163" OwnerUserId="24163" PostTypeId="1" Score="5" Tags="&lt;r&gt;" Title="Plotting summary statistics with mean, sd, min and max?" ViewCount="1786" />
  
  
  <row Body="&lt;p&gt;There are many things you could do. For example, you could look at the median rating of each attribute, with the idea that higher median ratings would be more important in general.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also look at the proportion of raters of each attribute who give it a high rating, on the theory that the things that matter in choosing a store are the items that the person things are important.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could do cluster analysis on the ratings, with the idea that perhaps clusters of people see sets of factors as important.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could do multidimensional scaling on similarities of ratings, to get at latent factors in the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you also have data on which stores the respondents &lt;em&gt;actually&lt;/em&gt; shop in, that leads to other possible analyses. If you have data on how much people &lt;em&gt;spend&lt;/em&gt; then, again, there are more possible analyses. &lt;/p&gt;&#10;&#10;&lt;p&gt;(That's just off the top of my head). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-27T12:06:46.407" Id="77841" LastActivityDate="2013-11-27T12:06:46.407" OwnerUserId="686" ParentId="77839" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Train-Test split is used to avoid overfitting in machine learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;In unsupervised clustering, you &lt;strong&gt;cannot&lt;/strong&gt; evaluate, and thus you cannot overfit in this way.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can however overfit in different ways, by choosing e.g. an unsupervised evaluation criterion that measures a quantity that your clustering procedue &lt;em&gt;also&lt;/em&gt; uses. You don't get the best result, but prefer the algorithm that is most related to your evaluation procedure.&#10;Don't use these measures to compare different algorithms.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-27T12:10:31.177" Id="77842" LastActivityDate="2013-11-27T12:10:31.177" OwnerUserId="7828" ParentId="77775" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I am not a LASSO expert but I am an expert in time series. If you have time series data or spatial data then I would studiously avoid a solution that was predicated on independent observations. Furthermore if there are unknown deterministic effects that have played havoc with your data (level shifts / time trends etc) then LASSO would be even less a good hammer. In closing when you have time series data you often need to segment the data when faced with parameters or error variance that change over time. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-27T12:14:31.353" Id="77843" LastActivityDate="2013-12-03T20:27:41.703" LastEditDate="2013-12-03T20:27:41.703" LastEditorUserId="7290" OwnerUserId="3382" ParentId="77834" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row AcceptedAnswerId="77972" AnswerCount="3" Body="&lt;p&gt;I ran a repeated design whereby I tested 30 males and 30 females across three different tasks. I want to understand how the behaviour of males and females is different and how that depends on the task. I used both the lmer and lme4 package to investigate this, however, I am stuck with trying to check assumptions for either method. The code I run is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm.full &amp;lt;- lmer(behaviour ~ task*sex + (1|rat/task), REML=FALSE, data=dat)&#10;lm.full2 &amp;lt;-lme(behaviour ~ task*sex, random = ~ 1|rat/task), method=&quot;ML&quot;, data=dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I checked if the interaction was the best model by comparing it with the simpler model without the interaction and running an anova:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm.base1 &amp;lt;- lmer(behaviour ~ task+sex+(1|rat/task), REML=FALSE, data=dat)&#10;lm.base2 &amp;lt;- lme(behaviour ~ task+sex, random= ~1|rat/task), method=&quot;ML&quot;, data=dat)&#10;anova(lm.base1, lm.full)&#10;anova(lm.base2, lm.full2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Q1: Is it ok to use these categorical predictors in a linear mixed model?&lt;br/&gt;&#10;Q2: Do I understand correctly it is fine the outcome variable (&quot;behaviour&quot;) does not need to be normally distributed itself (across sex/tasks)?&lt;br/&gt;&#10;Q3: How can I check homogeneity of variance? For a simple linear model I use &lt;code&gt;plot(LM$fitted.values,rstandard(LM))&lt;/code&gt;. Is using &lt;code&gt;plot(reside(lm.base1))&lt;/code&gt; sufficient?&lt;br/&gt;&#10;Q4: To check for normality is using the following code ok?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hist((resid(lm.base1) - mean(resid(lm.base1))) / sd(resid(lm.base1)), freq = FALSE); curve(dnorm, add = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-11-27T20:21:50.110" FavoriteCount="4" Id="77891" LastActivityDate="2013-11-28T18:00:36.077" LastEditDate="2013-11-28T15:35:17.517" LastEditorUserId="88" OwnerUserId="20112" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;assumptions&gt;&lt;lme&gt;" Title="Checking assumptions lmer/lme mixed models in R" ViewCount="5348" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working on some medical research. Given 3 values (BMI, age, gender) I have a list of baseline risks for the probability of an adverse event occurring in the next 10 years for an individual with a given BMI, age, gender.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also have a list of additional risk factors, given as Hazard Ratios. How can I properly compute the 10-year probability of an adverse event, by modifying the baseline risk if say, 1 or more risk factors apply to an individual.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, for example: given that an individual with age 30, gender Male, BMI 25 has a 5% risk of adverse event in next 10-years, assume some additional risk factors X and Y, which have respective HRs of 1.3 and 1.6, how can I modify the 5% risk to encompass the risk factors? I know that the HRs were computed using multivariable Cox proportional hazards for a 10 year period, with death considered as a competing hazard.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any additional statistical wisdom would be appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-27T21:40:06.940" Id="77900" LastActivityDate="2013-11-29T10:41:02.503" OwnerUserId="35411" PostTypeId="1" Score="3" Tags="&lt;probability&gt;" Title="Calculating 10 year probability using hazard ratios?" ViewCount="120" />
  <row Body="&lt;p&gt;By default, 'minsplit' is 20 and determines the minimal number of observations per leaf ('minbucket') as a third of 'minsplit' (see R-help). So in the first plot, since the minimal leaf size is $20/3 \approx 7$, the five very large 'manuf' values are not allowed to be separated from the smaller values. Set 'minbucket' to 3 in both versions would show the expected similarities. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that 'significant' has a quite different meaning in statistics than 'important'.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-27T21:55:35.590" Id="77903" LastActivityDate="2013-11-27T21:55:35.590" OwnerUserId="30351" ParentId="77899" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Q1: Yes, why not?&lt;/p&gt;&#10;&#10;&lt;p&gt;Q2: I think the requirement is that the errors are normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q3: Can be tested with Leven's test for example.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-28T02:35:18.807" Id="77913" LastActivityDate="2013-11-28T02:35:18.807" OwnerUserId="12719" ParentId="77891" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The description sounds to me as if it makes a MostPopular recommendation. But the MostPopular recommendation I did myself got much better results. So what does this recommender really return?&lt;/p&gt;&#10;&#10;&lt;p&gt;It is a Boolean data set. And I just count the occurrences of an item and sort them with the counts.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-28T02:38:53.440" Id="77914" LastActivityDate="2013-12-10T07:54:56.980" LastEditDate="2013-11-28T23:42:31.473" LastEditorUserId="22047" OwnerUserId="35418" PostTypeId="1" Score="-1" Tags="&lt;recommender-system&gt;" Title="What does ItemAverage Recommender do in Mahout" ViewCount="97" />
  <row Body="&lt;p&gt;There are myriad possibilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/F9u3S.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;... which plots the minimum, maximum, mean and mean $\pm$ sd for each sample using different symbols and then draws a rectangle, or perhaps better, something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/eG05s.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;... which plots the minimum, maximum, mean and mean $\pm$ sd for each sample using different symbols and then draws a line (in fact at present that's actually a rectangle as before, but drawn narrow; it should be changed to drawing a line)&lt;/p&gt;&#10;&#10;&lt;p&gt;If your numbers are on very different scales, but are all positive, you might consider working with logs, or you might do small multiples with different (but clearly marked) scales&lt;/p&gt;&#10;&#10;&lt;p&gt;Code ( presently not particularly 'nice' code, but at the moment this is just exploring ideas, it's not a tutorial on writing good R code):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fivenum.ms=function(x) {r=range(x);m=mean(x);s=sd(x);c(r[1],m-s,m,m+s,r[2])}&#10;eps=.015&#10;&#10;plot(factor(c(1,2)),range(c(A,B)),type=&quot;n&quot;,border=0)&#10;points((rep(c(1,2),each=5)),c(fivenum.ms(A),fivenum.ms(B)),col=rep(c(2,4),each=5),pch=rep(c(1,16,9,16,1),2),ylim=c(range(A,B)),cex=1.2,lwd=2,xlim=c(0.5,2.5),ylab=&quot;&quot;,xlab=&quot;&quot;)&#10;rect(1-1.2*eps,fivenum.ms(A)[2],1+1.4*eps,fivenum.ms(A)[4],lwd=2,col=2,den=0)&#10;rect(2-1.2*eps,fivenum.ms(B)[2],2+1.4*eps,fivenum.ms(B)[4],lwd=2,col=4,den=0)&#10;&#10;plot(factor(c(1,2)),range(c(A,B)),type=&quot;n&quot;,border=0)&#10;points((rep(c(1,2),each=5)),c(fivenum.ms(A),fivenum.ms(B)),col=rep(c(2,4),each=5),pch=rep(c(1,16,9,16,1),2),ylim=c(range(A,B)),cex=1.2,lwd=2,xlim=c(0.5,2.5),ylab=&quot;&quot;,xlab=&quot;&quot;)&#10;rect(1-eps/9,fivenum.ms(A)[2],1+eps/3,fivenum.ms(A)[4],lwd=2,col=2,den=0)&#10;rect(2-eps/9,fivenum.ms(B)[2],2+eps/3,fivenum.ms(B)[4],lwd=2,col=4,den=0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-11-28T03:40:14.763" Id="77916" LastActivityDate="2013-11-28T04:16:34.930" LastEditDate="2013-11-28T04:16:34.930" LastEditorUserId="805" OwnerUserId="805" ParentId="77836" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to estimate a simple Regression. I have 20 observations and 10 Regression-parameters. The degrees of freedom are too small to get reliable point estimates and p-values. I found the following Statement: &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Another advantage of MCMC estimation is the problem of degree of freedom. In the maximum&#10;likelihood estimation, if the parameters number is large compared with the obervation,&#10;the model becomes unstable and the obtained result would not be reliable. But we do not&#10;need to worry about it if we use MCMC estimation. In this note, for example, the number of parameteres to be estimated is larger than n, which is the observation number.&quot; (Estimating Markov Switching model using Gibbs sampling with a statistical computing software R)&lt;/p&gt;&#10;&#10;&lt;p&gt;Would bayesian inferenz (MCMC) of the Regression solve my problem? Or is the Statement only true in the specific case of the markov switching model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-28T09:57:50.787" FavoriteCount="1" Id="77939" LastActivityDate="2013-12-29T02:45:38.337" OwnerUserId="35425" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;mcmc&gt;" Title="MCMC and degree of freedoms (small number of observations)" ViewCount="103" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Is there a general rule of thumb about when robust regression or&#10;  quantile regression is preferred in the presence of outliers?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes. So long as we're comparing regression equivariant approaches, it is clearly possible to rank the various robust estimates of regression in terms of their capacity to find outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;The algorithm behind &lt;code&gt;rreg&lt;/code&gt; is described &lt;a href=&quot;http://www.stata.com/manuals13/rrreg.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;code&gt;rreg&lt;/code&gt; first performs an initial screening based on Cook’s distance $&amp;gt;1$&#10;  to eliminate gross outliers before calculating starting values  and&#10;  then performs Huber iterations followed by biweight iterations,  as&#10;  suggested by Li&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The Li estimate of regression is in a sense similar to an S-estimator but with a single starting point. &#10;This estimator is not used a lot and has not been studied much. I would advise you to use the FastS algorithm of  &lt;a href=&quot;http://www.stat.ualberta.ca/~wiens/stat578/papers/Salibian-Barrera%20&amp;amp;%20Yohai.pdf&quot; rel=&quot;nofollow&quot;&gt;Saliban-Barrera&amp;amp;Yohai, &lt;/a&gt; about which much more is known.&lt;/p&gt;&#10;&#10;&lt;p&gt;For more background on why the S-estimator, a robust estimator with re-descending $\rho$ function, is more reliable than quantile regression check &lt;a href=&quot;http://stats.stackexchange.com/a/46234/603&quot;&gt;this&lt;/a&gt; answer. The S-estimates of regression are implemented in Stata, check the &lt;a href=&quot;http://www.stata-journal.com/article.html?article=st0173&quot; rel=&quot;nofollow&quot;&gt;Verardi and Croux (2008)&lt;/a&gt; stata package and companion paper. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the second part of your question: the breakdown point of quantile regression is proportional to the quantile you estimate with it. So the $\tau=0.9$ &#10;quantile of the quantile regression is much less able to withstand outliers than &#10;the $\tau=0.5$ quantile (and is generally not considered robust).&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, the fact that an observation is flagged as an outlier does not imply &#10;anything about the quality, validity or reliability of the corresponding measurement. It simply means that the flagged observation is inconsistent with the multivariate pattern  fitting the bulk of the data. Indeed, in many fields (micro-array analysis, fraud identification) revealing such data points is often the primary objective of the study. &lt;/p&gt;&#10;&#10;&lt;p&gt;[1]Verardi, Croux (2008). Robust regression in Stata. &lt;em&gt;The Stata Journal&lt;/em&gt; 9(3):  439-453.&lt;br&gt;&#10;[2]Salibian-Barrera M., Yohai, V.J. (2006).&#10;A Fast Algorithm for S-Regression Estimates.&#10;Journal of Computational and Graphical Statistics, Vol. 15, 414--427.&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2013-11-28T10:09:35.037" Id="77941" LastActivityDate="2013-11-28T13:55:18.470" LastEditDate="2013-11-28T13:55:18.470" LastEditorUserId="603" OwnerUserId="603" ParentId="77889" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working on a project where the training labels are given to me as a probability value in the range [0,1].  My first approach was to fit a simple linear ridge regression to predict the probability.  This isn't ideal as:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Predictions from this model end up with values outside the range of 0 and 1&#10;2) I don't think linear regression works that well if you want to make it predict something in a fixed interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;I try the logit transformation, but since I have probabilities that are exactly 0 and 1, I perturb them slightly so I get 0.00001 and .999999 instead to avoid +/- infinity.  I train my model on the transformed labels, and then make a prediction on the test set and undo the transformation with the inverse of the logit function (logistic function).  Frustratingly though, this gives me even worse results than the naive linear regression!&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions on other transformations I can try or what I am doing wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-28T10:21:20.157" Id="77942" LastActivityDate="2013-11-28T10:21:20.157" OwnerUserId="17754" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;probability&gt;&lt;data-transformation&gt;" Title="Regression to predict probability - what transformation to use?" ViewCount="77" />
  <row Body="&lt;p&gt;As you stated, in some ways, the Bayesian inference solves your problem. However, it has to be considered that, roughly speaking, the less data, the more the priors speak. So in the presence of very little data, the prior distributions would have more influence, and consequently your inference can be highly conditioned by the choice of prior. In that sense, the Bayesian inference does not (always) solve your problem. Note that this property is not related to MCMC but to Bayesian inference.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-28T10:24:48.397" Id="77943" LastActivityDate="2013-11-28T22:32:22.473" LastEditDate="2013-11-28T22:32:22.473" LastEditorUserId="22311" OwnerUserId="14346" ParentId="77939" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to analyse the results of a questionnaire with SPSS. I have a number of questions with Likert scale and want to see if there is a link between the age and gender towards the answers given whether they strongly agree/agree/….&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried Chi square (no idea how to analyse the outcome) but I was told that chi square does not work with nominal data. Is that true? What would be the best method?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-28T17:24:35.847" Id="77980" LastActivityDate="2013-11-28T19:14:21.360" LastEditDate="2013-11-28T19:14:21.360" LastEditorUserId="22047" OwnerUserId="35449" PostTypeId="1" Score="0" Tags="&lt;likert&gt;" Title="How to analyse if there is a link between answers on Likert scale and gender/age?" ViewCount="133" />
  <row AnswerCount="0" Body="&lt;p&gt;Let a multivariate distribution be given by $P(Y,S_1,S_2)$, where all three variables are discrete, $Y$ is multivalued, $S_1=(0,1)$ and $S_2=(0,1)$, respectively, and all may be dependent. Define the quantity $X$ for difference in means on categry $j$ of $Y$ as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X=E(Y=j|S_1=1)-E(Y=j|S_2=1).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To give some background to this statistic, assume $S_1$ and $S_2$ are response indicators that indicate whether $Y$ is observed or not observed, e.g. in two indepdent surveys. $S_1$ and $S_2$ may be dependent, because the probability of response in two different surveys should be assumed correlated within the same individual. $X$ then provides the population difference in $Y$ between response groups to both surveys.&lt;/p&gt;&#10;&#10;&lt;p&gt;I estimate $X$ by &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\bar{x_1}=\frac{1}{\sum _{i=1}^{n}S_{1i}}\sum _{i=1}^{n} I(Y_i=j)S_{1i}-\frac{1}{\sum _{i=1}^{n}S_{2i}}\sum _{i=1}^{n} I(Y_i=j)S_{2i},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $I$ is the indicator function and the sum is taken over a sample of size $n$, where $i=1,..,n$. This is the differene in marginal (sample) proportions in category $j$ of a three-way contingency table made up by $P(Y,S_1,S_2)$, let's say $p_{1j}$ and $p_{2j}$. The goal is to derive the standard error for the difference between both quantities. We have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}(p_{1j}-p_{2j})=\text{Var}(p_{1j})+\text{Var}(p_{2j})-2\text{Cov}(p_{1j},p_{2j})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I know how to estimate the variance parts of the formula (i.e., $p_{1j}(1-p_{1j}/n_1$)), but I am clueless about the covariance. The covariance of a multinomial variable is $-n\pi_k\pi_l$ where $\pi$ is the population proportion and $k$ and $l$ two categories. But I do not think this formula applies here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for advice on where to move from here.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-28T18:03:15.617" Id="77984" LastActivityDate="2013-12-03T16:32:28.490" LastEditDate="2013-12-03T16:32:28.490" LastEditorUserId="24515" OwnerUserId="24515" PostTypeId="1" Score="3" Tags="&lt;estimation&gt;&lt;mathematical-statistics&gt;&lt;variance&gt;&lt;covariance&gt;" Title="Variance of a difference in marginal proportions in a three-way contingency table" ViewCount="126" />
  
  <row Body="&lt;p&gt;In order to compare the ground truth (i.e. correct/actual) target values with estimated (i.e. predicted) target values by the random forest , scikit-learn doesn't use the MSE but $R^2$ (unlike e.g. &lt;a href=&quot;http://www.mathworks.com/help/stats/regressionbaggedensemble.oobloss.html&quot; rel=&quot;nofollow&quot;&gt;MATLAB&lt;/a&gt; or (&lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/OOBestimation.pdf&quot; rel=&quot;nofollow&quot;&gt;Breiman 1996b&lt;/a&gt;)), as you can see in the code of &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/forest.py&quot; rel=&quot;nofollow&quot;&gt;forest.py&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;self.oob_score_ = 0.0&#10;for k in xrange(self.n_outputs_):&#10;    self.oob_score_ += r2_score(y[:, k], predictions[:, k])&#10;self.oob_score_ /= self.n_outputs_&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/metrics.py&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;r2_score()&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;http://en.wikipedia.org/wiki/Coefficient_of_determination&quot; rel=&quot;nofollow&quot;&gt;coefficient of determination aka. R2&lt;/a&gt;, whose best possible score is 1.0, and lower values are worse. &lt;/p&gt;&#10;&#10;&lt;p&gt;FYI:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://www.quora.com/Machine-Learning/What-is-the-out-of-bag-error-in-Random-Forests&quot; rel=&quot;nofollow&quot;&gt;What is the out of bag error in Random Forests?&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/32596/12359&quot;&gt;What is the difference between “coefficient of determination” and “mean squared error”?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Breiman, Leo. &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/OOBestimation.pdf&quot; rel=&quot;nofollow&quot;&gt;Out-of-bag estimation&lt;/a&gt;. Technical report, Statistics Department, University of California Berkeley, Berkeley CA 94708, 1996b. 33, 34, 1996.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-11-28T19:15:50.260" Id="77991" LastActivityDate="2013-11-28T19:15:50.260" OwnerUserId="12359" ParentId="70704" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="78117" AnswerCount="1" Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The &quot;classical&quot; MAP estimation: $$\hat\theta = \arg\max_{\theta}P(\theta|\mathbf{x})$$ where $\mathbf{x}$ are the observations and $\theta$ are the parameters. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In this &lt;a href=&quot;http://mitpress.mit.edu/sites/default/files/titles/content/9780262015776_sch_0001.pdf&quot; rel=&quot;nofollow&quot;&gt;book chapter&lt;/a&gt; (page 6, second item), MAP estimation for a MRF is to maximize $P(\mathbf{x}|\mathbf{z},\theta)$ w.r.t $\mathbf{x}$, where $\mathbf{x}$ is the sequence of states, $\mathbf{z}$ is the set of observed data.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In this &lt;a href=&quot;http://papers.nips.cc/paper/4165-map-estimation-for-graphical-models-by-likelihood-maximization&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; (page 2), that is to maximize $P(\mathbf{x}|\theta)$.    &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I would appreciate it if somebody could make the connection among these three clearer. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-11-28T20:16:12.530" Id="77999" LastActivityDate="2014-09-26T14:09:09.427" LastEditDate="2014-09-26T14:09:09.427" LastEditorUserId="35457" OwnerUserId="35457" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;estimation&gt;&lt;graphical-model&gt;" Title="Confusion: different definitions of MAP estimation in Graphical Models (MRFs)" ViewCount="98" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to perform something like a linear regression on my distribution of data, but I'm interested in a trendline that estimates the &lt;strong&gt;minimum&lt;/strong&gt;, &lt;em&gt;not mean&lt;/em&gt;, value for each time bin. I'd like to do this in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The image below shows a scatterplot of the minimum value for each time bin. The black line is a typical linear regression, which estimates the mean. What I'd like is something like what I painted in red - an estimation of the minimum.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/D0O7Q.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My data look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qGAiS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Those are just the first few lines but you get the idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-11-28T20:31:23.693" Id="78001" LastActivityDate="2013-11-28T20:31:23.693" OwnerUserId="29787" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;scatterplot&gt;&lt;minimum&gt;" Title="Find trendline for minimum (not mean) values in distribution" ViewCount="83" />
  <row Body="&lt;p&gt;Why doesn't the Taylor expansion look right?  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want a symmetric statistic, you can try look at the difference instead $\bar{x}-\bar{y}$, it is easy enough to work out the variance of $\bar{x}-\bar{y}$ with any distribution (not just normal) for $X, Y$, provided the variance exist (uniform??). This difference should be symmetric around 0. And you can use t-test to do the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Back to the ratio, if you really want to stick to the ratio, you can use the permutation test to work out the interval estimate. In R, it would look something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N=10&#10;x=runif(N,1,3)  # your data x&#10;y=runif(N,10,30)   # your data y&#10;ratio=mean(x)/mean(y)&#10;NP=100&#10;stat=rep(NA,NP)&#10;for(i in 1:NP){&#10;  id&amp;lt;-sample.int(2*N,size=N,replace=F)&#10;  stat[i]=mean(c(x,y)[id])/mean(c(x,y)[-id])&#10;}&#10;CI=quantile(stat,c(0.025,0.975))&#10;print(CI);print(ratio)&#10;(ratio&amp;lt;CI[2])&amp;amp;(ratio&amp;gt;CI[1])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-11-28T23:51:43.893" Id="78016" LastActivityDate="2013-11-28T23:51:43.893" OwnerUserId="28309" ParentId="77622" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You ask a very interesting question. The key problem is, as you state, that the theoretical distribution of both $X$ and $Y$ is unknown. If it was known, however, it might be possible to derive the variance of the ratio and thus find a sample estimate of the standard error. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose for a moment that both random variables follow a known distribution. As you noted, the normal distribution is a possibility, so that following the central limit theory the ratio is Cauchy distributed. I also think that response times to solve tasks are sometimes modelled by exponential distributions. Therefore, one could also assume the r.v. $X$ and $Y$ are exponentially distributed and their sum is hypoexponential.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, the ratio of $sum(X)$ and $sum(Y)$ is ratio distributed. It is a known problem with ratio distributions, unfortunately, that they often do neither have an existing expectation (mean) nor variance. Therefore the s.e. of the mean often does not exist. This is also the case for the Cauchy distribution and the ratio of two exponential variables, as it is for other ratio distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fortunately, there are also ratios of distributions that have well-defined means and variances. In the following, I will assume the population mean of the ratio exists and construct an example based on this assumption.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, you still do not know the distribution of your r.v. in practice. One option to get to the s.e. of the mean then is by non-parametric bootstrap, which I will demonstrate by example.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $X$ and $Y$ follow a scaled chi-square distribution with 1 and 5 degrees of freedom respectively. Then the ratio $sum(X)/sum(Y)$ is F-distributed with 1*n and 5*n degrees of freedom, where n is the number of summed r.v.. In practice n is the sample size.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n=10^3&#10;df1=1&#10;df2=5&#10;X&amp;lt;-rchisq(n,df=df1)/(df1) #Scaled Chi-square distribution with df=df1&#10;Y&amp;lt;-rchisq(n,df=df2)/(df2) #Scaled Chi-square distribution with df=df2&#10;ratio&amp;lt;-sum(X)/sum(Y) # F-distributed with df1*n and df2*n degrees of freedom&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You may verify that the mean of the F-distribution is known.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ratio #sample mean of ratio&#10;df2*n/(df2*n-2) #theoretical mean of ratio (mean of F-distribution with df1 and df2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now suppose we have a small $n=10$ sized sample from the same distribution. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n=10&#10;df1=1&#10;df2=5&#10;X&amp;lt;-rchisq(n,df=df1)/(df1) #Scaled Chi-square distribution with df=df1&#10;Y&amp;lt;-rchisq(n,df=df2)/(df2) #Scaled Chi-square distribution with df=df2&#10;ratio_sample&amp;lt;-sum(X)/sum(Y) # F-distributed&#10;df2*n/(df2*n-2) #theoretical mean of ratio (mean of F-distribution with df1 and df2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The bootstrap procedure samples with replacement from the data. I will draw 10,000 samples of size 10, respectively. I estimate the mean ratio, variance and s.d. of the bootstrapped distribution. The latter provides the s.e..&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;boot=10^4&#10;data&amp;lt;-data.frame(X,Y)&#10;bootsamples&amp;lt;-numeric()&#10;for(i in 1:boot){&#10;  temp &amp;lt;- data[sample(n,n,replace=T),]&#10;  bootsamples[i]&amp;lt;-sum(temp$X)/sum(temp$Y)&#10;  }&#10;ratio_var&amp;lt;-var(bootsamples)&#10;ratio_se&amp;lt;-sqrt(ratio_var)&#10;ratio_mean&amp;lt;-mean(bootsamples)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To summarize the results we can consider the classical confidence interval based on normal theory, but this is not immediately advisable due to the small sample size.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;c(ratio_mean-1.965*ratio_se,ratio_mean+1.965*ratio_se) #Classical 95% CI based on asymptotics&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Alternatively you may consider the 2.5 and 97.5 percentile of the bootstrapped distribution of ratios.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;quantile(bootsamples,probs=c(.025,.975)) #Bootstrapped 95% CI&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You may verify again that the bootstrapped confidence interval covers the true mean.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df2*n/(df2*n-2) #True mean&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Again, I should stress that the bootstrap will only work, if the expectation of the ratio and its variance exist, which is not certain for ratios. In that case the s.e. of your ratio $avg$ would not exist either and the problem could not be solved.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-29T00:28:26.833" Id="78017" LastActivityDate="2013-11-29T00:39:49.830" LastEditDate="2013-11-29T00:39:49.830" LastEditorUserId="24515" OwnerUserId="24515" ParentId="77622" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="78021" AnswerCount="1" Body="&lt;p&gt;I am fitting a linear model in R with many variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;lm(Y~X1+X2+...+X100)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to check also for all pairwise interactions. Writing &lt;code&gt;X1*X2*...*X100&lt;/code&gt; is not good since it checks the full model (more than pairwise). Writing all pairs explicitly is annoying (100 over 2). Is there a way to do this in R?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-11-29T01:22:22.247" Id="78020" LastActivityDate="2013-11-29T02:04:25.233" OwnerUserId="35465" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;" Title="How to check for interaction of all pairs of variables, in linear regression in R?" ViewCount="162" />
  
  
  <row Body="&lt;p&gt;User Rauparaha made an short but informative comment about ARIMA-models.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You could use other variables is transfer function / ARIMAX-model, but the problem is that after some point you would have to forecast values for the explanatory variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;We found out that MAPE for electricity consumption could be 15-20 percent when using only series own history but when you include number of customers and temperature you can get 2-3 percent MAPE. Only problem is that weather forecasts for long  run will decay quickly and after a week or two you do not have any extra information which would be useful.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Regards,  &lt;/p&gt;&#10;&#10;&lt;p&gt;Analyst&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-29T09:45:11.733" Id="78038" LastActivityDate="2013-11-29T09:45:11.733" OwnerUserId="28732" ParentId="78027" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have searched about chi-square test and what I read is only about its application to genetics like Mendel's law. My data is about internet marketing where my variable is about user sessions. What I got from chi-square test I tried to apply to user sessions. I made data for observed user sessions and then for expected user sessions. I know the sum of expected and observed should be same.&lt;/p&gt;&#10;&#10;&lt;p&gt;1st. Question how to make expected column (user sessions): Is it by selecting  randomly, keeping their sum equal to observed sum?&lt;/p&gt;&#10;&#10;&lt;p&gt;2nd. Is it applicable to my data? My expected user sessions were 4000 and what&#10;I observed was 4, so this single calculation becomes $(4-4000)^2/4000=3992$. By adding others in my outcome is like 21747!!!! But I read chi-square should be less than 1000. &lt;/p&gt;&#10;&#10;&lt;p&gt;Where did I go wrong? &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the table by day:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;day  expected observed&#10;1     4000      4   &#10;2     200       50&#10;3     234       200&#10;4     5000      289&#10;5      333       41&#10;6      3999      209&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Right now I am making expected values randomly as I am not clear about expected value. And one more important thing is I have one class in my case as its user sessions. Ask me if any ambiguity about question.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-11-29T10:33:58.197" Id="78041" LastActivityDate="2014-04-05T06:24:21.670" LastEditDate="2013-11-29T11:34:04.417" LastEditorUserId="22047" OwnerUserId="35471" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;chi-squared&gt;" Title="Chi-square application" ViewCount="114" />
  
  <row Body="&lt;p&gt;In structural models, you would add $Z_3$ to both equations. It will then have a direct effect $\beta_{z31}$, an indirect effect $(\beta_{x}\beta_{z32})$, and a total effect (sum of both) on $Y_i$. Omitting $Z_3$ from any oth the two structural equations will bias the parameters of the direct effects in the model.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-11-29T13:43:56.257" Id="78057" LastActivityDate="2013-11-30T13:04:04.323" LastEditDate="2013-11-30T13:04:04.323" LastEditorUserId="24515" OwnerUserId="24515" ParentId="78054" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;This article by Cousineau and Chartier discusses replacing outliers with the mean&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.redalyc.org/pdf/2990/299023509004.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.redalyc.org/pdf/2990/299023509004.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;They write:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Tabachnick and Fidell (2007) suggested  replacing the missing data&#10;  with the mean of the remaining  data in the corresponding cell.&#10;  However, this procedure will  tend to reduce the spread of the&#10;  population, make the  observed distribution more leptokurtic, and&#10;  possibly increase the likelihood of a type-I error. A more elaborate &#10;  technique, multiple imputations, involves replacing outliers  (or&#10;  missing data) with possible values (Elliott &amp;amp; Stettler,  2007;&#10;  Serfling &amp;amp; Dang, 2009).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There is also an R package &quot;outliers&quot; that has a function to replace outliers with the mean. I also saw a number of hits in my Google search that implies that SPSS also has such a function, but I am not familiar with that program. Perhaps if you follow the threads you can discover the technical basis for the practice.&lt;/p&gt;&#10;&#10;&lt;h3&gt;References&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Cousineau, D., &amp;amp; Chartier, S. (2010). Outliers detection and treatment: a review. International Journal of Psychological Research, 3(1), 58-67.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2013-11-29T15:26:44.310" Id="78071" LastActivityDate="2013-12-20T15:20:31.070" LastEditDate="2013-12-20T15:20:31.070" LastEditorUserId="22047" OwnerUserId="28380" ParentId="78063" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This question is a follow-up of &lt;a href=&quot;http://stats.stackexchange.com/questions/77211/estimating-population-size-of-a-subgroup-based-on-independent-samples-without-re&quot;&gt;Estimating population size of a subgroup based on independent samples without replacement&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let a bag have 1000 balls of arbitrary colors and unknowns sizes ($r$). Suppose we also known the total volume occupied by the balls ($t_v$). We want to estimate the total volume occupied by red balls ($t_{v}^{red}$).&lt;/p&gt;&#10;&#10;&lt;p&gt;We attempt to do this by taking $x$ independent samples (without replacement within each sample) of size $s$ of the original population. Let $s$ be 10% of the original population as an example.&lt;/p&gt;&#10;&#10;&lt;p&gt;Proceed to count the number of red balls in each sample, e.g., for $x = 10$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(9, 10, 10, 11, 13, 8, 5, 15, 12, 9)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To estimate the number of red balls in the original population, one can &lt;em&gt;naïvely&lt;/em&gt; calculate the average of the above list and divide by the sample. In this case: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(9 + 10 + 10 + 11 + 13 + 8 + 5 + 15 + 12 + 9) / 10 / 0.1 = 102&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What about $t_{v}^{red}$? Each sample will give us a set of sizes for the observed red balls. So:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sample1 = [3, 2, 1, 10, 2, 8, 1, 4, 1, 1] =&amp;gt; 33 =&amp;gt; V = 5098&#10;sample2 = [3, 6, 1, 3, 4, 1, 1, 1, 8, 1] =&amp;gt; 29 =&amp;gt; V = 2673&#10;sample3 = [1, 4, 3, 10, 1, 1, 5, 1, 1, 2] =&amp;gt; 29 =&amp;gt; V = 3861&#10;sample4 = [4, 1, 1, 1, 2, 6, 2, 6, 1, 1] =&amp;gt; 25 =&amp;gt; V = 1624&#10;sample5 = [1, 6, 1, 2, 4, 3, 7, 10, 10, 2] =&amp;gt; 46 =&amp;gt; V = 8381&#10;sample6 = [6, 6, 1, 2, 1, 10, 3, 2, 3, 1] =&amp;gt; 35 =&amp;gt; V = 4728&#10;sample7 = [1, 4, 2, 1, 4, 1, 2, 4, 10, 1] =&amp;gt; 30 =&amp;gt; V = 3807&#10;sample8 = [1, 8, 3, 3, 4, 3, 1, 8, 1, 5] =&amp;gt; 37 =&amp;gt; V = 4074&#10;sample9 = [7, 9, 2, 4, 4, 6, 6, 1, 8, 1] =&amp;gt; 48 =&amp;gt; V = 6766&#10;sample10 = [3, 8, 1, 2, 10, 7, 3, 7, 3, 1] =&amp;gt; 45 =&amp;gt; V = 7191&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, the estimated volume on each sample can vary considerably (from 1624 to 8381). And here lies my problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are thus:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How to estimate the total volume occupied by balls of a certain color?&lt;/li&gt;&#10;&lt;li&gt;We could hypothetically assume that the ball size follows a known distribution (in this case, zipfian). Would this help?&lt;/li&gt;&#10;&lt;li&gt;Instead of a number, how can I obtain a probability distribution of the estimated population size and volume?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Some clarifications over the original problem:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The population size is known &lt;em&gt;à priori&lt;/em&gt;.&lt;/li&gt;&#10;&lt;li&gt;The population volume is also known &lt;em&gt;à priori&lt;/em&gt;.&lt;/li&gt;&#10;&lt;li&gt;But the number of colors is unknown. Or the total volume of each color. Or both ratios.&lt;/li&gt;&#10;&lt;li&gt;Each sample take balls without replacement.&lt;/li&gt;&#10;&lt;li&gt;Samples are independent (after a sample is taken, all balls are replaced).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-11-30T00:52:16.473" Id="78102" LastActivityDate="2014-11-17T20:26:05.210" OwnerUserId="990" PostTypeId="1" Score="2" Tags="&lt;inference&gt;" Title="Estimating a sub-population characteristic based on independent samples without replacement" ViewCount="48" />
  <row Body="&lt;p&gt;The term inside your definition of &lt;code&gt;f&lt;/code&gt; :- &lt;code&gt;sum(-dexp(x,rate=theta,log=T))&lt;/code&gt; is NOT the likelihood, but something else. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is it that is being calculated?&lt;/p&gt;&#10;&#10;&lt;p&gt;When you consider what it is that is being optimized there, you will also understand why you're minimizing that function in order to maximize the likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;To quote your own algebra, here's the likelihood:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\cal{L}(\theta)=\prod_{i=1}^{n}\theta e^{-\theta x_i}=\theta^n e^{-\theta \sum_{i=1}^{n}x_i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;dexp&lt;/code&gt; with &lt;code&gt;log=TRUE&lt;/code&gt; doesn't return the density. Here's what the help says: &lt;code&gt;log, log.p   logical; if TRUE, probabilities p are given as log(p).&lt;/code&gt; ... that is when you say &lt;code&gt;log=TRUE&lt;/code&gt; you get the log of the density.&lt;/p&gt;&#10;&#10;&lt;p&gt;The likelihood at $\theta$ will be the product of the densities, taken at each data point.&lt;/p&gt;&#10;&#10;&lt;p&gt;The log-likelihood is the sum of the log-densities, over the data points, evaluated at a given $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, &lt;code&gt;sum(dexp(x,rate=theta,log=T))&lt;/code&gt; would be the &lt;em&gt;log-likelihood function&lt;/em&gt;. We'd want to maximize that.&lt;/p&gt;&#10;&#10;&lt;p&gt;But we have &lt;code&gt;sum(-dexp(x,rate=theta,log=T))&lt;/code&gt;  (don't ask me why they didn't write the obviously equivalent but presumably faster &lt;code&gt;-sum(dexp(x,rate=theta,log=T))&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;That is, the program is minimizing the negative log-likelihood, which is equivalent to maximizing the log-likelihood. Here's the result on calling &lt;code&gt;f&lt;/code&gt; on theta values between 1 and 3:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dErxc.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;By contrast, this is what the likelihood function looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lEsAv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;code&gt;sum(dexp(x,rate=theta,log=T))&lt;/code&gt; is calculating $θ^ne^{−θ∑^n_{i=1}x_i}$?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It's calculating the log of that quantity.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;But here I see I have the minus sign in every program related to MLE in my lecture sheet. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Minimizing rather than maximizing is a convention. There's no particular need for it.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The R documentation is saying that in &lt;code&gt;optim&lt;/code&gt; function &lt;code&gt;par Initial values for the parameters to be optimized over&lt;/code&gt;. How can I select the initial values?&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Would you please tell me how can I relate this program&lt;br&gt;&#10;  &lt;code&gt;fexp = function(theta, x){ prod(dexp(x,rate=(1/theta))) }&lt;/code&gt;&lt;br&gt;&#10;  &lt;code&gt;res3&amp;lt;-optimize(f=fexp,interval=c(0,50), maximum=T, x=x)&lt;/code&gt;&lt;br&gt;&#10;  &lt;code&gt;res3&lt;/code&gt;  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;with my above program that I have posted in the question?&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Why here is the &lt;code&gt;prod&lt;/code&gt; function being called?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Because the likelihood is a product.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;And why here we have mentioned maximum=T?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Because it's computing the likelihood, which we want to maximize.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I notice another issue with the above code: it says &lt;code&gt;rate = 1/theta&lt;/code&gt;. That implies&#10;that the &lt;code&gt;theta&lt;/code&gt; there is not the rate parameter of your earlier mathematics and code, but is in fact a &lt;em&gt;scale parameter&lt;/em&gt;. Watch out for that! Another thing to watch out for is that likelihood calculations often have underflow problems (and sometimes, overflow problems).&lt;/p&gt;&#10;" CommentCount="21" CreationDate="2013-11-30T01:22:54.980" Id="78106" LastActivityDate="2013-11-30T10:15:34.727" LastEditDate="2013-11-30T10:15:34.727" LastEditorUserId="805" OwnerUserId="805" ParentId="78104" PostTypeId="2" Score="5" />
  <row AnswerCount="3" Body="&lt;p&gt;I am slightly irritated about weak stationarity in connection to ARCH/GARCH models. I do not know the answer and I am not sure about it:&lt;/p&gt;&#10;&#10;&lt;p&gt;The basic question is: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Do we have to test weak stationarity before applying an ARMA-GARCH&#10;  model?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Further on it can be said:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;ADF and others test the mean equation, but this is not for the&#10;  volatility equation, so what test do we have to use for the&#10;  autocovariance-stationarity?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My thoughts:&lt;/p&gt;&#10;&#10;&lt;p&gt;Standard ARMA models assume the unconditional mean and unconditional variance to be constant. For ARMA-GARCH models this is also the case: The unconditional mean and unconditional variance need to be constant, whereas in case of the ARMA-GARCH models the conditional variance does not need to be constant. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is correct that for the mean equation we may have to think about using a trend-stationary or difference-stationary model. But this is only concerning the mean equation, yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Conditional variance can be tested by testing for ARCH effects (Box-Ljung, Lagrange Multiplier).&lt;/p&gt;&#10;&#10;&lt;p&gt;So for ARMA-GARCH models we still need weak stationarity, since the unconditional mean and unconditional variance need to be constant. So I am not sure, but we have to test for weak stationarity before applying an ARMA-GARCH model? And especially with financial returns, do we also have to test for it? And which test do we use (and which command is implemented in R, so what command can you suggest?)&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that if the unconditional variance is nonstationary (not finite and then it is also not constant) an integrated GARCH may be appropriate. But just because it is not constant I cannot say I use an integrated GARCH model?&lt;/p&gt;&#10;&#10;&lt;p&gt;I also know that for ARMA-GARCH processes all the &quot;characteristical&quot; roots lie outside the unit circle. So in case of a ARMA-GARCH(1,1) $\alpha_1+\beta_1&amp;lt;1$ is necessary. But I only see this after estimation? This is not a test for covariance stationarity?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: It basically pins down to (see the comments):&#10;How can I test the unconditional variance to be constant? I mean in order to apply a GARCH model I have to make sure that I have constant unconditional mean (ADF test and so) and I have to test for constant unconditional variance (how?). I know that I have to further make sure that I have nonconstant conditional variance for GARCH processes, otherwise having a constant conditional variance ARMA is sufficient (test for ARCH effects).&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT 2: There is a Wavelet Spectrum test in the locits package, what about this test?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-11-30T09:26:10.587" Id="78120" LastActivityDate="2015-02-24T21:54:22.447" LastEditDate="2013-11-30T09:57:18.413" LastEditorUserId="25675" OwnerUserId="25675" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;stationarity&gt;&lt;volatility-forecasting&gt;" Title="Weak stationarity and ARMA-ARCH/GARCH models?" ViewCount="568" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am an undergrad student and I have a project for my probability class. Basically, I have a dataset about the hurricanes that impacted my country for a series of years.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my probability Book, (Probability ans Statistics with R) there is an (not complete) example of how to check if the data follows a poisson distribution, they begin trying to prove that this 3 criteria are followed: (From my book, page 120 (criteria) page 122-123 example)&lt;/p&gt;&#10;&#10;&lt;p&gt;1- The number of outcomes in non-overlapping intervals are independent. In other words, the number of outcomes in the interval of time (0,t] are independent from the number of outcomes in the interval of time (t, t+h] , h &gt; 0 &lt;/p&gt;&#10;&#10;&lt;p&gt;2- The probability of two or more outcomes in a sufficiently short interval is virtually zero. In other words, provided h is sufficiently small, the probability of obtaining two or more outcomes in the interval (t,t+h] is negligible compared to the probability of obtaining one or zero outcomes in the same interval of time.&lt;/p&gt;&#10;&#10;&lt;p&gt;3- The probability of exactly one outcome in a sufficiently short interval or small region is proportional to the length of the interval or region. In other words, the probability of one outcome in an interval of length h is lambda*h.&lt;/p&gt;&#10;&#10;&lt;p&gt;But criterion 3 is left &quot;as an exercise&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A- Can someone tell me if there is a more &quot;easy&quot; way to see if my dataset follows a Poisson distribution? &lt;/p&gt;&#10;&#10;&lt;p&gt;B- Can someone explain me criterion 1 and 3 with some type of example (if it is with R, fantastic)?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: Sorry for the long post. Also, I have to convert the data so that I have a table like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  number of hurricanes       | 0 | 1 | 2  etc.&#10;  -----------------------------------------&#10;  total years that have      |   |   |&#10;  that number of hurricanes  |   |   |&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2013-11-30T15:12:16.803" FavoriteCount="3" Id="78139" LastActivityDate="2014-07-09T21:28:09.807" LastEditDate="2013-11-30T21:28:14.400" LastEditorUserId="8076" OwnerUserId="35507" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;self-study&gt;&lt;poisson&gt;&lt;poisson-process&gt;" Title="How to know if a data follows a Poisson Distribution in R?" ViewCount="6592" />
  <row Body="&lt;p&gt;If your data follow a truncated normal distribution, this link gives you a implementation in R language for the computation of the mean and variance of a truncated normal distribution : &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.r-bloggers.com/truncated-normal-distribution/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-bloggers.com/truncated-normal-distribution/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-11-30T16:20:38.137" Id="78147" LastActivityDate="2013-11-30T16:20:38.137" OwnerUserId="35505" ParentId="8382" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;This should be easily possible, as long as you have some software which is very flexible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd try &lt;a href=&quot;http://elki.dbs.ifi.lmu.de/&quot; rel=&quot;nofollow&quot;&gt;ELKI&lt;/a&gt;, and implement my own distance function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;d(x,y)=\begin{cases}&#10;0 &amp;amp; \text{iff }x\text{ and }y\text{ are in the same class} \\&#10;\varepsilon+\text{Euclidean}(x,y) &amp;amp; \text{otherwise}&#10;\end{cases}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then hierarchical clustering should first merge all objects of the same class, and construct a hierarchy of the existing classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that hierarchical clustering scales quite badly, it's in $\mathcal{O}(n^3)$ when implemented naively (ELKI has a $\mathcal{O}(n^2)$ implementation for single-linkage). You may actually get a substantial speedup by re-implementing hierarchical clustering yourself for your particular use case; by starting at the point where the same-label clusters have already been formed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-01T09:50:17.543" Id="78191" LastActivityDate="2013-12-01T09:50:17.543" OwnerUserId="7828" ParentId="77825" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have set of 16 questions that have been answered on a numerical scale, from 1 to 7. They are answered by 200 couples, and I want to show there is a difference in the answers on gender. On a casual glance at the data, I see there is little difference between the genders for 6 of the questions, but is for the other. &lt;/p&gt;&#10;&#10;&lt;p&gt;If I run a series of t-tests for each, and show significant differences for 10 of them; I should be able to conclude that on the set as a whole, there are differences.&#10;Is there a better way to show that for this series, answers differ based on gender? Mu&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-01T19:50:45.277" Id="78223" LastActivityDate="2014-02-01T23:31:53.463" OwnerUserId="35534" PostTypeId="1" Score="2" Tags="&lt;multivariate-analysis&gt;" Title="How to show that answers to a set of 16 questions are different based on gender." ViewCount="75" />
  
  <row Body="&lt;p&gt;Do not perform backward selection. Nor forward. Nor stepwise. Don't do it for Cox or OLS or logistic or any other model. Instead, think about what variables you have, why you have them, what they mean, what theory says about them, how including them affects other variables  and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you &lt;em&gt;must&lt;/em&gt; use some automated method, LASSO or LAR have nice properties.&lt;/p&gt;&#10;&#10;&lt;p&gt;But no automated method is as good as your brain and your knowledge.&lt;/p&gt;&#10;&#10;&lt;p&gt;If a variable doesn't meet one of the assumptions, consider modifying the variable; or consider another method of analysis. Also, with a large N, the test of proportionality may be overly conservative. You also might consider interactions. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-01T23:37:45.157" Id="78246" LastActivityDate="2013-12-01T23:37:45.157" OwnerUserId="686" ParentId="78242" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I try to understand parameter estimation and learning problems at Graphical Models, especially in directed ones (Bayesian Networks). But first of all, I try to understand what &lt;em&gt;exactly&lt;/em&gt; a parameter means in a Bayesian Network. The separation between a &quot;variable&quot; and a &quot;parameter&quot; becomes blurry when describing learning problems especially in Bayesian Learning, where the parameters are also variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;So generally, we have a stochastic process, which is represented by the graph $G$. This graph consists of $S$ random variables, $X_1,X_2,...,X_S$ and the probability distribution over these variables is $P(X_1,X_2,...,X_S)=\prod_{i=1}^{S} P(X_i|parents(X_i))$. Now, where do the parameters fit here?  From what I have understood so far:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;By the $P(X_1,X_2,...,X_S)=\prod_{i=1}^{S} P(X_i|parents(X_i))$ equation, we implicitly assume that a parameter set $\theta$ is already given and we actually mean $P(X_1,X_2,...,X_S|\theta_{1},\theta_{2},...,\theta_{S})=\prod_{i=1}^{S} P(X_i|parents(X_i),\theta_{i})$. If we run the process $G$, $N$ times in a i.i.d. fashion, then it is either: There is a single correct set for $\theta$ and each time we run the process, the variables are instantiated according to this one and only true $\theta$ (maximum likelihood view) or $\theta$ has a prior distribution of $P(\theta)$ and the &quot;nature&quot; draws a $\theta$ a priori and then our all $N$ previous and any future samples are generated with this $\theta$ (Bayesian view). Therefore, we integrate over all possible $\theta$ when making inference about a new sample. Is this thought pattern correct?&lt;/li&gt;&#10;&lt;li&gt;If we think of this as a generative process, then can we say that for a variable $X_i$, an instantiation of its parents, $parents(X_{i})=\pi$ selects a subset of $\theta_{i}$ as $\theta_{i}^{\pi}$ and then generates $X_{i}$ according to the distribution $P(X_{i}|parents(X_{i})=\pi , \theta_{i}^{\pi})$? I think of this subset thing because $X_{i}$ has different distributions conditioned on different values of its parents and each of these distributions can have different parameters (or shared ones of course) among $\theta_{i}$. Again, is this correct or am I missing or misunderstood something completely?&lt;/li&gt;&#10;&lt;li&gt;In the graphical model, we add for each $X_{i}$ a new node $\theta_{i}$ which is a new parent of $X_{i}$ and does not have any parents for itself. (There can be shared parameters among random variables, as well.) So, for each sample of the process $G$, we have a new directed graph $G_{n}$ whose nodes are connected to the corresponding $\theta_{i}$ nodes and given all $\theta$ nodes, these $N$ samples are independent from each other. (Of course this is from a theoretical point of view, this &quot;plate notation&quot; thingy is used for real applications as far as I know.) Finally, is this correct or wrong?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What I aim with this question is to verify or invalidate my current understanding of the &quot;parameter&quot; concept in Graphical Models. I need a solid understanding since I need to cope with advanced concepts like learning with the EM Algorithm, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-02T00:34:40.717" Id="78250" LastActivityDate="2013-12-02T10:40:12.220" LastEditDate="2013-12-02T10:40:12.220" LastEditorUserId="88" OwnerUserId="31611" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;graphical-model&gt;&lt;bayes-network&gt;&lt;parameterization&gt;" Title="Parameters and parameter estimation in graphical models" ViewCount="116" />
  
  
  <row Body="&lt;p&gt;You say that you'd calculate the slope as follows:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;So normally you would calculate &lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$S_{XX} = \sum_i (x_i-\bar x)^2\\ &#10;S_{XY} = \sum_i ((x_i-\bar x)(y_i-\bar y))\\ &#10;S_{YY} = \sum(y_i-\bar y)^2$$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Then $b_2 = S_{XY}/S_{XX}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So imagine you have a set of x-values and y-values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    y       x&#10;1 2.3 0.36772&#10;2 5.3 1.64873&#10;3 6.5 7.38910&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Step 1:&lt;/p&gt;&#10;&#10;&lt;p&gt;calculate a new $x$, $x_1 = \ln(x)$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    y   x1&#10;1 2.3 -1.0&#10;2 5.3  0.5&#10;3 6.5  2.0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now regress $y$ on this new $x_1$ as usual&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; SXX = 4.50; SXY= 6.30; SYY = 9.36&#10;&#10; b2 = SXY/SXX = 1.4&#10;&#10; b1 = mean(y) - b2 . mean(x) = 4.7 - 1.4 . 0.5 = 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RI9zo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-02T04:54:36.943" Id="78262" LastActivityDate="2013-12-02T05:08:36.870" LastEditDate="2013-12-02T05:08:36.870" LastEditorUserId="805" OwnerUserId="805" ParentId="78256" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In this study, subjects are measured continuously over the day via electrocardiography (ECG). During the day, certain trigger events occur randomly. Once all the data is collected, the trigger events are examined and everytime a trigger event occured, the ECG is used to check for the presence of a certain medical condition. This means, that the number of observations per subject are different. In total there are $n=2240$ observations and $m=42$ subjects. The number of observations per subject range from 13 to 216.&lt;/p&gt;&#10;&#10;&lt;p&gt;This study has already been analyzed and I'm trying to understand the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;To analyze the data a logistic regression with random intercept was choosen. I will use &lt;code&gt;glmer&lt;/code&gt; from &lt;code&gt;lme4&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt; to reproduce the results. Here $Y$ is a binary 0/1 variable and $X1, \ldots, X3$ are covariates of interest and &lt;code&gt;id&lt;/code&gt; is the subject indicator. If I fit a model with just one variable I get this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Generalized linear mixed model fit by maximum likelihood ['glmerMod']&#10; Family: binomial ( logit )&#10;Formula: Y ~ X1 + (1 | id) &#10;   Data: data &#10;&#10;      AIC       BIC    logLik  deviance &#10;1548.1535 1565.2962 -771.0768 1542.1535 &#10;&#10;Random effects:&#10; Groups Name        Variance Std.Dev.&#10; id     (Intercept) 0.02596  0.1611  &#10;Number of obs: 2240, groups: id, 42&#10;&#10;Fixed effects:&#10;              Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -2.1235717  0.0756041 -28.088  &amp;lt; 2e-16 ***&#10;X1           0.0007553  0.0001924   3.927 8.62e-05 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Correlation of Fixed Effects:&#10;   (Intr)&#10;X1 -0.166&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What we see is that the variance of the random subject effect is 0.026. It is small, but not equal to zero. If I add two more covariates, I get:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Generalized linear mixed model fit by maximum likelihood ['glmerMod']&#10; Family: binomial ( logit )&#10;Formula: Y ~ X1 + X2 + X3 + (1 | id) &#10;   Data: data &#10;&#10;      AIC       BIC    logLik  deviance &#10;1514.2474 1542.8186 -752.1237 1504.2474 &#10;&#10;Random effects:&#10; Groups Name        Variance  Std.Dev. &#10; id     (Intercept) 1.021e-11 3.195e-06&#10;Number of obs: 2240, groups: id, 42&#10;&#10;Fixed effects:&#10;              Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -2.1770583  0.0727357 -29.931  &amp;lt; 2e-16 ***&#10;X1           0.0009218  0.0001914   4.816 1.46e-06 ***&#10;X2          -0.3782134  0.0625888  -6.043 1.51e-09 ***&#10;X3           0.0041024  0.0045297   0.906    0.365    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Correlation of Fixed Effects:&#10;   (Intr) X1     X2    &#10;X1 -0.229              &#10;X2  0.281 -0.131       &#10;X3 -0.044  0.013  0.054&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now the variance it technically 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;How would I explain this? My interpretation is that due to the added covariates, all the variation is accounted for and there is no more subject specific variation left. Is this reasonable?&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional info about the covariates:&#10;$X1$ and $X2$ are trigger specific measurements (length and intensity).Their value usually changes for different trigger events. Hence they are different for each measurement even for the same subject. $X3$ is the age of the subject.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-02T09:49:09.140" Id="78282" LastActivityDate="2013-12-02T09:57:03.833" LastEditDate="2013-12-02T09:57:03.833" LastEditorUserId="35555" OwnerUserId="35555" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;random-effects-model&gt;&lt;longitudinal&gt;&lt;glmm&gt;" Title="Random effect with 0 variance in GLMM" ViewCount="290" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose that we have two sets of variables $\textbf{X}$ and $\textbf{Y}$. Let $$ \textbf{X} = \begin{pmatrix} X_1 \\ X_2\\ \vdots \\X_p \end{pmatrix}$$ and $$ \textbf{Y} = \begin{pmatrix} Y_1 \\ Y_2\\ \vdots \\Y_q \end{pmatrix}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $p \leq q$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Define the following linear combinations: $$U_i = a_{i1}X_1 + a_{i2}X_2 + \cdots + a_{ip}X_p$$ $$V_i = b_{i1}Y_1 + b_{i2}Y_2 + \cdots + b_{iq}Y_q$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Does it matter what we call the first canonical variate pair? For example is the first canonical variate pair $(U_1, V_1)$? Or would we define the first canonical variate pair as $(U_1, V_3)$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-02T18:18:41.260" Id="78331" LastActivityDate="2013-12-02T19:07:12.627" LastEditDate="2013-12-02T19:07:12.627" LastEditorUserId="21478" OwnerUserId="21478" PostTypeId="1" Score="1" Tags="&lt;canonical-correlation&gt;" Title="Canonical Correlation Definitions" ViewCount="43" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have 2 datasets (nonpaired), P1 &amp;amp; P2, and I want to know 'scale parameter' of one to the other, aka P1=a*P2. &lt;/p&gt;&#10;&#10;&lt;p&gt;As both datasets are highly nonnormal, I am performing a Mann-Whitney Test on the data. I take the log of the data, do the MW test, get the Hodges-Lehmann shift estimate (HL) and then take the antilog of the HL shift estimate to get the scale estimate. I do the same thing with confidence intervals. &lt;/p&gt;&#10;&#10;&lt;p&gt;This all seems to work fine, until I deal w/ datasets which are positive &amp;amp; negative, since the log of a negative number is imaginary, and then everything needless to say explodes...&lt;/p&gt;&#10;&#10;&lt;p&gt;What method can I use to get the estimate for positive &amp;amp; negative data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-03T01:20:18.250" Id="78373" LastActivityDate="2013-12-08T11:18:52.340" LastEditDate="2013-12-03T01:34:16.020" LastEditorUserId="22047" OwnerUserId="35026" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;confidence-interval&gt;&lt;mann-whitney-u-test&gt;" Title="scale estimate for positive and negative data" ViewCount="105" />
  
  
  <row Body="&lt;p&gt;Why not try them both and see which works best? I think method 1 is probably better (and will give you a much larger dataset to work with), but I think you should just try both methods and see which model performs better.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-03T05:56:00.267" Id="78382" LastActivityDate="2013-12-03T05:56:00.267" OwnerUserId="8451" ParentId="78377" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps I should also note that my initial attempt at analyzing this data used a hierarchical model in rJAGS sampling the means from uniform distributions and variances from gamma distributions but I found the choice of priors on the group level variance heavily influenced the results. Perhaps there is a better way to use that approach to estimate? Any advice on the best way to do this would be welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;End Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This question is related to these two previous questions of mine. @Elvis has suggested I turn it into its own question as the comments were getting lengthy:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/78339/how-to-use-profile-likelihood&quot;&gt;How to use profile likelihood?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/77528/what-is-the-relationship-between-profile-likelihood-and-confidence-intervals&quot;&gt;What is the relationship between profile likelihood and confidence intervals?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have data like the following based on animals performing a behavioral task:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Data can take any value from 0 to 20&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Data from a relatively large number (~50) of previous results with control animals that indicates the expected distribution of results (at least for the control group) is not normal and looks like the top plot. &lt;/p&gt;&#10;&#10;&lt;p&gt;3) I have new data shown in the boxplots in the middle panel. Group 1 is a control that should be the same as the previous results (although no two groups of animals and conditions are ever &lt;em&gt;exactly&lt;/em&gt; the same). Group 2 is a control that received placebo treatment which is expected to have no effect, thus it should be the same as Group 1. Group 3 received a drug.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) The goal of the study is to determine if the drug may be affecting the behavior of the animals.&lt;/p&gt;&#10;&#10;&lt;p&gt;5) There are many other factors that may affect the behavior of animals so it is the common assumption that the best that can be done is see if the drug affected the behavior &lt;em&gt;on average&lt;/em&gt;. Thus I wish to compare the average outcomes of each group.&lt;/p&gt;&#10;&#10;&lt;p&gt;6) Previous studies have compared using one-way ANOVA and post-hoc t-tests then reporting p-values. This bothers me as the assumptions of the tests (normality) appear to be violated, using an arbitrary cutoff to decide &quot;significance&quot; (eg p&amp;lt;0.05) seems illogical, and I would never expect any two groups of animals to be exactly the same (null hypothesis of two means are equal is always false). Therefore I would like to instead simply estimate the means and show what means are most likely for each group using likelihoods and say something like the mean of group 3 is 20x more likely to be 15 than the mean of group 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;7) I &lt;em&gt;may&lt;/em&gt; also/instead want to calculate confidence intervals for the means. It is not clear to me whether this is something I should want to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;8) Note that this is not my actual data but was instead gotten by sampling from the &quot;prior/previous&quot; density, eg: &lt;code&gt;sample(x=prior.dens$x,size=12, prob=prior.dens$y)&lt;/code&gt; So the appearance of a true difference between groups is false here. However this is very similar to the situation with my real data.&lt;/p&gt;&#10;&#10;&lt;p&gt;9) I have calculated the profile likelihood (lower plot) using this R code which appears to be valid based on the responses to my previous questions:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;muVals &amp;lt;- seq(0,20, length = 10000)&#10;profile.likelihood&amp;lt;-function(dat, muVals){&#10;  likVals &amp;lt;- sapply(muVals,&#10;                    function(mu){&#10;                      (sum((dat - mu)^2) /&#10;                         sum((dat - mean(dat))^2)) ^ (-length(dat)/2)&#10;                    }&#10;  )&#10;&#10;return(cbind(muVals,likVals))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wsmXv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;dput() of new data:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, &#10;2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 13.6986301369863, &#10;16.1643835616438, 12.0547945205479, 12.1722113502935, 9.74559686888454, &#10;0.430528375733855, 11.3502935420744, 10.6457925636008, 9.9412915851272, &#10;10.7240704500978, 10.958904109589, 11.6242661448141, 17.9701232444495, &#10;15.9326690901071, 7.98247314058244, 14.4031004607677, 13.5198221541941, &#10;2.82421704847366, 16.114045586437, 19.328767512925, 3.74181577004492, &#10;17.4085859861225, 19.2017483590171, 8.26946665905416, 10.0207103956491, &#10;16.1689247898757, 13.9989542039111, 9.80047978740185, 17.8596440777183, &#10;18.1706223106012, 18.1891529858112, 11.7567204562947), .Dim = c(32L, &#10;2L))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;dput() of prior density&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;structure(list(x = c(0, 0.0391389432485323, 0.0782778864970646, &#10;0.117416829745597, 0.156555772994129, 0.195694716242661, 0.234833659491194, &#10;0.273972602739726, 0.313111545988258, 0.352250489236791, 0.391389432485323, &#10;0.430528375733855, 0.469667318982387, 0.50880626223092, 0.547945205479452, &#10;0.587084148727984, 0.626223091976517, 0.665362035225049, 0.704500978473581, &#10;0.743639921722113, 0.782778864970646, 0.821917808219178, 0.86105675146771, &#10;0.900195694716243, 0.939334637964775, 0.978473581213307, 1.01761252446184, &#10;1.05675146771037, 1.0958904109589, 1.13502935420744, 1.17416829745597, &#10;1.2133072407045, 1.25244618395303, 1.29158512720157, 1.3307240704501, &#10;1.36986301369863, 1.40900195694716, 1.44814090019569, 1.48727984344423, &#10;1.52641878669276, 1.56555772994129, 1.60469667318982, 1.64383561643836, &#10;1.68297455968689, 1.72211350293542, 1.76125244618395, 1.80039138943249, &#10;1.83953033268102, 1.87866927592955, 1.91780821917808, 1.95694716242661, &#10;1.99608610567515, 2.03522504892368, 2.07436399217221, 2.11350293542074, &#10;2.15264187866928, 2.19178082191781, 2.23091976516634, 2.27005870841487, &#10;2.30919765166341, 2.34833659491194, 2.38747553816047, 2.426614481409, &#10;2.46575342465753, 2.50489236790607, 2.5440313111546, 2.58317025440313, &#10;2.62230919765166, 2.6614481409002, 2.70058708414873, 2.73972602739726, &#10;2.77886497064579, 2.81800391389432, 2.85714285714286, 2.89628180039139, &#10;2.93542074363992, 2.97455968688845, 3.01369863013699, 3.05283757338552, &#10;3.09197651663405, 3.13111545988258, 3.17025440313112, 3.20939334637965, &#10;3.24853228962818, 3.28767123287671, 3.32681017612524, 3.36594911937378, &#10;3.40508806262231, 3.44422700587084, 3.48336594911937, 3.52250489236791, &#10;3.56164383561644, 3.60078277886497, 3.6399217221135, 3.67906066536204, &#10;3.71819960861057, 3.7573385518591, 3.79647749510763, 3.83561643835616, &#10;3.8747553816047, 3.91389432485323, 3.95303326810176, 3.99217221135029, &#10;4.03131115459883, 4.07045009784736, 4.10958904109589, 4.14872798434442, &#10;4.18786692759295, 4.22700587084149, 4.26614481409002, 4.30528375733855, &#10;4.34442270058708, 4.38356164383562, 4.42270058708415, 4.46183953033268, &#10;4.50097847358121, 4.54011741682975, 4.57925636007828, 4.61839530332681, &#10;4.65753424657534, 4.69667318982387, 4.73581213307241, 4.77495107632094, &#10;4.81409001956947, 4.853228962818, 4.89236790606654, 4.93150684931507, &#10;4.9706457925636, 5.00978473581213, 5.04892367906066, 5.0880626223092, &#10;5.12720156555773, 5.16634050880626, 5.20547945205479, 5.24461839530333, &#10;5.28375733855186, 5.32289628180039, 5.36203522504892, 5.40117416829746, &#10;5.44031311154599, 5.47945205479452, 5.51859099804305, 5.55772994129159, &#10;5.59686888454012, 5.63600782778865, 5.67514677103718, 5.71428571428571, &#10;5.75342465753425, 5.79256360078278, 5.83170254403131, 5.87084148727984, &#10;5.90998043052838, 5.94911937377691, 5.98825831702544, 6.02739726027397, &#10;6.0665362035225, 6.10567514677104, 6.14481409001957, 6.1839530332681, &#10;6.22309197651663, 6.26223091976517, 6.3013698630137, 6.34050880626223, &#10;6.37964774951076, 6.4187866927593, 6.45792563600783, 6.49706457925636, &#10;6.53620352250489, 6.57534246575342, 6.61448140900196, 6.65362035225049, &#10;6.69275929549902, 6.73189823874755, 6.77103718199609, 6.81017612524462, &#10;6.84931506849315, 6.88845401174168, 6.92759295499021, 6.96673189823875, &#10;7.00587084148728, 7.04500978473581, 7.08414872798434, 7.12328767123288, &#10;7.16242661448141, 7.20156555772994, 7.24070450097847, 7.27984344422701, &#10;7.31898238747554, 7.35812133072407, 7.3972602739726, 7.43639921722114, &#10;7.47553816046967, 7.5146771037182, 7.55381604696673, 7.59295499021526, &#10;7.6320939334638, 7.67123287671233, 7.71037181996086, 7.74951076320939, &#10;7.78864970645793, 7.82778864970646, 7.86692759295499, 7.90606653620352, &#10;7.94520547945205, 7.98434442270059, 8.02348336594912, 8.06262230919765, &#10;8.10176125244618, 8.14090019569472, 8.18003913894325, 8.21917808219178, &#10;8.25831702544031, 8.29745596868884, 8.33659491193738, 8.37573385518591, &#10;8.41487279843444, 8.45401174168297, 8.49315068493151, 8.53228962818004, &#10;8.57142857142857, 8.6105675146771, 8.64970645792564, 8.68884540117417, &#10;8.7279843444227, 8.76712328767123, 8.80626223091977, 8.8454011741683, &#10;8.88454011741683, 8.92367906066536, 8.96281800391389, 9.00195694716243, &#10;9.04109589041096, 9.08023483365949, 9.11937377690802, 9.15851272015655, &#10;9.19765166340509, 9.23679060665362, 9.27592954990215, 9.31506849315068, &#10;9.35420743639922, 9.39334637964775, 9.43248532289628, 9.47162426614481, &#10;9.51076320939335, 9.54990215264188, 9.58904109589041, 9.62818003913894, &#10;9.66731898238747, 9.70645792563601, 9.74559686888454, 9.78473581213307, &#10;9.8238747553816, 9.86301369863014, 9.90215264187867, 9.9412915851272, &#10;9.98043052837573, 10.0195694716243, 10.0587084148728, 10.0978473581213, &#10;10.1369863013699, 10.1761252446184, 10.2152641878669, 10.2544031311155, &#10;10.293542074364, 10.3326810176125, 10.3718199608611, 10.4109589041096, &#10;10.4500978473581, 10.4892367906067, 10.5283757338552, 10.5675146771037, &#10;10.6066536203522, 10.6457925636008, 10.6849315068493, 10.7240704500978, &#10;10.7632093933464, 10.8023483365949, 10.8414872798434, 10.880626223092, &#10;10.9197651663405, 10.958904109589, 10.9980430528376, 11.0371819960861, &#10;11.0763209393346, 11.1154598825832, 11.1545988258317, 11.1937377690802, &#10;11.2328767123288, 11.2720156555773, 11.3111545988258, 11.3502935420744, &#10;11.3894324853229, 11.4285714285714, 11.46771037182, 11.5068493150685, &#10;11.545988258317, 11.5851272015656, 11.6242661448141, 11.6634050880626, &#10;11.7025440313112, 11.7416829745597, 11.7808219178082, 11.8199608610568, &#10;11.8590998043053, 11.8982387475538, 11.9373776908023, 11.9765166340509, &#10;12.0156555772994, 12.0547945205479, 12.0939334637965, 12.133072407045, &#10;12.1722113502935, 12.2113502935421, 12.2504892367906, 12.2896281800391, &#10;12.3287671232877, 12.3679060665362, 12.4070450097847, 12.4461839530333, &#10;12.4853228962818, 12.5244618395303, 12.5636007827789, 12.6027397260274, &#10;12.6418786692759, 12.6810176125245, 12.720156555773, 12.7592954990215, &#10;12.7984344422701, 12.8375733855186, 12.8767123287671, 12.9158512720157, &#10;12.9549902152642, 12.9941291585127, 13.0332681017613, 13.0724070450098, &#10;13.1115459882583, 13.1506849315068, 13.1898238747554, 13.2289628180039, &#10;13.2681017612524, 13.307240704501, 13.3463796477495, 13.385518590998, &#10;13.4246575342466, 13.4637964774951, 13.5029354207436, 13.5420743639922, &#10;13.5812133072407, 13.6203522504892, 13.6594911937378, 13.6986301369863, &#10;13.7377690802348, 13.7769080234834, 13.8160469667319, 13.8551859099804, &#10;13.894324853229, 13.9334637964775, 13.972602739726, 14.0117416829746, &#10;14.0508806262231, 14.0900195694716, 14.1291585127202, 14.1682974559687, &#10;14.2074363992172, 14.2465753424658, 14.2857142857143, 14.3248532289628, &#10;14.3639921722113, 14.4031311154599, 14.4422700587084, 14.4814090019569, &#10;14.5205479452055, 14.559686888454, 14.5988258317025, 14.6379647749511, &#10;14.6771037181996, 14.7162426614481, 14.7553816046967, 14.7945205479452, &#10;14.8336594911937, 14.8727984344423, 14.9119373776908, 14.9510763209393, &#10;14.9902152641879, 15.0293542074364, 15.0684931506849, 15.1076320939335, &#10;15.146771037182, 15.1859099804305, 15.2250489236791, 15.2641878669276, &#10;15.3033268101761, 15.3424657534247, 15.3816046966732, 15.4207436399217, &#10;15.4598825831703, 15.4990215264188, 15.5381604696673, 15.5772994129159, &#10;15.6164383561644, 15.6555772994129, 15.6947162426614, 15.73385518591, &#10;15.7729941291585, 15.812133072407, 15.8512720156556, 15.8904109589041, &#10;15.9295499021526, 15.9686888454012, 16.0078277886497, 16.0469667318982, &#10;16.0861056751468, 16.1252446183953, 16.1643835616438, 16.2035225048924, &#10;16.2426614481409, 16.2818003913894, 16.320939334638, 16.3600782778865, &#10;16.399217221135, 16.4383561643836, 16.4774951076321, 16.5166340508806, &#10;16.5557729941292, 16.5949119373777, 16.6340508806262, 16.6731898238748, &#10;16.7123287671233, 16.7514677103718, 16.7906066536204, 16.8297455968689, &#10;16.8688845401174, 16.9080234833659, 16.9471624266145, 16.986301369863, &#10;17.0254403131115, 17.0645792563601, 17.1037181996086, 17.1428571428571, &#10;17.1819960861057, 17.2211350293542, 17.2602739726027, 17.2994129158513, &#10;17.3385518590998, 17.3776908023483, 17.4168297455969, 17.4559686888454, &#10;17.4951076320939, 17.5342465753425, 17.573385518591, 17.6125244618395, &#10;17.6516634050881, 17.6908023483366, 17.7299412915851, 17.7690802348337, &#10;17.8082191780822, 17.8473581213307, 17.8864970645793, 17.9256360078278, &#10;17.9647749510763, 18.0039138943249, 18.0430528375734, 18.0821917808219, &#10;18.1213307240705, 18.160469667319, 18.1996086105675, 18.238747553816, &#10;18.2778864970646, 18.3170254403131, 18.3561643835616, 18.3953033268102, &#10;18.4344422700587, 18.4735812133072, 18.5127201565558, 18.5518590998043, &#10;18.5909980430528, 18.6301369863014, 18.6692759295499, 18.7084148727984, &#10;18.747553816047, 18.7866927592955, 18.825831702544, 18.8649706457926, &#10;18.9041095890411, 18.9432485322896, 18.9823874755382, 19.0215264187867, &#10;19.0606653620352, 19.0998043052838, 19.1389432485323, 19.1780821917808, &#10;19.2172211350294, 19.2563600782779, 19.2954990215264, 19.3346379647749, &#10;19.3737769080235, 19.412915851272, 19.4520547945205, 19.4911937377691, &#10;19.5303326810176, 19.5694716242661, 19.6086105675147, 19.6477495107632, &#10;19.6868884540117, 19.7260273972603, 19.7651663405088, 19.8043052837573, &#10;19.8434442270059, 19.8825831702544, 19.9217221135029, 19.9608610567515, &#10;20), y = c(0.0217667400980039, 0.0227118960613568, 0.0236399642746827, &#10;0.024547462294379, 0.0254284817500052, 0.0262697990497476, 0.0270761925341352, &#10;0.0278447201687443, 0.0285725349842736, 0.0292431898522125, 0.0298599237216537, &#10;0.0304262344415342, 0.0309405476728982, 0.0313967502030459, 0.0317805594200756, &#10;0.0321087524689367, 0.0323813637754389, 0.0325985887432935, 0.0327456085503099, &#10;0.0328338748111955, 0.032871094252979, 0.032858917719392, 0.0327952170079012, &#10;0.0326741710852976, 0.0325130172841641, 0.0323142824586605, 0.0320805363813515, &#10;0.0318066450090944, 0.0315032349150802, 0.0311762923391493, 0.0308283666713486, &#10;0.0304606397730375, 0.0300742685852501, 0.0296767520098838, 0.0292699115474762, &#10;0.0288554692880644, 0.0284340054317109, 0.0280086390955194, 0.0275806684358679, &#10;0.0271507171781247, 0.0267191796498216, 0.0262860885684517, 0.0258516152315375, &#10;0.025415436557538, 0.0249771598811644, 0.0245350861062508, 0.0240885560431356, &#10;0.023637102870035, 0.023179973519991, 0.0227156094045636, 0.022241184162465, &#10;0.0217581061784532, 0.0212659006410898, 0.0207641513896269, 0.020249460771043, &#10;0.0197238756753159, 0.0191886405031699, 0.0186441419420248, 0.0180900824959544, &#10;0.0175265997231419, 0.0169574756929679, 0.0163840336778012, 0.0158076842167135, &#10;0.0152307532198712, 0.0146567117250279, 0.0140876768708212, 0.0135257717396179, &#10;0.0129746875992836, 0.0124411735382691, 0.0119248408581489, 0.0114281180660412, &#10;0.0109534391260541, 0.0105125675916208, 0.0101040127121235, 0.0097278039838056, &#10;0.00938617839440894, 0.00908600262449913, 0.00883795563273938, &#10;0.00863315495962393, 0.00847325872834633, 0.00835985054125376, &#10;0.0083104707966061, 0.00831574579983596, 0.00837242006745159, &#10;0.0084811812661746, 0.00864895114585229, 0.0088847277032945, &#10;0.0091733846139949, 0.00951451815849311, 0.00990760864554539, &#10;0.0103676944983387, 0.0108803185070754, 0.0114398691731338, 0.0120446169019066, &#10;0.0126975354786556, 0.0134003348410125, 0.014138322350119, 0.0149085888122459, &#10;0.0157081161739461, 0.0165397010226779, 0.0173908415086893, 0.0182558660392905, &#10;0.0191308446396648, 0.0200115146141704, 0.0208896684310384, 0.0217595621107009, &#10;0.0226168398271489, 0.0234571367239625, 0.0242658576551506, 0.0250415384796808, &#10;0.025781743996249, 0.0264824465880188, 0.0271330271813973, 0.0277197602228349, &#10;0.0282514207578494, 0.0287250463560853, 0.0291378145433322, 0.0294643336036007, &#10;0.0297185497373367, 0.0299034233614394, 0.0300178272409036, 0.0300510165057044, &#10;0.0299933390096322, 0.029864296171915, 0.0296646737354628, 0.0293954446422498, &#10;0.02903609387537, 0.0286087954739586, 0.0281202389214091, 0.0275730852809592, &#10;0.0269631864592354, 0.0262908755920268, 0.025574205859082, 0.0248169722677519, &#10;0.024023046736675, 0.0231883685262104, 0.0223289370148913, 0.0214507109581384, &#10;0.0205578894465608, 0.0196543161993343, 0.0187475783069642, 0.0178440120329414, &#10;0.0169474708137715, 0.0160617456778675, 0.0151982413151739, 0.0143579764501946, &#10;0.0135431035900457, 0.0127565541603801, 0.0120065112945572, 0.0113013352426194, &#10;0.010635019980095, 0.0100094469431421, 0.00942639127892244, 0.0089039890191574, &#10;0.00843070966487366, 0.00800519340240519, 0.00762818830262024, &#10;0.00730797543721608, 0.00704980283664362, 0.00684141502281347, &#10;0.00668265968608497, 0.00657330236985191, 0.00652994734551145, &#10;0.00653626373538692, 0.00658916348197513, 0.00668767640137901, &#10;0.00683740677506747, 0.00703949814277543, 0.00728169259995597, &#10;0.00756244741730784, 0.00788017305139687, 0.00824447686134098, &#10;0.00864128336177512, 0.00906733428314206, 0.00952075397149063, &#10;0.0100031836945359, 0.010512665615019, 0.0110414546541446, 0.0115878311927959, &#10;0.0121501195536992, 0.0127308183673382, 0.01332332344487, 0.0139262209784856, &#10;0.0145387177488175, 0.015161575268352, 0.0157951749967284, 0.016438040193859, &#10;0.0170909905985346, 0.0177550383014629, 0.0184378178562425, 0.019138615981402, &#10;0.0198599761627206, 0.0206051085684539, 0.0213835326226107, 0.0222057991861808, &#10;0.0230705544613072, 0.0239831651499172, 0.0249491866774483, 0.0260019858815712, &#10;0.0271307566962957, 0.0283404432091247, 0.0296377725554787, 0.0310483777483443, &#10;0.0325909750094088, 0.0342501536507845, 0.0360320721161026, 0.0379427304435736, &#10;0.0400427505040893, 0.0422911469270196, 0.0446880555428848, 0.0472364098981542, &#10;0.0499665903504324, 0.0528865496293085, 0.0559605500295509, 0.0591864336230866, &#10;0.0625614652392627, 0.0661323901936574, 0.0698369942423732, 0.0736642649820459, &#10;0.0776053170213216, 0.0816654985557755, 0.0858243890440125, 0.0900462454820139, &#10;0.0943168565446659, 0.0986215759610166, 0.102937806780721, 0.107234576343045, &#10;0.111494854705369, 0.115701725627134, 0.119818728151249, 0.123810263483841, &#10;0.127676838260079, 0.131403340542788, 0.134975040188428, 0.138301149786118, &#10;0.141429558148372, 0.144353043040705, 0.147063324586097, 0.149509011012514, &#10;0.151678338013187, 0.153617609445566, 0.155327745461799, 0.156810642833852, &#10;0.157987822962458, 0.15895731472489, 0.159731001895812, 0.160320192816835, &#10;0.16070927729284, 0.160927335641176, 0.161026262422779, 0.161024035352265, &#10;0.160939101818898, 0.160783676919756, 0.160610951636628, 0.160440813332408, &#10;0.160292840283904, 0.160202196337883, 0.160205045445256, 0.160307691092289, &#10;0.160525158759831, 0.160872099587403, 0.161427322298974, 0.162146341126922, &#10;0.16303494787568, 0.1640978213177, 0.165374536988198, 0.166861439709314, &#10;0.168517967752217, 0.170337253164731, 0.172312150290867, 0.174476220294316, &#10;0.17675201562322, 0.17912237546253, 0.181569351447975, 0.184077063209181, &#10;0.186601937519015, 0.189110996532675, 0.19158078566652, 0.193986444864657, &#10;0.196248038237016, 0.198366671014338, 0.2003193238609, 0.202083414374645, &#10;0.203583109911179, 0.204784727823111, 0.20571235693773, 0.206350205706743, &#10;0.206678507619118, 0.206556713002678, 0.206095969055307, 0.205290936474901, &#10;0.20413741111321, 0.202556572638439, 0.20056355298721, 0.198228613635869, &#10;0.195558644146613, 0.192555647948946, 0.189126333152985, 0.185410492636506, &#10;0.181423717899618, 0.177182210388674, 0.172658716028772, 0.167900183255391, &#10;0.16296802586402, 0.157882368007903, 0.152661212543965, 0.147304635977954, &#10;0.141882524743064, 0.136414358489409, 0.130919304274034, 0.125422474679471, &#10;0.119957697079395, 0.11454052598248, 0.109185666974252, 0.103909993572189, &#10;0.0987683404859644, 0.093741308688079, 0.0888376270971578, 0.0840654236882268, &#10;0.0794666168513133, 0.0750427325158831, 0.0707727313029462, 0.0666591512693038, &#10;0.0627091344513033, 0.0589774733641864, 0.0554044324211085, 0.0519885093076509, &#10;0.0487279204345587, 0.0456550213639223, 0.0427535393057037, 0.0399945562439461, &#10;0.0373743649778989, 0.0348937863392637, 0.0325878723180957, 0.0304033659564744, &#10;0.0283361981437062, 0.0263823330082348, 0.0245619867105529, 0.0228595658836749, &#10;0.0212541351296348, 0.0197423154392725, 0.0183242669772017, 0.0170220160814398, &#10;0.0158005734119126, 0.0146575124941681, 0.0135905002875402, 0.0126143097870055, &#10;0.0117191044664318, 0.0108918081116421, 0.0101309359870987, 0.0094379000631823, &#10;0.00883019944826195, 0.00828380391399826, 0.00779783511360308, &#10;0.00737145971429946, 0.00701812393006059, 0.0067311252542151, &#10;0.00650097052493056, 0.00632717374230014, 0.00621198669319391, &#10;0.00617143557767508, 0.0061855594050197, 0.00625405591658972, &#10;0.00637663424018689, 0.00656643085439909, 0.00681747969859399, &#10;0.00712155184640286, 0.00747843433306984, 0.0078907086989986, &#10;0.00837340291572697, 0.00890806427672648, 0.009494513737272, &#10;0.010132574128414, 0.010835143782014, 0.0115961949265227, 0.0124081127363209, &#10;0.0132707231223347, 0.0141867435373067, 0.0151701717571193, 0.0162034512423126, &#10;0.0172863373562776, 0.0184185690183941, 0.019612438019485, 0.0208613703536178, &#10;0.0221579723104729, 0.0235016744919049, 0.0248946535533411, 0.0263479485897112, &#10;0.0278446638511839, 0.0293835133691365, 0.0309631084740302, 0.0325912348690895, &#10;0.0342591472113346, 0.0359590659180276, 0.0376882779600787, 0.0394452615206507, &#10;0.0412293518464991, 0.043027543361404, 0.0448353514472685, 0.0466481043572906, &#10;0.0484577337141087, 0.0502538712990142, 0.0520305447610213, 0.0537813149370389, &#10;0.0554964047538021, 0.0571515262745735, 0.0587502027060215, 0.0602849490847391, &#10;0.0617482500354599, 0.0631066082295335, 0.0643579690671984, 0.0655057363703737, &#10;0.0665429490020974, 0.0674531640036809, 0.0681929810868339, 0.0687958988058021, &#10;0.0692572514303207, 0.0695726946039624, 0.069695167044965, 0.0696433847901905, &#10;0.0694350823030356, 0.0690697581056959, 0.068535116834938, 0.0677961078509799, &#10;0.0669066143489608, 0.0658704054221769, 0.0646916508125923, 0.0633398550663818, &#10;0.0618485040466386, 0.0602410483121883, 0.0585250659382392, 0.0567013372657685, &#10;0.0547654519871085, 0.0527582945347469, 0.0506891518402187, 0.0485673654999008, &#10;0.0463957642449994, 0.0441983057887947, 0.0419884488508511, 0.0397750260303709, &#10;0.0375684836047041, 0.0353879440268247, 0.0332384672584751, 0.0311267921300081, &#10;0.0290593901126105, 0.0270601872571371, 0.0251287992452735, 0.0232632081303122, &#10;0.0214669543868477, 0.0197503127220639, 0.0181339971366657, 0.0165964380609598, &#10;0.0151383742283497, 0.013760299683845, 0.012485591080821, 0.0112979221667829, &#10;0.0101874296294859, 0.00915261833812058, 0.00819857079937494, &#10;0.00733611052320258, 0.00654025398573884, 0.00580844808959148, &#10;0.0051380838601794, 0.00454221827421074, 0.00400446040436533, &#10;0.00351615254678973, 0.0030745243393893, 0.00268072850148871, &#10;0.00233768602660161, 0.00202984861125548, 0.00175479227131742, &#10;0.00151014383233181, 0.00130072566668639, 0.00111721604283242, &#10;0.000955112980316734, 0.000812623159815952, 0.000689590092917738, &#10;0.0005860912862362, 0.000495639552545513, 0.000417014922709681, &#10;0.000349049284657548, 0.000292958836410902, 0.00024515801710463, &#10;0.000204040052154968, 0.000168884867932852, 0.000139474416390484, &#10;0.000115548025328669, 9.51646202245106e-05, 7.79159203801693e-05, &#10;6.34177645960676e-05, 5.18748729574288e-05, 4.22967579760247e-05, &#10;3.42684953287067e-05, 2.75891372479148e-05, 2.21737075433437e-05, &#10;1.79085413159829e-05, 1.43641470627961e-05, 1.14430817192601e-05, &#10;9.05508126397165e-06, 7.22026287729582e-06, 5.73639534240461e-06, &#10;4.52392445545382e-06, 3.54207412160509e-06, 2.77052765004019e-06, &#10;2.18172878629954e-06, 1.7042057736597e-06, 1.32082991902191e-06, &#10;1.01596818604684e-06, 7.89867561242482e-07, 6.11506981747829e-07, &#10;4.69391759719292e-07, 3.57363517027134e-07), bw = 0.715374250961732, &#10;    n = 35L, call = density.default(x = dat.prior, from = 0, &#10;        to = 20), data.name = &quot;dat.prior&quot;, has.na = FALSE), .Names = c(&quot;x&quot;, &#10;&quot;y&quot;, &quot;bw&quot;, &quot;n&quot;, &quot;call&quot;, &quot;data.name&quot;, &quot;has.na&quot;), class = &quot;density&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-03T15:41:18.847" Id="78435" LastActivityDate="2013-12-08T21:19:27.883" LastEditDate="2013-12-04T13:53:23.013" LastEditorUserId="31334" OwnerUserId="31334" PostTypeId="1" Score="1" Tags="&lt;group-differences&gt;&lt;likelihood&gt;" Title="How can I use likelihoods to compare these three groups? Should I want to do this?" ViewCount="78" />
  <row AcceptedAnswerId="103303" AnswerCount="2" Body="&lt;p&gt;I am dealing with some data which plotted looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/k118s.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to add a level of significance to my claim that the values of the two time courses are different over the latter half of the experiment (data is sampled at 60 Hz, standard errors of the mean are shaded).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried two approaches:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Compare per-interval means over this interval - got $p = 2.8 \times 10^{-1}$ &lt;/li&gt;&#10;&lt;li&gt;Compare all values in the interval - got $p = 5.6 \times 10^{-53}$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now, the first approach tells me that my claim is pretty much not significantly supported by my data. The second one tells me that it is - but the p value I get is ridiculously low, though the approach seems sensible and I would not call it inflated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please tell me what you think is better or how you would advise me to go about this? I have tried fitting a mixed model to this, but the nonlinear time course makes this a bit difficult, and it will surely take me some time to test multiple models with multiple nonlinear components.&lt;/p&gt;&#10;&#10;&lt;p&gt;I plan to do this eventually, but I am currently looking for a somewhat faster (even if less accurate) approach.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-03T20:03:27.783" Id="78468" LastActivityDate="2014-06-13T17:18:47.350" OwnerUserId="32504" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;statistical-significance&gt;&lt;mixed-model&gt;&lt;t-test&gt;" Title="Significance of time course difference over interval" ViewCount="68" />
  <row AcceptedAnswerId="78598" AnswerCount="4" Body="&lt;p&gt;Lets say you want to create a random forest model that predicts whether a user will click on your search engine ad. Lets say the training data set marks the independent variable (i.e. whether the user clicked or not) as a 0 if it's a miss, and a 1 if it's a hit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Should this type of model be a classifier or a regression model? Or can it be either?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying this both ways, and I am finding that in the classifier version of this model, the model predicts a miss nearly every time. I'm not sure if there is a problem in the way I've defined the model, or if this is by design, because for a given data point, if you repeated that data point 100 times it might only be a hit 10% of the time, even for the likely-to-be-a-hit data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, there is almost no single point where the user is actually likely to click on the ad, so I think that maybe given that the probability is always less than 0.5, the model is simply predicting a miss every time &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-12-03T20:13:50.260" Id="78469" LastActivityDate="2013-12-05T16:14:14.730" LastEditDate="2013-12-03T22:31:44.083" LastEditorUserId="88" OwnerUserId="35639" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;classification&gt;&lt;random-forest&gt;&lt;binary&gt;" Title="Classifier or regression for binary system?" ViewCount="169" />
  <row Body="&lt;p&gt;This is is close as I can get for your ARMA model ... plus a constant which depends on the actual data. Please change the sign of the coefficients to express them in your right-hand side terms. If you wish to post the data I will try and report the constant to you.         &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       1        1.000000000000000&#10;       2       -1.154041993068264        lag1               a&#10;       3   7.326340000632480E-001        lag2               b&#10;       4  -4.651066263382714E-001        lag3               c&#10;       5   2.952690891292149E-001        ....               ...&#10;       6  -1.874491354414452E-001&#10;       7   1.190005309440590E-001&#10;       8  -8.287885419594633E-002&#10;       9   9.676872930271280E-003&#10;      10  -6.143282623675761E-003&#10;      11   3.900012087200063E-003&#10;      12  -2.475890368723683E-003&#10;      13   1.571798492127172E-003&#10;      14  -9.978432531028447E-004&#10;      15  -6.224545775121130E-003&#10;      16  -3.620878821088098E-002&#10;      17   2.298684926867434E-002&#10;      18  -1.459301085204409E-002&#10;      19   9.264252061637956E-003&#10;      20  -5.881333683072044E-003&#10;      21   3.733715972050327E-003&#10;      22  -8.784687417549196E-003&#10;      23  -3.198550250991580E-002&#10;      24   2.030573132401301E-002&#10;      25  -1.289092533328742E-002&#10;      26   8.183697168881411E-003&#10;      27  -5.195352359928535E-003&#10;      28   3.298226411218046E-003&#10;      29  -8.093270907345322E-003&#10;      30  -2.999450789465562E-002&#10;      31   1.904176488445195E-002&#10;      32  -1.208850670890158E-002&#10;      33   7.674288351836509E-003&#10;      34  -4.871958391996022E-003&#10;      35   3.092922429434164E-003&#10;      36  -7.574829155282404E-003&#10;      37  -2.805089569614140E-002&#10;      38   1.780787877968092E-002&#10;      39  -1.130518433589381E-002&#10;      40   7.177002631799651E-003&#10;      41  -4.556260671780234E-003&#10;      42   2.892504346763182E-003&#10;      43  -7.084596012492448E-003&#10;      44  -2.623640052502665E-002&#10;      45   1.665596155024383E-002&#10;      46  -1.057389922442183E-002&#10;      47   6.712752336210348E-003&#10;      48  -4.261535217133788E-003&#10;      49   2.705400333170054E-003&#10;      50  -6.626297855728592E-003&#10;      51  -2.453914557980987E-002&#10;      52   1.557847330708616E-002&#10;      53  -9.889864738374296E-003&#10;      54   6.278498708782277E-003&#10;      55  -3.985852898799137E-003&#10;      56   2.530385696925117E-003&#10;&#10;  ....................................&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-03T20:42:49.427" Id="78473" LastActivityDate="2013-12-03T21:03:56.027" LastEditDate="2013-12-03T21:03:56.027" LastEditorUserId="3382" OwnerUserId="3382" ParentId="78458" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a survey data set with missing values and I generated 10 multiple imputations in which the missing values were imputed. There are several categorical variables in the data sets and I'd like to see if there's any association among these variables using chi-squared test.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I've been searching how to run this analysis in either SAS or Stata but didn't find a solution. It seems that mi estimate doesn't support svy: tab in Stata. I can run chi-squared test on each imputed data set using in SAS using proc surveyfreq, however, I don't know how to combine the results. Can anyone please guide me through the steps on how to test the associations between categorical variables? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance!&#10;Jin&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-03T21:46:13.510" Id="78479" LastActivityDate="2014-01-03T15:48:50.953" OwnerUserId="32145" PostTypeId="1" Score="3" Tags="&lt;chi-squared&gt;&lt;data-imputation&gt;" Title="How to run chi-squared test on imputed data" ViewCount="450" />
  <row Body="&lt;p&gt;One important &quot;take away&quot; is that if treatment assignments are disproportionate between subgroups, one must take subgroups into account when analyzing the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;A second important &quot;take away&quot; is that observational studies are especially prone to delivering wrong answers due to the unknown presence of Simpson's paradox.  That's because we cannot correct for the fact that Treatment A tended to be given to the more difficult cases if we don't know that it was.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In a properly randomized study we can either (1) allocate treatment randomly so that giving an &quot;unfair advantage&quot; to one treatment is highly unlikely and will automatically get taken care of in the data analysis or, (2) if there is an important reason to do so, allocate the treatments randomly but disproportionately based on some known issue and then take that issue into account during the analysis.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-03T22:21:22.710" Id="78481" LastActivityDate="2013-12-03T22:21:22.710" OwnerUserId="8871" ParentId="78255" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="78490" AnswerCount="2" Body="&lt;p&gt;I have two vectors of data of different sample size. Both are the same experiment, but executed in a different environment. Since the sample sizes are different and they don't share any subject, is it safe to assume there is no dependency possible?  From what I gathered, there can be no correlation nor can I create a scatterplot, which automatically means there can be no linear dependency.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know it must be a simple question, but it's surprisingly hard to find something about it. I might not be using the correct terms.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-04T00:58:29.970" Id="78489" LastActivityDate="2013-12-04T13:17:36.177" OwnerUserId="35521" PostTypeId="1" Score="3" Tags="&lt;paired-data&gt;&lt;dependence&gt;" Title="Can non-paired data have dependency?" ViewCount="76" />
  
  
  
  <row AcceptedAnswerId="78530" AnswerCount="1" Body="&lt;p&gt;I'm learning Bayesian Linear Regression from a book, the linear model is  $$p(w|x,\phi,\sigma^2)=Norm_w[\phi^Tx,\sigma^2]$$, as put in the book, we use Bayes approach to do the parameters estimation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here comes the problem: I thought it is pretty clear that we should introduce a &lt;strong&gt;conjugate&lt;/strong&gt; prior for the parameters $\phi$ and $\sigma^2$, which should have a &lt;strong&gt;normal-scaled inverse gamma distribution&lt;/strong&gt;, right? But the book first assume that $\sigma^2$ is known, and introduce a prior distribution for $\phi$ alone, which is a 0-mean Gaussian, and does the estimation for $\phi$. After all that, it assumes $\sigma^2$ is not known, and estimates it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why should we separate them?&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE&lt;/p&gt;&#10;&#10;&lt;p&gt;The book is &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt;, p. 232, Section &quot;Baysian Linear Regression.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I just found this article which also assumes one is known and later assumes it is unknown.&#10;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.4002&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;Bayesian Linear Regression&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-04T13:12:21.920" Id="78526" LastActivityDate="2013-12-04T13:56:01.913" LastEditDate="2013-12-04T13:56:01.913" LastEditorUserId="22311" OwnerUserId="30540" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;bayesian&gt;" Title="Bayesian Linear Regression is so hard to understand?" ViewCount="175" />
  <row AcceptedAnswerId="78537" AnswerCount="1" Body="&lt;p&gt;I have data from survey, and &#10;Trying to build a linear regression model using R like&#10;A~ B&#10;however, want to control C, D, E, F, G. like Age, Sex, and other confounding variables.&#10;I tried to make some models using lm, glm, etc.&#10;and fitted it to my data.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in Multivariate regression models, I cannot get the graph like below.&#10;To use 'segmented' function of R to cut-off point, I guess, I should able to get estimated equation between A and B, while all other variables are controlled.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RuWC4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The image above describes what I want to do.&#10;linear model between A and B, but actual model includes confounders C, D, F, and somehow'controlled' them.&#10;The author described that &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;fitted the two linear regression models for high B and low B, and&#10;  calculated the sums of squares of residuals (=observed A -estimated&#10;  A), from the two models for each B. The models with the lowest&#10;  residual sums of squares were the best models.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It seems to be drawn from R(and author said so), but I cannot find any good approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm afraid this question seems to 'tool specific' question/&#10;If so, I'll amend this question, and update it with any help I got.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-04T13:31:46.130" Id="78529" LastActivityDate="2013-12-04T14:28:40.970" LastEditDate="2013-12-04T13:38:20.203" LastEditorUserId="35074" OwnerUserId="35074" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;data-visualization&gt;&lt;residuals&gt;" Title="How to make linear regression model while 'control' confounding variables?" ViewCount="564" />
  <row Body="&lt;p&gt;Here is one example I found for microarray data. The measured expression has been reported to be strongly correlated with position on the &quot;chips&quot;. This is a case where randomizing the position of the samples may lead to increased chance of making a labeling error so those doing the technical work may choose not to randomize if they do not think it is important.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Random assignment of experimental units to treatments controls the&#10;  likelihood that any factor other than the treatment is the cause of&#10;  the association (1,2)⁠. In some microarray platforms such as Illumina®&#10;  and NimbleGenTM, multiple biological samples can be hybridized to a&#10;  single chip. Chip and sample position effects may affect accuracy and&#10;  reproducibility of microarray experiments unless balance and&#10;  randomization is considered in the experimental design (4). Our aim&#10;  was to compare the impact of these effects in a confounded and a&#10;  randomized experiment.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://churchill.jax.org/publications/presentations/CTC09%20RAV%20Poster.pdf&quot; rel=&quot;nofollow&quot;&gt;Importance of Randomization in Microarray&#10;Experimental Designs with Illumina Platforms&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Ricardo A. Verdugo, Christian F. Deschepper, and Gary A. Churchill.&#10;The Jackson Laboratory, Bar Harbor, ME 04609,&#10;Institut de Recherches Cliniques, Montreal, QC, Canada.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-04T14:12:03.710" Id="78538" LastActivityDate="2013-12-04T14:12:03.710" OwnerUserId="31334" ParentId="74262" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="78587" AnswerCount="1" Body="&lt;p&gt;I´ve been researching about automatic determination of parameters for DBSCAN (a density-based clustering algorithm -- &lt;a href=&quot;http://en.wikipedia.org/wiki/DBSCAN&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/DBSCAN&lt;/a&gt;), especially &lt;em&gt;eps&lt;/em&gt;, and have found the following paper, which proposes a method for determining the best &lt;em&gt;eps&lt;/em&gt; from the data themselves: &lt;a href=&quot;http://www.joics.com/publishedpapers/2012_9_7_1967_1973.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.joics.com/publishedpapers/2012_9_7_1967_1973.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the paper claims, in page 1969, section 3.1, about a distance matrix DIST$_{n \times n}$:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;We calculate the value of each element in DIST$_{n\times n}$ successively, and then sort them in an ascending order line by line. We use DIST$_{n \times i}$ to express the $i$th column value in DIST$_{n \times n}$. As all data in DIST$_{n \times i}$ obeys the Poisson distribution[6], we use the maximum likelihood estimation in mathematics to estimate the value of parameter $\lambda$. That is to say, $\lambda$ can be obtained by means of the geometrical mean of the value of DIST$_{n \times i}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The paper goes to claim that this $\lambda$ can be used as &lt;em&gt;eps&lt;/em&gt;. Is there a reason for this claim? I have read about the Poisson distribution, but nowhere I have found found support for the claim that distances should follow this distribution (I admit to not being very knowledgeable about probability distributions, however). If this claim is not spurious, it would enormously simplify working with DBSCAN, as it would diminish the guesswork necessary to determine parameters. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-04T16:54:23.403" Id="78556" LastActivityDate="2014-11-28T03:17:40.393" LastEditDate="2013-12-04T18:08:57.880" LastEditorUserId="22047" OwnerUserId="35679" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;clustering&gt;&lt;poisson&gt;&lt;pdf&gt;&lt;distance&gt;" Title="Use of a Poisson distribution from a distance matrix to determine dbscan parameters" ViewCount="290" />
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GxSgU.jpg&quot; alt=&quot;enter image description here&quot;&gt;I have a question. I set up a hypothesis: The greater the number of franchise units within a franchised company, the greater the use of the intranet. On the one hand we have the number of franchise units of each franchise company, on the other hand if the company using intranet or not (answers: YES / NO). Could I establish a correlation between date or do some other statistical methods? Can I code Yes as 1, and No as 0? &#10;Thank you in advance for your reply.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-12-04T17:31:50.287" FavoriteCount="0" Id="78562" LastActivityDate="2013-12-04T17:39:09.060" LastEditDate="2013-12-04T17:39:09.060" LastEditorUserId="35683" OwnerUserId="35683" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="YES/ NO ANSWERS" ViewCount="94" />
  <row Body="&lt;p&gt;A nice overview of sampling techniques for this purpose can be found in&lt;br&gt;&#10;&lt;em&gt;[Non-Uniformity Issues and Workarounds in Bounded-Size Sampling, VLDB 2013]&lt;/em&gt; and&lt;br&gt;&#10;&lt;em&gt;[Maintaining bounded-size sample synopses of evolving datasets, VLDB 2007]&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer can be found in Section 2.3 from &lt;em&gt;[Non-Uniformity Issues and Workarounds in Bounded-Size Sampling, VLDB 2013]&lt;/em&gt;. It is a uniform sample, as long as we perform subsampling based on the original datasize rather than on the sample size. In this case, sample size is probabilistic.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-04T17:57:47.100" Id="78569" LastActivityDate="2013-12-04T17:57:47.100" OwnerUserId="35474" ParentId="78446" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Calibration and &quot;data dregding&quot; are not really differentiated by statistical tests, but by motives. If you are simply looking for patterns in an undirected manner, then you are data dredging. However, if you have a well formulated model that is known to capture the general features of a phenomenon, save for the specific values of some parameters, then what you are doing is calibrating. The difference is how well formluated your model is. If you are using the data to determine the &lt;em&gt;form&lt;/em&gt; of your model, then you are essentially data dredging or pattern-finding (less negative connotation). This is exploratory data analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example of calibration from my own field of environmental engineering is when we want to relate the cloudiness (turbidity) of a facility's effluent to the total mass of solids in the effluent. The general &lt;em&gt;form&lt;/em&gt; of the relationship is Total Solids = a*Turbidity. However, the coefficient a is variable, so we use field data to &lt;em&gt;calibrate&lt;/em&gt; it. Once a is determined, is generally stays pretty constant as long as the facility's process is not changed. So, that is a concrete example. &lt;/p&gt;&#10;&#10;&lt;p&gt;It all depends on whether or not you know the &lt;em&gt;form&lt;/em&gt; of the model for the underlying process under study. That's how I've differentiated it in the past.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Expansion after OP's edits&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree that if you found something unexpected and then happend to notice that another model fit, then you would be effectively data dredging. The problem with trying to justify your new model as statistically significant, &lt;em&gt;post hoc&lt;/em&gt;, is that its hard to quantify the probability that you would have another model that happened to fit and that you would have noticed it. It's &lt;em&gt;sort of&lt;/em&gt; like the situation when two friends randomly meet in a city and ask &lt;em&gt;what are the odds of that&lt;/em&gt;...this is not an exact analogy by any means, but the point is that when you are dealing with complex data and many data points and you don't have a clear testing plan at taht start, you are entering a situation where your risk of spurious identification is high, but the exact &quot;Type I&quot; risk you are taking on is no longer quantifiable. At this point, the most you can really say is that an alternative model is suggestive, but you really need to get a validation dataset to be any more confident (i.e., I wouldn't go to publication based on such finding, unless I could indepenedently verify). If you &lt;em&gt;feel&lt;/em&gt; that it is right, given your professional judgement, then by all means use it but know that you cannot use statsitics to back up your model at this point.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you &lt;em&gt;must&lt;/em&gt; get some kind of quantification, perhaps there is a way to simulate your data under the model you &lt;em&gt;think&lt;/em&gt; is correct and see how often the alternative model would provide a better fit and vice versa. I'm not familiar with your problem, but that is one &quot;gut check&quot; that might be helpful.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-04T18:31:37.437" Id="78571" LastActivityDate="2013-12-05T14:19:45.980" LastEditDate="2013-12-05T14:19:45.980" LastEditorDisplayName="user31668" OwnerDisplayName="user31668" ParentId="78532" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="80090" AnswerCount="1" Body="&lt;p&gt;I'm using the &lt;code&gt;igraph&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; to analyze network data.  I'm currently trying to calculate some centrality measures for the vertices of my graph and the corresponding centralization scores.  My network is both directed and weighted.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(igraph)&#10;set.seed(12152)&#10;&#10;m &amp;lt;- expand.grid(from = 1:4, to = 1:4)&#10;m &amp;lt;- m[m$from != m$to, ]&#10;m$weight &amp;lt;- sample(1:7, 12, replace = T)&#10;&#10;g &amp;lt;- graph.data.frame(m)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have no trouble using the &lt;code&gt;closeness&lt;/code&gt; function to obtain the closeness centrality for each vertex:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;closeness(g, mode = &quot;in&quot;)&#10;closeness(g, mode = &quot;out&quot;)&#10;closeness(g, mode = &quot;total&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, it appears that the &lt;code&gt;centralization.closeness&lt;/code&gt; function from &lt;code&gt;igraph&lt;/code&gt; does not work for directed graphs.  &lt;code&gt;igraph&lt;/code&gt; does include a way to calculate a custom centralization from the individual centrality scores in a graph (the &lt;code&gt;centralize.scores&lt;/code&gt; function), but that function requires the user to specify the theoretical maximum of the centrality measure, and it's not obvious to me what that would be in this weighted example (I believe the built-in &lt;code&gt;centralization.closeness.tmax&lt;/code&gt; function in &lt;code&gt;igraph&lt;/code&gt; assumes an unweighted graph).&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know how to calculate a centralization score in a weighted graph?  Is there a good way to accomplish this in R with &lt;code&gt;igraph&lt;/code&gt; or some other package?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-04T22:57:44.560" FavoriteCount="2" Id="78593" LastActivityDate="2013-12-19T03:24:07.107" LastEditDate="2013-12-04T23:26:39.567" LastEditorUserId="11091" OwnerUserId="11091" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;graph-theory&gt;&lt;networks&gt;&lt;social-network&gt;&lt;igraph&gt;" Title="Centralization measures in weighted graphs" ViewCount="409" />
  
  
  
  
  
  <row Body="&lt;p&gt;For the zero death rates, the package won't work as it has to take log of the death rates and obviously you can't take log of zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can set the values to missing but it is incorrect as you do have the values! &lt;/p&gt;&#10;&#10;&lt;p&gt;You can also try to put a very very very small number, but here again you are going to 'fake' the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;What you can do is to try to group the ages in such a way that you never have zero values.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-05T14:51:14.630" Id="78654" LastActivityDate="2013-12-05T14:51:14.630" OwnerUserId="34858" ParentId="67707" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;A Poisson GLM can incorporate both continuous and categorical variables (like any regression-type model). Why do you think it could not?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-12-05T15:28:36.567" Id="78660" LastActivityDate="2013-12-05T15:28:36.567" OwnerUserId="686" ParentId="78657" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I can suggest a strategy from a classification performance viewpoint. Since the classification problem relies not only on the amount of classes and how 'similar' they are, but also on the features which describe this classification problem, I can't say if it is necessarily 'optimal'. Assuming your features have similar values for A1 and A2 (and also B1 and B2), I believe a good strategy for you is to grow a classification tree. See this &lt;a href=&quot;http://en.wikipedia.org/wiki/Decision_tree_learning&quot; rel=&quot;nofollow&quot;&gt;Wikipedia article&lt;/a&gt;, the following &lt;a href=&quot;http://www.stat.cmu.edu/~cshalizi/350/lectures/22/lecture-22.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; and these &lt;a href=&quot;http://www.youtube.com/watch?v=p17C9q2M00Q&quot; rel=&quot;nofollow&quot;&gt;videos&lt;/a&gt; for in depth details. The idea behind this classification is what you intuitively phrased as a 'Hierarchical strategy'.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The idea behind this strategy is to iteratively subdivide the feature space into regions. At each iteration, choose the 'best' feature and value that divides the data (according to some purity measure). As you stated &lt;em&gt;&quot;split 'A1 and A2' from 'B1 and B2' then split A1 from A2 and B1 from B2&quot;&lt;/em&gt;. Provided that your feature space can be separated, this classification strategy should work.&lt;/p&gt;&#10;&#10;&lt;p&gt;The final output will look like the image below. The nodes contain decisions (based on your training set) and the leaves represent the class outcome. The algorithm is interpretable, so you can easily see how well your classification strategy worked.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/19bCU.png&quot; alt=&quot;image from Wikipedia article&quot;&gt;   &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-05T15:52:20.573" Id="78664" LastActivityDate="2013-12-05T16:29:12.407" LastEditDate="2013-12-05T16:29:12.407" LastEditorUserId="28740" OwnerUserId="27495" ParentId="78643" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Categorical variables can be represented several different ways in a regression model.  The most common, by far, is reference cell coding.  From your description (and my prior), I suspect that is what was used in your case.  The standard statistical output will give you two tests.  Let's say that A is the reference level, you will have a test of B vs. A, and a test of C vs. A (n.b., C can significantly differ from B, but not A, and not show up in these tests).  These tests are usually not what you really want to know.  You should test a multi-category variable by dropping &lt;em&gt;both&lt;/em&gt; dummy variables and performing a nested model test.  Unless you had an a-priori plan to test if a pre-specified level is necessary and it is not 'significant', you should retain the entire variable (i.e., all levels).  If you did have such an a-priori hypothesis (i.e., that was the point of your study, you can drop only the level in question and perform a nested model test.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It may help you to read about some of these topics.  Here are some references for further study:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Coding strategies for categorical variables:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm&quot;&gt;UCLA's stats help website&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;I discuss reference cell coding here: &lt;a href=&quot;http://stats.stackexchange.com/questions/21282/regression-based-for-example-on-days-of-week#21292&quot;&gt;Regression based for example on days of week&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Problems with modifying your model based on what you find, when you didn't have a pre-specified hypothesis:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;While it's not framed exactly like your situation, you may be able to get the idea from my answer here: &lt;a href=&quot;http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856&quot;&gt;Algorithms for automatic model selection&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Issues with multiple comparisons:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You might skim some of the CV threads categorized under the &lt;a href=&quot;/questions/tagged/multiple-comparisons&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;multiple-comparisons&amp;#39;&quot; rel=&quot;tag&quot;&gt;multiple-comparisons&lt;/a&gt; tag  &lt;/li&gt;&#10;&lt;li&gt;the Wikipedia page for &lt;a href=&quot;http://en.wikipedia.org/wiki/Multiple_comparisons&quot;&gt;multiple comparisons&lt;/a&gt;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Nested model tests:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Although discussed in terms of testing for moderation, my answer here should be clear enough to get the idea: &lt;a href=&quot;http://stats.stackexchange.com/questions/33059/testing-for-moderation-with-continuous-vs-categorical-moderators/33090#33090&quot;&gt;Testing for moderation with continuous vs. categorical moderators&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-12-05T16:10:39.320" Id="78665" LastActivityDate="2013-12-05T16:10:39.320" OwnerUserId="7290" ParentId="78644" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;Yes because the marginal distribution of $u_i$ is $N(\mu_i,\Sigma_{ii})$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The marginal distribution in this case is defined as the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\int ... \int\int...\int N(\mu,\Sigma) du_1 ... du_{i-1} du_{i+1} ... du_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which can be shown to equal the density of a $N(\mu_i,\Sigma_{ii})$ distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have the marginal, you can apply the law of the unconscious statistician which says that if X is a random variables with density $f_X$ then $E[g(X)] = \int g(x) f_X(x) dx$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-05T17:10:23.120" Id="78678" LastActivityDate="2013-12-05T22:06:32.807" LastEditDate="2013-12-05T22:06:32.807" LastEditorUserId="23801" OwnerUserId="23801" ParentId="78628" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The p-value applies to upper, lower, and double tailed tests. It is the probability that the test statistic would be at least as contradictory to your null hypothesis as you currently observe assuming your null hypothesis is true. So, for upper tail tests, you are comparing $H_o: a = b$ vs. $H_a: a&amp;gt;b$, in this case, the p-value is the probability that the test statistic would be at least as high as you observe, assuming a=b, so you calculate 1 - CDF of the test statistic under the null hypothesis and see if it meets your type I error rate requirement.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-05T20:58:23.213" Id="78705" LastActivityDate="2013-12-05T20:58:23.213" OwnerDisplayName="user31668" ParentId="78695" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I have no rigorous or philosophical basis for answering this, but I've heard the &quot;stats is not math&quot; complaint often from people, usually physics types. I think people want guarantees certainty from their math, and statistics (usually) offers only probabilistic conclusions with associated p values. Actually, this is exactly what I love about stats. We live in a fundamentally uncertain world, and we do the best we can to understand it. And we do a great job, all things considered.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-12-05T21:13:04.520" CreationDate="2013-12-05T21:13:04.520" Id="78708" LastActivityDate="2013-12-05T21:13:04.520" OwnerUserId="8844" ParentId="78579" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Okay, so, in the traditional Bernoulli Urn problem, we have an urn with a number N, possibly infinite, of coloured balls, and there are k possible colours. That one I grok.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, what if I don't actually &lt;em&gt;know&lt;/em&gt; what k is? That is, what if I have an urn with N balls and an unknown but finite and strictly positive number of possible colours?&lt;/p&gt;&#10;&#10;&lt;p&gt;The main question is, in fact, what my priors should be. What's the prior that there is exactly one colour? Exactly two? At least two? How do I update on the relative frequencies of each colour? Is this problem even solvable?&lt;/p&gt;&#10;&#10;&lt;p&gt;My first lines of thinking are to have a vector of parameters $\vec \theta \in \mathbb R^\infty$ such that the first parameter is the number of colours in the urn (let's call it $\alpha$) and the remaining are the relative frequencies of each colour. If $P(A=n|\vec\theta)$ is the probability that the colour of the next draw will be n given the knowledge contained by $\vec\theta$, we'd have:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$\vec\theta = (\alpha, p_1, p_2, p_3, ...)$&lt;/li&gt;&#10;&lt;li&gt;$\alpha \in \mathbb N^*$&lt;/li&gt;&#10;&lt;li&gt;$\left(\sum\limits_{n=1}^\infty P(\alpha = n) \right)= 1$&lt;/li&gt;&#10;&lt;li&gt;$\left(\sum\limits_{n=1}^\infty p_n\right) = 1$&lt;/li&gt;&#10;&lt;li&gt;$\forall n &amp;gt; \alpha : p_n = 0$&lt;/li&gt;&#10;&lt;li&gt;$\forall n \in \mathbb N^* : P(A=n|\vec\theta) = p_n$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;However, this is just wild speculation on my part. I'm mostly curious about whether this is even in principle solvable. What I'd want to know is a way to compute both the prior (objective/uninformative) and posterior distributions of $P(\vec\theta)$ or, in other words, the pdfs $P(\alpha)$, $P(p_1)$, $P(p_2)$, etc. How to start with them and how to update on them.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-12-05T23:54:18.870" Id="78717" LastActivityDate="2013-12-06T13:28:33.773" LastEditDate="2013-12-06T13:28:33.773" LastEditorUserId="31147" OwnerUserId="31147" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;conditional-probability&gt;&lt;bernoulli-distribution&gt;" Title="Unknown number of colours Bernoulli Urn" ViewCount="53" />
  <row AnswerCount="0" Body="&lt;p&gt;I am running an analysis looking at individual voter turnout where it is a 1 for voting and 0 for not voting.  I have some information on individuals including age, gender, and several years of past voting history.  I can also geocode the information and add all sort of other grouped or clustered data including school district level, county level, township level, city/village level, and block group and census block level data.  As I have several prior years of voter history data, I would like to add year specific data such as polling data, unemployment rates, personal income, etc.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is two fold, is GLMM the best solution to this problem?  If so, I am having trouble determining which level data (geographic level or year/time level) should be on the Data Structure subject line, and which data (if any) should be in the repeated measures line.  I believe (and correct me if I am wrong) that the geographic level data should be in the subject section and the year/time should be in the repeated measures section.  Any Thoughts?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-06T02:52:47.990" Id="78725" LastActivityDate="2013-12-06T17:35:24.900" LastEditDate="2013-12-06T17:35:24.900" LastEditorUserId="35751" OwnerUserId="35751" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;repeated-measures&gt;&lt;glmm&gt;&lt;census&gt;" Title="SPSS GLMM Data Structure setup advice" ViewCount="176" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was curious if there was any bounds or approaches to getting good estimates for a probability from the following generating process.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we have 2 sets of objects $A$ and $B$ where both sets are large and possibly infinite. The generating process is that $a \in A$ is drawn from a categorical distribution over $A$. $a$ then dictates another categorical distribution over $B$ from which a $b$ is drawn.&lt;/p&gt;&#10;&#10;&lt;p&gt;From this scenario I would like to estimate P($b_i$ | $b_j$) = $\sum_{a \in A} P(b_i | a) P(a | b_j)$. In other words given that $b_j$ has been generated what is the probability that $b_i$ could have been generated instead?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in either some bounds or procedures that could help me estimate this by sampling enough $a$. If someone is familiar with a model that could be used in this situation that would be awesome as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-06T05:20:11.600" Id="78729" LastActivityDate="2013-12-06T07:18:45.707" LastEditDate="2013-12-06T07:18:45.707" LastEditorUserId="2116" OwnerUserId="35757" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;graphical-model&gt;&lt;bounds&gt;" Title="Bounds (or model) for estimating probability from generating process" ViewCount="40" />
  <row AcceptedAnswerId="78740" AnswerCount="1" Body="&lt;p&gt;I have a binary response variable (it is a presence/absence variable) and a ordinal discrete predictor:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;response : [1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 ... ]&#10;predictor : [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 ... ]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd like to know whether there is a higher probability to encounter whatever is recorded in the response variable when the levels of the predictor is higher or lower.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I have better to perform a Binary GLM: &lt;code&gt;response ~ predictor&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Or do I have better to count the number of occurrences (number of one in &lt;code&gt;response&lt;/code&gt;) in each level of the predictor and run a Poisson GLM: &lt;code&gt;response2 ~ predictor2&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;response2 : [15 22 21]&#10;predictor2 : [1 2 3]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;1: Are both models correct? What is the difference?&lt;/p&gt;&#10;&#10;&lt;p&gt;2: Is there a difference between Binary GLM and logistic regression? Is one a special case of the other?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-06T07:33:36.533" Id="78733" LastActivityDate="2013-12-06T13:38:46.903" LastEditDate="2013-12-06T10:07:30.893" LastEditorUserId="22047" OwnerUserId="24097" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;&lt;binomial&gt;&lt;count-data&gt;" Title="Poisson or binomial regression?" ViewCount="131" />
  
  <row Body="&lt;p&gt;Tongue firmly in cheek:&lt;/p&gt;&#10;&#10;&lt;p&gt;Einstein apparently wrote &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;As far as the laws of mathematics refer to reality, they are not&#10;  certain; and as far as they are certain, they do not refer to reality.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;so statistics is the branch of maths that describes reality. ;o)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd say statistics is a branch of mathematics in the same way that logic is a branch of mathematics.  It certainly includes an element of philosophy, but I don't think it is the only branch of mathematics where that is the case (see e.g. Morris Kline, &quot;Mathematics - The Loss of Certainty&quot;, Oxford University Press, 1980).&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2013-12-06T09:10:25.327" CreationDate="2013-12-06T09:10:25.327" Id="78738" LastActivityDate="2013-12-06T09:10:25.327" OwnerUserId="887" ParentId="78579" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I'm not very knowledgeable on convolutional networks but when you apply back propagation to regular neural networks you differentiate the error value of the network with respect to each connections weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the error value is the difference between the actual network output and the desired output.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to back propagate this error value, the activation functions also have to be differentiated; but this is still with respect to connection weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I would be expecting to see your functions f being differentiated with respect to the connection weight values, and not with respect to a convolution kernel k. &lt;/p&gt;&#10;&#10;&lt;p&gt;Though my lack of knowledge of convolutional networks could be hiding some subtlety of the question. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-06T10:31:52.940" Id="78746" LastActivityDate="2013-12-06T15:05:20.963" LastEditDate="2013-12-06T15:05:20.963" LastEditorUserId="34953" OwnerUserId="34953" ParentId="78424" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;In the one-way model you look at only the effect of the sound system, whereas in the two-way model you look als at the effect of distractors and the interaction between distractors and the soundsystem.&lt;/p&gt;&#10;&#10;&lt;p&gt;When you change the model, it is not unusual that the values of the model change as wel. Your two-way anova model seems to explain your data better than the onway anova model. &lt;/p&gt;&#10;&#10;&lt;p&gt;It's also probably good to do a one-way anova with the distractors variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-06T11:29:52.400" Id="78749" LastActivityDate="2013-12-06T11:40:06.493" LastEditDate="2013-12-06T11:40:06.493" LastEditorUserId="22387" OwnerUserId="22387" ParentId="78748" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;It seems like for managing with ordered measurements researchers &lt;strong&gt;usually&lt;/strong&gt; deal with &lt;a href=&quot;http://en.wikipedia.org/wiki/Polychoric_correlation&quot; rel=&quot;nofollow&quot;&gt;Polychoric Correlation&lt;/a&gt;. (For example, for making matrix before doing Factor Analysis.) Why so?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;Kendall Tau Rank Correlation Coefficient&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;Spearman's rank correlation coefficient&lt;/a&gt; are also suitable for ordered data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any points 'pro' and 'contra' for these correlation coefficients are welcome.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-06T17:58:03.773" FavoriteCount="1" Id="78791" LastActivityDate="2013-12-07T07:55:02.380" LastEditDate="2013-12-07T07:55:02.380" LastEditorUserId="805" OwnerUserId="25529" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;ordered-variables&gt;" Title="Correlation coefficients for ordered data: Kendall's Tau vs Polychoric vs Spearman's rho" ViewCount="274" />
  <row AcceptedAnswerId="78838" AnswerCount="1" Body="&lt;p&gt;I have a data set like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df&#10;&#10;Income     Education_in_years&#10;40,000     10&#10;50,000      9&#10;70,000     12&#10;30,000      5&#10;100,000    20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to create a bivariate distribution from this and try to guess probability of income given eduction in years.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can build the linear model as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lin &amp;lt;- lm(Income~Eduction_in_years, data=df)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I could come up with a formula like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Income = a*education_in_years + e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What I would like to do is, given the model, create a bivariate normal, and run a 'what-if' analysis to determine &lt;code&gt;income&lt;/code&gt; given &lt;code&gt;eduction_in_years&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody walk me through how to create a bivariate normal distribution from this dataset and determine the income level given the education?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-12-06T20:36:37.343" Id="78807" LastActivityDate="2013-12-07T05:03:21.923" LastEditDate="2013-12-06T21:01:47.357" LastEditorUserId="7290" OwnerUserId="16362" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;bivariate&gt;" Title="How to create a bivariate normal distribution" ViewCount="100" />
  
  
  <row Body="&lt;h2&gt;Software&lt;/h2&gt;&#10;&#10;&lt;p&gt;The software you list (SPSS, SAS) are statistics packages. They are hardly suitable for actual data mining (kernel methods, neural nets, deep learning, ...). That said, you could easily replace both of those by R which works perfectly on any platform.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of data mining software, to my experience, you should be looking at things like Python, MATLAB, Mahout, ... All of these work on any software platform.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Operating System&lt;/h2&gt;&#10;&#10;&lt;p&gt;I am probably biased, but in my opinion Linux is by far the best choice (OSX a distant second). It may have a steep learning curve but once you know the basics your productivity is much, &lt;em&gt;much&lt;/em&gt; higher than on Windows due to the myriad of available tools. &lt;/p&gt;&#10;&#10;&lt;p&gt;Linux also makes using open-source software easier, which is becoming increasingly popular in machine learning. Check out &lt;a href=&quot;http://www.mloss.org/&quot; rel=&quot;nofollow&quot;&gt;mloss.org&lt;/a&gt; for example.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Desktop vs laptop&lt;/h2&gt;&#10;&#10;&lt;p&gt;I believe this is mainly a matter of opinion. I personally prefer to work on a desktop. The main objective criterion here is whether you will be working in different locations (maybe at costumers' offices, ...).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-07T10:04:33.970" Id="78843" LastActivityDate="2013-12-07T10:04:33.970" OwnerUserId="25433" ParentId="78797" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You almost always have such problems with learned systems. One solution is to assume a prior distribution over your parameters, for example a Dirichlet distribution which would basically lead you to the assumption that you observed state x or transition y at least z times.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-07T13:21:51.003" Id="78851" LastActivityDate="2013-12-07T13:21:51.003" OwnerUserId="27854" ParentId="48435" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What common practice is might depend on your field of research. The manual of the American Psychological Association (APA), which is one of the most often used citation styles, states (p. 139, 6th edition): &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Do not use any value smaller than &lt;em&gt;p &amp;lt; 0.001&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2013-12-07T13:49:14.290" Id="78852" LastActivityDate="2014-11-23T09:15:07.760" LastEditDate="2014-11-23T09:15:07.760" LastEditorUserId="22387" OwnerUserId="22387" ParentId="78839" PostTypeId="2" Score="16" />
  
  <row Body="&lt;p&gt;The simple solution is to stratify your data by states and then partition each state randomly into the training and testing sets. Of course, this might just push the problem from the testing stage to the deployment stage if the real world has states that your data does not.&lt;/p&gt;&#10;&#10;&lt;p&gt;The comment by kutschkem is a more general solution: place a prior distribution on your parameters.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-07T14:41:07.147" Id="78859" LastActivityDate="2013-12-07T15:35:46.587" LastEditDate="2013-12-07T15:35:46.587" LastEditorUserId="1764" OwnerUserId="1764" ParentId="48435" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The key difference is that we are not attempting to optimize anything.  Instead, we are attempting to estimate a density function - but not by estimating in some optimal manner a small number of parameters, instead by generating a lot of random numbers from the density function and going from there.  So MCMC is really a random number generation technique, not an optimization technique.&lt;/p&gt;&#10;&#10;&lt;p&gt;To see what a gradient descent-like algorithm with a stochastic component looks like, check out &lt;a href=&quot;http://en.wikipedia.org/wiki/Stochastic_approximation&quot;&gt;stochastic approximation&lt;/a&gt;.  The &lt;a href=&quot;http://www.jhuapl.edu/spsa/&quot;&gt;simultaneous perturbation&lt;/a&gt; variant is quite effective and accessible.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-07T19:31:45.513" Id="78884" LastActivityDate="2013-12-07T19:31:45.513" OwnerUserId="7555" ParentId="78876" PostTypeId="2" Score="10" />
  <row AnswerCount="0" Body="&lt;p&gt;When you do a Principal Component Analysis (PCA), your dataset generally looks like the following one:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Country    Var1    Var2    Var3&#10;  A          2      18      23&#10;  B          3      16      28&#10;  C          1      19      33&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But what happens, when you have more than one observation per country or in my case actor (because you have a time series):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Obs.    BACE    PWR       CC      SC     TASK     DIS     IGB   I1       I2      I3     I4a      I4b   Punish Threaten Oppose   Appeal  Promise Reward   P1      P2       P3     P4      P5&#10;Ar_P1   0.3176  0.2115  0.6527  0.4615  0.6627  0.1185  0.0718  0.75    0.24    0.60    0.25    0.19    0.06    0.00    0.06    0.81    0.03    0.03    0.40    0.15    0.17    0.32    0.9456&#10;Ar_P2   0.2838  0.2292  0.5847  0.4983  0.6312  0.1608  0.1350  0.47    0.23    0.16    0.53    0.69    0.12    0.02    0.12    0.46    0.05    0.22    0.30    0.12    0.10    0.24    0.9760&#10;Ar_P3   0.2831  0.2081  0.6175  0.5222  0.6041  0.1385  0.1161  0.56    0.25    0.23    0.44    0.51    0.09    0.02    0.11    0.55    0.07    0.16    0.42    0.21    0.11    0.25    0.9725&#10;Bu_P1   0.3902  0.2680  0.5426  0.3049  0.5194  0.1143  0.1522  0.58    0.30    0.17    0.42    0.47    0.06    0.04    0.11    0.50    0.12    0.17    0.43    0.25    0.11    0.35    0.9615&#10;Bu_P2   0.3575  0.2781  0.5661  0.3219  0.5066  0.2685  0.1646  0.54    0.23    0.20    0.46    0.53    0.10    0.05    0.08    0.53    0.08    0.16    0.30    0.14    0.09    0.30    0.9730&#10;Bu_P3   0.4185  0.2975  0.5674  0.2854  0.4879  0.2447  0.1532  0.58    0.26    0.20    0.42    0.60    0.11    0.03    0.07    0.53    0.08    0.19    0.39    0.21    0.12    0.34    0.9592&#10;Ch_P1   0.2963  0.2331  0.6130  0.4507  0.7299  0.0548  0.1227  0.61    0.32    0.20    0.39    0.49    0.05    0.04    0.11    0.53    0.09    0.19    0.50    0.27    0.15    0.29    0.9565&#10;Ch_P2   0.3215  0.2720  0.5821  0.4531  0.6480  0.2281  0.1203  0.35    0.15    0.10    0.65    0.73    0.16    0.06    0.11    0.41    0.06    0.21    0.28    0.13    0.09    0.23    0.9793&#10;Ch_P3   0.3509  0.3119  0.5627  0.3972  0.5885  0.1889  0.1538  0.38    0.16    0.12    0.62    0.73    0.17    0.02    0.13    0.42    0.07    0.20    0.25    0.11    0.10    0.25    0.9750&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When you do the PCA and plot it, all the actors will appear three times (because of the three observations per actor). The more observations, the more often the actor label appears.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; pca.data &amp;lt;- prcomp(data, scale=TRUE)&#10; biplot(pca.data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can you do a PCA with time series data, where every actor is plotted only once?&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2013-12-07T19:36:09.850" Id="78885" LastActivityDate="2013-12-08T16:27:18.853" LastEditDate="2013-12-08T16:27:18.853" LastEditorUserId="35828" OwnerUserId="35828" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;pca&gt;" Title="Principal component analysis with time series" ViewCount="1496" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Would someone please tell me what an F test is and what is shows? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The term &quot;F test&quot; may be any test whose sampling distribution under the null hypothesis has an F-distribution. There are several quite distinct tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are a couple of the more common ones:&lt;/p&gt;&#10;&#10;&lt;p&gt;(i) an F-test for equality of means of multiple (more than two) groups (also called ANOVA, short for &lt;em&gt;ANalysis Of VAriance&lt;/em&gt;, since inequality of means will tend to lead to two different estimates of variance being different). If you reject the null, you have concluded that the sample means are too far apart to explain by random variation with equal population means; the alternative explanation is that the population means are not equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;This test assumes normality but isn't especially sensitive to mild deviations from it.&lt;/p&gt;&#10;&#10;&lt;p&gt;(ii) a test for equality of variances. If you reject the null, you have concluded that the sample variances are too different to explain by random variation with equal variances; the alternative explanation is that the population variances are not equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;This test assumes normality and is &lt;em&gt;really&lt;/em&gt; sensitive to that assumption. I advise against using it, except under very limited circumstances. You are not in those circumstances.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also what is an alpha?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It is the chosen type I error rate. Do you know how hypothesis tests work?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_significance#Role_in_statistical_hypothesis_testing&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Statistical_significance#Role_in_statistical_hypothesis_testing&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Type_I_and_type_II_errors&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Type_I_and_type_II_errors&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;And how do I evaluate the p values?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Do you mean 'how do I calculate them?' or 'how do I interpret them?'&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know what the &lt;a href=&quot;http://en.wikipedia.org/wiki/P-value&quot; rel=&quot;nofollow&quot;&gt;definition of a p-value&lt;/a&gt; is? &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I posted two questions earlier and one was 'put on hold' because it was not in the right place, and the second one was marked as a duplicate because of the one put on hold. Neither of which answered my question.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You have had several requests for clarification; you may need to do a bit of research first, and then perhaps you could edit the questions.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-08T01:29:08.867" Id="78906" LastActivityDate="2013-12-08T08:40:16.203" LastEditDate="2013-12-08T08:40:16.203" LastEditorUserId="805" OwnerUserId="805" ParentId="78896" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have the regular hypothesis where I'm testing whether the mean of one variable is larger than the mean of the other variable or not:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;H0: mean(variable1) &amp;lt;= mean(variable2)&#10;H1: mean(variable1) &amp;gt; mean(variable2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've ran the Welch's test variation of the student t test in R and my output is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Welch Two Sample t-test&#10;&#10;data:  x and y &#10;t = -2.8207, df = 43.367, p-value = 0.9964&#10;alternative hypothesis: true difference in means is greater than 0 &#10;95 percent confidence interval:&#10; -0.08806587         Inf &#10;sample estimates:&#10;mean of x mean of y &#10;0.6708163 0.7260000 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The significance level I'm using is 5%. So based on the output, I know I can't reject the H0 hypothesis on a 5% significance level.  Since the difference in mean from x and y is about -0.05 and is within the 95% CI, I can yet again conclude that I can't reject the H0 hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Subquestion: Does analyzing both the result with the CI and the p-value somehow empower my statement?  Can I be 'more confident' that H0 can or cannot be rejected if I analyze the CI if my p-value already indicates it's above or below the significance level?  I assume a contradiction between p-value testing and CI testing cannot occur.&lt;/p&gt;&#10;&#10;&lt;p&gt;Main question: how should I interpret the t-value exactly?  From what I learned, the 'further' the t-value strays from 0, the more likely that the effect is 'statistically significant'.  First of all, I'm not sure what it means exactly when they say an effect is statistically significant. I'm guessing it means our test is more likely to be accurate? And how far from 0 would be safe and why?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-12-08T13:58:07.357" Id="78953" LastActivityDate="2013-12-08T15:51:42.183" LastEditDate="2013-12-08T15:51:42.183" LastEditorUserId="35521" OwnerUserId="35521" PostTypeId="1" Score="1" Tags="&lt;t-test&gt;" Title="What is the exact meaning of the t-value in the student t test?" ViewCount="281" />
  <row AnswerCount="1" Body="&lt;p&gt;Could anyone help me perform a Kruskal-Wallis test in Stata?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not familiar with Kruskal-Wallis test, but I have to perform a nonparametric test, since the normality hypothesis is rejected, to confirm the results of a parametric test already performed (F-test). I have got the data already ranked in groups though I am not understanding the results. &lt;/p&gt;&#10;&#10;&lt;p&gt;chi-squared =   505.166 with 179 d.f.&#10;probability =     0.0001&lt;/p&gt;&#10;&#10;&lt;p&gt;chi-squared with ties =   505.195 with 179 d.f.&#10;probability =     0.0001&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-08T14:48:02.417" Id="78957" LastActivityDate="2013-12-08T15:00:58.247" LastEditDate="2013-12-08T14:58:54.423" LastEditorUserId="22047" OwnerUserId="35852" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;" Title="Perform a Kruskal-Wallis test in Stata" ViewCount="63" />
  <row Body="&lt;p&gt;Probably what you will need to use is the &lt;em&gt;Parametric Bootstrap Cross-fitting Method&lt;/em&gt;.  Here is the basic procedure:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fit each model to the data.  Estimate the models' parameters and extract your favorite measure of goodness of fit.  We will call the model with the higher value for this GoF measure $A$ and the other model $B$.  Calculate the difference $d$ between the two measures of GoF, and store that value.  (Be sure you are clear about whether higher or lower numbers of your GoF measure indicate a better fit--i.e., whether $d&amp;gt;0$ implies $A$ is better or worse.)  &lt;/li&gt;&#10;&lt;li&gt;Using the fitted parameters for $A$ from step 1, generate a large number of synthetic datasets (say 1000).  With each of these datasets, fit both of your models, extract their GoF measures, compute $d$ and store it.  &lt;/li&gt;&#10;&lt;li&gt;Using the fitted parameters for $B$ from step 1, generate another set of (1000) synthetic datasets.  With these datasets, again fit your models, compute the $d$s and store them.  &lt;/li&gt;&#10;&lt;li&gt;You now know what the sampling distribution of $d$ looks like when the true model is $A$ and when the true model is $B$.  Determine the cutpoint, $d_\text{cut}$, that optimally differentiates between the models.  If you want, you can bring prior knowledge to bear by differentially weighting the alternatives.  &lt;/li&gt;&#10;&lt;li&gt;Compare your found $d$ from step 1 to $d_\text{cut}$ and select the corresponding model.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I demonstrate this approach &lt;a href=&quot;http://stats.stackexchange.com/a/138425/7290&quot;&gt;here&lt;/a&gt;.  (There is another description of PBCM in this answer: &lt;a href=&quot;http://stats.stackexchange.com/questions/2828/measures-of-model-complexity/2890#2890&quot;&gt;Measures of model complexity&lt;/a&gt;.)  Here is the reference:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Wagonmakers, E.J., Ratcliff, R., Gomez, P., &amp;amp; Iverson, G.J. (2004).  Assessing model mimicry using the parametric bootstrap, &lt;em&gt;Journal of Mathematical Psychology, 48&lt;/em&gt;, pp. 28-50.  (&lt;a href=&quot;http://www.ejwagenmakers.com/2004/PBCM.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;) &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2013-12-08T17:29:49.667" Id="78977" LastActivityDate="2015-02-19T21:45:06.017" LastEditDate="2015-02-19T21:45:06.017" LastEditorUserId="7290" OwnerUserId="7290" ParentId="78973" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="78992" AnswerCount="1" Body="&lt;p&gt;I'd like a test to determine whether a number of collected 2D points (say, x,y in (0,1]) are distributed uniformly across the space.  The points will be one or more 'paths' through the space - collections of points resembling movement through the 2D space.  I want to determine, for the entire area, whether the entire set of points within all paths looks uniformly distributed across that area or if the points have some degree of proximity (e.g., if most paths stay in one corner of the graph).&lt;/p&gt;&#10;&#10;&lt;p&gt;I've considered these:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Simple regression, but I don't expect to know how the points are distributed if they aren't uniform (polynomial doesn't seem to be a good fit)&lt;/li&gt;&#10;&lt;li&gt;Taking discrete chunks of the space and computing the sum of square error between the expected density and the observed density, or using a Chi-squared on the segments.  &lt;/li&gt;&#10;&lt;li&gt;Some sort of clustering algorithm/test - not sure what I'd use here, but again, I'm not sure clusters are what I'm looking for&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any other suggestions/ideas?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-12-08T19:43:59.387" Id="78989" LastActivityDate="2013-12-09T19:37:09.717" LastEditDate="2013-12-09T19:37:09.717" LastEditorUserId="12612" OwnerUserId="12612" PostTypeId="1" Score="2" Tags="&lt;goodness-of-fit&gt;" Title="What's the best way to test the uniformity of location data?" ViewCount="64" />
  
  
  <row AcceptedAnswerId="79006" AnswerCount="1" Body="&lt;p&gt;Assume a true population of size N consisting of red and blue balls, where p represents the frequency of red in the population and q represents the frequency of blue in the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say that N=1000, p=0.7 and q=0.3.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I take a sample from the population and observe the frequencies in the sample to infer the true population proportions, the smaller the sample the more variance there will be.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to calculate the extent to which different sample sizes will over or underestimate the true proportions?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-08T21:38:17.940" Id="79005" LastActivityDate="2013-12-08T22:08:54.460" LastEditDate="2013-12-08T21:45:00.133" LastEditorUserId="35878" OwnerUserId="35878" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;sampling&gt;&lt;sample-size&gt;" Title="Calculating sampling bias when estimating population proportions" ViewCount="191" />
  
  <row AcceptedAnswerId="79014" AnswerCount="1" Body="&lt;p&gt;I have come across a bit of a dilemma in my exam revision, this is not a topic I am particularly strong with so help is most appreciated:&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that $X_1,X_2,...$ is an i.i.d sequence, where $E(X^2)=5$ and $E(X)=0$,&#10;Using central limit theorem, find the distribution of &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\sum_{i=1}^nX_i}{\sqrt{5n}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here I have let $ S_n= \sum_{i=1}^nX_i $ and we know that $E(X)=0$, then we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Yn=\frac{\sum_{i=1}^nX_i - nE(x))}{σ\sqrt{n}} = \frac{\sum_{i=1}^nX_i}{\sqrt{5n}} -&amp;gt; N(0,1) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Next I am asked to apply the strong law of large numbers to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{\sum_{i=1}^nX_i^2}{n} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;to show that it converges almost surely and compute its finite limit&lt;/p&gt;&#10;&#10;&lt;p&gt;I have had a go at this by letting $ S_n= \sum_{i=1}^nX_i^2 $ and $ E(S_n)= \sum_{i=1}^nE(X_i^2) $ and plugging this into to the strong law of large numbers formula ( $\frac{S_n-E(S_n)}{n}-&amp;gt; 0$ a.s.) to find &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \frac{\sum_{i=1}^nX_i^2}{n} $ -&gt; $ E(X_i^2) $ a.s.&lt;/p&gt;&#10;&#10;&lt;p&gt;However I do not feel as though I am showing that $ \frac{\sum_{i=1}^nX_i^2}{n} $ is converging almost surely using this method?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas would be appreciated &lt;/p&gt;&#10;&#10;&lt;p&gt;Also lastly I have to find, by using central limit theorem, the distribution of &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\sum_{i=1}^nXi}{\sqrt{\sum_{i=1}^nX_i^2}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am lost on this one completely, although I am making the assumption as we are using central limit theorem with it them it will be N(0,1) but as I say that is just a guess,&lt;/p&gt;&#10;&#10;&lt;p&gt;Help/hints/explanations would be really appreciated&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-08T21:49:01.873" Id="79007" LastActivityDate="2013-12-08T22:35:48.280" OwnerUserId="35879" PostTypeId="1" Score="3" Tags="&lt;convergence&gt;&lt;central-limit-theorem&gt;" Title="Law of Large numbers and central limit theorem" ViewCount="94" />
  <row Body="&lt;p&gt;There are some step-by-step guides in Shalizi's notes here : &lt;a href=&quot;http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf&lt;/a&gt;, &#10;one being the cars data set from R and another being art and music articles from the New York Times. (Inferring the topic of an article from the words contained in it is a very active research area.) If you don't know/don't want to learn R then you could still use his notes and graphics. &lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: forgot to say that there are also several good examples in a book by Everitt and Hothorn, which is available on SpringerLink. As I recall, one data set is jet fighters and there is also Roman pottery.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-08T21:49:03.860" Id="79008" LastActivityDate="2013-12-08T21:49:03.860" OwnerUserId="13818" ParentId="78990" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Your models have a problem, you have added no error to your &lt;em&gt;y&lt;/em&gt; so you get a perfect fit with your &lt;em&gt;x&lt;/em&gt; variable. So let's fix this first:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;ds &amp;lt;- data.frame(x=1:10,y=1:10+rnorm(10, 0, 0.5),z=rep(c(&quot;A&quot;,&quot;B&quot;),each=5))&#10;m1 &amp;lt;- glm(y ~ x , data = ds)&#10;m2 &amp;lt;- glm(y ~ z , data = ds)&#10;m3 &amp;lt;- glm(y ~ x + z , data = ds)&#10;m4 &amp;lt;- glm(y ~ x * z , data = ds)&#10;summary(m1)&#10;summary(m2)&#10;summary(m3)&#10;summary(m4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now you get:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#m3&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)   0.3810     0.4124   0.924    0.386    &#10;x             0.9053     0.1144   7.914 9.77e-05 ***&#10;zB            0.3547     0.6571   0.540    0.606 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#m4&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  -0.1552     0.4677  -0.332 0.751219    &#10;x             1.0840     0.1410   7.687 0.000254 ***&#10;zB            2.3208     1.2374   1.876 0.109830    &#10;x:zB         -0.3575     0.1994  -1.793 0.123219    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now why do you get those odd estimates for B and for the interaction? If you look at R-squared in models &lt;em&gt;m1&lt;/em&gt; and &lt;em&gt;m2&lt;/em&gt; you'll notice that R^2 for &lt;em&gt;m1&lt;/em&gt; is 0.98 and for &lt;em&gt;m2&lt;/em&gt; is 0.77. That is, you are already explaining almost all the variability with just one variable so when you add another there's not much to add to the fit of the model. Look how the &lt;code&gt;B&lt;/code&gt; coefficient is not significant in &lt;code&gt;m3&lt;/code&gt; but it was in &lt;code&gt;m2&lt;/code&gt;. &#10;You can interpret the coefficient of &lt;code&gt;B&lt;/code&gt; in &lt;code&gt;m3&lt;/code&gt; as the independent effect of &lt;code&gt;B&lt;/code&gt; in &lt;code&gt;y&lt;/code&gt; taking into account the effect of &lt;code&gt;x&lt;/code&gt; in &lt;code&gt;y&lt;/code&gt;. Since &lt;code&gt;y&lt;/code&gt; is almost fully explained by &lt;code&gt;x&lt;/code&gt; the estimated independent effect of &lt;code&gt;B&lt;/code&gt; is much lower than what you'd expect.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-08T23:59:40.833" Id="79017" LastActivityDate="2013-12-08T23:59:40.833" OwnerUserId="28281" ParentId="78999" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;In order to calculate a sample size for a 1-proportion test in Minitab, one has to input the power and the comparison proportion. Could someone provide a definition for the comparison proportion please?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-09T00:46:18.677" Id="79023" LastActivityDate="2013-12-09T00:46:18.677" OwnerUserId="35886" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;sample-size&gt;&lt;proportion&gt;&lt;power-analysis&gt;&lt;power&gt;" Title="What is the Comparison p used in a sample size calculation for a 1-proportion test in Minitab?" ViewCount="158" />
  <row AnswerCount="0" Body="&lt;p&gt;I work with habitat use data (summarized binomial data: visited / did not visit) which I fitted with a mixed model of the binomial family with a random factor accounting for repeated sampling of the same individual. I simply want basic descriptions of the data - effects of sex and year on the proportion of time spent in each habitat.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code is as follow (for a basic model without interaction):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Colony.glmmSexYear &amp;lt;- glmmPQL(cbind(ColoYes.allnoF24, ColoNo.allnoF24) ~ &#10;                              year.coded + sex, random= ~1| BirdID, &#10;                              family=binomial(), data=mydata)&#10;summary(Colony.glmmSexYear)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is the fixed effects section of my output: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fixed effects: cbind(ColoYes.allnoF24, ColoNo.allnoF24) ~ year.coded + sex&#10;&#10;                     Value Std.Error DF   t-value p-value&#10;(Intercept)      1.4442270 0.1824173 74  7.917161  0.0000&#10;year.codedY2011 -0.1733713 0.1864744 74 -0.929732  0.3555&#10;year.codedY2012 -0.3004284 0.2027411 74 -1.481833  0.1426&#10;sexM             0.4403108 0.1507471 74  2.920857  0.0046&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You were supposed to see that 2011-2012 is also not significant (not shown). &lt;/p&gt;&#10;&#10;&lt;p&gt;In traditional linear models I would report the p value for the whole treatment &quot;year&quot; from an ANOVA associated with a F test (i.e, just one p value), not the individual p values of the various year comparisons (3 p-values). But my understanding is that producing an ANOVA from &lt;code&gt;glmmPQL&lt;/code&gt; is not possible (?). Reporting those three individual p values of my multiple year comparisons really seems clumsy and out of place...&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the proper way to do this? Should I not be using &lt;code&gt;glmmPQL&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Specification: I cut into the background description of the study for simplicity, but this is not a resource selection study - please do not suggest using resource selection models.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-09T03:25:26.363" Id="79029" LastActivityDate="2013-12-09T04:01:53.540" LastEditDate="2013-12-09T04:01:53.540" LastEditorUserId="7290" OwnerUserId="35893" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;random-effects-model&gt;&lt;glmm&gt;" Title="How can I get a &quot;whole treatment effect&quot; for a 3 level categorical variable in a GLMM (glmmPQL in R)" ViewCount="52" />
  <row Body="&lt;p&gt;Unfortunately, answering this question in full really requires you to be more specific about what type of pattern you are hoping to uncover.&lt;/p&gt;&#10;&#10;&lt;p&gt;One type of pattern that you may want to consider is one which often arises in the context of time series data (e.g., data sets consisting of a single quantity which rises and falls over time, such as the price of a stock or commodity): one of the data sets could be correlated with another via a time lag.  (If your data sets are not time series, don't worry; the technique that I'm about to describe should work in principle on any x-vs.-y data set that is amenable to display in a line chart, it's just that it's most &lt;em&gt;common&lt;/em&gt; to use it in cases where the x-axis represents a time variable.)&lt;/p&gt;&#10;&#10;&lt;p&gt;As a simple illustrative example of a time-lagged correlation, consider the relationship over the course of a few years observation time between day length and daily temperature at so-called &quot;temperate&quot; middle latitudes (e.g., roughly ~40 to 50 degrees N or S, a region which covers most of Europe and quite a lot of the major population centers of North America and Asia).  If you happen to live in that part of the world, you will have probably noticed by now that, although the longest and shortest days of the year are usually June 21 and December 21, the coldest and warmest temperatures of the year typically peak a few weeks later, around mid-January and mid-July.  (This is because large masses such as the ocean act effectively as &quot;heat reservoirs&quot;, and require a few weeks to catch up to changing trends in solar conditions.)&lt;/p&gt;&#10;&#10;&lt;p&gt;An analysis paradigm which is capable of &quot;catching&quot; these types of time-lagged patterns is &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-correlation&quot; rel=&quot;nofollow&quot;&gt;cross correlation&lt;/a&gt;; essentially it works a lot like the &lt;code&gt;correl&lt;/code&gt; function that you mentioned in Excel, but with the additional twist that pairs of time series curves are shifted forward and backward with respect to one another over a range of input time lags, and you search for an &quot;optimal&quot; choice of time lag which tends to maximize the correlation effect, if indeed there is one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although the illustrative example that I chose (length of day vs. daily temperature) has an obvious annual periodicity, the cross correlation formalism doesn't actually require periodicity in the data in order to uncover a signal.  Any ostensibly correlated data (e.g., labor participation rate vs. national GDP) will work as long as it has some bumps and wiggles in it, even if the large scale pattern in the data is that it goes up and down erratically as opposed to periodically over time.  Furthermore, the time lag need not be limited to just a few weeks as in the example that I chose; in principle this type of analysis is capable of uncovering &lt;em&gt;any&lt;/em&gt; time-lagged correlation, even one that takes months or years to materialize.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data does have some periodicity to it, a host of other methods potentially become available to you.  You could try a form of &lt;a href=&quot;http://en.wikipedia.org/wiki/Time%E2%80%93frequency_analysis&quot; rel=&quot;nofollow&quot;&gt;time-frequency analysis&lt;/a&gt; such as classic &lt;a href=&quot;http://en.wikipedia.org/wiki/Fourier_analysis&quot; rel=&quot;nofollow&quot;&gt;Fourier analysis&lt;/a&gt; or the more modern techniques of &lt;a href=&quot;http://users.rowan.edu/~polikar/WAVELETS/WTpart1.html&quot; rel=&quot;nofollow&quot;&gt;wavelet analysis&lt;/a&gt;.  In this paradigm, the idea is to use automated algorithms to uncover hidden periodic behavior which is difficult for you to see with the naked eye because of noise in the data.  For purposes of your particular problem, you would declare any two data sets to share the same &quot;pattern&quot; if you uncovered shared evidence of similar periodicities (e.g., a given pair of data sets might display a weekly pattern of variation, or lunar, or monthly, or annual, or every eleven years like the &lt;a href=&quot;http://solarscience.msfc.nasa.gov/SunspotCycle.shtml&quot; rel=&quot;nofollow&quot;&gt;sunspot cycle&lt;/a&gt;, or whatever).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-09T04:59:47.730" Id="79033" LastActivityDate="2013-12-09T04:59:47.730" OwnerUserId="28566" ParentId="78734" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Let $X \sim U(0,1)$ and let $0 &amp;lt; a &amp;lt; 1$, then find &lt;/p&gt;&#10;&#10;&lt;p&gt;i) $P(X \le x|X &amp;gt; a)$&lt;/p&gt;&#10;&#10;&lt;p&gt;ii) $P(X \le x|X &amp;lt; a)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the answers, but I'm wondering if someone can explain the process to me. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-09T06:25:54.093" Id="79037" LastActivityDate="2013-12-09T13:46:16.980" LastEditDate="2013-12-09T12:17:18.350" LastEditorUserId="1352" OwnerUserId="35897" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Help on a probability theory question" ViewCount="49" />
  <row AnswerCount="1" Body="&lt;p&gt;I am wondering if for a PCA for which I rescale my numeric variables, I need to rescale my dummy variables as well ? I have read on the internet that I should not but I do not see why.&#10;I guess the question is very similar to this one : &lt;a href=&quot;http://stats.stackexchange.com/questions/69568/whether-to-rescale-indicator-binary-dummy-predictors-for-lasso&quot;&gt;whether to rescale indicator / binary / dummy predictors for LASSO&lt;/a&gt; but I was not sure about the answer. If it is no, please explain why. &#10;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-09T09:33:39.443" FavoriteCount="1" Id="79042" LastActivityDate="2014-05-29T02:50:40.500" LastEditDate="2013-12-09T10:05:52.567" LastEditorUserId="35572" OwnerUserId="35572" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;&lt;pca&gt;&lt;normalization&gt;" Title="Do I need to rescale dummy variables for PCA?" ViewCount="139" />
  
  <row Body="&lt;p&gt;One solution is actually given as an example in the book on the &lt;code&gt;multcomp&lt;/code&gt; package, section 4.6:&lt;/p&gt;&#10;&#10;&lt;p&gt;Bretz, F., Hothorn, T., &amp;amp; Westfall, P. H. (2011). &lt;em&gt;Multiple comparisons using R&lt;/em&gt;. Boca Raton, FL: CRC Press.&lt;/p&gt;&#10;&#10;&lt;p&gt;One only needs to slightly adapt your code (everything needs to be in one data.frame instead of floating around):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(multcomp)&#10;require(sandwich)&#10;&#10;set.seed(81)&#10;pred3 = rnorm(24)&#10;df &amp;lt;- data.frame(pred1 = rep(c('Car', 'Bike', 'Train', 'Airplane'), 6), pred2 = rep(c('High', 'Low', 'Middle'), 8)&#10;, resp = c(rnorm(12, sd = 1), rnorm(12, sd = 5)))&#10;&#10;m &amp;lt;- aov(resp ~ pred1 + pred2, df)&#10;&#10;tukey &amp;lt;- glht(m, linfct = mcp(pred1 = &quot;Tukey&quot;) , vcov = sandwich)&#10;&#10;summary(tukey, test = adjusted())&#10;&#10;##          Simultaneous Tests for General Linear Hypotheses&#10;## &#10;## Multiple Comparisons of Means: Tukey Contrasts&#10;## &#10;## &#10;## Fit: aov(formula = resp ~ pred1 + pred2, data = df)&#10;## &#10;## Linear Hypotheses:&#10;##                       Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;## Bike - Airplane == 0     1.559      1.166    1.34    0.547  &#10;## Car - Airplane == 0      1.239      1.241    1.00    0.748  &#10;## Train - Airplane == 0    2.509      0.915    2.74    0.058 .&#10;## Car - Bike == 0         -0.320      1.422   -0.23    0.996  &#10;## Train - Bike == 0        0.950      1.149    0.83    0.838  &#10;## Train - Car == 0         1.270      1.225    1.04    0.726  &#10;## ---&#10;## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;## (Adjusted p values reported -- single-step method)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that &lt;code&gt;glht&lt;/code&gt; as default uses the single-step method as adjustment for alpha-error accumulation. If you want something else, you need to feed it to &lt;code&gt;adjusted()&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, using the Bonferroni-Holm correction (which I tend to use as I don't understand what single-step actually does):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(tukey, test = adjusted(&quot;holm&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you want no alpha error correction, which I do not recommend, this is also possible:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(tukey , test = adjusted(&quot;none&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-09T11:18:22.213" Id="79044" LastActivityDate="2013-12-09T11:18:22.213" OwnerUserId="442" ParentId="78792" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Here are my 2ct on the topic&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The chemometrics lecture where I first learned PCA used solution (2), but it was not numerically oriented, and my numerics lecture was only an introduction and didn't discuss SVD as far as I recall. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If I understand &lt;a href=&quot;http://sysrun.haifa.il.ibm.com/hrl/bigml/files/Holmes.pdf&quot; rel=&quot;nofollow&quot;&gt;Holmes: Fast SVD for Large-Scale Matrices&lt;/a&gt; correctly, your idea has been used to get a computationally fast SVD of long matrices.&lt;br&gt;&#10;That would mean that a good SVD implementation may internally follow (2) if it encounters suitable matrices (I don't know whether there are still better possibilities). This would mean that for a high-level implementation it is better to use the SVD (1) and leave it to the BLAS to take care of which algorithm to use internally.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Quick practical check: OpenBLAS's svd doesn't seem to make this distinction, on a matrix of 5e4 x 100, &lt;code&gt;svd (X, nu = 0)&lt;/code&gt; takes on median 3.5 s, while &lt;code&gt;svd (crossprod (X), nu = 0)&lt;/code&gt; takes 54 ms (called from R with &lt;code&gt;microbenchmark&lt;/code&gt;).&lt;br&gt;&#10;The squaring of the eigenvalues of course is fast, and up to that the results of both calls are equvalent.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;timing  &amp;lt;- microbenchmark (svd (X, nu = 0), svd (crossprod (X), nu = 0), times = 10)&#10;timing&#10;# Unit: milliseconds&#10;#                      expr        min         lq    median         uq        max neval&#10;#            svd(X, nu = 0) 3383.77710 3422.68455 3507.2597 3542.91083 3724.24130    10&#10;# svd(crossprod(X), nu = 0)   48.49297   50.16464   53.6881   56.28776   59.21218    10&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;update: Have a look at &lt;a href=&quot;http://dx.doi.org/10.1016/S0169-7439%2897%2900010-5&quot; rel=&quot;nofollow&quot;&gt;Wu, W.; Massart, D. &amp;amp; de Jong, S.: The kernel PCA algorithms for wide data. Part I: Theory and algorithms , Chemometrics and Intelligent Laboratory Systems , 36, 165 - 172 (1997). DOI: http://dx.doi.org/10.1016/S0169-7439(97)00010-5&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;This paper discusses numerical and computational properties of 4 different algorithms for PCA: SVD, eigen decomposition (EVD), NIPALS and POWER. &lt;/p&gt;&#10;&#10;&lt;p&gt;They are related as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;computes on      extract all PCs at once       sequential extraction    &#10;X                SVD                           NIPALS    &#10;X'X              EVD                           POWER&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The context of the paper are &lt;em&gt;wide&lt;/em&gt; $\mathbf X^{(30 \times 500)}$, and they work on $\mathbf{XX'}$ (kernel PCA) - this is just the opposite situation as the one you ask about. So to answer your question about long matrix behaviour, you need to exchange the meaning of &quot;kernel&quot; and &quot;classical&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/69cim.png&quot; alt=&quot;performance comparison&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Not surprisingly, EVD and SVD change places depending on whether the classical or kernel algorithms are used. In the context of this question this means that one or the other may be better depending on the shape of the matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;But from their discussion of &quot;classical&quot; SVD and EVD it is clear that the decomposition of $\mathbf{X'X}$ is a very usual way to calculate the PCA. However, they do not specify which SVD algorithm is used other than that they use Matlab's &lt;code&gt;svd ()&lt;/code&gt; function. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    &amp;gt; sessionInfo ()&#10;    R version 3.0.2 (2013-09-25)&#10;    Platform: x86_64-pc-linux-gnu (64-bit)&#10;&#10;    locale:&#10;     [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C               LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8     LC_MONETARY=de_DE.UTF-8   &#10;     [6] LC_MESSAGES=de_DE.UTF-8    LC_PAPER=de_DE.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C            &#10;    [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       &#10;&#10;    attached base packages:&#10;    [1] stats     graphics  grDevices utils     datasets  methods   base     &#10;&#10;    other attached packages:&#10;    [1] microbenchmark_1.3-0&#10;&#10;loaded via a namespace (and not attached):&#10;[1] tools_3.0.2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$ dpkg --list libopenblas*&#10;[...]&#10;ii  libopenblas-base              0.1alpha2.2-3                 Optimized BLAS (linear algebra) library based on GotoBLAS2&#10;ii  libopenblas-dev               0.1alpha2.2-3                 Optimized BLAS (linear algebra) library based on GotoBLAS2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2013-12-09T14:39:02.133" Id="79072" LastActivityDate="2014-01-07T11:35:24.320" LastEditDate="2014-01-07T11:35:24.320" LastEditorUserId="4598" OwnerUserId="4598" ParentId="79043" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You have a case of &lt;strong&gt;data-dependent&lt;/strong&gt; &lt;a href=&quot;http://en.wikipedia.org/wiki/Truncated_regression_model&quot; rel=&quot;nofollow&quot;&gt;truncated regression model&lt;/a&gt;, which is easily handled by the R package &lt;a href=&quot;http://cran.r-project.org/web/packages/truncreg/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;truncreg&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some code that simulates such data, and then estimates the parameters of it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(truncnorm)&#10;library(truncreg)&#10;truncPoints = rnorm(100)&#10;vBeta = c(1, 3, 6)&#10;mX =  matrix(rnorm(100*3), nrow = 100)&#10;vY = rtruncnorm(100, a=truncPoints, &#10;                b=Inf, &#10;                mean = truncPoints + mX %*% vBeta,  # mean of the truncated normal &#10;                sd = 1)&#10;&#10;&#10;truncreg(vY ~ mX, point = truncPoints)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-09T18:17:32.533" Id="79096" LastActivityDate="2013-12-09T18:17:32.533" OwnerUserId="8141" ParentId="78915" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Psuedo F describes the ratio of between cluster variance to within-cluster variance. If  Psuedo F is decreasing, that means either the within-cluster variance is increasing or staying static (denominator) or the between cluster variance is decreasing (numerator).&lt;/p&gt;&#10;&#10;&lt;p&gt;Within cluster variance really just measures how tight your clusters fit together. The higher the number the more dispersed the cluster, the lower the number the more focused the cluster. Between cluster variance measures how seperated clusters are from each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;K-means objective is to minimize within cluster variance (necessarily maximizing between cluster variance). So the way that you can interpret this is: as the number of clusters increase the within cluster variance increases, making the actual clusters more dispersed/less compact and therefore less effective (and potentially closer to other clusters). &lt;/p&gt;&#10;&#10;&lt;p&gt;With that said, all of your interpretations are possible. But before going ahead and writing off k-means, you should try looking at the elbow method (plot # of clusters vs. between-group variance divided by total group variance) - if there's no elbow in the plot it's usually a good sign that k-means won't provide useful results (or at least that's my litmus test). &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example of the elbow method using R code (from &lt;a href=&quot;http://www.statmethods.net/advstats/cluster.html&quot; rel=&quot;nofollow&quot;&gt;http://www.statmethods.net/advstats/cluster.html&lt;/a&gt;). Where &quot;mydata&quot; is your data. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Determine number of clusters&#10;wss &amp;lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))&#10;for (i in 2:20) wss[i] &amp;lt;- sum(kmeans(mydata,centers=i)$withinss)&#10;plot(1:20, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;,&#10;ylab=&quot;Within groups sum of squares&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2013-12-09T19:19:49.820" Id="79104" LastActivityDate="2013-12-09T19:25:48.267" LastEditDate="2013-12-09T19:25:48.267" LastEditorUserId="776" OwnerUserId="776" ParentId="79097" PostTypeId="2" Score="1" />
  
  <row Body="&lt;pre&gt;&lt;code&gt;=SQRT(SUM(G7:G16*(H7:H16-(SUMPRODUCT(G7:G16,H7:H16)/SUM(G7:G16)))^2)/&#10;     ((COUNTIFS(G7:G16,&quot;&amp;lt;&amp;gt;0&quot;)-1)/COUNTIFS(G7:G16,&quot;&amp;lt;&amp;gt;0&quot;)*SUM(G7:G16)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Column &lt;code&gt;G&lt;/code&gt; are weights, Column &lt;code&gt;H&lt;/code&gt; are values&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-09T21:55:09.813" Id="79121" LastActivityDate="2013-12-09T21:58:19.053" LastEditDate="2013-12-09T21:58:19.053" LastEditorUserId="7290" OwnerUserId="35936" ParentId="6534" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;So there's a fairly well characterized difference between relative risk and absolute risk for conventional cohort studies, and for many questions, the absolute risk is arguably more appropriate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an analogous way to measure absolute difference for accelerated failure time models? I suspect there isn't, because these models assume proportional survival times, so the relative measure will be constant, but the absolute measure will vary over time, but I wanted to make sure before I abandoned this line of reasoning.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would it be possible to do it at a fixed point in time, say the median - and do you think this would be of any value?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-09T22:27:10.170" FavoriteCount="1" Id="79129" LastActivityDate="2013-12-19T08:58:54.573" OwnerUserId="5836" PostTypeId="1" Score="1" Tags="&lt;survival&gt;&lt;absolute-risk&gt;" Title="Absolute vs. Relative Difference in Survival Time - Is this possible?" ViewCount="54" />
  <row AnswerCount="0" Body="&lt;p&gt;I have some ranges of values and frequencies of each range. Participants choosed an entire range when asked, and not a particular value.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Range [0,20] : frequency = 20 &lt;/p&gt;&#10;&#10;&lt;p&gt;Range (20,60], frequency = 10 &lt;/p&gt;&#10;&#10;&lt;p&gt;Range (60, 100], frequency = 15 &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to figure out how to get a confidence interval, something like get an average interval of the values that i have, having a confidence level of 95%. &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that ranges represent for example how many minutes a person waited in line at the supermarket, I would like to say &quot;Hey...given the data, you will probably wait 30-40 minutes in line if you go to the supermarket&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;Can you help or point me to some links where I can read how to achieve this? &#10;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-09T22:46:31.860" Id="79131" LastActivityDate="2013-12-09T22:46:31.860" OwnerUserId="35938" PostTypeId="1" Score="0" Tags="&lt;confidence-interval&gt;" Title="computing confidence interval for ranges, confidence level 0.95" ViewCount="37" />
  <row AnswerCount="0" Body="&lt;p&gt;Could anyone give me a hint on the following problem? Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $f$ be a random variable over a probability space $(\Omega,A,P)$. Show that&lt;/p&gt;&#10;&#10;&lt;p&gt;$f$ is integrable $\iff $ $\sum\limits_{k=1}^\infty P(\{|f|&amp;gt;n\}) &amp;lt; \infty.$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: Consider the sets $A_n:=\{n-1&amp;lt;|f|\le n\}$, $n\in \mathbb{N}$ and the random variables $L:=\sum\limits_{k=1}^\infty (n-1)1_{A_n}$ and $R:=\sum\limits_{k=1}^\infty n1_{A_n}$,$n\in\mathbb{N}$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-10T00:04:16.627" Id="79140" LastActivityDate="2013-12-10T00:34:16.420" LastEditDate="2013-12-10T00:34:16.420" LastEditorUserId="8013" OwnerUserId="35483" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;random-variable&gt;&lt;integral&gt;" Title="Sufficient and necessary condition for the integrability of a random variable" ViewCount="35" />
  
  <row Body="&lt;p&gt;From my point of view the main difference between proportion and probability is the three axioms of probability which proportions don't have. i.e.&#10;(i) Probability always lies between 0 and 1.&#10;(ii) Probability sure event is one.&#10;(iii) P(A or B) = P(A) +P(B), A and B are mutually exclusive events&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-10T09:40:44.020" Id="79179" LastActivityDate="2013-12-10T09:44:58.327" LastEditDate="2013-12-10T09:44:58.327" LastEditorUserId="22047" OwnerUserId="35955" ParentId="1525" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm working with a dataset that contains information about consumption of apples. The dataset contains the amount of apple consumed in g/day. The problem with this is that the data points fall into 3 categories:&#10;- a real number (e.g. 54 g/day, people weighing their consumption) &#10;- a discrete number (number of apples per day), which is converted to g/day using the average weight of an apple (+/- 100g)&#10;- 0, people not consuming any apples&lt;/p&gt;&#10;&#10;&lt;p&gt;A histogram of the data looks like this:&#10;&lt;img src=&quot;http://i.stack.imgur.com/0I2xf.png&quot; alt=&quot;Histogram&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips on either transforming this data into something analyzable or fitting a distribution would be greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to use this data to perform a risk analysis. For this we need to fit a distribution to the values and then sample from this distribution. The classic distributions included in the software I use (@Risk) obviously deliver horrible performance (as measured by AIC/BIC criteria)&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-12-10T10:01:00.637" Id="79180" LastActivityDate="2013-12-10T10:30:35.937" LastEditDate="2013-12-10T10:30:35.937" LastEditorUserId="35956" OwnerUserId="35956" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;fitting&gt;&lt;mixture&gt;&lt;continuous&gt;" Title="Fitting a &quot;pseudo&quot; discrete dataset" ViewCount="51" />
  
  <row Body="&lt;p&gt;I only like the area under the ROC curve ($c$-index) because it happens to be a concordance probability.  $c$ is a building block of rank correlation coefficients.  For example, Somers' $D_{xy} = 2\times (c - \frac{1}{2})$.  For ordinal $Y$, $D_{xy}$ is an excellent measure of predictive discrimination, and the R &lt;code&gt;rms&lt;/code&gt; package provides easy ways to get bootstrap overfitting-corrected estimates of $D_{xy}$.  You can backsolve for a generalized $c$-index (generalized AUROC).  There are reasons not to consider each level of $Y$ separately because this does not exploit the ordinal nature of $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;code&gt;rms&lt;/code&gt; there are two functions for ordinal regression: &lt;code&gt;lrm&lt;/code&gt; and &lt;code&gt;orm&lt;/code&gt;, the latter handling continuous $Y$ and providing more distribution families (link functions) than proportional odds.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-10T12:46:56.360" Id="79191" LastActivityDate="2013-12-10T12:46:56.360" OwnerUserId="4253" ParentId="23992" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="79193" AnswerCount="1" Body="&lt;p&gt;What is the difference between doing linear regression with a Gaussian Radial Basis Function (RBF) and doing linear regression with a Gaussian kernel?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-10T12:53:12.197" FavoriteCount="3" Id="79192" LastActivityDate="2014-11-04T13:03:11.040" LastEditDate="2014-11-04T13:03:11.040" LastEditorUserId="-1" OwnerUserId="35965" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;normal-distribution&gt;&lt;kernel&gt;" Title="Gaussian RBF vs. Gaussian kernel" ViewCount="726" />
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: When trying to calculate the variance of timeseries sums I get a negative variance, mostly due to autocovariances at large lag steps. Does not seem realistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a timeseries which is calculated from another timeseries using a regression equation.&#10;I would like to propagate the uncertainty in the regression to the final timeseries. Then I want to sum (or take mean values) different segments of the timeseries over different timeperiods, and get the uncertainty of the sums. The timeseries is originally in 1 hour frequency and I want to sum over periods of 1 day (resampling to daily frequency) up to several years. The timeseries is strongly autocorrelated at short lag times.&lt;/p&gt;&#10;&#10;&lt;p&gt;For getting the variance of the sum (in the case of 3 elements being summed):&#10;$$Var(a+b+c)= \\ Var(a)+Var(b)+Var(c) + 2 \times (Cov(a,b) + Cov(a,c)+Cov(b,c))$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I use &lt;code&gt;r&lt;/code&gt; for the calculations. I get the variances for each timeseries element as $SE^2$, where $SE$ is the standard error (&lt;code&gt;se.fit&lt;/code&gt;) returned from r's &lt;code&gt;predict()&lt;/code&gt; function using the regression model. The covariances I get from the autocovariance function &lt;code&gt;acf()&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some code and a selection of the data (excuse clumsy R code, I'm very new to R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#tsY is the predicted timeseries from the regression&#10;tsY=c(81.4,  79.0,  83.4,   81.7,   75.7,   68.3,   62.3,   57.2,   52.6,   48.8,   45.4,   42.6,   39.9,   37.6,   35.6,   33.8,   32.2,   30.8,   29.6,   28.4,   27.3,   26.2,   25.0,   23.9)&#10;#tsSE is the standard error from the prediction (se.fit)&#10;tsSE=c(1.55,  1.49, 1.60,   1.56,   1.41,   1.23,   1.09,   0.97,   0.87,   0.78,   0.71,   0.65,   0.60,   0.55,   0.51,   0.48,   0.45,   0.42,   0.40,   0.38,   0.36,   0.34,   0.32,   0.30)&#10;&#10;tsVar=tsSE^2&#10;&#10;#create a matrix of the autocovariances at different lag times, diagonal is lag=0&#10;#rows and columns are indicies in timeseries&#10;covmat&amp;lt;-matrix(numeric(0), length(tsY),length(tsY)) &#10;for ( i in (1:(length(tsY)) ) ) {&#10;  if (i == 1) {&#10;    autocov&amp;lt;-acf(tsY, type='covariance', lag.max= length(tsY))&#10;    autocovvec&amp;lt;-autocov$acf[1:nrow(autocov$acf)]&#10;    covmat[i:length(tsY),i]=autocovvec&#10;  }  else {&#10;    autocov&amp;lt;-acf(tsY[-(1:i-1)], type='covariance', lag.max= length(tsY))&#10;    autocovvec&amp;lt;-autocov$acf[1:nrow(autocov$acf)]&#10;    covmat[i:length(tsY),i]=autocovvec&#10;  }&#10;&#10;}&#10;&#10;# sum the matrix columns, but not the diagonal&#10;sumofColumns &amp;lt;- rep(NA, ncol(covmat))&#10;for (i in (1:ncol(covmat))) {&#10;  if (i == 1) {&#10;    sumofColumns[i]=sum(covmat[-(1),i])  &#10;  } else{ &#10;    sumofColumns[i]=sum(covmat[-(1:i),i])  &#10;  }&#10;}&#10;&#10;sumofCov=sum(sumofColumns) # sum of the covariance (Cov(a,b) + Cov(a,c)+...)&#10;sumofVar=sum(tsVar) # sum of the variances of each timeseries element&#10;varofSum=sumofVar+2*sumofCov # variance of the sum of the timeseries&#10;&#10;# from the covmat the negative variance occurs at larger lag times.&#10;acf(tsY, type='covariance', lag.max= length(tsY))&#10;&#10;&amp;gt; sumofCov&#10;[1] -1151.529&#10;&amp;gt; varofSum&#10;[1] -2283.246&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;So I have the following questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;ol&gt;&#10;  &lt;li&gt;&lt;p&gt;Did I completely misunderstand how to calculate variance of sums?&lt;/p&gt;&lt;/li&gt;&#10;  &lt;li&gt;&lt;p&gt;Is it better to use a cutoff from the max lags to be considered in the autocovariance? If so how would one determine this? This would especially be important with the complete data where the length is several thousand. &lt;/p&gt;&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;  &#10;  &lt;p&gt;&lt;strike&gt;3. Why is the covariance negative in this sample data at large? When plotting tsY  &lt;code&gt;plot(tsY)&lt;/code&gt; it looks like the covariance/correlation should remain positive.&lt;/strike&gt; Because it is the variation in direction from their means.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Comment on &lt;strong&gt;question 2&lt;/strong&gt; above:&#10;  I have realized that using n-1 lags, as above in the code, does not make a lot of sense. There appear to be few different ways to determine the maximum lags to consider.  Box &amp;amp; Jenkins (1970) suggest n/4 and R by default 10*log10(n). This does not answer the question however, of how to determine an appropriate cutoff for summing the covariances.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Does it make sense to look at the partial autocorrelation (function pacf()), in order not to overestimate the effect of the auto covariance in the summation term? The partial autocorrelation for my data is significantly different from zero only at 1 or 2 lags. Similarly, fitting an AR model using ar() function, I also get an order of 1 or 2.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Cheers&lt;/p&gt;&#10;&#10;&lt;p&gt;Related post &lt;a href=&quot;http://stats.stackexchange.com/questions/10943/variance-on-the-sum-of-predicted-values-from-a-mixed-effect-model-on-a-timeserie&quot;&gt;Variance on the sum of predicted values from a mixed effect model on a timeseries&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-10T14:57:04.860" FavoriteCount="2" Id="79216" LastActivityDate="2013-12-14T00:19:00.350" LastEditDate="2013-12-13T13:35:33.580" LastEditorUserId="24139" OwnerUserId="24139" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;variance&gt;&lt;covariance&gt;&lt;autocorrelation&gt;" Title="Summing variance of autocorrelated timeseries" ViewCount="204" />
  <row Body="&lt;p&gt;Importance sampling is a method to evaluate an integral of the form&#10;$$E[h(X)] = \int h(x)f(x) dx$$&#10;when this cannot be directly computed. Instead of sampling from the density $f$ as in classical Monte Carlo integration, importance sampling (IS) samples from an arbitrary density $g$ which must be a valid pdf:&#10;$$E[h(X)] = \int h(x)\frac{f(x)}{g(x)}g(x) dx$$&#10;Based on samples $X_1,...,X_n$ generated from $g$ (and not from $f$), the expectation under $g$ converges in probability to&#10;$$\frac{1}{n} \sum_{j=1}^{n} \frac{f(X_j)}{g(X_j)}h(X_j) \rightarrow E[h(X)]$$&#10;as in classical Monte Carlo integration. IS has the advantage that it puts very little restrictions on the blanket function $g$. Usual choices for $g$ are standard distributions that are either easy to simulate or which are efficient in the approximation of the integral. Directly sampling from $f$ as in Monte Carlo integration is normally not efficient hence IS sampling requires fewer samples to achive the desired result.&lt;/p&gt;&#10;&#10;&lt;p&gt;For further reference see Robert, C.P. and Casella, G. (2004) &quot;Monte Carlo Statistical Methods&quot;, 2nd Edition, Springer Texts in Statistics, Springer Science+Business Media Inc., New York&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-10T15:29:47.990" Id="79218" LastActivityDate="2013-12-10T18:24:17.763" LastEditDate="2013-12-10T18:24:17.763" LastEditorUserId="26338" OwnerUserId="26338" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;In laymans terms, you are calculating the distance between the actual points and the predicted line, which you can do whether or not the line is linear. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can check it out easily in Excel.  If you're using 2007-2010, enter some x and y data, select it, then:&#10;1. Go to the Insert tab&#10;2. Charts section&#10;3. Scatter chart&#10;4. Select the chart to get the Chart Tools tabs&#10;5. Click on layout&#10;6. Analysis section&#10;7. Click on the trendline drop down&#10;8. Click on &quot;more trendline options&quot; (you can also access this by right clicking on the graph, adding a trendline, and then right clicking on the trendline to format it)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, you can display the R-squared and adjust the type of line being added to your data.  Watching your R squared automatically recalculate can help you get idea of what kind of model to use. &lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. Excel won't calculate R-squared values for lines where it doesn't make sense (ex. moving average trend lines)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-10T16:39:38.217" Id="79227" LastActivityDate="2013-12-10T16:46:34.600" LastEditDate="2013-12-10T16:46:34.600" LastEditorUserId="35977" OwnerUserId="35977" ParentId="79225" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There are methods for analyzing rank based endogenous variables in SEM. These are based on either polychoric correlations, or or ordinal/probit regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;LISREL uses the polychoric correlation methods (I believe). Mplus and Lavaan use ordinal logistic/probit methods. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because you're using an appropriate method, there are no bad things that happen. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example, comparing polr() in R with lavaan.&lt;/p&gt;&#10;&#10;&lt;p&gt;Set up the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lavaan)&#10;set.seed(54321)&#10;y &amp;lt;- sample(1:5, 1000, TRUE)&#10;x &amp;lt;- runif(1000)&#10;d &amp;lt;- as.data.frame(cbind(y, x))&#10;d$y &amp;lt;- ordered(d$y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Run polr:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fit1 &amp;lt;- polr(y ~ x, data=d)&#10;&amp;gt; summary(fit1)&#10;&#10;Re-fitting to get Hessian&#10;&#10;Call:&#10;polr(formula = y ~ x, data = d)&#10;&#10;Coefficients:&#10;Value Std. Error t value&#10;x -0.1445     0.1914 -0.7553&#10;&#10;Intercepts:&#10;    Value    Std. Error t value &#10;1|2  -1.4638   0.1233   -11.8671&#10;2|3  -0.4434   0.1142    -3.8835&#10;3|4   0.3597   0.1139     3.1590&#10;4|5   1.2790   0.1213    10.5468&#10;&#10;Residual Deviance: 3216.968 &#10;AIC: 3226.968 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Run very simple model in Lavaan:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fit2 &amp;lt;- sem(myModel, data=d, estimator=&quot;WLSMV&quot;)&#10;&amp;gt; summary(fit2)&#10;&#10;                   Estimate  Std.err  Z-value  P(&amp;gt;|z|)&#10;Regressions:&#10;  y ~&#10;    x                -0.086    0.118   -0.732    0.464&#10;&#10;Thresholds:&#10;    y|t1             -0.887    0.072  -12.314    0.000&#10;    y|t2             -0.275    0.069   -3.967    0.000&#10;    y|t3              0.227    0.069    3.276    0.001&#10;    y|t4              0.779    0.071   10.907    0.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Notice that the z/t-values are almsot the same ( 0.732 for lavaan,  0.755 for polr). The coefficients are about 1.6 times higher in polr than in lavaan because polr uses a logistic link function by default, but lavaan uses probit. If you add method=&quot;probit&quot; to the polr, the parameter estimates will be almost identical.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-10T17:35:38.030" Id="79237" LastActivityDate="2013-12-13T14:44:31.580" LastEditDate="2013-12-13T14:44:31.580" LastEditorUserId="17072" OwnerUserId="17072" ParentId="78893" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What good is finding the global optimum? You optimize some &lt;em&gt;number&lt;/em&gt;, not the actual result.&lt;/p&gt;&#10;&#10;&lt;p&gt;k-means is based on &lt;strong&gt;assumptions that won't hold on real data anyway&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So it's not much more than a &lt;em&gt;heuristic&lt;/em&gt; anyway.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why find the gloabl optimum of a heuristic?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-10T18:47:56.353" Id="79242" LastActivityDate="2013-12-10T18:47:56.353" OwnerUserId="7828" ParentId="79152" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;As written, your question can't work, since y is a 0-1 variable and you're doing logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;If&lt;/em&gt; you mean that the linear predictor had a nonlinear relationship with one of the independent variables, that is, $\eta = a + bf(x)$, say, for some nonlinear $f$ (with all other variables held constant), then you can write $x^* = f(x)$ and put $x^*$ in your logistic regression as an independent variable.  [In a logistic regression, $\eta = \text{logit}(P[Y=1])$]&lt;/p&gt;&#10;&#10;&lt;p&gt;This is quite commonly done in linear models and generalized linear models; there's a linear relationship, but it's with a transformed independent variable. Under the usual assumptions you need for a GLM, the transformed variable works perfectly well as a predictor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if $f$ is known &lt;em&gt;and&lt;/em&gt; that coefficient, $b$ is known, you don't put $x^*$ in as a predictor, because $x^{**} = bf(x)$ is then an alternative predictor with coefficient 1; those come in as offsets (e.g. specified in R by using the &lt;code&gt;offset&lt;/code&gt; argument). (In ordinary regression you could let $y^* = y-x^{**}$ instead, for the same effect.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I will assume the coefficient of $f$ is unknown (though you specified it to be 1).&lt;/p&gt;&#10;&#10;&lt;p&gt;In your particular case $x^* = f(x) = (x-4)^2$. If you were unsure about the &quot;4&quot; there (e.g. if it's just a rough guess or something, rather than a value that's definitely known), then you could instead use two new variables, $x^*_l = x-4$ and $x^*_q = (x-4)^2$ both as predictors, which will capture a general quadratic relationship (with the additional benefit that if the '4' is nearly right, the estimates be nearly uncorrelated with each other and with the intercept.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2013-12-10T22:21:27.997" Id="79260" LastActivityDate="2013-12-18T00:55:45.257" LastEditDate="2013-12-18T00:55:45.257" LastEditorUserId="805" OwnerUserId="805" ParentId="79259" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The number of parameters (n+3, including the unknown error variance) exceeds the size of the sample (that's the &quot;incidental parameters&quot; situation). The two orthogonality restrictions that you mention reduce the number of unknowns by 2, no more.&lt;/p&gt;&#10;&#10;&lt;p&gt;But if you are interested mainly in estimating the $\beta$'s, then these restrictions permit you (through a method-of-moments approach) to obtain unbiased and consistent estimates of the two betas by standard OLS. &#10;This has general validity -when a regressor is assumed orthogonal to the others in a multiple regression setting, it can &quot;leave the scene&quot; without affecting the least-square estimates of the other parameters.&lt;br&gt;&#10;The two orthogonality restrictions in moment notation are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(X_ig(X_i))=0,\;\; E(g(X_i))=0\;\; \forall i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The method of moments approach dictates to obtain estimates by estimating the sample analogues of these restrictions. These sample analogues are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac 1n \sum_{i=1}^nx_ig(x_i)=0,\;\; \frac 1n \sum_{i=1}^ng(X_i)=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;From the regression equation we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_i - \beta_0 - \beta_1 X_i - \epsilon_i = g(x_i) ,\;\; \forall i$$&#10;Substituting we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac 1n \sum_{i=1}^nx_i\Big(y_i - \beta_0 - \beta_1 x_i - \epsilon_i\Big)=0\\ \frac 1n \sum_{i=1}^n\Big(y_i - \beta_0 - \beta_1 x_i - \epsilon_i\Big)=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The unknown error term is ignored. Carrying out the multiplications we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac 1n \sum_{i=1}^n\Big(x_iy_i - \beta_0x_i - \beta_1 x_i^2 \Big)=0\\ \frac 1n \sum_{i=1}^n\Big(y_i - \beta_0 - \beta_1 x_i \Big)=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which lead us to the usual OLS estimators in a simple regression setting (the bar denoting the sample mean)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat \beta_1 = \frac {\frac 1n \sum_{i=1}^nx_iy_i - \bar y\bar x}{\frac 1n \sum_{i=1}^nx_i^2 - \bar x}, \;\; \hat \beta_0 = \bar y -\hat \beta_1\bar x$$&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-12-11T00:15:32.590" Id="79272" LastActivityDate="2013-12-11T00:15:32.590" OwnerUserId="28746" ParentId="79262" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="79280" AnswerCount="1" Body="&lt;p&gt;I have split my data set into four categories but one of them only has four data points in it. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have run K-S normality tests on the other three categories which are each normal and when the data set is not split into four it is also normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this fourth category, do I assume it is normal, and run a parametric analysis, or assume it is non-normal and run a nonparametric analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is one of many calculations I have run on the same method using the K-S test for normality but would it be inconsistent for me to suddenly switch to a Shapiro-Wilk test for this one analysis?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-11T00:31:44.160" FavoriteCount="1" Id="79273" LastActivityDate="2014-01-22T13:37:44.780" LastEditDate="2014-01-22T13:37:44.780" LastEditorUserId="2970" OwnerUserId="35454" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;&lt;normality&gt;&lt;kolmogorov-smirnov&gt;" Title="Small sample sizes and the Kolmogorov-Smirnov test" ViewCount="334" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a question regarding repeated measures and GLMs:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose i have counted the abundance of some species in lakes at different time points - every lake received a different treatment at time = 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df &amp;lt;- structure(list(y = c(1, 3, 5, 1, 4, 1, 4, 1, 1, 0, 5, 2, 3, 2, &#10;                           3, 2, 4, 4, 3, 2, 1, 3, 8, 1, 5, 4, 6, 3, 5, 0, 1, 2, 0, 2, 6, &#10;                           1, 7, 3, 3, 2, 11, 0, 0, 1, 0, 1, 3, 0, 10, 6, 6, 2, 9, 0, 0, &#10;                           2, 0, 0, 3, 1, 10, 7, 4, 3, 12, 0, 0, 1, 0, 2, 4, 0, 8, 5, 3, &#10;                           4, 8, 1, 3, 5, 0, 5, 4, 2, 3, 4, 4, 2, 7, 1, 8, 4, 3, 7, 5, 7, &#10;                           4, 7, 3, 4, 7, 2, 7, 5, 3, 3, 6, 12, 7, 7, 1, 5, 20, 4, 10, 4, &#10;                           3, 4, 14, 15, 4, 7, 3, 2, 14, 1, 8, 8, 1, 3, 9, 15), &#10;                     time = structure(c(1L, &#10;                             1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, &#10;                             2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, &#10;                             3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, &#10;                             5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, &#10;                             6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, &#10;                             7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, &#10;                             9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, &#10;                             10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L, &#10;                             11L, 11L, 11L, 11L, 11L, 11L, 11L), &#10;                                      .Label = c(&quot;-4&quot;, &quot;-1&quot;, &quot;0.1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;4&quot;, &quot;8&quot;, &quot;12&quot;, &#10;                                                 &quot;15&quot;, &quot;19&quot;, &quot;24&quot;), class = &quot;factor&quot;), &#10;                     treatment = structure(c(2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, &#10;                                             3L, 1L, 4L, 2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, 3L, 1L, 4L, &#10;                                             2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, 3L, 1L, 4L, 2L, 1L, 1L, &#10;                                             3L, 1L, 5L, 4L, 2L, 5L, 3L, 1L, 4L, 2L, 1L, 1L, 3L, 1L, 5L, &#10;                                             4L, 2L, 5L, 3L, 1L, 4L, 2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, &#10;                                             3L, 1L, 4L, 2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, 3L, 1L, 4L, &#10;                                             2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, 3L, 1L, 4L, 2L, 1L, 1L, &#10;                                             3L, 1L, 5L, 4L, 2L, 5L, 3L, 1L, 4L, 2L, 1L, 1L, 3L, 1L, 5L, &#10;                                             4L, 2L, 5L, 3L, 1L, 4L, 2L, 1L, 1L, 3L, 1L, 5L, 4L, 2L, 5L, &#10;                                             3L, 1L, 4L), &#10;                                           .Label = c(&quot;0&quot;, &quot;0.1&quot;, &quot;0.9&quot;, &quot;6&quot;, &quot;44&quot;), &#10;                                           class = &quot;factor&quot;), &#10;                     plots = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, &#10;                                         11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, &#10;                                         1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, &#10;                                         3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, &#10;                                         5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, &#10;                                         7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, &#10;                                         9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, &#10;                                         11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, &#10;                                         1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, &#10;                                         3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), &#10;                                       .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;), &#10;                                       class = &quot;factor&quot;)), &#10;                .Names = c(&quot;y&quot;,&quot;time&quot;, &quot;treatment&quot;, &quot;plots&quot;), &#10;                row.names = c(NA, -132L), class = &quot;data.frame&quot;)&#10;str(df)&#10;# y : Counts&#10;# time : sampling time&#10;# treatment: treatment applied &#10;# plots: every plot/lake forms a series, treatment has been applied to different plots&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here is the time-course of the counts plotted - the lines are just a smoother... &#10;&lt;img src=&quot;http://i.stack.imgur.com/YGitR.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;There seems to be some interaction between treatment and time (counts drop downafter t = 0, but then recover).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am mainly interested in the treatment and treatment:time interaction. The time effect is of minor interest - since it is known that there is time course over the year...&lt;/p&gt;&#10;&#10;&lt;p&gt;I could fit a negative-binomial GLM to this data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(MASS)&#10;# negbin GLM with interaction&#10;mod_nb &amp;lt;- glm.nb(y ~ time * treatment, data = df)&#10;(mod_nb_aov &amp;lt;- anova(mod_nb))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Q1: Does this take the repeated measure of the same lakes into account? (since i have time a fixed factor in the model)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I am not sure about Q1, I thought that I could use restricted permutations to take this into account (permuted lakes, keeping timely neighbored observations together).&lt;/p&gt;&#10;&#10;&lt;p&gt;This could be done with the permute-package quite easily - something along these lines:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(permute)&#10;# permute complete time-series, but not within series&#10;control &amp;lt;- how(within = Within(type = 'none'),&#10;               plots = Plots(strata = df$plots, type = 'free'),&#10;               nperm=200)&#10;permutations &amp;lt;- shuffleSet(nrow(df), control = control)&#10;&#10;out &amp;lt;- NULL&#10;for(i in 1:nrow(permutations)){&#10;  df_perm &amp;lt;- df[permutations[i, ] , c('time', 'treatment')]&#10;  out[[i]] &amp;lt;-  glm.nb(df$y ~ time * treatment, data = df_perm)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Q2-x: But the I wondered if this is not redundant? - Keeping time and restricting permutations. Maybe I should fit a model &lt;code&gt;y ~ time:treatment + treatment&lt;/code&gt;?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that this could be done using mixed models, however is there also a way via &lt;strong&gt;restricted permutations&lt;/strong&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope I have clearly described the problem...&#10;Any thoughts are appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-11T09:47:37.847" Id="79311" LastActivityDate="2014-02-11T08:34:52.887" LastEditDate="2013-12-11T13:05:46.620" LastEditorUserId="1050" OwnerUserId="1050" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;generalized-linear-model&gt;&lt;negative-binomial&gt;&lt;permutation&gt;" Title="Repeated Measures GLM / restricted permutations" ViewCount="139" />
  <row Body="&lt;p&gt;You need to recalculate the PCA for each of the CV surrogate models. Acutally the same already applies to possible previous pre-processing steps that use more than just one case for their calculation (e.g. centering, variance scaling).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;But I just want to decide the optimal dimension k via PCA. Is it more convenient to do it once?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;More convenient, maybe, but the result will be overfit as @aplassard explains. This means that &lt;strong&gt;you cannot get the correct $k$ without recalculating the PCA&lt;/strong&gt;: doing the PCA on the whole data will yield too high $k$. This will be particularly serious with small $n$. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;When optimizing the model complexity, in theory a constant optimistic bias does not hurt the decision. However, in practice, the optimistically biased &quot;shortcuts&quot; I've encountered so far are not even close to constant but point towareds too high model complexity.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You say that you are in a small $n$ situation. In that case, is there any chance that you could fix $k$ for the PCA by your knowledge about data and application? In small sample size situations, two things happen&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You likely do not have enough samples to do the model comparisons necessary for the optimization (fixing of $k$).&lt;/li&gt;&#10;&lt;li&gt;But nevertheless attempting to do this means that you need a second, outer (independent) validation, thus splitting your data again. Unless, of course,  you'll validate against a data set that you don't have before.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For what it is worth, for the data sets I usually work with (spectroscopic classification, p ca. 10² - 10³, n &amp;lt; 100, sometimes even n &amp;lt; 10 (but 100 - 1000 measurements/rows per patient/batch/case) not recalculating the PCA within the CV loop &lt;strong&gt;underestimates the error typically by a factor of at least 2 - 3&lt;/strong&gt;, and I've seen &quot;perfect&quot; classification internally being only 80 % or even 65% with proper separation of test and training data in the crossvalidation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-11T11:14:19.303" Id="79317" LastActivityDate="2013-12-11T11:14:19.303" OwnerUserId="4598" ParentId="79277" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Is there any statistical tool available to test the hypothesis regarding differences in means of two independent &lt;strong&gt;non-randomly&lt;/strong&gt; selected samples. Is &lt;strong&gt;Mann-Whitney-Wilcoxon test&lt;/strong&gt; suitable for such conditions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-11T12:02:18.753" Id="79324" LastActivityDate="2013-12-11T12:18:59.670" LastEditDate="2013-12-11T12:18:59.670" LastEditorUserId="22047" OwnerUserId="36026" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;nonparametric&gt;" Title="Two independent non-random samples" ViewCount="29" />
  <row Body="&lt;p&gt;I'm far from an expert on statistics, but one thing that has been emphasised in the stats courses I have done to date is the issue of &quot;practical significance&quot;. I believe this alludes to what what Jeromy and gung are talking about when referring to &quot;effect size&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;We had an example in class of a 12 week diet that had statistically significant weight loss results, but the 95% confidence interval showed a mean weight loss of between 0.2 and 1.2 kg (OK, data was probably made up but it illustrates a point). While &quot;statistically significantly&quot;&quot; different from zero, is a 200gram weight loss over 12 weeks a &quot;practically significant&quot; result to an overweight person trying to get healthy?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-11T12:12:38.507" Id="79326" LastActivityDate="2013-12-11T12:12:38.507" OwnerUserId="36029" ParentId="79289" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Hey I have a question that I found in a textbook for some practice before a test, but there aren't any solutions for it. I'm pretty sure it's related to testing a hypothesis, but I'm not sure. &lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone can point me into the right direction of solving this that would be great.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Q1: Five soft drink bottling companies have agreed to implement a time management program in hopes of increasing productivity (measured in cases of soft drinks bottled per hour). The number of cases of soft drinks bottled per hour before and after implementation of the program are listed below. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        Company&#10;     #  1   2   3   4   5&#10;Before 500 475 525 490 530&#10;After  510 480 525 495 533&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Test at $\alpha$ = .05 if the time management program is efficient in increasing the productivity.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-11T14:00:15.437" Id="79334" LastActivityDate="2013-12-11T16:10:45.053" LastEditDate="2013-12-11T16:10:45.053" LastEditorUserId="88" OwnerUserId="36035" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;probability&gt;&lt;self-study&gt;" Title="Testing hypothesis &amp; $\alpha$" ViewCount="177" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a ranked list say A,B,C,D,E,F,G and a subset of this list ranked with another strategy &#10;lets say D,B,C,F. And my assumption is that the strategy used for ranking the sublist is better because its a feedback strategy. Now I want to re-rank my initial ranked list according to feedback ranking of sublist and initial ranking available. Please, suggest some good algorithm in this direction&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-11T16:07:22.863" FavoriteCount="1" Id="79353" LastActivityDate="2014-03-06T07:48:57.513" LastEditDate="2013-12-11T16:09:31.533" LastEditorUserId="88" OwnerUserId="36039" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;ranking&gt;" Title="Re-ranking a list" ViewCount="77" />
  <row AnswerCount="1" Body="&lt;p&gt;I have data collected from an experiment organized as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Two sites, each with 30 trees. 15 are treated, 15 are control at each site. From each tree, we sample three pieces of the stem, and three pieces of the roots, so 6 level 1 samples per tree which is represented by one of two factor levels (root, stem). Then, from those stem / root samples, we take two samples by dissecting different tissues within the sample, which is represented by one of two factor levels for tissue type (tissue type A, tissue type B). These samples are measured as a continuous variable. Total number of observations is 720; 2 sites * 30 trees * (three stem samples + three root samples) * (one tissue A sample + one tissue B sample). Data looks like this...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        ï..Site Tree Treatment Organ Sample Tissue Total_Length&#10;    1        L  LT1         T     R      1 Phloem           30&#10;    2        L  LT1         T     R      1  Xylem           28&#10;    3        L  LT1         T     R      2 Phloem           46&#10;    4        L  LT1         T     R      2  Xylem           38&#10;    5        L  LT1         T     R      3 Phloem          103&#10;    6        L  LT1         T     R      3  Xylem           53&#10;    7        L  LT1         T     S      1 Phloem           29&#10;    8        L  LT1         T     S      1  Xylem           21&#10;    9        L  LT1         T     S      2 Phloem           56&#10;    10       L  LT1         T     S      2  Xylem           49&#10;    11       L  LT1         T     S      3 Phloem           41&#10;    12       L  LT1         T     S      3  Xylem           30&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am attempting to fit a mixed effects model using R and lme4, but am new to mixed models. I'd like to model the response as the Treatment + Level 1 Factor (stem, root) + Level 2 Factor (tissue A, tissue B), with random effects for the specific samples nested within the two levels. &lt;/p&gt;&#10;&#10;&lt;p&gt;In R, I am doing this using lmer, as follows&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit &amp;lt;- lmer(Response ~ Treatment + Organ + Tissue + (1|Tree/Organ/Sample)) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;From my understanding (...which is not certain, and why I am posting!) the term:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(1|Tree/Organ/Sample)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Specifies that 'Sample' is nested within the organ samples, which is nested within the tree. Is this sort of nesting relevant / valid? Sorry if this question is not clear, if so, please specify where I can elaborate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks to any help you can offer.&lt;/p&gt;&#10;&#10;&lt;p&gt;~Erik&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-11T16:48:08.407" FavoriteCount="1" Id="79360" LastActivityDate="2013-12-11T21:20:14.580" OwnerUserId="36046" PostTypeId="1" Score="3" Tags="&lt;lmer&gt;&lt;model&gt;&lt;lme4&gt;&lt;mixed&gt;" Title="Mixed Effects Model with Nesting" ViewCount="604" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a survey dataset. We asked numerous questions to get at one point: &quot;how integrated are you in your community?&quot; I want to run factor analysis to get a &quot;community integration&quot; factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I&#10;(1) run FA on all the survey variables, and use the factor that looks like the &quot;community integration&quot; factor&#10;OR&#10;(2) run FA on just the &quot;community integration&quot; variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;When I run option 1, there is a factor that is clearly the &quot;community integration.&quot; So both options are feasible.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the implications of the two methods? What, theoretically or mathematically, is the difference between what I am capturing between the two approaches?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-11T18:04:07.697" Id="79368" LastActivityDate="2013-12-11T18:04:07.697" OwnerUserId="22088" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;dataset&gt;&lt;factor-analysis&gt;" Title="Run factor analysis on all variables or just those relevant to the factor we seek?" ViewCount="25" />
  <row AnswerCount="2" Body="&lt;p&gt;This might be a question in general: due to computational burden, I have to use a subset of my complete data (say, 1,000 out of the complete 10,000 observations) to get a p-value of a test. The test itself is from Monte Carlo simulations. My question is, is there a way to quantify the uncertainty of the p-value &lt;strong&gt;due to the use of a subset of the 1,000 observations instead of using the complete dataset?&lt;/strong&gt; Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-11T18:27:23.133" Id="79371" LastActivityDate="2014-03-15T16:28:13.670" OwnerUserId="14156" PostTypeId="1" Score="1" Tags="&lt;p-value&gt;&lt;simulation&gt;&lt;uncertainty&gt;" Title="standard error of the estimated p-values from simulations" ViewCount="96" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a Poisson approximation to binomial question, posted below. I'm not too sure if I'm using the proper formula: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(x) = e^{-np}(np)^x/x!$$ .     &lt;/p&gt;&#10;&#10;&lt;p&gt;Of anyone can tell me if I'm doing this right, that would be great. &lt;/p&gt;&#10;&#10;&lt;p&gt;Again these are just practice questions not homework.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Q: A salesperson has found that the probability of making a sale on a&#10;  particular product manufactured by him or her company is .05. If the salesperson&#10;  contacts 140 potential customers, what is the probability he or she will sell at least&#10;  2 of these products? Use and justify Poisson approximation to Binomial.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What I'm doing:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(x) = e^{-140 (.05)}(140*.05)^2/2!$$ .   &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-12-11T22:24:43.830" Id="79397" LastActivityDate="2013-12-29T02:37:57.847" LastEditDate="2013-12-12T01:34:45.153" LastEditorUserId="22047" OwnerUserId="36035" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;binomial&gt;&lt;poisson&gt;" Title="Poisson Approximation to Binomial" ViewCount="277" />
  <row AcceptedAnswerId="79407" AnswerCount="1" Body="&lt;p&gt;I have run a multiple regression in which the model as a whole is significant and explains about 13% of the variance. However, I need to find the amount of variance explained by each significant predictor. How can I do this using R?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's some sample data and code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;D = data.frame(&#10;    dv = c( 0.75, 1.00, 1.00, 0.75, 0.50, 0.75, 1.00, 1.00, 0.75, 0.50 ),&#10;    iv1 = c( 0.75, 1.00, 1.00, 0.75, 0.75, 1.00, 0.50, 0.50, 0.75, 0.25 ),&#10;    iv2 = c( 0.882, 0.867, 0.900, 0.333, 0.875, 0.500, 0.882, 0.875, 0.778, 0.867 ),&#10;    iv3 = c( 1.000, 0.067, 1.000, 0.933, 0.875, 0.500, 0.588, 0.875, 1.000, 0.467 ),&#10;    iv4 = c( 0.889, 1.000, 0.905, 0.938, 0.833, 0.882, 0.444, 0.588, 0.895, 0.812 ),&#10;    iv5 = c( 18, 16, 21, 16, 18, 17, 18, 17, 19, 16 ) )&#10;fit = lm( dv ~ iv1 + iv2 + iv3 + iv4 + iv5, data=D )&#10;summary( fit )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's the output with my actual data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call: lm(formula = posttestScore ~ pretestScore + probCategorySame + &#10;    probDataRelated + practiceAccuracy + practiceNumTrials, data = D)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-0.6881 -0.1185  0.0516  0.1359  0.3690 &#10;&#10;Coefficients:&#10;                  Estimate Std. Error t value Pr(&amp;gt;|t|)&#10; (Intercept)        0.77364    0.10603    7.30  8.5e-13 ***&#10; iv1                0.29267    0.03091    9.47  &amp;lt; 2e-16 ***&#10; iv2                0.06354    0.02456    2.59   0.0099 **&#10; iv3                0.00553    0.02637    0.21   0.8340&#10; iv4               -0.02642    0.06505   -0.41   0.6847&#10; iv5               -0.00941    0.00501   -1.88   0.0607 .  &#10;--- Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 0.18 on 665 degrees of freedom&#10; Multiple R-squared:  0.13,      Adjusted R-squared:  0.123&#10; F-statistic: 19.8 on 5 and 665 DF,  p-value: &amp;lt;2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This question has been answered &lt;a href=&quot;http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression&quot;&gt;here&lt;/a&gt;, but the accepted answer only addresses uncorrelated predictors, and while there is an additional response that addresses correlated predictors, it only provides a general hint, not a specific solution. I would like to know what to do if my predictors are correlated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-11T22:35:05.907" Id="79399" LastActivityDate="2013-12-12T05:15:05.123" OwnerUserId="12647" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;variance&gt;" Title="Calculate variance explained by each predictor in multiple regression using R" ViewCount="3212" />
  <row Body="&lt;p&gt;Yes, absolutely--with another Monte Carlo simulation.  Here's what you do: perform your 1000-observation subsampling exercise $N$ times, drawing randomly from the 10000 observations of the full data set with replacement each time.  On each iteration, calculate your $p$-value, using whatever procedure you have already previously defined.  You will end up with a distribution of $N$ different $p$-values, and you can do things for example like calculating the one sigma standard deviation of that distribution in order to calculate the uncertainty due to the random sampling procedure itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, for this specific case, with samples of 1000 observations drawn from a master population of only 10000, such a Monte Carlo procedure may not make much sense for you from a speed or efficiency perspective, because by the time you get to $N=10$ (roughly about the of minimal number of draws you would want to have in order to begin calculating a meaningful standard deviation estimate for $p$), you will already be drawing a total of 10000 samples anyway just in order to obtain your estimates.  On the other hand, if you had a total population of 10,000,000 observations, and your goal was to obtain a $p$-value using only 100 samples instead of 1000, then setting $N=10$ or even $N=100$ would not be so unreasonable, because then the total number of samples that you would use (either 10*100=1000 or 100*100=10000, depending on how large you set $N$) in your calculation procedure would still be far less than the master population of 10,000,000.  Anyway, in principle the method is sound, you just have to be a little careful with how large you choose your set of sample observations, and how many times $N$ your run the test, in order to obtain a &quot;good-enough&quot; statistical estimate of the information that you want without having to expend more computational effort than necessary to achieve it.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-11T23:01:37.887" Id="79402" LastActivityDate="2013-12-11T23:01:37.887" OwnerUserId="28566" ParentId="79371" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I think this comes from the fact that in the real world you don't really expect the standard null hypothesis to be true. If you're comparing the means of two populations, the null hypothesis says that $\mu_1 = \mu_2$, that is the two means are &lt;em&gt;exactly&lt;/em&gt; equal. In many situations however a more accurate null hypothesis would say that $\mu_1$ and $\mu_2$ are &lt;em&gt;almost&lt;/em&gt; equal (whatever that means).&lt;/p&gt;&#10;&#10;&lt;p&gt;For small sample sizes, the difference between means will only give a low p-value if the measured difference is &quot;relatively&quot; large. However for sufficiently large sample sizes even a tiny difference in means can become statistically significant, even though for practical purposes the numbers are the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is some good information for this question here as well:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/79289/why-is-statistically-significant-not-enough/79293#79293&quot;&gt;Why is &amp;quot;statistically significant&amp;quot; not enough?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-12T02:11:37.983" Id="79414" LastActivityDate="2013-12-12T02:11:37.983" OwnerUserId="27840" ParentId="79406" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;I have taken one introductory statistics class and am finishing up an econometrics class, but these classes are more about applications and just applying formulas without any explanations as to how these formulas came about.  I am looking for a good and thorough textbook or book on statistical and probability theory where the author explains the beginnings, the theory, the intuition behind these formulas.  Not just proofs but exactly how were these formulas born.&lt;/p&gt;&#10;&#10;&lt;p&gt;i.e: How did the formula to calculate variance come about?  How was this formula derived?  What exactly and intuitively are degrees of freedom and how did the concept come about? Where did probability distributions arise? How were all these statistical formulas thought up in the first place? These are just a few of many questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;My mathematics background includes: multivariable calc, linear algebra, intro stats, discrete mathematics.&lt;/p&gt;&#10;" ClosedDate="2013-12-12T07:15:03.280" CommentCount="1" CreationDate="2013-12-12T02:43:55.477" FavoriteCount="2" Id="79415" LastActivityDate="2013-12-12T16:49:47.440" OwnerUserId="35841" PostTypeId="1" Score="1" Tags="&lt;references&gt;&lt;education&gt;" Title="Good book for Statistical and Probability Theory" ViewCount="256" />
  
  <row Body="&lt;p&gt;The real problem is that this question is misguided. It is not machine learning vs statistics, it is machine learning against real scientific advance. If a machine learning device gives the right predictions 90% of the time but I cannot understand &quot;why&quot;, what is the contribution of machine learning to science at large? Imagine if machine learning techniques were used to predict the positions of planets: there would be a lot of smug people thinking that they can accurately predict a number of things with their SVMs, but what would they really know about the problem they have in their hands? Obviously, science does not really advance by numerical predictions, it advances by means of models (mental, mathematical) who let us see far beyond than just numbers.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-12-12T06:28:14.313" CreationDate="2013-12-12T06:28:14.313" Id="79429" LastActivityDate="2013-12-12T06:28:14.313" OwnerUserId="36080" ParentId="6" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The Pareto distribution was developed in the late 19th century by Italian economist Vilfredo Pareto to describe the allocation of income and wealth among individuals. The distribution has found application in many other areas such as computing science, civil engineering, stock markets, physics, astronomy or insurance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Its pdf is given by&#10;$$f(x|a,b) = \frac{ba^b}{x^{b+1}}$$&#10;for $x \geq a &amp;gt; 0$ and $b&amp;gt;0$, where $a$ provides the lower bound that a Pareto distributed random variable can take whilst $b$ determines the heaviness of the right tail. &lt;/p&gt;&#10;&#10;&lt;p&gt;The first two moments are&#10;$$\frac{ab}{b-1}$$&#10;for the mean and&#10;$$\frac{ba^2}{(b-1)^2(b-2)}$$&#10;for the variance (with $b&amp;gt;2$).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-12T09:20:20.240" Id="79439" LastActivityDate="2013-12-12T11:15:04.877" LastEditDate="2013-12-12T11:15:04.877" LastEditorUserId="805" OwnerUserId="26338" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;Whatever you do, plot your raw data or minimally make them available some how. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you choose median absolute deviation (MAD), do make it absolutely clear whether it is of deviations from the mean or the median, as I've seen MAD used as an abbreviation for both and in any case any ambiguity benefits no-one. &lt;/p&gt;&#10;&#10;&lt;p&gt;Plotting +/- MAD as error bars has a loose connection to the widely used box plots in which median and quartiles are shown in a box and there are various different recipes for what is shown outside the box. &lt;/p&gt;&#10;&#10;&lt;p&gt;MAD is approximately |quartile $-$ median| in a symmetric distribution. For a symmetric distribution it's immaterial whether MAD is MAD from median and or from mean or &quot;quartile&quot; is the upper or lower quartile. MAD will be similar to (upper q. $-$ median) and (median $-$ lower q.)  even in many asymmetric distributions. There are various slightly different rules for quartiles, which may cause minor puzzles, but is not central here. &lt;/p&gt;&#10;&#10;&lt;p&gt;A bigger question is this: if outliers make your standard errors dubious, how come you want to show means, because they will be affected too? As @John implies, a median is clearly a possibility. Also, would you be better off on a logarithmic or other transformed scale for your y variable too? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-12T10:20:37.123" Id="79442" LastActivityDate="2013-12-12T11:25:18.140" LastEditDate="2013-12-12T11:25:18.140" LastEditorUserId="22047" OwnerUserId="22047" ParentId="79420" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You can have a look at the paper &quot;&lt;a href=&quot;http://souhaib-bentaieb.com/conferences/boosting-multi-step-autoregressive-forecasts/&quot; rel=&quot;nofollow&quot;&gt;Boosting multi-step autoregressive forecasts&lt;/a&gt;&quot; by Souhaib Ben Taieb and Rob J. Hyndman.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main idea is to boost a traditional autoregressive (linear) model using a gradient boosting approach.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-12T10:39:38.860" Id="79443" LastActivityDate="2013-12-12T10:39:38.860" OwnerUserId="9447" ParentId="30171" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for some advice for a colleague who is dealing with regression models for which it is know, that the continuous covariate of interest $X_1$ was measured with error. More precisely, we know only know that the true $X_1$ lies within an interval $(X_L, X_R)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My first idea was to illustrate/describe the effect of the measurment error by fitting three models of the form&#10;\begin{align*}&#10;\text{Model 1:} \quad Y &amp;amp; = \beta_0 + \beta_1X_M + \beta_2 X_2  +... \\&#10;\text{Model 2:} \quad Y &amp;amp; = \beta_0 + \beta_1X_L + \beta_2 X_2  +... \\&#10;\text{Model 3:} \quad Y &amp;amp; = \beta_0 + \beta_1X_R + \beta_2 X_2  +... \\&#10;\end{align*}&#10;where $X_M$ is the midpoint of the interval. &lt;/p&gt;&#10;&#10;&lt;p&gt;What other regression methods are available that take into account that the true value of $X_1 \in (X_L, X_R)$? Specifically, when using linear regression models, logistic regression models, or generalized linear mixed models.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is some information on the Wikipedia-Site about Errors-in-variables models (&lt;a href=&quot;http://en.wikipedia.org/wiki/Errors-in-variables_models&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Errors-in-variables_models&lt;/a&gt;), but I'm not sure if those methods can be applied in this case.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-12T11:10:31.197" Id="79444" LastActivityDate="2013-12-12T11:10:31.197" OwnerUserId="35286" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;measurement-error&gt;&lt;error-propagation&gt;" Title="Regression with error in covariates" ViewCount="49" />
  
  <row AcceptedAnswerId="79718" AnswerCount="1" Body="&lt;p&gt;I want to simulate or calculate probabilities of combinations of group membership for different sample sizes (e.g., n= 3, 4, 5, 10, or 100) for two groups (of the same sample size). Each outcome could be male/female and young/old. The population is 50% male and 50% young.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the probability of getting: &lt;/p&gt;&#10;&#10;&lt;p&gt;all male in Group 1 AND all female in Group 2 &lt;/p&gt;&#10;&#10;&lt;p&gt;OR &lt;/p&gt;&#10;&#10;&lt;p&gt;all male in Group 2 AND all female in Group 1 &lt;/p&gt;&#10;&#10;&lt;p&gt;OR &lt;/p&gt;&#10;&#10;&lt;p&gt;all young in Group 1 AND all old in Group 2&lt;/p&gt;&#10;&#10;&lt;p&gt;OR &lt;/p&gt;&#10;&#10;&lt;p&gt;all young in Group 2 AND all old in Group 1&lt;/p&gt;&#10;&#10;&lt;p&gt;I would also like to be able to include additional categories (e.g., eats their vegetables, doesn't eat their vegetables) and be able to choose the percent of the population that falls into one category or the other. The categories can be independent of each other, but it would be better if there was the ability to make membership dependent (e.g., females eat their vegetables 70% of the time and males 50% of the time). It would also be better if this was not limited to categories with 2 types (e.g., be able to do 4th, 5th, or 6th grader).&lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to calculate the probability of getting completely unbalanced groups for different combinations of confounds and I can't figure out how to do this with R. This is an expansion on &lt;a href=&quot;http://stats.stackexchange.com/questions/74350/is-randomization-reliable-with-small-samples&quot;&gt;this question&lt;/a&gt;, but the approach used there is slow for large sample sizes and sort of convoluted.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Reworded question:&#10;There are two groups that are independent samples from the same population of gradeschoolers, call them treatment and control. The population consists of 50% males, 50% females, 25% each in 3rd-6th grade. There are equal number of males and females in each grade. Also 75% of females eat vegetables, while only 50% of males eat vegetables regardless of schoolgrade. &lt;/p&gt;&#10;&#10;&lt;p&gt;For different sample sizes I want to calculate the chance of getting&lt;/p&gt;&#10;&#10;&lt;p&gt;All males in one group (treatment/control) while there are all females in the other&lt;/p&gt;&#10;&#10;&lt;p&gt;OR&lt;/p&gt;&#10;&#10;&lt;p&gt;All students of the same grade in one group while the second group consists of all students of the same grade but different than the first. For example all 3rd graders in the treatment group and all 6th graders in the control group.&lt;/p&gt;&#10;&#10;&lt;p&gt;OR&lt;/p&gt;&#10;&#10;&lt;p&gt;All vegetable eaters in one group and all non-vegetable eaters in the second group.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-12T13:33:51.963" Id="79457" LastActivityDate="2013-12-19T08:25:17.627" LastEditDate="2013-12-12T20:11:28.657" LastEditorUserId="31334" OwnerUserId="31334" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;permutation&gt;&lt;randomization&gt;&lt;confounding&gt;" Title="How to calculate permutations of categorical variables with R" ViewCount="370" />
  <row Body="&lt;p&gt;Here is a solution using R:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/P3Hrq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R Code:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Make up data&#10;age&amp;lt;-runif(10, 30,70)&#10;agesd&amp;lt;-runif(10, 0.1,5)&#10;bpf&amp;lt;-runif(10, 0,1)&#10;bpfsd&amp;lt;-runif(10, 0.01,.2)    &#10;pop.size&amp;lt;-runif(10,5,50)&#10;&#10;#The plot&#10;plot(age,bpf, pch=16, cex=log(pop.size), col=rainbow(length(size)), &#10;ylim=c(0,1),xlim=c(20,90))&#10;segments(age+agesd,bpf,age-agesd,bpf, lwd=2)&#10;segments(age,bpf+bpfsd,age,bpf-bpfsd, lwd=2)    &#10;legend(&quot;topright&quot;, legend=paste(&quot;Study&quot;,1:10), &#10;col=rainbow(length(size)), pt.cex=1.5, pch=16)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-12-12T14:12:30.160" Id="79462" LastActivityDate="2013-12-12T14:12:30.160" OwnerUserId="31334" ParentId="79447" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="79487" AnswerCount="1" Body="&lt;p&gt;How do you prove that GARCH(1,1) = ARCH(infinity)? Need some guidance on how to start on this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-12T18:29:36.720" Id="79485" LastActivityDate="2013-12-12T18:49:15.883" LastEditDate="2013-12-12T18:49:15.883" LastEditorUserId="7290" OwnerUserId="8662" PostTypeId="1" Score="0" Tags="&lt;garch&gt;&lt;proof&gt;" Title="Why is GARCH(1,1) = ARCH(infinity)?" ViewCount="327" />
  <row AcceptedAnswerId="79506" AnswerCount="1" Body="&lt;p&gt;I've recently come across an old Excel sheet that is used to help assess whether someone has collected enough samples when performing a time study (essentially trying to figure out the distribution of time required to complete a task). In the data column (labeled &quot;time&quot;), the person performing the time study enters the length of time it took to complete a task, with each row representing one trial. The sheet contains the following formula, labeled &quot;Current % Error&quot;:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}\frac{t \sigma}{\mu \sqrt{n-1}}\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $t$ is a t-score which varies based on sample size, $\mu$ is sample average, and $\sigma$ is sample standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, &lt;strong&gt;what algorithm is this&lt;/strong&gt;? I know that we can measure confidence intervals using the similar equation $\mu \pm \frac{t \sigma}{\sqrt{n-1}}$, but I'm not sure what's being calculated when we divide by the mean. What does this correspond to, if anything?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tpk8V.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-12T21:23:04.647" Id="79503" LastActivityDate="2013-12-13T23:39:09.763" LastEditDate="2013-12-12T21:33:33.103" LastEditorUserId="919" OwnerUserId="2019" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;" Title="Help identifying an error function" ViewCount="44" />
  <row Body="&lt;p&gt;Intuitively, I figured that person B's didn't help, since there was no correlation.  However, I went back to Bays to solve it more rigorously and found an (at least interesting to me) result.&lt;/p&gt;&#10;&#10;&lt;p&gt;assume N choices, all of which are equally probable&lt;/p&gt;&#10;&#10;&lt;p&gt;P[ans is 1 | A says 1 AND B says 1] = P[ A says 1 AND B says 1 | ans is 1 ] P[ ans is 1 ] / P[ A says 1 and B says 1 ]&lt;/p&gt;&#10;&#10;&lt;p&gt;PP[ans is 1 | A says 1 AND B says 1] = (3/5) (2/5) (1/N) / ( (3/5) (2/5) (1/N) + (N-1) (1/N) (1/(N-1)) (2/5) (3/5) ), explanation below&lt;/p&gt;&#10;&#10;&lt;p&gt;PP[ ans is 1 | A says 1 AND B says 1] = 1 / 2&lt;/p&gt;&#10;&#10;&lt;p&gt;The other 1/2 chance is spread over all other (the incorrect) answers, but if there are only two choices (i.e. true/false), then in the event that A and B agree, you can guess either, it does not matter.  You have less confidence if B agrees.&lt;/p&gt;&#10;&#10;&lt;p&gt;*explanation of P[]&lt;/p&gt;&#10;&#10;&lt;p&gt;since A is right 3/5 and B is right 2/5,&#10;P[ A says 1 AND B says 1 | ans is 1 ] = P[ A 1 | ans 1 ] P[ B 1 | ans 1 ] = (3/5) (2/5)&lt;/p&gt;&#10;&#10;&lt;p&gt;since the a priori probability of answer 1 is 1/N, &#10;P[ A says 1 and B says 1 ] = P[ A says 1 AND B says 1 | ans is 1 ] P[ ans is 1 ] = (3/5) (2/5) (1/N)&lt;/p&gt;&#10;&#10;&lt;p&gt;P[ A says 1 and B says 1 ] = P[ A 1 and B 1 | ans 1 ] P[ ans 1 ] + P[ A 1 and B 1 | ans NOT 1 ] P[ ans NOT 1 ]&#10;since there are N-1 ways for the answer to not be 1, each with a priori probability 1/N, but only a (1/(N-1)) chance to be wrong in such a way to suggest the answer is 1, and the probabilities of being wrong are A 2/5 and B 3/5,&#10;P[ A says 1 and B says 1 ] = (3/5) (2/5) (1/N)  + (N-1) (1/N) (1/(N-1)) (2/5) (3/5)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-12T22:01:44.430" Id="79507" LastActivityDate="2013-12-12T22:01:44.430" OwnerUserId="36115" ParentId="79243" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;take your hand as the plane, and a finger as the normal direction, then it should be clear that the plane is defined by two quantities, the direction of the normal and distance - how far your finger is planted in the plane. that distance is what b is representing.&lt;/p&gt;&#10;&#10;&lt;p&gt;so yes &#10;b=-n.r_0&lt;/p&gt;&#10;&#10;&lt;p&gt;b is the distance of the hyperplane to the origin (assuming n is unit length). So all points on the plane have the same &quot;perpendicular distance&quot; to the origin, and that's why you have a constant b , on the one side, and a seemingly varying quantity w.x on the other side. &lt;/p&gt;&#10;&#10;&lt;p&gt;draw a plane, and a line joining the origin to any arbitrary point in the plane, together with the shortest line joining the origin to the plane (which is precisely the one that enters the plane at right angles). Then you can decompose all points on the plane as a vector from the origin to closest point on plane together with an orthogonal vector within the plane. So that distance from origin to closest point doesnt change and that is what is giving you b. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-12T23:08:56.927" Id="79515" LastActivityDate="2013-12-12T23:08:56.927" OwnerUserId="27556" ParentId="79500" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;@Nick Cox's comment &lt;a href=&quot;http://stats.stackexchange.com/questions/79510/why-use-a-null-hypothesis-not-predicted-by-theory#comment155924_79510&quot;&gt;above&lt;/a&gt; &quot;most null hypotheses aren't predicted by theory&quot; gets pretty close to the distinction that Meehl is trying to draw in your excerpt. &lt;/p&gt;&#10;&#10;&lt;p&gt;When used as a &lt;strong&gt;goodness of fit&lt;/strong&gt; test, the null hypothesis has to be specified by a theory. Specifically, it requires a  generative model that can calculate the expected frequency $E_i$ of each event $i$. Suppose you're wondering whether a coin is fair. The null hypothesis comes from a specific theory (&quot;the coin is fair iff $E_{\textrm{heads}}=E_{\textrm{tails}} = \frac{1}{2}$&quot;) and calculating the $\chi^2$ statistic provides a specific test of that theory. In fact, the $\chi^2$ test provides a way to test almost any model against your data (you need a to be able to calculate a CDF), but &lt;em&gt;you&lt;/em&gt; have to provide the model, or at the very least a procedure for getting the model (people often test against the best-fitting distribution from a specific family). This makes these results fairly &quot;strong&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, when used as a &lt;strong&gt;test of independence&lt;/strong&gt;, that's all you really get. You don't need to provide a reason why the two variables are independent or a specific hypothesis about how the two covary, which makes this result conceptually &quot;weaker&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Meehl's broader point is that the null hypothesis is almost always slightly wrong: there are always factors that are beyond your ability to control--or even identify. If you have enough statistical power, you may therefore reject null hypotheses for the most mundane reasons (People advocate for effect size measurements or confidence intervals instead of $t$-tests for similar reasons). To avoid this, he wants researchers to build theories/models that make specific, quantifiable predictions and then measure their success by evaluating the model's performance. This ensures that the underlying phenomenon is understood.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, why doesn't everyone do this? Some parts of physics actually do operate this way. For example, the Standard Model makes &lt;a href=&quot;http://profmattstrassler.com/articles-and-posts/the-higgs-particle/the-standard-model-higgs/seeking-and-studying-the-standard-model-higgs-particle/&quot; rel=&quot;nofollow&quot;&gt;specific predictions&lt;/a&gt; about what a Higgs Boson would look like. These closely match the experimental data, which is why Peter Higgs and François Englert are presumably enjoying their Nobel Prize. In other fields, however, we just don't know enough. We can often predict whether a manipulation will make it harder or easier for people to notice a light or tone, but we know so little about perception and the brain that it's very difficult to predict exactly how big that effect will be (and trust me, it's not for want of trying!).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;hr&gt;&#10;There have been several attempts to defend significance testing. Some are pragmatic arguments (&quot;We need to make an actual decision&quot; or, more snarkily, how else would you phrase $H_0$: &quot;I am pregnant&quot;?) or philosophical arguments (&quot;Any deviation from the theory, no matter how small, is actually interesting&quot;). No-effect null hypotheses also seem more defensible when the experimental conditions can be tightly controlled (e.g., in a within-subjects psychophysics experiment with a good design) than when dealing with messy observational data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tukey has argued that the null hypothesis testing is valuable not because we're particularly interested in whether a population mean is exactly zero, but because if we cannot rule out the fact that it is zero, we also cannot rule out small effects in either the positive OR negative direction, which leaves us ignorant as to the sign of the effect. He and a few others (Kaiser, Braver) propose treating a hypothesis test as a three-way decision: $u_1 &amp;gt; u_2$, $u_1 &amp;lt; u_2$, or &quot;we can't tell.&quot; Of course, a confidence interval could also lead you in the same direction, but on the other hand, reporting whether a CI includes zero is just a hypothesis test in disguise.&lt;/p&gt;&#10;&#10;&lt;p&gt;R. Chris Farley has taught a class on the &quot;statistical testing controversy.&quot; Though it's ten years old, the &lt;a href=&quot;http://www.uic.edu/classes/psych/psych548/fraley/&quot; rel=&quot;nofollow&quot;&gt;syllabus&lt;/a&gt; might still be of interest. Weeks 4, 5, and 13 &quot;defend&quot; significance testing from the critiques advanced elsewhere in the reading list. &lt;/p&gt;&#10;&#10;&lt;p&gt;Bill Thompson has compiled a list of &lt;a href=&quot;http://warnercnr.colostate.edu/~anderson/thompson2.html&quot; rel=&quot;nofollow&quot;&gt;19 articles supporting statistical hypothesis tests&lt;/a&gt;; however, it is a companion to his immense list of &lt;a href=&quot;http://warnercnr.colostate.edu/~anderson/thompson1.html&quot; rel=&quot;nofollow&quot;&gt;402 citations arguing against their indiscriminate use in observational studies&lt;/a&gt;. (Note the subtle shift in title though…)&lt;/p&gt;&#10;&#10;&lt;p&gt;All that said, I think both parameter estimation and hypothesis testing would still count as &quot;weak&quot; according to Meehl. He'd prefer that we test generative models, built prior to the experiments, than perform post-hoc analyses of the resulting data.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-13T05:08:18.820" Id="79540" LastActivityDate="2013-12-17T05:53:19.490" LastEditDate="2013-12-17T05:53:19.490" LastEditorUserId="7250" OwnerUserId="7250" ParentId="79510" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;When do Metropolis sampling or MCMC, we need a target distribution $P_{target}(\theta)$, and a proposal distribution $P_{proposal}(\theta)$, then a value $\theta_i$ is generated via $P_{proposal}(\theta)$, we need to compute the probability whether to accept or reject this $\theta_i$ via $P_{target}(\theta)$, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is, we should know $P_{target}(\theta)$ before we doing this Metropolis process, right? Then what is this target distribution, does it have to do with my prior belief of $\theta$? If I've already known it, why bother doing this Metropolis sampling, can't we just use grid approximation?&lt;/p&gt;&#10;&#10;&lt;p&gt;As I read in the book, it takes the product of the likelihood and prior of $\theta$ as the target distribution, why? Different prior belief will result into different target distribution, does it mean we don't have a fixed desired target distribution?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-13T05:47:34.413" FavoriteCount="2" Id="79547" LastActivityDate="2014-01-02T09:29:59.337" LastEditDate="2013-12-13T08:47:52.087" LastEditorUserId="30540" OwnerUserId="30540" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;mcmc&gt;&lt;metropolis-hastings&gt;" Title="Metropolis algorithm, what is the target distrbution and how to compose it?" ViewCount="137" />
  <row Body="&lt;p&gt;I would say that covariance gives you additional information, which is the scale of the input and output standard deviation since $\sigma_{xy}=\sigma_{x}\sigma_{y}\rho_{xy}$ where the terms denote the covariance, the standard deviations, and the correlation, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;On example, where the covariance is needed is modeling data with a multivariate Gaussian distribution $$p(\mathbf x) = \frac{1}{\sqrt{|2\pi\Sigma|}}\exp\left(-\frac{1}{2}(\mathbf x - \mu)^\top \Sigma^{-1}(\mathbf x - \mu)\right).$$&#10;Here, the data covariance is the maximum likelihood estimator of the covariance of the Gaussian. A direct consequence of that is the formula for the conditional mean $$\mathbf x_2 = \Sigma_{21}\Sigma^{-1}_{11}(\mathbf x_1 - \mu_1) + \mu_2$$ where $\mathbf x_1$ and $\mathbf x_2$ denote a partition of the vector $\mathbf x$. The formula basically means that you predict parts of $\mathbf x$ from another part. In case of Gaussian data, this is the optimal linear predictor for least squares error.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-13T06:44:16.720" Id="79550" LastActivityDate="2013-12-13T06:44:16.720" OwnerUserId="6000" ParentId="79549" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If the process parameters are changing the process is out of (statistical) control, as you say. It is not necessarily producing bad product&amp;mdash;yet. The idea is to preempt quality problems by investigating signs of process instability &amp;amp; fixing them as necessary, while sparing engineers' time by limiting the number of wild-goose chases they're sent on.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The control chart limits should be fitted on data from an in-control process, as assessed by formal tests or informal diagnostics.&lt;sup&gt;&amp;dagger;&lt;/sup&gt; So historical, yes, but aspirational, no; there's no sense in triggering an alarm every day to tell you that your process parameters aren't what they ought to be if you already know that.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;New data should be plotted in real time against the pre-specified control limits. You &lt;a href=&quot;http://www.isixsigma.com/tools-templates/control-charts/when-recalculate-control-limits/&quot; rel=&quot;nofollow&quot;&gt;shouldn't recalculate&lt;/a&gt; those limits for each new data point.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&amp;dagger; The informal diagnostics might include plotting a control chart of past data &amp;amp; judging it to look like that of an in-control process. Obviously this kind of retrospective use of control charts is a secondary one.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-13T13:08:39.820" Id="79571" LastActivityDate="2013-12-13T14:22:45.050" LastEditDate="2013-12-13T14:22:45.050" LastEditorUserId="17230" OwnerUserId="17230" ParentId="79568" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;There really isn't a question here, but the distinction you cite is not correct. Point estimates rely on &lt;em&gt;consistency&lt;/em&gt;, a &lt;em&gt;first-order property&lt;/em&gt;, such as the strong law of large numbers, which guarantees (almost surely) that the estimate approaches the true value as sample size approaches infinity. A consistent estimator will approach the true value almost surely. That is all you need for a point estimator.&lt;/p&gt;&#10;&#10;&lt;p&gt;Interval estimates rely on &lt;em&gt;second-order properties&lt;/em&gt; that quantify the spread of the estimator or its distribution. In practice, this means either relying on an exact sampling distribution (this is rare) or relying on asymptotic results such as different versions of the central limit theorem, or extreme value theorems (for extremal estimators).&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2013-12-13T15:28:08.170" Id="79585" LastActivityDate="2013-12-13T15:55:08.847" LastEditDate="2013-12-13T15:55:08.847" LastEditorUserId="17230" OwnerDisplayName="user31668" ParentId="79583" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/076191904X&quot; rel=&quot;nofollow&quot;&gt;Raudenbush &amp;amp; Bryk (2002)&lt;/a&gt; is a widely cited reference (20k hits on Google Scholar)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1848728468&quot; rel=&quot;nofollow&quot;&gt;Hox (2010)&lt;/a&gt; is also popular, and also accessible (not as technical as Raudenbush &amp;amp; Bryk [2002])&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2013-12-13T17:34:40.997" CreationDate="2013-12-13T17:28:47.927" Id="79596" LastActivityDate="2013-12-13T17:28:47.927" OwnerUserId="24808" ParentId="79592" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think the &lt;a href=&quot;http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot; rel=&quot;nofollow&quot;&gt;wilcoxon signed-rank test&lt;/a&gt; will work in this situation.  The null hypothesis is that the 2 sets of ranks are equal.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-13T18:00:24.210" Id="79600" LastActivityDate="2013-12-13T18:00:24.210" OwnerUserId="2817" ParentId="79597" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If $B(t)$, $t\ge 0$ is a Brownian motion, then $B(t)-B(s)$ has $N(0,t-s)$ distribution. From there, you just need to figure out how to compute the 4th moment of a Gaussian with given variance, which I trust you can do.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-13T18:28:48.463" Id="79603" LastActivityDate="2013-12-13T18:28:48.463" OwnerUserId="24949" ParentId="79602" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="79613" AnswerCount="1" Body="&lt;p&gt;I have a load versus capacity problem, and I'm trying to determine the likelihood of failure. Here's a simple example of what I mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have load and capacity discrete pdfs that were determined through a series of computer calculations that analyzed different input values (Monte Carlo simulations to address uncertainties). The figures below are a simplified version of the output:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/LoVIt.png&quot; alt=&quot;Load Distribution&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KRGZa.png&quot; alt=&quot;Capacity Distribution&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, there is some amount of overlap between the two. I am trying to determine the probability that the system will fail (i.e., the load with exceed the capacity).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this as simple as integrating the overlapping area of the empirical cdfs and multiplying, or am I looking for something more complicated, like a Bhattacharyya coefficient? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any advice! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-13T19:17:19.197" Id="79608" LastActivityDate="2013-12-13T20:11:27.503" OwnerUserId="36164" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;nonparametric&gt;&lt;monte-carlo&gt;" Title="Overlap Probability of Empirical Distributions" ViewCount="98" />
  <row Body="&lt;p&gt;There is an excellent package called &lt;a href=&quot;http://cran.r-project.org/web/packages/car/index.html&quot; rel=&quot;nofollow&quot;&gt;car&lt;/a&gt; based on the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/141297514X&quot; rel=&quot;nofollow&quot;&gt;An R Companion to Applied Regression&lt;/a&gt;. Check out this &lt;a href=&quot;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/scripts.html&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt; for R scripts. This book is a good companion for Kutner et. al.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-01-15T15:28:23.077" CreationDate="2013-12-13T20:11:02.990" Id="79612" LastActivityDate="2013-12-13T20:11:02.990" OwnerUserId="29137" ParentId="64406" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;if you mean for $X = (X_1, \ldots, X_n)$, then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_X(x_1,\ldots,x_n) = \prod\limits_{j=1}^n \lambda e^{-\lambda x_j} = \lambda^n e^{- \lambda \sum_jx_j}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$l_X(\lambda) = \log f_X(x_1,\ldots,x_n) = \log(\lambda^n e^{- \lambda \sum_jx_j}) = nlog(\lambda) - \lambda (\sum_j x_j)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\mathcal{d}}{\mathcal{d\lambda}} l_X(\lambda) = \frac{n}{\lambda} - \sum_{i=1}^n  x_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\mathcal{d}^2}{\mathcal{d}\lambda^2} l_X(\lambda) = \frac{-n}{\lambda^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$- E \left[ \frac{\mathcal{d}^2}{\mathcal{d}\lambda^2} l_X(\lambda) \right] = \frac{n}{\lambda^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Generally if $x_i$ are i.i.d. you can take the $I(\theta)$ for a single observation $x_i$ and obtain the Fisher information for $X$ with $nI(\theta)$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-13T20:38:48.857" Id="79617" LastActivityDate="2014-02-03T12:36:48.753" LastEditDate="2014-02-03T12:36:48.753" LastEditorUserId="-1" OwnerUserId="34768" ParentId="79516" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I following &lt;a href=&quot;http://www.kdnuggets.com/data_mining_course/assignments/final-project.html&quot; rel=&quot;nofollow&quot;&gt;this &lt;/a&gt; data mining project. It explains how to do it but I don't understand the T-value part.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&quot;For each class, generate subsets with top 2,4,6,8,10,12,15,20,25, and 30 top genes with the highest T-value&quot;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So I calculate the T-value like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(Avg1 - Avg2) / \sqrt(Stdev1^2/N1+ Stdev2^2/N2)$$&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$Avg1$ is the average for one class across the gene sample and $Avg2$ is&#10;the average for the other 4 classes.&lt;/li&gt;&#10;&lt;li&gt;$Stdev1$ is the standard deviation for one class and $Stdev2$ is the&#10;standard deviation for the other classes.&lt;/li&gt;&#10;&lt;li&gt;$N1$ is the number of samples that have the class whose $T-value$ we are&#10;interested in, and $N2$ is the number of samples that does not have the&#10;$T-value$ that we are interested in.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;How do I use that T-value to create those subsets?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vhwEX.jpg&quot; alt=&quot;The Data&quot;&gt;&#10;The bold values in the data represent Average and Stdev per column.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-13T23:40:02.150" Id="79630" LastActivityDate="2014-05-22T07:41:35.080" LastEditDate="2013-12-14T13:43:19.610" LastEditorUserId="10849" OwnerUserId="32157" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;&lt;data-cleaning&gt;" Title="How to generate subsets with the highest T-value" ViewCount="65" />
  <row AnswerCount="1" Body="&lt;p&gt;If $X \sim Laplace(\mu, b)$ and $\mu = 0$ such that $X \sim Laplace(0,b)$, it becomes a form of the exponential family. Unfortunately, one source tells me that it becomes $|X| \sim exponential(b)$, another source says that $X \sim exponential(\frac{1}{b})$, and my notes say that $X \sim exponential(b)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can any of y'all help me out on this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-14T01:06:18.613" Id="79633" LastActivityDate="2013-12-30T01:44:32.943" OwnerUserId="36181" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;exponential&gt;" Title="Confusion on the relationship between laplace distribution and exponential distribution" ViewCount="100" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have the following observations&lt;/p&gt;&#10;&#10;&lt;p&gt;Oberservation ; Count &lt;/p&gt;&#10;&#10;&lt;p&gt;-1.67 ; 726 &lt;/p&gt;&#10;&#10;&lt;p&gt;18.33 ; 33&lt;/p&gt;&#10;&#10;&lt;p&gt;148.33 ; 15&lt;/p&gt;&#10;&#10;&lt;p&gt;This is obviusly not normal distributed :S&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I make a test for $H_0: \mu = 0$ or even better is it possible to make a confidence interval for the mean?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-14T12:38:40.163" Id="79653" LastActivityDate="2013-12-14T14:58:59.440" OwnerUserId="36138" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;" Title="Testing on non-normal distributed discrete values" ViewCount="66" />
  
  
  
  
  
  <row Body="&lt;p&gt;Here is a hint to help you get started:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5mSnv.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;x + w + y = 80% (Equation 1).&lt;br&gt;&#10;x + w = 60% (Equation 2).&lt;br&gt;&#10;w + y = 70% (Equation 3).  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you understand and solve the above equations, you will find the answers.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-14T23:53:15.243" Id="79694" LastActivityDate="2013-12-14T23:53:15.243" OwnerUserId="22468" ParentId="79693" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You can just estimate the probability density function of price. If they are a mixture of normal distributions, hopefully you will observe several peaks in your mixture of Gaussian kernels. It can be implemented easily with Python, and I believe there are packages for other languages as well.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from scipy.stats import kde&#10;import matplotlib.pyplot as plt     &#10;density = kde.gaussian_kde(x) # x: list of price&#10;xgrid = np.linspace(x.min(), x.max(), Num_Price)   &#10;plt.plot(xgrid, density(xgrid))&#10;plt.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-15T01:53:24.800" Id="79704" LastActivityDate="2013-12-15T01:53:24.800" OwnerUserId="35099" ParentId="79314" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="79717" AnswerCount="4" Body="&lt;p&gt;In mixed model, we assume the random effects (parameters) are random variables that follow normal distributions. It looks very similar to the Bayesian method, in which all the parameters are assumed to be random.&lt;/p&gt;&#10;&#10;&lt;p&gt;So is the random effect model kind of special case of Bayesian method?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-15T05:12:15.697" FavoriteCount="4" Id="79712" LastActivityDate="2013-12-15T14:31:21.187" OwnerUserId="31418" PostTypeId="1" Score="8" Tags="&lt;bayesian&gt;&lt;mixed-model&gt;" Title="Mixed model idea and Bayesian method" ViewCount="136" />
  
  <row Body="&lt;p&gt;Random effects are a way to specify a distributionial assumption by using conditional distributions. For example, the random one-way ANOVA model is:&#10;$$(y_{ij} \mid \mu_i) \sim_{\text{iid}} {\cal N}(\mu_i, \sigma^2_w), \quad j=1,\ldots,J, &#10;\qquad &#10;\mu_i \sim_{\text{iid}} {\cal N}(\mu, \sigma^2_b), \quad i=1,\ldots,I.$$ &#10;And this distributional assumption is equivalent to &#10;$$\begin{pmatrix} y_{i1} \\ \vdots \\ y_{iJ} \end{pmatrix} \sim_{\text{iid}} {\cal N}\left(\begin{pmatrix} \mu \\ \vdots \\ \mu \end{pmatrix}, \Sigma\right), \quad i=1,\ldots,I$$&#10;where $\Sigma$ has an exchangeable structure (with diagonal entry $\sigma^2_b+\sigma^2_w$ and covariance $\sigma^2_b$).&#10;To Bayesianify the model, you need to assign prior distributions on $\mu$ and $\Sigma$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-15T14:31:21.187" Id="79733" LastActivityDate="2013-12-15T14:31:21.187" OwnerUserId="8402" ParentId="79712" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="79759" AnswerCount="2" Body="&lt;p&gt;I'm trying to understand Radial Basis Function Network. &#10;I have (don' know how to write proper formatter mathematical functions here..):&lt;/p&gt;&#10;&#10;&lt;p&gt;$x = [ -1.0000, -0.5000, 0,0.5000,1.0000]$&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i = f(x_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(x) = \ \frac{1}{(1+x^2)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = [0.5000, 0.8000,1.0000, 0.8000,0.5000]$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xy = [ -1.0000    0.5000&#10;       -0.5000    0.8000&#10;             0    1.0000&#10;        0.5000    0.8000&#10;        1.0000    0.5000]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then there is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Phi_j(x) = exp(-(|x - t_j|/4))$&lt;/p&gt;&#10;&#10;&lt;p&gt;$t_j = -1 + (j - 1)\ \times\ \frac{2}{m_1 - 1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$m_1 &amp;lt;= N$&lt;/p&gt;&#10;&#10;&lt;p&gt;And then G matrix is written like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$G = (Phi_j(x_i,t_j))$&lt;/p&gt;&#10;&#10;&lt;p&gt;This matrix is used to calculate weights:&lt;/p&gt;&#10;&#10;&lt;p&gt;$w = G^+y$&lt;/p&gt;&#10;&#10;&lt;p&gt;$G^+ = (G'G)^-1G'$&lt;/p&gt;&#10;&#10;&lt;p&gt;So it looks for me that G uses Phi function with two inputs, but Phi is written as one input function. Do I miss something here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-15T16:01:42.227" Id="79740" LastActivityDate="2013-12-16T00:13:56.673" LastEditDate="2013-12-16T00:13:56.673" LastEditorUserId="88" OwnerUserId="30876" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;matlab&gt;&lt;neural-networks&gt;&lt;distance-functions&gt;" Title="Radial basis function network - G function?" ViewCount="115" />
  <row AcceptedAnswerId="79770" AnswerCount="1" Body="&lt;p&gt;I am taking a little data analysis course using SAS software and I need help with pretty much the basics.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is &lt;a href=&quot;http://www.amstat.org/publications/jse/datasets/fishcatch.txt&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; full description in the American Statistical Association page. &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically I have the data obtained from caught fish (species, weight, length etc)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Obs    Spec  Wt        Lt1     Lt2     Lt3     HtP    WhP    Sex&#10;----+----1----+----2----+----3----+----4----+----5----+----6----+----7----+-&#10;    1      1     242.0     23.2    25.4    30.0    38.4   13.4   NA&#10;    2      1     290.0     24.0    26.3    31.2    40.0   13.8   NA&#10;    3      1     340.0     23.9    26.5    31.1    39.8   15.1   NA&#10;    4      1     363.0     26.3    29.0    33.5    38.0   13.3   NA&#10;    5      1     430.0     26.5    29.0    34.0    36.6   15.1   NA&#10;    ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to predict weight using other known characteristics. How can I prove of disprove that the same regression function would work for all fish species? Would checking hypothesis that the weight distributions (for different species) are statistically different be enough?&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I have a bunch of models apparently suggested by experts&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Weight=a+b*(Length3*Height*Width)+epsilon&#10;Log(Weight)=a+b1*Length3+epsilon&#10;Weight^(1/3)=a+b1*Length3+epsilon&#10;Log(Weight)=a+b1*Length3+b2*Height+b3*Width+epsilon&#10;Weight^(1/3)=a+b1*Length3+b2*Height+b3*Width+epsilon&#10;Weight=a*Length3^b1*Height^b2*Width^b3+epsilon&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How should I compare them? Simply look for the best root mean square error?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-15T18:44:19.033" Id="79745" LastActivityDate="2013-12-16T02:41:31.350" LastEditDate="2013-12-15T19:20:04.210" LastEditorUserId="919" OwnerUserId="27798" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;anova&gt;&lt;model-selection&gt;&lt;sas&gt;" Title="How to compare performance of multiple regression models?" ViewCount="184" />
  
  <row AcceptedAnswerId="79772" AnswerCount="1" Body="&lt;p&gt;Consider the following model for $Y_t$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Delta$log($Y_{T+1})$ = $u_T$ where $u_T$ ~ IID Normal(0,$\sigma^2$).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to forecast $Y_{T+1}$. Taking exponentials and then expectations, we see that &lt;/p&gt;&#10;&#10;&lt;p&gt;$E(Y_{T+1}|\Omega_T)$ = $Y_T E(e^{u_{T+1}}|\Omega_T)$ = $Y_TE(e^{u_{T+1}})$ since $u_{T+1}$ is IID. Also, since $u_t$ ~ Normal(0,$\sigma^2$), we know (from the moment generating function) that &#10;$E(e^{u_t})$ = $e^{(1/2)\sigma^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Question: Why is there a tendancy for the one step ahead forecast to be above its previous value, and not below. In mathematical terms, why is $E(Y_{T+1}|\Omega_T) \geq Y_T$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-16T01:08:19.370" Id="79764" LastActivityDate="2014-09-12T05:54:06.847" LastEditDate="2014-09-12T05:54:06.847" LastEditorUserId="30192" OwnerUserId="30192" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;econometrics&gt;" Title="Why is the expection of $E(Y_{T+1}|\Omega_T)$ greater than or equal to its previous value?" ViewCount="71" />
  
  
  
  <row AcceptedAnswerId="79782" AnswerCount="1" Body="&lt;p&gt;Below is the graph of two variables, X and Y, each representing count data. N=348. Note the scales of the axes:&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/ih1yU.jpg&quot; alt=&quot;http://i.imgur.com/tNGyTX5.jpg&quot;&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Y is very approximately lognormal, but X has no decent fit (including Poisson, negative binomial, lognormal and gamma of the log transform).&lt;br&gt;&#10;Spearman coefficient between X and Y is close to 0, and p-value to reject no correlation is very high.  &lt;/p&gt;&#10;&#10;&lt;p&gt;From the plot, there appears to be no combinations of extreme values of both x and y.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I log transform both X and Y, the following plot results:&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/DVVHU.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;Clearly any appearance of pattern has disappeared.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Why is there a lack of combinations of &quot;extreme&quot; values on the linear scale, but not on the log scale?&lt;/li&gt;&#10;&lt;li&gt;Is there any significance to the lack of combination of extreme values on the linear scale, and is there anyway to investigate further?  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The purpose of this study is exploratory.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-16T03:58:04.187" FavoriteCount="5" Id="79777" LastActivityDate="2013-12-16T08:44:29.210" OwnerUserId="21972" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;correlation&gt;&lt;lognormal&gt;&lt;eda&gt;" Title="Variables lack correlation, but have pattern" ViewCount="309" />
  
  
  
  <row Body="&lt;p&gt;@ mpiktas   Just to briefly mention two  small oversights in version 3 of your answer. Firstly, the phrase &quot;speedier version&quot; has clearly been left in by error. Secondly, the word &quot;:=&quot; has been missed out in the code. Fixing the latter fixes the former :=) &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(data.table);ddt &amp;lt;- data.table(dt)&#10;f0&amp;lt;-function() plyr::ddply(dt,~location,transform,lvar=lg(var))&#10;f1&amp;lt;-function() ddt[,transform(.SD,lvar=lg(var)),by=c(&quot;location&quot;)]&#10;f2&amp;lt;-function() ddt[,lvar:=lg(var),by=location]&#10;r0&amp;lt;-f0();r1&amp;lt;-f1();r2&amp;lt;-f2();all.equal(r0,r1,r2,check.attributes = FALSE)&#10;boxplot(microbenchmark::microbenchmark(f0(),f1(),f2(),times=1000L))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AuM3S.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-16T10:58:05.700" Id="79792" LastActivityDate="2013-12-16T10:58:05.700" OwnerUserId="273" ParentId="25889" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering if there is a better way of formulating this and also if this is a correct way of formulating it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'm trying to represent in the frequency distribution is the count of documents that contain a certain count of the term I'm interested in. For example, the document sample contains 50 documents with 1 occurrence of the term, 30 documents with 2 occurrences, 10 with 3, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking to answer three questions: 1. is &lt;em&gt;frequency distribution of a term frequency&lt;/em&gt; a correct way to formulate what I described, 2. if yes, is it clear what I mean, and 3. is there a better way to formulate it?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-16T11:39:57.720" Id="79795" LastActivityDate="2013-12-16T13:26:38.187" LastEditDate="2013-12-16T13:26:38.187" LastEditorUserId="35002" OwnerUserId="35002" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;terminology&gt;&lt;frequency&gt;" Title="Is there a better term for frequency distribution of a term frequency?" ViewCount="36" />
  
  <row Body="&lt;p&gt;According to &lt;a href=&quot;http://stackoverflow.com/questions/19009530/how-to-use-scipy-stats-kstest-basic-questions-about-kolmogorovsmirnov-test&quot;&gt;this SO question&lt;/a&gt; and &lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.7.x/reference/generated/scipy.stats.kstest.html&quot; rel=&quot;nofollow&quot;&gt;the docs&lt;/a&gt;, it seems that the Python KS test default reference distribution is a normal distribution with $\mu = 0$ and $\sigma = 1$ ($N(0,1)$).  See the SO question for more instructions on how to change the reference distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;In answer to your more specific question, you were using it correctly.  In your first analysis, you had $p \approx 0.7$ when comparing 2 distributions which were $N(0,1)$.  You then added 1 to all the terms in one distribution, so that the means were different, and $p = 0$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-16T15:16:23.170" Id="79818" LastActivityDate="2013-12-16T15:16:23.170" OwnerUserId="10530" ParentId="79811" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;There is no &quot;included classification&quot; problem, you either have the features to properly predict your target value or not. To continue with your simplified example, if you had only &quot;p1&quot; the best you can do is use a model that can return a range of the possible values (since there are 2 distributions there, and you don't have enough information in your features). &lt;/p&gt;&#10;&#10;&lt;p&gt;If you have p2, just do normal regression. There is no &quot;trick&quot; in particular. &lt;/p&gt;&#10;&#10;&lt;p&gt;For your example, the p1 case is one where the distributions overlap and you lack the information needed to distinguish between them. It is not possible to create that information from nothing. You need to get more features for your data, thats simply the end of it. You can't extract something out of nothing. &lt;/p&gt;&#10;&#10;&lt;p&gt;The alternative case is where you have 2 distributions that do not overlap (i.e.: your features can disambiguate between the underlying distributions). In such a case you simply need to use a model / create a model that can handle them together. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, there can be a mix - where there is some moderate amount of overlap between two underlying distributions. It happens. Again, pick the model that gets the most of it right / is most appropriate for your data. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-17T04:57:38.887" Id="79886" LastActivityDate="2013-12-17T04:57:38.887" OwnerUserId="34874" ParentId="79796" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;See later in the help where it says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Note that you can set n larger than length(p) which means the&#10;  unobserved p-values are assumed to be greater than all the observed p&#10;  for &quot;bonferroni&quot; and &quot;holm&quot; methods and equal to 1 for the other&#10;  methods.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I can't think, offhand, of a case where you would want to do this, but perhaps if you have a great many p-values, and you are only interested in corrected values for some of them, this saves some typing.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-17T11:06:43.213" Id="79906" LastActivityDate="2013-12-17T11:06:43.213" OwnerUserId="686" ParentId="79899" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;This is clearly better framed as a large supervised learning problem with a great deal of hands-on assumptions. Is it possible to generate a dictionary of relevant tags, such as with a database of medical manuscripts and their corresponding MeSH terms? By calculating the frequency of such terms within each document, one can arrange those results in a large matrix. After constructing the frequency matrix, one can calculate its spectral decomposition and identify the salient terms in each document by those that have the highest orthonormal weight on the first principle component. If the matrix is rank deficient, sparse matrix methods would be a better option, such as is the case with SparCl.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-17T19:48:49.010" Id="79953" LastActivityDate="2013-12-17T19:48:49.010" OwnerUserId="8013" ParentId="79951" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;MLE requires knowledge of at least the marginal distributions. When using MLE, we usually estimate the parameters of a joint distribution by making an iid assumption, then factoring the joint distribution as a product of the marginals, which we know. There are variations, but this is the idea in most cases. So MLE is a parametric method.&lt;/p&gt;&#10;&#10;&lt;p&gt;The EM algorithm is a method for maximizing the likelihood functions that come up as part of a MLE algorithm. It is often (usually?) used for numerical solutions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whenever we use MLE, we need at least the marginal distributions, and some assumption about how the joint is related to the marginals (independence, etc.). Therefore both methods rely on knowledge of distributions.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-17T21:48:06.337" Id="79962" LastActivityDate="2013-12-17T21:48:06.337" OwnerUserId="35985" ParentId="79932" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I can't add comments; your question is incomplete as provides little information of your implementation details.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good implementation vectorizes forward and backward propagation in the first step; then all matrix vector multiplications can be compressed into matrix matrix multiplication by training with mini batches. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are done with those steps and your gradients passed with less than 10^-4 tolerance you should check out ?gemm_ [matrix * matrix] calls in fast matrix algebra package which are implemented parallel and optimized to the last bit. MKL is one of those packages. Try not to implement ?gemm_ sub routines on your own, harder then it seems. Of course having large theta weights you might want to move to CUDA. &lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on the initial criteria there may still be more room for optimization; providing the data set and weights are large enough you can go parallel; or even explore distributed weight updates.&lt;/p&gt;&#10;&#10;&lt;p&gt;good luck&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-17T22:07:10.743" Id="79964" LastActivityDate="2013-12-17T22:07:10.743" OwnerUserId="36350" ParentId="71083" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Stats question I need help with... Any help would be great thanks&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Scores for a civil service exam are normally distributed, with a standard deviation of 6.9. To be eligible for civil service employment, you must score in the top 5%. If the lowest score you can earn and still be eligible for employment is an 84, what is the mean score for the exam? Round your answer to the nearest tenth.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="4" CreationDate="2013-12-18T00:24:36.767" Id="79971" LastActivityDate="2013-12-18T00:41:42.250" LastEditDate="2013-12-18T00:41:42.250" LastEditorUserId="32257" OwnerUserId="32257" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;" Title="Normal distribution calculation problem" ViewCount="59" />
  <row AcceptedAnswerId="79976" AnswerCount="1" Body="&lt;p&gt;This is a homework question:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that $X_0=1$ and that for $n\geq 1$ $$X_n\sim  \left\{ &#10;  \begin{array}{l l}&#10;    U(0,X_{n-1}) &amp;amp; \quad \text{with probability $1-X_{n-1}/2$}\\&#10;    U(X_{n-1},1) &amp;amp; \quad \text{with probability $X_{n-1}/2$}&#10;  \end{array} \right.$$ Determine the limiting behavior of $X_n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My attempt:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Check if this is a (sub/super)martingale&lt;/p&gt;&#10;&#10;&lt;p&gt;2) By the martingale convergence theorems, since it is bounded (below by 0, above by 1) this random variable must converge to something. &lt;/p&gt;&#10;&#10;&lt;p&gt;3) It can't possible converge to some value $x\in (0,1)$ since if $X_n=x$, then theres a high probability that $X_{n+1}$ is more than $\epsilon$ units away from $x$ for any $\epsilon&amp;gt;0$. So it should converge to either 0 or 1 (or both). &lt;/p&gt;&#10;&#10;&lt;p&gt;4) My guess is $X_n \overset{a.s.}{\longrightarrow} X\sim \text{bernoulli}(p=1/2)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;A quick simulation however suggests that it converges almost surely to just 0! &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;quiz5_number2 &amp;lt;- function(reps=1000, path=NULL){&#10;  x0&amp;lt;-1&#10;  path&amp;lt;-c(path,x0)&#10;  for(i in 1:reps){&#10;    y&amp;lt;-rbinom(1, size=1, prob= 1-x0/2) &#10;    x1&amp;lt;- runif(1, min=0,max=x0)*y + runif(1, min=x0,max=1)*(1-y)&#10;    x0&amp;lt;-x1&#10;    path&amp;lt;-c(path,x0)&#10;  }&#10;  path&#10;}&#10;path&amp;lt;-quiz5_number2(100)&#10;plot(path)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/th4TL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How would you answer this problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;I realize the problem seems poorly worded. Blame my teacher :) &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: On further analysis it is quite simple to see that while 0 is an 'absorbing' state (if you get to zero you will stay there), 1 is not an absorbing state (if you are at 1 you have 0.5 chance to be drawn from uniform(0,1) and 0.5 chance to stay). So I guess it makes a lot of sense that it converges to 0. Thank you for reading. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-18T00:25:04.573" FavoriteCount="1" Id="79972" LastActivityDate="2013-12-18T02:35:20.043" LastEditDate="2013-12-18T01:23:41.773" LastEditorUserId="17661" OwnerUserId="17661" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;asymptotics&gt;&lt;martingale&gt;" Title="Limiting behavior of a martingale" ViewCount="66" />
  <row Body="&lt;p&gt;The trick to word problems like this in general (not just in stats) is extricating what's important from what's not important. Once you've done that, the trick to solving math and stats problems is to start with what you already know. This is true even in cutting-edge mathematical research. It sounds obvious but it's strangely an acquired skill.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here's what we know:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Attempted activation distance&quot; is normally distributed random variable, with $\mu$=6 and $\sigma$=2.75. Let's call it $X$. You're looking for the probability that X is greater than 10. In math terms, this means you're looking for $\mathrm{Pr}(X &amp;gt; 10)$. This should be a sign that you're looking to plug $X$ into its cumulative distribution function (CDF).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the CDF is defined as $\mathrm{F}(X)=\mathrm{Pr}(X \leq 10)$. Fortunately, there are only two possible places where X can be: less than or equal to 10, or greater than 10. It has to be one or the other and it can't be both. Now think $X\leq 10$ as one &quot;event&quot; and $X&amp;gt;10$ as a second &quot;event.&quot;  The probability that one event occurs or another even occurs is equal to the sum of their probabilities. $\mathrm{Pr}(X \leq 10 \mathrm{\ or\ } X &amp;gt; 10)=\mathrm{Pr}(X \leq 10)+\mathrm{Pr}(X &amp;gt; 10)$. And we know one of them &lt;em&gt;has&lt;/em&gt; to occur, so $\mathrm{Pr}(X \leq 10 \mathrm{\ or\ } X &amp;gt; 10)=1$. Move around the equation and we get $\mathrm{Pr}(X &amp;gt; 10)=1-\mathrm{Pr}(X \leq 10)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We actually know enough about $X$ to tell a computer how to calculate $1-\mathrm{Pr}(X \leq 10)$. But you don't learn anything that way so I'll just put the code at the end. You can't calculate $\mathrm{Pr}(X \leq 10)$ by hand, but most textbooks have a table at the end that looks something &lt;a href=&quot;http://image.mathcaptain.com/cms/images/95/normal-table-large.png&quot; rel=&quot;nofollow&quot;&gt;like this&lt;/a&gt;. Find your $X$ using the edges (ones and 10ths on the left, 100th on the top), and that will point you to the probability $\mathrm{Pr}(X \leq 10)$. &lt;strong&gt;&lt;em&gt;HOWEVER&lt;/em&gt;&lt;/strong&gt;, those tables are constructed to assume $\mu =0$ and $\sigma =1$, which is not the case here.&lt;/p&gt;&#10;&#10;&lt;p&gt;A random variable with $\mu =0$ and $\sigma =1$ is said to be &quot;standardized&quot; and is usually denoted with $Z$. There is a well-known formula for &quot;standardizing&quot; an arbitrary random variable: $\frac{Y-\mu}{\sigma}=Z$, where $Z$ is what we'll call the standardized random variable and $Y$ is &lt;em&gt;any&lt;/em&gt; random variable. In most cases, you can freely mess with an equation or inequality (such as $X\leq 10$) by applying the same formula to both sides. So let's plug in both sides of $X\leq 10$ to get $\frac{X-\mu}{\sigma}\leq \frac{10-\mu}{\sigma}$. Note that this is the same thing as saying $Z\leq \frac{10-\mu}{\sigma}$. In this case, we know $\mu$ and $\sigma$, so go ahead and plug them in to get $Z\leq \mathrm{number}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now because $Z$ is standardized, we can just use the table to find $\mathrm{Pr}(Z\leq \mathrm{number})$. Finally, compute $1-\mathrm{Pr}(Z\leq \mathrm{number})$. That's your answer. Note that it's pretty close to zero. This is because $\mathrm{number}$ is quite far from the mean. If people on average usually stand 6 feet from the TV with the remote, and a standard deviation is 2.75, very few people will try to turn on the TV from more than 10 feet.&lt;/p&gt;&#10;&#10;&lt;p&gt;When you're done, go to WolframAlpha and type in &lt;code&gt;1-CDF[NormalDistribution[6,2.75],10]&lt;/code&gt; to see if you get a similar answer. Computers can compute &lt;em&gt;any&lt;/em&gt; normal distribution -- that's where your Z table comes from in the first place. But you don't learn anything about statistics from typing stuff into a computer.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-18T02:16:42.240" Id="79977" LastActivityDate="2014-07-10T06:16:24.157" LastEditDate="2014-07-10T06:16:24.157" LastEditorUserId="48148" OwnerUserId="36229" ParentId="79970" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;(This could have been a comment but gets a bit too long).&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;final precision score&quot; seems vaguely defined here.  If you just had three independent experiments trying to measure the same underlying parameter (let's call it &quot;truthness&quot;), you could just pool your results and work out the overall score of $\frac{\sum{TP}}{\sum{TP + FP}}$.  But in fact it looks like you want to compare three independent experiments that are measuring &lt;em&gt;different&lt;/em&gt; underlying parameters - the truthness as measured by rule 1, by rule 2 and rule 3.  There's no statistical way of judging between the three rules - perhaps rule 1 is the best measure and it's a really bad machine learning method, perhaps rule 3 and 2 are and it's good.  You need some extra (non-statistical) information to judge which of the rules is more useful for you.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-18T07:23:52.357" Id="79989" LastActivityDate="2013-12-18T07:23:52.357" OwnerUserId="7972" ParentId="79986" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The most important thing to understand about a X% confidence interval is that the following statement is false:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There is a X% chance that the CI contains the &quot;true&quot; value.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Instead, it means that if you duplicated your experiment an infinite number of times, 95% of the time your estimated 95% CI will contain the true value. If you can get your head around that, you'll be ahead of many people. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can certainly learn a lot more by searching this site for &quot;confidence interval&quot;, including critiques of their use. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/13655/what-does-a-confidence-interval-vs-a-credible-interval-actually-express&quot;&gt;What does a confidence interval (vs. a credible interval) actually express?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-18T10:31:09.980" Id="80000" LastActivityDate="2013-12-18T11:20:21.433" LastEditDate="2013-12-18T11:20:21.433" LastEditorUserId="17230" OwnerUserId="16049" ParentId="79998" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="80323" AnswerCount="2" Body="&lt;p&gt;First a theoretical question. I know that natively, an hierarchical clustering algorithm is of complexity on the cube of number of samples N. This is due to the fact that in each iteration, one has to go over the entire distance matrix to find the smallest value. &lt;/p&gt;&#10;&#10;&lt;p&gt;But, it is possible to implement it in lower complexity (of N sqr). How it is done and can you reffer me to an article or description of the implementation? &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I used scikit library to do HClustering. It was fine but it is limited to 15,000 samples or so. I want to cluster much more. can someone refer me to an implementation that fits larger numbers (and preferably runs at sqr N)?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-12-18T11:57:33.990" Id="80007" LastActivityDate="2013-12-21T21:47:11.503" LastEditDate="2013-12-18T13:03:54.163" LastEditorUserId="3277" OwnerUserId="34962" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;hierarchical&gt;" Title="How to implement hierarchical clustering in $O(N^2)$ instead on $O(N^3)$" ViewCount="123" />
  <row Body="&lt;p&gt;Not entirely sure but it seems that &lt;code&gt;penalized&lt;/code&gt; can only handle ordered factors:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(penalized)&#10;data(nki70) # Some example data from penalized package (also see help-file)&#10;&#10;# Change class of variables to numeric to create similar data&#10;nki70$x1 &amp;lt;- as.numeric(nki70$ER) # dichotomous, 1 or 2&#10;nki70$x2 &amp;lt;- as.numeric(nki70$Age) # continuous&#10;nki70$x3 &amp;lt;- as.numeric(nki70$Grade) # categorical, 1,2 or 3&#10;&#10;# Fit L2 model with as.factor, indeed also gives coefficients for ref. level&#10;pen2 &amp;lt;- penalized(Surv(time, event)~x1+x2+as.factor(x3), data = nki70, lambda2 = 10)&#10;coefficients(pen2, &quot;all&quot;)&#10;&#10;# Gives:&#10;#         x1             x2 as.factor(x3)1 as.factor(x3)2 as.factor(x3)3 &#10;#-0.22878408    -0.06065468     0.28389881     0.05984886    -0.34374767 &#10;&#10;# Fit L2 model with as.ordered, which does not give coefficients for ref. level&#10;pen3 &amp;lt;- penalized(Surv(time, event)~x1+x2+as.ordered(x3), data = nki70, lambda2 = 10)&#10;coefficients(pen3, &quot;all&quot;)&#10;&#10;# Gives:&#10;#         x1                x2 as.ordered(x3)&amp;gt;=2 as.ordered(x3)&amp;gt;=3 &#10;#-0.22560366       -0.06036498       -0.26055093       -0.36240053 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This seems to work, although I am not sure what exactly is going on here. I you want to know the details you might be better off asking this at stackoverflow or contact the package maintainer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-18T12:20:26.430" Id="80010" LastActivityDate="2013-12-18T12:20:26.430" OwnerUserId="27812" ParentId="79133" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Slide 5 of &lt;a href=&quot;http://www.utexas.edu/courses/schwab/sw388r7/SolvingProblems/AssumptionOfLinearity.ppt&quot; rel=&quot;nofollow&quot;&gt;Data Analysis &amp;amp; Computers II&lt;/a&gt; attempts to answer your question in three different ways: &lt;/p&gt;&#10;&#10;&lt;p&gt;There are both graphical and statistical methods for evaluating linearity. Graphical methods include the examination of scatter plots, often overlaid with a trend line. While commonly recommended, this strategy is difficult to implement. Statistical methods include &#10;i) diagnostic hypothesis tests for linearity, ii) a rule of thumb that says a relationship is linear if the difference between the linear correlation coefficient (r) and the nonlinear correlation coefficient (eta) is small, and iii) examining patterns of correlation coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;see also these links:&#10;&lt;a href=&quot;http://stackoverflow.com/questions/3002638/eta-eta-squared-routines-in-r&quot;&gt;http://stackoverflow.com/questions/3002638/eta-eta-squared-routines-in-r&lt;/a&gt;&#10;&lt;a href=&quot;http://stackoverflow.com/questions/3013772/non-graphical-linearity-estimation&quot;&gt;http://stackoverflow.com/questions/3013772/non-graphical-linearity-estimation&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-18T13:32:31.757" Id="80017" LastActivityDate="2013-12-18T13:40:37.063" LastEditDate="2013-12-18T13:40:37.063" LastEditorUserId="273" OwnerUserId="273" ParentId="79995" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_{i} = \alpha + \beta{d_{i}} + X^{'}_{i}\gamma + \epsilon_{i} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $d_{i}$ is a dummy variable and $X^{'}_{i}$ is a vector of control variables. I want to estimate the the causal effect of $d_{i}$. Suppose that the regression is saturated in $X_{i}$ so that $E(d_{i}|X_{i})$ is linear. But suppose also that the model above is not actually the conditional expectation function (which is non-linear).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to find how $\beta$ is related to $E(y_{1,i}-y_{0,i})$ (the causal effect on $y_{i}$ of switching the dummy $d_{i}$ on and off). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now since the regression is saturated in $X_{i}$ I can write $\beta$ as $\frac{Cov(y_{i},\hat{d_{i}})}{Var(\hat{d_{i}})}$ where $d_{i}$ is the residual from regressing $d_{i}$ on the covariates $X^{'}_{i}$. And I know $\hat{d_{i}} = d_{i} - E(d_{i}|X_{i})$ so therefore:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \beta = \frac{E(y_{i}(d_{i}-E(d_{i}|X_{i})))}{E((d_{i}-E(d_{i}|X_{i}))^{2})} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;So here's my question: in the notes I am working from, the next step is written as:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \beta = \frac{E(E(y_{i}|d_{i},X_{i})(d_{i}-E(d_{i}|X_{i})))}{E((d_{i}-E(d_{i}|X_{i}))^{2})} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am confused because if this step has been reached using the Law of Iterated Expectations, then why hasn't the expectation sign also been applied to the second term inside the bracket (I am assuming they can be separated due to Conditional Independence)? And wouldn't that term equal zero?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-18T14:58:17.273" Id="80023" LastActivityDate="2014-05-24T09:04:52.453" LastEditDate="2013-12-19T17:22:04.407" LastEditorUserId="36315" OwnerUserId="36315" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="How do I derive this coefficient formula in a regression with non-linear conditional expectation function" ViewCount="132" />
  <row Body="&lt;p&gt;usually it is feasible to iterate over predicted probabilities with various cut-off points  from 0 to 1 with an increment of, say 0.01, and to construct some metric that is of interest to you (i.e. which you want to maximize). Be it accuracy, sensitivity, specificity, K-S score or value of other variable that may be not part of your model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then plot the cutoff VS that variable and you will have an idea which cut-off works best for you. And once the cutoff is determined just perform the cross validation with that value.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-18T15:33:15.330" Id="80027" LastActivityDate="2013-12-18T15:33:15.330" OwnerUserId="21674" ParentId="80014" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm reading the article &lt;a href=&quot;http://dx.doi.org/10.1109/TIT.2005.862083&quot; rel=&quot;nofollow&quot;&gt;Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information&lt;/a&gt; (Candes, Romberg and Tao, 2004). &lt;/p&gt;&#10;&#10;&lt;p&gt;In this article they are talking about recovering the function $f$ whose fourier coefficients are known on some domain $\Omega$, by solving the following optimization problems:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \min ||g||_{TV}  \space\space\space \text{s.t.} \space\space\space\hat{g}(w)=\hat{f}(w),w \in \Omega$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \min ||g||_{L_1}  \space\space\space \text{s.t.} \space\space\space\hat{g}(w)=\hat{f}(w),w \in \Omega$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone please give me a reference that suggests how to actually solve these optimization problems (that combines both $g$ and $\hat{g}$)? &lt;/p&gt;&#10;&#10;&lt;p&gt;A relevant R package would also be nice.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-18T16:20:55.947" Id="80035" LastActivityDate="2013-12-18T17:40:25.393" LastEditDate="2013-12-18T17:40:25.393" LastEditorUserId="2970" OwnerUserId="36377" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;references&gt;&lt;optimization&gt;" Title="Compressed sensing: Optimization in $L_1$ norm and total variation with fourier coefficients" ViewCount="101" />
  
  <row AcceptedAnswerId="80052" AnswerCount="1" Body="&lt;p&gt;I'm somewhat new to understanding statistics meaningfully, and the bootstrap in particular.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say you're doing something (polling, running experiment, whatever), and can afford to get 1000 samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;From what I understand, before the bootstrap method existed (&gt; 25-30 years ago), you'd be better off with, say, 10 sets of 100 samples each and calculating your statistics based on sampling distributions.  (As an aside, is there a good way to calculate the ratio of sets/samples?  I'm know that you get a smaller standard error when n is larger, but don't know if there is a &quot;sweet spot&quot;, and from my rather limited understanding, it's preferable to have n &gt; 30 to avoid messing around with t statistics).&lt;/p&gt;&#10;&#10;&lt;p&gt;But now that bootstrapping exists, my understanding is that you get one set of all 1000 samples (since this will be a better proxy for real poplulation than 100 would be), and do the bootstrapping resampling magic based on that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct?  Or is best practice some hybrid of the two?  Or does it ultimately depend on what the actual activity is?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-18T16:38:19.243" FavoriteCount="2" Id="80039" LastActivityDate="2013-12-18T18:33:34.183" LastEditDate="2013-12-18T17:44:08.393" LastEditorUserId="35531" OwnerUserId="35531" PostTypeId="1" Score="2" Tags="&lt;sampling&gt;&lt;bootstrap&gt;&lt;sample-size&gt;" Title="Multiple samples vs. bootstrapping" ViewCount="96" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have produced a model using the &lt;code&gt;ctree&lt;/code&gt; function in R, and want to know whether this tree is actually explaining my data well. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to explain the presence or absence of landscape disturbances using environmental factors. &lt;code&gt;rpart&lt;/code&gt; gives an $R^2$ value, but since my data are not well-balanced (PRESENCE contains a relatively small number of 1s), I believe the &lt;code&gt;ctree&lt;/code&gt; model will work better. A (simplified) version of my model is: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit &amp;lt;- ctree (PRESENCE ~ A + B + C + D + E + F, data=dat);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where PRESENCE is a factor containing values of (0,1), &#10;five of the independent variables (denoted by the letters A-F) are &#10;numerical values (environmental data),&#10;and the remaining variable is categorical.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe there is a way to make an $R^2$ value using the &lt;code&gt;predict&lt;/code&gt; function (as in &lt;a href=&quot;http://stats.stackexchange.com/questions/23772/getting-r-square-value-from-ctree&quot;&gt;Getting R square value from ctree&lt;/a&gt;), but I'm fairly new to R and haven't had any success trying to code this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be much appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-18T22:12:50.647" Id="80068" LastActivityDate="2013-12-18T22:25:12.353" LastEditDate="2013-12-18T22:25:12.353" LastEditorUserId="805" OwnerUserId="36395" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;cart&gt;&lt;r-squared&gt;" Title="Classification Tree Analysis - Assessment of tree explanatory power (R-square?) using the party package in R" ViewCount="164" />
  
  <row Body="&lt;p&gt;In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;estimate=function(y,z,u=1e-9){&#10;  ys=sort(unique(y))&#10;  # Inf signifies x's never observed (as they are higher than max y)&#10;  zs=c(sort(unique(z))[-1],Inf)&#10;  counts=xtabs(~z+y)&#10;  observed=rbind(counts[-1,],rep(0,length(ys)))&#10;  marginalHidden=counts[1,]&#10;  m=sapply(seq(ys),function(i)zs&amp;gt;ys[i])&#10;  d=rep(1/length(zs),length(zs))&#10;  while(T){&#10;    # allocate hidden data according to current parameters&#10;    p=apply(m*d,2,function(v)v/sum(v))&#10;    # can result in fractional counts&#10;    hidden=sweep(p,2,marginalHidden,'*')&#10;    total=observed+hidden&#10;    d2=apply(total,1,sum)/sum(total)&#10;    msd=mean((d2-d)^2)&#10;    if(msd&amp;lt;u^2)&#10;      break;&#10;    d=d2&#10;  }&#10;  d&#10;}&#10;&#10;xSupport=c(3,5,7)&#10;xDistribution=c(1/4,1/2,1/4)&#10;x=sample(xSupport,1000,replace=T,prob=xDistribution)&#10;ySupport=c(4,6)&#10;yDistribution=c(1/2,1/2)&#10;y=sample(ySupport,length(x),replace=T,prob=yDistribution)&#10;z=ifelse(x&amp;lt;y,x,0)&#10;&#10;estimate(y,z)&#10;table(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2013-12-18T23:02:11.093" Id="80073" LastActivityDate="2013-12-19T00:02:12.300" LastEditDate="2013-12-19T00:02:12.300" LastEditorUserId="20598" OwnerUserId="20598" ParentId="79118" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I don´t quite understand why you worked with an integral when you´re dealing with a discrete variable. Anyways, this link should help: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://math.stackexchange.com/questions/328230/poisson-process-conditional-probability-question&quot;&gt;http://math.stackexchange.com/questions/328230/poisson-process-conditional-probability-question&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-19T02:04:14.957" Id="80087" LastActivityDate="2013-12-19T02:04:14.957" OwnerUserId="36403" ParentId="80086" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;All centrality measures are dependent on the shape of your data. &lt;a href=&quot;http://www.math.wvu.edu/~cqzhang/Publication-files/my-paper/INS-2012-Laplacian-W.pdf&quot; rel=&quot;nofollow&quot;&gt;Laplacian centrality&lt;/a&gt; is a convincing measure of centrality for weighted graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define a matrix to store our weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;$ W_{ij} = \left\{&#10;     \begin{array}{lr}&#10;       w_{ij} &amp;amp; : i \neq j\\&#10;       0 &amp;amp; : i = j&#10;     \end{array}&#10;   \right. $&lt;/p&gt;&#10;&#10;&lt;p&gt;Define a matrix, where the diagonal is the sum of the weights associated with a node.&lt;/p&gt;&#10;&#10;&lt;p&gt;$ X_{ij} = \left\{&#10;     \begin{array}{lr}&#10;       0 &amp;amp; : i \neq j\\&#10;       \sum\limits_{i=0}^n W_{i}&amp;amp; : i = j&#10;     \end{array}&#10;   \right. $&lt;/p&gt;&#10;&#10;&lt;p&gt;The Laplacian is then defined by&lt;/p&gt;&#10;&#10;&lt;p&gt;$L = X - W$&lt;/p&gt;&#10;&#10;&lt;p&gt;We can define a property of the graph, Laplacian energy.&lt;/p&gt;&#10;&#10;&lt;p&gt;$E = \sum\limits_{i=0}^n\lambda_i^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\lambda$s are the eigenvalues associated with the Laplacian.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rather than eigensolving our matrices, we can equivalently solve.&lt;/p&gt;&#10;&#10;&lt;p&gt;$E = \sum\limits_{i=0}^n x_i^2 + 2\sum\limits_{i&amp;lt;j}w_{ij}^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;To define the importance of a particular node in a graph, we remove that node and calculate the energy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following data, generated from an RBF kernel of 1000 multivariate normal observations centered at the origin with a standard deviation of unity. The indices are the same for both figures. The data was presorted according to the distance of each observation $\in \mathbb R^n$ from the origin.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Cv8A1.jpg&quot; alt=&quot;Sample data.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The importance of the Laplacian is beyond the scope of this answer. Laplacians are central to many piercing theorems in spectral graph theory and many practical results in the literature of manifold learning and clustering. I'd highly recommend reading up on the subject if you think you'll be dealing with weighted graphs in the near future.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-19T02:48:17.210" Id="80090" LastActivityDate="2013-12-19T03:24:07.107" LastEditDate="2013-12-19T03:24:07.107" LastEditorUserId="9568" OwnerUserId="9568" ParentId="78593" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I'm not knowledgeable on the method you are proposing but another possible (and probably simpler) solution is to use Generalized Least Squares.  Generalized Least Squares can be used when the residuals exhibit non-constant variance and/or are correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R one can use the gls function and specify a particular correlation structure among the residuals.  More info here: &lt;a href=&quot;http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch5.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch5.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-19T03:19:50.443" Id="80092" LastActivityDate="2013-12-19T03:19:50.443" OwnerUserId="2310" ParentId="80091" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I currently look for a set of ngrams in many sets of documents to establish a relevancy score for each set - &lt;em&gt;eg. I look for the n-gram &quot;adhesive tape&quot; in ~1M sets of 1-500 documents.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The values I have at my disposal are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the total documents in each set&lt;/li&gt;&#10;&lt;li&gt;the total documents each ngram was found in within each set&lt;/li&gt;&#10;&lt;li&gt;the total number of times each ngram was found in each document in each set&lt;/li&gt;&#10;&lt;li&gt;the total number of times each ngram was found in each set&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I've tried using raw occurrences of each ngram, and number of documents each ngram occurs in, and various combo calculations.  None of these are really establishing very good relevancy.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problems I'm facing ( among others I'm probably not thinking of ) are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;different ngrams occur more often than other ngrams - &lt;em&gt;eg. &quot;adhesive&quot; will simply be used more in english text than &quot;adhesive tape&quot;.&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;the number of documents in each set will vary&lt;/li&gt;&#10;&lt;li&gt;the amount of text in each document will vary&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What are some statistical methods I could try for this type of relevancy extraction from free text?  I have a limited background in this type of semantic analysis.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-19T03:39:33.683" Id="80093" LastActivityDate="2013-12-19T03:39:33.683" OwnerUserId="36405" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;text-mining&gt;&lt;natural-language&gt;&lt;nlp&gt;" Title="Calculating and Normalizing ngram relevancy scores from free text extraction" ViewCount="75" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I frequently read that Bonferroni correction also works for dependent hypotheses. However, I don't think that is true and I have a counter example. Can somebody please tell me (a) where my mistake is or (b) whether I am correct on this.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Setting up the counter example&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume we are testing two hypoetheses. Let $H_{1}=0$ is the first&#10;hypothesis is false and $H_{1}=1$ otherwise. Define $H_{2}$ similarly.&#10;Let $p_{1},p_{2}$ be the p-values associated with the two hypotheses&#10;and let $[\![\cdot]\!]$ denote the indicator function for the set&#10;specified inside the brackets. &lt;/p&gt;&#10;&#10;&lt;p&gt;For fixed $\theta\in [0,1]$ define&#10;\begin{eqnarray*}&#10;P\left(p_{1},p_{2}|H_{1}=0,H_{2}=0\right) &amp;amp; = &amp;amp; \frac{1}{2\theta}[\![0\le p_{1}\le\theta]\!]+\frac{1}{2\theta}[\![0\le p_{2}\le\theta]\!]\\&#10;P\left(p_{1},p_{2}|H_{1}=0,H_{2}=1\right) &amp;amp; = &amp;amp; P\left(p_{1},p_{2}|H_{1}=1,H_{2}=0\right)\\&#10; &amp;amp; = &amp;amp; \frac{1}{\left(1-\theta\right)^{2}}[\![\theta\le p_{1}\le1]\!]\cdot[\![\theta\le p_{2}\le1]\!]&#10;\end{eqnarray*}&#10;which are obviously probability densities over $[0,1]^{2}$. Here is a plot of the two densities&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IW7EO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Marginalization yields&#10;\begin{eqnarray*}&#10;P\left(p_{1}|H_{1}=0,H_{2}=0\right) &amp;amp; = &amp;amp; \frac{1}{2\theta}[\![0\le p_{1}\le\theta]\!]+\frac{1}{2}\\&#10;P\left(p_{1}|H_{1}=0,H_{2}=1\right) &amp;amp; = &amp;amp; \frac{1}{\left(1-\theta\right)}[\![\theta\le p_{1}\le1]\!]&#10;\end{eqnarray*}&#10;and similarly for $p_{2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, let&#10;\begin{eqnarray*}&#10;P\left(H_{2}=0|H_{1}=0\right) &amp;amp; = &amp;amp; P\left(H_{1}=0|H_{2}=0\right)=\frac{2\theta}{1+\theta}\\&#10;P\left(H_{2}=1|H_{1}=0\right) &amp;amp; = &amp;amp; P\left(H_{1}=1|H_{2}=0\right)=\frac{1-\theta}{1+\theta}.&#10;\end{eqnarray*}&#10;This implies that&#10;\begin{eqnarray*}&#10;P\left(p_{1}|H_{1}=0\right) &amp;amp; = &amp;amp; \sum_{h_{2}\in\{0,1\}}P\left(p_{1}|H_{1}=0,h_{2}\right)P\left(h_{2}|H_{1}=0\right)\\&#10; &amp;amp; = &amp;amp; \frac{1}{2\theta}[\![0\le p_{1}\le\theta]\!]\frac{2\theta}{1+\theta}+\frac{1}{2}\frac{2\theta}{1+\theta}+\frac{1}{\left(1-\theta\right)}[\![\theta\le p_{1}\le1]\!]\frac{1-\theta}{1+\theta}\\&#10; &amp;amp; = &amp;amp; \frac{1}{1+\theta}[\![0\le p_{1}\le\theta]\!]+\frac{\theta}{1+\theta}+\frac{1}{1+\theta}[\![\theta\le p_{1}\le1]\!]\\&#10; &amp;amp; = &amp;amp; U\left[0,1\right]&#10;\end{eqnarray*}&#10;is uniform as required for p-values under the Null hypothesis.&#10;The same holds true for $p_{2}$ because of symmetry.&lt;/p&gt;&#10;&#10;&lt;p&gt;To get the joint distribution $P\left(H_{1},H_{2}\right)$ we compute&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{eqnarray*}&#10;P\left(H_{2}=0|H_{1}=0\right)P\left(H_{1}=0\right) &amp;amp; = &amp;amp; P\left(H_{1}=0|H_{2}=0\right)P\left(H_{2}=0\right)\\&#10;\Leftrightarrow\frac{2\theta}{1+\theta}P\left(H_{1}=0\right) &amp;amp; = &amp;amp; \frac{2\theta}{1+\theta}P\left(H_{2}=0\right)\\&#10;\Leftrightarrow P\left(H_{1}=0\right) &amp;amp; = &amp;amp; P\left(H_{2}=0\right):=q&#10;\end{eqnarray*}&#10;Therefore, the joint distribution is given by &#10;\begin{eqnarray*}&#10;P\left(H_{1},H_{2}\right) &amp;amp; = &amp;amp; \begin{array}{ccc}&#10; &amp;amp; H_{2}=0 &amp;amp; H_{2}=1\\&#10;H_{1}=0 &amp;amp; \frac{2\theta}{1+\theta}q &amp;amp; \frac{1-\theta}{1+\theta}q\\&#10;H_{1}=1 &amp;amp; \frac{1-\theta}{1+\theta}q &amp;amp; \frac{1+\theta-2q}{1+\theta}&#10;\end{array}&#10;\end{eqnarray*}&#10;which means that $0\le q\le\frac{1+\theta}{2}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why it is a counter example&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let $\theta=\frac{\alpha}{2}$ for the significance level $\alpha$&#10;of interest. The probability to get at least one false positive with the corrected significance level $\frac{\alpha}{2}$ given that both&#10;hypotheses are false (i.e. $H_{i}=0$) is given by&#10;\begin{eqnarray*}&#10;P\left(\left(p_{1}\le\frac{\alpha}{2}\right)\vee\left(p_{2}\le\frac{\alpha}{2}\right)|H_{1}=0,H_{2}=0\right) &amp;amp; = &amp;amp; 1&#10;\end{eqnarray*}&#10;because all values of $p_{1}$ and $p_{2}$ are lower than $\frac{\alpha}{2}$&#10;given that $H_1=0$ and $H_2=0$ by construction. The Bonferroni correction, however, would claim that&#10;the FWER is less than $\alpha$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-19T10:43:02.063" FavoriteCount="2" Id="80112" LastActivityDate="2013-12-19T10:48:04.750" LastEditDate="2013-12-19T10:48:04.750" LastEditorUserId="6000" OwnerUserId="6000" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;mathematical-statistics&gt;&lt;multiple-comparisons&gt;&lt;p-value&gt;&lt;bonferroni&gt;" Title="Is Bonferroni correction too liberal for some dependent hypotheses?" ViewCount="107" />
  
  
  <row Body="&lt;p&gt;If data are MCAR, one would like to find an unbiased estimated of alpha. This could possibly be done via multiple imputation or listwise deletion. However, the latter might lead to severe loss of data. A third way is something like pairwise deletion which is implemented via an &lt;code&gt;na.rm&lt;/code&gt; option in &lt;code&gt;alpha()&lt;/code&gt; of the &lt;code&gt;ltm&lt;/code&gt; package and in &lt;code&gt;cronbach.alpha()&lt;/code&gt; of the &lt;code&gt;psych&lt;/code&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;At least IMHO, the former estimate of &lt;strong&gt;unstandardized&lt;/strong&gt; alpha with missing data is biased (see below). This is due to the calculation of the total variance $\sigma^2_x$ via &lt;code&gt;var(rowSums(dat, na.rm = TRUE))&lt;/code&gt;. If the data are centered around 0, positive and negative values cancel each other out in the calculation of &lt;code&gt;rowSums&lt;/code&gt;. With missing data, this leads to a bias of &lt;code&gt;rowSums&lt;/code&gt; towards 0 and therefore to an underestimation of $\sigma^2_x$ (and alpha, in turn). Contrarily, if the data are mostly positive (or negative), missings will lead to a bias of &lt;code&gt;rowSums&lt;/code&gt; towards zero this time resulting in an overestimation of $\sigma^2_x$ (and alpha, in turn). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(&quot;MASS&quot;); require(&quot;ltm&quot;); require(&quot;psych&quot;)&#10;n &amp;lt;- 10000&#10;it &amp;lt;- 20&#10;V &amp;lt;- matrix(.4, ncol = it, nrow = it)&#10;diag(V) &amp;lt;- 1&#10;dat &amp;lt;- mvrnorm(n, rep(0, it), V)  # mean of 0!!!&#10;p &amp;lt;- c(0, .1, .2, .3)&#10;names(p) &amp;lt;- paste(&quot;% miss=&quot;, p, sep=&quot;&quot;)&#10;cols &amp;lt;- c(&quot;alpha.ltm&quot;, &quot;var.tot.ltm&quot;, &quot;alpha.psych&quot;, &quot;var.tot.psych&quot;)&#10;names(cols) &amp;lt;- cols&#10;res &amp;lt;- matrix(nrow = length(p), ncol = length(cols), dimnames = list(names(p), names(cols)))&#10;for(i in 1:length(p)){&#10;  m1 &amp;lt;- matrix(rbinom(n * it, 1, p[i]), nrow = n, ncol = it)&#10;  dat1 &amp;lt;- dat&#10;  dat1[m1 == 1] &amp;lt;- NA&#10;  res[i, 1] &amp;lt;- cronbach.alpha(dat1, standardized = FALSE, na.rm = TRUE)$alpha&#10;      res[i, 2] &amp;lt;- var(rowSums(dat1, na.rm = TRUE))&#10;      res[i, 3] &amp;lt;- alpha(as.data.frame(dat1), na.rm = TRUE)$total[[1]]&#10;  res[i, 4] &amp;lt;- sum(cov(dat1, use = &quot;pairwise&quot;))&#10;}&#10;round(res, 2)&#10;##            alpha.ltm var.tot.ltm alpha.psych var.tot.psych&#10;## % miss=0        0.93      168.35        0.93        168.35&#10;## % miss=0.1      0.90      138.21        0.93        168.32&#10;## % miss=0.2      0.86      110.34        0.93        167.88&#10;## % miss=0.3      0.81       86.26        0.93        167.41&#10;dat &amp;lt;- mvrnorm(n, rep(10, it), V)  # this time, mean of 10!!!&#10;for(i in 1:length(p)){&#10;  m1 &amp;lt;- matrix(rbinom(n * it, 1, p[i]), nrow = n, ncol = it)&#10;  dat1 &amp;lt;- dat&#10;  dat1[m1 == 1] &amp;lt;- NA&#10;  res[i, 1] &amp;lt;- cronbach.alpha(dat1, standardized = FALSE, na.rm = TRUE)$alpha&#10;      res[i, 2] &amp;lt;- var(rowSums(dat1, na.rm = TRUE))&#10;      res[i, 3] &amp;lt;- alpha(as.data.frame(dat1), na.rm = TRUE)$total[[1]]&#10;  res[i, 4] &amp;lt;- sum(cov(dat1, use = &quot;pairwise&quot;))&#10;}&#10;round(res, 2)&#10;##            alpha.ltm var.tot.ltm alpha.psych var.tot.psych&#10;## % miss=0        0.93      168.31        0.93        168.31&#10;## % miss=0.1      0.99      316.27        0.93        168.60&#10;## % miss=0.2      1.00      430.78        0.93        167.61&#10;## % miss=0.3      1.01      511.30        0.93        167.43&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-19T12:27:54.790" Id="80121" LastActivityDate="2014-09-25T11:20:02.817" LastEditDate="2014-09-25T11:20:02.817" LastEditorUserId="27276" OwnerUserId="27276" ParentId="77125" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider the 3-D real random vector $(X_1,X_2,X_3)$ which is uniformly distributed on the surface of a unit sphere. What can be told about the distribution of $(aX_1,bX_2,cX_3)$, where $a,b,c,$ are non-zero and non-identical real constants? Is it true to say that they are distributed uniformly on the surface of an ellipsoid with the corresponding parameters $a,b,c$? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-19T13:28:26.443" Id="80128" LastActivityDate="2013-12-19T13:28:26.443" OwnerUserId="36425" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;random-generation&gt;&lt;uniform&gt;" Title="Transforming a uniform-on-sphere random vector" ViewCount="38" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm very new to Bayesian analysis, and I've come up with the following model. My goal is to get for each individual &quot;test unit&quot; a distribution that describes the lift in success rate under one of several test conditions. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#data&#10;N &amp;lt;- 50000 #Number of test units&#10;T &amp;lt;- 2 #Number of test conditions&#10;succs&amp;lt;-structure(c(...N x T...))&#10;trials&amp;lt;-structure(c(...N x T...))&#10;&#10;#model&#10;model {&#10;for (j in 1:T) {&#10;    TestCondition[j] ~ dnorm(0, TestCondition.tau)&#10;}&#10;for (i in 1:N) {&#10;    Unit[i] ~ dnorm(0, Unit.tau)&#10;}&#10;for (i in 1:N) {&#10;    for(j in 1:T){&#10;        succs[i,j] ~ dbin(p[i,j],trials[i,j])&#10;        eps[i,j] ~ dnorm(0, eps.tau)&#10;        logit(p[i,j]) &amp;lt;- mu + Unit[i] + TestCondition[j] + eps[i,j]&#10;        for(k in 1:j-1){ #Only compute pairwise when non-redundant&#10;            delta[i,(j-1)*(j-2)/2+k] &amp;lt;- p[i,j]-p[i,k]&#10;        }&#10;    }&#10;}&#10;mu ~ dlogis(0, 1)&#10;TestCondition.tau ~ dgamma(2,2)&#10;Unit.tau ~ dgamma(2,2)&#10;eps.tau ~ dgamma(2,1)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;1) Is the above model appropriate for the problem I'm trying to solve?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The model takes a long time for large datasets (which is basically all of my datasets). Are there any ways to reformulate the model so it can handle large datasets better?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-19T15:10:40.307" Id="80141" LastActivityDate="2013-12-19T15:10:40.307" OwnerUserId="29597" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;bayesian&gt;&lt;mcmc&gt;&lt;jags&gt;&lt;bugs&gt;" Title="Is this JAGS model ok, and can it be made faster?" ViewCount="81" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://stats.stackexchange.com/questions/26247/estimating-a-survival-probability-in-r/26291#26291&quot;&gt;method&lt;/a&gt; you link to in your comment should work, if you choose to follow the neural-network survival analysis approach in the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2813661/&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; I linked to in my comment. For each patient in the model that approach uses a list of probabilities of being alive at each time of interest: 1/0 for patients known to have died, and for &quot;censored&quot; cases a 1 until last follow-up and thereafter the KM survival estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that, however, I urge you to consider looking at the other neural-network approaches noted in that article and any other more recent developments; I have a fair amount of experience with survival analysis, but not with neural-network approaches. Also, although neural-network approaches can give good predictive behavior, the hidden variables make it difficult to say what predictor variables really &quot;matter,&quot; something that clinicians typically care about. The &lt;code&gt;Survival&lt;/code&gt; &lt;a href=&quot;http://watson.nci.nih.gov/cran_mirror/web/views&quot; rel=&quot;nofollow&quot;&gt;Task View&lt;/a&gt; page available at CRAN mirrors shows other approaches for high-dimensional data like yours that might give results easier to interpret heuristically, and the &lt;code&gt;MachineLearning&lt;/code&gt; Task View page shows what's available for neural network and other machine-learning approaches in R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-19T16:13:09.577" Id="80148" LastActivityDate="2013-12-19T16:13:09.577" OwnerUserId="28500" ParentId="80049" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The split/join metric measures the number of 'moves' required to go from the first clustering to the second clustering, where each 'move' consists of splitting off a single element off of one cluster and then either attaching it to another cluster (which also counts as a move) or starting a new cluster. There is a further requirement, which is not very important for the intuition, that these moves are 'aligned' with the lattice of partitions. This means that the path sketched out by the 'moves' also contains the largest common subclustering of the two clusterings. In your case, a single node or element is, I assume, a single cell. The intuition thus is, that for the left instance, eight cells need to be rearranged in order to obtain one of the clusterings from the other.&#10;The question below and its answers may also be interesting:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/24961/comparing-clusterings-rand-index-vs-variation-of-information&quot;&gt;Comparing clusterings: Rand index vs variation of information&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-19T17:05:03.863" Id="80154" LastActivityDate="2013-12-20T11:45:48.690" LastEditDate="2013-12-20T11:45:48.690" LastEditorUserId="4495" OwnerUserId="4495" ParentId="80142" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I have carried out a linear regression. This is the form of the model:&lt;/p&gt;&#10;&#10;&lt;p&gt;bounded (0-1) response variable ~ factor1 (2 levels) + factor2 (5 levels) + interaction between factor1:factor2 + factor3 (2 levels)&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample size is about 350.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have plotted the residuals against the predicted values from that model:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UJ1Db.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As there is a distinct pattern in the residuals, it appears linear regression is not a suitable model. I have carried out several other linear regressions lately with different data, and I have repeatedly seen this pattern in the residuals. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this pattern provide information on what model would be best to use? Should an interaction term be added? Is there a predictor missing? Should a non-linear model be used? Or instead, does this particular pattern in the residuals actually not provide any indicators as to what model to use?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-12-19T20:50:45.757" Id="80173" LastActivityDate="2014-12-30T20:14:29.127" LastEditDate="2013-12-19T21:12:38.817" LastEditorUserId="12492" OwnerUserId="12492" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;prediction&gt;&lt;residuals&gt;" Title="Patterns in residuals plot from linear regression: do they tell us what model to use?" ViewCount="381" />
  <row Body="&lt;p&gt;With regard to &lt;em&gt;generating&lt;/em&gt; (e.g., your own) datasets for similar purposes, you might be interested in:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Chatterjee, S. &amp;amp; Firat, A. (2007). &lt;a href=&quot;http://www.jstor.org/discover/10.2307/27643902?uid=3739776&amp;amp;uid=2129&amp;amp;uid=2&amp;amp;uid=70&amp;amp;uid=4&amp;amp;uid=3739256&amp;amp;sid=21102682457467&quot; rel=&quot;nofollow&quot;&gt;Generating data with identical statistics but dissimilar graphics: A follow up to the Anscombe dataset&lt;/a&gt;. &lt;em&gt;The American Statistician, 61&lt;/em&gt;, 3, pp. 248–254.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As far as datasets that are simply &lt;em&gt;used&lt;/em&gt; to demonstrate tricky / counter-intuitive phenomena in statistics, there a lot, but you need to specify what phenomena you want to demonstrate.  For example, with respect to demonstrating &lt;a href=&quot;http://en.wikipedia.org/wiki/Simpson%27s_paradox&quot; rel=&quot;nofollow&quot;&gt;Simpson's paradox&lt;/a&gt;, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Simpson%27s_paradox#Berkeley_gender_bias_case&quot; rel=&quot;nofollow&quot;&gt;Berkeley gender bias case dataset&lt;/a&gt; is very famous.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For a great discussion of the most famous dataset of all, see: &lt;a href=&quot;http://stats.stackexchange.com/questions/74776/what-aspects-of-the-iris-data-set-make-it-so-successful-as-an-example-teaching&quot;&gt;What aspects of the &amp;quot;Iris&amp;quot; data set make it so successful as an example/teaching/test data set&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-12-20T13:50:39.327" CreationDate="2013-12-20T03:58:26.047" Id="80198" LastActivityDate="2013-12-23T02:28:31.473" LastEditDate="2013-12-23T02:28:31.473" LastEditorUserId="556" OwnerUserId="7290" ParentId="80196" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;Data sets that act as counterexamples to popular misunderstandings* do exist - I've constructed many myself under various circumstances, but most of them wouldn't be interesting to you, I'm sure. &lt;/p&gt;&#10;&#10;&lt;p&gt;*(which is what the Anscombe data does, since it's a response to people operating under the misunderstanding that the quality of a model can be discerned from the identical statistics you mentioned)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll include a few here that might be of greater interest than most of the ones I generate:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) One example (of quite a few) are some example discrete distributions (and thereby data sets) I constructed to counter the common assertion that zero third-moment skewness implies symmetry. (Kendall and Stuart's &lt;em&gt;Advanced Theory of Statistics&lt;/em&gt; offers a more impressive continuous family.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's one of those discrete distribution examples: &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array}{cccc}&#10;\\&#10;x&amp;amp;-4&amp;amp;1&amp;amp;5\\&#10;\hline&#10;P(X=x)&amp;amp;2/6&amp;amp;3/6&amp;amp;1/6&#10;\\&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;(A data set for a counterexample in the sample case is thereby obvious: $-4, -4, 1, 1, 1, 5$)&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, this distribution &lt;em&gt;isn't&lt;/em&gt; symmetric, yet its third moment skewness is zero. Similarly, one can readily construct counterexamples to a similar assertion with respect to the second most common skewness measure, the second &lt;a href=&quot;http://en.wikipedia.org/wiki/Skewness#Pearson.27s_skewness_coefficients&quot;&gt;Pearson skewness coefficient&lt;/a&gt; ($3(\frac{mean-median}{\sigma})$). &lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed I have also come up with distributions and/or data sets for which the two measures are opposite in sign - which suffices to counter the idea that skewness is a single, easily understood concept, rather than a somewhat slippery idea we don't really know how to suitably measure in many cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) There are another couple of collections of counterexample data sets I constructed in response to people's over-reliance on histograms, especially with only a few bins and only at one bin-width and bin-origin; which leads to mistakenly confident assertions about distributional shape. These data sets and example displays can be found &lt;a href=&quot;http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-a-histogram/51753#51753&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's one of the examples from there. This is the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  1.03, 1.24, 1.47, 1.52, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, &#10;  1.99, 2.72, 2.75, 2.78, 2.81, 2.84, 2.87, 2.90, 2.93, 2.96, 2.99, 3.60, &#10;  3.64, 3.66, 3.72, 3.77, 3.88, 3.91, 4.14, 4.54, 4.77, 4.81, 5.62&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here are two histograms:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dfxUJ.png&quot; alt=&quot;Skew vs bell&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That's the the 34 observations above in both cases, just with different breakpoints, one with binwidth $1$ and the other with binwidth $0.8$. The plots were generated in R as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(1.03, 1.24, 1.47, 1.52, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, &#10;  1.99, 2.72, 2.75, 2.78, 2.81, 2.84, 2.87, 2.9, 2.93, 2.96, 2.99, 3.6, &#10;  3.64, 3.66, 3.72, 3.77, 3.88, 3.91, 4.14, 4.54, 4.77, 4.81, 5.62)&#10;hist(x,breaks=seq(0.3,6.7,by=0.8),xlim=c(0,6.7),col=&quot;green3&quot;,freq=FALSE)&#10;hist(x,breaks=0:8,col=&quot;aquamarine&quot;,freq=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;3) I recently constructed some data sets to demonstrate the intransitivity of the Wilcoxon-Mann-Whitney test - that is, to show that one might reject a one tailed alternative for each of three or four pairs of data sets, A, B, and C, (and D in the four sample case) such that one concluded that $P(B&amp;gt;A)&amp;gt;\frac{1}{2}$ (i.e. conclude that B tends to be bigger than A), and similarly for C against B, and A against C (or D against C and A against D for the 4 sample case); each tends to be larger (in the sense that it has more than even chance of being larger) than the&#10;previous one in the cycle.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's one such data set, with 30 observations in each sample, labelled A to D:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       1     2     3     4     5     6     7     8     9    10    11    12&#10; A  1.58  2.10 16.64 17.34 18.74 19.90  1.53  2.78 16.48 17.53 18.57 19.05&#10; B  3.35  4.62  5.03 20.97 21.25 22.92  3.12  4.83  5.29 20.82 21.64 22.06&#10; C  6.63  7.92  8.15  9.97 23.34 24.70  6.40  7.54  8.24  9.37 23.33 24.26&#10; D 10.21 11.19 12.99 13.22 14.17 15.99 10.32 11.33 12.65 13.24 14.90 15.50&#10;&#10;      13    14    15    16    17    18    19    20    21    22    23    24&#10; A  1.64  2.01 16.79 17.10 18.14 19.70  1.25  2.73 16.19 17.76 18.82 19.08&#10; B  3.39  4.67  5.34 20.52 21.10 22.29  3.38  4.96  5.70 20.45 21.67 22.89&#10; C  6.18  7.74  8.63  9.62 23.07 24.80  6.54  7.37  8.37  9.09 23.22 24.16&#10; D 10.20 11.47 12.54 13.08 14.45 15.38 10.87 11.56 12.98 13.99 14.82 15.65&#10;&#10;      25    26    27    28    29    30&#10; A  1.42  2.56 16.73 17.01 18.86 19.98&#10; B  3.44  4.13  6.00 20.85 21.82 22.05&#10; C  6.57  7.58  8.81  9.08 23.43 24.45&#10; D 10.29 11.48 12.19 13.09 14.68 15.36&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's an example test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; wilcox.test(adf$A,adf$B,alt=&quot;less&quot;,conf.int=TRUE)&#10;&#10;    Wilcoxon rank sum test&#10;&#10;data:  adf$A and adf$B&#10;W = 300, p-value = 0.01317&#10;alternative hypothesis: true location shift is less than 0&#10;95 percent confidence interval:&#10;      -Inf -1.336372&#10;sample estimates:&#10;difference in location &#10;             -2.500199 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you see, the one-sided test rejects the null; values from A tend to be smaller than values from B. The same conclusion (at the same p-value) applies to B vs C, C vs D and D vs A. This cycle of rejections, of itself, is not automatically a problem, if we don't interpret it to mean something it doesn't. (It's a simple matter to obtain much smaller p-values with similar, but larger, samples.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The larger &quot;paradox&quot; here comes when you compute the (one-sided in this case) intervals for a location shift -- in every case 0 is excluded (the intervals aren't identical in each case). This leads us to the conclusion that as we move across the data columns from A to B to C to D, the location moves to the right, and yet the same happens again when we move back to A.&lt;/p&gt;&#10;&#10;&lt;p&gt;With a larger versions of these data sets (similar distribution of values, but more of them), we can get significance (one or two tailed) at substantially smaller significance levels, so that one might use Bonferroni adjustments for example, and still conclude each group came from a distribution which was shifted up from the next one.&lt;/p&gt;&#10;&#10;&lt;p&gt;This shows us, among other things, that a rejection in the Wilcoxon-Mann-Whitney doesn't of itself automatically justify a claim of a location shift.&lt;/p&gt;&#10;&#10;&lt;p&gt;(While it's not the case for these data, it's also possible to construct sets where the sample means are constant, while results like the above apply.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Added in later edit: A very informative and educational reference on this is &lt;/p&gt;&#10;&#10;&lt;p&gt;Brown BM, and Hettmansperger TP. (2002)&lt;br&gt;&#10;Kruskal-Wallis, multiple comaprisons and Efron dice.&lt;br&gt;&#10;&lt;em&gt;Aust&amp;amp;N.Z. J. Stat.&lt;/em&gt;, &lt;strong&gt;44&lt;/strong&gt;, 427–438.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Another couple of related counterexamples come up &lt;a href=&quot;http://stats.stackexchange.com/questions/83030/can-anova-be-significant-when-none-of-the-pairwise-t-tests-is/83083#83083&quot;&gt;here&lt;/a&gt; - where an ANOVA may be significant, but all pairwise comparisons aren't (interpreted two different ways there, yielding different counterexamples).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;So there's three counterexample data sets that contradict misunderstandings one might encounter.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you might guess, I construct such counterexamples reasonably often (as do many other people), usually as the need arises. For some of these common misunderstandings, you can characterize the counterexamples in such a way that new ones may be generated at will (though more often, a certain level of work is involved).&lt;/p&gt;&#10;&#10;&lt;p&gt;If there are particular kinds of things you might be interested in, I might be able to locate more such sets (mine or those of other people), or perhaps even construct some. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;One useful trick for generating random regression data that has coefficients that you want is as follows (the part in parentheses is an outline of R code):&lt;/p&gt;&#10;&#10;&lt;p&gt;a) set up the coefficients you want with no noise (&lt;code&gt;y = b0 + b1 * x1 + b2 * x2&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;b) generate error term with desired characteristics (&lt;code&gt;n = rnorm(length(y),s=0.4&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;c) set up a regression of noise on the same x's  (&lt;code&gt;nfit = lm(n~x1+x2)&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;d) add the residuals from that to the y variable  (&lt;code&gt;y = y + nfit$residuals&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Done. (the whole thing can actually be done in a couple of lines of R)&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2013-12-20T13:50:39.327" CreationDate="2013-12-20T04:55:25.813" Id="80200" LastActivityDate="2014-04-09T00:32:05.530" LastEditDate="2014-04-09T00:32:05.530" LastEditorUserId="805" OwnerUserId="805" ParentId="80196" PostTypeId="2" Score="13" />
  <row AnswerCount="0" Body="&lt;p&gt;I am studying the ARMA models, and I am breaking my head on something probably straightforward but that I can't see.&lt;/p&gt;&#10;&#10;&lt;p&gt;I generated in R a simple AR(1) model, and used the function ARIMA to estimate its coefficients. I get two coefficients, an AR(1) coefficient and an intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I am trying to calculate myself the fitted values. On another post on StackExchange I found that I can find the fitted value Y(i) by doing &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Y(i-1)*model$coef[1] + ((1-model$coef[1])*model$coef[2])&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody please explain why one has to multiply the intercept with 1 minus the AR(1) coefficient?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;Kasper&lt;/p&gt;&#10;&#10;&lt;p&gt;PS This is the other question I was reffering to: &lt;a href=&quot;http://stats.stackexchange.com/questions/32575/computing-the-fitted-value-for-the-first-observation-in-a-time-series&quot;&gt;Computing the fitted value for the first observation in a time series&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-20T11:07:25.773" Id="80211" LastActivityDate="2013-12-20T11:07:25.773" OwnerUserId="36462" PostTypeId="1" Score="2" Tags="&lt;arma&gt;" Title="How do I calculate the fitted values of an Arma model" ViewCount="125" />
  
  <row Body="&lt;p&gt;I would also add to the list using part-of-speech tagging and removing uninformative parts of speech.&lt;/p&gt;&#10;&#10;&lt;p&gt;nlp.stanford.edu/downloads/tagger.shtml&lt;/p&gt;&#10;&#10;&lt;p&gt;The above site links to an efficient tagger and provides documentation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-20T14:19:25.850" Id="80225" LastActivityDate="2013-12-20T14:19:25.850" OwnerUserId="36381" ParentId="74617" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I am writing my thesis on VaR and ES risk measurements and have encountered some issues with how to best test the accuracy of ES statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding of the topic is that backtesting ES adequately is extremely difficult or even impossible, as it is not an elicitable risk measure. And I also assume the number of approaches in the literature are scarcity because of exactly this property.&lt;/p&gt;&#10;&#10;&lt;p&gt;However very recently D.Tasche et.al. (&lt;a href=&quot;http://arxiv.org/pdf/1312.1645v2.pdf&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/pdf/1312.1645v2.pdf&lt;/a&gt;) uploaded a paper where it was argued that ES is not directly elicitable, but indirectly elicitable because it can be approximated by several VaR estimates. Thus, they state ES can be backtested reasonably by backtesting several VaRs (at different confidence levels) related to the ES estimate. Their specific proposition is:$E{S_\gamma }(L) = {1 \over {1 - \gamma }}\int_\gamma ^1 q u(L)du \approx {1 \over 4}\left( {{{\rm{q}}_\gamma }{\rm{(L)  +  }}{{\rm{q}}_{0.75\gamma  + 0.25}}(L) + {{\rm{q}}_{0.5\gamma  + 0.5}}(L) + {{\rm{q}}_{0.25\gamma  + 0.75}}(L)} \right)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where L is the loss distribution and gamma the chosen confidence level.&lt;/p&gt;&#10;&#10;&lt;p&gt;From my simple calculations, proxying financial data with a student-t(6) the accumulated VaRs with this approach seems to deviate much from the analytical ES.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if anyone find the Tasche et.al. approach appealing/adequate for backtesting ES? Further I am also interested in knowing what the &quot;state of the art&quot; approach is in the industry (if any particular) for backtesting ES.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be greatly appreciated&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-20T14:37:28.053" Id="80226" LastActivityDate="2013-12-20T14:37:28.053" OwnerUserId="30470" PostTypeId="1" Score="1" Tags="&lt;quantiles&gt;&lt;validation&gt;" Title="Evaluating Expected Shortfall" ViewCount="65" />
  <row Body="&lt;p&gt;I think this is an instrumentation case. You want a missing X, not a missing Y.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y~X&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But X is frequently missing or mismeasured.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X~Z and Z does not impact Y- except through X.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then you can run:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; X~Z&#10; Y~Predicted(X)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And require some adjustment for the standard errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;You also may want to look at the Heckmann 2 step procedure if you have a lot of sample attrition.&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Heckman_correction&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Heckman_correction&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-20T15:58:17.257" Id="80232" LastActivityDate="2013-12-20T16:04:13.340" LastEditDate="2013-12-20T16:04:13.340" LastEditorUserId="24028" OwnerUserId="24028" ParentId="80206" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="80250" AnswerCount="1" Body="&lt;p&gt;Let's say there are two products &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; both of which have some value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a population of &lt;code&gt;10.000&lt;/code&gt; people choosing between these two products. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to come up with a function that adjusts the price of &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; based on the frequency of people choosing the products.&lt;/p&gt;&#10;&#10;&lt;p&gt;The function should satisfy the following criteria:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;1.&lt;/code&gt; The value of A and B should always sum to 1 and be in the range between 0 and 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;2.&lt;/code&gt; The more people are choosing a product the lower should its value be and similarly the fewer people are choosing a product the higher should its value be.&lt;/p&gt;&#10;&#10;&lt;p&gt;To illustrate let's say that &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;on day 1:&lt;/code&gt; &lt;code&gt;6000&lt;/code&gt; people are choosing product &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;4000&lt;/code&gt; are choosing product &lt;code&gt;B&lt;/code&gt;    &lt;/p&gt;&#10;&#10;&lt;p&gt;and their value is &lt;code&gt;0.6&lt;/code&gt; and &lt;code&gt;0.4&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;on day 2:&lt;/code&gt; &lt;code&gt;7000&lt;/code&gt; people are choosing product &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;3000&lt;/code&gt; are choosing product &lt;code&gt;B&lt;/code&gt; the value of &lt;code&gt;A&lt;/code&gt; should decrease and the value of &lt;code&gt;B&lt;/code&gt; should increase.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;3.&lt;/code&gt; The adjustment of the value should not be linearly increasing or decreasing based on the frequency of people using the product but the increase or decrease should be steeper when the proportion of people using the products is close to &lt;code&gt;50%&lt;/code&gt; but should increase or decrease slower when the people choosing the products is skewed (e.g. &lt;code&gt;90&lt;/code&gt; vs. &lt;code&gt;10%&lt;/code&gt;).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-20T16:21:23.083" Id="80236" LastActivityDate="2013-12-20T18:13:29.267" OwnerUserId="16880" PostTypeId="1" Score="1" Tags="&lt;proportion&gt;" Title="Frequency based discounting of values" ViewCount="21" />
  <row AcceptedAnswerId="80276" AnswerCount="1" Body="&lt;p&gt;I'm currently studying brownian motion and came across two kinds of geometric brownian motion.&#10;&lt;a href=&quot;http://homepage.univie.ac.at/kujtim.avdiu/dateien/BrownianMotion.pdf&quot; rel=&quot;nofollow&quot;&gt;http://homepage.univie.ac.at/kujtim.avdiu/dateien/BrownianMotion.pdf&lt;/a&gt; (page 14)&#10;&lt;a href=&quot;http://www.math.unl.edu/~sdunbar1/MathematicalFinance/Lessons/StochasticCalculus/GeometricBrownianMotion/geometricbrownian.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.math.unl.edu/~sdunbar1/MathematicalFinance/Lessons/StochasticCalculus/GeometricBrownianMotion/geometricbrownian.pdf&lt;/a&gt; (page 2)&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't see the difference between the two formulas. I prefer the 2nd one since it seems simpler but both give different kind of expectation and variance and this is making it confusing for me to understand.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain the difference between the two?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-20T16:58:00.940" Id="80240" LastActivityDate="2013-12-20T22:41:19.637" OwnerUserId="31104" PostTypeId="1" Score="1" Tags="&lt;stochastic-processes&gt;" Title="Differences between different Geometric Brownian Motion" ViewCount="77" />
  
  <row AnswerCount="1" Body="&lt;p&gt;In Andy Field's &lt;a href=&quot;http://www.statisticshell.com/docs/repeatedmeasures.pdf&quot; rel=&quot;nofollow&quot;&gt;example&lt;/a&gt; for one-way ANOVA with repeated measures, he measures &quot;retching time&quot; for eight different celebrities eating four different gross foods.&lt;/p&gt;&#10;&#10;&lt;p&gt;He tests things like sphericity, runs one-way ANOVA and post hoc tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a similar set up but with multiple measures for each subject-factor combination. Imagine in Andy's example that for each celebrity-food pair I'd measured 50 different retching times.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've seen it suggested, on this site even, to just take the average across these 50 trials. Is this &quot;legal&quot;. Shouldn't I have to verify that the variance for each celebrity-food pair is small? If so how? Should these trials go into one large analysis or do I just need to preprocess my data?&lt;/p&gt;&#10;&#10;&lt;p&gt;As a last cry for help. How is the best accomplished with a program like SPSS?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-20T17:21:52.560" Id="80243" LastActivityDate="2014-02-23T19:39:59.127" OwnerUserId="36474" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;spss&gt;&lt;repeated-measures&gt;" Title="What is the correct ANOVA for repeated measures with multiple measurements for each within-subject factor?" ViewCount="137" />
  <row AcceptedAnswerId="81210" AnswerCount="2" Body="&lt;p&gt;I have many satellite raster images available from different sensors. From these, the coarser ones have a very abundant temporal resolution. The medium resolution rasters tend to have less acquisition dates but still some degree of information is available. The finer resolution ones have a very low temporal resolution, spanning from 2 to 6 observed dates in under two years. I was wondering if anyone knows of any efforts to study this type of multi-scale time series in any way? I would be interested in predicting future values at the finer scales using the information available from the coarser ones. It makes sense to me that the data must be related (yes the images cover the same regions) but I have no idea how to start coupling this information in a predictive model. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-20T17:59:45.153" FavoriteCount="2" Id="80247" LastActivityDate="2014-01-05T02:26:14.383" LastEditDate="2013-12-24T21:30:25.137" LastEditorUserId="11748" OwnerUserId="11748" PostTypeId="1" Score="8" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;predictive-models&gt;&lt;multivariate-regression&gt;" Title="Coupling time series information from sources with multiple spatial resolutions/scales" ViewCount="169" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In the following table is represented the relative frequency distribution of 100 families related to the class of annual income (in thousands of dollars):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Income:                         10-25   25-40   40-60   60-100  100-200&#10;Relative frequencies:           0,48    0,25    0,15    0,10    0,02&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Calculate arithmetic mean and Gini index.&lt;/p&gt;&#10;&#10;&lt;p&gt;I started calculating the middle value of the classes and the cumulative frequency:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Income (middle value):          17,5    32,5    50      80      150&#10;Cumulative frequency:           0,48    0,73    0,88    0,90    1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Problem is, how do I calculate the arithmetic mean? I though it was the summation of the middle values * their relative frequency, which outputs 35,025 (8,4 + 8,125 + 7,5 + 8 + 3). The problem is that the sum of the residuals from the estimated mean isn't zero!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;17,5-35,025 = -17,525   +&#10;32,5-35,025 = -2,525    +&#10;50-35,025   = 14,975    +&#10;80-35,025   = 44,975    +&#10;150-35,025  = 114,975   =&#10;&#10;154,875 ≠ 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is 35,025 still the correct value for arithmetic mean even if the sum of residuals isn't 0?&lt;/p&gt;&#10;&#10;&lt;p&gt;The other question: Gini index. It's a concentration estimator. It's calculated in this way:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9XDPe.png&quot; alt=&quot;http://upload.wikimedia.org/math/7/5/f/75f99e8670d20b9a7b95bf19dbd31802.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where Qi are cumulative frequency and Pi are cumulative frequency in case of equidistribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you help me? :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-20T18:52:44.510" Id="80260" LastActivityDate="2013-12-20T19:46:41.670" LastEditDate="2013-12-20T19:46:41.670" LastEditorUserId="36479" OwnerUserId="36479" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;variance&gt;&lt;mean&gt;&lt;arithmetic&gt;&lt;gini&gt;" Title="Arithmetic mean and Gini index in a grouped distribution" ViewCount="159" />
  
  <row Body="&lt;p&gt;Perhaps it's the use of $\mu$ in both formulas that is confusing you? The first reference gives the definition of geometric Brownian motion as&#10;$$\frac{dS_t}{S_t} = \mu dt + \sigma dW_t$$&#10;and the second as&#10;$$\frac{dS_t}{S_t} = r dt + \sigma dW_t$$&#10;so $\mu$ in the first reference is $r$ in the second. The second reference then uses $\mu$ as a notation for $r-\frac{1}{2}\sigma^2$. So in other words, the symbol $\mu$ in the second reference does not have the same meaning as the symbol $\mu$ in the first reference. But both are mathematically the same.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-20T22:41:19.637" Id="80276" LastActivityDate="2013-12-20T22:41:19.637" OwnerUserId="13818" ParentId="80240" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I came across this in the &lt;a href=&quot;http://en.wikipedia.org/wiki/Factor_analysis&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page&lt;/a&gt; about Factor Analysis. Is that true that &lt;em&gt;direct oblimin rotation results in greater eigen values&lt;/em&gt;? If that is true, what's the reason behind it and does it generalize to other &lt;strong&gt;oblique&lt;/strong&gt; rotations ? (to avoid any confusions consider we only consider PCA as the factor extraction methods)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-21T20:45:47.627" FavoriteCount="1" Id="80314" LastActivityDate="2013-12-22T13:44:52.917" LastEditDate="2013-12-22T13:44:52.917" LastEditorUserId="28637" OwnerUserId="28637" PostTypeId="1" Score="2" Tags="&lt;factor-analysis&gt;&lt;rotation&gt;" Title="Why direct oblimin rotation results in greater eigen values?" ViewCount="173" />
  <row Body="&lt;p&gt;I came across a very simple and interesting explanation for this eigenvalue&gt;1 criterion:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;... .Those with eigenvalues less than 1.00 are not considered to be stable. They account for less variability than does a single variable and are not retained in the analysis. In this sense, you end up with fewer factors than original number of variables.&#10;  (Girden, 2001)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I paraphrase like so: &lt;strong&gt;because you don't want to end up with more or equal number of factors as the number of your variables is. When a factor has an eigen value less than 1.00, in a sense it has less than one variable in it.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&#10;Girden, E. R. (2001). Evaluating research articles from start to finish. Thousand Oaks, Calif., Sage Publications.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-21T20:57:50.737" Id="80318" LastActivityDate="2013-12-21T20:57:50.737" OwnerUserId="28637" ParentId="72439" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;WLLN, yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a general claim: Suppose $\{ f_n \}$, $f$, and $g$ are random variables, and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\sqrt{n} (f_n - f) \stackrel{d}{\mapsto} g.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say the CDF of $g$ is continuous everywhere. Then $f_n \rightarrow f$ in probability. This is because $\sqrt{n} (f_n - f)$ is bounded in probability/uniformly tight.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-22T05:11:49.433" Id="80340" LastActivityDate="2013-12-22T05:11:49.433" OwnerUserId="26223" ParentId="78818" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;Peter's response highlights the main issues with missing values. If you are looking for a thorough non-technical discussion, could I point you towards the &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/arm/missing.pdf&quot; rel=&quot;nofollow&quot;&gt;chapter in Gelman and Hill&lt;/a&gt; (pdf). &lt;/p&gt;&#10;&#10;&lt;p&gt;I assume you are working from survey data, in which case you could have quite a few variables with which you can perform imputation. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are doing your analysis in R, you may want to check out random forest missing values imputation, or rfImpute in the randomForest package. This approach has the advantage that it deals well with both categorical and continuous data, and the underlying model is quite flexible. In a paper last year (it's not a great paper!) I give a little description of how it works (see section 4 of &lt;a href=&quot;http://grattan.edu.au/static/files/assets/0b7377cf/164_graduate_winners_non-financial_benefits.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;All the best&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-22T23:46:20.360" Id="80381" LastActivityDate="2013-12-22T23:57:53.457" LastEditDate="2013-12-22T23:57:53.457" LastEditorUserId="10123" OwnerUserId="10123" ParentId="73514" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="80423" AnswerCount="1" Body="&lt;p&gt;I found excellent notes on ARCH and GARCH models &lt;a href=&quot;http://public.econ.duke.edu/~boller/Econ.350/talk_garch_11.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. On page 3 it is given that: &lt;/p&gt;&#10;&#10;&lt;p&gt;Standard time series models:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{eqnarray*}&#10;Y_{t} &amp;amp; = &amp;amp; E\left(Y_{t}|\Omega_{t-1}\right)+\epsilon_{t}\\&#10;E\left(Y_{t}|\Omega_{t-1}\right) &amp;amp; = &amp;amp; \mu_{t}\left(\theta\right)\\&#10;E\left(Y_{t}|\Omega_{t-1}\right) &amp;amp; = &amp;amp; E\left(\epsilon_{t}^{2}|\Omega_{t-1}\right)=\sigma^{2}&#10;\end{eqnarray*}&lt;/p&gt;&#10;&#10;&lt;p&gt;What does this $\Omega$ means here? Any help will be highly appreciated. Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-23T15:45:44.747" Id="80410" LastActivityDate="2013-12-23T18:03:24.157" OwnerUserId="3903" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;garch&gt;&lt;arch&gt;" Title="ARCH and GARCH Models" ViewCount="136" />
  
  
  
  <row AcceptedAnswerId="80441" AnswerCount="2" Body="&lt;p&gt;I am looking for a statistical model to detect grouping patterns among a population X of n element with respect to their association with k elements of population Y.  In modern marketing, an example of that would be &quot;customers who bought X also bought Y&quot;, such as on Amazon etc.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X (person)  Y (purchase)&#10;------------------------&#10;Jimmy       Lure&#10;Joe         Fishing Rod&#10;Jimmy       Fishing Rod&#10;Sue         Sewing Machine&#10;Sue         Thread&#10;Patty       Sewing Machine&#10;Jack        Tennis Balls&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Above is just a very simple example.  But imagine that 80% of people who bought a &lt;code&gt;Fishing Rod&lt;/code&gt; also bought &lt;code&gt;Lure&lt;/code&gt; and similar for &lt;code&gt;Sewing Machine&lt;/code&gt; vs &lt;code&gt;Thread&lt;/code&gt;.  While the pattern may be obvious to the naked eye, I am looking for a more deterministic algorithm to detect this seemingly simple pattern.  Imagine that this is a simple two dimensional array.  Could you produce a pseudo code (or Java, Python, C or whatever suits you) algorithm that I could use to determine the most closely related elements in X based on their association with elements from Y?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-23T20:46:00.013" FavoriteCount="1" Id="80440" LastActivityDate="2013-12-24T11:59:35.310" LastEditDate="2013-12-23T20:54:48.290" LastEditorUserId="14499" OwnerUserId="14499" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;data-mining&gt;&lt;pattern-recognition&gt;" Title="How to mine the &quot;customers who bought X also bought Y&quot; type of correlation?" ViewCount="173" />
  
  <row Body="&lt;p&gt;There are two possibilities here: scaling the sample range to get an approximate idea of the &lt;em&gt;sample&lt;/em&gt; standard deviation, and using the sample range to produce an estimate of the &lt;em&gt;population&lt;/em&gt; $\sigma$ (my earlier comment failed to make the distinction clear). &lt;/p&gt;&#10;&#10;&lt;p&gt;I think your question is asking about the second, but I'll deal with both.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll tackle these two in order.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The ratio of sample range ($\max(x) - \min(x)$) to sample standard deviation is sometimes call the &lt;a href=&quot;http://en.wikipedia.org/wiki/Studentized_range&quot; rel=&quot;nofollow&quot;&gt;studentized range&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;So if the middle of the distribution of studentized range was 3, it could make sense to approximate the sample standard deviation from the range by dividing the range by 3.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Some people are no doubt wondering why we'd bother - after all why not just calculate the standard deviation instead of a noisy approximation of it? Perhaps we're trying to an eyeball estimate from a scatterplot or something, and the range is relatively quick to get by eye. Sometimes we can get the minimum and maximum but not the standard deviation.]&lt;/p&gt;&#10;&#10;&lt;p&gt;So for the normal distribution, how is the studentized range distributed?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are simulated distributions, for 100000 simulations at various sample sizes for normal data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sDOK7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         mean        sd&#10;n=6   2.663658 0.2198905&#10;n=10  3.164088 0.3085827&#10;n=30  4.119035 0.4355710&#10;n=100 5.025396 0.4884935&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;If instead we're trying to estimate the population standard deviation, what matters is the distribution of the range at $\sigma=1$ (since we can work out the distribution for other $\sigma$ is obtained by direct scaling):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lsLeh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          mean        sd&#10;n=6   2.536377 0.8495843&#10;n=10  3.080769 0.8013887&#10;n=30  4.088134 0.6918155&#10;n=100 5.016027 0.6044998&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This results in somewhat different distributional shape and different mean and standard deviation (though Slutsky's theorem suggests that as $n$ becomes large they should be more and more similar).&lt;/p&gt;&#10;&#10;&lt;p&gt;-&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/69575/relationship-between-the-range-and-the-standard-deviation&quot;&gt;The answers to this question&lt;/a&gt; discusses the interesting property that at n=6, for many different distributions the range divided by 2.5 provides a reasonable approximation to the standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tippett produced extensive tables of the expected value of the sample range for the standardized normal in 1925 (and briefer tables of the standard deviation and standardized 3rd and 4th moments). &lt;/p&gt;&#10;&#10;&lt;p&gt;Tippett, L.H.C. (1925),&lt;br&gt;&#10;On the Extreme Individuals and the Range of Samples Taken from a Normal Population,&lt;br&gt;&#10;&lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;17&lt;/strong&gt; (3-4): 364-387 &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In either case (approximating $s$ or estimating $\sigma$ for samples from normal distributions), around sample sizes of 8-9, dividing by 3 produces a reasonably good estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;In very large samples, you'd expect a kind of &lt;strong&gt;&lt;em&gt;6&lt;/em&gt;&lt;/strong&gt;-sigma rule to apply, since the &quot;3-sigma&quot; rule is 3 standard deviations &lt;em&gt;either side&lt;/em&gt; of the mean. But it's not an asymptotic result since in sufficiently large samples you expect to see the range exceed $6\sigma$. Indeed, by n=1000, we're already close to $6.5\sigma$, and by n=10,000 its near $7.7\sigma$, and for n=100,000 it's somewhere above $8.75\sigma$; the mean of the distribution of the range continues to increase as $n$ increases, but at what appears to be roughly as the &lt;em&gt;square-root of the log&lt;/em&gt; of the sample size (that's a pretty slow increase).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dqsld.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Update based on discussion in comments:&lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds from your comments that the distribution of values are actually reasonably right skew. &lt;/p&gt;&#10;&#10;&lt;p&gt;I just did a quick simulation to see (at sample size 100) what gamma shape parameter would result in a typical ratio of (max-mean)/(mean-min) of around 1.76 (it turns out to be about $\alpha=7$). So then the question is, for that shape, and at that sample size, &lt;em&gt;how much difference does it make if you use the normal values above?&lt;/em&gt;. The somewhat surprising answer is 'hardly any at all'.&lt;/p&gt;&#10;&#10;&lt;p&gt;You'd want to check at some other sample sizes, but the upshot is - if the actual distribution of values on which your extremes and mean are based is a shifted gamma with shape parameter around 7 (which is moderately skew) - then, at least near n=100, estimates of $\sigma$ you produce from scaling max-min as if they were normal should be about right.&lt;/p&gt;&#10;&#10;&lt;p&gt;That surprises me that it had even this level of robustness, but it should reassure you a little, at least. &lt;/p&gt;&#10;&#10;&lt;p&gt;Having repeated the simulation exercise at n=30, and a shape parameter of 7, if the ratio of (max-mean)/(mean-min) doesn't tend to be much larger than 1.76, you should be pretty safe - at least &lt;em&gt;on average&lt;/em&gt; - using that normal-based rule to estimate $\sigma$ if the distribution of results isn't much heavier tailed than gamma.&lt;/p&gt;&#10;" CommentCount="18" CreationDate="2013-12-24T00:29:50.120" Id="80449" LastActivityDate="2013-12-26T04:17:59.670" LastEditDate="2013-12-26T04:17:59.670" LastEditorUserId="805" OwnerUserId="805" ParentId="80444" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You can still calculate SD for percent change the same way as you normally do.  However, keep in mind that the distribution for percent change may be different (skewed or otherwise non-normal) so check your assumptions.  But you should be able to run a standard t-test or ANOVA otherwise.  Just realize that your SD will be different.  Often a better way of analyzing change from baseline (instead of percent change) is to model the dependent variable while putting the baseline value as a covariate in the model (i.e, in a regression or ANCOVA).  Then you don't need to worry about a different SD for percent change.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-24T02:56:16.807" Id="80456" LastActivityDate="2013-12-24T02:56:16.807" OwnerUserId="24404" ParentId="80451" PostTypeId="2" Score="1" />
  
  
  <row Body="A maximum is the largest value in a set, function, variable, distribution etc." CommentCount="0" CreationDate="2013-12-24T08:40:53.833" Id="80472" LastActivityDate="2013-12-24T10:04:08.333" LastEditDate="2013-12-24T10:04:08.333" LastEditorUserId="26338" OwnerUserId="26338" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;The frequency measures the number of times an event occurs in a data set. The absolute frequency counts how often each event occurred in the sample whereas the relative frequency divides the absolute frequencies by the total number of events. This gives the relative probabilities of the occurrence of a particular event.&lt;/p&gt;&#10;&#10;&lt;p&gt;Frequencies are often plotted by histograms or kernel density estimates. When plotting absolute and relative frequencies with the histogram the shape of the distribution does not change. Only the values on the x-axis change.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-24T09:09:12.077" Id="80474" LastActivityDate="2013-12-24T12:20:48.790" LastEditDate="2013-12-24T12:20:48.790" LastEditorUserId="26338" OwnerUserId="26338" PostTypeId="5" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Imagine we have the following mixed model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod &amp;lt;- lmer(y ~ x + z +(1|g), data = dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y &amp;lt;- 1:12&#10;x &amp;lt;- 1:12 + rnorm(12)*2&#10;z &amp;lt;- 1:12 + rnorm (12)^2&#10;g &amp;lt;- c(letters[1:4], letters[1:4], letters[c(1,2,5,6)])&#10;dat &amp;lt;- data.frame(y=y,x=x,z=z,g=g)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;One problem with this model is that, &lt;code&gt;cor(x,z)&lt;/code&gt; is very high. So imagine we want to do a PCA with &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; and then pit &lt;code&gt;PC1&lt;/code&gt; and &lt;code&gt;PCA2&lt;/code&gt; to &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However this is not a simple &lt;code&gt;glm&lt;/code&gt; as we have complex &lt;code&gt;group&lt;/code&gt; structure &lt;code&gt;g&lt;/code&gt;. Can we do a simple &lt;code&gt;PCA&lt;/code&gt; on &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; then include this in &lt;code&gt;mod&lt;/code&gt; or do we have to do something more complex accounting for the group structure &lt;code&gt;g&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-24T11:26:42.510" Id="80479" LastActivityDate="2013-12-24T11:26:42.510" OwnerUserId="19744" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;pca&gt;&lt;lme4&gt;" Title="PCA for predictors in lmm or glmm" ViewCount="70" />
  <row Body="&lt;p&gt;Recursive partitioning (CART) will require an enormous sample size in order to achieve stability and have good discrimination ability.  It will not handle continuous variables appropriately, and its apparent interactions are often spurious.  I suggest fitting a pre-specified logistic regression model with pre-specified sensible interactions, and using a quadratic penalty if the sample size does not support the pre-specified degrees of freedom.  Then the search for a set of $X$ yielding maximum $\hat{P}$ is an algebraic problem &lt;em&gt;except&lt;/em&gt; that this is subject to a large multiple comparison problem that will result in regression to the mean (overprediction of high $P$).  You can bootstrap the entire process to (1) check the stability of the solution for $X$ and (2) find a multiplicity-penalized confidence interval for $P_{\textrm max}$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-24T12:20:43.083" Id="80483" LastActivityDate="2013-12-24T12:20:43.083" OwnerUserId="4253" ParentId="78370" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $Y_{n}$ be a sequence of independent and identically distributed random variables and $X_{n}=\frac{Y_{n}}{n}$ . show that $X_{n}$ converges in probability. decide whether $X_{n}$ converges a.e or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks for help.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-24T13:02:39.840" FavoriteCount="0" Id="80484" LastActivityDate="2013-12-24T13:02:39.840" OwnerUserId="32089" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;convergence&gt;" Title="$X_{n}$ converges in probability where $X_{n}=\frac{Y_{n}}{n}$" ViewCount="68" />
  <row AnswerCount="0" Body="&lt;p&gt;I wish to test some model fitting software and I would like to generate synthetic datasets for this test. The synthetic data is supposed to originate from a experiment that measures the change in concentration of a substance over time. I can generate error free data for such an experiment using a differential equation model. My question however is what kind of noise should I add so that the data resembles a real experimental dataset? My current two choices are to either add noise from a gaussian or exponential distribution, any preferences? (Eg adding gaussian noise could potentially generate negative concentrations) &lt;/p&gt;&#10;&#10;&lt;p&gt;My other concern is should the degree of noise be equal for all data points? For example I would imagine there would be more uncertainty in small concentrations that larger ones, simply because it is more difficult to measure small concentrations. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-24T18:03:20.397" Id="80495" LastActivityDate="2013-12-24T19:12:18.890" LastEditDate="2013-12-24T19:12:18.890" LastEditorUserId="36563" OwnerUserId="36563" PostTypeId="1" Score="0" Tags="&lt;curve-fitting&gt;&lt;synthetic-data&gt;" Title="Generating artificial experimental data to test model fitting software" ViewCount="60" />
  
  
  <row Body="&lt;p&gt;To obtain the AIC or BIC criteria in ARMA models, you need to find the number of estimated parameters in the model (except for the residual variance) including the constant term if it is estimated as mentioned in &quot;&lt;a href=&quot;http://books.google.ca/books?id=Bqm5kJC8hgMC&amp;amp;pg=PA164&amp;amp;lpg=PA164&amp;amp;dq=AIC%20number%20of%20parameter%20ARIMA&amp;amp;source=bl&amp;amp;ots=lOj1jRc28S&amp;amp;sig=oyFl8pcax5ukf_UeHL2WNGX7C5w&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=9-y6Uob3IOqe2gX_mYHQBw&amp;amp;ved=0CD0Q6AEwAzgK#v=onepage&amp;amp;q=AIC%20number%20of%20parameter%20ARIMA&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Time Series Analysis and Forecasting by Example&lt;/a&gt;&quot;, by  Søren Bisgaard, Murat Kulahci, page 164. So the number of estimated parameters in ARMA(p,q) that contains an intercept or constant term is $p+q+1$ and $p+q$ when you don't have an intercept. See for example Section 6.5 in &quot;&lt;a href=&quot;http://books.google.ca/books?id=MrNY3s2difIC&amp;amp;printsec=frontcover&amp;amp;dq=time%20series%20analysis%20with%20applications%20in%20r&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=h-m6UoPCL8Pr2AXRpYB4&amp;amp;ved=0CDUQ6AEwAQ#v=onepage&amp;amp;q=time%20series%20analysis%20with%20applications%20in%20r&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Time Series Analysis: With Applications in R&lt;/a&gt;&quot; by Jonathan D. Cryer, Kung-Sik Chan.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-25T14:48:18.450" Id="80530" LastActivityDate="2013-12-25T14:48:18.450" OwnerUserId="13138" ParentId="80528" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Two types of treatment were considered to relieve pain after surgery. &#10;Pain is measured as follows:&#10; first 30 min after first interpleural (IP) injection (hour zero) in recovery room,  then every 4 hours in resting position &lt;strong&gt;before and 30 min after&lt;/strong&gt; IP injection (i.e., hours 4, 8, 12, 16, 20, and 24 postoperatively) in ICU using faces pain scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The repeated measurements are both &lt;strong&gt;before-after&lt;/strong&gt; and &lt;strong&gt;longitudinal&lt;/strong&gt; (0, 4, 8, 12, 16, 20 and 24 hours).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The dependent variable (pain) is recorded on an ordinal scale, and we can't subtract  difference before-after use this difference in ordinal logistic models.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Which analysis is better to highlight the difference between the two treatment groups?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-26T17:42:23.630" Id="80580" LastActivityDate="2013-12-26T18:24:31.293" LastEditDate="2013-12-26T18:24:31.293" LastEditorUserId="930" OwnerUserId="36605" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;&lt;ordinal&gt;&lt;longitudinal&gt;" Title="Before-after measurement and ordinal longitudinal analysis" ViewCount="59" />
  
  
  <row AcceptedAnswerId="80615" AnswerCount="1" Body="&lt;p&gt;Let $X_{n}$ and $Z$ be random variables on probability space $(\Omega ,\mathcal F,P)$ and $Z$ be integrable. show that $$X_{n} \ge Z \qquad \Longrightarrow \qquad E[\liminf_{n\to \infty}  X_{n}] \le \liminf_{n\to \infty}  E[X_{n}] $$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-27T06:41:43.437" Id="80613" LastActivityDate="2013-12-27T07:11:30.600" OwnerUserId="34694" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;random-variable&gt;" Title="$X_{n}$ and $Z$ be random variables if $X_{n} \ge Z$ then $ E[\liminf_{n\to \infty} X_{n}] \le \liminf_{n\to \infty} E[X_{n}] $" ViewCount="38" />
  
  <row Body="&lt;p&gt;Short answer: Torgo describes the usual method of generating such curves.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can choose your threshold (= cut-off limit in the cited text) at any value.  The cited text refers to one such choice as a working point.&lt;br&gt;&#10;That is, for a given working point, you'll observe exactly one (precision; recall) pair, i.e. one &lt;em&gt;point&lt;/em&gt; in your graph. The precision-recall-&lt;em&gt;curve&lt;/em&gt; is obtained by varying the threshold over the whole range of the classifier's continuous output (&quot;scores&quot;, posterior probabilities, &quot;votes&quot;) thus generating a curve from many working points. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edit with respect to the comment:&lt;/p&gt;&#10;&#10;&lt;p&gt;I think &quot;varying the threshold&quot; is the usual way to &lt;em&gt;explain&lt;/em&gt; or &lt;em&gt;define&lt;/em&gt; the curve. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the &lt;em&gt;calculation&lt;/em&gt;, it is more efficient to sort the scores, and then see how precision and recall change when adding the next case: precision and recall can only change when the change in the threshold is large enough to cover the next score. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider this example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;case   true class   predicted score (high =&amp;gt; class B)&#10;1      A            0.2&#10;3      B            0.5&#10;2      A            0.6&#10;4      B            0.9&#10;&#10;threshold      recall    precision&#10;&amp;gt; 0.9          N/A       0.0&#10;(0.6, 0.9]     0.5       1.0        &#10;(0.5, 0.6]     0.5       0.5&#10;(0.2, 0.5]     1.0       0.67&#10;&amp;lt; 0.2          1.0       0.5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That is, the precision-recall-curve acutally consists of points. It jumps from one point to the next when the threshold &quot;crosses&quot; an acutally predicted score. A smooth curve will result only for large numbers of test cases.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-27T07:46:10.980" Id="80617" LastActivityDate="2013-12-29T18:14:59.827" LastEditDate="2013-12-29T18:14:59.827" LastEditorUserId="4598" OwnerUserId="4598" ParentId="80599" PostTypeId="2" Score="2" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have a time series as following. It has an upward trend and weak sesonality (found in ACF). I tried spectral analysis, but the residual error kept staying over 600, which is not small enough for a prediction. Any advice? Thx!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ts &amp;lt;- c(1530.040 3527.839 3327.613 3613.749 2971.990 3747.925 3352.686 5203.718 4550.774 3497.677 2719.688 2618.564 1378.669 3743.931 5094.959 3998.374 3649.445 3709.277 4078.814 6663.439 5774.322 4881.767&#10;4233.501 4824.077 3207.395 6078.317 5398.443 3541.734 2852.284 3253.765 4716.344 5239.078 5267.733 4072.201 3796.414 4309.636 3751.775 5954.923 5627.322 3862.649 3673.138 4551.715 5904.327 5712.579 6108.412 4706.165 5230.390 3756.871 3079.763 5934.894 5172.300 3194.798 3167.828 4333.597 4512.727 7609.356 7728.995 6024.994 4947.637 4491.105)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-12-27T16:26:32.420" Id="80646" LastActivityDate="2013-12-27T22:04:48.183" LastEditDate="2013-12-27T17:16:39.823" LastEditorUserId="21599" OwnerUserId="36625" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;spectral-analysis&gt;" Title="How to reduce residual error?" ViewCount="278" />
  
  
  <row Body="CAR refers to the Conditional Auto-regressive models, which is used in spatial statistics." CommentCount="0" CreationDate="2013-12-27T20:57:42.850" Id="80665" LastActivityDate="2013-12-27T21:09:58.513" LastEditDate="2013-12-27T21:09:58.513" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="4" Score="0" />
  
  
  <row Body="Sweave is a tool that allows to embed the R code for complete data analyses in latex documents to create dynamic reports." CommentCount="0" CreationDate="2013-12-27T23:03:35.557" Id="80688" LastActivityDate="2013-12-27T23:43:27.147" LastEditDate="2013-12-27T23:43:27.147" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;NumPy is the fundamental package for scientific computing with Python. It contains among other things:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a powerful N-dimensional array object&lt;/li&gt;&#10;&lt;li&gt;sophisticated (broadcasting) functions&lt;/li&gt;&#10;&lt;li&gt;tools for integrating C/C++ and Fortran code&lt;/li&gt;&#10;&lt;li&gt;useful linear algebra, Fourier transform, and random number capabilities&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-27T23:33:05.263" Id="80694" LastActivityDate="2013-12-28T00:00:11.373" LastEditDate="2013-12-28T00:00:11.373" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="5" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am looking into computing a correlation between two variables, x and y, each having a general normal distribution and not i.i.d. normal distribution. In particular these two variables have a particular variance-covariance structure, i.e., Var(x)=A and Var(y)=B. Is it still OK to compute correlation between x and y as shown bellow or should I use any other approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;Cor(x, y) = Cov(x, y) / sqrt(Var(x)*Vary)&lt;/p&gt;&#10;&#10;&lt;p&gt;In detail I have a set of simulated/true values stored in the variable x that has a particular structure due to the nature of values - there is some underlying data generation process giving rise to variance-covariance structure Var(x)=A. I then try to estimate these values based on some data which gives me variable y. Due to the nature of x, there is also an underlying structure in y that can be described by Var(y)=B. A and B are not the same due to conditioning on the observed data. I want to evaluate Cor(x, y) to validate the quality of estimation and could use a simple standard approach, but I wonder if I am making a mistake here by treating x and y as iid observations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-28T16:49:22.247" Id="80739" LastActivityDate="2014-02-27T19:41:19.643" OwnerUserId="7761" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Correlation between two intra correlated variables" ViewCount="68" />
  <row Body="&lt;p&gt;Sorry, a bit new here so please excuse me if this doesn't help too much.&lt;/p&gt;&#10;&#10;&lt;p&gt;The US Social Security Administration keeps records of births and deaths and has their information available for purchase (apparently for a hefty price): &lt;a href=&quot;https://faq.ssa.gov/ics/support/KBAnswer.asp?questionID=1872&amp;amp;hitOffset=118%20117%20116%2093%2092%2091%2072%2071%2070%2065%2064%2063%2056%2055%2054%2025%2024%2023%2013%2012%2011&amp;amp;docID=4584&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However I found a source that claims to have bought it and is offering it for free (as well as offering the data sorted by date on the site): &lt;a href=&quot;http://ssdmf.info/download.html&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming you can just use that as your sample and go through all the data with a script and find how many people actually die on their birthday. I would do that myself but I have 20 min left to download (they're about 1.5GB) so I'll try to get back to you on the statistics myself if I find the time to write up a script.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course the United States can't represent the entire world's population but it is a good start. I'm assuming you will see a higher rate in deaths on birthdays because of &quot;first world problems&quot; because we're using the United States and I think the effect would be less visible across the world...&lt;/p&gt;&#10;&#10;&lt;h2&gt;Update - Numbers :D&lt;/h2&gt;&#10;&#10;&lt;p&gt;I've ran through the Social Security Death Master File from the free source, so there's no way knowing if the information is valid. However, given the size that they're ~3 Gigabytes each and that there's no reason for anyone to spoof these kind of files... I'll assume they are valid.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can see the code that I used to run through it here: &lt;a href=&quot;http://pastebin.com/9wUFuvpN&quot;&gt;http://pastebin.com/9wUFuvpN&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's written in C#, it reads through the lines of the death index one by one and then parses the date using regex. I assumed that the file was basically this format: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;`(Social Security Number)(First Name)   (LastName)   (Middle Name)    (Some Letter)(MM-DD-YYYY of Death)(MM-DD-YYYY Of Birth)`&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I had regex just pick out the last part for the dates of birth/death, check if any of the fields are just 0 (which I'm assuming it means that Social Security couldn't get a valid month/date for the record), and discard the 0's. Then it'll check if the day of birth and month of birth match the day of death/month of death and add that to the died on birthday count. It'll add all records that aren't 0's to the death count.&lt;/p&gt;&#10;&#10;&lt;p&gt;It outputs the results in this format:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Deaths On Birthday/Total Deaths   Lines Looked Through - People With a 0 in any of their record&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's be great if someone could double check that code, as I've found quite a few errors I've made before and could only tell because my results made no statistical sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the console output: &lt;img src=&quot;http://i.stack.imgur.com/tQ6hZ.png&quot; alt=&quot;Console Output&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Doing some math...&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;File 1 had 44665 Deaths on a Birthday out of 14879058 Deaths in Total&lt;/li&gt;&#10;&lt;li&gt;File 2 had 47060 Deaths on a Birthday out of 15278724 Deaths in Total&lt;/li&gt;&#10;&lt;li&gt;File 3 had 49289 Deaths on a Birthday out of 15374049 Deaths in Total&lt;/li&gt;&#10;&lt;li&gt;Total we have 141014 Deaths on a Birthday out of 45531831.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So we have ~0.3097% chance of dying on a birthday while statistically (1/365) would lead us to believe there is only ~0.27397% chance of dying on a birthday. That is indeed a 13% increase in chance of death on a birthday from 1/365. Of course this sample is only for Americans and only has 45 million records, I'm sure organizations who originally published their paper had access to much more reliable and larger death indexes. However, I think that it is indeed valid that deaths on a birthday is more likely than death on any other day.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a Time article citing jumps in reasons for death on birthdays: &lt;a href=&quot;http://newsfeed.time.com/2012/06/12/study-youre-most-likely-to-die-on-your-own-birthday/&quot;&gt;Article&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; @cbeleites pointed out that I forgot to account for same day deaths, which would be a huge factor in increasing deaths on birthdays. Strictly speaking my data is still valid but I did not throw out if a person died on the same day they were born. It's interesting that my results were not affected too heavily by this error so it seems that these records don't include death on first day. I'll look into it later. I'm thinking there would be very interesting statistics I can look for such as death on days of the month and make a heatmap of some sort. I'll probably try to do that sometime...&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-28T16:53:47.473" Id="80740" LastActivityDate="2013-12-30T01:26:07.760" LastEditDate="2013-12-30T01:26:07.760" LastEditorUserId="36656" OwnerUserId="36656" ParentId="80738" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;If you are testing whether 44 occurrences out of 63 can be a sample from the binomial population with probability 80% of a success in individual independent trial, then the first test is certainly correct.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-28T18:07:22.460" Id="80742" LastActivityDate="2013-12-28T18:07:22.460" OwnerUserId="36545" ParentId="80741" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;There is no precise way.&lt;/p&gt;&#10;&#10;&lt;p&gt;My suggestion would be to &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;draw a cumulative frequency graph on some graph paper, &lt;/li&gt;&#10;&lt;li&gt;attempt to join the dots with a smooth freehand curve, &lt;/li&gt;&#10;&lt;li&gt;read off your interpolated cumulative figures at $22$ and $58$,&lt;/li&gt;&#10;&lt;li&gt;use the difference as your estimate of the number of workers earning between the two amounts, and&lt;/li&gt;&#10;&lt;li&gt;divide by the total number of workers &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If your answer to (4) is $342$ or slightly more then you are probably on the right track&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-28T23:46:47.270" Id="80754" LastActivityDate="2013-12-28T23:46:47.270" OwnerUserId="2958" ParentId="80733" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Actually, an earlier paper (predating the &quot;random forest&quot; terminology) from Leo Breiman considered all features as you suggest &lt;a href=&quot;http://www.machine-learning.martinsewell.com/ensembles/bagging/Breiman1996.pdf&quot;&gt;[Breiman 1996]&lt;/a&gt;. Below are some relevant excerpts from this article. First, some notation:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A learning set of $\mathcal{L}$ consists of data $\left\{ (y_n,x_n),n=1,...,N\right\}$ where the $y$'s are either class labels or a numerical response. Assume we have a procedure for using this learning set to form a predictor $\varphi(\textbf{x},\mathcal{L})$ -- if the input is $\textbf{x}$ we predict $y$ by $\varphi(\textbf{x},\mathcal{L})$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The idea behind bagging (and, by extension, random forests) is to stochastically construct many different predictors $\varphi$ and aggregate this population predictors into a single predictor, denoted $\varphi_B$, which generally has superior performance to any of the individual predictors. Stochasticity is added by constructing each predictor on a bootstrap sample of the original training set, rather than the entirety of $\mathcal{L}$. As Gavin Simpson mentioned, selecting a random subset of features adds even more stochasticity to the construction of individual predictors. Breiman (1996) writes:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A critical factor in whether bagging will improve accuracy is the stability of the procedure for constructing $\varphi$. If changes in $\mathcal{L}$, i.e. a [bootstrap] replicate of $\mathcal{L}$, produces small changes in $\varphi$, then $\varphi_B$ will be close to $\varphi$. &lt;strong&gt;Improvement will occur for unstable procedures where a small change in $\mathcal{L}$ can result in large changes in $\varphi$&lt;/strong&gt; . . . The evidence, both experimental and theoretical, is that bagging can push a good but unstable procedures a significant step towards optimality. On the other hand, it can slightly degrade the performance of stable procedures. &lt;em&gt;[Emphasis Added]&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The paper then goes on to provide said theoretical and experimental evidence. Breiman's seminal description of random forests &lt;a href=&quot;http://machinelearning202.pbworks.com/w/file/fetch/60606349/breiman_randomforests.pdf&quot;&gt;(Breiman, 1999)&lt;/a&gt; credits Tim Kam Ho for developing &quot;the random subspace method&quot; which adds additional instability by selecting $m$ random features to grow each tree (rather than each node). The original publication &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=709601&amp;amp;url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=709601&quot;&gt;(Ho, 1998)&lt;/a&gt; appears to be behind a paywall unfortunately.&lt;/p&gt;&#10;&#10;&lt;p&gt;One final note -- one interesting question is what to choose for $m$? That is, how many random features should we select to split on for each node? There is a section in Breiman (1999) devoted to this question. The basic conclusion is that the choice does not matter substantially, as long as $m$ is relatively small compared to the total number of features.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-29T01:04:09.470" Id="80759" LastActivityDate="2013-12-29T01:04:09.470" OwnerUserId="35917" ParentId="80751" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;According to &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_distribution#Poisson_approximation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia: Poisson approximation&lt;/a&gt;, the use of this approximation seems ok as n=140 and np=7&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Poisson approximation The binomial distribution converges towards the&#10;  Poisson distribution as the number of trials goes to infinity while&#10;  the product np remains fixed. Therefore the Poisson distribution with&#10;  parameter λ = np can be used as an approximation to B(n, p) of the&#10;  binomial distribution if n is sufficiently large and p is sufficiently&#10;  small. According to two rules of thumb, this approximation is good if&#10;  n ≥ 20 and p ≤ 0.05, or if n ≥ 100 and np ≤ 10.[9]&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;From wikipedia, it is clear your P(x) is a mass function where giving the probability of a particular value of x, not a cumulative distribution function giving the probability of a value below or equal to x.&lt;/p&gt;&#10;&#10;&lt;p&gt;To find the probability for x (the number of sales) to be at least 2, you can calculate either:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Prob{x&amp;gt;=2} = Sum(P(x) for integer x&amp;gt;=2)&lt;/code&gt; which is an infinitely long sum, or&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Prob{x&amp;gt;=2} = 1 - Prob(x&amp;lt;2) = 1 - P(0) - P(1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would imagine the second method to be easier, yielding:&lt;/p&gt;&#10;&#10;&lt;p&gt;Prob{x&gt;=2} = 1 - exp(-7) - 7 exp(-7) = 1 - 8 exp(-7) = 0.9927&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-29T02:37:57.847" Id="80770" LastActivityDate="2013-12-29T02:37:57.847" OwnerUserId="87" ParentId="79397" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;code&gt;dlm&lt;/code&gt; refers to the &lt;code&gt;R&lt;/code&gt; package for Bayesian and likelihood analysis of dynamic linear models. It can be used for maximum likelihood, Kalman filtering and smoothing, and Bayesian analysis of normal linear state space models, also known as dynamic linear models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-29T05:24:37.310" Id="80779" LastActivityDate="2013-12-29T11:25:09.867" LastEditDate="2013-12-29T11:25:09.867" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="5" Score="0" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have count data for a pair of test and control populations of samples for a variable that takes a value of either A or B for a sample:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            A   |   B&#10;         -------+-------&#10;Control     7   |   1&#10;Test        3   |   5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The hypothesis is that the test condition stimulates the conversion of the observed parameter from a state with a value of A to a state with a value of B, and I can use the Fisher Exact test on the data to accept or reject this hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose we have three such pairs of test and control populations, with varying numbers of observations carried out for each population for the test or control condition.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                1                2                 3&#10;&#10;            A   |   B        A   |   B         A   |   B&#10;         -------+------   -------+-------   -------+-------&#10;Control     7   |   1       70   |  10         7   |   1&#10;Test        3   |   5        3   |   5       300   | 500&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can conduct the Exact test thrice, once for each pair, and obtain three P values, which we somehow assess together (majority rule?) to come to a conclusion.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, is there some other statistical test that can be used, perhaps one that yields a single P value, or is more robust?&lt;/p&gt;&#10;&#10;&lt;p&gt;[Adding information about the actual experiment]&lt;/p&gt;&#10;&#10;&lt;p&gt;RNA molecules produced from a certain gene in the cells of the human body get edited because of exposure to the test condition, resulting in the conversion of a specific nucleotide base of the RNA from being an adenine (A; equivalent to the hypothetical state A above) to a guanine (G; equivalent to the hypothetical state B above). The editing is assayed by sequencing the RNA molecules, with varying number of molecules being sequenced for the control and test conditions. The three tables allude to data from three humans.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-29T09:35:32.513" Id="80789" LastActivityDate="2014-01-07T04:10:31.330" LastEditDate="2013-12-29T09:51:57.600" LastEditorUserId="4045" OwnerUserId="4045" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;fishersexact&gt;&lt;paired-data&gt;" Title="Alternative to multiple Fisher's Exact tests" ViewCount="260" />
  <row Body="&lt;p&gt;If you should view your individual vectors as time series (because of the signs of autocorrelation you mentioned), you should be more interested in a cross-correlation function (ccf) of the two time series. This functions is essentially the product-moment correlation as a function of lag, or time-offset, between the series. This link &lt;a href=&quot;http://www.ltrr.arizona.edu/~dmeko/notes_10.pdf&quot; rel=&quot;nofollow&quot;&gt;contains the relevant details&lt;/a&gt;, including statistical significance of the ccf. The example considered is as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Consider the two time series of annual tree-ring index plotted in Figure 1. The plots&#10;suggest the series are positively correlated, but the year-to-year variations are too&#10;noisy to visually judge whether one series tends to lead or lag the other. Both series,&#10;however, appear to be positively autocorrelated, as positive departures from the mean&#10;tend to follow positive departures, and negative departures tend to follow negative&#10;departures. &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The link above makes a reference to the 6th edition of Chatfield's  &lt;em&gt;The Analysis of Time Series&lt;/em&gt; masterpiece (and the 5th edition will have that material in chapter 8 entitled Bivariate Processes) if you want to read more on this. In particular, if a test is required for a non-zero correlation between two time series, both series must first be converted to white noise (the book will tell you how) and then the usual theory applies. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-29T14:51:37.530" Id="80799" LastActivityDate="2013-12-29T14:51:37.530" OwnerUserId="273" ParentId="74038" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;There are many ways to combine the information from the individual tests. Some examples follow. Where it makes sense to use ones near the top of the list, I'd lean toward those rather than the two at the end:&lt;/p&gt;&#10;&#10;&lt;p&gt;(a) If in the three situations, both test and control are believed to be independent draws from the same population of values (a 'test' population with constant proportion, and a control 'population' with its won constant proportion - just different sized samples being drawn in each case), then you can simply combine the data tables and test that. Point and interval estimates based on the combined data reflect the common population values.&lt;/p&gt;&#10;&#10;&lt;p&gt;(b) even when you don't assume a constant control proportion and a constant test proportion (as in (a) above), under the null the difference in proportions should still be zero. You can estimate the difference in proportion for each case and add the estimated proportions and add the variances of the estimates to construct a single statistic. If the difference in proportions were constant, you could get point and interval estimates for it, but the test still works as a test even when you don't assume a constant difference in proportion -- it will be sensitive to a tendency of the differences to be in the same direction. It would usually be reasonable to use a normal approximation for this test statistic, but you might also look at simulating distributions under the null.&lt;/p&gt;&#10;&#10;&lt;p&gt;(c) (again) in the case where the test and control proportions are not assumed to be constant across the three experiments under the alternative, you could still construct a statistic that combines information from the tables in other ways. One example would be to assume it's not the difference in proportions that's constant under the alternative, but that the log-odds is constant; you could then combine estimates of the log-odds (such as by forming weighted averages of them) and use that as an overall test statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;(d) You could combined (by addition) chi-square values for the individual tables; the chi-square approximation should be better in the combined case, though again it should be possible to construct simulated null distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;(e) if the tests are independent, you can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_method&quot; rel=&quot;nofollow&quot;&gt;Fisher procedure&lt;/a&gt; (see also &lt;a href=&quot;http://stats.stackexchange.com/questions/31070/understanding-fishers-combined-test&quot;&gt;here&lt;/a&gt;), which is effectively to multiply p-values, as one would tend to with independent probabilities (though by working on the log-scale, it's easier to compute the distribution). &lt;/p&gt;&#10;&#10;&lt;p&gt;If the nulls are true, the p-values have a uniform distribution. The $-2 \ln p_i$ should be exponentially distributed with mean $2$ (i.e. $\chi^2_2$) and adding those will give something that under the null should be $\chi^2_6$. If the combined result is unusually large for a $\chi^2_6$, you'd reject the null that the p-values were drawn from a uniform distribution in favor of the alternative that they tended to be smaller. In this particular case we have the slight problem that - even under the null - the p-values are discrete, so if the numbers are very small you might want to consider simulation under the null here as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;(f) you &lt;em&gt;could&lt;/em&gt; even add p-values. If the common nulls are true, the p-values (again) should be uniform; the sum of the p-values should have the distribution of a sum of uniforms; again this sum can be tested (in this case you test whether the sum of the p-values is too small to have come from a sum-of-uniforms), though again the discreteness may be an issue in some cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where it's reasonable (from your prior knowledge of the situation) to make some assumptions (such as constant proportions, constant differences of proportions, constant log odds, or whatever) you should probably do so; this is usually more meaningful than say falling back on case (e), even though it's still a perfectly valid thing to do.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-29T17:29:04.743" Id="80815" LastActivityDate="2013-12-29T17:45:29.780" LastEditDate="2013-12-29T17:45:29.780" LastEditorUserId="805" OwnerUserId="805" ParentId="80789" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;We can be even more precise than @Mike Shi's data: the &lt;a href=&quot;http://www.savethechildren.org/atf/cf/%7B9def2ebe-10ae-432c-9bd0-df91d2eba74a%7D/SOWM-BIRTH-DAY-RISK-INDEX-2013.PDF&quot;&gt;most dangerous of all birthdays is the very first one&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The 1st day mortality rates reported there are around 0.2 % for industrialized countries and 0.8 % average for all countries. Which means that the risk of dying on the day of birth is at least as high as the risk of dying at &lt;em&gt;any&lt;/em&gt; of the following birth days*. &lt;/p&gt;&#10;&#10;&lt;p&gt;* I think it is a safe assumption that 1st day deaths do not appear in @Mark Shi's file, as the US 1st day mortality rates are reported to be 0.3 % (&lt;a href=&quot;http://www.ctvnews.ca/world/canada-has-2nd-highest-rate-of-1st-day-infant-deaths-in-industrialized-world-report-1.1270425&quot;&gt;other source: 0.26 %&lt;/a&gt;). Which is almost the total birth day death rate in the social security file. So either babies who die at the day of birth do not get a social security number, or dying on a birth day &gt; 1 year is extremely improbable.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;side note:&lt;br&gt;&#10;There are &lt;a href=&quot;http://news.nationalpost.com/2010/12/20/christmas-the-deadliest-day-of-the-year-study/&quot;&gt;other days, such as Chirstmas and New Years Eve which are known to have higher-than-average mortality rates as well&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-29T18:47:54.217" Id="80820" LastActivityDate="2013-12-29T18:47:54.217" OwnerUserId="4598" ParentId="80738" PostTypeId="2" Score="7" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm stuck on deciding what would be the best approach for my current problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have an array of approximately 1500 data points and I'm trying to find a best fit line through these points based on an complex expression with two free parameters. I found those parameters with Matlab's &lt;em&gt;lsqcurvefit&lt;/em&gt; function and I calculate the r squared to be approximately 0.7 after fitting the line. My next step is to impose a criteria to the original dataset, which reduces the number of valid data to ~ 400. Since the criteria is one of the basic assumptions for the theory behind the dataset, I expect to see a &quot;reduction&quot; in the scatter, i.e. increase in r squared. &lt;/p&gt;&#10;&#10;&lt;p&gt;First thing that came to mind was doing the fitting procedure again on the reduced dataset and then comparing the two new fitting parameters with the old ones (theory doesn't say anything about the default or best values of the parameters). However, this doesn't say anything about the scatter. Regarding this, I've looked at &lt;a href=&quot;http://www.graphpad.com/support/faqid/1765/&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;, which describes using a t-test to compare the values of the parameters. However, the sample sizes need to be approximately the same which isn't true in my case (1500 &amp;lt;-&gt; 400).&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice or help is greatly appreciated. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-12-30T01:00:07.733" Id="80839" LastActivityDate="2013-12-30T01:00:07.733" OwnerUserId="36696" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;t-test&gt;" Title="Determining if a criteria reduces scatter" ViewCount="39" />
  
  <row Body="&lt;p&gt;The best way is to find out why there is a multicollinearity in your explanatory variables and remove it! Maybe looking at the pairwise correlation among your explanatory variables will give you an idea. In addition to this, there are some other ways for dealing with multicollinearity including:    &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&quot;Do nothing&quot;! Believe it or not this is one simple way but most of the times it is not recommended.&lt;/li&gt;&#10;&lt;li&gt;Dropping some of the variables is another solution, but it is like putting your head in the sand!&lt;/li&gt;&#10;&lt;li&gt;Using an unweighted or weighted average of two variables that are highly correlated, but only if it seems logical. However, this only works when the two variables are actually a replications of one variable.&lt;/li&gt;&#10;&lt;li&gt;Centering the variables by subtracting each explanatory variable from their means.&lt;/li&gt;&#10;&lt;li&gt;You can use a little bit more advanced method to remove multicollinearity such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Principal_component_analysis&quot; rel=&quot;nofollow&quot;&gt;Principal Component Analysis&lt;/a&gt;. Here, you create a new set of variable (that are a linear combination of your explanatory variables) and use this new set of variables for your modeling. These new set of variables will be constructed in such a way that they are uncorrelated and therefore there will be no multicollinearity. In other words, you are re-defining your explanatory variables, by using some statistical analysis. See e.g. page 192 of &lt;a href=&quot;http://books.google.ca/books?id=Us4YE8lJVYMC&amp;amp;pg=PA179&amp;amp;dq=multicollinearity&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=ptjAUuffEvSA2AW7noHgAw&amp;amp;sqi=2&amp;amp;ved=0CFMQ6AEwBg#v=onepage&amp;amp;q=multicollinearity&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Regression Analysis&lt;/a&gt; by Rudolf J. Freund, William J. Wilson.&lt;/li&gt;&#10;&lt;li&gt;Sometimes, you have some prior knowledge about your explanatory variables and you will use this knowledge to re-define some new variables even without applying the Principal Component Analysis. For example, you may decide to use $X_i/X_1$ or $X_i-X_1$, of course if they have some meaning when modeling.&lt;/li&gt;&#10;&lt;li&gt;If your explanatory variables are like polynomials, then you can use orthogonal polynomials (such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Legendre_polynomials#Orthogonality&quot; rel=&quot;nofollow&quot;&gt;Legendre polynomials&lt;/a&gt;) as well to reduce multicollinearity.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-12-30T02:49:37.363" Id="80847" LastActivityDate="2013-12-30T03:10:16.157" LastEditDate="2013-12-30T03:10:16.157" LastEditorUserId="13138" OwnerUserId="13138" ParentId="80824" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It sounds like your recordings are all of the same group of captive birds, and you're not distinguishing among the calls of individual birds in this group. If this is the case, you effectively have data from one &quot;subject,&quot; so you'd need another way to control for differences among individual birds in your group. Lacking this, there's no justification for RM ANOVA; you can't separate subjects error from total error when you only have one subject—the group as a whole. I guess you could treat the 15 days as different subjects and try to separate noisy-vs.-quiet-day variance from time-of-day variance, but if consecutive days are more similar than widely separated days (e.g., if the first day is more similar to the second day than the last day), you'd still lose the ability to control for that by using RM ANOVA.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the typical, simple analysis for this kind of problem is the chi-squared goodness of fit test. Basically, this tests how well your data fit a given model of expected frequencies. It sounds like you'd want to test the usual, default model, which would represent the expectation that calls occur equally often at all three times of day. If the test produces a significant result, you can reject this model as a null hypothesis, and conclude that calls are likely to occur with different frequencies at one or more of your times of day. Using this test, you can also test different models than simple equality, which you may wish to do if you have particular theories you'd like to falsify.  However, &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Problems&quot; rel=&quot;nofollow&quot;&gt;Wikipedia's page on this test&lt;/a&gt; indicates your results won't be reliable if even one of your three cells' expected frequencies is less than 5. Null hypotheses aside, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;Cramér's V&lt;/a&gt; should work in this case as an effect size estimate for the similarity of the frequencies across your three times of day if you're willing to just pool all your different days' observations into three cells of frequencies for each particular time of day.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do want to separate out time-of-day effects from other time effects (e.g., differences between days, relationships between consecutive days), your analyses will have to be more complex than the chi-square goodness of fit test. If you're willing to do more complex analyses, a more ambitious answer than mine might be able to serve that aim. In such a case, I'd recommend editing your question to specify what kinds of effects you want to separate from one another, or commenting on my answer to specify its shortcomings for your purposes.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-30T02:52:52.943" Id="80848" LastActivityDate="2013-12-30T02:52:52.943" OwnerUserId="32036" ParentId="80834" PostTypeId="2" Score="0" />
  <row AnswerCount="10" Body="&lt;p&gt;I am not a statistician but my research work involves statistics (analyzing data, reading literature, etc.). I was again reminded from a comment on one of my questions posted &lt;a href=&quot;http://stats.stackexchange.com/questions/80789/alternative-to-multiple-fishers-exact-tests&quot;&gt;here&lt;/a&gt; that there are some common words that have particularly specific meanings or connotations for those who are well-practiced in the field of statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;It will be helpful to have a list of such words and may be phrases along with some comments.&lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2013-12-30T05:19:00.987" CreationDate="2013-12-30T03:37:33.393" FavoriteCount="1" Id="80849" LastActivityDate="2014-01-08T16:02:07.667" OwnerUserId="4045" PostTypeId="1" Score="12" Tags="&lt;terminology&gt;" Title="Common words that have particular statistical meanings" ViewCount="457" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of multivariate time series observations that I am trying to model using VAR processes, using AIC to choose the best model. However, instead of determining the best model order for each individual sample (itself multivariate), I would like to figure out the best model order for fitting &lt;em&gt;all&lt;/em&gt; the samples. However, each sample will still have unique parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, how can multiple AICs be combined?&lt;/p&gt;&#10;&#10;&lt;p&gt;More information:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;All criteria [AIC, HQC, SWC] add a penalty to the one-step ahead MSE which depends on the sample size $T$, the number of variables $m$ and the number of lags $q$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(F. Canova, &lt;em&gt;Methods for Applied Macroeconomic Research&lt;/em&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;I have $N$ samples, all with the same number of variables $m$, all cropped to identical length $T$.  I would like to compute the pool information criterion for some lag $q$. I can easily sum or average the one-step ahead MSEs for each sample. But what is the best way to pool the penalty terms? The naive approaches would be to either sum (/average) the penalty terms or just apply a single penalty term. The latter seems to capture the situation where you are fitting the same VAR model to each sample. Does the former capture the situation where you are allowing the parameters to vary for each sample but keeping the number of lags $q$ constant?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-30T03:45:13.910" Id="80850" LastActivityDate="2013-12-30T06:27:14.707" LastEditDate="2013-12-30T06:27:14.707" LastEditorUserId="36689" OwnerUserId="36689" PostTypeId="1" Score="3" Tags="&lt;model-selection&gt;&lt;aic&gt;&lt;var&gt;" Title="Computing a multi-sample (i.e., pooled) Akaike Information Criterion" ViewCount="69" />
  
  <row Body="&lt;p&gt;The following book has a chapter (Ch.9) devoted to &quot;When Bootstrapping Fails Along with Remedies for Failures&quot;:&lt;/p&gt;&#10;&#10;&lt;p&gt;M. R. Chernick, &lt;em&gt;Bootstrap methods: A guide for practitioners and researchers&lt;/em&gt;, 2nd ed. Hoboken N.J.: Wiley-Interscience, 2008.&lt;/p&gt;&#10;&#10;&lt;p&gt;The topics are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Too Small of a Sample Size&lt;/li&gt;&#10;&lt;li&gt;Distributions with Infinite Moments&lt;/li&gt;&#10;&lt;li&gt;Estimating Extreme Values&lt;/li&gt;&#10;&lt;li&gt;Survey Sampling&lt;/li&gt;&#10;&lt;li&gt;Data Sequences that Are &lt;em&gt;M&lt;/em&gt;-Dependent&lt;/li&gt;&#10;&lt;li&gt;Unstable Autoregressive Processes&lt;/li&gt;&#10;&lt;li&gt;Long-Range Dependence&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2013-12-30T13:46:24.387" Id="80875" LastActivityDate="2013-12-30T13:46:24.387" OwnerUserId="5805" ParentId="9664" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Regarding the first question, you instead want to estimate a shrinkage factor that can be applied to your regression coefficients to arrive at your &quot;final&quot; model. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Draw a bootstrap sample.  &lt;/li&gt;&#10;&lt;li&gt;Estimate your model (using your full modelling process, whatever this is) in this sample. &lt;/li&gt;&#10;&lt;li&gt;In the original data, calcuate the linear prediction using the boostrap coefficients. &lt;/li&gt;&#10;&lt;li&gt;In the original data, regress the outcome on the linear predictor you just calculated.&lt;/li&gt;&#10;&lt;li&gt;Save the regression coefficent from this model.  &lt;/li&gt;&#10;&lt;li&gt;Repeat this process many times. &lt;/li&gt;&#10;&lt;li&gt;Average all of the regression ceofficients from step 5. This is your shrinkage factor.  &lt;/li&gt;&#10;&lt;li&gt;Shink your &quot;final&quot; model in your original data using this value.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Exactly how to do this will depend on what software you are using. &lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/8668867?dopt=Abstract&quot; rel=&quot;nofollow&quot;&gt;Harrell 1996&lt;/a&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Also see &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1467-9574.00157/abstract&quot; rel=&quot;nofollow&quot;&gt;Steyerberg 2001&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-12-30T14:55:38.090" Id="80882" LastActivityDate="2013-12-30T15:02:37.563" LastEditDate="2013-12-30T15:02:37.563" LastEditorUserId="16049" OwnerUserId="16049" ParentId="80880" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Let us generally assume $U$ is an affine subspace.  Letting $\nu$ be a unit normal to $U$ and $\delta$ the distance from $U$ to the origin (in the $\nu$ direction), &lt;/p&gt;&#10;&#10;&lt;p&gt;$$U = \{x\in \mathbb{R}^n \ |\  \nu \cdot x = \delta\}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The vector $\nu$ can be completed to a basis $\{\nu, e_2, e_3, \ldots, e_n\}$ of $\mathbb{R}^n$ in which $\nu$ is orthogonal to all the $e_i.$  In this basis the distribution becomes the product of a Normal distribution in the $\nu$ direction with variance $\nu\prime C \nu$ and mean $\nu\cdot \mu$ and the distance between any $x$ and $U$ is $|\nu \cdot x - \delta|$.  The question is thereby reduced to this one-dimensional context where the answer is readily obtained.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Example&lt;/h3&gt;&#10;&#10;&lt;p&gt;Suppose $X$ is bivariate Normal with mean $(2,-3)$ and diagonal variance matrix $\Sigma$ having $4$ and $1$ on the diagonals.  Let $U$ be the hyperplane given by normal vector $(1,1)$ and distance $1$ from the origin.  Here are 10,000 simulated values along with $U$ (a line, shown in red).  Simulated points within 0.05 of $U$ are highlighted.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VzpVA.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Using a million ($10^6$) simulated values, the limiting ratio of $P(d)/d$ is estimated as $0.1411 \pm 0.00035$.  The correct value, computed as described above, is $0.14087,$ in satisfactory agreement.  The &lt;code&gt;R&lt;/code&gt; code to produce this simulation, draw the plot, and compute the correct value follows.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# Simulate from a multivariate normal distribution.&#10;#&#10;require(mvtnorm)&#10;set.seed(17)&#10;sigma &amp;lt;- diag(c(4, 1))&#10;mu &amp;lt;- c(2, -3)&#10;N &amp;lt;- 10^6&#10;x &amp;lt;- rmvnorm(N, mu, sigma)&#10;if (N &amp;lt;= 10^4) plot(x, cex=1/2, col=&quot;#00000040&quot;, asp=1)&#10;#&#10;# Describe and plot an affine hyperplane nu.x == d.&#10;#&#10;nu &amp;lt;- c(1,1)/sqrt(2)&#10;d &amp;lt;- 1&#10;if (N &amp;lt;= 10^4) abline(1/nu[2], -nu[1]/nu[2], col=&quot;Red&quot;)&#10;#&#10;# Show values close to the hyperplane and estimate their probability.&#10;#&#10;eps &amp;lt;- 0.05&#10;i &amp;lt;- abs(x %*% nu - d) &amp;lt; eps&#10;if (N &amp;lt;= 10^4) points(x[i, ], cex=1/2, col=&quot;Red&quot;)&#10;p &amp;lt;- mean(i) / (2*eps)&#10;n.dec &amp;lt;- ceiling(log(N, base=10)/2)+1&#10;#&#10;# Perform an exact calculation.&#10;#&#10;s &amp;lt;- nu %*% sigma %*% nu&#10;z &amp;lt;- dnorm(d, mean=sum(nu*mu), sd=sqrt(s))&#10;#&#10;# Display the results.&#10;#&#10;cat(&quot;Lim(P(d)/d) equals&quot;, round(z, n.dec+1))&#10;cat(&quot;Lim(P(d)/d) is approximately&quot;, round(p, n.dec), &#10;    &quot;+-&quot;, round(sqrt(p*(1-p)/N), n.dec+1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This code only computes the limiting probability to be at distance $0$ from $U$.  When performing the calculation for nonzero distances, do not forget that there are two sides to the hyperplane: you must double the value of the normal PDF.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-30T17:34:35.523" Id="80894" LastActivityDate="2013-12-30T17:34:35.523" OwnerUserId="919" ParentId="80648" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I used R optim() to find min log likelihood, however the diagonal elements of the inverse of Hessian matrix turns to be negative..&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; round(diag(solve(KFopt$hessian)),2)&#10;[1]    0.00   -0.08   -0.03    0.00   -0.47   -0.18 -167.32&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does that mean the optimization is wrong? So what should I do if I want to extract standard deviation of each estimates?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-12-31T03:46:47.463" Id="80914" LastActivityDate="2013-12-31T03:46:47.463" OwnerUserId="20966" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;optimization&gt;&lt;kalman-filter&gt;" Title="Negative Hessian matrix in R optim" ViewCount="136" />
  <row Body="&lt;p&gt;Sounds a lot like a &lt;a href=&quot;https://en.wikipedia.org/wiki/Computerized_adaptive_testing&quot;&gt;computerized adaptive testing (CAT)&lt;/a&gt; application. This is just one small hint, not an attempt at a comprehensive solution, so I hope others will keep the answers coming. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming that you're hoping to predict responses to the unasked questions from an optimally small subset of questions to such a degree of accuracy that there is effectively no need to actually ask the questions to which the answers can be predicted from previous responses. Specifically, I'm assuming a couple things about your original meaning:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;Some positive/negative features exclude others.&quot; = Some features can be used to predict the absence of others very accurately, maybe even without any error at all.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;In order to 'cut' the number of plausible subsequent questions&quot; = The purpose is to reduce the number of follow-up questions that mostly provide information that is redundant with information collected by already-asked questions.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If I've misinterpreted these parts, my hint may be misleading; otherwise, I think I'm at least pointing in the right general direction. I don't know much more about CAT than this general purpose that it serves, so I expect you'd be better equipped than me to  efficiently study it further.&lt;/p&gt;&#10;&#10;&lt;p&gt;One other idea concerns a slightly different approach, whereby you'd try to reduce the overall number of questions you care to ask at all of future users. You could begin to do this by analyzing the latent factor structure of your existing data using something like multidimensional &lt;a href=&quot;https://en.wikipedia.org/wiki/Item_response_theory&quot;&gt;item response theory&lt;/a&gt; (MIRT; see, for instance, &lt;a href=&quot;http://www.jstor.org/discover/10.2307/3657937?uid=3739560&amp;amp;uid=2129&amp;amp;uid=2&amp;amp;uid=70&amp;amp;uid=4&amp;amp;uid=3739256&amp;amp;sid=21103183564401&quot;&gt;Maydeu-Olivares, 2001&lt;/a&gt;; &lt;a href=&quot;http://www.jsswr.org/article/view/5951&quot;&gt;Osteen, 2010&lt;/a&gt;). If you find that a lot of your items provide information about the same underlying factors, this could help you understand your total pool of information in terms of a shorter list of broader factors. If you find that list (of the latent factors in your set of questions) contains enough of what you really want to know, you might choose to eliminate some questions that don't predict the latent factors very well and don't provide other important information. You might even consider retaining only one or two of the items that best predict each latent factor, depending on what you ultimately want to do with these data. This tangential idea of mine assumes that some of your questions are disposable. Also, disposing some questions would probably only simplify your problem somewhat, not really solve it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I think both CAT and MIRT would assume that your binary data are indicators of (an) underlying continuous dimension(s). If that's not the case, both ideas may be misleading, and you might want to say a little more about the nature of your data to help inform future answers (or edits to my own).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-31T11:09:07.033" Id="80927" LastActivityDate="2013-12-31T11:09:07.033" OwnerUserId="32036" ParentId="80921" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;The difference is due to the fact that Stata's &lt;code&gt;sampsi&lt;/code&gt; command (deprecated as of Stata 13 and replaced by &lt;code&gt;power&lt;/code&gt;) uses the continuity correction by default, whereas R's &lt;code&gt;power.prop.test()&lt;/code&gt; does not (for details on the formula used by Stata, see &lt;a href=&quot;http://www.stata.com/manuals13/psspowertwoproportions.pdf#psspowertwoproportions&quot;&gt;[PSS] power twoproportions&lt;/a&gt;).  This can be changed with the &lt;code&gt;nocontinuity&lt;/code&gt; option, e.g.,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sampsi 0.70 0.85, power(0.90) alpha(0.05) nocontinuity&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which yields a sample size of 161 per group.  Use of the continuity correction yields a more conservative test (i.e., larger sample size), and obviously matters less as the sample size increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Frank Harrell, in the documentation for &lt;code&gt;bpower&lt;/code&gt; (part of his &lt;a href=&quot;http://cran.r-project.org/web/packages/Hmisc/index.html&quot;&gt;Hmisc&lt;/a&gt; package), points out that the formula without the continuity correction is pretty accurate, thereby providing some justification for forgoing the correction.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-12-31T17:25:27.223" Id="80942" LastActivityDate="2013-12-31T17:25:27.223" OwnerUserId="36755" ParentId="80808" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;this is a partial answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all you may find a similar question here : &lt;a href=&quot;http://mathoverflow.net/questions/22081/how-to-compare-two-cluster-solutions&quot;&gt;http://mathoverflow.net/questions/22081/how-to-compare-two-cluster-solutions&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I do not know much about weka but what you could do is comparing clustering methods using silhouette coefficient, inertia or gap_statistic (Tibshirani 2001, may cost a bit to compute, can find the article on google). These coefficients have the advantage not to require the knowledge of the ground truth classes (contrary to adjusted rand-index for example). &lt;/p&gt;&#10;&#10;&lt;p&gt;For kmeans and hierarchical clustering, you can choose the number of cluster k by looking at the maximum of silhouette coefficient, an elbow in inertia plot vs number of cluster or the maximum gap statistic. I do not know what to do for DBSCAN, maybe silhouette is still relevant ? Inertia and gap statistic will not be relevant because they assume kind of &quot;round&quot; clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a way, to compare DBSCAN and the two others, you have to get a sense of your data distributions. Kmeans and hierarchical will perform well if the underlying clusters are convex whereas dbscan can find clusters of any shape. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this is useful. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-12-31T18:09:12.583" Id="80944" LastActivityDate="2013-12-31T18:09:12.583" OwnerUserId="35572" ParentId="80943" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Is there anyway that I can perform LASSO with Negative Binomial Regression on R?&#10;I am performing a negative binomial regression on my dataset because the data are too dispersed to impose poisson regression. Meanwhile, I am also facing some multicollinearity problem. I already tried using &lt;code&gt;glmnet&lt;/code&gt; with &lt;code&gt;family = poisson&lt;/code&gt;, but the data is not fitting very well (for both alpha = 0 and alpha = 1)...I honestly don't know what to do to analyze this big mess of data :/&lt;/p&gt;&#10;&#10;&lt;p&gt;thank you&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: here is variance-covariance table of the negative binomial fit&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       8.392729e+18  1.239178e+06  -3.624090e+05  1.896258e+17  -3.702521e+17&#10;       1.239178e+06  1.119052e-04   5.201989e-06 -1.877590e+05  -2.558095e+05&#10;      -3.624090e+05  5.201989e-06   5.179343e-06 -8.021543e+04  -1.436381e+05&#10;       1.896258e+17 -1.877590e+05  -8.021543e+04  2.193290e+17   6.413947e+16&#10;      -3.702521e+17 -2.558095e+05  -1.436381e+05  6.413947e+16   2.142183e+17&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2013-12-31T22:09:27.413" Id="80955" LastActivityDate="2014-01-01T00:41:55.457" LastEditDate="2014-01-01T00:41:55.457" LastEditorUserId="35176" OwnerUserId="35176" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;regression&gt;&lt;generalized-linear-model&gt;" Title="Lasso on Negative Binomial Regression Model" ViewCount="283" />
  <row Body="&lt;p&gt;You might consider &lt;a href=&quot;https://en.wikipedia.org/wiki/Levene%27s_test&quot; rel=&quot;nofollow&quot;&gt;Levene's test&lt;/a&gt;, and &lt;a href=&quot;http://stats.stackexchange.com/q/15722/32036&quot;&gt;this discussion of how to use it in R&lt;/a&gt;. It may be less sensitive to non-normal distributions than Bartlett's test, according to the following reference. There's also the &lt;a href=&quot;https://en.wikipedia.org/wiki/Brown%E2%80%93Forsythe_test&quot; rel=&quot;nofollow&quot;&gt;Brown-Forsythe test&lt;/a&gt; for ANOVAs on transformed response variables, though this may be another version of Levene's test, judging from &lt;a href=&quot;http://stats.stackexchange.com/q/6477/32036&quot;&gt;this question&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds like your independent variable (IV) has discrete categories, but if this isn't the case, &lt;a href=&quot;http://stats.stackexchange.com/a/15471/32036&quot;&gt;this answer&lt;/a&gt; provides loads of good recommendations for testing homoscedasticity across a continuous dimensional IV.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Bartlett's test. &lt;em&gt;NIST/SEMATECH e-Handbook of Statistical Methods&lt;/em&gt;, 1.3.5.7. Available online, URL: &lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm&lt;/a&gt;. Retrieved December 31, 2013.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-01T03:45:15.610" Id="80965" LastActivityDate="2014-01-01T19:50:49.390" LastEditDate="2014-01-01T19:50:49.390" LastEditorUserId="32036" OwnerUserId="32036" ParentId="76973" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;This &lt;a href=&quot;http://stats.stackexchange.com/questions/31985/definition-and-origin-of-cross-entropy&quot;&gt;question&lt;/a&gt; gives a quantitative definition of cross entropy,&#10;in terms of it's formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for a more notional definition,&#10;wikipedia says: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In information theory, the cross entropy between two probability&#10;  distributions measures the average number of bits needed to identify&#10;  an event from a set of possibilities, &lt;em&gt;if a coding scheme is used based&#10;  on a given probability distribution q, rather than the &quot;true&quot;&#10;  distribution p.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I have Emphasised the part that is giving me trouble in understanding this.&#10;I would like a nice definition that doesn't require separate (pre-existing) understanding of Entropy.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-01T05:24:12.653" FavoriteCount="1" Id="80967" LastActivityDate="2014-02-23T15:23:54.493" OwnerUserId="36769" PostTypeId="1" Score="2" Tags="&lt;entropy&gt;&lt;information-theory&gt;" Title="Qualitively what is Cross Entropy" ViewCount="108" />
  <row AnswerCount="2" Body="&lt;p&gt;For a stats question the data was not normally distributed but the question required a two way ANOVA, a transformation was therefore used and all worked out fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the next part requires the data set to be split by one of the nominal variables (in SPSS) and a t-test to be run.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously the transformed data is normally distributed and therefore a t-test is applicable, however the original data (not normally distributed) could also be applicable using a non-parametric test. Which one would be the best to use and why? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-12-31T23:50:36.387" FavoriteCount="1" Id="80969" LastActivityDate="2014-02-28T01:36:08.637" LastEditDate="2014-02-28T01:36:08.637" LastEditorUserId="805" OwnerDisplayName="Pink Teddy" OwnerUserId="36778" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;nonparametric&gt;&lt;parametric&gt;" Title="After a transformation of data do I carry out a t-test or a non-parametric" ViewCount="93" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to solve the &#10;stochastic differential equation $$\mathrm dX(t)=X(t)^2 \mathrm dt+X(t)\mathrm dB(t)$$&#10;with condition $X(0)=1$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-01-01T10:35:32.913" FavoriteCount="2" Id="80975" LastActivityDate="2014-01-01T23:43:36.220" LastEditDate="2014-01-01T23:43:36.220" LastEditorUserId="2970" OwnerUserId="34694" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;stochastic-processes&gt;" Title="How to solve $\mathrm dX(t)=X(t)^2 \mathrm dt+X(t)\mathrm dB(t)$ with condition $X(0)=1$?" ViewCount="135" />
  <row AcceptedAnswerId="81078" AnswerCount="1" Body="&lt;p&gt;Show that $M(t)=e^{\frac{t}{2}}\sin(B(t))$ is a martingale by using Ito s formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;$B(t)$ is brownian-motion.&lt;/p&gt;&#10;&#10;&lt;p&gt;i must show $s \le t  $ then $E(M(t)|\mathcal F_{s})=M(s)$ but i dont know how use ito formula&lt;br&gt;&#10; thanks for help&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-01T17:32:28.850" FavoriteCount="3" Id="80995" LastActivityDate="2014-01-03T00:47:59.927" LastEditDate="2014-01-02T07:49:59.800" LastEditorUserId="32089" OwnerUserId="32089" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;stochastic-processes&gt;" Title="I want to show $M(t)=e^{\frac{t}{2}}\sin(B(t))$ is a martingale by using Ito s formula" ViewCount="239" />
  
  <row Body="&lt;p&gt;The OLS estimator in the linear regression model is quite rare in &#10;having the property that it can be represented in closed form, that is&#10;without needing to be expressed as the optimizer of a function. It is, however, an optimizer of a function -- the residual sum of squares &#10;function -- and can be computed as such.&lt;/p&gt;&#10;&#10;&lt;p&gt;The MLE in the logistic regression model is also the optimizer of a &#10;suitably defined log-likelihood function, but since it is not available&#10;in a closed form expression, it must be computed as an optimizer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most statistical estimators are only expressible as optimizers of&#10;appropriately constructed functions of the data called criterion functions.&#10;Such optimizers require the use of appropriate numerical optimization &#10;algorithms. &#10;Optimizers of functions can be computed in R using the &lt;code&gt;optim()&lt;/code&gt; function &#10;that provides some general purpose optimization algorithms, or one of the more&#10;specialized packages such as &lt;a href=&quot;http://cran.r-project.org/web/packages/optimx/index.html&quot;&gt;&lt;code&gt;optimx&lt;/code&gt;&lt;/a&gt;. Knowing which &#10;optimization algorithm to use for different types of models and statistical criterion &#10;functions is key.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Linear regression residual sum of squares&lt;/h2&gt;&#10;&#10;&lt;p&gt;The OLS estimator is defined as the optimizer of the well-known residual sum of&#10;squares function:&#10;$$&#10;\begin{align}&#10;\hat{\boldsymbol{\beta}} &amp;amp;= \arg\min_{\boldsymbol{\beta}}\left(\boldsymbol{Y} - \mathbf{X}\boldsymbol{\beta}\right)'\left(\boldsymbol{Y} - \mathbf{X}\boldsymbol{\beta}\right) \\&#10;&amp;amp;= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{Y}&#10;\end{align}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of a twice differentiable, convex function like the residual sum of squares, &#10;most gradient-based optimizers do good job. In this case, I will be using the BFGS&#10;algorithm.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#================================================&#10;# reading in the data &amp;amp; pre-processing&#10;#================================================&#10;urlSheatherData = &quot;http://www.stat.tamu.edu/~sheather/book/docs/datasets/MichelinNY.csv&quot;&#10;dfSheather = as.data.frame(read.csv(urlSheatherData, header = TRUE))&#10;&#10;# create the design matrices&#10;vY = as.matrix(dfSheather['InMichelin'])&#10;mX = as.matrix(dfSheather[c('Service','Decor', 'Food', 'Price')])&#10;&#10;# add an intercept to the predictor variables&#10;mX = cbind(1, mX)&#10;&#10;# the number of variables and observations&#10;iK = ncol(mX)&#10;iN = nrow(mX)&#10;&#10;#================================================&#10;# compute the linear regression parameters as &#10;#   an optimal value&#10;#================================================&#10;# the residual sum of squares criterion function&#10;fnRSS = function(vBeta, vY, mX) {&#10;  return(sum((vY - mX %*% vBeta)^2))&#10;}&#10;&#10;# arbitrary starting values&#10;vBeta0 = rep(0, ncol(mX))&#10;&#10;# minimise the RSS function to get the parameter estimates&#10;optimLinReg = optim(vBeta0, fnRSS,&#10;                   mX = mX, vY = vY, method = 'BFGS', &#10;                   hessian=TRUE)&#10;&#10;#================================================&#10;# compare to the LM function&#10;#================================================&#10;linregSheather = lm(InMichelin ~ Service + Decor + Food + Price,&#10;                    data = dfSheather)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This yields:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; print(cbind(coef(linregSheather), optimLinReg$par))&#10;                    [,1]         [,2]&#10;(Intercept) -1.492092490 -1.492093965&#10;Service     -0.011176619 -0.011176583&#10;Decor        0.044193000  0.044193023&#10;Food         0.057733737  0.057733770&#10;Price        0.001797941  0.001797934&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h2&gt;Logistic regression log-likelihood&lt;/h2&gt;&#10;&#10;&lt;p&gt;The criterion function corresponding to the MLE in the logistic regression model is&#10;the log-likelihood function.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{align}&#10;\log L_n(\boldsymbol{\beta}) &amp;amp;= \sum_{i=1}^n \left(Y_i \log \Lambda(\boldsymbol{X}_i'\boldsymbol{\beta}) + &#10; (1-Y_i)\log(1 - \Lambda(\boldsymbol{X}_i'\boldsymbol{\beta}))\right)  &#10;\end{align}&#10;$$&#10;where $\Lambda(k) = 1/(1+ \exp(-k))$ is the logistic function. The parameter estimates are the optimizers of this function&#10;$$&#10;\hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}}\log L_n(\boldsymbol{\beta})&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I show how to construct and optimize the criterion function using the &lt;code&gt;optim()&lt;/code&gt; function&#10;once again employing the BFGS algorithm.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#================================================&#10;# compute the logistic regression parameters as &#10;#   an optimal value&#10;#================================================&#10;# define the logistic transformation&#10;logit = function(mX, vBeta) {&#10;  return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )&#10;}&#10;&#10;# stable parametrisation of the log-likelihood function&#10;# Note: The negative of the log-likelihood is being returned, since we will be&#10;# /minimising/ the function.&#10;logLikelihoodLogitStable = function(vBeta, mX, vY) {&#10;  return(-sum(&#10;    vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))&#10;    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))&#10;    ) &#10;  ) &#10;}&#10;&#10;# initial set of parameters&#10;vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01) # arbitrary starting parameters&#10;&#10;# minimise the (negative) log-likelihood to get the logit fit&#10;optimLogit = optim(vBeta0, logLikelihoodLogitStable,&#10;                   mX = mX, vY = vY, method = 'BFGS', &#10;                   hessian=TRUE)&#10;&#10;#================================================&#10;# test against the implementation in R&#10;# NOTE glm uses IRWLS: &#10;# http://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares&#10;# rather than the BFGS algorithm that we have reported&#10;#================================================&#10;logitSheather = glm(InMichelin ~ Service + Decor + Food + Price,&#10;                                  data = dfSheather, &#10;                         family = binomial, x = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This yields &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; print(cbind(coef(logitSheather), optimLogit$par))&#10;                    [,1]         [,2]&#10;(Intercept) -11.19745057 -11.19661798&#10;Service      -0.19242411  -0.19249119&#10;Decor         0.09997273   0.09992445&#10;Food          0.40484706   0.40483753&#10;Price         0.09171953   0.09175369&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As a caveat, note that numerical optimization algorithms require careful use or you can end up with&#10;all sorts of pathological solutions. Until you understand them well, it is best to &#10;use the available packaged options that allow you to concentrate on specifying the model&#10;rather than worrying about how to numerically compute the estimates.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-01T21:16:04.977" Id="81004" LastActivityDate="2014-01-10T19:57:45.873" LastEditDate="2014-01-10T19:57:45.873" LastEditorUserId="8141" OwnerUserId="8141" ParentId="81000" PostTypeId="2" Score="16" />
  <row Body="&lt;p&gt;Maybe I'm missing something, but this question seems to be easier than it, at first, appears to be. It's not a change of variable problem (which would be messy) but simply a change in labeling with a change of scale. $t$ is an index, not a random variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;A process is Gaussian if every random subset of variables from the index set has a multivariate Gaussian distribution. Take a collection of variables with indeces $t_1,t_2, \ldots, t_k$. They are jointly Brownian (modulo the scale change), and so jointly multivariate normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Every random variable from a Brownian process has mean 0, so the process defined here has mean 0 everywhere.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, to the covariance function. Let's look at the variance first. Call the new process $X(t)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variance of the Brownian at $t$ is $t$. The variance of $X(t)$ is $e^{-2\alpha t} e^{2 \alpha t}$, which is 1. That's the point of the scaling factor, clearly.&lt;/p&gt;&#10;&#10;&lt;p&gt;The covariance of the Brownian at $t$ and $s$ is $\text{min}(s,t)$, so the covariance of $X(t)$ and $X(s)$ will be &lt;/p&gt;&#10;&#10;&lt;p&gt;$e^{-\alpha(s+t)} \text{min}(e^{2\alpha t}, e^{2 \alpha s})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if $s &amp;lt; t$, the covariance will be $e^{- \alpha (t-s)}$. Which is kinda cool.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-01T23:05:03.357" Id="81008" LastActivityDate="2014-01-01T23:05:03.357" OwnerUserId="14188" ParentId="78087" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;I'm not sure if ARIMA would be able to decompose the data into interprettable/decomposed time series. &lt;/p&gt;&#10;&#10;&lt;p&gt;I used &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_ucm_sect006.htm&quot; rel=&quot;nofollow&quot;&gt;Proc UCM&lt;/a&gt; in SAS which is a state space model. Following is the code of a basic structural model. The data is decomposed into Trend (level+slope) + Seasonal and White noise (Irregular). There is not much white noise left after you model. I'm assuming this is what you were looking for. The model fits the data like a glove. If you have access to SAS, you can try running the following code, you can get the decomposed series.&lt;/p&gt;&#10;&#10;&lt;p&gt;Level + Slope+ Seasonal + Irregular (not Shown).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1dyFK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/j23yi.png&quot; alt=&quot;Forecast Output which is comination of level+slope+season:&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UglD2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also try &lt;code&gt;STL decomposition&lt;/code&gt; in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;ods graphics on;&#10;       proc ucm data = data;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      id date interval = second; &#10;      model value;&#10;      irregular plot = smooth;&#10;      level ;&#10;      slope variance = 0 noest;&#10;      season length = 80 ;&#10;       dep lags = 2;&#10;      forecast plot=(forecasts decomp) lead = 80 outfor=forecast; &#10;   run ; &#10;ods graphics off ;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Hope this is helpful.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-01-02T21:23:35.230" Id="81077" LastActivityDate="2014-01-02T22:52:11.527" LastEditDate="2014-01-02T22:52:11.527" LastEditorUserId="29137" OwnerUserId="29137" ParentId="80710" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;$$E(X \mid X\le t) = \int_{-\infty}^tx\frac {f_X(x)}{F_X(t)}dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;is the correct expression, and it is the &quot;truncated expected value&quot; which is a shorthand for &quot;expected value of a random variable whose support is truncated&quot;.   &lt;/p&gt;&#10;&#10;&lt;p&gt;The integral without the scaling by $F_X(x)$ (the cdf of $X$) can appear in the following situation: define the random variable&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y = X\cdot \mathbf 1_{\{X\le t\}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathbf 1_{\{X\le t\}}$ is the indicator function taking the value $1$ when $X\le t$, zero otherwise. So $Y$ equals $X$ if $X\le t$, and it equals $0$ otherwise (it is a &quot;censored&quot; version of $X$). We can write&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(Y) = E(Y \mid X\le t) \cdot P(X\le t) + E(Y \mid X&amp;gt; t) \cdot P(X&amp;gt; t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E(X\cdot \mathbf 1_{\{X\le t\}} \mid X\le t) \cdot P(X\le t) + E(X\cdot \mathbf 1_{\{X\le t\}} \mid X&amp;gt; t) \cdot P(X&amp;gt; t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$= E(X \mid X\le t) \cdot P(X\le t) + E(X\cdot 0 \mid X&amp;gt; t) \cdot P(X&amp;gt; t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$= \int_{-\infty}^tx\frac {f_X(x)}{F_X(t)}dx\cdot F_X(t) +0 = \int_{-\infty}^tx f_X(x)dx$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-03T00:21:13.587" Id="81084" LastActivityDate="2014-01-03T01:45:33.770" LastEditDate="2014-01-03T01:45:33.770" LastEditorUserId="28746" OwnerUserId="28746" ParentId="81082" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&quot;Conditional mean&quot; or &quot;constrained mean&quot; are two variations I have seen. A more general version is &quot;conditional moment.&quot;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-03T00:52:03.217" Id="81085" LastActivityDate="2014-01-03T00:52:03.217" OwnerUserId="24073" ParentId="81082" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Reliability has to with being able to replicate results. So if you apply the same measurment technique multiple times in similar situations you should get similar results. An unreliable measurement adds a lot of random noise to your measurement.&lt;/p&gt;&#10;&#10;&lt;p&gt;Validity has to do with measuring what you want to measure. This is a much more theoretical concept: if this is a survey you just need to think about what the questions are, how a respondent might interpret that question (and the possible answer categories), the theoretical concept you want to measure, and whether these all match up.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also see &lt;a href=&quot;http://en.wikipedia.org/wiki/Reliability_%28psychometrics%29&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Validity_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-01-03T09:06:21.267" Id="81104" LastActivityDate="2014-01-03T09:06:21.267" OwnerUserId="23853" ParentId="81098" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The basic function of regression analyses is to estimate the strength of predictive associations between predictor (independent) variables and target (dependent) variables, usually both in terms of individual predictors and the total set of predictors (when there is more than one, as you've implied is relevant to your question's motivation). Much like a bivariate correlation, these predictive associations are essentially matters of effect size first, and can only become matters of significance by asking a question about these estimates, such as, &quot;Do I think that more than 5% of all the other random samples like mine could exhibit predictive associations as strong or stronger than the one present in my sample, yet in fact be drawn from a population in which there is no association overall?&quot; This is the question a typical null hypothesis significance test aims to answer about a regression model or regression coefficient. It is a question about an association that doesn't inherently need to involve matters of significance; one can ask it about any point estimate of a regression coefficient regardless of what data is used in producing the estimate, let alone whether that estimate is different from zero at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;@ExpectoPatronum's comment is false. Even the correlation of completely uncorrelated data can be calculated ($r = 0$). If someone sets out to test &quot;the significance&quot; of this correlation, one probably means to test the two-tailed significance of its difference from zero. Since $r = 0, t_{(N-2)} = 0, p = 1$, but this doesn't stop one from using the relevant formulas to &quot;calculate&quot; these results.  The same is true in linear regression. If all $\beta = 0$, then the model's $R^2 = 0$, the omnibus $F_{(N-k-1)} = 0, p =1$, and all individual $t_{(N-k-1)}=0, p=1$. Totally &quot;insignificant&quot; results in the common sense of being identical to zero, but still a completely possible result of the analysis. If this result weren't a possible outcome of any significance test, one couldn't really claim to be falsifying the typical null hypothesis with it. Regardless, the significance tests aren't really necessary to consider when estimating $\beta$ or $R^2$, which are the only definitional products of bare-bones regression analysis. One can consider the significance of one's estimates of these parameters after calculating them if desired, but the estimation process is the more basic statistical procedure that has to precede the usual significance question, not the other way around.&lt;/p&gt;&#10;&#10;&lt;p&gt;To be fair, a more interesting interpretation of &quot;significance&quot; in this question might be with respect to a prior significance test of the data's distributional normality or homoscedasticity, but I assume these aren't the significance tests you have in mind. You seem to be worried about the weakness or uncertainty of causal(?) associations. At no extreme do these qualities of data utterly preclude regression analysis; they do not affect one's basic ability to perform the analysis at all. The validity of regression results is a different matter, however.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-03T12:53:01.583" Id="81119" LastActivityDate="2014-01-03T12:53:01.583" OwnerUserId="32036" ParentId="81102" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm essentially learning about Latent Dirichlet Allocation. I'm watching a video here: &lt;a href=&quot;http://videolectures.net/mlss09uk_blei_tm/&quot; rel=&quot;nofollow&quot;&gt;http://videolectures.net/mlss09uk_blei_tm/&lt;/a&gt; and stuck at minute 45 when he started to explain on sampling from the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I tried to consult a machine learning book that doesn't have a detailed introductory on Dirichelt distribution. In the book I'm reading it mentioned an example on sampling &quot;probability vectors&quot; from the Dirichlet distribution, but what does that mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand sampling from a distribution as getting random values for the random variables according to the distribution. So let p_X,Y(x,y) but the pmf of any distribution, sampling from this distribuiton means I get a random (x,y) (i.e. random values for x and y). To get the probablity of the getting the event (X=x AND Y=y) we evalute the pmf of the distribution ... so we get only one number. But what are &quot;probability vectors&quot; here!!&lt;/p&gt;&#10;&#10;&lt;p&gt;I attached a screenshot for the book. I really hope you can help! &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y9g7q.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-03T13:24:58.490" Id="81123" LastActivityDate="2014-01-03T14:11:33.983" LastEditDate="2014-01-03T13:47:33.427" LastEditorUserId="27779" OwnerUserId="27779" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;sampling&gt;&lt;dirichlet-distribution&gt;" Title="What does it mean to sample a probability vector from a Dirichlet distribution?" ViewCount="164" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I searched on the net, but I could not find any really good answer to my question which is very easy.&lt;/p&gt;&#10;&#10;&lt;p&gt;A linear classifier is a classifier which can classify the samples correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;But Given a function, like for example an f that uses sigmoid, and classifies to girl/boy for example if the result of sigmoid is less or more than 0.5, how do we know if f is actually a linear function?&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, given a function (and not the data), how we are able to know if that function is a linear classifier or not? Should we prove something? Are there any references/bibliography that can help me understand better? Any other example that can help me better?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-03T15:16:46.023" Id="81134" LastActivityDate="2014-01-03T15:16:46.023" OwnerUserId="36829" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;linear-model&gt;&lt;nonlinear-regression&gt;" Title="How do we know that a classifier is linear or not?" ViewCount="42" />
  
  <row AcceptedAnswerId="81179" AnswerCount="1" Body="&lt;p&gt;Let $X$ and $Y$ be two independent random variables with respective pdfs:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f \left(x;\theta_i \right) =\begin{cases} \frac{1}{\theta_i} e^{-x/ {\theta_i}} \quad 0&amp;lt;x&amp;lt;\infty, 0&amp;lt;\theta_i&amp;lt; \infty \\ 0 \quad \text{elsewhere} \end{cases} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $i=1,2$. Two indepedent samples are drawn in order to test $H_0: \theta_1 =\theta_2 $ against $H_1 : \theta_1 \neq \theta_2 $ of sizes $n_1$ and $n_2$ from these distributions. I need to show that the LRT $\Lambda$ can be written as a function of a statistic having $F$ distribution, under $H_0$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Since the mle of this distribution is $\hat{\theta}=\bar{x} $, the LRT statistic becomes (I am skipping a few tedious steps here):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \Lambda =\frac{\bar{x}^{n_1} \bar{y}^{n_2} \left( n_1+n_2 \right)}{n_1 \bar{x}+n_2 \bar{y}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the $F$ distribution is defined as the quotient of two independent chi-square random variables, each one over their respective degrees of freedom. Additionally, since $X_i,Y_i \sim \Gamma \left( 1,\theta_1 \right)$ under the null, then $\sum X_i \sim \Gamma \left(n_1 ,\theta_1 \right)$ and$\sum Y_i \sim \Gamma \left(n_2, \theta_1 \right) $. &lt;/p&gt;&#10;&#10;&lt;p&gt;But how can I proceed from here? Any hints?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-01-03T18:11:41.323" Id="81151" LastActivityDate="2014-01-04T03:22:29.537" LastEditDate="2014-01-03T19:48:23.760" LastEditorUserId="31420" OwnerUserId="31420" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;likelihood-ratio&gt;" Title="Likelihood Ratio for two-sample Exponential distribution" ViewCount="1221" />
  <row Body="&lt;p&gt;In ecological research, nonrandom assignment of treatments to experimental units (subjects) is standard practice when sample sizes are small and there is evidence of one or more confounding variables.  This nonrandom assignment &quot;intersperses&quot; the subjects across the spectrum of possibly confounding variables, which is exactly what random assignment is supposed to do.  But at small sample sizes, randomization is more likely to perform poorly at this (as demonstrated above) and therefore it can be a bad idea to rely on it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because randomization is advocated so strongly in most fields (and rightfully so), it is easy to forget that the end goal is to reduce bias rather than to adhere to strict randomization.  However, it is incumbent upon the researcher(s) to characterize the suite of confounding variables effectively and to carry out the nonrandom assignment in a defensible way that is blind to experimental outcomes and makes use of all available information and context.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a summary, see pp. 192-198 in &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CDEQFjAA&amp;amp;url=http://www.masterenbiodiversidad.org/docs/asig3/Hurlbert_1984_Pseudoreplication.pdf&amp;amp;ei=Q_vGUq_wFMrkyAHPjIHoCw&amp;amp;usg=AFQjCNGJdwhqWvmN8C00lsanaYPY-lht4g&amp;amp;sig2=mgWZPkPcwHtjYXbd4AbLhw&amp;amp;bvm=bv.58187178,d.aWc&quot; rel=&quot;nofollow&quot;&gt;Hurlbert, Stuart H. 1984. Pseudoreplication and the design of field experiments.  Ecological Monographs 54(2) pp.187-211.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-01-03T18:38:45.973" Id="81152" LastActivityDate="2014-01-03T18:38:45.973" OwnerUserId="36835" ParentId="74350" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Yes you can. The method you are looking for is called exponentially weighted least squares method. It is a variation on the recursive least squares method:&#10;\begin{align}&#10;Θ ̂(k+1)&amp;amp;=Θ ̂(k)+K[z(k+1)-x^T (k+1)  Θ ̂(k)]  \\&#10;K(k+1) &amp;amp;= D(k)  x(k+1) [λ+x^T (k+1)D(k)x(k+1)]^(-1)  \\&#10;D(k+1) &amp;amp;=\frac 1 λ \bigg(D(k)-D(k)x(k+1)\bigg[λ+x^T (k+1)D(k)x(k+1)\bigg]^{-1} x^T (k+1)D(k)\bigg)&#10;\end{align}&#10;$0.9&amp;lt;λ&amp;lt;1$ typically.&lt;/p&gt;&#10;&#10;&lt;p&gt;Its a method developed to account for time varying parameters but are still in a linear format. which comes from the cost function:&#10;$$J(Θ)=1/2 ∑_(i=k-m)^k▒〖λ^(k-i)  [z(i)-x^T (i)Θ]〗^2 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Ordinary Least squares is calculated from the following for comparison:&lt;/p&gt;&#10;&#10;&lt;p&gt;the cost function being:&#10;$$J(Θ)=1/2 ∑_(i=i)^k▒[z(i)-x^T (i)Θ]^2 $$&#10;with &#10;\begin{align}&#10;Θ(k)       &amp;amp;= D(k) X_k^T Z_k     \\&#10;Cov[Θ ̂(k)] &amp;amp;= σ^2 D(k)           \\&#10;D(k)       &amp;amp;= [X_k^T X_k ]^{-1}&#10;\end{align}&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-03T21:41:08.003" Id="81159" LastActivityDate="2014-01-03T22:13:03.320" LastEditDate="2014-01-03T22:13:03.320" LastEditorUserId="7290" OwnerUserId="36840" ParentId="9931" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This statistic is common in shooting such as a rifle. The widest distance between shots is known as the &quot;group size.&quot; There is no closed form for the statistic. Thus the paper you referenced by Taylor and Grubbs uses Monte-Carlo techniques to estimate the parameters. In real life for shooting that is really &quot;good enough.&quot; Nobody shoots 1,000 groups of 10 shots per group. So the real world sampling in shooting is poor. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-04T08:25:43.193" Id="81185" LastActivityDate="2014-01-04T08:25:43.193" OwnerUserId="36850" ParentId="76564" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;This approach reminds me a lot of &lt;a href=&quot;https://en.wikipedia.org/wiki/Synthetic_Aperture_Personality_Assessment&quot; rel=&quot;nofollow&quot;&gt;Synthetic Aperture Personality Assessment (SAPA)&lt;/a&gt;. You may want to read further into it to see what you can learn about the method and judge how closely it resembles what you have in mind. If it's sufficiently equivalent for your purposes, the news appears to be good. &lt;a href=&quot;https://personality-project.org/r/html/00.psych-package.html&quot; rel=&quot;nofollow&quot;&gt;This page&lt;/a&gt; says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;5) For some applications, data matrices are synthetically combined from sampling different items for different people. So called Synthetic Aperture Personality Assessement (SAPA) techniques allow the formation of large correlation or covariance matrices even though no one person has taken all of the items. To analyze such data sets, it is easy to form item composites based upon the covariance matrix of the items, rather than original data set. These matrices may then be analyzed using a number of functions (e.g., cluster.cor, factor.pa, ICLUST, principal , mat.regress, and factor2cluster.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-01-04T12:34:50.073" Id="81199" LastActivityDate="2014-01-04T12:34:50.073" OwnerUserId="32036" ParentId="81178" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="81211" AnswerCount="1" Body="&lt;p&gt;I'm having trouble understanding an equality  given in a book (&quot;Speckle Phenomena in Optics&quot; by Joseph Goodman p.145) for a zero mean, stationary Gaussian process:&#10;$\overline{\exp(i [\phi(x_1)-\phi(x_2)])}=\exp(-\sigma^2[1-\mu(x_1-x_2)])$&#10;where $\sigma^2$ is the variance of the process $\phi$ and $\mu$ is the normalized autocorrelation of the process.&#10;Could anyone derive this equality for me or give me a reference that explains it?&#10;The author says: &quot;from the relationship of the average to characteristic functions we have ...&quot;&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-04T13:56:57.117" Id="81206" LastActivityDate="2014-01-04T15:12:57.093" LastEditDate="2014-01-04T14:03:40.647" LastEditorUserId="36858" OwnerUserId="36858" PostTypeId="1" Score="0" Tags="&lt;stochastic-processes&gt;&lt;stationarity&gt;&lt;gaussian-process&gt;" Title="Relationship between average and characteristic function of a Gaussian process" ViewCount="129" />
  <row Body="&lt;p&gt;It is not a good idea to mess with the CPI calculations done by the BLS. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you need to remove the effect food prices, there are several alternative data set options: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;pick the data set with less food and energy&lt;/li&gt;&#10;&lt;li&gt;use the PPI instead where you do have more choices to pick from if you want to exclude food&lt;/li&gt;&#10;&lt;li&gt;email the BLS to tell them what you need and they may better direct you to an appropriate data set and/or sell you the appropriate data set&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-01-04T14:57:49.113" Id="81213" LastActivityDate="2014-01-04T14:57:49.113" OwnerUserId="36539" ParentId="74522" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think you will have to make some kind of assumption about the proportion of dark/light hair-color individuals in studies 3 and 4. Then you can treat the 'dark' color variable as a proportion that is equal to 0 in samples composed entirely of light hair-color individuals, 1 in samples composed entirely of dark hair-color individuals, and something in between when it is a mix (as in studies 3 and 4). For gender, you just set up a standard dummy variable. Then you can analyze these data with the &lt;code&gt;rma.mv&lt;/code&gt; function from the &lt;code&gt;metafor&lt;/code&gt; package (which allows fitting network-type meta-analytic models). Here is how this would be done, assuming that 50% of the individuals in studies 3 and 4 were dark hair-color individuals:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# dummy variable for gender and proportion of dark hair-color individuals&#10;my_data$male &amp;lt;- c(1,0,1,0,1,0,1,0,1,1,0,0,1,1,0)&#10;my_data$dark &amp;lt;- c(1,1,1,1,.5,.5,.5,.5,1,0,1,0,1,0,0)&#10;&#10;# compute sampling variances of the means&#10;my_data$vi &amp;lt;- my_data$std.dev^2 / my_data$sampleSize&#10;&#10;# need a variable to indicate the arm within each study &#10;my_data$arm &amp;lt;- sequence(table(my_data$study))&#10;&#10;# meta-analysis model using correlated random effects within each study&#10;res &amp;lt;- rma.mv(mean, vi, mods = ~ male*dark, random = list(~ arm | study), data=my_data)&#10;res&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output from this is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Multivariate Meta-Analysis Model (k = 15; method: REML)&#10;&#10;Variance Components: &#10;&#10;outer factor: study (nlvls = 7)&#10;inner factor: arm   (nlvls = 3)&#10;&#10;            estim    sqrt  fixed&#10;tau^2      6.8909  2.6250     no&#10;rho        0.3316             no&#10;&#10;Test for Residual Heterogeneity: &#10;QE(df = 11) = 1353.4541, p-val &amp;lt; .0001&#10;&#10;Test of Moderators (coefficient(s) 2,3,4): &#10;QM(df = 3) = 38.9377, p-val &amp;lt; .0001&#10;&#10;Model Results:&#10;&#10;           estimate      se     zval    pval     ci.lb    ci.ub     &#10;intrcpt     21.9548  1.5913  13.7965  &amp;lt;.0001   18.8359  25.0738  ***&#10;male        -9.0630  2.0448  -4.4322  &amp;lt;.0001  -13.0708  -5.0552  ***&#10;dark        -6.8660  2.1176  -3.2424  0.0012  -11.0164  -2.7156   **&#10;male:dark    4.9911  2.7210   1.8343  0.0666   -0.3419  10.3241    .&#10;&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Maybe there is an interaction, but strictly speaking, it is not significant at $\alpha = .05$ (two-sided). Actually, let's get the predicted means for the 'female-light', 'female-dark', 'male-light', and 'male-dark' combinations:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;predict(res, newmods=rbind(c(0,0,0), c(0,1,0), c(1,0,0), c(1,1,1)))&#10;&#10;     pred     se   ci.lb   ci.ub   cr.lb   cr.ub&#10;1 21.9548 1.5913 18.8359 25.0738 15.9383 27.9714&#10;2 15.0888 1.3667 12.4101 17.7676  9.2882 20.8894&#10;3 12.8918 1.5816  9.7919 15.9917  6.8851 18.8985&#10;4 11.0169 1.2026  8.6599 13.3739  5.3577 16.6761&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, for females, going from 'light' to 'dark' results in a pretty substantial decrease, but not so for males.&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, I made an assumption about the proportion of dark hair-color individuals in studies 3 and 4. My suggestion now would be to carry out a sensitivity analysis, varying those proportions within a sensible range (e.g., .3 to .7), checking whether the conclusions depend on the specific proportions assumed.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-01-04T20:01:07.137" Id="81230" LastActivityDate="2014-01-04T20:01:07.137" OwnerUserId="1934" ParentId="81218" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;code&gt;rmr&lt;/code&gt; is a &lt;code&gt;R&lt;/code&gt; package allows an &lt;code&gt;R&lt;/code&gt; programmer to perform statistical analysis via MapReduce on a Hadoop cluster. More details can be found &lt;a href=&quot;https://github.com/RevolutionAnalytics/RHadoop/wiki/rmr&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-04T20:16:42.427" Id="81231" LastActivityDate="2014-01-04T20:19:46.340" LastEditDate="2014-01-04T20:19:46.340" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="5" Score="0" />
  <row Body="rmr is a R package allows an R programmer to perform statistical analysis via MapReduce on a Hadoop cluster." CommentCount="0" CreationDate="2014-01-04T20:16:42.427" Id="81232" LastActivityDate="2014-01-04T20:19:49.043" LastEditDate="2014-01-04T20:19:49.043" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="4" Score="0" />
  
  <row Body="&lt;p&gt;Random-access memory (RAM) is a form of computer data storage. A random-access device allows stored data to be accessed directly in any random order. [Wikipedia]&lt;/p&gt;&#10;&#10;&lt;p&gt;On this site, tag &lt;a href=&quot;/questions/tagged/ram&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;ram&amp;#39;&quot; rel=&quot;tag&quot;&gt;ram&lt;/a&gt; is used in the discussion of computation issues.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-04T20:20:37.903" Id="81233" LastActivityDate="2014-01-04T20:30:16.553" LastEditDate="2014-01-04T20:30:16.553" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="5" Score="0" />
  <row Body="Random-access memory (RAM) is a form of computer data storage. A random-access device allows stored data to be accessed directly in any random order." CommentCount="0" CreationDate="2014-01-04T20:20:37.903" Id="81234" LastActivityDate="2014-01-04T20:30:11.020" LastEditDate="2014-01-04T20:30:11.020" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="4" Score="0" />
  
  
  
  <row Body="&lt;p&gt;Another interesting example of non-Poisson counting process is represented by the zero-truncated Poisson distribution (ZTPD). ZTPD can fit data concerning the number of languages subjects can speak in physiological conditions. In this instance, Poisson distribution is ill-behaving, because the number of spoken languages is by definition &gt;=1: hence 0 is ruled out a priori. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-06-16T18:42:37.393" CreationDate="2014-01-05T14:03:30.850" Id="81272" LastActivityDate="2014-01-05T14:03:30.850" OwnerUserId="25292" ParentId="37589" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The &lt;em&gt;joint&lt;/em&gt; density of $X$ and $\Theta$ is &#10;$$f_{X,\Theta}(x,\theta)= f_{X\mid \Theta}(x\mid \Theta=\theta)f_\Theta(\theta)&#10;= \begin{cases} \theta e^{-\theta x}, &amp;amp; 0 &amp;lt; x &amp;lt; \infty, 0 &amp;lt; \theta &amp;lt; 1,\\&#10;0, &amp;amp;\text{otherwise.}\end{cases}$$&#10;Thus, for $0 &amp;lt; x &amp;lt; \infty$, the &lt;em&gt;marginal&lt;/em&gt; density of $X$ is &#10;should be found by integrating the joint density of $X$ and $\Theta$ with&#10;respect to $\theta$ over&#10;the interval $(0,1)$ instead of $(0,\infty)$ the way you have it. Note that&#10;your purported density $\Gamma(n+1)y^{-(n+1)}$ is not a valid density function&#10;since it does not integrate to $1$ over $(0,\infty)$ even when $n=1$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-05T17:39:41.533" Id="81293" LastActivityDate="2014-01-05T17:39:41.533" OwnerUserId="6633" ParentId="81283" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you're looking for a book, I'd suggest you take a look at Gelman &amp;amp; Hill's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/052168689X&quot; rel=&quot;nofollow&quot;&gt;Data Analysis Using Regression and Multilevel/Hierarchical Models&lt;/a&gt; at Amazon to see if it suits your needs. (There are some additional resources at their &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/arm/&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt;.) It may not be what you want/need, but it's worth a look.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-05T18:56:20.093" Id="81297" LastActivityDate="2014-01-05T18:56:20.093" OwnerUserId="1764" ParentId="81295" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;A couple more points to consider:&lt;/p&gt;&#10;&#10;&lt;p&gt;As mentioned in a comment, an underfill is largely inappropriate if the x axis is not at a natural y zero point. This might be because the y axis is scaled to start at a number other than zero, or because the units used do not have a natural zero interpretation (e.g. Kelvin has a natural zero, while Celsius does not.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, a case when an underfill is particularly valid is if the data themselves could be considered underfilled. For example, a line chart of the height of a mountain makes sense to be underfilled, the fill colour represents earth, while unfilled represents air.&lt;/p&gt;&#10;&#10;&lt;p&gt;A related example might be count data. If we stacked all the individuals at each x point, we would get a bar chart. If interpolating between the bars makes sense we would end up with a line chart with an underfill. &lt;/p&gt;&#10;&#10;&lt;p&gt;This &lt;a href=&quot;http://www.evl.uic.edu/davidson/CurrentProjects98/ET_VisualInfo/VDQIpage141.gif&quot; rel=&quot;nofollow&quot;&gt;image&lt;/a&gt; from the 'visual display of quantitative information' Might explain it a little better. It shows which military units were in Europe during the second war (I think). Stacking the units at each time point gives you an underfilled bar chart. Drawing a line over the top of the data gives you an underfilled line chart.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-05T19:50:17.607" Id="81301" LastActivityDate="2014-01-05T19:50:17.607" OwnerUserId="25628" ParentId="81116" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;You have a few questions combined here.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Moderation. This is another term for interaction. To test for moderation you can include the interactions of self differentiation with the demographic variables. This presumes you have a large enough sample to avoid overfitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Hierarchical regression. This depends on what you are trying to find out. One idea is to first include the IV and the moderating variables, but not the interaction, then to include the interactions and see what effects are large and how the addition of the interactions changes things. This is part of the general problem of model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Type of sampling. This does not directly affect which type of regression you can or should use; it affects how you can genaralize your results to other populations. Depending on the type of purposive sampling, there may be worked-out methods for dealing with changes. But random sampling is simplest to deal with (not necessarily &lt;em&gt;best&lt;/em&gt; but simplest). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-05T22:02:43.737" Id="81313" LastActivityDate="2014-01-05T22:02:43.737" OwnerUserId="686" ParentId="81292" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;For some reason my supervisor wants me to centre only the independent variables that are used in interaction terms. I have never heard of such a practice. Does it make sense to partially centre data matrix $X$, that is, centering only some of the columns of data matrix $X$, not all of them?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T05:19:34.233" FavoriteCount="1" Id="81334" LastActivityDate="2014-01-06T07:55:40.430" LastEditDate="2014-01-06T05:23:13.000" LastEditorUserId="32036" OwnerUserId="35176" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;interaction&gt;&lt;centering&gt;" Title="Does it make sense to partially scale the data matrix X in regression?" ViewCount="57" />
  <row Body="&lt;p&gt;As mentioned, one way to solve this is through multivariable logistic regression. You can fit a model that predicts the probability of outcome (Status) from the other variables:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;fit &amp;lt;- lrm(Status ~ Age + Tenure + Function + ...)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There are several packages that can help:  &lt;code&gt;glm&lt;/code&gt; was already mentioned, and I like &lt;code&gt;rms&lt;/code&gt;. Check out the documentation in &lt;code&gt;rms&lt;/code&gt; because it has a detailed example for exactly your kind of problem, that you can work through. There are several books on logistic regression, including &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441929185&quot; rel=&quot;nofollow&quot;&gt;one&lt;/a&gt; by &lt;code&gt;rms&lt;/code&gt;'s author. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T06:13:45.643" Id="81340" LastActivityDate="2014-01-06T06:13:45.643" OwnerUserId="20657" ParentId="81335" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I wanted to share another reference. It's a &quot;comment&quot; paper, and actually part of an argument between two economists, Smith and Campbell, and a statistician, Marquardt. But it's relevant.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1980.10477430#.UspHKcvNVn8&quot; rel=&quot;nofollow&quot;&gt;Link to the Marquardt JASA article&lt;/a&gt;. Sorry to refer you to a non open-access article, but I highly recommend the read if you get a chance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Marquardt is strongly arguing in favor of centering and scaling for various reasons, one being that you have prior knowledge about the absolute size of these variables (they shouldn't be much bigger than 3, based on reasoning using Chebychev's inequality), and this is a justification for &quot;shrinkage&quot; in techniques like ridge regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;And of course there's the interpretation issue as well. He draws a cool picture of a parabola fit to data. Almost all the data is on the right side of the parabola, so that it's almost linear in a positive direction. But the coefficient on the linear term is negative! (Remember there's a squared term in there.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now a parabola $ax^2 + bx + c$ is really just like having a term interact with itself. That same phenomenon would also happen with interactions, where the main effect would have a nonsense value because you never see the interacting value at zero. Of course, some people you shouldn't try to estimate main effects when there are interactions.&lt;/p&gt;&#10;&#10;&lt;p&gt;To sum it up, terms with interactions and polynomial terms are where you'll get the most out of your effort. If you're going to apply shrinkage or a Bayesian prior that says parameter estimates should be small, maybe you should standardize all your numeric variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T06:16:07.717" Id="81341" LastActivityDate="2014-01-06T07:55:40.430" LastEditDate="2014-01-06T07:55:40.430" LastEditorUserId="32036" OwnerUserId="35131" ParentId="81334" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row Body="&lt;p&gt;Models that involve complex polynomial functions or too many independent variables may fit particular samples' covariance structures overly well, such that some existing (and any potential, additional) terms increase model fit by modeling sampling error, not systematic covariance that is likely to replicate or represent theoretically useful relationships. When used to predict other data (e.g., future outcomes, out-of-sample data), overfitting increases prediction error.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot; rel=&quot;nofollow&quot;&gt;The Wikipedia page&lt;/a&gt; offers illustrations, lists of potential solutions, and special treatment of the topic as it relates to machine learning.  See also:&lt;/p&gt;&#10;&#10;&lt;p&gt;Leinweber, D. J. (2007). Stupid data miner tricks: Overfitting the S&amp;amp;P 500. &lt;em&gt;The Journal of Investing, 16&lt;/em&gt;(1), 15–22. Available online, URL: &lt;a href=&quot;http://www.finanzaonline.com/forum/attachments/econometria-e-modelli-di-trading-operativo/903701d1213616349-variazione-della-vix-e-rendimento-dello-s-p500-dataminejune_2000.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.finanzaonline.com/forum/attachments/econometria-e-modelli-di-trading-operativo/903701d1213616349-variazione-della-vix-e-rendimento-dello-s-p500-dataminejune_2000.pdf&lt;/a&gt;. Accessed January 6, 2014.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tetko, I. V., Livingstone, D. J., &amp;amp; Luik, A. I. (1995). Neural network studies. 1. Comparison of overfitting and overtraining. &lt;em&gt;J. Chem. Inf. Comput. Sci. 35&lt;/em&gt;(5), 826–833. doi:10.1021/ci00027a006.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T11:38:10.380" Id="81362" LastActivityDate="2014-01-06T11:46:56.493" LastEditDate="2014-01-06T11:46:56.493" LastEditorUserId="32036" OwnerUserId="32036" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;This is a logistic regression question. You can fit a model like&#10;&lt;code&gt;fit &amp;lt;- (status ~ age + tenure + function...)&lt;/code&gt;. This can be done with a variety of techniques, I recommend the package &lt;code&gt;rms&lt;/code&gt;. You may also want to review Frank Harrell's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441929185&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt;. You can validate, calibrate, plot, derive a formula, etc using the functions in the package. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T05:45:07.470" Id="81371" LastActivityDate="2014-01-06T05:45:07.470" OwnerDisplayName="koenbro" OwnerUserId="20657" ParentId="81369" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The main problem is that if you're going to use the ratio as your response variable, you should be using the &lt;code&gt;weights&lt;/code&gt; argument.  You must have ignored a warning about &quot;non-integer #successes in a binomial glm&quot; ...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Dilution &amp;lt;- c(1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4)&#10;NoofPlates &amp;lt;- rep(x=5, times=10)&#10;NoPositive &amp;lt;- c(0, 0, 2, 2, 3, 4, 5, 5, 5, 5)&#10;Data &amp;lt;- data.frame(Dilution,  NoofPlates, NoPositive)&#10;&#10;&#10;fm1 &amp;lt;- glm(formula=NoPositive/NoofPlates~log(Dilution),&#10;     family=binomial(&quot;logit&quot;), data=Data, weights=NoofPlates)&#10;&#10;coef(summary(fm1))&#10;##               Estimate Std. Error  z value     Pr(&amp;gt;|z|)&#10;## (Intercept)   4.173698  1.2522190 3.333042 0.0008590205&#10;## log(Dilution) 1.622552  0.4571016 3.549653 0.0003857398&#10;&#10;anova(fm1,test=&quot;Chisq&quot;)&#10;##               Df Deviance Resid. Df Resid. Dev  Pr(&amp;gt;Chi)    &#10;## NULL                              9     41.212              &#10;## log(Dilution)  1   37.979         8      3.233 7.151e-10 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this case (with a single parameter), &lt;code&gt;aod::wald.test()&lt;/code&gt; gives&#10;exactly the same p-value as &lt;code&gt;summary()&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Wald and LRT tests are still fairly different, as are the Wald&#10;vs profile confidence intervals --&lt;/p&gt;&#10;&#10;&lt;p&gt;Wald:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;confint.default(fm1)&#10;##                   2.5 %   97.5 %&#10;## (Intercept)   1.7193940 6.628002&#10;## log(Dilution) 0.7266493 2.518455&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Profile:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;confint(fm1)&#10;##                   2.5 %   97.5 %&#10;## (Intercept)   2.2009398 7.267565&#10;## log(Dilution) 0.9014053 2.757092&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T13:28:15.573" Id="81379" LastActivityDate="2014-01-06T13:28:15.573" OwnerUserId="2126" ParentId="81368" PostTypeId="2" Score="5" />
  
  
  
  
  
  <row Body="&lt;p&gt;AIC and BIC hold the same interpretation in terms of model comparison. That is, the larger difference in either AIC or BIC indicates stronger evidence for one model over the other (the lower the better). It's just the the AIC doesn't penalize the number of parameters as strongly as BIC. There is also a correction to the AIC (the AICc) that is used for smaller sample sizes. More information on the comparison of AIC/BIC can be found &lt;a href=&quot;http://en.wikipedia.org/wiki/Akaike_information_criterion&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-06T21:38:19.617" Id="81431" LastActivityDate="2014-01-06T21:38:19.617" OwnerUserId="21654" ParentId="81427" PostTypeId="2" Score="7" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like to know if there is a way to compare the relationship between a factor (length) and a response variable (lipid content) among multiple groups? I have given up on ANCOVA, as length is continuous. I am unsure if a multiple regression allows the use of a categorical variable (group membership) as an IV.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-06T22:25:32.657" Id="81434" LastActivityDate="2014-01-06T23:21:05.350" LastEditDate="2014-01-06T23:18:55.657" LastEditorUserId="32036" OwnerUserId="36943" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;categorical-data&gt;&lt;ancova&gt;&lt;group-differences&gt;&lt;continuous&gt;" Title="Compare regressions among more than 2 groups" ViewCount="27" />
  
  
  
  <row Body="&lt;p&gt;You are talking about two different things and you are mixing them up. In the first case you have two models (1 and 2) and you obtained their AIC like $AIC_1$ and $AIC_2$. IF you want to compare these two models based on their AIC's, then model with lower AIC would be the preferred one  i.e. if $AIC_1&amp;lt; AIC_2$ then you pick up model 1 and vise versa.&lt;br&gt;&#10; In the 2nd case, you have a set of candidate models like models $(1, 2, ..., n)$ and for each model you calculate AIC differences as $\Delta_i= AIC_i- AIC_{min}$, where $AIC_i$ is the AIC for the $i$th model and $AIC_{min}$ is the minimum of AIC among all the models. Now the model with $\Delta_i &amp;gt;10$ have no support and can be ommited from further consideration as explained in &lt;a href=&quot;http://books.google.ca/books?id=fT1Iu-h6E-oC&amp;amp;pg=PA51&amp;amp;dq=model%20selection%20and%20multi%20dimensional%20inference&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=dUXLUp-3N4fK2wWBlYHQCg&amp;amp;ved=0CC4Q6AEwAA#v=onepage&amp;amp;q=model%20selection%20and%20multi%20dimensional%20inference&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Model Selection and Multi-Model Inference: A Practical Information-Theoretic Approach&lt;/a&gt; by Kenneth P. Burnham, David R. Anderson, page 71. So the larger is the $\Delta_i$, the weaker would be your model. Here the best model has $\Delta_i\equiv\Delta_{min}\equiv0.$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-07T00:10:17.533" Id="81444" LastActivityDate="2014-01-07T00:16:54.900" LastEditDate="2014-01-07T00:16:54.900" LastEditorUserId="13138" OwnerUserId="13138" ParentId="81427" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;From &lt;a href=&quot;http://semanticvoid.com/blog/2007/02/23/similarity-measure-cosine-similarity-or-euclidean-distance-or-both/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bX8Kr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let us consider two documents A and B represented by the vectors in the above figure. The cosine treats both vectors as unit vectors by normalizing them, giving you a measure of the angle between the two vectors. It does provide an accurate measure of similarity but with no regard to magnitude. But magnitude is an important factor while considering similarity.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2014-01-07T12:00:38.630" Id="81484" LastActivityDate="2014-01-07T12:38:20.497" LastEditDate="2014-01-07T12:38:20.497" LastEditorUserId="32036" OwnerUserId="16049" ParentId="81481" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You may want to take a look at this question: &lt;a href=&quot;http://stats.stackexchange.com/q/2492/32036&quot;&gt;Is normality testing &amp;#39;essentially useless&amp;#39;?&lt;/a&gt; Answers discuss the Shapiro-Wilk test, particularly &lt;a href=&quot;http://stats.stackexchange.com/a/2498/32036&quot;&gt;the accepted answer&lt;/a&gt;, which includes a simulation test. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your problem may be different than most if you're not concerned with the distribution for the sake of meeting another planned analysis' assumptions though. Fitting a normal distribution to your data may only prompt you to ignore its peculiarities if they're small enough. If there isn't another analysis you need to perform that assumes normality, rather than trying to fit a known distribution to yours, you might consider describing your distribution in terms of its skewness and kurtosis, and add confidence intervals if you like (but consider &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/21298573&quot; rel=&quot;nofollow&quot;&gt;relevant precautions&lt;/a&gt; in doing so).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-07T13:04:49.950" Id="81491" LastActivityDate="2014-01-07T13:04:49.950" OwnerUserId="32036" ParentId="81477" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The issue is that the chi-square approximation to the distribution of the test statistic relies on the counts being roughly normally distributed. If many of the expected counts are very small, the approximation may be poor.&lt;/p&gt;&#10;&#10;&lt;p&gt;The noshow category will be a big contributor to the problem; one thing to consider is merging noshow and fail. You'll still get the warning but it won't affect the results nearly so much and the distribution should be quite reasonable (the rule that's being applied before the warning is given is too strict).&lt;/p&gt;&#10;&#10;&lt;p&gt;But in any case, you can deal with the problem &lt;em&gt;very&lt;/em&gt; easily in R; set the &lt;code&gt;simulate.p.value&lt;/code&gt; argument to &lt;code&gt;TRUE&lt;/code&gt;; then you aren't reliant on the chi-square approximation to the distribution of the test statistic.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-07T13:58:27.310" Id="81498" LastActivityDate="2014-01-07T13:58:27.310" OwnerUserId="805" ParentId="81483" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="81605" AnswerCount="1" Body="&lt;p&gt;I have one million of keywords (from search queries in google), and I need to group them semantically. I have already done some research and I have found information about how to extract keywords and cluster them from a large corpus, but in my case I don't have any large documents, only those keywords.&lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine that clustering these keywords semantically is impossible (although I hope I am wrong) since I guess you need a large text to extract its meaning, and each of my keywords has a maximum of 4 or 5 words. I thought about crawling the web and getting myself a large corpus and use some of the techniques I have seen like TF-IDF and then applying a k-means algorithm, and then I could extract the keywords from those documents and its subject, and then I could compare my keywords to those extracted and cluster them accordingly... But I don't know if this would work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone tell me if my approach is correct? If so, once I have clustered the keywords from the documents I get from the web, what kinds of techniques would I need to cluster my own keywords?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-07T17:09:16.227" FavoriteCount="1" Id="81519" LastActivityDate="2014-01-08T13:57:03.623" LastEditDate="2014-01-07T17:43:16.293" LastEditorUserId="7290" OwnerUserId="36987" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;text-mining&gt;" Title="Keyword clustering" ViewCount="233" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Given $p$-variate normal $X \sim \mathcal{N}\left(\mu,\Sigma\right)$, consider the random variable&#10;$$&#10;t_* = \frac{X_1}{\sqrt{\sum_{2\le i \le p} X_i^2}}.&#10;$$&#10;For some values of $\Sigma$, this generalizes the doubly noncentral t-distribution (which generalizes the singly noncentral t-, which generalizes the central t-distribution). Is this a known distribution? In particular, I would like to know how to compute the CDF, and the first two moments, say.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect that some transformation takes this $t_*$ to a form which is amenable to analysis, but I am not seeing it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-07T17:35:00.573" Id="81525" LastActivityDate="2014-01-07T17:35:00.573" OwnerUserId="795" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;t-distribution&gt;&lt;non-central&gt;" Title="Is there a known generalization of the doubly noncentral t-distribution?" ViewCount="43" />
  <row AcceptedAnswerId="81531" AnswerCount="1" Body="&lt;p&gt;I draw a random vector of dimensionality $k$, each dichotomous element of which taking on a value in $\{0,1\}$. The probability that any element will be $1$ is captured in the $k$-dimensional parameter vector $\bf{p}$, the elements of which are in $[0,1]$ and sum to $1$ (or are normalized as such prior to use). A special condition is that the random vector must sum to some value $c$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're familiar with R, I  can obtain such random vectors in R for some $k$, $c$, and $\bf{p}$ by &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v &amp;lt;- vector(length=k)&#10;v[sample(1:k, c, prob=p)] &amp;lt;- 1&#10;v&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am hoping someone can point me to a distribution that captures this. Ultimately, I need a &lt;strong&gt;notation&lt;/strong&gt; to use to describe this succinctly in a paper.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much for your time.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-07T17:49:02.867" Id="81526" LastActivityDate="2014-01-07T20:01:08.233" LastEditDate="2014-01-07T18:37:39.927" LastEditorUserId="24000" OwnerUserId="24000" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;distributions&gt;&lt;random-variable&gt;&lt;notation&gt;" Title="Notation for Random Bernoulli-Like Vector With Fixed Sum" ViewCount="144" />
  <row Body="&lt;p&gt;Hurdle models assume that there is only one process by which a zero can be produced, while zero-inflated models assume that there are 2 different processes that can produce a zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hurdle models assume 2 types of subjects: (1) those who never experience the outcome and (2) those who always experience the outcome at least once.  Zero-inflated models conceptualize subjects as (1) those who never experience the outcome and (2) those who can experience the outcome but don't always.&lt;/p&gt;&#10;&#10;&lt;p&gt;In simple terms: both zero-inflated and hurdle models are described in two parts.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The first is the on-off part, which is a binary process.  The system is &quot;off&quot; with probability $\pi$ and &quot;on&quot; with probability $1-\pi$.  (Here, $\pi$ is known as the inflation probability.)  When the system is &quot;off,&quot; only zero counts are possible.  This part is the same for zero-inflated and hurdle models.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second part is the counting part, which occurs when the system is &quot;on.&quot;  This is where zero-inflated and hurdle models differ.  In zero-inflated models, counts can still be zero.  In hurdle models they must be nonzero.  For this part, zero-inflated models use a &quot;usual&quot; discrete probability distribution while hurdle models use a zero-truncated discrete probability distribution function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example of a hurdle model: An automobile manufacturer wants to compare two quality control programs for its automobiles.  It will compare them on the basis of the number of warranty claims filed.  For each program, a set of randomly selected customers are followed for 1 year and the number of warranty claims they file is counted.  The inflation probabilities for each of the two programs are then compared.  The “off” state is “filed zero claims” while the “on” state is “filed at least one claim.”  &lt;/p&gt;&#10;&#10;&lt;p&gt;Example of a zero-inflated model:  In the same study above, the researchers find out that some repairs on the automobiles were fixed without the filing of a warranty claim.  In this way, the zeroes are a mixture of the absence of quality control problems as well as the presence of quality control problems that involved no warranty claims.  The “off” state means “filed zero claims” while the “on” state means “filed at least one claim OR had repairs fixed without filing a claim.”&lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3238139/&quot;&gt;here&lt;/a&gt; for a study in which both types of models were applied to the same data set.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-07T20:15:14.213" Id="81532" LastActivityDate="2014-01-08T13:25:42.643" LastEditDate="2014-01-08T13:25:42.643" LastEditorUserId="22311" OwnerUserId="36835" ParentId="81457" PostTypeId="2" Score="12" />
  <row AcceptedAnswerId="81589" AnswerCount="2" Body="&lt;p&gt;I'm currently experimenting with gridsearch to train a support vector machine. I understand that, if I have parameter gamma and C, the R function tune.svm performs a 10-fold cross validation for all combinations of these 2 parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I did not know how to start, I tried to get some information about it, for example &lt;a href=&quot;http://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine#Parameter_selection&quot; rel=&quot;nofollow&quot;&gt;2&lt;/a&gt; suggests values that are not linear, e.g. C in the range {10, 100, 1000}.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far I use the examples from my second wikipedia link, which is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gammas = 2^(-15:3)&#10;costs = 2^(-5:15)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which results into 399 combinations.&lt;/p&gt;&#10;&#10;&lt;p&gt;This takes very, very long (~2000 samples). For example for the kernel &quot;radial&quot; my best result is gamma = 0.5 and cost = 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Couldn't I get the same result if I just used values like (1, 2, 3, 4, ... 10) for costs and (0, 0.5, 1, 1.5, 2) for gammas? I know this example is constructed because I already know the result.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;But why this exponential scale?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are so many values between 0 and 1 that I think this is a waste of computation time and only so few very big numbers that it couldn't find a very exact result anyway. It would only make sense for me if this was used to find a smaller range, let's say we then know the best cost is 2^3 and then we search around that. But it's nowhere mentioned that is performed that way.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-07T21:26:19.133" Id="81537" LastActivityDate="2014-01-08T12:30:20.133" OwnerUserId="12947" PostTypeId="1" Score="0" Tags="&lt;svm&gt;" Title="Gridsearch for SVM parameter estimation" ViewCount="255" />
  <row Body="&lt;p&gt;There are two issues that make it hard to give you a concise answer.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;What do you want to do?&lt;/strong&gt; The median &quot;text miner&quot; probably works on articles scraped from the web, but there are also people using similar tools to analyse DNA, identify malicious software (or users), and nearly anything else that involves sequences of tokens. (See also @AdamO's comment above).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;How do you want to evaluate it?&lt;/strong&gt; Classification is easy: your data has some labels and just have to determine how well the algorithm has reproduced them from the rest of your data. Clustering, on the other hand, is essentially asking your code to find some sensible way to arrange your data. Since you don't have ground-truth labels, it is not clear to me how you would directly compute precision and recall from a clustering run. People sometimes use EM (or another alignment technique) to align their cluster assignments with labels, but you still need to verify that it's not clustering according to some other plausible category (author rather than topic, for example).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Furthermore, comparing precision and recall (or any other performance metric) against other papers isn't going to be very helpful unless you're also running on the same data sets. Some corpora have become de-facto standards. &#10;The RCV1 data set is one common benchmark. It's a huge collection of ~ a million news articles from Reuters, labelled with a controlled vocabulary. &lt;a href=&quot;http://www.cs.cmu.edu/~wcohen/postscript/ecai2010-pic-final.pdf&quot; rel=&quot;nofollow&quot;&gt;Lin and Cohen (2010)&lt;/a&gt; tested a few classification algorithms on it, so that might be one place to start. &lt;a href=&quot;http://management.haifa.ac.il/index.php/en/2013-01-16-13-45-34/info?id=513%3arbekkerman&quot; rel=&quot;nofollow&quot;&gt;Ron Bekkerman&lt;/a&gt; has published some experiments using the &lt;a href=&quot;https://www.cs.cmu.edu/~enron/&quot; rel=&quot;nofollow&quot;&gt;Enron Corpus&lt;/a&gt;, as have several other people, so that might be a place to look if you're interested in &quot;real&quot; email or author identification. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are other, more specialized data sets floating around too, ranging from movie reviews (labelled with ratings) to biomedical texts (with gene and protein names identified), depending on your interests. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-08T00:08:06.973" Id="81548" LastActivityDate="2014-01-08T00:08:06.973" OwnerUserId="7250" ParentId="81513" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It is obvious that using &quot;option 1&quot; dataset you virtually declare that similar respondents are those respondents who bought (or stole) the same items at the same turn or visit. I don't believe this is your research goal. In addition, problematic NA responses arise.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Option2&quot; dataset is what you should use, but do recode 2 into 1, to make the data binary. You then can take these variables in TwoStep as categorical variables. But here arises another doubt. TwoStep is for &lt;em&gt;nominal&lt;/em&gt; categorical variables only; and you probably will not want to treat the binary variables as dichotomous nominal. Treating them as nominal means that, for you, respondents who did &lt;em&gt;not&lt;/em&gt; buy the same item are as similar with each other as those who &lt;em&gt;did&lt;/em&gt; bought the same item. Rather, you'll want to treat those &quot;0 and 0&quot; respondents as neither similar nor dissimilar, - this suggests using similarity measures &lt;a href=&quot;http://stats.stackexchange.com/a/61910/3277&quot;&gt;such as Jaccard&lt;/a&gt; measure. But this in turn precludes using TwoStep and requires using Hierarchical clustering or other clustering methods apt for binary data (those other, unfortunalely, are not found in SPSS).&lt;/p&gt;&#10;&#10;&lt;p&gt;But you can't do hierarchical clustering of 75000 respondents - its too many, both practically and theoretically, for hierarchical method. You see - you've got trapped.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way out will be to look for clustering algorithms (outside SPSS) &lt;strong&gt;which are for&lt;/strong&gt; &lt;code&gt;big (large) sparse binary data&lt;/code&gt;. Search this site for this word combination: you'll get a few related questions, to read.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way out may be to do &lt;strong&gt;hierarchical clustering on random subsets&lt;/strong&gt; of your data (subsets of size, say, 500 respondents). Clustering of subsamples and cross-validation is beneficial, as it escapes overfitting threat. But, in the context of clustering, it is quite a big work. I recommend you to read papers on cluster analysis by subsamples.&lt;/p&gt;&#10;&#10;&lt;p&gt;A third and the easiest way will be to do &lt;strong&gt;K-means&lt;/strong&gt; clustering of your data. It solves the problem of big dataset. However, K-means is often seen as theoretically inappropriate for binary as well as count variables. They say that because the method involves computation of floating point geometric centroids, it requires interval, ideally - continuous, variables. That said, people use K-means with binary or count data &quot;all the time&quot;. It appears to me that in high-dimensional settings such as yours (1500 variables) the relative contributions of &quot;continuety&quot; and &quot;dimensionality&quot; to the formation of centroids shifts towards dimensionality anyway - even if you had quite fine-grained Likert scale variables. That seems to excuse, to an extent, applying K-means to a wide binary dataset as yours.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you choose K-means then I recommend you to normalize each row (respondent) in the dataset to unit sum-of-squares (= unit sum, since the data are binary) prior clustering. Why to do it? According to &lt;a href=&quot;http://stats.stackexchange.com/a/36158/3277&quot;&gt;this&lt;/a&gt;, when you normalize vector magnitudes (L2 norms) you make the euclidean distance between the vectors directly reflect cosine similarity between them: $d^2=2(1-cos)$. And it is cosine similarity (= Ochiai binary measure) which is a &lt;a href=&quot;http://stats.stackexchange.com/a/61910/3277&quot;&gt;justifiable alternative&lt;/a&gt; to Jaccard measure I mentioned in the 2nd paragraph above. Both Jaccard and Ochiai treat &quot;0 and 0&quot; respondents as neither similar nor dissimilar - that is what you need. So, using K-means on such way normalized data is in a sense analogous to using hierarchical clustering on Ochiai measure.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-08T00:26:05.607" Id="81549" LastActivityDate="2014-01-08T10:14:38.437" LastEditDate="2014-01-08T10:14:38.437" LastEditorUserId="3277" OwnerUserId="3277" ParentId="81539" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="81554" AnswerCount="2" Body="&lt;p&gt;I have a question regarding interactions in GLM.&#10;I run a Poisson regression with the purpose of predicting claims in insurance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following problem :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model 1 : CLAIMS ~ X1 + X2&#10;model 2 : CLAIMS ~ X1 + X2 + X1:X2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;X1 has 10 levels, X2 has 4 levels&lt;/p&gt;&#10;&#10;&lt;p&gt;When I do a deviance test (Chi Square), my result is that model 2 is significantly better compared to model 1 (at the 5% level).&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other side the AIC of model 2 is worse. (higher)&lt;/p&gt;&#10;&#10;&lt;p&gt;When I look at the individual parameter estimates and their corresponding standard error, I see that only 3 out of 27 levels from X1:X2 differ significantly from the base level.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure how to interpret these observations. Can someone help me and say something meaningful about it? And more important, how to deal with it?&lt;/p&gt;&#10;&#10;&lt;p&gt;My attempt would be: &quot;There are interactions in the model that are meaningful, however most of them are not. This explains the increasing AIC, although the deviance test says that model 2 is the better one.&quot; Should I try to construct a few dummy variables for these three interactions that seem significant? (However, I am not sure this is a correct approach, because it is only significant relative to the base level...) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-08T01:28:00.000" Id="81551" LastActivityDate="2014-01-11T18:53:52.790" LastEditDate="2014-01-08T01:43:15.383" LastEditorUserId="32036" OwnerUserId="31016" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;&lt;interaction&gt;&lt;interpretation&gt;&lt;aic&gt;&lt;poisson-regression&gt;" Title="Include specific interaction terms" ViewCount="125" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The first (default) method to correct for the &quot;multiple comparisons&quot; problem is to use a Bonferroni adjustment to the p-value.  The new $\alpha$ level is $\alpha^*=\frac{\alpha}{\#tests}$. Thus, divide the Type I error level of $\alpha=0.05$ by the number of tests you have.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Use the Benjamini-Hochberg False Discovery Rate approach (1995), which is less conservative. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-01-08T04:19:55.620" Id="81559" LastActivityDate="2014-01-08T04:19:55.620" OwnerUserId="32398" ParentId="81557" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;It depends on the range and scale of your input data.  If you are mean-zero standardizing your feature values, then try $\sigma=4$.  If you are normalizing feature values to a range of [0,1] then you can still try $\sigma=4$, but a value of $\sigma=1$ might be better.  Remember, you have to decrease the learning rate $\alpha$ and the size of the neighborhood function with increasing iterations, as none of the metrics stay constant throughout the iterations in SOM.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It also depends on how large your SOM is.   If it's a 10 by 10, then use for example $\sigma=5$.  Otherwise, if its a 100 by 100 map, use $\sigma=50$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In unsupervised classification, $\sigma$ is sometimes based on the Euclidean distance between the centroids of the first and and second closest clusters.    &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-08T04:28:26.230" Id="81560" LastActivityDate="2014-01-08T04:39:06.287" LastEditDate="2014-01-08T04:39:06.287" LastEditorUserId="32398" OwnerUserId="32398" ParentId="81543" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row Body="&lt;p&gt;Your sample does not need to be normally distributed to use moderation analysis. Where did you read this? You may be be thinking of the residuals from the model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Purposive sampling has not much to do with the distribution of the data; it will affect estimation of parameters. &lt;/p&gt;&#10;&#10;&lt;p&gt;Neither the moderating nor the independent variable has to be normally distributed. Indeed, they could be dichotomous or categorical. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-08T13:37:18.433" Id="81600" LastActivityDate="2014-01-08T13:37:18.433" OwnerUserId="686" ParentId="81594" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a distance measure, which ranges between 0 and 1, between several pairs of populations. I need to figure out if each distance is statistically different from zero using permutations, any suggestions on how to achieve that? Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-08T17:01:55.147" Id="81625" LastActivityDate="2014-01-08T17:25:38.767" OwnerUserId="36491" PostTypeId="1" Score="0" Tags="&lt;permutation&gt;" Title="Permulations to test if values are different than zero?" ViewCount="60" />
  <row AcceptedAnswerId="81735" AnswerCount="3" Body="&lt;p&gt;I have a question about the random walk of two kings in a 3&amp;times;3 chessboard.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each king is moving randomly with equal probability on this chessboard - vertically, horizontally and diagonally. Τhe two kings are moving independently from each other in the same chessboard. Both of them start in the same square, and then they move independently.&lt;/p&gt;&#10;&#10;&lt;p&gt;How could we find the probability in time $n$ both of them are in the same square, as $n$ goes to infinity?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-08T17:06:33.867" Id="81626" LastActivityDate="2014-01-10T16:33:25.067" LastEditDate="2014-01-10T16:33:25.067" LastEditorUserId="37036" OwnerUserId="37036" PostTypeId="1" Score="8" Tags="&lt;self-study&gt;&lt;markov-chain&gt;&lt;random-walk&gt;" Title="Random walk: kings on a chessboard" ViewCount="439" />
  
  
  
  
  
  <row AcceptedAnswerId="81675" AnswerCount="1" Body="&lt;p&gt;I am trying to implement Bayesian Classification on the data set as follows: &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Problem: classify whether a given person is a male or a female based on the measured features. The features include height, weight, and foot size.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Example training set below (borrowed from &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3fprY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is some samples to be classified as a male or female.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sex       height(feet)  weight(lbs)      foot size(inches)&#10;sample         6            130            8&#10;sample         4            140            5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Actual:'female' and the output is (0.99, 'female')&#10;Actual:'female' and the output is (0.89, 'male')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here, i want to evaluate the classification accuracy. I am confused between two options for evaluation: &#10;First option, it classified 1 of 2 test data correctly, so the accuracy is 50%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second option, the probabilities of being female are (0.99 + 0.11) /2 = 0.55, so the accuracy is 55%.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Which option makes sense to evaluate the classification accuracy of this method? What are the meaning of each options? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-08T23:04:32.157" Id="81672" LastActivityDate="2014-01-08T23:37:44.707" LastEditDate="2014-01-08T23:13:35.023" LastEditorUserId="37057" OwnerUserId="37057" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;probability&gt;&lt;classification&gt;&lt;naive-bayes&gt;" Title="Bayesian Classification evaluation" ViewCount="64" />
  
  <row Body="&lt;p&gt;It's not clear what you mean by 'derive from' in this case. I can explain how an MA(1) fits into what the theorem says.&lt;/p&gt;&#10;&#10;&lt;p&gt;The MA(1) process is a process of the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_t - \mu = \varepsilon_t - \theta_1 \varepsilon_{t-1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;or it is sometimes written:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_t - \mu = \varepsilon_t + \theta_1 \varepsilon_{t-1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;(e.g. see &lt;a href=&quot;http://en.wikipedia.org/wiki/Moving-average_model&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now &lt;a href=&quot;http://en.wikipedia.org/wiki/Wold%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;Wold's decomposition theorem&lt;/a&gt; says every covariance-stationary time series $Y_{t}$ can be written as the sum of two time series, one deterministic and one stochastic:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_t=\sum_{j=0}^\infty b_j \varepsilon_{t-j}+\eta_t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here $\eta_t$ is the deterministic part.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\eta_t=\mu$. Let $b_0 = 1$ and $b_j=0$ for all $j&amp;gt;1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then starting from the Wold decomposition:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_t=\sum_{j=0}^\infty b_j \varepsilon_{t-j}+\eta_t\\&#10;= 1\cdot\varepsilon_{t-0}+b_1 \varepsilon_{t-1}+\mu\\&#10;=\mu + \varepsilon_t+b_1 \varepsilon_{t-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is an MA(1) with $\theta_1=-b_1$ (if the MA(1) is written in the first form above) or with $\theta_1=b_1$ (if written in the second form above). This makes it clear that the Wold decomposition holds for an MA(1) process.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-09T01:24:51.767" Id="81678" LastActivityDate="2014-01-09T02:40:33.813" LastEditDate="2014-01-09T02:40:33.813" LastEditorUserId="805" OwnerUserId="805" ParentId="81658" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Questions involving the calculation or interpretation of eigenvalues should use &lt;a href=&quot;/questions/tagged/eigenvalues&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;eigenvalues&amp;#39;&quot; rel=&quot;tag&quot;&gt;eigenvalues&lt;/a&gt;. This may include factor analysis, principal components analysis or regression, or other model estimation functions that require a positive definite matrix (of which all eigenvalues are positive).&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Eigenvalues&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;An eigenvector of a square matrix A is a non-zero vector v that, when the matrix is multiplied by v, yields a constant multiple of v, the multiplier being commonly denoted by λ. That is:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;A v = λ v&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The number λ is called the eigenvalue of A corresponding to v.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In the context of factor analysis, a factor's eigenvalue is the sum of all variables' squared loadings on that factor. The factor loading is the correlation of the variable with the factor. The squared loading is the variance explained in the variable by the factor. The factor's eigenvalue divided by the sum of all eigenvalues is the proportion of total variance explained by the factor. From &lt;a href=&quot;https://en.wikipedia.org/wiki/Factor_analysis#Terminology&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If a factor has a low eigenvalue, then it contributes little to the explanation of variances in the variables and may be ignored as redundant with more important factors.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;All of the above is also true of principal components analysis. Principal components regression eliminates components with small eigenvalues for the similar purpose of reducing the dimensionality of a set of regressors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many &lt;a href=&quot;https://en.wikipedia.org/wiki/Factor_analysis#Criteria_for_determining_the_number_of_factors&quot; rel=&quot;nofollow&quot;&gt;criteria for identifying an appropriate threshold&lt;/a&gt; exist, and vary in their utility. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-09T10:16:54.883" Id="81697" LastActivityDate="2014-01-09T11:20:54.327" LastEditDate="2014-01-09T11:20:54.327" LastEditorUserId="32036" OwnerUserId="32036" PostTypeId="5" Score="0" />
  <row AcceptedAnswerId="81703" AnswerCount="1" Body="&lt;p&gt;I'm new to data analysis so this is kind of a simple question.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to understand why I cannot reproduce a survival curve generated by a fitted exponential model from Stata. I use the coefficients and make my function in R to plot but it looks nothing similar. I believe it's one of those daft problems where I am not interpreting something properly. I illustrate below.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, some data in Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;use http://www.ats.ucla.edu/stat/data/uis.dta, clear&#10;gen id = ID&#10;drop ID&#10;stset time, failure(censor)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then we can fit a null exponential model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;streg, dist(exponential) nohr&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which gives the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Exponential regression -- log relative-hazard form &#10;&#10;No. of subjects =          628                     Number of obs   =       628&#10;No. of failures =          508&#10;Time at risk    =       147394&#10;                                                   LR chi2(0)      =     -0.00&#10;Log likelihood  =    -1043.531                     Prob &amp;gt; chi2     =         .&#10;&#10;------------------------------------------------------------------------------&#10;          _t |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       _cons |  -5.670383   .0443678  -127.80   0.000    -5.757342   -5.583424&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I take the survivor function from Stata's documentation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;S(t)=\exp(-\lambda_{j} t_{j})&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, in R, I plot this out with the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;S &amp;lt;- function(x) exp(-5.670383*x) # 'x' acts like 't'&#10;curve(S, 0, 1000)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This curve is not equivalent to Stata's, given by:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;stcurve, surv&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where did I go wrong in my interpretation? Is my equation using the correct parameterization?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. Why am I reproducing these curves? A little to do with overlaying curves but now that I have this problem, I have to know where I went wrong.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-09T10:19:40.363" Id="81699" LastActivityDate="2014-01-10T15:51:43.127" LastEditDate="2014-01-09T10:51:54.763" LastEditorUserId="23853" OwnerUserId="25174" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;survival&gt;&lt;stata&gt;&lt;exponential&gt;&lt;parameterization&gt;" Title="What is the parameterization of exponential distribution for survival in Stata?" ViewCount="265" />
  
  <row Body="&lt;p&gt;As far as I know, model comparison should only be done on the same dataset. For models with different configurations, you can try Akaike information criterion (AIC) or  Bayesian information criterion (BIC).&lt;/p&gt;&#10;&#10;&lt;p&gt;Your 2 models have the same regressors but different coefficients. The easiest thing to do is to check the confidence intervals for overlapping. For example, if CI of $a_1$ overlaps CI of $a_2$ or covers $a_2$ if only one model is fit, then there is no significant difference between them. You can do this for all 3 coefficients. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-09T15:57:45.830" Id="81726" LastActivityDate="2014-01-09T15:57:45.830" OwnerUserId="37051" ParentId="81717" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="81742" AnswerCount="1" Body="&lt;p&gt;I have read the questions posted here about how to deal with the situation of significant interaction terms but insignificant 'main' effects, and my main take-away is that it depends on your research question how to proceed and interpret your results.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would appreciate your thoughts on my interpretation in the context of my research question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hypothesis: The relation between height and weight is stronger for males than for females.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Weight = a + b_1 \times Height + b_2 \times Male + b_3 \times (Height \times Male) + e$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Results:&lt;/p&gt;&#10;&#10;&lt;p&gt;$b_1$ is &lt;strong&gt;not&lt;/strong&gt; significantly different from zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;$b_3$, the interaction term, is positive and significantly different from zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Can I conclude that I found evidence in support of my hypothesis (that the relation between height and weight is stronger for males than for females)?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-09T18:02:03.307" Id="81737" LastActivityDate="2014-01-09T18:34:29.920" LastEditDate="2014-01-09T18:29:28.710" LastEditorUserId="34817" OwnerUserId="34817" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;interaction&gt;&lt;interpretation&gt;" Title="Interpretation of interaction terms if main effect is insignificant" ViewCount="676" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;From an urn containing 1 ball each of n different colours, a ball is drawn, its colour noted and then it is returned to the urn along with k balls of the same colour.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the distribution of the number of balls of each colour in a multicolour urn (after a certain number of draws)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, what is the distribution of wait times before drawing the first ball of a given colour in the multicolour urn model?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT&lt;/p&gt;&#10;&#10;&lt;p&gt;in simulations i've run, i've found that the number of balls of each colour is described by the negative binomial, and the time to the first ball for each colour given by the Lomax (pareto type II) distribution. Ideally I'd like to find a proof or a reference to confirm these results.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-09T21:06:48.930" Id="81757" LastActivityDate="2014-03-06T22:52:39.033" LastEditDate="2014-03-06T22:52:39.033" LastEditorUserId="20009" OwnerUserId="20009" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;stochastic-processes&gt;" Title="Polya urn waiting time distributions" ViewCount="47" />
  <row Body="&lt;p&gt;You raise a good point.  Don't be too surprised that you are getting the same results with the two different correlation structures.  Using a camera lens analogy, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1439815127&quot; rel=&quot;nofollow&quot;&gt;Walter Stroup (2013, p.438)&lt;/a&gt; describes GEE-type (i.e. R-side) repeated measures models as &quot;lesser quality wide-angle lens[es]&quot; whose &quot;greater depth of field makes them capable of a useable image even when somewhat out of focus.&quot;  On the other hand, G-side GLMMs are more like a &quot;high quality telephoto lens&quot; that is &quot;very sharp when in focus but with a shallow depth of field ...&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;GEE-type models do not correspond to any real-world probability process that gives rise to the data.  Instead, they provide a technique for estimating the mean of the marginal distribution.  As @AdamO points out, they are consistent for estimating the marginal mean and therefore are (to some degree) robust to correlation structure misspecification.  If you experiment with different correlation structures, you will find that you get roughly similar results.  If I were you, I'd be more concerned about overdispersion than the correlation structure.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you fit a GLMM and use integral estimation (i.e. Laplace or quadrature) instead of pseudo-likelihood, then you can fit different correlation structures and compare them on the basis of fit statistics (AIC, BIC, etc.).  See &lt;a href=&quot;http://glmm.wikidot.com/pkg-comparison&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for available packages.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The GLMM will be more sensitive to the correlation structure and it will also target a different parameter than your GEE. Instead of the mean of the marginal distribution, it will target a parameter closer to the median of the marginal distribution, but it is based on a real-world probability process.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-09T21:11:29.820" Id="81758" LastActivityDate="2014-01-09T21:11:29.820" OwnerUserId="36835" ParentId="81744" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Let's say I have monthly data on cars on the road and number of car accidents. I want to know whether cars on the road is somehow related to the number of car accidents. Of course, I could conduct regression analysis or correlation analysis. However, when I have a set of 50 or so months with the number of cars on the road and number of accidents, how can I perform hypothesis testing to test whether a rise in cars on the road was related to an increase in automobile accidents.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tagged as R because I'll be using R though my question relates to 'how would I solve this basic problem&quot;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-09T21:22:57.133" FavoriteCount="1" Id="81760" LastActivityDate="2014-01-09T21:22:57.133" OwnerUserId="29640" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;" Title="Hypothesis Testing With Only Two Data Points" ViewCount="58" />
  <row AcceptedAnswerId="113233" AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;If $X_1, ... , X_n$ be iid exponential with mean $1/\lambda$. Let $S_n = X_1 + ... + X_n$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;a) Show that $S_n$ is $\Gamma(n, 1/\lambda)$. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Each $X_i$ is $\Gamma(1, 1/\lambda)$ by the definition of an exponential distribution. We can look at the mgf of $S_n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$M(t) = E(e^{t \sum X_i}) = E(e^{tX_1})...E(e^{tX_n}) = (\frac{1}{1-t/\lambda})...(\frac{1}{1-t/\lambda}) = (\frac{1}{1-t/\lambda})^n,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is the mgf of $\Gamma(n, 1/\lambda)$. So $S_n \sim \Gamma(n, 1/\lambda)$.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;b) Show that $\frac{(S_n - n/\lambda)}{\sqrt{n}}$ converges in distribution and identify the asymptotic distribution. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;By the Central Limit Theorem, we know that $\frac{(S_n - n\mu)}{\sqrt{n}(1/\lambda^2)} = \frac{(S_n/n - \mu)}{(1/\lambda^2)(1/\sqrt{n})}$ converges in distribution to $N(0,1)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;We see that $\frac{(S_n/n - 1/\lambda)}{(1/\lambda^2)(1/\sqrt{n})} = \frac{(S_n/\sqrt{n} - \sqrt{n}/\lambda)}{(1/\lambda^2)}(\frac{\sqrt{n}}{\sqrt{n}}) = \frac{S_n - n/\lambda}{(1/\lambda^2)(\sqrt{n})}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, $$\frac{S_n - n/\lambda}{(1/\lambda^2)(\sqrt{n})} \xrightarrow{D} N(0,1)$$ implies that $$ \frac{S_n - n/\lambda}{(\sqrt{n})} = (1/\lambda^2)\frac{S_n - n/\lambda}{(1/\lambda^2)(\sqrt{n})} \xrightarrow{D} N(0, 1/\lambda^4).$$&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;c) Show that $\sqrt{n}(\sqrt{S_n/n} - \sqrt{1/\lambda})$ converges in distribution and identify the asymptotic distribution. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;There is a theorem in our textbook (&lt;em&gt;Introduction to Mathematical Statistics&lt;/em&gt; by Hogg) which states,&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;em&gt;Let $\{X_n\}$ be a sequence of random variables such that $\sqrt{n}(X_n - \theta) \xrightarrow{D} N(0, \sigma^2)$. Suppose the function $g(x)$ is differentiable at $\theta$ and $g'(\theta) \not= 0$. Then $\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{D} N(0, \sigma^2(g'(\theta)^2)$&lt;/em&gt;.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;From part b), we see that $$\frac{\sqrt{n}(S_n/n - 1/\lambda)}{(1/\lambda^2)} \rightarrow N(0,1).$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;So $$\sqrt{n}(S_n/n - 1/\lambda) \rightarrow N(0, 1/\lambda^4)$$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now we apply the theorem. We have $g(x)=\sqrt{x}$. So $g'(x)=1/(2\sqrt{x})$, and $g'(\frac{1}{\lambda}) = \frac{\sqrt{\lambda}}{2}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So by the theorem, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt{n}(\sqrt{S_n/n} - \sqrt{1/\lambda}) \rightarrow N(0, (\frac{\sqrt{\lambda}}{2\lambda^4})).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you think my answers are correct? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-09T21:29:51.533" Id="81762" LastActivityDate="2014-08-26T01:41:16.657" LastEditDate="2014-08-26T01:37:43.243" LastEditorUserId="28746" OwnerUserId="21471" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;convergence&gt;&lt;central-limit-theorem&gt;" Title="Convergence in distribution (central limit theorem)" ViewCount="132" />
  
  <row Body="&lt;p&gt;It is often necessary to know a little about the system being explored before sensible hypotheses come to mind and it is very useful to know about the variation and noise in an assay prior to designing an experiment. Exploratory experiments and analyses are good for that. Don't be too quick to decide that a dataset is definitive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, you should know that hypotheses that are suggested by the data in exploratory analyses will have a high chance of giving you a spurious 'significant' result if you test them using the same data, so ideally the exploratory analyses lead to the design and running of new experiments to specifically test hypotheses.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-10T00:47:06.040" Id="81776" LastActivityDate="2014-01-10T00:47:06.040" OwnerUserId="1679" ParentId="81773" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I think the columns of $W$ are eigenvectors of the covariance matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;$(x-m) \cdot (x-m)^T \cdot (w_1, w_2, ... w_k) = (\lambda_1w_1, \lambda_2w_2,...\lambda_kw_k)$&lt;/p&gt;&#10;&#10;&lt;p&gt;so,&lt;/p&gt;&#10;&#10;&lt;p&gt;$(w_1, w_2, ... w_k)^T \cdot (x-m) \cdot (x-m)^T \cdot (w_1, w_2, ... w_k)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$= (w_1, w_2, ... w_k)^T \cdot(\lambda_1w_1, \lambda_2w_2,...\lambda_kw_k)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$= diag[\lambda_1,\lambda_2,...\lambda_k]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since&lt;/p&gt;&#10;&#10;&lt;p&gt;$(x-m)^T \cdot (w_1, w_2, ... w_k) = ((w_1, w_2, ... w_k)^T \cdot (x-m))^T$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$z = diag[\sqrt\lambda_1,\sqrt\lambda_2,...\sqrt\lambda_k]$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-10T01:41:07.127" Id="81782" LastActivityDate="2014-01-10T01:41:07.127" OwnerUserId="35099" ParentId="81715" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="81799" AnswerCount="2" Body="&lt;p&gt;I cannot understand how step 2 transformed to step 3,&#10;anybody help me please ??? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bsghu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-10T09:08:33.383" FavoriteCount="1" Id="81793" LastActivityDate="2014-01-10T10:52:07.700" LastEditDate="2014-01-10T09:51:02.310" LastEditorUserId="36894" OwnerUserId="36894" PostTypeId="1" Score="0" Tags="&lt;cdf&gt;&lt;function&gt;&lt;geometric-distribution&gt;" Title="cumulative distribution function , cdf problem" ViewCount="54" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I need an estimate of the average value $\bar{X}$ of the data within the shaded area of this graph, and its corresponding confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4nIRV.png&quot; alt=&quot;ideal case&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the ideal case, the data would reflect some minor oscillations.  I'm tempted to ignore this, and use the $CLT$ so the standard error would be $SE=\frac{S}{\sqrt{N}}$, where $S$ is the sample standard deviation, and $N$ is the total number of data points in the range.  But I'm uncomfortable with this approach because I know this is not completely correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the main problem is that sometimes the data is not stationary for the whole range, and can show multiple stationary segments, as shown below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SMHEV.png&quot; alt=&quot;worst case&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;These issues invalidate any assumption of normality.  I'm struggling to identify a proper method to calculate a single value for the mean and confidence interval for the whole data range.  Any ideas?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-10T16:46:20.630" Id="81843" LastActivityDate="2014-01-10T19:31:02.520" OwnerUserId="25697" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;confidence-interval&gt;&lt;non-stationary&gt;" Title="How to estimate confidence interval of the sample mean of a non-stationary time series?" ViewCount="222" />
  <row Body="&lt;p&gt;An ANOVA doesn't require normally distributed data, but normally distributed residuals. That is, a normally distributed dependent variable conditional on the predictors in the model. However, you refer to the transformed variable as a &quot;explanatory variable.&quot; I take that to be an independent variable, and thus it does not need to be transformed to fit into the assumptions of an ANOVA framework.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-01-10T16:57:41.197" Id="81846" LastActivityDate="2014-01-10T16:57:41.197" OwnerUserId="21654" ParentId="81835" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Typically, OLS is typically not motivated by comparing the estimated response, $\hat{Y_i}$, to the observed response $Y_i$. Instead, if given a new set of values for the predictor value $X_{new}$, the OLS model predicts what the dependent variable would be $\hat{Y}_{new}$ in a typical case.&lt;/p&gt;&#10;&#10;&lt;p&gt;The point is that $\hat{Y}_i$ is typically not considered &quot;better&quot; than $Y_i$, but rather a more accurate reflection of what you expect $Y$ to be at a particular value for $X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there are situations when you may think $\hat{Y}_i$ more accurately reflects the truth than $Y_i$ (perhaps for an outlier arising from a malfunction in your data collection). This would be highly dependent on the details of your data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-10T20:35:37.320" Id="81875" LastActivityDate="2014-01-10T22:26:06.800" LastEditDate="2014-01-10T22:26:06.800" LastEditorUserId="35917" OwnerUserId="35917" ParentId="81871" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;You only need to log-transform the mRNA expression, not $x$. If you didn't log-transform, you could use the rank-based Kruskal-Wallis (KW) non-parametric variant of ANOVA.  (Kruskal-Wallis is much less sensitive to skewness and outliers, and doesn't require homoscedasticity).  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you go the ANOVA route with log(mRNA), you have to ensure the variance of log-mRNA is equal across the groups (homoscedasticity--&gt;equal variance of log-$y$ between groups).  You also can't have outliers when using ANOVA, since they will bias the results.&#10;For outlier effect on averages, consider the average of 1, 2, 3, and 1000000, which is near 250000.  However, if you use ranks, the the 1,2,3,1000000 turns into 1,2,3,4 for which the average is 2.5.  KW is based on ranks.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also assign a rank to the mRNA levels (without a log-transform), and use the ranks in ANOVA. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-11T00:20:47.753" Id="81895" LastActivityDate="2014-01-11T00:20:47.753" OwnerUserId="32398" ParentId="81887" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Have you thought about orthogonalizing the entire data matrix with PCA?  You could replace the columns of $\mathbf{X}$ with the un-correlated principal components (eigenvectors normalized to their $\sqrt{\lambda_m}$). &lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds like you don't have grouping categorical variables among the 40 variables as well.  In this, the only thing you are left with is measuring the association between variables.  Indeed, if you are trying to linear and non-linear assessments on sensitivity analysis and variance explanation, then break up the data using a &quot;divide and conquer&quot; approach to solve a large problem by solving smaller problems.  Mixtures of variables generated from DOW, LHS, and genetic algorithms sounds quite complex -- but as long as you generate questions singly, and then do the associated analysis to answer the problem, you can work through your analytic goals.  &lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, there doesn't exist variance explanation approaches that allow you to pull out non-linear and linear components using the same model, unless you code what you are doing using non-linear regression and linear regression. There are packages that allow you to fit data based on equations, so maybe look at those (IGOR, EGRET, AMFIT(Poisson), MATLAB, etc.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;Last, be careful of the &quot;so what?&quot; question, whereby after you have done all of your model checking, a reader could ask why you did all of this on simulated data.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-11T00:28:18.613" Id="81896" LastActivityDate="2014-01-11T20:58:38.613" LastEditDate="2014-01-11T20:58:38.613" LastEditorUserId="32398" OwnerUserId="32398" ParentId="81852" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;For a good overview of different types of plots (and examples good and bad), including a timeline of graphical development, there is plenty to explore here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.datavis.ca/gallery/&quot; rel=&quot;nofollow&quot;&gt;http://www.datavis.ca/gallery/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-01-11T03:43:24.100" CreationDate="2014-01-11T03:43:24.100" Id="81905" LastActivityDate="2014-01-11T03:43:24.100" OwnerUserId="36985" ParentId="78844" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am running a PCA for quality in a water resource. I have 15 original variables. Running the PCA using Minitab, it turned out that the first PC is responsible for 99.7% of the variation. Looking at the eigenvectors, all the original values have almost the same correlation value with PC1 (about 0.25). What does that mean? Note: I tried to use the log of the data, and the results were very much the same.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-11T07:19:15.730" Id="81911" LastActivityDate="2014-01-11T11:55:06.257" LastEditDate="2014-01-11T11:55:06.257" LastEditorUserId="22047" OwnerUserId="37178" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;pca&gt;" Title="Interpreting PCA results" ViewCount="86" />
  
  <row Body="&lt;p&gt;The Breusch-Pagan test only checks for the linear form of heteroskedasticity i.e. it models the error variance as $\sigma_i^2 = \sigma^2h(z_i'\alpha)$ where $z_i$ is a vector of your independent variables. It tests $H_0: \alpha = 0$ versus $H_a: \alpha \neq 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The White test on the other hand is more generic. It relies on the intuition that if there is no heteroskedasticity the classical error variance esitmator should gives you standard error estimates close enough to those estimated by the robust estimator. Therefore, it is able to detect more general form of heteroskedasticity than the Breusch-Pagan test. &lt;/p&gt;&#10;&#10;&lt;p&gt;A shortcoming of the White test is that it can lose its power very quickly particularly if the model has many regressors. This could be the reason for the results such as yours.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-11T12:57:55.063" Id="81923" LastActivityDate="2014-01-11T12:57:55.063" OwnerUserId="25543" ParentId="47490" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;This is an R question.  I have $n$ observations of the variable $y$, with each observation, $i$, weighted by $w(i)$.  Each $w(i)$ weight falls between zero and 1, with the sum of the weights, $m&amp;lt;n$. I am testing whether the weighted mean of $y$ equals zero, with $std(mean)$ denoting the weighted sample standard deviation of the mean, computed using &lt;code&gt;cov.wt&lt;/code&gt; and dividing by &lt;code&gt;sqrt(m)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;At a later time, I would like to run a weighted regression of $y$ against some explanatory variables.  However, as an intermediate step, I have run the regression &lt;code&gt;lm(y~1,weights=w)&lt;/code&gt;, which returns an intercept equal to the weighted mean of $y$.  However, the standard error of the intercept is based on $n-1$ degress of freedom and, therefore, is smaller than the standard deviation of the weighted sample standard deviation of the mean, computed using &lt;code&gt;cov.wt&lt;/code&gt; and dividing by &lt;code&gt;sqrt(m)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to run a regression in R in which the degrees of freedom in the simple regression above would be $m-1$ rather than $n-1$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, I have $m$ &quot;cases&quot; of the variable $y$.  In most cases, there is only one observation.  But if there were two observations in a given case, I would give each a weight of $1/2$, if three observations in a case, each would get a weight of $1/3$, etc.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-11T16:43:39.907" Id="81938" LastActivityDate="2014-01-11T17:51:20.060" LastEditDate="2014-01-11T17:51:20.060" LastEditorUserId="13680" OwnerUserId="37195" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;weighted-mean&gt;&lt;weighted-regression&gt;" Title="Weighted Standard Deviation vs. Weighted Regression Standard Error" ViewCount="192" />
  
  
  <row Body="&lt;p&gt;I can think of a number of ways in which a triplet could be unusual. You mention Euclidean distance, which is certainly one of them - but first you'd have to decide distance from what; one choice would be the mean of each of the members of the triplet. Then you could find mean of the first, second and third members of the triplets and calculate, for each triplet, how far each was from its mean. Then square the distances and sum them. You would then have a Euclidean distance for each triplet and you could order them from large to small and find quantiles. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another metric would be median absolute distance. Yet another would be maximum distance of any member of the triplet from the mean for that member. &lt;/p&gt;&#10;&#10;&lt;p&gt;Unless the triplets are fairly odd, I think all these measures will be highly correlated. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-11T19:00:42.360" Id="81949" LastActivityDate="2014-01-11T19:00:42.360" OwnerUserId="686" ParentId="81937" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I got a little confused with the squares and the sums. As far as I know, the variance or total sum of squares (TSS) is smth like&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i}^{n} (x_i - \bar x)^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the sum of squares within (SSW) is&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{j}^{K} \sum_{i}^{n} (x_i - c_j)^2 \qquad i \in C_j$ where k ist the number of clusters&lt;/p&gt;&#10;&#10;&lt;p&gt;and that&lt;/p&gt;&#10;&#10;&lt;p&gt;$TSS = SSW + SSB$&lt;/p&gt;&#10;&#10;&lt;p&gt;Correct so far?&lt;/p&gt;&#10;&#10;&lt;p&gt;I therefore can do $TSS - SSW = SSB$&lt;/p&gt;&#10;&#10;&lt;p&gt;But what is the direct way to get $SSB$ from a given codebook? Preferablly I'd like to know, how to get there in numpy/scipy cause reading the equations is kind of hard for me..&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;from scipy.cluster.vq import vq&#10;&#10;# X.shape[0] -&amp;gt; observations&#10;# X.shape[1] -&amp;gt; features&#10;partition, euc_distance_to_centroids = vq(X, codebook)&#10;&#10;TSS = np.sum((X-X.mean(0))**2)      &#10;SSW = np.sum(euc_distance_to_centroids**2)&#10;SSB = TSS - SSW&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think I'm missing the number of observations per cluster, when doing the SSB&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; X&#10;array([[ 2.,  4.,  2.],&#10;       [ 1.,  3.,  1.],&#10;       [ 3.,  4.,  2.],&#10;       [ 2.,  3.,  2.],&#10;       [ 1.,  5.,  5.]])&#10;&amp;gt;&amp;gt;&amp;gt; codebook&#10;array([[ 1.  ,  3.  ,  1.  ],&#10;       [ 2.33,  3.67,  2.  ],&#10;       [ 1.  ,  5.  ,  5.  ]])&#10;&amp;gt;&amp;gt;&amp;gt; TSS&#10;14.800000000000001&#10;&amp;gt;&amp;gt;&amp;gt; SSW&#10;1.3333333333333333&#10;&amp;gt;&amp;gt;&amp;gt; SSB&#10;13.466666666666667&#10;&amp;gt;&amp;gt;&amp;gt; ((X.mean(0)-codebook)**2).sum() # How do I put the &quot;num_clust_obs&quot; in here?&#10;12.542222222222223                  # Obviously not correct..&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-01-11T20:51:24.153" Id="81954" LastActivityDate="2015-02-10T06:09:21.333" LastEditDate="2014-01-12T10:28:35.077" LastEditorDisplayName="user35349" OwnerDisplayName="user35349" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;python&gt;&lt;sums-of-squares&gt;&lt;scipy&gt;&lt;numpy&gt;" Title="SSB - Sum of squares between clusters" ViewCount="641" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a time series which is daily data for workdays only. There is an almost obvious seasonality comparing the plot of the raw data that appears at the end of each month. However, there are not the same number of workdays for every month. Some times a month has 20 or 21 workdays. Do you have any idea how to treat the data? Do you suggest to manipulate the data in order to create a series with a constant frequency? The problem is that in R I have to set the frequency before running the ARIMA. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-11T23:42:17.323" FavoriteCount="1" Id="81967" LastActivityDate="2014-01-12T00:05:24.410" LastEditDate="2014-01-12T00:05:24.410" LastEditorUserId="7290" OwnerUserId="37203" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;" Title="Seasonality and work days in time series" ViewCount="84" />
  
  <row AcceptedAnswerId="81977" AnswerCount="2" Body="&lt;p&gt;I just read in a rather well-respected (popular) science magazine (the German PM, 02/2013, p.36) about an interesting experiment (without a source, unfortunately). It caught my attention because intuitively I doubted the significance of the result, but the information provided was sufficient for reproducing the statistical testing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The researchers wondered whether getting cold in cold weather increases the odds of catching a cold. So they randomly split a group of 180 students into two groups. One group had to hold their feet into cold water for 20 minutes. The other kept their shoes on. Kind of a funny manipulation, I think, but on the other hand I am not a doctor and maybe doctors think funny. Ethical issues aside.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyways, after 5 days, 13 of the students in the treatment group had a cold, but only 5 in the group that kept their shoes on. The odds ratio of this experiment thus is 2.87.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the rather small sample size, I started wondering if this difference may be significant. So I conducted two tests. &lt;/p&gt;&#10;&#10;&lt;p&gt;First a simple test of equality of proportions using the normal approximation. This test has $z=1.988$ with $p=0.0468$. My guess is that this is what the researchers tested. This is truely just significant. However this z-test is only valid in large samples, if I am not mistaken, due to the normal approximation. Furthermore, the prevalence rates are rather small and I wonder whether this may not affect the coverage rate of the confidence interval of the effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my second try was a chi-square test of independence, both with Monte-Carlo simulation and standard Pearson Chi-square. Here I find p-values both about $p=.082$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now that's all not so reassuring about the results. I wondered if there are more options to test this data and what your thoughts on the two tests are (in particular the assumptions of the first, significant, test)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-12T02:08:46.287" FavoriteCount="1" Id="81975" LastActivityDate="2014-01-20T20:29:59.537" LastEditDate="2014-01-17T14:51:02.703" LastEditorUserId="7290" OwnerUserId="24515" PostTypeId="1" Score="9" Tags="&lt;hypothesis-testing&gt;&lt;chi-squared&gt;&lt;experiment-design&gt;&lt;proportion&gt;&lt;biostatistics&gt;" Title="The $z$-test vs the $\chi^2$-test for comparing the odds of catching a cold in 2 groups" ViewCount="335" />
  <row Body="&lt;p&gt;I'd use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests&quot;&gt;permutation test&lt;/a&gt; instead of either the Normal approximation or the chi-square.  The permutation test is exact and most powerful, conditional upon the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, we can't calculate all the permutations of the groups, but we can generate a lot of random permutations of the data and get a pretty precise value:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;group &amp;lt;- c(rep(&quot;A&quot;,90),rep(&quot;B&quot;,90))&#10;n_a &amp;lt;- rep(0,100000)&#10;for (i in 1:length(n_a)) {&#10;   temp &amp;lt;- sample(group, size=18)&#10;   n_a[i] &amp;lt;- sum(temp == &quot;A&quot;)&#10;}&#10;&amp;gt; mean(n_a &amp;gt;= 13)&#10;[1] 0.03904&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which would indicate a p-value of 0.039.&lt;/p&gt;&#10;&#10;&lt;p&gt;HOWEVER, and this is a big however, I'm guessing that the assumption that the subjects getting colds are independent events is violated.  These individuals are students, presumably at the same school.  Imagine two of them share a class, or a dorm, or some other activity, or a cafeteria (in a school with multiple cafeterias); the events &quot;#1 gets a cold&quot; and &quot;#2 gets a cold&quot; are not independent.  I could imagine that a student would say &quot;let's sign up for this experiment!&quot; to his/her roommate or friends; I could imagine that the students were recruited from classes that the professors taught; I could imagine a lot of ways that the assumption of independence is violated.  Perhaps the paper, which I have not read, addresses some of these, but it's hard to see how it could address all of them, and the others that come quickly to mind.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-12T04:09:36.493" Id="81977" LastActivityDate="2014-01-12T04:09:36.493" OwnerUserId="7555" ParentId="81975" PostTypeId="2" Score="11" />
  
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Let $X$ and $Y$ be iid $N(0, \sigma^2)$ variables. &lt;/p&gt;&#10;  &#10;  &lt;ol&gt;&#10;  &lt;li&gt;Show that $X^2+Y^2$ and $X/ \sqrt{X^2 + Y^2}$ are independent. &lt;strong&gt;Hint&lt;/strong&gt;: Compute the joint distribution of the first and the square of the second expression. &lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Since $X  \sim N(0, \sigma^2)$, we know that $X/\sigma \sim N(0,1)$. So $X^2/\sigma^2 \sim \chi^2(1)$. But this means that $X^2 = \sigma^2(X^2/\sigma^2) \sim \chi^2(\sigma^2)$. Of course, we have the same result for $Y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $W_1 = X^2/(X^2 + Y^2)$ and $W_2 = X^2 + Y^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We get the joint distribution, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{W_1, W_2}(w_1, w_2) = \left[ \left( \frac{1}{\Gamma(\sigma^2/2)2^{\sigma^2/2}} \right)^2 (w_1(1-w_1))^{\sigma^2/2 - 1} \right] \left[ (w_2)^{\sigma^2 - 2} e^{-w_2} \right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a theorem in my textbook (&lt;em&gt;Introduction to Mathematical Statistics&lt;/em&gt; By Hogg) which says &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;Let the random variables $X_1$ and $X_2$ have supports $S_1$ and $S_2$ respectively, and have the joint pdf $f(x_1, x_2)$. Then $X_1$ and $X_2$ are independent if and only if $f(x_1, x_2)$ cna be written as a product of a nonnegative function of $x_1$ and a nonnegative function of $x_2$. That is $$f(x_1, x_2) \equiv g(x_1)h(x_2),$$ where $g(x_1) &amp;gt; 0$, $x_1 \in S_1$, zero elsewhere, and $h(x_2) &amp;gt; 0$, $x_2 \in S_2$, zero elsewhere.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So, by this theorem, $W_1$ and $W_2$ are independent. (We know that $\left( \frac{1}{\Gamma(\sigma^2/2)2^{\sigma^2/2}} \right)^2 (w_1(1-w_1))^{\sigma^2/2 - 1}$ is positive because $w_1 = \frac{x^2}{x^2 + y^2} \leq 1$.)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;ol&gt;&#10;  &lt;li&gt;Show that $X+Y$ and $X-Y$ are independent. &lt;strong&gt;Hint:&lt;/strong&gt; Use a standard property of the normal distribution. &lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;$X+Y \sim N(0,2)$ and $X-Y \sim N(0,2)$. Let $U_1 = X+Y$ and $U_2 = X-Y$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;We have, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{U_1, U_2}(u_1, ,u_2) =  \left[ \left( \frac{1}{4 \pi} \right) e^{-u_1^2}{4} \right] \left[ e^{ (-u_2^2)/4} \right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;By the above theorem, $X+Y$ and $X-Y$ are independent. &lt;/p&gt;&#10;&#10;&lt;p&gt;Do you think my answers are correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks you in advance  &lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-01-12T12:02:22.350" Id="81997" LastActivityDate="2014-01-13T15:17:49.893" LastEditDate="2014-01-13T13:26:07.207" LastEditorUserId="21471" OwnerUserId="21471" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;&lt;independence&gt;" Title="Determining if these random variables are independent" ViewCount="78" />
  
  <row Body="&lt;p&gt;One thing worth adding is that the most likely reason your 30-year-old textbook used the absolute mean deviation as opposed to standard deviation is that it is easier to calculate by hand (no squaring / square roots).  Now that calculators are readily accessible to high school students, there is no reason not to ask them to calculate standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are still some situations where absolute deviations are used instead of standard deviations in complex model fitting.  Absolute deviations are less sensitive to extreme outliers (values far from the mean/trendline) compared to standard deviations because they don't square that distance before adding it to the values from other data points.  Since model fitting methods aim to reduce the total deviation from the trendline (according to whichever method deviation is calculation), methods that use standard deviation can end up creating a trendline that diverges away from the majority of points in order to be closer to an outlier.  Using absolute deviations reduces this distortion, but at the cost of making calculation of the trendline more complicated.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's because, as others have noted, the standard deviation has mathematical properties and relationships which generally make it more useful in statistics.  But &quot;useful&quot; should never be confused with perfect.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-12T16:47:14.543" Id="82015" LastActivityDate="2014-01-12T16:47:14.543" OwnerUserId="36985" ParentId="81986" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose you have a large set of estimates (predictors) presumably created with a generalized linear model (GLM) with gamma error and an inverse link (possibly a log link).  You know that there are 3 variables used in the model: state, type, and territory (zip code groups).  You do not know the betas (&lt;em&gt;i.e.&lt;/em&gt;, estimated coefficients); you can only estimate them by comparing the estimates.  For example, the prediction for territory 1 in a particular state is $x\%$ different from territory 2, so there is a beta that creates that difference in the output.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way--via simulation, closed form, or whatever--to create a bound for the variance of each estimate? &lt;/p&gt;&#10;&#10;&lt;p&gt;I know this is a very strange question, but this does have useful practical applications and your thoughts would be greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The question again condensed:&lt;/strong&gt;  You have estimates from a GLM with gamma error and inverse link.  You know the estimates vary by state, type and territory.   Can you estimate or find an outer bound for the variance of each estimate using any technique you can conjure up?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-12T17:24:42.330" Id="82017" LastActivityDate="2014-01-12T17:43:17.090" LastEditDate="2014-01-12T17:43:17.090" LastEditorUserId="919" OwnerUserId="37222" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;generalized-linear-model&gt;" Title="Estimating variance when only the estimates are available" ViewCount="26" />
  <row Body="&lt;p&gt;(1) Your approach is correct;&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Use the same mean value and standard deviation of training data to scale your test data before PCA implementation;&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) The same as (2). The general approach for the weight updates is the same between online and offline learning. The only difference is that ofﬂine learning will sum the error over all inputs (batch gradient descent), while the online learning will compute the error for each input once at a time (stochastic gradient descent). But both should use the same scaling method.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-12T21:16:45.133" Id="82033" LastActivityDate="2014-01-12T21:16:45.133" OwnerUserId="35099" ParentId="81982" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I understand the basic principle behind the algorithm for LLE consists of three steps.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Finding the neighborhood of each data point by some metric such as k-nn.&lt;/li&gt;&#10;&lt;li&gt;Find weights for each neighbor which denote the effect the neighbor has on the data point.&lt;/li&gt;&#10;&lt;li&gt;Construct the low dimensional embedding of the data based on the computed weights.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;But the mathematical explanation of steps 2 and 3 are confusing in all the text books and online resources that I have read. I am not able to reason why the formulas are used.&lt;/p&gt;&#10;&#10;&lt;p&gt;How are these steps performed in practice? Is there any intuitive way of explaining the mathematical formulas used? &lt;/p&gt;&#10;&#10;&lt;p&gt;References:&#10;&lt;a href=&quot;http://www.cs.nyu.edu/~roweis/lle/publications.html&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.nyu.edu/~roweis/lle/publications.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-12T22:30:35.387" FavoriteCount="1" Id="82037" LastActivityDate="2014-04-19T10:41:00.527" LastEditDate="2014-01-13T10:10:17.837" LastEditorUserId="37078" OwnerUserId="37078" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;model-selection&gt;&lt;feature-selection&gt;&lt;dimensionality-reduction&gt;&lt;nonlinear&gt;" Title="Explain steps of LLE (local linear embedding) algorithm?" ViewCount="272" />
  <row Body="&lt;p&gt;I would ask yourself the following questions: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How often do the missing values occur?&lt;/li&gt;&#10;&lt;li&gt;Are the missing values biased to a particular characteristic that would materially change your results? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;After you answer these questions then you can make a decision whether to exclude them or make an adjustment.  For example, if there is a low occurrence and they do not seem to be biased anywhere (missing values occur randomly) I would probably exclude and document why.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to get complicated you can build a separate model to try a feel in these values.  However, the cost of this may not be worth the benefit.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-13T00:28:22.510" Id="82042" LastActivityDate="2014-08-02T22:05:41.797" LastEditDate="2014-08-02T22:05:41.797" LastEditorUserId="22468" OwnerUserId="37222" ParentId="82016" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Can anyone tell me what is meant by the phrase 'weak learner'? Is it supposed to be a weak hypothesis? I am confused about the relationship between a weak learner and a weak classifier. Are both the same or is there some difference? &lt;/p&gt;&#10;&#10;&lt;p&gt;In the adaboost algorithm, &lt;code&gt;T=10&lt;/code&gt;. What is meant by that? Why do we select &lt;code&gt;T=10&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-13T03:43:47.203" FavoriteCount="2" Id="82049" LastActivityDate="2014-01-13T18:10:55.577" LastEditDate="2014-01-13T04:31:23.763" LastEditorUserId="7290" OwnerUserId="37241" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;" Title="What is meant by 'weak learner'?" ViewCount="429" />
  <row AcceptedAnswerId="82056" AnswerCount="1" Body="&lt;p&gt;It is possible that this is only an English language question (in which case, please move it), but I ask in case there is any statistical or mathematical logic to the answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm sure the phrase should be &quot;chi-squared&quot; and not &quot;chi-square&quot;. It is &lt;strong&gt;χ2&lt;/strong&gt; after all.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we have a square (a box) and its side length is χ, even though its area is χ2, perhaps it &lt;em&gt;can&lt;/em&gt; be called a χ square. Have I just answered my own question? Sorry.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to &lt;a href=&quot;http://languagetips.wordpress.com/2012/07/05/weekly-language-usage-tips-chi-square-or-chi-squared-pick-a-preposition/&quot; rel=&quot;nofollow&quot;&gt;this blog post&lt;/a&gt;, &quot;chi-squared&quot; is more correct but &quot;chi-square&quot; is more popular.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-13T05:05:35.510" Id="82055" LastActivityDate="2014-01-13T06:01:58.303" LastEditDate="2014-01-13T05:40:52.660" LastEditorUserId="7290" OwnerUserId="37167" PostTypeId="1" Score="3" Tags="&lt;chi-squared&gt;&lt;terminology&gt;" Title="Why is the term &quot;chi-square&quot; acceptable?" ViewCount="71" />
  
  <row Body="&lt;p&gt;As already pointed out by Ben Bolker, I think a GLMM might not be the adequate analysis strategy for you, although it might seem appropriate at first due to the nested structure of classes in schools. However, as the &lt;code&gt;lme4&lt;/code&gt; &lt;a href=&quot;http://glmm.wikidot.com/faq#toc32&quot; rel=&quot;nofollow&quot;&gt;faq tells&lt;/a&gt; you:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One point of particular relevance to 'modern' mixed model estimation&#10;  (rather than 'classical' method-of-moments estimation) is that, for&#10;  practical purposes, there must be a reasonable number of&#10;  random-effects levels (e.g. blocks) — more than 5 or 6 at a minimum.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;As you have only two levels, for your random factor school and only two or four levels for class, running a &lt;code&gt;glmm&lt;/code&gt; will not work properly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Simply run a binomial &lt;code&gt;glm&lt;/code&gt; on your dv, e.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- glm(success ~ age + gender + repeated + school * class, family = binomial, &#10;      data = your.df)&#10;summary(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-01-13T08:04:11.587" Id="82068" LastActivityDate="2014-01-18T08:29:29.897" LastEditDate="2014-01-18T08:29:29.897" LastEditorUserId="442" OwnerUserId="442" ParentId="82014" PostTypeId="2" Score="7" />
  <row AnswerCount="1" Body="&lt;p&gt;96 participants were randomly assigned to one of the two groups regardless of their motivation level. Participants in both the groups were exposed to both the experimental conditions of gain framed messages and loss framed messages. The only difference was - counterbalancing such that the participants were shown gain frame message in group 1 then its corresponding loss frame message was shown in group 2.&#10;So here the within group factor is: framing condition - gain frame and loss frame&#10;between group factor means that this should vary across two groups but in my study both participants of high and low motivation were present in both groups and exposed to both gain frame and low frame messages.&#10;Participants in two groups were different but exposed to same but counterbalanced design.&#10;I am not sure if this is a 2 x 2 mixed anova design with framing condition as within group and motivation as between group factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-13T12:53:26.550" Id="82094" LastActivityDate="2014-01-13T18:09:29.287" OwnerUserId="37257" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;mixed&gt;" Title="2 x 2 mixed anova?" ViewCount="61" />
  
  <row Body="&lt;p&gt;The functional form of the model is going to be very important here. In fact there might be interaction effects between the treatments (&lt;em&gt;sensitivity of breaking to bending might depend on whether it has been put through fire before&lt;/em&gt;) and hence you need to use a non-linear functional form&lt;/p&gt;&#10;&#10;&lt;p&gt;So, instead of a form like: $$y=\beta_{fire}x_{fire}+ \beta_{bending}x_{bending} + .. $$ you might want to use a form: $$y=\beta_{bending-fire}x_{bending}x_{fire} + ..+\beta_{fire}x_{fire}+ \beta_{bending}x_{bending} + .. $$&lt;/p&gt;&#10;&#10;&lt;p&gt;You should start with this simple linear model and then move on to random forests since they will automatically create these interactions if they are important&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-13T15:13:03.300" Id="82114" LastActivityDate="2014-01-13T15:13:03.300" OwnerUserId="37263" ParentId="24782" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What would be &quot;typical&quot; values? What would be extreme values? &lt;/p&gt;&#10;&#10;&lt;p&gt;You can think of I as a function of three variables $I(u,A,B)$. Since $u, A, B \in \mathbb{R}$, it's not that easy to represent. You can either make a 1d plot of $I(u;A,B)$ vs $u$ for fixed values of $A$ and $B$, where you then have to take some representative values of $A$ and $B$. You could also make 2d color or contour plots where only $B$ is fixed, or only $A$, so you'd have $I(u,A;B)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing you can do for a 3d plot where the axis are for $u$, $A$ and $B$ would be a constant-value contour, i.e. make a 3d plot of the surface at which $I(u,A,B) = c$ for some constant $c$. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-12T14:47:31.413" Id="82116" LastActivityDate="2014-01-12T14:47:31.413" OwnerDisplayName="Lagerbaer" OwnerUserId="9853" ParentId="82115" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;My current experiment aims to explore the effect of viewing condition, difficulty and depth perception on object grasping. I'm mostly interested in the effect of individual differences in depth perception.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've previously been specifying my model as a mixed-factor ANOVA, with viewing condition (binocular, monocular) and difficulty (5 levels) as repeated-measures factors and ability to perceive depth as a between subjects factor (raw data for this was logarithmic, I separated the participants into two groups ['Good' vs 'Poor'] depending on which side of a threshold their scores fell on).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I'm interested in individual differences, the idea of using mixed-models is appealing, but I can't quite grasp how to explore the effect of depth perception using this technique.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that viewing condition and difficulty would be fixed factors. I think that both ID and depth perception would be random effects, so I can specify the model like so:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;fit.1 &amp;lt;- lme(Accuracy ~ Difficulty*View, random=~1|ID/DepthPerception, data=mydata)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I correct in thinking that I can leave the depth perception data in its logarithmic form? Having fit this model, I cannot figure out how to interpret the effect of depth perception - only the fixed effects show up in the results.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-13T16:25:18.633" Id="82131" LastActivityDate="2014-01-13T16:25:18.633" OwnerUserId="13554" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lme&gt;&lt;mixed-effect&gt;" Title="Mixed-model specification in R - exploring individual differences" ViewCount="86" />
  
  
  
  
  <row AcceptedAnswerId="109332" AnswerCount="2" Body="&lt;p&gt;I have an example from a book I am working through (car starting problem, with fuel &amp;amp; dirty spark plugs), but need a little help please.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have probabilities: P(NS) = 0.016, P(NF) = 0.001, P(F) = 0.999,  P(SPD) = 0.05 and P(SPC) = 0.95.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where S = car starts, NS = car doesn't start, F = there is fuel in car, NF = there is no fuel in car, SPD = spark plugs are dirty and SPC = spark plugs are clean.&lt;/p&gt;&#10;&#10;&lt;p&gt;And probabilities:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;........   NS   S&#10;NF   SPD   1    0&#10;NF   SPC   1    0&#10;F    SPD   0.1  0.9&#10;F    SPC   0.01 0.99&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(So for example, the first row: if the car has no fuel (NF) and spark plugs are dirtty (SPD) the car will not start with probability one)&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to calculate P(SPD|NS).&lt;/p&gt;&#10;&#10;&lt;p&gt;My attempt:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;P(SPD|NS) = P(SPD and NS) / P(NS) = P(NS|SPD)P(SPD) / P(NS)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I need P(NS|SPD) – need for all fuel status (F or NF).&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this should be = 1*0.001 + 0.1*0.999. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am going in circles trying to write the above line as probabilities – what is the proper way to present this in terms of NS, SPD, F and NF (assuming it is right). &#10;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-13T19:03:36.587" Id="82159" LastActivityDate="2014-07-25T11:07:48.003" LastEditDate="2014-01-13T19:24:14.470" LastEditorUserId="20650" OwnerUserId="20650" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;bayes&gt;" Title="Small conditional probabilies textbook question" ViewCount="100" />
  <row AcceptedAnswerId="82168" AnswerCount="1" Body="&lt;p&gt;I have fit a probit model in R. However, in addition to the $z$- and $p$-values, I would like to know the Wald $\chi^2$ values for the individual explanatory variables. How do I obtain / calculate the Wald $\chi^2$ values for parameter estimates?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-13T19:54:04.567" Id="82164" LastActivityDate="2014-01-13T20:26:21.693" LastEditDate="2014-01-13T20:26:21.693" LastEditorUserId="7290" OwnerUserId="36793" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;chi-squared&gt;&lt;probit&gt;&lt;ordered-variables&gt;" Title="Wald $\chi^2$ value for a probit model" ViewCount="46" />
  <row AnswerCount="0" Body="&lt;p&gt;I asked this question on the Mathematics stack exchange site but I didn't get a response. If posting it here means it should be closed or migrated from there, is there a way that i can do that? Or can a moderator to it?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In many queuing models it is assumed that the service time follows an exponential distribution with parameter $\mu=1/\lambda$, where $\lambda$ is the average rate of service. An example might be a bank teller who, on average, is able to service customers at a rate of 1 every 10 minutes.&lt;/p&gt;&#10;&#10;&lt;p&gt;This assumption is obviously unrealistic in some respects. For example, there is certainly a minimum service time under which a teller could never complete a service routine. However, the exponential distribution has no such lower bound (other than a service time of 0) and in fact seems to assume that these extremely low service times are the most likely outcome. How is it that exponential service times end up being a reasonable approximation when they seem to imply that very small service times are the most likely outcome (which seems unrealistic) and don't account for any sort of floor on what service times are capable of being achieved?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-13T21:45:17.653" Id="82180" LastActivityDate="2014-01-13T21:45:17.653" OwnerUserId="23840" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;exponential&gt;" Title="Exponential Service Times when Minimum Service Time is Reasonable in Queuing Theory" ViewCount="45" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;My question is, how to properly calculate the difference between population means?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/William_Sealy_Gosset&quot; rel=&quot;nofollow&quot;&gt;Gosset&lt;/a&gt; asked himself the same question and came up with the t-test (or in this case the &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_Unequal_sample_sizes.2C_unequal_variances&quot; rel=&quot;nofollow&quot;&gt;two sample t-test&lt;/a&gt;), which means that you are thinking about this right!&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I would expect to obtain not a single number but rather again, a mean of the estimated difference and some measure of variability, like 95% CI.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;And you obtain exactly that. &lt;/p&gt;&#10;&#10;&lt;p&gt;As the sample mean is a function of random variables, and has its own distribution, so is the difference of means.&#10;In case your two samples are large enough, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;central limit theorem&lt;/a&gt; states that the sample mean has approximately normal distribution. Additionally, sums of &lt;strong&gt;independent&lt;/strong&gt; Gaussians &lt;a href=&quot;http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables&quot; rel=&quot;nofollow&quot;&gt;are also Gaussian distributed&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{x} \sim N(\mu_X,\frac{\sigma^2_X}{n_1})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{x}_c \sim N(\mu_{X_c},\frac{\sigma^2_{X_c}}{n_2})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{x} - \hat{x}_c \sim N(\mu_{X} - \mu_{X_c},\frac{\sigma^2_{X}}{n_1}+\frac{\sigma^2_{X_c}}{n_2})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Observe that the difference of the sample means $\hat{x}$ for treatment group and $\hat{x}_c$ for control i. does have a Gaussian distribution, ii. its mean lies between the two population means $\mu_X$ and $\mu_{X_c}$, iii. it has larger variance than any of the sample means. &lt;/p&gt;&#10;&#10;&lt;p&gt;The statistical test mentioned above is based on calculating the probability of outcomes using exactly the above probability distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as the ratio $\frac{\hat{x}}{\hat{x}_c}$ is concerned, it is a much more complicated story. Basically, except for very specific cases, the distribution is not Gaussian and the estimator is biased.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-13T23:17:51.260" Id="82189" LastActivityDate="2014-01-13T23:39:30.643" LastEditDate="2014-01-13T23:39:30.643" LastEditorUserId="12436" OwnerUserId="12436" ParentId="82186" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;could you take the determinant of their covariance matrices? That would be the total variance of the dataset for each, I think there's an F test for such a comparison. I saw the test here:&#10;&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/1096-8644(200009)113:1%3C79::AID-AJPA7%3E3.0.CO;2-3/abstract&quot; rel=&quot;nofollow&quot;&gt;http://onlinelibrary.wiley.com/doi/10.1002/1096-8644(200009)113:1%3C79::AID-AJPA7%3E3.0.CO;2-3/abstract&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-14T03:42:03.827" Id="82208" LastActivityDate="2014-01-14T03:42:03.827" OwnerUserId="8762" ParentId="82171" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using &lt;code&gt;earth package&lt;/code&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/earth/index.html&quot; rel=&quot;nofollow&quot;&gt;earth: Multivariate Adaptive Regression Spline Models&lt;/a&gt; regression to get a constant piecewise approximation of my data. I want to plot a band of confidence around it.  Does this make sense to estimate a confidence interval of the smoothed function? If yes how can I do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that confidence intervals cannot be calculated directly, since it is a non parametric regression but I want to have a coherent result like (plot a band of confidence around my smoothed function) what I can get using &lt;code&gt;lm&lt;/code&gt; or &lt;code&gt;loess&lt;/code&gt; smoothing.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I add some code to clarify my idea, Here what I would do if I am using linear regression or &lt;code&gt;loess&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;x &amp;lt;- rnorm(15)&#10;dat &amp;lt;- data.frame(x = x, y = c(x + rnorm(15)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Using lm&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod &amp;lt;- lm(y ~ x,data=dat)&#10;predfit &amp;lt;- predict(mod,se=TRUE,interval=&quot;confidence&quot;)$fit&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Using loess&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;level=0.95&#10;mod &amp;lt;- loess(y~x,data=dat)&#10;pred &amp;lt;- predict(mod, se = TRUE)&#10;y = pred$fit&#10;    ci &amp;lt;- pred$se.fit * qt(level / 2 + .5, pred$df)&#10;data.frame(ymin = y - ci,&#10;           ymax = y + ci)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My question how to get &lt;code&gt;ymin&lt;/code&gt; and &lt;code&gt;ymax&lt;/code&gt; if I use &lt;code&gt;earth&lt;/code&gt; :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(earth)&#10;mod = earth(y~x,data=dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2014-01-14T11:35:35.123" FavoriteCount="1" Id="82236" LastActivityDate="2014-01-21T07:43:11.007" LastEditDate="2014-01-21T07:43:11.007" LastEditorUserId="28218" OwnerUserId="17304" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;smoothing&gt;" Title="confidence band around a smoothed function" ViewCount="362" />
  <row Body="&lt;p&gt;First off, be aware that the term &quot;normalize&quot; is ambiguous within statistical science. You apply it to scaling by (value $-$ mean) / standard deviation, which is commonly also described as standardization. But it is also often applied to transformations that produce versions of a variable that are more nearly normal (Gaussian) in distribution. Standardization itself does not affect how far a distribution is normal, as it is merely a linear transformation, and skewness and kurtosis (for example), and more generally all measures of distribution shape, remain as they were. &lt;/p&gt;&#10;&#10;&lt;p&gt;As for principal component analysis (PCA), prior standardization is common, indeed arguably essential, whenever the individual variables are measured using different units of measurement. Conversely, PCA without standardization can make sense so long as all variables are measured in the same units. The difference corresponds to basing PCA on the correlation matrix (prior standardization) and on the covariance matrix (no prior standardization). Without standardization, PCA results are inevitably dominated by the variables with highest variance; if that is desired (or at worst unproblematic), then you will not be troubled. &lt;/p&gt;&#10;&#10;&lt;p&gt;If skewness is very high, you have a choice. Often results will be clearer if PCA is applied to transformed variables. For example, the effects of outliers or extreme data points will often be muted when variables are transformed. Conversely, PCA as a transformation technique does not depend on, or assume, that any (let alone all) of the variables fed to it being normally distributed. &lt;/p&gt;&#10;&#10;&lt;p&gt;In abstraction, it is difficult to advise in detail, but it will often be sensible to apply PCA both to the original data when highly skewed and to transformed data, and then to report either or both results, depending on what is helpful scientifically or substantively. &lt;/p&gt;&#10;&#10;&lt;p&gt;PCA itself is indifferent to whether variables are transformed in the same way, or indeed to whether some variables are transformed and others are not. Whenever it makes sense, there is some appeal in transforming variables in the same way, but this is perhaps more a question of taste than of technique. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a simple example, if several variables are all measures of size in some sense, then skewness is very likely. Transforming all variables by taking logarithms (so long as all values are positive) will then often be valuable as a precursor to PCA, but neither analysis should be thought of as &quot;correct&quot;; rather they give complementary views of the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note 1: I rather doubt that you &quot;have to&quot; do PCA unless you are committed to some exercise as part of a course of study. It seems very likely that some kind of Poisson modelling would be closer to scientific goals and just as fruitful as PCA, but without detail on those goals that is a matter of speculation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note 2: In the case of positive integers, roots and logarithms both have merit as transformations. I note that you state that your data are Poisson distributed without showing any evidence. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-14T14:49:08.860" Id="82255" LastActivityDate="2014-01-14T16:07:39.813" LastEditDate="2014-01-14T16:07:39.813" LastEditorUserId="22047" OwnerUserId="22047" ParentId="82249" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;As Nick points out, there is some confusion as to whether you mean standardisation (making each variable zero mean and unit variance vectors) or a transformation to make each variable more normally distributed. As you mentioned skewed data, I will address that point.&lt;/p&gt;&#10;&#10;&lt;p&gt;The square root or log transformations may be applied to the counts to downweight the effects of extreme values, which may dominate the construction of the early axes if doing so would explain large amounts of variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;For count data though, PCA is rarely competitive in terms of variance explained. Correspondence Analysis will tend to ordinate such data as well it not better than PCA, though it is working on relative compositions (counts) not absolute-valued compositions; in PCA, the following samples with observations on 3 variables&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(1,  5, 1)&#10;y &amp;lt;- c(5, 25, 5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;would be assumed markedly different, but under CA, as these have the same relative composition, would be considered exactly equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;At this point you should be asking yourself what it is that you hope to achieve with the ordination/dimension reduction and let that lead to you an appropriate method.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-14T15:04:34.860" Id="82257" LastActivityDate="2014-01-14T15:04:34.860" OwnerUserId="1390" ParentId="82249" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="82262" AnswerCount="1" Body="&lt;p&gt;Given a distribution A with a mean of $\mu_1$ and standard deviation of $\sigma_1$, how can I generate:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Distribution B with a mean of $\mu_2$ and standard deviation of $\sigma_2$ and a correlation of $X_1$ with distribution A&lt;/li&gt;&#10;&lt;li&gt;Distribution C with a mean of $\mu_3$ and standard deviation of $\sigma_3$ and a correlation of $X_2$ with distribution B and $X_3$ with distribution A&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Can someone please tell me if this even makes sense? My naive approach was the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Generate A with the given parameters&lt;/li&gt;&#10;&lt;li&gt;Generate B with the given parameters and then see if the generated values have the specified correlation with A. If not, regenerate B until this correlation is achieved.&lt;/li&gt;&#10;&lt;li&gt;Generate C using the approach in Step 2.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;However, I am not quite sure if this approach will terminate. Is there a better way to achieve this? I'd love to see an example in R. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-14T16:22:02.543" Id="82261" LastActivityDate="2014-01-14T17:13:39.713" LastEditDate="2014-01-14T17:13:39.713" LastEditorUserId="1390" OwnerUserId="2164" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;distributions&gt;&lt;correlation&gt;&lt;normal-distribution&gt;" Title="Generating correlated distributions with a certain mean and standard deviation?" ViewCount="166" />
  <row AcceptedAnswerId="82283" AnswerCount="1" Body="&lt;p&gt;I'm watching a Machine Learning course offered by Alex Smola at &lt;a href=&quot;http://www.youtube.com/watch?v=OmJbGrQ93CA&quot; rel=&quot;nofollow&quot;&gt;http://www.youtube.com/watch?v=OmJbGrQ93CA&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;14 minutes in and im confused. At that part he starts to explain Linear Regression. I've watched other introduction to Machine Learning courses and also feel lost. I have difficulty understanding the equations even when their explained. I feel I've missed something. I understand there is no silver bullet and im not looking for one but for someone who is starting in this area and feels that the introduction is too difficult are there recommend resources I should be reading/watching to get up to speed in this area ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I find other algorithms such as k-means and hierarchical clustering much easier to understand but maybe im not understanding at good enough level if im finding this course so difficult.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2014-01-14T19:24:01.017" CreationDate="2014-01-14T16:50:05.990" FavoriteCount="1" Id="82263" LastActivityDate="2014-01-14T20:25:58.213" OwnerUserId="21817" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;" Title="Where to go for introduction to introduction to Machine Learning?" ViewCount="109" />
  
  
  <row Body="&lt;p&gt;I'm not sure that I completely understand your supervisor's suggestion, but the principle that I use when choosing how to create a graph is the make sure that the graph represents the analysis that I'm reporting in my paper.  Based on this principle, I would use whatever model to create your graph that you're reporting in your paper.   Thus, if you are reporting the following model: &lt;/p&gt;&#10;&#10;&lt;p&gt;$y = var1 + var2 + var1 * var2$&lt;/p&gt;&#10;&#10;&lt;p&gt;then I would use this model to obtain the predicted values that you plot on your graph.  On the other hand, if you are reporting the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = var1 + var2 + var3 + var4 + var5 + var6 + var7 + var8 + var9 + var1 * var2$&lt;/p&gt;&#10;&#10;&lt;p&gt;then I would plot the $var1 * var2$ interaction from this model, mean-centering var3 through var9 when you obtain the predicted values for your graph.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that the model with your control variables is the one that you are reporting in your paper, I have included some R code simulating data and creating a graph using those data below.  You may want to consider plotting your $y$ points marginalized for your various control variables; if you do not know how to do this, I describe how to accomplish this &lt;a href=&quot;http://stats.stackexchange.com/questions/71413/best-way-to-visually-present-relationships-from-a-multiple-linear-model/71440#71440&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Set the seed&#10;set.seed(2314)&#10;&#10;# Create the data&#10;dat &amp;lt;- matrix(NA, nrow = 200, ncol = 9)&#10;colnames(dat) &amp;lt;- paste0(&quot;var&quot;, 1:9)&#10;dat &amp;lt;- data.frame(dat)&#10;for(i in 1:9)&#10;{&#10;  dat[, paste0(&quot;var&quot;, i)] &amp;lt;- rnorm(200, sd = 1)&#10;}&#10;dat$y &amp;lt;- .5 * dat$var1 + .5 * dat$var2 + .5 * dat$var1 * dat$var2 + rnorm(200, sd = 1)&#10;&#10;# Fit the model&#10;mod &amp;lt;- lm(y ~ var1 * var2 + var3 + var4 + var5 + var6 + var7 + var8 + var9, data = dat)&#10;&#10;# Create a matrix of desired predicted values for the model.  I am holding the control variables&#10;# constant at their means&#10;pX &amp;lt;- expand.grid(var1 = seq(min(dat$var1), max(dat$var1), by = .1), &#10;                  var2 = c(mean(dat$var2) - sd(dat$var2), mean(dat$var2) + sd(dat$var2)),&#10;                  var3 = mean(dat$var3),&#10;                  var4 = mean(dat$var4),&#10;                  var5 = mean(dat$var5),&#10;                  var6 = mean(dat$var6),&#10;                  var7 = mean(dat$var7),&#10;                  var8 = mean(dat$var8),&#10;                  var9 = mean(dat$var9)&#10;                  )&#10;&#10;# Get the predicted values&#10;pY &amp;lt;- predict(mod, pX)&#10;&#10;# Create a plotting space&#10;plot(dat$var1, dat$y, frame = F, type = &quot;n&quot;, xlab = &quot;var1&quot;, ylab = &quot;y&quot;)&#10;&#10;# Plot the points.  Points for var1 below the median on var2 are plotted in red, &#10;# points for var1 above the median on var2 are plotted in blue&#10;points(dat[dat$var2 &amp;lt; median(dat$var2), &quot;var1&quot;], dat[dat$var2 &amp;lt; median(dat$var2), &quot;y&quot;], pch = 16, cex = .5, col = &quot;red&quot;)&#10;points(dat[dat$var2 &amp;gt;= median(dat$var2), &quot;var1&quot;], dat[dat$var2 &amp;gt;= median(dat$var2), &quot;y&quot;], pch = 16, cex = .5, col = &quot;blue&quot;)&#10;&#10;# Plot the lines. Lines are colored to be consistent with the points&#10;lines(pX[pX$var2 == mean(dat$var2) - sd(dat$var2), &quot;var1&quot;], pY[pX$var2 == mean(dat$var2) - sd(dat$var2)], col = &quot;red&quot;, lwd = 2)&#10;lines(pX[pX$var2 == mean(dat$var2) + sd(dat$var2), &quot;var1&quot;], pY[pX$var2 == mean(dat$var2) + sd(dat$var2)], col = &quot;blue&quot;, lwd = 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2zefK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-14T17:46:16.307" Id="82269" LastActivityDate="2014-01-14T20:13:00.040" LastEditDate="2014-01-14T20:13:00.040" LastEditorUserId="11091" OwnerUserId="11091" ParentId="82264" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Because contour plots--especially 3D contour plots--are usually difficult to interpret and plots of $I$ against $u$ are familiar to physicists, consider a &lt;em&gt;small multiple&lt;/em&gt; of such plots where $A$ and $B$ range through selected values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Experimentation is in order.  You might, for instance, overlay multiple graphs for fixed $A$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XUktH.png&quot; alt=&quot;A multiple&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In each of these plots $B$ varies through the sequence $(-6, -1, 0, 1, 2)$ with color denoting the value of $B$; a legend would be helpful.  (The first value of $B$ is drawn in blue; the next values are drawn in red, gold, green, and so on.)&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also overlay multiple graphs for fixed $B$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zz8BP.png&quot; alt=&quot;B multiple&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In each of these plots $A$ varies through the sequence $(-2, 0, 2, 4, 6, 8)$.  Again, a legend would help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because these tableaux convey the same information in different ways, if space is available you might publish both versions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice how, to assist visual comparison across the cells of each tableau, identical scales and ranges on the axes were used.  Sometimes values vary so much this is not feasible, in which case you need to draw the reader's attention to the changes in the scales.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another thing worth considering is how, if at all, to &lt;em&gt;standardize&lt;/em&gt; the plots.  It might be more meaningful physically, for instance, to scale them all so that the minimum $I(1/2)$ is equal to a constant value.  For other purposes you might standardize them to make their slopes at a distinguished value, such as $I^\prime(1) = 1/\left(\sqrt{e} \sqrt{A-e B+e}\right)$, equal to a constant.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-14T20:05:58.757" Id="82281" LastActivityDate="2014-01-14T20:05:58.757" OwnerUserId="919" ParentId="82115" PostTypeId="2" Score="7" />
  
  <row AnswerCount="0" Body="&lt;p&gt;When using SAS callable IVEware for multiple imputation it will occasionally throw out a warning for too many iterations.  Can someone give me an idea of how many warnings are acceptable (if any) for a particular variable?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-14T21:32:36.100" Id="82290" LastActivityDate="2014-01-14T21:51:33.423" LastEditDate="2014-01-14T21:51:33.423" LastEditorUserId="32036" OwnerUserId="28835" PostTypeId="1" Score="0" Tags="&lt;sas&gt;&lt;multiple-imputation&gt;&lt;error-message&gt;" Title="IVEware warnings" ViewCount="42" />
  
  
  
  
  <row AcceptedAnswerId="82332" AnswerCount="2" Body="&lt;p&gt;I think I have some idea about hierarchical clustering but there a few subtle questions. I use the R example below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;plot( hclust(dist(USArrests), &quot;ave&quot;) )&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What exactly does the y-axis &quot;Height&quot; mean? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Looking at North Carolina and California (rather on the left). Is California &quot;closer&quot; to North Carolina than Arizona? Can I make this interpretation? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;Hawaii (right) joins the cluster rather late. I can see this as it is &quot;higher&quot; than other states. In general how can I interpret the fact that labels are &quot;higher&quot; or &quot;lower&quot; in the dendrogram correctly? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks for any comments or references.&lt;img src=&quot;http://i.stack.imgur.com/tzoq6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-01-15T11:04:31.587" FavoriteCount="1" Id="82326" LastActivityDate="2014-12-26T19:05:51.513" LastEditDate="2014-01-15T12:46:17.923" LastEditorUserId="12147" OwnerUserId="12147" PostTypeId="1" Score="3" Tags="&lt;clustering&gt;&lt;interpretation&gt;&lt;hierarchical&gt;" Title="How to interpret the dendrogram of a hierarchical cluster analysis" ViewCount="5690" />
  <row Body="&lt;p&gt;No, you can't do it in an unsupervised way. The reason is simple: somehow you need to incorporate a rule of when two nodes should be connected; or if the final graph is weighted, you need a rule about the weights you expect given the $a(i,j)$ and $b(i,j)$. In the first case you end up doing classification, in the later regression. The missing rule is the free parameter you try to find, that is how $a$ and $b$ are related. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you can't make an arbitrary decision for the free parameter (e.g. just take their sum, that is free parameter equals to one), then you need to build a dataset with several different combinations of $a$ and $b$, the required result in each case and find the free parameter using a supervised approach. By the way, I don't know your data, but you already made some assumption about that rule: that it connects $a$ and $b$ in a linear way.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-15T11:50:30.067" Id="82333" LastActivityDate="2014-01-15T12:08:37.957" LastEditDate="2014-01-15T12:08:37.957" LastEditorUserId="37188" OwnerUserId="37188" ParentId="82311" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;If you want to minimize the number of samples, you are probably better off by estimating the $p$-value using (# positives + 1) / (# resamples + 1), see: (Davison and Hinkley 1997, chapter 4). In that case you can get a fine estimate of the Monte Carlo confidence interval using the 2.5th and 97.5th percentiles from the beta distribution with parameters # positives + 1, and # resamples + 1 - # positives. I discussed the logic behind using the beta distribution on pages 9 and 10 of &lt;a href=&quot;http://www.maartenbuis.nl/presentations/gsug13.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; presentation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Davison, A.C. and D.V. Hinkley (1997). &lt;em&gt;Bootstrap methods and their application&lt;/em&gt;. Cambridge university press.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-01-15T15:19:28.473" Id="82360" LastActivityDate="2014-01-16T08:39:11.677" LastEditDate="2014-01-16T08:39:11.677" LastEditorUserId="23853" OwnerUserId="23853" ParentId="82344" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I run a few different projects throughout India and want to understand how many unique individuals I've provided services to total. I don't collect names or unique identifiers, so all I know is a) the amount of people that have accessed services per project and b) the total population of India, which can further be refined by states or cities. However, there will be some individuals who access services from more than one project, thus, there will be overlap. What sort of statistical test can I apply to understand the probability of overlap between projects?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-15T16:04:42.013" FavoriteCount="0" Id="82368" LastActivityDate="2014-01-15T16:47:10.747" OwnerUserId="37356" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;population&gt;" Title="Finding overlap in two similar populations" ViewCount="46" />
  <row Body="&lt;p&gt;I have tried using Random Forest for multiple imputation in MICE to handle missing data in survival analysis. I used bootstrapping to account for sampling variability in the imputation models. I found that Random Forest MICE performed better than parametric MICE when there were interactions between predictor variables that were not included in the imputation model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The CALIBERrfimpute package provides a function for Random Forest imputation in MICE:&lt;br&gt;&#10;&lt;a href=&quot;http://cran.r-project.org/web/packages/CALIBERrfimpute/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/CALIBERrfimpute/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is an article describing tests of the method on simulated data and a real epidemiological dataset:&lt;br&gt;&#10;&lt;a href=&quot;http://dx.doi.org/10.1093/aje/kwt312&quot; rel=&quot;nofollow&quot;&gt;http://dx.doi.org/10.1093/aje/kwt312&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-15T21:00:01.353" Id="82395" LastActivityDate="2014-01-15T21:18:45.087" LastEditDate="2014-01-15T21:18:45.087" LastEditorUserId="37364" OwnerUserId="37364" ParentId="49270" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Perhaps a hierarchical log-linear model would work. Picture your data as a 2 x 10 x 2 array: the first dimension is agree/disagree; the second dimension contains the questions and the third dimension indexes the two groups. You fit a series of models until you get one that fits. Thus&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;condition on the margins only. This assumes independence throughout. In R, the call would be loglin(x, list(1,2,3)). &lt;/li&gt;&#10;&lt;li&gt;Condition on the margins and the relationship between answer and questions. loglin(x, list(c(1,2),3). This allows relationships between the questions and allows for different numbers in the two groups.&lt;/li&gt;&#10;&lt;li&gt;Then fit the full model, loglin(x, list(c(1,2,3)).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If the two groups behave the same way, then model 2 would be non-significant. If not, then not -- and you would need model 3, which implies that the groups differ.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This does not account for the fact the questions are nested in subjects, as noted by @Aniko, but the log-linear model accepts the pattern of responses it gets and attempts to model it across both groups. If there are clear differences between the groups with respect to how they respond, this test should pick them up.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another possibility would be to write down the likelihood under the null hypothesis of no difference between groups and the alternate. Then do a likelihood ratio test. If you are prepared to ignore correlations between questions, the likelihood would be that of a multinomial distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are concerned about correlations between the questions, you might want to go for a latent variable model with categorical predictors and look at the difference between the groups using a SEM analysis. You would want a lot of data for this and a plausible reason for a latent variable model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-15T21:41:58.980" Id="82399" LastActivityDate="2014-01-15T22:24:05.533" LastEditDate="2014-01-15T22:24:05.533" LastEditorUserId="14188" OwnerUserId="14188" ParentId="82300" PostTypeId="2" Score="0" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I major in bioinformatics. We all know that temperature changes during a year. I find that a disease incidence is really high when temperature is relatively high, while it becomes really low when the temperature is relatively low; that is to say, they are related. I want to find out a way to prove that they are related, not just intuitively feel that they are related. Any suggestions?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-16T01:51:43.220" Id="82423" LastActivityDate="2014-03-20T00:14:37.870" LastEditDate="2014-03-20T00:14:37.870" LastEditorUserId="32036" OwnerDisplayName="Vico_Wu" OwnerUserId="40307" PostTypeId="1" Score="2" Tags="&lt;seasonality&gt;&lt;incidence-rate-ratio&gt;" Title="How to prove that one variable's change is influenced by another variable; that is to say, are they related?" ViewCount="107" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;i have an experiment wherein respondents were tested in two time points. however, respondents were tested at t1 and t2 OR t1 and t3 OR t1 and t4. Hence, data is missing at t2,t3,and t4 for 3/4 of respondents. i meet the missing at random assumption, but is there anything else I need to consider before i start up the imputation machine?&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-16T10:11:56.833" Id="82444" LastActivityDate="2014-08-08T17:21:02.873" OwnerUserId="37395" PostTypeId="1" Score="0" Tags="&lt;missing-data&gt;&lt;longitudinal&gt;&lt;multiple-imputation&gt;" Title="multiple imputation for a longitudinal study" ViewCount="52" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In analyzing panel data is it important to see the correlation matrix?if so then should we have to drop those variables which has higher correlation?Is there any rule of thumb?How do I decide which variables to take as my independent and control variable?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-16T11:16:15.867" Id="82451" LastActivityDate="2014-01-16T11:16:15.867" OwnerUserId="37384" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="correlation matrix to determine independent variables" ViewCount="82" />
  <row AcceptedAnswerId="82457" AnswerCount="1" Body="&lt;p&gt;I have results from three archaeological populations in which I found that the prevalence of a certain disease was:&lt;/p&gt;&#10;&#10;&lt;p&gt;65% (13/20) in population A, &lt;/p&gt;&#10;&#10;&lt;p&gt;31.25% (5/16) in population B and &lt;/p&gt;&#10;&#10;&lt;p&gt;46.60% (48/103) in population C &lt;/p&gt;&#10;&#10;&lt;p&gt;I did a chi-squared test and found that while population A was significantly more affected than population B (χ2 = 4.05, df=1, P= 0.04), population A was not significantly more affected than population C (χ2 = 2.27, df=1, P= 0.13)&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I then learned that you can test each population (2x1 contingency table) to see if its results are significant. The sample sizes are quite small and these tests have shown that the results from each population are not significant (e.g. SPSS tells me that the fact that 5 out of 16 individuals were affected by disease in population B is not significant: χ2 = 1.80, df=1, P= 0.18) . Does then render the comparison of populations meaningless?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-16T11:54:43.393" Id="82454" LastActivityDate="2014-01-16T12:41:15.427" LastEditDate="2014-01-16T12:06:44.447" LastEditorUserId="35657" OwnerUserId="35657" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;" Title="Are my tests of significance valid?" ViewCount="60" />
  
  
  
  <row AcceptedAnswerId="82496" AnswerCount="1" Body="&lt;p&gt;I am trying to explain datasets gathered from simulation with various regression models. Linear regression seems not applicable because my data after transformations still violate&#10;the assumptions for linearity, normality of errors etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have built several non-parametric models (splines, regression trees, SVR and Gaussian processes), however I am not quite sure if I should transform the data or not for the analysis. From the theory I have so far understood that non-parametric models make no / very few assumptions about the structure of underlying data, however it is not clear to me if this means that I can apply non-parametrics to my original data without transformations of any kind. What  does the term &quot;few&quot; assumptions mean? Unfortunately I haven't managed to find something more concrete on this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another question is that splines for example derive the model with the least squares method. How is it possible that in this case no assumptions on the data are necessary as opposed to  linear regression?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-16T16:41:04.807" Id="82485" LastActivityDate="2014-01-16T17:47:46.760" LastEditDate="2014-01-16T17:25:16.773" LastEditorUserId="7290" OwnerUserId="32331" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;nonparametric&gt;&lt;data-transformation&gt;" Title="Non-parametric regression models and data transformations" ViewCount="89" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to estimate all the possible interactions of three variables with linear model in R.&#10;My dataset is like this:&lt;/p&gt;&#10;&#10;&lt;p&gt; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    a   b   c   Freq&#10;1   0   0   No  457&#10;2   1   0   No  14&#10;3   0   1   No  17&#10;4   1   1   No  2&#10;5   0   0   Yes 450&#10;6   1   0   Yes 16&#10;7   0   1   Yes 25&#10;8   1   1   Yes 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And I make a linear model in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm.model = glm (Freq ~ A * B * C, data = mytable, family = poisson)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But I have my own expected tables from randomization, so I want to feed my randomization tables into the linear model as its 'expected tables', instead of letting it generate its own expected tables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm still pretty new to linear model, so do you know how can I solve this problem in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-16T18:24:25.943" Id="82499" LastActivityDate="2014-01-16T18:58:12.943" LastEditDate="2014-01-16T18:58:12.943" LastEditorUserId="8013" OwnerUserId="37415" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;" Title="How can I feed the randomized tables into the linear model as its 'expected tables'" ViewCount="23" />
  <row AnswerCount="1" Body="&lt;p&gt;I have multiple time series where at any point in the time series, an event can occur that I believe has an effect on the time series. This event can happen at different times for each of the different time series. How would I go about estimating the effect of this event on a time series where the event has not yet occurred? Ideally this would be some function of the time series itself. Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-16T19:46:54.097" Id="82507" LastActivityDate="2015-01-14T17:30:13.033" OwnerUserId="10810" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;estimation&gt;&lt;forecasting&gt;" Title="Estimating effect of binary event on multiple time series" ViewCount="34" />
  
  <row Body="&lt;p&gt;To put @ziggystar's response in terms of machine learning jargon: the idea behind bootstrap aggregation techniques (e.g. Random Forests) is to fit many low-bias, high-variance models to data with some element of &quot;randomness&quot; or &quot;instability.&quot; In the case of random forests, instability is added through bootstrapping and by picking a random set of features to split each node of the tree. Averaging across these noisy, but low-bias, trees alleviates the high variance of any individual tree.&lt;/p&gt;&#10;&#10;&lt;p&gt;While regression/classification trees are &quot;low-bias, high-variance&quot; models, linear regression models are typically the opposite - &quot;high-bias, low-variance.&quot; Thus, the problem one often faces with linear models is reducing bias, not reducing variance. Bootstrap aggregation is simply not made to do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;An addition problem is that bootstrapping may not provide enough &quot;randomness&quot; or &quot;instability&quot; in a typical linear model. I would expect a regression tree to be more sensitive to the randomness of bootstrap samples, since each leaf typically only holds a handful of data points. Additionally, regression trees can be stochastically grown by splitting the tree on a random subset of variables at each node. See this previous question for why this is important: &lt;a href=&quot;http://stats.stackexchange.com/questions/80751/why-are-random-forests-splitted-based-on-m-random-features&quot;&gt;Why are Random Forests splitted based on m random features?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;All that being said, you can certainly use bootstrapping on linear models &lt;a href=&quot;http://www.sagepub.com/upm-data/21122_Chapter_21.pdf&quot; rel=&quot;nofollow&quot;&gt;[LINK]&lt;/a&gt;, and this can be very helpful in certain contexts. However, the motivation is much different from bootstrap aggregation techniques.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-16T20:10:03.973" Id="82511" LastActivityDate="2014-01-17T01:56:12.263" LastEditDate="2014-01-17T01:56:12.263" LastEditorUserId="35917" OwnerUserId="35917" ParentId="82503" PostTypeId="2" Score="10" />
  
  <row AcceptedAnswerId="82517" AnswerCount="3" Body="&lt;p&gt;My dataset is a biological analysis of a disease which takes 3 months to develop, and we're looking at treatment of that disease.  The data is gene expression data and we want to identify changes in gene expression over time.  This resembles a 2x2 design in which I have equal sized groups as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          trtmnt     no-trtmnt&#10;1 month     10           10&#10;3 month     10           10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After thinking about it, I'm not sure if this is really valid for a 2-way ANOVA.  The disease develops over the period of 3 months, so the 1 month subjects should look the same regardless of treatment.  There are multiple ways to look at this:&lt;/p&gt;&#10;&#10;&lt;p&gt;1)  I could compare trtmnt and no-trtmnt at the 3-month stage.  Simple two-group analysis.&#10;2)  I could look at 1 month vs 3 month for trtmnt, and then 1 month vs 3 month for no-trtmnt.  This would be 2, two-group analyses.  But then I would have to compare the two different analyses to look for gene change in common or different.  This is where it starts to resemble a 2-way ANOVA, but not really.&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate any thoughts.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:   I should have pointed out that these are animal studies, so we have 10 animals in each group. Rather than taking samples from each animal twice - once at 1 month and once at 3 months - we have 4 different groups, all of which were used for sampling. I hope that makes sense.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-16T21:13:55.813" Id="82515" LastActivityDate="2014-01-16T22:33:33.537" LastEditDate="2014-01-16T21:36:12.447" LastEditorUserId="36206" OwnerUserId="36206" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;genetics&gt;&lt;microarray&gt;" Title="Is this really suited for 2-way ANOVA?" ViewCount="70" />
  <row Body="&lt;p&gt;A 2x2 ANOVA is fine here. You are looking for an interaction. That means that the treatment effect has changed across months.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do not test simple effects because it's a pointless increase in tests that would increase Type I error or cause corrections and increase Type II. The interaction tells you what you need to know, whether the effect of treatment varies over months.&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you had records of outcome measures at 1 month for your 3 month trt group then you could do a mixed ANOVA on that group. That would be more powerful. Your N here is very small.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-16T22:33:33.537" Id="82524" LastActivityDate="2014-01-16T22:33:33.537" OwnerUserId="601" ParentId="82515" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;Are the concepts of descriptive vs inferential statistics and parametric vs non-parametric statistics orthogonal? As in, can we have a descriptive parametric statistic or a descriptive non-parametric statistic? if so what are some examples of? I know you can have an inferential parametric or non-parametric statistic. &lt;/p&gt;&#10;&#10;&lt;p&gt;My original thought was that parametric vs non-parametric statistics falls in a subcategory of inferential statistics, and that inferential statistics makes use of descriptive statistics. but some sources I read seem to say that you can have descriptive non-parametric stats.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone can shed some light on this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-17T04:50:04.653" FavoriteCount="2" Id="82552" LastActivityDate="2014-02-18T10:38:54.137" OwnerUserId="12347" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;nonparametric&gt;&lt;inference&gt;&lt;descriptive-statistics&gt;" Title="Descriptive and Inferential vs Parametric and Non-Parametric Statistics" ViewCount="753" />
  
  <row Body="&lt;p&gt;Here is &lt;code&gt;R&lt;/code&gt; example using package &lt;code&gt;MASS&lt;/code&gt; with &lt;code&gt;polr&lt;/code&gt; function (ordered factorial response) which can use logistic link or be probit.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS);&#10;recmodel=polr(recom~trust+solutions+proact+qs+es,data=surveydata,method=c(&quot;logistic&quot;));  &#10;summary(recmodel);  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think it is important to give &lt;code&gt;R&lt;/code&gt; notice that all variables are ordinal which means that it must not use them as interval variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Inference can be done as a comparison to some baseline. For example value 1 can be such.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-17T10:28:00.830" Id="82573" LastActivityDate="2014-01-17T11:46:45.013" LastEditDate="2014-01-17T11:46:45.013" LastEditorUserId="22586" OwnerUserId="28732" ParentId="82436" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Package &lt;code&gt;visreg&lt;/code&gt; can make effect plots similar to GAM (but perhaps not identical?) and does give the plot components as output as well, formatted as a list. Using plyr one can make a dataframe of the output. Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot &amp;lt;- visreg(model, type = &quot;contrast&quot;)&#10;smooths &amp;lt;- ldply(plot, function(part)   &#10;  data.frame(x=part$x$xx, smooth=part$y$fit, lower=part$y$lwr, upper=part$y$upr))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-17T10:34:01.903" Id="82574" LastActivityDate="2014-01-17T10:34:01.903" OwnerUserId="13380" ParentId="7795" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been handed a dataset with many poor variables and what appear to be outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at histograms and box plots of the distributions, many cannot even form a box plot due to the distribution.&#10;Some of the distributions are just plain terrible with a chunk of single numbers and a second chunk of single numbers with an arbitrary gap.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nonreal example, variable x1, 500 entries = &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;50 entries with value 45,&#10;200 entries with value 200, &#10;250 variables with value 400.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that these may well be genuine.&lt;/p&gt;&#10;&#10;&lt;p&gt;Others have dramatic outliers beyond an otherwise normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;One problem is that I cannot legitimately exclude the outliers (if justified I would absolutely remove them) as I do not (and cannot) find their cause—but I feel I will have to remove most anyway.&#10;Also, I can only remove the outlier, not the whole row, otherwise my data set will have very, very few members—is this acceptable?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-01-17T11:26:03.910" Id="82582" LastActivityDate="2014-01-17T11:32:56.993" LastEditDate="2014-01-17T11:32:56.993" LastEditorUserId="32036" OwnerUserId="32181" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;data-transformation&gt;&lt;outliers&gt;" Title="A dataset with some very, very poor X variables - distribution and outliers" ViewCount="67" />
  <row AnswerCount="0" Body="&lt;p&gt;Assume I have data looking like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;head(dt)&#10;&#10;      Mass    RT       A&#10;1 294.9862 6.157 2013154&#10;2 264.0223 6.202  230690&#10;3 283.9639 6.354   65035&#10;4 261.9819 6.362 1607568&#10;5 289.9770 6.480   26762&#10;6 264.0221 6.520  190739&#10;&#10;library(ggplot2)&#10;ggplot( data = dt, aes( x = RT, y = Mass ) ) +&#10;  geom_density2d( alpha = 0.8, , h = c(0.1, 1.3) ) +&#10;  geom_point( aes( colour = log2(A)), size = 5  ) +&#10;  scale_color_gradientn( &quot;ld( A )&quot;, colours= rev(heat.colors(255)) ) +&#10;  theme_bw()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8GbeY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What I want do is to group the points of my data by a kernel density method (as indicated by the density layer of the plot). But I do not really have an idea how to achieve this in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I used already &lt;code&gt;npudens&lt;/code&gt; from the &lt;code&gt;np&lt;/code&gt; package, but did not even really understand how to extract/find the density values from the resulting object.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(np)&#10;npudens( ~RT+Mass,data=test.tab )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The reason why I choose &lt;code&gt;npudens&lt;/code&gt; over &lt;code&gt;kde2d&lt;/code&gt; is, that I'd also like to provide conditional bandwidths to the density estimator, i.e. in a way that the bandwidths are linear functions of &lt;code&gt;Mass&lt;/code&gt; and &lt;code&gt;RT&lt;/code&gt; as for example 0.005% of the mass and 5% of the RT. Any help is highly appreciated. I am new to these methods and feel a little bit lost.&lt;/p&gt;&#10;&#10;&lt;p&gt;For reproduction here is the &lt;code&gt;dput&lt;/code&gt; of my data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dt &amp;lt;- structure(list(Mass = c(294.9862, 264.0223, 283.9639, 261.9819, &#10;289.977, 264.0221, 272.0176, 294.986, 264.0221, 283.9639, 261.9818, &#10;290.0401, 272.0176, 294.9861, 264.0222, 283.9636, 261.9819, 272.0176, &#10;294.9862, 283.9635, 261.9818, 289.977, 272.0173, 294.9863, 283.964, &#10;261.9818, 290.0397, 272.0174, 294.9861, 283.9641, 261.9819, 289.9764, &#10;290.0398, 272.0174, 294.9862, 264.0222, 283.9637, 261.9818, 289.9769, &#10;290.0402, 272.0174, 294.9861, 264.0223, 279.9958, 283.9637, 261.9818, &#10;289.9766, 290.0401, 272.0174, 294.9862, 264.0222, 283.9638, 261.9819, &#10;290.04, 272.0172, 294.9861, 283.9638, 261.9819, 289.9769, 290.04, &#10;272.0173, 294.9862, 264.0223, 261.9819, 290.0401, 272.0175, 294.9862, &#10;264.0224, 283.9637, 261.982, 272.0173, 294.9862, 283.9635, 261.9816, &#10;290.0402, 272.0172, 294.9861, 264.0222, 279.9961, 283.9637, 261.9815, &#10;272.0171, 294.9862, 264.0222, 283.9638, 261.9818, 272.0174, 294.9859, &#10;264.0224, 283.9638, 272.0169, 294.986, 264.0225, 283.9636, 272.017, &#10;294.9861, 264.0225, 283.9635, 272.0168, 290.0401), RT = c(6.157, &#10;6.202, 6.354, 6.362, 6.48, 6.52, 6.543, 6.153, 6.193, 6.348, &#10;6.362, 6.514, 6.54, 6.153, 6.191, 6.359, 6.368, 6.542, 6.161, &#10;6.362, 6.37, 6.479, 6.546, 6.157, 6.359, 6.371, 6.528, 6.544, &#10;6.151, 6.355, 6.363, 6.471, 6.528, 6.539, 6.159, 6.205, 6.365, &#10;6.372, 6.472, 6.529, 6.54, 6.155, 6.2, 6.204, 6.373, 6.38, 6.479, &#10;6.529, 6.539, 6.161, 6.202, 6.373, 6.386, 6.538, 6.547, 6.163, &#10;6.376, 6.387, 6.472, 6.537, 6.54, 6.156, 6.194, 6.374, 6.529, &#10;6.532, 6.155, 6.193, 6.377, 6.382, 6.532, 6.162, 6.362, 6.379, &#10;6.53, 6.534, 6.165, 6.198, 6.215, 6.387, 6.387, 6.539, 6.166, &#10;6.201, 6.378, 6.397, 6.541, 6.169, 6.208, 6.393, 6.54, 6.163, &#10;6.203, 6.384, 6.542, 6.164, 6.201, 6.401, 6.535, 6.548), A = c(2013154, &#10;230690, 65035, 1607568, 26762, 190739, 121136, 2357853, 263807, &#10;68086, 1551442, 59292, 118160, 2529240, 313389, 64018, 1519476, &#10;119259, 2984633, 80162, 1680411, 35609, 157181, 3182405, 81694, &#10;1721163, 49815, 152731, 3283306, 76425, 1623899, 35616, 45969, &#10;155354, 2859136, 630649, 80974, 1435648, 30731, 50551, 160070, &#10;3190792, 679125, 72635, 83406, 1190889, 32199, 48741, 153469, &#10;3188184, 662134, 81513, 1066302, 54851, 151038, 2883657, 68430, &#10;974675, 28558, 48020, 146510, 3052724, 679433, 1242839, 55651, &#10;137011, 3178104, 702678, 62029, 903551, 149964, 2448393, 62758, &#10;461936, 51645, 138286, 2492192, 736078, 91805, 67877, 450461, &#10;146526, 2678001, 731678, 57059, 355304, 147477, 756005, 686878, &#10;58532, 120400, 841196, 754836, 52527, 115968, 862693, 742445, &#10;53013, 111391, 68416)), .Names = c(&quot;Mass&quot;, &quot;RT&quot;, &quot;A&quot;), row.names = c(NA, &#10;-100L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-01-17T13:49:09.263" Id="82598" LastActivityDate="2014-01-17T14:03:26.173" LastEditDate="2014-01-17T14:03:26.173" LastEditorUserId="6440" OwnerUserId="6440" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;clustering&gt;&lt;kernel-density-estimate&gt;" Title="Clustering 2d data using kernel density methods" ViewCount="136" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the vectors of co-ordinates of the grid points $(x, y)$ and the joint probability values, $z$. Could you recommend a way to do marginalization for the below joint distribution? (In my problem, I have random sampled grid points from the joint distribution. Of course, if the points were lined up on straight lines then this is not so difficult).  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GeO8m.png&quot; alt=&quot;irregular grids&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-17T14:26:28.547" Id="82602" LastActivityDate="2014-01-17T14:56:06.040" LastEditDate="2014-01-17T14:56:06.040" LastEditorUserId="7290" OwnerUserId="30676" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;joint-distribution&gt;&lt;marginal&gt;" Title="Marginalization from randomly-spaced grids of the joint distribution" ViewCount="39" />
  <row AcceptedAnswerId="82608" AnswerCount="3" Body="&lt;p&gt;I'm trying to understand the result I see in my graph below. Usually, I tend to use Excel and get a linear-regression line but in the case below I'm using R and I get a polynomial regression with the command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(visual1, aes(ISSUE_DATE,COUNTED)) + geom_point() + geom_smooth()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So my questions boil down to this: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What is the gray area (arrow #1) around the blue regression line? Is this the standard deviation of the polynomial regression? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Can I say that the whatever is outside the gray area (arrow #2) is an 'outlier' and whatever falls inside the gray area (arrow #3) is within the standard deviation? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XRurN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-17T14:53:18.417" FavoriteCount="3" Id="82603" LastActivityDate="2014-08-15T15:15:49.203" LastEditDate="2014-08-15T15:15:49.203" LastEditorUserId="7290" OwnerUserId="37429" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;regression&gt;&lt;data-visualization&gt;&lt;outliers&gt;&lt;basic-concepts&gt;" Title="Understanding the confidence band from a polynomial regression" ViewCount="1377" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm interested in conducting an individual patient data (IPD) meta-analysis in which the main outcome of interest is a correlation coefficient between and IV and DV.&lt;/p&gt;&#10;&#10;&lt;p&gt;The IV is measured slightly differently across studies, and conversion to make them all match might not be possible. Clearly, I can still do a two-stage IPD meta-analysis based on correlation coefficients calculated for each study individually. I would also like to conduct a one-stage IPD analysis for a few reasons: to more flexibly model an important covariate, to assess whether the relationship between IV and DV is linear, and to model aggregation bias (per &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3463584/&quot; rel=&quot;nofollow&quot;&gt;Stewart et al. 2012&lt;/a&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;Given the heterogeneity in the IV measures, is this still possible?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-17T19:47:42.920" Id="82630" LastActivityDate="2014-02-03T21:21:03.667" OwnerUserId="11511" PostTypeId="1" Score="0" Tags="&lt;meta-analysis&gt;" Title="Conducting IPD meta-analysis when studies use different measures" ViewCount="88" />
  
  <row Body="&lt;p&gt;While there are specific methods for calculating confidence intervals for the parameters in a beta distribution, I’ll describe a few general methods, that can be used for (almost) &lt;em&gt;all sorts of distributions&lt;/em&gt;, including the beta distribution, and are easily implemented in R.&lt;/p&gt;&#10;&#10;&lt;h1&gt;Profile likelihood confidence intervals&lt;/h1&gt;&#10;&#10;&lt;p&gt;Let’s begin with maximum likelihood estimation with corresponding profile likelihood confidence intervals. First we need some sample data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Sample size&#10;n = 10&#10;&#10;# Parameters of the beta distribution&#10;alpha = 10&#10;beta = 1.4&#10;&#10;# Simulate some data&#10;set.seed(1)&#10;x = rbeta(n, alpha, beta)&#10;&#10;# Note that the distribution is not symmetrical&#10;curve(dbeta(x,alpha,beta))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rLg45.png&quot; alt=&quot;Probability density function for the beta distribution.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The real/theoretical mean is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; alpha/(alpha+beta)&#10;0.877193&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now we have to create a function for calculating the negative log likelihood function for a sample from the beta distribution, with the mean as one of the parameters. We can use the &lt;code&gt;dbeta()&lt;/code&gt; function, but since this doesn’t use a parametrisation involving the mean, we have have to express its parameters (&lt;i&gt;α&lt;/i&gt; and &lt;i&gt;β&lt;/i&gt;) as a function of the mean and some other parameter (like the standard deviation):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Negative log likelihood for the beta distribution&#10;nloglikbeta = function(mu, sig) {&#10;  alpha = mu^2*(1-mu)/sig^2-mu&#10;  beta = alpha*(1/mu-1)&#10;  -sum(dbeta(x, alpha, beta, log=TRUE))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To find the maximum likelihood estimate, we can use the &lt;code&gt;mle()&lt;/code&gt; function in the &lt;code&gt;stats4&lt;/code&gt; library:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(stats4)&#10;est = mle(nloglikbeta, start=list(mu=mean(x), sig=sd(x)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Just ignore the warnings for now. They’re caused by the optimisation algorithms trying invalid values for the parameters, giving negative values for &lt;i&gt;α&lt;/i&gt; and/or &lt;i&gt;β&lt;/i&gt;. (To avoid the warning, you can add a &lt;code&gt;lower&lt;/code&gt; argument and change the optimisation &lt;code&gt;method&lt;/code&gt; used.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we have both estimates and confidence intervals for our two parameters:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; est&#10;Call:&#10;mle(minuslogl = nloglikbeta, start = list(mu = mean(x), sig = sd(x)))&#10;&#10;Coefficients:&#10;        mu        sig &#10;0.87304148 0.07129112&#10;&#10;&amp;gt; confint(est)&#10;Profiling...&#10;         2.5 %    97.5 %&#10;mu  0.81336555 0.9120350&#10;sig 0.04679421 0.1276783&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that, as expected, the confidence intervals are &lt;em&gt;not&lt;/em&gt; symmetrical:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(1,2))&#10;plot(profile(est)) # Profile likelihood plot&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HEAeA.png&quot; alt=&quot;Profile likelihood plot for the beta distribution.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(The second-outer magenta lines show the 95% confidence interval.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Also note that even with just 10 observations, we get very good estimates (a narrow confidence interval).&lt;/p&gt;&#10;&#10;&lt;p&gt;As an alternative to &lt;code&gt;mle()&lt;/code&gt;, you can use the &lt;code&gt;fitdistr()&lt;/code&gt; function from the &lt;code&gt;MASS&lt;/code&gt; package. This too calculates the maximum likelihood estimator, and has the advantage that you only need to supply the density, not the negative log likelihood, but doesn’t give you profile likelihood confidence intervals, only asymptotic (symmetrical) confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;A better option is &lt;code&gt;mle2()&lt;/code&gt; (and related functions) from the &lt;code&gt;bbmle&lt;/code&gt; package, which is somewhat more flexible and powerful than &lt;code&gt;mle()&lt;/code&gt;, and gives slightly nicer plots.&lt;/p&gt;&#10;&#10;&lt;h1&gt;Bootstrap confidence intervals&lt;/h1&gt;&#10;&#10;&lt;p&gt;Another option is to use the bootstrap. It’s extremely easy to use in R, and you don’t even have to supply a density function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(simpleboot)&#10;&amp;gt; x.boot = one.boot(x, mean, R=10^4)&#10;&amp;gt; hist(x.boot)                # Looks good&#10;&amp;gt; boot.ci(x.boot, type=&quot;bca&quot;) # Confidence interval&#10;BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS&#10;Based on 10000 bootstrap replicates&#10;&#10;CALL : &#10;boot.ci(boot.out = x.boot, type = &quot;bca&quot;)&#10;&#10;Intervals : &#10;Level       BCa          &#10;95%   ( 0.8246,  0.9132 )  &#10;Calculations and Intervals on Original Scale&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The bootstrap has the added advantage that it works even if your data doesn’t come from a beta distribution.&lt;/p&gt;&#10;&#10;&lt;h1&gt;Asymptotic confidence intervals&lt;/h1&gt;&#10;&#10;&lt;p&gt;For confidence intervals on the mean, let’s not forget the good old asymptotic confidence intervals based on the central limit theorem (and the &lt;i&gt;t&lt;/i&gt;-distribution). As long as we have either a large sample size (so the CLT applies and the distribution of the sample mean is approximately normal) or large values of both &lt;i&gt;α&lt;/i&gt; and &lt;i&gt;β&lt;/i&gt; (so that the beta distribution itself is approximately normal), it works well. Here we have neither, but the confidence interval still isn’t too bad:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; t.test(x)$conf.int&#10;[1] 0.8190565 0.9268349&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For just slightly larges values of &lt;i&gt;n&lt;/i&gt; (and not too extreme values of the two parameters), the asymptotic confidence interval works exceedingly well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-17T21:31:54.813" Id="82640" LastActivityDate="2014-01-17T21:31:54.813" OwnerUserId="28791" ParentId="82475" PostTypeId="2" Score="12" />
  
  <row Body="&lt;p&gt;One can construct a confidence interval for the median difference using Walsh averages. It does assume that the distribution of the differences is symmetric, but it seems to be OK here.  See &lt;a href=&quot;http://www.stat.umn.edu/geyer/old03/5102/notes/rank.pdf&quot; rel=&quot;nofollow&quot;&gt;this document&lt;/a&gt; for an explanation of the procedure. It appears that somebody even wrote a &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/13714-nonparametric-statistical-toolbox/content/npar_wilcoxsr_ci.m&quot; rel=&quot;nofollow&quot;&gt;Matlab function&lt;/a&gt; for this calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT: implementation in Matlab&lt;/strong&gt;&#10;The &lt;code&gt;psignrank&lt;/code&gt; function in R calculates the cumulative distribution of the Sign-rank test statistic. If Matlab does not have that function, then you probably can't easily calculate an exact confidence interval. However you can calculate an approximate interval as described &lt;a href=&quot;http://fisher.stat.wmich.edu/joe/Stat364/srw.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. I think that is the calculation attempted at the end of the Matlab code.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-17T22:38:28.863" Id="82646" LastActivityDate="2014-01-20T13:16:40.653" LastEditDate="2014-01-20T13:16:40.653" LastEditorUserId="279" OwnerUserId="279" ParentId="82635" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="85769" AnswerCount="1" Body="&lt;p&gt;Suppose that I am building a hierarchical model of performance and the data is hierarchically structured (e.g., multiple customers rating a single salesperson). I might want to use variance pooling in this situation to allow individuals who have not had many customers yet borrow information from those who have had more customers. Then again, those with fewer customers may have less experience, and may generate fewer leads because they are poor quality salespeople in general. Here, the sample size is correlated with one or more possible predictors. How to account for this in hierarchical models, especially Bayesian ones? Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-18T08:47:27.093" Id="82657" LastActivityDate="2014-02-08T16:54:17.323" OwnerUserId="7616" PostTypeId="1" Score="3" Tags="&lt;hierarchical-bayesian&gt;&lt;pooling&gt;" Title="Variance pooling when sample size is a predictor" ViewCount="73" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to construct Bayesian network for a 800  genes(genes are my node/variables). I have only 30 cancer samples and 30 normal sample.so I want to create network for cancer samples and for the normal samples&#10;whether my data is reasonable to learn Bayesian network ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-18T08:49:14.587" FavoriteCount="1" Id="82658" LastActivityDate="2014-01-19T12:31:41.727" OwnerUserId="32397" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;bayesian&gt;&lt;bayes-network&gt;" Title="Sufficient number of sample to learn Bayesian network?" ViewCount="176" />
  <row Body="&lt;p&gt;It depends on why you are making models. Two main reasons to construct survival models are (1) to make predictions or (2) to model effect sizes of covariates.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to use them in a predictive setting in which you want to obtain an expected survival time given a set of covariates, neural networks are likely the best choice because they are universal approximators and make less assumptions than the usual (semi-)parametric models. Another option which is less popular but not less powerful is &lt;a href=&quot;ftp://ftp.esat.kuleuven.be/pub/SISTA/vanbelle/reports/10-59.pdf&quot; rel=&quot;nofollow&quot;&gt;support vector machines&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are modelling to quantify effect sizes, neural networks won't be of much use. Both Cox proportional hazards and accelerated failure time models can be used for this goal. Cox PH models are &lt;em&gt;by far&lt;/em&gt; the most widely used in clinical settings, in which the hazard ratio gives a measure of effect size for each covariate/interaction. In engineering settings, however, AFT models are the weapon of choice.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-18T11:50:42.513" Id="82662" LastActivityDate="2014-01-18T12:01:03.850" LastEditDate="2014-01-18T12:01:03.850" LastEditorUserId="25433" OwnerUserId="25433" ParentId="82659" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;As the Miller &amp;amp; Chapman (2001) paper explains in great detail, using a covariate that is correlated with the independent variables is usually a bad idea. Hence simply adding &lt;code&gt;study&lt;/code&gt; to the model will lead to wrong conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note however, that in your case only pretraining is correlated with the dv, so you could still use it as a covariate for the training sessions in the following steps:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Compute a regression with the &lt;code&gt;posttest&lt;/code&gt; scores as dv and only &lt;code&gt;study&lt;/code&gt; as iv.&lt;/li&gt;&#10;&lt;li&gt;Take the residuals of the regression and add the value of the intercept (both from the model computed in step 1).&lt;/li&gt;&#10;&lt;li&gt;Use the values from step 2 as the new &lt;code&gt;posttest&lt;/code&gt; scores (those should be now on the same scale as the pretest scores, but the influence of &lt;code&gt;study&lt;/code&gt; should be removed) and run the same ANOVA as your first one.&lt;/li&gt;&#10;&lt;li&gt;Note that your df for posttest are now one off, so you need to take this manually into account by subtracting 1 from all effects involving posttest (note that this most likely is not trivial as you also need to correctly apply the df-corrections, so for all effects involving &lt;code&gt;probtype&lt;/code&gt; you cannot simply subtract one, but you need to subtract one from the uncorrected df and then need to obtain the correct Greenhouse-Geisser epsilon and multiply the new df with it).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;As the saying goes, the implementation of this is left as an exercise to the reader.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-18T15:06:19.330" Id="82672" LastActivityDate="2014-01-18T15:06:19.330" OwnerUserId="442" ParentId="82642" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Generate data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(28)&#10;fakeData &amp;lt;- data.frame(ID = rep(1:10, each = 6),&#10;          months = rep(round(runif(10, min = 60, max = 80)), each = 18),&#10;          similarity = rep(c(&quot;Sim&quot;, &quot;Dissim&quot;), each = 3),&#10;          response = sample(c(0, 1), 60, replace = TRUE))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Fit initial model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;fakeModel &amp;lt;- glmer(response ~ months * similarity + (similarity | ID), &#10;                   family = binomial, data = fakeData)&#10;summary(fakeModel)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I'm going to write a generic function that replicates your data-generation code above.&#10;(I'm a little bit nervous about the way you generated your data by implicit replication of&#10;the vectors -- I would prefer to explicitly write out the expected amount of replication for&#10;each column.  I'm not sure that the code below will work properly if you change the numbers ...)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;datfun &amp;lt;- function(nindiv=10,nperindiv=6,&#10;                   nsim=3, monthmin=60, monthmax=80) {&#10;      data.frame(ID=factor(rep(1:nindiv, each=nperindiv)),&#10;      months = rep(round(runif(nindiv, min=monthmin, max=monthmax)),&#10;                    each=nsim*nperindiv),&#10;      similarity = rep(c(&quot;Sim&quot;,&quot;Dissim&quot;), each=nsim),&#10;      response = sample( 0:1, nindiv*nperindiv, replace=TRUE))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Once you've done this, however, it's easy (using the development version of &lt;code&gt;lme4&lt;/code&gt;) to&#10;simulate new responses with the same parameters as the initial model (or to change the parameters as you choose):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;simdat &amp;lt;- simulate(formula(fakeModel), newdat=datfun(),&#10;                   newparam=list(theta=getME(fakeModel,&quot;theta&quot;),&#10;                                 beta=getME(fakeModel,&quot;beta&quot;)),&#10;                   family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result is a data frame with 1 column.  If you want to do this multiple times per data set size you may want to set &lt;code&gt;nsim&lt;/code&gt; larger ...&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-18T19:00:34.167" Id="82685" LastActivityDate="2014-01-18T19:00:34.167" OwnerUserId="2126" ParentId="82615" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a multiple regression model with 4 independent variable. I want to test heteroscedasticity with the Breusch-Pagan test. If I do a studentized BP test the p-value is 0,173, with a non-studentized test, it is 0,89.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-18T23:33:01.380" FavoriteCount="1" Id="82695" LastActivityDate="2014-01-18T23:33:01.380" OwnerUserId="37461" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;heteroscedasticity&gt;" Title="What is the difference between the studentized and non-studentized Breusch-Pagan test?" ViewCount="37" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to compute the maximum likelihood estimator of a general two-dimensional discrete probability $p_{ij}$ with $i=1,\ldots,N$ and $j=1,\ldots,N$ given a set of counts $c_{ij}$ and the marginal probabilities $\sum_j p_{ij} = p^{(x)}_i$ and $\sum_i p_{ij} = p^{(y)}_j$.  The values $c_{ij}$, $p^{(x)}_i$ and $p^{(y)}_j$ are inputs.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know if there is a closed form solution to the above problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have worked on the problem using Lagrange multipliers.  The objective function is &#10;$$&#10;F\left(\{p_{ij}\}, \{\lambda^{(x)}_i\}, \{\lambda^{(y)}_j\}\right) = \sum_{ij} c_{ij} \log p_{ij} - &#10;    \sum_{i} \lambda^{(x)}_i \left(\sum_j p_{ij} - p^{(x)}_i\right) -&#10;    \sum_{j} \lambda^{(y)}_j \left(\sum_i p_{ij} - p^{(y)}_j\right).&#10;$$&#10;The $p_{ij}$ derivatives result in the equations $p_{ij} = \frac{c_{ij}}{\lambda^{(x)}_i+\lambda^{(y)}_j}$ for all $i,j$.  Substituting these equations into the $\lambda$ derivative conditions gives&#10;$$&#10;\sum_j \frac{c_{ij}}{\lambda^{(x)}_i+\lambda^{(y)}_j} = p^{(x)}_i \text{ for all $i$}&#10;$$&#10;and &#10;$$&#10;\sum_i \frac{c_{ij}}{\lambda^{(x)}_i+\lambda^{(y)}_j} = p^{(y)}_j \text{ for all $j$}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not know how to solve the above equations except by numerical methods.  I would like a closed form if possible.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-19T02:10:29.413" Id="82704" LastActivityDate="2014-01-19T06:11:33.143" OwnerUserId="27307" PostTypeId="1" Score="2" Tags="&lt;maximum-likelihood&gt;&lt;marginal&gt;" Title="Maximum likelihood estimator of discrete distribution with known marginals" ViewCount="55" />
  
  <row Body="&lt;p&gt;Typically decision trees (for instance C4.5, implemented as J48 in Weka you used) are non parametric, that is they don't make any assumption regarding the distribution of the data. As long as the normalization doesn't change the ranks of the data (and I know of no normalization that does that), the results will be exactly the same (you will only get different splitting levels).&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course this doesn't hold for algorithms making parametric assumptions (logistic regression, etc.) So you shouldn't always normalize, but you should decide to do it or not depending on your algorithm.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-19T10:49:46.123" Id="82730" LastActivityDate="2014-01-19T10:49:46.123" OwnerUserId="36682" ParentId="82726" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Using a synthetic, two dimensional dataset of 200 points, euclidean distance and complete linkage I am not able to reproduce the discrepancies which you encountered.&#10;Also the &lt;code&gt;clusterCrit&lt;/code&gt; package and another implementation return the same values&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; # fpc&#10;&amp;gt; ch1 &amp;lt;- calinhara(X, pc, cn=max(pc))&#10;&amp;gt; # clusterSim&#10;&amp;gt; ch2 &amp;lt;- index.G1 (X,pc,d=NULL,centrotypes=&quot;centroids&quot;)&#10;&amp;gt; # clusterCrit&#10;&amp;gt; ch3 &amp;lt;- as.numeric(intCriteria(X,pc,&quot;Calinski_Harabasz&quot;))&#10;&amp;gt; &#10;&amp;gt; cat('fpc: ', ch1, '\nclusterSim: ', ch2, '\nclusterCrit: ', ch3)&#10;fpc:  369.0315 &#10;clusterSim:  369.0315 &#10;clusterCrit:  369.0315&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Python&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; itn.calinski_harabasz(X, pc)&#10;369.0315384638188&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-19T12:22:22.193" Id="82733" LastActivityDate="2014-01-19T12:22:22.193" OwnerDisplayName="user35349" ParentId="15839" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;To the best of my &lt;a href=&quot;http://www.jehps.net/Decembre2008/Bock.pdf&quot; rel=&quot;nofollow&quot;&gt;knowledge&lt;/a&gt;, the name &#10;'k-means' was first used in &lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.bsmsp/1200512992&quot; rel=&quot;nofollow&quot;&gt;MacQueen (1967)&lt;/a&gt;. The name refers to the improved algorithm proposed in that paper and not to the original one. Section 3 of that paper contains an application &#10;(which is missing from earlier papers such as Steinhaus (1956)). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;J. MacQueen (1967). Some methods for classification and analysis of multivariate observations. Proc. Fifth Berkeley Symp. on Math. Statist. and Prob., Vol. 1 (Univ. of Calif. Press, 1967), 281--297.&lt;/li&gt;&#10;&lt;li&gt;Steinhaus (1956). Sur la division des corps mat ́eriels en parties.&#10;Bulletin de l’Académie Polonaise des Sciences, Classe III, vol. IV, no. 12, 801-804.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-01-19T13:06:49.200" Id="82740" LastActivityDate="2014-01-19T13:06:49.200" OwnerUserId="603" ParentId="82709" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Although ttnphns's comment is slightly in jest - it actually has bearing on your question. We may consider different phenomenon as being caused by a set of related factors (which may or may not be measured). So for example say we have a latent factor of $\lambda$ that affects responses to a set of Likert items on a survey.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{align*}&#10;  y_1 = 0.5\lambda + e \\&#10;  y_2 = 0.7\lambda + e \\&#10;  y_3 = 0.6\lambda + e &#10;\end{align*}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;In this example $y_1$, $y_2$ and $y_3$ will all have a positive correlation because they are all related the same way through $\lambda$. For many datasets it may be that many of the items have some variable that is underlying in common. For example in the vitamin and mineral contents if the food samples are of different size I would expect more vitamins and minerals for larger food samples, making the marginal correlations of each positively correlated. Another explanation might be producers that intentionally increase vitamin content also increase mineral content (as they aren't really competing with one another and may be marketed as healthy foods).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of Likert items, as Peter Flom stated in a comment, we typically construct the survey to identify these underlying latent factors, so it is by construction that many items are positively correlated. Also the anchors are somewhat arbitrary, but questions stated positively (e.g. &quot;Do you support the death penalty?&quot;) tend to be measured more accurately than negated questions (e.g. &quot;Do you not support the death penalty?&quot;). It is also the case that you could assign different numeric values to the Likert items, but it is typical to have a scale of $1$ to $n$ (with $n$ being the different potential responses) as the default for coding the values. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note you could arbitrarily flip this coding though, so if all of the correlations in the sample were positive, you could flip half the variables so the correlations were equal. Often times there is an arbitrariness in how we represent values, e.g. if you have a nominal category of men and women you could set $\text{men} = 1$ and $\text{women} = 0$ or you could do it the obverse way. Again people may make these arbitrary coding decisions to make items appear to have positive correlations.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-01-19T15:03:03.073" Id="82744" LastActivityDate="2014-01-20T14:10:08.623" LastEditDate="2014-01-20T14:10:08.623" LastEditorUserId="17230" OwnerUserId="1036" ParentId="82729" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;If you just want to know the difference between male and female on these three scores, then you don't need any tests. If you want to assess whether the difference is statistically significant, you  could use 3 t tests (if the assumptions are met) or a nonparametric equivalent (if they are not).&lt;/p&gt;&#10;&#10;&lt;p&gt;You would use MANOVA if you were also interested in accounting for the relationships among the three dependent variables. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-19T16:02:15.470" Id="82747" LastActivityDate="2014-01-19T16:02:15.470" OwnerUserId="686" ParentId="82746" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In bayesian analysis using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Inverse-gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;inverse Gamma distribution&lt;/a&gt; as a prior for the precision (the inverse of the variance) is a common choice. Or the inverse Wishart distribution for multivariate models. Adding a prior on the variance improves robustness against outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a nice paper by Andrew Gelman: &quot;Prior distributions for variance parameters in hierarchical models&quot; where he discusses what good choices for the priors on the variances can be.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-19T16:38:07.000" Id="82749" LastActivityDate="2014-01-19T16:38:07.000" OwnerUserId="17908" ParentId="82128" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I suggest you to put a exact p-value for the testing you've done. However if the p-value is very small, e.g. 0.000001, then I would write it as p-value &amp;lt; 0.0001. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-19T17:45:27.867" Id="82755" LastActivityDate="2014-01-19T17:45:27.867" OwnerUserId="37518" ParentId="82754" PostTypeId="2" Score="1" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I do think that those 90 columns of binary data can be reduced to some smaller number of columns, so that the computational time can be reduced significantly&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This assumption seems unfounded to me. Since the computational time of calculating pairwise dissimilarities corresponds &lt;code&gt;O(n^2)&lt;/code&gt; the &lt;strong&gt;effect of dimensionality reduction will be merely noticeable&lt;/strong&gt;. I mean if it takes two days, you wouldn't mind 2-3 hours less.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I have in mind is&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Do you really need all pairwise dissimilarities? Often one actually doesn't. Therefore a spatial index can be used.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you do need them all: Do you need to update them often? How about keeping the dissimilarities. Adding a single item will take few time&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Anyway you should try to reformulate the problem. 10 or 100 variables will make little difference in accomplishing your current approach.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-19T22:00:16.047" Id="82769" LastActivityDate="2014-01-19T22:00:16.047" OwnerDisplayName="user35349" ParentId="74505" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;According to &lt;a href=&quot;http://www.guttmacher.org/pubs/fb_contr_use.html&quot; rel=&quot;nofollow&quot;&gt;multiple&lt;/a&gt; &lt;a href=&quot;http://www.cdc.gov/reproductivehealth/unintendedpregnancy/contraception.htm&quot; rel=&quot;nofollow&quot;&gt;sources&lt;/a&gt;, the probability that, after one year of typical use, a woman who uses male condoms will get pregnant is $18\%$. Presumably, this assumes that the woman will have sex at an average frequency, but I found it hard to find information about this.&lt;/p&gt;&#10;&#10;&lt;p&gt;What's surprising is that I calculated the probability of unintended pregnancy over four years using male condoms to be very close to $55\%$. I calculated this by noting that, in order not to be pregnant after four years, contraception has to be successful $4$ times. In other words, the probability of successful contraception over four years is $(1 - .18)^4 = .4521$. Therefore, the probability of contraception failing and producing a pregnancy sometime during those four years is $54.79\%$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this technique sound?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-20T00:25:17.360" Id="82777" LastActivityDate="2014-01-20T01:24:05.437" LastEditDate="2014-01-20T01:08:02.577" LastEditorUserId="22047" OwnerUserId="11101" PostTypeId="1" Score="1" Tags="&lt;probability&gt;" Title="Failure Rate of Contraception over n years" ViewCount="44" />
  <row AnswerCount="1" Body="&lt;p&gt;Does anyone know, considering an ARIMAX model that fitting a stationary process Y, then do the exogenous components for the model need to be (weakly) stationary?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think exogenous components can be any process, even non-deterministic ones, am I right?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-20T01:32:58.307" Id="82782" LastActivityDate="2015-02-11T17:21:49.760" LastEditDate="2014-07-03T08:07:56.087" LastEditorUserId="2116" OwnerUserId="18492" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;arima&gt;" Title="ARIMAX model's exogenous components?" ViewCount="124" />
  <row Body="&lt;p&gt;A part of this answer that I've learned since asking is that not binning and binning seeks to answer two slightly different questions - &lt;em&gt;What is the incremental change in the data?&lt;/em&gt; and &lt;em&gt;What is the difference between the lowest and the highest?&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Not binning says &quot;this is a quantification of the trend seen in the data&quot; and binning says &quot;I don't have enough information to say how much this changes by each increment, but I can say that the top is different from the bottom&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-20T02:31:42.353" Id="82785" LastActivityDate="2014-01-20T02:31:42.353" OwnerUserId="29547" ParentId="68834" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="82816" AnswerCount="1" Body="&lt;p&gt;Let $X$ have the pdf $f_X (x;\theta)=1/(2\theta)$ for $-\theta&amp;lt;x&amp;lt;\theta$, zero elsewhere where $\theta&amp;gt;0$ Is the statistic $Y=|X|$ a sufficient statistic for $\theta$?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My book claims it is but I do not see it. A transformation argument shows that the distribution of $|X|$ is uniform with $f_X (x;\theta)=1/\theta$ for $0&amp;lt;x&amp;lt;\theta$. Thus the conditional probability&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta)}{f_{|X|}(|X|;\theta)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;does&lt;/em&gt; depend on $\theta$ and therefore, $|X|$ cannot be a sufficient statistic for $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there something that I am doing wrong here? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-01-20T11:19:37.317" Id="82811" LastActivityDate="2014-01-20T12:10:38.137" LastEditDate="2014-01-20T11:25:58.323" LastEditorUserId="31420" OwnerUserId="31420" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;conditional-probability&gt;&lt;sufficient-statistics&gt;" Title="Sufficient Statistic for Uniform" ViewCount="117" />
  
  <row Body="&lt;p&gt;Going from your comment-&lt;em&gt;Which areas and habitats are preferred by males and which are preferred by females?&lt;/em&gt; Here you probably want regression with scat density as the DV and sex and habitat as IVs (and possibly their interaction). However, if you have repeated measures over time on the same animals, you might need a multilevel model (this assumes you have scat density and can tell which scat is from males and females). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;-Are there any differences or similarities?&lt;/em&gt; This can be looked at with descriptive statistics&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Sex, besides being a nominal variable, would also be a dependent one.&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Sex cannot be a DV. Sex is not caused by habitat! (Except, I suppose, for some hermaphroditic animals)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-20T12:38:58.783" Id="82819" LastActivityDate="2014-01-20T12:38:58.783" OwnerUserId="686" ParentId="82723" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;What is the probability of absorption at $ 0 $, as a function of position $ x $, for a 1D random walk (on $ \mathbb{Z} $) with asymmetric step sizes?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, suppose that you can take two steps to the right with probability $ p $, or one step to the left with probability $ q=1-p $. I calculated the probabilities for $ x&amp;gt;0 $ via difference equations*, but I'm stuck for the case $ x&amp;lt;0 $. Obviously it matters whether $ x $ is even or odd; in the case that $ p=1 $ the probabilities should bounce between $ 0 $ (if $ x $ odd) and $ 1 $ (if $ x $ even).&lt;/p&gt;&#10;&#10;&lt;p&gt;*For $ x&amp;gt;0 $, I found $ P(x)=\left(\frac{-p+\sqrt{4p-3p^{2}}}{2p}\right)^{x} $.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-20T18:11:35.230" Id="82849" LastActivityDate="2014-01-21T08:51:44.533" LastEditDate="2014-01-21T08:51:44.533" LastEditorUserId="35396" OwnerUserId="35396" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;stochastic-processes&gt;&lt;linear-algebra&gt;&lt;random-walk&gt;" Title="Absorption probability in 1D RW with asymmetric step sizes, $ x&lt;0 $" ViewCount="29" />
  
  <row AnswerCount="0" Body="&lt;p&gt;If $X^\dagger$ is the pseudo-inverse of $X$, $\beta = X^\dagger y$ is the least squares solution for $\beta$ when $y=X\beta$. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the overdetermined case, applying $X^{\dagger,L} = (X^TX)^{-1}X^T$ corresponds to the minimum norm residual (superscript $L$ used to indicate left inverse).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the underdetermined case, applying $X^{\dagger,R} = X^T(XX^T)^{-1}$ corresponds to the minimum norm $\beta$ (superscript $R$ used to indicate right inverse).&lt;/p&gt;&#10;&#10;&lt;p&gt;If I solve for $\beta$ in an underdetermined system using $X^{\dagger,L}$ rather than $X^{\dagger,R}$, what does the solution correspond to, conceptually?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-21T00:47:17.583" Id="82875" LastActivityDate="2014-01-21T02:02:05.040" LastEditDate="2014-01-21T02:02:05.040" LastEditorUserId="37561" OwnerUserId="37561" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;linear-model&gt;&lt;least-squares&gt;&lt;matrix-inverse&gt;&lt;underdetermined&gt;" Title="Interpretation of regression coefficients obtained from applying left inverse of regressor matrix in an underdetermined system?" ViewCount="83" />
  
  
  <row Body="&lt;p&gt;Combining two normally distributed dataset into one, can result in a not-normally distributed dataset. This is best illustrated by an example:&lt;/p&gt;&#10;&#10;&lt;p&gt;(supposing you're working with R)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# creat two normal distributed datasets&#10;x &amp;lt;- data.frame(var=rnorm(5000,2,1)) # 5000 observations, mean = 2, sd = 1&#10;y &amp;lt;- data.frame(var=rnorm(5000,6,1)) # 5000 observations, mean = 6, sd = 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When you make density plots the look like this for x and y respectively:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/uURBk.png&quot; alt=&quot;X density plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3Zexf.png&quot; alt=&quot;Y density plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Both look rather normally distrubed. Now combine the two datasets.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# combining the datasets&#10;z &amp;lt;- rbind(x,y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When you make a density plot of &lt;code&gt;z&lt;/code&gt;, you can see that it doesn't look like a normal distribution at all:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KoeN6.png&quot; alt=&quot;Z density plot&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-21T10:05:29.480" Id="82901" LastActivityDate="2014-01-21T10:34:27.053" LastEditDate="2014-01-21T10:34:27.053" LastEditorUserId="22387" OwnerUserId="22387" ParentId="82895" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;was wondering if someone could help clarify an ANCOVA for me, or maybe post me in the right direction.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a set of outcome measures that have been administered pre treatment and post treatment. I do not have a control group. My data is in a long format (i.e. each individual has data on 2 rows, one for pre treatment one for post treatment). &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to do an ANCOVA with baseline scores as a covariate. Time is my fixed factor. I've been told to have my data in the long format (by someone quite senior), but I'm wondering whether I need to transform it into wide format, and then use outcome score as my DV. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using SPSS for the analysis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-21T15:09:25.703" Id="82916" LastActivityDate="2014-01-21T15:55:52.190" LastEditDate="2014-01-21T15:34:28.017" LastEditorUserId="11091" OwnerUserId="37611" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;ancova&gt;" Title="Data formatting for an ANCOVA in SPSS" ViewCount="83" />
  <row Body="&lt;p&gt;Broadly, data can be formatted in &quot;long&quot; format (in which each &quot;person&quot; can have multiple rows) or &quot;wide&quot; format (in which each &quot;person&quot; can have only one row).&lt;/p&gt;&#10;&#10;&lt;p&gt;When data are in long format, you must differentiate observations from the same person through the values in another variable.  So, in repeated measures data where you measure &lt;code&gt;var&lt;/code&gt; at three time points, you might have a &lt;code&gt;time&lt;/code&gt; variable to tell you what makes the three measures on &lt;code&gt;var&lt;/code&gt; from the same person different from each other.  Here is an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id time        var&#10; 1    1  1.3178821&#10; 1    2 -0.5910276&#10; 1    3  2.2601515&#10; 2    1 -0.2065183&#10; 2    2 -0.5657661&#10; 2    3 -0.9088965&#10; 3    1 -1.6664465&#10; 3    2  0.7458109&#10; 3    3 -0.7942371&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When data are in wide format, column names play the role that the &lt;code&gt;time&lt;/code&gt; variable once did.  So, instead of having three columns named &lt;code&gt;var&lt;/code&gt; for the three observations on &lt;code&gt;var&lt;/code&gt; from the same person, you need to name your three variables &lt;code&gt;var1&lt;/code&gt;, &lt;code&gt;var2&lt;/code&gt;, and &lt;code&gt;var3&lt;/code&gt;.  Here are the same data that I showed above in wide format:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id       var1       var2       var3&#10; 1  1.3178821 -0.5910276  2.2601515&#10; 2 -0.2065183 -0.5657661 -0.9088965&#10; 3 -1.6664465  0.7458109 -0.7942371&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The benefit of long format data is that this formatting is very intuitive, allows for unequal spacing between whatever variable is used to differentiate the measurements on &lt;code&gt;var&lt;/code&gt; (i.e., I could have used values of $1$, $2$, and $5$ instead of $1$, $2$, and $3$), and allows for different variable values for different people (i.e., person 1 could have had the values of $4$, $5$, and $6$, while everyone else could have had $1$, $2$, and $3$).  The benefit of wide format data is that it is more compact, at least in the situations where you can use this formatting.&lt;/p&gt;&#10;&#10;&lt;p&gt;Different software packages require different data formats.  In general, software packages specialized for multi-level modeling require long format data, while standard linear model / regression packages require wide format data.  So, yes, you should format your data in wide format to conduct your ANCOVA.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-21T15:45:48.493" Id="82919" LastActivityDate="2014-01-21T15:55:52.190" LastEditDate="2014-01-21T15:55:52.190" LastEditorUserId="11091" OwnerUserId="11091" ParentId="82916" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="83023" AnswerCount="2" Body="&lt;p&gt;So I've been playing around with SVMs and I wonder if this is a good thing to do:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a set of continuous features (0 to 1) and a set of categorical features that I converted to dummy variables. In this particular case, I encode the date of the measurement in a dummy variable:&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 3 periods that I have data from and I reserved 3 feature numbers for them:&lt;/p&gt;&#10;&#10;&lt;p&gt;20:&#10;21:&#10;22:&lt;/p&gt;&#10;&#10;&lt;p&gt;So depending on which period the data comes from, different features will get 1 assigned; the others will get 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Will the SVM work properly with this or this is a bad thing to do?&lt;/p&gt;&#10;&#10;&lt;p&gt;I use SVMLight and a linear kernel.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-21T16:42:07.537" FavoriteCount="3" Id="82923" LastActivityDate="2014-01-23T08:49:07.037" LastEditDate="2014-01-23T08:49:07.037" LastEditorUserId="22047" OwnerUserId="37616" PostTypeId="1" Score="2" Tags="&lt;categorical-data&gt;&lt;svm&gt;&lt;feature-selection&gt;&lt;linear-model&gt;&lt;feature-construction&gt;" Title="Mixing continuous and binary data with linear SVM?" ViewCount="1079" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a 2-class problem involving many features. Does a linear support vector machine (SVM) classifier only take into account the values of these features and nothing more? Does it see the relationships between the variables? For example, does it work like this: if feature number 80 is &quot;0&quot;, then feature 2: needs to be over 0.2 for class 1 and if feature number 80 is &quot;1&quot; then feature 2: needs to be over 0.8 to be in class 1? Does it do it with many variables at once, and do the values of other variables influence how the SVM is influenced by other variables?&lt;/p&gt;&#10;" ClosedDate="2014-01-21T17:38:19.460" CommentCount="0" CreationDate="2014-01-21T16:47:48.513" Id="82924" LastActivityDate="2014-01-21T17:38:31.317" LastEditDate="2014-01-21T17:37:57.553" LastEditorUserId="8580" OwnerUserId="37616" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;svm&gt;&lt;kernel&gt;&lt;feature-construction&gt;" Title="How does a linear SVM work?" ViewCount="74" />
  <row Body="&lt;p&gt;(I don't know if it is proper to answer old questions, especially with a shaky answer, but here goes.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm only learning mixed models myself, but I think you are not doing anything wrong. The parameter estimates are simply calculated in a way that one level of a categorical variable becomes the reference point to which every other level is compared. I think there is a way to change the reference point (at least change from last to first, not sure if you can pick it freely), but in case you want to do multiple comparisons between levels, the answer is not to change the reference point, but to do separate pairwise comparisons. In your case, it would mean adding the following row to the syntax:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/EMMEANS=TABLES(genotype*medication) COMPARE(genotype)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(or perhaps COMPARE(medication), if you prefer them shown in another order).&lt;/p&gt;&#10;&#10;&lt;p&gt;With EMMEANS you see the estimates in a more natural form (and not in form of effects relative to intercept) and can do pairwise comparisons between individual levels. However, if there are many comparisons, you should remember to do some kind of correction, possibly with a false discovery rate procedure (&lt;a href=&quot;http://en.wikipedia.org/wiki/False_discovery_rate&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/False_discovery_rate&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-21T16:49:09.657" Id="82925" LastActivityDate="2014-01-21T16:49:09.657" OwnerUserId="35115" ParentId="51584" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am reading through the &lt;a href=&quot;http://courses.cs.washington.edu/courses/cse312/11wi/slides/12em.pdf&quot; rel=&quot;nofollow&quot;&gt;EM-Algorithm&lt;/a&gt; but on the slide 39, I don't get how &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(D) = P(D|A)P(A) + P(D|B)P(B)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to understand it in order to get my head around modelling with mixture of Gaussians. Can anyone explain me how to get that result?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-21T17:01:34.163" Id="82929" LastActivityDate="2014-01-21T23:44:23.480" LastEditDate="2014-01-21T23:44:23.480" LastEditorUserId="88" OwnerUserId="37618" PostTypeId="1" Score="2" Tags="&lt;expectation-maximization&gt;&lt;gaussian-mixture&gt;" Title="Confusion about the EM algorithm" ViewCount="69" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am doing Survival Analysis with interval-censored data and I want to apply Turnbull's nonparametric estimator to the analysis of the covariates (suggested by Turnbull (1976)). &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know if there anyone knows any statistical package in R wich produces the survival curve estimate based on Turnbull´s algorithm.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-21T19:12:30.383" Id="82948" LastActivityDate="2014-10-14T18:25:43.393" OwnerUserId="35001" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;estimation&gt;&lt;survival&gt;" Title="How to use Turnbull's nonparametric estimator for interval-censored data in R" ViewCount="205" />
  <row AnswerCount="0" Body="&lt;p&gt;My background is computer science. I am fairly new to monte carlo sampling methods and, although I understand the math, I have hard time coming up with intuitive examples for importance sampling. More precisely, could someone provide examples of:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;an original distribution one cannot sample from but one can estimate&lt;/li&gt;&#10;&lt;li&gt;an importance distribution which can be sampled from and adequate for this original distribution.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" ClosedDate="2014-01-21T21:54:25.687" CommentCount="0" CreationDate="2014-01-21T18:00:51.887" FavoriteCount="0" Id="82955" LastActivityDate="2014-01-21T21:41:14.493" LastEditDate="2014-01-21T21:29:15.470" LastEditorUserId="7071" OwnerDisplayName="James" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;" Title="Intuitve examples of importance sampling" ViewCount="24" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let's say I'm constructing a linear model with the intention of predicting automobile sales volume. Let's say that the consumer auto purchasing cycle takes 4 months, and so we'd 'lag' each observation by four data points. So if out data was monthly, out data may end up looking like the following&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;month   lag_month     sales      visits       gas price&#10;jan      &#10;feb&#10;march&#10;april    jan           500        50000        3.55&#10;may      feb           550        45000        3.87&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Given the lag in consumer shopping behavior, I want/'need' to lag the variables. However, I also feel like I have to lag the variables for the sake of prediction. Let's say I run a regression using this data and get the following estimates, I could input feb numbers to prediction sales four months from now, yes/no? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sales = -0.05 + 2*(visits) + .0.35*(gas)&#10;sales = -0.05 + 2*(550) + .0.35*(3.87)  - four month lag to predict four months from now&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The question I have relates to the process. This process seems incredibly poor (statistically unsound) and I'm wondering what are the problems with utilizing such an approach (four month lag). What are alternatives to lagging when the goal is prediction?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-21T22:39:59.253" Id="82960" LastActivityDate="2014-01-21T23:45:23.860" LastEditDate="2014-01-21T23:45:23.860" LastEditorUserId="88" OwnerUserId="29612" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;linear-model&gt;" Title="Problems and alternatives to using a four month lag in a linear regression model" ViewCount="68" />
  <row Body="&lt;p&gt;Gibbs and MCMC methods are sampling methods. EM methods are optimisation methods. Two very different things. The first ones sample from the posterior distribution, the other one finds a maximum/minimum.&lt;/p&gt;&#10;&#10;&lt;p&gt;EM algorithms are used whenever you can conduct the E and M steps relatively easy. Otherwise, you usually go for some other sort of optimisation method such as the one you mention.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-22T00:35:32.353" Id="82966" LastActivityDate="2014-01-22T00:35:32.353" OwnerUserId="37643" ParentId="82946" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Since you're looking for a pdf of a fixed parameter given some data, you could go with Bayesian analysis: You can use either an improper prior (Jeffrey's prior) or an informative prior if you have some idea of where the variance is. Then you multiply this by the likelihood of observing the sample statistic given a particular value of the variance. Finally, you normalise this to get a posterior distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is too mathematically tedious, you can also create a &quot;confidence distribution&quot; for the true variance, which is just the left-sided p-value of your test statistic as a function of the null hypothesis for variance. Take the derivative of this to get the pdf.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-22T00:47:55.163" Id="82967" LastActivityDate="2014-01-22T00:47:55.163" OwnerDisplayName="user31668" ParentId="82914" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="82983" AnswerCount="1" Body="&lt;p&gt;I have a unix shell data. Each session starts with &lt;code&gt;&amp;lt;begin&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt;. Between &lt;code&gt;&amp;lt;begin&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;end&amp;gt;&lt;/code&gt;, the info contains users command in the shell session. From this data, I need to figure out how many users are on the system with probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the is something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;lt;begin&amp;gt;&#10;pwd&#10;cd ~&#10;&amp;lt;end&amp;gt;&#10;&amp;lt;begin&amp;gt;&#10;ls&#10;vi&#10;&amp;lt;end&amp;gt;&#10;&amp;lt;begin&amp;gt;&#10;cat&#10;ls&#10;pwd&#10;&amp;lt;end&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have put this data into this type of data frame:&lt;/p&gt;&#10;&#10;&lt;p&gt;head(result,20)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  user  dat&#10;     1  elm&#10;     2 gzip&#10;     2  &amp;lt;1&amp;gt;&#10;     2   ls&#10;     2   ci&#10;     2   -l&#10;     2  &amp;lt;1&amp;gt;&#10;    2   ci&#10;    2   -l&#10;    2  &amp;lt;1&amp;gt;&#10;    2   ci&#10;    2   -l&#10;    2  &amp;lt;1&amp;gt;&#10;    2   ci&#10;    2   -l&#10;    2  &amp;lt;1&amp;gt;&#10;    2   ls&#10;    2   ls&#10;    2  &amp;lt;1&amp;gt;&#10;   2   ls&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I need this dat to be like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;user ls cd vi cat&#10; 1   5   0  0  0&#10; 2   3   2  5   6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;etc, &lt;/p&gt;&#10;&#10;&lt;p&gt;any ideas how I would do this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-22T01:02:45.110" Id="82970" LastActivityDate="2014-01-22T17:30:10.747" LastEditDate="2014-01-22T17:30:10.747" LastEditorUserId="37572" OwnerUserId="37572" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;machine-learning&gt;" Title="What machine learning technique I need to apply?" ViewCount="120" />
  <row Body="&lt;p&gt;Yes, this is because the $\alpha, \beta$ node $d$-separates $m_2$ and $y_3$.  See &lt;em&gt;Probabilistic Reasoning in Intelligent Systems&lt;/em&gt; for an explanation of $d$-separation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-22T01:08:28.720" Id="82971" LastActivityDate="2014-01-22T01:08:28.720" OwnerUserId="858" ParentId="82969" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Yes, it can. Spin-glass models for community detection can compute modularity from weighted, signed graphs. You'll want Traag and Bruggeman &lt;a href=&quot;http://arxiv.org/abs/0811.2329&quot; rel=&quot;nofollow&quot;&gt;&quot;Community detection in networks with positive and negative links&quot;&lt;/a&gt; as a reference. The function &lt;a href=&quot;http://igraph.sourceforge.net/doc/R/spinglass.community.html&quot; rel=&quot;nofollow&quot;&gt;&quot;spinglass.community()&quot;&lt;/a&gt; in igraph can find the communities and return the graph's modularity.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-22T03:01:50.533" Id="82977" LastActivityDate="2014-01-22T03:01:50.533" OwnerUserId="36673" ParentId="82913" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to &quot;justify&quot; the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fitMLnull &amp;lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is &lt;em&gt;not&lt;/em&gt; estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-22T04:15:09.093" Id="82981" LastActivityDate="2014-01-22T04:15:09.093" OwnerUserId="17329" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;multilevel-analysis&gt;&lt;glmer&gt;" Title="How to estimate ICC (degree of clustering) in hierarchical logistic regression?" ViewCount="622" />
  
  <row AcceptedAnswerId="83013" AnswerCount="2" Body="&lt;p&gt;Am I missing something? When I use SVMLight, there's always a lot of vectors in the model file.&lt;/p&gt;&#10;&#10;&lt;p&gt;But when I create one with SVMPerf - there's only one vector in the file? (?)&lt;/p&gt;&#10;&#10;&lt;p&gt;But still, it works fine. Am I missing something? &lt;/p&gt;&#10;&#10;&lt;p&gt;Also the number of documents that it supposedly used is way smaller than I provided him with ( I supplied 400,000 examples and it says it used 165 training documents? )&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-22T12:19:21.793" Id="83006" LastActivityDate="2014-01-22T13:44:40.337" OwnerUserId="37616" PostTypeId="1" Score="1" Tags="&lt;svm&gt;" Title="SVMPerf - only one vector in the model file?" ViewCount="76" />
  <row Body="&lt;p&gt;You can use the &lt;code&gt;lm&lt;/code&gt; function for a single index model (SIM):  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rm=market return  &#10;rs=stock return for single stock  &#10;&#10;sim=lm(rs~rm,data=returns);  &#10;summary(sim);  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This returns a model fitted by least squares.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want predictions and you have a new data set with market return values named as &lt;code&gt;newdata&lt;/code&gt;, then you can get predictions this way:   &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;predicts=predict(sim,data=newdata);  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For risk metrics, I think beta-vector is a good measure since it describes covariance between a single stock's return and market risk in a single index model.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For a multiple index model (MIM), you can use the following code:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;electret=return of stock for company producing electricity  &#10;gas=commodity price index for gas  &#10;oil=commodity price index for oil  &#10;&#10;mim=lm(electret~gas+oil,data=yourdata);  &#10;summary(mim);  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Betas tell you how much stock return is affected by movements in these indices.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-22T13:18:47.987" Id="83010" LastActivityDate="2014-05-29T14:17:31.327" LastEditDate="2014-05-29T14:17:31.327" LastEditorUserId="32036" OwnerUserId="28732" ParentId="82879" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The community and @BrianDiggs may correct me if I am wrong, but I believe you can get a p-value for your problem as follows. A p-value for a two sided test is defined as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$2*\text{min}[P(X \le x|H_0),P(X \ge x|H_0)]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you order the bootstrapped coefficients by size and then determine the proportions larger and smaller zero, the minimum proportion times two should give you a p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;I normally use the following function in such a situation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;twosidep&amp;lt;-function(data){&#10;  p1&amp;lt;-sum(data&amp;gt;0)/length(data)&#10;  p2&amp;lt;-sum(data&amp;lt;0)/length(data)&#10;  p&amp;lt;-min(p1,p2)*2&#10;  return(p)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-22T17:23:09.987" Id="83038" LastActivityDate="2014-01-22T17:23:09.987" OwnerUserId="24515" ParentId="83012" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;Perform a likelihood-ratio test between the model including the two parameters &amp;amp; a model excluding both: twice the difference in the log-likelihoods of the models has approximately a chi-squared distribution with degrees of freedom equal to the difference in the number of free parameters (two in this case).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-22T17:23:23.923" Id="83039" LastActivityDate="2014-01-22T17:23:23.923" OwnerUserId="17230" ParentId="83035" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;I think you are going wrong very early, when you state that &#10;$$\begin{align}&#10;\mathbb P(Y_1&amp;gt;y_1\,|\,Y_n=y_n) &amp;amp;= \mathbb P(Y_1,...,Y_n&amp;gt;y_1\,|\,Y_n=y_n) =\\&#10;&amp;amp;= \mathbb P(Y_1,...,Y_{n-1}&amp;gt;y_1)I_{(y_1,\infty)}\!(y_n).&#10;\end{align}$$ &#10;You need to keep the information that all the variables with the exception of $Y_n$ are less than or equal to $y_n$. So (wlog assume $X_n = Y_n$)&#10; $$\begin{align}&#10;\mathbb P(Y_1&amp;gt;y_1\,|\,Y_n=y_n) &amp;amp;= \mathbb P(Y_1,...,Y_n&amp;gt;y_1\,|\,Y_n=y_n) =\\&#10;&amp;amp;= \mathbb P(y_1 &amp;lt;X_1,...,X_{n-1}\leq y_n)I_{(y_1,\infty)}\!(y_n) = \\&#10;&amp;amp; = [F(y_n) - F(y_1)]^{n-1} I_{(y_1,\infty)}\!(y_n).&#10;\end{align}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-22T19:49:13.603" Id="83052" LastActivityDate="2014-01-22T19:49:13.603" OwnerUserId="279" ParentId="83025" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="83122" AnswerCount="1" Body="&lt;p&gt;We're trying to improve search results, and we're trying to determine if certain changes (adding a word to a synonym list, removing it from the query, etc) have a statistically significant improvement on the results for future searches.&lt;/p&gt;&#10;&#10;&lt;p&gt;We currently have data for past queries and their matching &quot;correct&quot; results.  We also have tests to sample from those past queries, perform them again, and look at the percentage of &quot;good matches&quot;. A good match being one for which the known correct result is within the top recommendations by the search engine.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a concrete example: say we want to know if adding a synonym for show-&gt;display will improve results.  We look through past queries and find all of those containing the word 'show', then we find the number of good matches.  Then we add the synonym map, and test again for the same sample of queries.&lt;/p&gt;&#10;&#10;&lt;p&gt;The results let us know how many queries improved, how many got worse, how many stayed the same, total number of samples, and total number of good matches (before and after the test).&lt;/p&gt;&#10;&#10;&lt;p&gt;To get an idea of the numbers, we have about 260,000 past queries, and for most words there are less than 10,000 queries which contain those words (and thus we can test the entire sample).&lt;/p&gt;&#10;&#10;&lt;p&gt;For this example the results are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sample size containing 'show': 5250 &#10;Good matches (base):           3212&#10;Good matches (test):           3208&#10;Improved:                      8&#10;Got worse:                     12&#10;Total change:                  -4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The question is which test(s) would be the most helpful in determining if this change is likely to represent the change in future unknown queries? (Using a 95% confidence level)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the things I've tried on R, and their results:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Two sample T-test on both sets of data (represented by vectors of 1s and 0s) - not significant&lt;/li&gt;&#10;&lt;li&gt;T-test on the difference of the sets - not significant&lt;/li&gt;&#10;&lt;li&gt;Bootstrapping on the difference in mean of each set, and doing a T-test of that result - significant&lt;/li&gt;&#10;&lt;li&gt;Bootstrapping each sample on their mean, then T-test two sample t-test of the results - significant&lt;/li&gt;&#10;&lt;li&gt;Bootstrapping each sample on their mean, then t-test of the difference in the results - significant&lt;/li&gt;&#10;&lt;li&gt;McNemar test - significant&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2014-01-22T21:34:08.437" Id="83064" LastActivityDate="2014-01-23T14:35:13.307" LastEditDate="2014-01-22T22:16:12.147" LastEditorUserId="37682" OwnerUserId="37682" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;t-test&gt;&lt;bootstrap&gt;&lt;binary-data&gt;&lt;mcnemar-test&gt;" Title="Determine if an action increases the proportion of 1s in binary data with unknown population" ViewCount="84" />
  
  <row Body="&lt;p&gt;Note: There was something wrong with my original example. I stupidly got caught by R's silent argument recycling.  My new example is quite similar to my old one. Hopefully everything is right now.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example I made that has the ANOVA significant at the 5% level but none of the 6 pairwise comparisons are significant, &lt;em&gt;even at the 5% level&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;g1:  10.71871  10.42931   9.46897   9.87644&#10;g2:  10.64672   9.71863  10.04724  10.32505  10.22259  10.18082  10.76919  10.65447 &#10;g3:  10.90556  10.94722  10.78947  10.96914  10.37724  10.81035  10.79333   9.94447 &#10;g4:  10.81105  10.58746  10.96241  10.59571&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1HXhE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the ANOVA:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;             Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  &#10;as.factor(g)  3  1.341  0.4469   3.191 0.0458 *&#10;Residuals    20  2.800  0.1400        &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's the two sample t-test p-values (equal variance assumption):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        g2     g3     g4&#10; g1   0.4680 0.0543 0.0809 &#10; g2          0.0550 0.0543 &#10; g3                 0.8108&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;With a little more fiddling with group means or individual points, the difference in significance could be made more striking (in that I could make the first p-value smaller and the lowest of the set of six p-values for the t-test higher).&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Here's an additional example that was originally generated with noise about a trend, which shows how much better you can do if you move points around a little:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;g1:  7.27374 10.31746 10.54047  9.76779&#10;g2: 10.33672 11.33857 10.53057 11.13335 10.42108  9.97780 10.45676 10.16201&#10;g3: 10.13160 10.79660  9.64026 10.74844 10.51241 11.08612 10.58339 10.86740&#10;g4: 10.88055 13.47504 11.87896 10.11403&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The F has a p-value below 3% and none of the t's has a p-value below 8%. (For a 3 group example - but with a somewhat larger p-value on the F - omit the second group)&lt;/p&gt;&#10;&#10;&lt;p&gt;And here's a really simple, if more artificial, example with 3 groups:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;g1: 1.0  2.1&#10;g2: 2.15 2.3 3.0 3.7 3.85&#10;g3: 3.9  5.0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(In this case, the largest variance is on the middle group - but because of the larger sample size there, the standard error of the group mean is still smaller)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Multiple comparisons t-tests&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;whuber suggested I consider the multiple comparisons case. It proves to be quite interesting. &lt;/p&gt;&#10;&#10;&lt;p&gt;The case for multiple comparisons (all conducted at the original significance level - i.e. without adjusting alpha for multiple comparisons) is somewhat more difficult to achieve, as playing around with larger and smaller variances or more and fewer d.f. in the different groups don't help in the same way as they do with ordinary two-sample t-tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, we do still have the tools of manipulating the number of groups and the significance level; if we choose more groups and smaller significance levels, it again becomes relatively straightforward to identify cases. Here's one:&lt;/p&gt;&#10;&#10;&lt;p&gt;Take eight groups with $n_i=2$. Define the values in the first four groups to be (2,2.5) and in the last four groups to be (3.5,4), and take&#10;$\alpha=0.0025$ (say). Then we have a significant F:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(aov(values~ind,gs2))&#10;            Df Sum Sq Mean Sq F value  Pr(&amp;gt;F)   &#10;ind          7      9   1.286   10.29 0.00191 &#10;Residuals    8      1   0.125                   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Yet the smallest p-value on the pairwise comparisons is not significant that that level:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; with(gs2,pairwise.t.test(values,ind,p.adjust.method=&quot;none&quot;))&#10;&#10;        Pairwise comparisons using t tests with pooled SD &#10;&#10;data:  values and ind &#10;&#10;   g1     g2     g3     g4     g5     g6     g7    &#10;g2 1.0000 -      -      -      -      -      -     &#10;g3 1.0000 1.0000 -      -      -      -      -     &#10;g4 1.0000 1.0000 1.0000 -      -      -      -     &#10;g5 0.0028 0.0028 0.0028 0.0028 -      -      -     &#10;g6 0.0028 0.0028 0.0028 0.0028 1.0000 -      -     &#10;g7 0.0028 0.0028 0.0028 0.0028 1.0000 1.0000 -     &#10;g8 0.0028 0.0028 0.0028 0.0028 1.0000 1.0000 1.0000&#10;&#10;P value adjustment method: none &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="20" CreationDate="2014-01-23T02:18:43.660" Id="83083" LastActivityDate="2014-01-24T00:50:42.257" LastEditDate="2014-01-24T00:50:42.257" LastEditorUserId="805" OwnerUserId="805" ParentId="83030" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;As to another way of estimating the variance, yes there is. First off, you need to correct for the number of estimated parameters in $\hat{\sigma}_u^2$, so the denominator becomes: $NT-N-K$ (I'm guessing this was a typo). Second of all, we want to estimate $\sigma_u^2$ and $\sigma_c^2$. You have already found $\hat\sigma_u^2$, so using:&#10;$$&#10;\sigma_v^2=\sigma_u^2+\sigma_c^2\Leftrightarrow&#10;\sigma_c^2=\sigma_v^2-\sigma_u^2,&#10;$$&#10;we can find $\hat\sigma_c^2$ by running the Pooled OLS estimator on your sample (why? because this is consistent, although not efficient under you assumptions) and obtain $\hat\sigma_v^2$. Then it's just a matter of subtracting the two variance estimates from each other. Then you are ready to apply your FGLS-estimator.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-23T09:14:51.727" Id="83095" LastActivityDate="2014-01-23T09:14:51.727" OwnerUserId="34944" ParentId="81713" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I think a key point here is to realize that from a statistical point of view you are testing lots of hypoteses of &quot;difference in performance&quot;.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;I think the step point should be to &lt;strong&gt;step back and check whether you can possibly get a sensible answer from all these calculations&lt;/strong&gt;.&lt;br&gt;&#10;You say you want to compare 5 models $\times$ 3200 data sets (not sure whether I understood correctly how exactly you want to compare). That is a huge number of comparisons.&lt;br&gt;&#10;If you think in terms of correct/wrong predictions (i.e. dichotomized outcome as opposed to looking at continuous scores): what order of magnitude of difference in the performance do you expect? &lt;/p&gt;&#10;&#10;&lt;p&gt;McNemar's test looks at the numbers of cases misclassified by model A but not by model B (&quot;b&quot;) compared to the number of cases misclassified by B but not by A (&quot;c&quot;): only those cases show differences between the models. Here's a plot of the $p$ values for different combinations:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/eGdC1.png&quot; alt=&quot;p values McNemar&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this applies to a &lt;em&gt;single&lt;/em&gt; comparison.&lt;br&gt;&#10; So even in &lt;strong&gt;best case&lt;/strong&gt; you need a &lt;strong&gt;difference of at least 12.5 %&lt;/strong&gt; (0 vs. 6 cases) for a &lt;em&gt;single&lt;/em&gt; comparison of the proportion-type performance you are looking at. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Keep in mind that also a optimization of the type &quot;pick the hyperparameters that gave maximum observed performance in the tuning tests&quot; is a massive multiple comparison situation in the sense outlined above. In other words, you have hardly any chance here to do a meaningful optimization, you will only &quot;skim&quot; the testing variance.  &lt;/p&gt;&#10;&#10;&lt;p&gt;As @DikranMarsupial says: Optimization is the root of all evil in statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;If at all possible, I'd recommend to &lt;strong&gt;avoid the optimization and instead fix the hyperparameters&lt;/strong&gt; by your knowledge about the application, the data and the classifier. If your classifier doesn't allow you to do that, use another that is more suitable in that respect.&lt;br&gt;&#10;In the light of the McNemar thoughts above, unless you think that your hyperparameters may lead to far worse predictions than the truely optimal hyperparameters (far worse meaning e.g. 20 % misclassifications instead of 2 %) there is no need to worry here. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you still insist on tuning: Even if you have to report the final performance with proportion-type characteristics (% correct classified, sensitivtity, specificity, predictive values, ...) make sure that for the tuning you use a proper scoring rule, e.g. Brier's score. This has 2 advantages: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;they react continuously and thus are sensitive to small changes in the performance that the proportions cannot detect&lt;/li&gt;&#10;&lt;li&gt;they typically have lower variance &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The harder you optimize (the more models are compared during tuning), the more likely you are to overfit during the tuning step. The proper scoring rules can &lt;em&gt;somewhat&lt;/em&gt; mitigate this. But again, they won't work miracles.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Some arguments for doing a rather low number of iterations for the outer validation of the tuned model: Consider $i \times$ $k$-fold cross validation. In each of the $i$ runs, each case is tested exactly once. Thus the variance you observe over the $i$ runs is due to model instability. So &lt;em&gt;measure&lt;/em&gt; the stability of the predictions with rather small $i$. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If the variance is $\ll$ the variance due to a finite number of 48 test cases, there is no need for further iterations.&lt;br&gt;&#10;With a true e.g. correct classification rate of $p = 90 \%$, you get $\sigma (&#10;\hat p) = \sqrt{\frac{p (1 - p)}{n}} \approx 4 \%$  &lt;/li&gt;&#10;&lt;li&gt;If the variance is high, your models are unstable. Which also means that most likely your optimization failed (check whether the chosen hyperparameters are stable). Do you relly need a good estimate of the performance when you know your modeling strategy failed?  &lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Also, I'd go for 6-fold cross validation here: if your models are much worse when trained on 40 cases as opposed to train on 43 cases, you are in trouble anyways.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What ever you do for the training, it is of utmost importance that the finally chosen model is validated by independent test cases:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;all kinds of preprocessing that calculate on more than one case, e.g. calculating the mean for centering, doing a PCA projection etc.&lt;/li&gt;&#10;&lt;li&gt;all kinds of data-driven optimization like feature selection&#10;need to be re-calculated as part of the training for each of the resampling surrogate models.&lt;/li&gt;&#10;&lt;li&gt;IMHO there are very few exceptions to this. One would be if you refrain from selecting a final model. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I work with vibrational spectra. For that type of data, a number of sensible pre-processing alternatives exist. &lt;/p&gt;&#10;&#10;&lt;p&gt;In my experience, which of the alternatives is chosen often has a surprisingly low influence on the predictive performance of the model. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you read German, I could give you my Diplomarbeit which shows that  (though not looking at interactions). &lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You can also look at  &lt;a href=&quot;http://dx.doi.org/10.1016/j.trac.2013.04.015&quot; rel=&quot;nofollow&quot;&gt;Engel &lt;em&gt;et al.&lt;/em&gt;: Breaking with trends in pre-processing? , TrAC Trends in Analytical Chemistry , 50, 96 - 106 (2013). DOI: 10.1016/j.trac.2013.04.015&lt;/a&gt;&lt;br&gt;&#10;They &lt;em&gt;observe&lt;/em&gt; something like 93% accuracy for their best combination of pre-processing methods compared to ca. 88% without pre-processing.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A quick simulation of &quot;best of 4912 tests with 267 cases $\times$ randomly predicting 83% correct&quot; gives me almost always (&gt; 99% of the runs) a best observed performance of 90 % or better, and in almost 1% of the runs 93% or better. Which underlines their message that choosing a good pre-processing strategy is not a trivial task at all also from a statistical point of view. (The simple simulation is just a very rough approximation to whether the observed difference in performance is significant: I model an unpaired comparison of independent outcomes, which is &lt;em&gt;not&lt;/em&gt; the case for their data points). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I'd recommend to run some Null hypothesis simulation/permutation test also for your data. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2014-01-23T11:54:01.720" Id="83111" LastActivityDate="2014-01-23T14:39:07.727" LastEditDate="2014-01-23T14:39:07.727" LastEditorUserId="4598" OwnerUserId="4598" ParentId="83093" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;They live in different spaces and mean different things.&lt;/p&gt;&#10;&#10;&lt;p&gt;A credible interval $[a,b]$ is a subset of the parameter space such that&#10;$$&#10;  P(a\leq\Theta\leq b\mid X_1=x_1,\dots,X_n=x_n) = \alpha \, ,&#10;$$&#10;and it means that, after seeing the data, you believe that with probability $\alpha$ the parameter value is inside this interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;A prediction interval $[u,v]$ is a subset of the sampling space such that&#10;$$&#10;  P(u\leq X_{n+1}\leq v\mid X_1=x_1,\dots,X_n=x_n) = \gamma \, ,&#10;$$&#10;and it means that, after seeing the data, you believe that with probability $\gamma$ the value of a future observation $X_{n+1}$ will be inside this interval.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-23T20:12:50.590" Id="83158" LastActivityDate="2014-01-23T20:12:50.590" OwnerUserId="9394" ParentId="71134" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Let's say I have two samples. If I want to tell whether they are pulled from different populations, I can run a t-test. But let's say I want to test whether the samples are from the same population. How does one do this? That is, how do I calculate the statistical probability that these two samples were pulled from the same population?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-23T20:41:36.010" Id="83163" LastActivityDate="2014-03-27T22:47:45.353" LastEditDate="2014-01-23T21:12:29.930" LastEditorUserId="37742" OwnerUserId="37742" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;" Title="Statistical test to tell whether two samples are pulled from the same population?" ViewCount="1818" />
  <row Body="&lt;p&gt;I remember LibSVM uses RBF as default kernel, so you can't really have feature weights, since the solution is in the dual. Try changing to linear kernel, that should get you some weights out.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-23T22:05:06.473" Id="83171" LastActivityDate="2014-01-23T22:05:06.473" OwnerUserId="37188" ParentId="83140" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I suggest using a paired t-test because the accuracies on different data sets should not be compared directly. Each data set you test on should form a pair in your t test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on your example, you would be doing something like this in R to compare SIFT and SURF:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;SIFT &amp;lt;- c(0.90, 0.84, 0.90, 0.45)&#10;SURF &amp;lt;- c(0.84, 0.67, 0.45, 0.34)&#10;SIFT_v_SURF &amp;lt;- t.test(SIFT,SURF,paired=TRUE,alternative=&quot;greater&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note: by using a t-test you are assuming normality which may not be the case.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-01-23T22:49:50.193" Id="83175" LastActivityDate="2014-01-23T22:55:15.343" LastEditDate="2014-01-23T22:55:15.343" LastEditorUserId="25433" OwnerUserId="25433" ParentId="83173" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The t-test is for comparing 2 groups (or one group to a theoretical value).  With 3 groups (tests) you would need ANOVA and since there is blocking (the generalization of pairing) due to the different datasets you would be using randomized block ANOVA or a mixed effects model.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, these methods depend on approximate normality and with the nature of your data, it is not likely to be approximately normal and your sample size is not large enough to invoke the CLT.  A permutation test is probably your best option given your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is R code for one possible way to do a permutation test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;SIFT &amp;lt;- c(0.90, 0.84, 0.90, 0.45)&#10;SURF &amp;lt;- c(0.84, 0.67, 0.45, 0.34)&#10;ORB &amp;lt;- c(0.34, 0.45, 0.45, 0.23)&#10;&#10;tmpdat &amp;lt;- rbind( SIFT, SURF, ORB )&#10;&#10;tmpfun &amp;lt;- function(m) diff( range( rowMeans(m) ) )&#10;&#10;out &amp;lt;- c( tmpfun(tmpdat), &#10;    replicate( 9999, tmpfun( apply(tmpdat, 2, sample) ) ) )&#10;hist(out)&#10;abline(v=out[1])&#10;mean( out &amp;gt;= out[1] )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-01-23T22:50:01.597" Id="83176" LastActivityDate="2014-01-23T22:57:07.637" LastEditDate="2014-01-23T22:57:07.637" LastEditorUserId="4505" OwnerUserId="4505" ParentId="83173" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="83190" AnswerCount="1" Body="&lt;p&gt;Any good places to start getting into Bayesian statistics? I'm a graduate student in social sciences, with a decent amount of stats classes under my belt, but I'm far from fluent. Any references would be much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" ClosedDate="2014-01-24T01:10:10.817" CommentCount="0" CreationDate="2014-01-24T00:35:57.630" FavoriteCount="2" Id="83189" LastActivityDate="2014-01-24T00:46:55.590" OwnerUserId="37767" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;bayes&gt;" Title="Bayesian references" ViewCount="67" />
  
  
  
  <row Body="&lt;p&gt;If you want to test the null  of no autocorrelation using 10 lags, it is enough to run the command as stated in the question. You should not run it once for every lag.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: To be clear, including 10 lags tests the null hypothesis of no autocorrelation against the alternative that at least one of the included lags exhibits non-zero autocorrelation, which to me sounds like what you want.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-24T08:55:13.920" Id="83212" LastActivityDate="2014-01-24T09:28:26.690" LastEditDate="2014-01-24T09:28:26.690" LastEditorUserId="37483" OwnerUserId="37483" ParentId="83210" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;It has just been pointed out to me that I made a mistake in my simulation. So my theoretically based assumption was sound: under the exponential AFT model coxph() and survreg(,dist=&quot;exponential&quot;) should provide on average the same answer. However my implementation of the exponential AFT model in the R simulation was incorrect. I simulated:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logT     &amp;lt;- -d1*X1 + e&#10;T        &amp;lt;- exp(logT)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, under my assumptions, it is not logT that is exponentially distributed, but T. So instead, I should have replaced the above two lines in my code with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;T     &amp;lt;- exp(-d1*X1)*e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After this change, the code produced the results I expected.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-24T12:07:14.277" Id="83230" LastActivityDate="2014-01-24T12:07:14.277" OwnerUserId="37734" ParentId="83144" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am building a prediction model based on SVM model with RBF kernel. The training samples represent features obtained from genome regions (called binding sites) that are targeted (regulated) by proteins. The model will learn to distinguish between regions targeted by proteins and regions that are not. (I created positive and negative data).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is that the positive data contains redundant training samples (i.e. identical binding sites at different locations of the genome) so, shall I keep the redundant training samples (to make the model learn more about a specific pattern of targeted region) or I should keep only the unique training samples?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: I have in the training samples 10,000 samples represent 10,000 different target regions (binding sites) of proteins. If I take the unique, it will be around 7,000 samples. (Because the binding sites might be identical in different locations)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-24T12:55:28.053" Id="83232" LastActivityDate="2014-01-24T12:55:28.053" OwnerUserId="26471" PostTypeId="1" Score="1" Tags="&lt;classification&gt;" Title="Redundant Training Samples" ViewCount="36" />
  
  <row Body="&lt;p&gt;My answer is based on making some assumptions about your question.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Assumption 1: Dependent variable is binary.&lt;br&gt;&#10;Assumption 2: Independent variable (predictor) is ordinal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 1:&lt;br&gt;&#10;If these assumptions are accurate, then the answer is yes. A binary logistic regression would be appropriate as the type of regression one runs is dependent on the dependent variable. A helpful hint is that if possible, have your independent variables have a meaningful 0 (e.g., &quot;never&quot; or &quot;neutral&quot; or sample mean). This tends to help with interpretation of the regression weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 2:&lt;br&gt;&#10;Based on the information you provided, it sounds as if you have the individual items of a scale (or subscale). For example, if I have 3 questions about different symptoms of a disease, they should factor together. I could average these items or sum the items to create a scale score that is then used as an independent or dependent variable in regression. This is commonly done in many different fields (particularly any field that uses surveys).&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this is helpful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-24T14:12:27.613" Id="83235" LastActivityDate="2014-01-24T14:33:41.797" LastEditDate="2014-01-24T14:33:41.797" LastEditorUserId="37736" OwnerUserId="37736" ParentId="71918" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="83298" AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&#10;for $x_1...x_n \sim Pois(\lambda)$&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Find a CI for $\lambda$ (using the MLE).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Find a CI for $\psi=\sqrt \lambda$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On 1, I found that the MLE is $\bar x$ and then using the normal approximation (the only thing we know how to do in this course) I calculated $\mu=\lambda, \sigma=\sqrt \frac \lambda n$&#10;so the $CI = [\hat \lambda \pm Z_{1-0.5a}\sqrt{\frac {\hat \lambda} n}] $. I want to verify the correctness of the CI. and also - I don't really know how to do part 2. I cannot use linearity of the expectation for calculating it...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-24T16:04:36.927" Id="83245" LastActivityDate="2014-02-21T00:41:42.180" LastEditDate="2014-02-21T00:41:42.180" LastEditorUserId="805" OwnerUserId="36772" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;confidence-interval&gt;&lt;poisson&gt;" Title="CI for Poisson square root" ViewCount="64" />
  <row AcceptedAnswerId="83288" AnswerCount="2" Body="&lt;p&gt;For revision, I am working out a multiplicative model for sales data, and then conducting a simple error analysis (actual sales - forecasted). I understand the process but in my lecturer's mark scheme he has put 'You should mention the usefulness of the ACF of the errors'. How would I go about this? My error results are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Errors: -7.75 1.67 -2.26 0.776 3.88 4.61 6.024 3.112 -0.49 -1.44 -7.688 2.448 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-24T17:43:43.620" Id="83254" LastActivityDate="2014-01-25T01:53:02.860" LastEditDate="2014-01-24T23:26:49.310" LastEditorUserId="17230" OwnerUserId="37837" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;autocorrelation&gt;&lt;model&gt;" Title="ACF of the errors" ViewCount="106" />
  <row AcceptedAnswerId="83323" AnswerCount="1" Body="&lt;p&gt;I used the clogit function (from the survival package) to run a conditional logistic regression in R with a big dataset of 1:M matched pairs with n=300368964 and number of events= 39995.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- clogit(Alliance ~ OVB + CVC + BVB + strata(Strata), method=&quot;exact&quot;)    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I received following results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                 coef  exp(coef)   se(coef)       z Pr(&amp;gt;|z|)    &#10;OVB        -0.0498174  0.9514031  0.0166275  -2.996  0.00273 ** &#10;BVB         0.0277405  1.0281289  0.0304956   0.910  0.36300    &#10;CVC         1.1709851  3.2251683  0.1089709  10.746  &amp;lt; 2e-16 ***&#10;EarlyStage -1.3215824  0.2667129  0.0205851 -64.201  &amp;lt; 2e-16 ***&#10;AvgVCSize   0.0087976  1.0088364  0.0002035  43.224  &amp;lt; 2e-16 ***&#10;NumberVC    0.0643579  1.0664740  0.0034502  18.653  &amp;lt; 2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Rsquare= 0   (max possible= 0.001 )&#10;Likelihood ratio test= 6511  on 6 df,   p=0&#10;Wald test            = 6471  on 6 df,   p=0&#10;Score (logrank) test = 6801  on 6 df,   p=0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since Rsquare equals 0 and the test ratios seems very high, I tried to plot the results to check whether the model fits. But I wasn't able to plot it properly.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would online many papers which use the ratio Prob &gt; chi2 = 0 from Stata as test ratio to proof the model fit. &lt;/p&gt;&#10;&#10;&lt;p&gt;How could I calculate this ratio in R? Are there any other ways I could check the model fit of my clogit results?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would appreciate any help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks you very much in advance.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-01-24T18:07:13.890" Id="83260" LastActivityDate="2014-01-25T15:11:04.480" LastEditDate="2014-01-25T15:04:09.757" LastEditorUserId="37838" OwnerUserId="37838" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;survival&gt;" Title="How to calculate Prob &gt; chi2 in R to test model fit of conditional logistic regression" ViewCount="315" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a regression model that looks like the one below.&#10;$$&#10;Y = \beta_0 + \beta_1T + \beta_2T^2 + \beta_3T^3 + \beta_4D + \beta_5D*T + \beta_6D*T^2 + \beta_7D*T^3&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where &lt;code&gt;T&lt;/code&gt; is a time variable, which starts from 0 - the starting point, and &lt;code&gt;T^2&lt;/code&gt; and &lt;code&gt;T^3&lt;/code&gt; are the natural polynomials of &lt;code&gt;T&lt;/code&gt;; &lt;code&gt;D&lt;/code&gt; is a dummy variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;T&lt;/code&gt;, &lt;code&gt;T^2&lt;/code&gt;, and &lt;code&gt;T^3&lt;/code&gt; are highly collinear. To reduce collinearity, one possible option is to use orthogonal polynomials. However, one issue I have with using orthogonal polynomials is that the intercept of the orthogonal polynomial regression is no longer the predicted value of &lt;code&gt;Y&lt;/code&gt; at &lt;code&gt;T = 0&lt;/code&gt; (and when &lt;code&gt;D = 0&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;I crucially need the intercept to indicate the mean value of &lt;code&gt;Y&lt;/code&gt; at &lt;code&gt;T = 0&lt;/code&gt; because of theoretical importance. &lt;/p&gt;&#10;&#10;&lt;h3&gt;So my questions are:&lt;/h3&gt;&#10;&#10;&lt;h3&gt;1). Is it possible to center orthogonal polynomials in some way (e.g., centering around the minimum value??) so that the intercept will be equivalent to the mean value of &lt;code&gt;Y&lt;/code&gt; when T= 0`  But I am not sure how this can be properly done given that there are a quadratic term and a cubic term involved.&lt;/h3&gt;&#10;&#10;&lt;h3&gt;2). If the answer to the previous question is &quot;No&quot;, I wonder if there are any alternative ways in which I can deal with multicollinearity while making the intercept mean what I want it to mean - namely B0 = mean of Y when T = 0 (and D = 0).&lt;/h3&gt;&#10;" CommentCount="3" CreationDate="2014-01-24T18:07:32.230" Id="83261" LastActivityDate="2014-01-24T18:07:32.230" OwnerUserId="18615" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;multicollinearity&gt;&lt;polynomial&gt;&lt;intercept&gt;" Title="Is it possible to center orthogonal polynomials in multiple regression" ViewCount="122" />
  
  
  <row Body="&lt;p&gt;If the outcome depends on treatment as well as other observable factors, controlling for the latter often improves the precision of the impact estimate (i.e., the standard error of the treatment effect will be smaller). When sample size is small, this can be helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a simple simulation where even though treatment is random, the standard error shrinks by a third:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. set obs 100&#10;obs was 0, now 100&#10;&#10;. gen treat =mod(_n,2)&#10;&#10;. gen x=rnormal()&#10;&#10;. gen y = 2 + 3*treat + 1*x + rnormal()&#10;&#10;. reg y treat&#10;&#10;      Source |       SS       df       MS              Number of obs =     100&#10;-------------+------------------------------           F(  1,    98) =  112.75&#10;       Model |  209.354021     1  209.354021           Prob &amp;gt; F      =  0.0000&#10;    Residual |  181.973854    98  1.85687606           R-squared     =  0.5350&#10;-------------+------------------------------           Adj R-squared =  0.5302&#10;       Total |  391.327875    99  3.95280682           Root MSE      =  1.3627&#10;&#10;------------------------------------------------------------------------------&#10;           y |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       treat |   2.893814   .2725345    10.62   0.000     2.352978     3.43465&#10;       _cons |   2.051611    .192711    10.65   0.000     1.669183     2.43404&#10;------------------------------------------------------------------------------&#10;&#10;. reg y treat x&#10;&#10;      Source |       SS       df       MS              Number of obs =     100&#10;-------------+------------------------------           F(  2,    97) =  180.50&#10;       Model |  308.447668     2  154.223834           Prob &amp;gt; F      =  0.0000&#10;    Residual |  82.8802074    97  .854435127           R-squared     =  0.7882&#10;-------------+------------------------------           Adj R-squared =  0.7838&#10;       Total |  391.327875    99  3.95280682           Root MSE      =  .92436&#10;&#10;------------------------------------------------------------------------------&#10;           y |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       treat |   2.918349   .1848854    15.78   0.000     2.551403    3.285295&#10;           x |   1.058636   .0983022    10.77   0.000     .8635335    1.253739&#10;       _cons |   1.996209    .130825    15.26   0.000     1.736558     2.25586&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-01-24T22:38:10.340" Id="83279" LastActivityDate="2014-01-24T22:48:43.963" LastEditDate="2014-01-24T22:48:43.963" LastEditorUserId="7071" OwnerUserId="7071" ParentId="83277" PostTypeId="2" Score="10" />
  
  <row AnswerCount="2" Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; in Wikipedia it says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For sufficiently large values of λ, (say λ&gt;1000), the normal distribution with mean λ and variance λ (standard deviation $\sqrt{\lambda}$), is an excellent approximation to the Poisson distribution. If λ is greater than about 10, then the normal distribution is a good approximation if an appropriate continuity correction is performed, i.e., P(X ≤ x), where (lower-case) x is a non-negative integer, is replaced by P(X ≤ x + 0.5).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$F_\mathrm{Poisson}(x;\lambda) \approx F_\mathrm{normal}(x;\mu=\lambda,\sigma^2=\lambda)$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Unfortunately this isn't cited. I want to be able to show / prove this with some rigour. How can you actually &lt;em&gt;say&lt;/em&gt; the normal distribution is a good approximation when $\lambda &amp;gt; 1000$, how do you quantify this 'excellent' approximation, what measures were used? &lt;/p&gt;&#10;&#10;&lt;p&gt;The furthest I've got with this is &lt;a href=&quot;http://www.johndcook.com/normal_approx_to_poisson.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; where John talks about using the Berry–Esseen theorem and approximates the error in the two CDFs. From what I can see he does not try any values of $\lambda \geq 1000$. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-24T23:49:45.207" Id="83283" LastActivityDate="2014-02-05T16:22:10.837" LastEditDate="2014-02-05T16:22:10.837" LastEditorUserId="88" OwnerUserId="28904" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;poisson&gt;&lt;approximation&gt;" Title="Normal approximation to the Poisson distribution" ViewCount="1644" />
  <row AnswerCount="0" Body="&lt;p&gt;I have an experiment organized as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;N repeated measures were done for each of M different conditions (each one having a different mean value)&lt;/li&gt;&#10;&lt;li&gt;The experiment was done P times, each time using a different measurement method.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I now want to estimate which of the P methods has the lowest overall coefficient of variation, but I am unsure on how to follow. I initially thought about computing the CV for each of the M conditions (based on the N repeated measures) and then finding the &quot;average&quot; CV for each measurement method. However, in this case I would not know how to test for significance between the different measurement methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be the best way to show whether a given measurement method has a lower &quot;overall&quot; dispersion than the others with the data I have available?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-25T01:11:23.327" Id="83284" LastActivityDate="2014-01-25T01:25:36.880" LastEditDate="2014-01-25T01:25:36.880" LastEditorUserId="13070" OwnerUserId="13070" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;coefficient-of-variation&gt;" Title="How do I compare multiple coefficients of variation?" ViewCount="99" />
  <row AnswerCount="0" Body="&lt;p&gt;I have 6 variables and 125 observations, which I am modelling using a VAR model, in which I put all variables in as edogenous, as all relationships interest me (the bidirectionality). I have carried out granger tests, detrended my data (i.e., checked the stationarity, taken logs, log_difference as necessary) so all variables come into the model stationary. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the lag I set &lt;code&gt;lag.max = 6&lt;/code&gt;, and use the AIC criterion in the model (which never returns a lag larger than 5 anyway).&lt;/p&gt;&#10;&#10;&lt;p&gt;The model works fine and I can read the answers off, with significance here and there, more or less confirming my hypotheses that I made before the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking at how the inventories of oil are related to another factor (Net Convenience Yield, WTI Oil) This relationship gets 2 lags from AIC. Then my results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(equations=&quot;NCY_WTI_3M&quot;,VAR(data.final[,c(&quot;NCY_WTI_3M&quot;,&quot;NCY_BRE_3M&quot;,&quot;SP&quot;,&quot;INV&quot;)],&#10;type=&quot;const&quot;, ic=&quot;AIC&quot;, lag.max=LagMax))&#10;&#10;    VAR Estimation Results:&#10;    ========================= &#10;    Endogenous variables: NCY_WTI_3M, NCY_BRE_3M, SP, INV &#10;    Deterministic variables: const &#10;    Sample size: 123 &#10;    Log Likelihood: -763.052 &#10;    Roots of the characteristic polynomial:&#10;    0.8509 0.8509 0.5029 0.5029 0.4318 0.4318 0.2642 0.2151&#10;    Call:&#10;    VAR(y = data.final[, c(&quot;NCY_WTI_3M&quot;, &quot;NCY_BRE_3M&quot;, &quot;SP&quot;, &quot;INV&quot;)], &#10;        type = &quot;const&quot;, lag.max = LagMax, ic = &quot;AIC&quot;)&#10;&#10;&#10;    Estimation results for equation NCY_WTI_3M: &#10;    =========================================== &#10;    NCY_WTI_3M = NCY_WTI_3M.l1 + NCY_BRE_3M.l1 + SP.l1 + INV.l1 + NCY_WTI_3M.l2 + NCY_BRE_3M.l2 + SP.l2 + INV.l2 + const &#10;&#10;                    Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;    NCY_WTI_3M.l1  5.018e-01  9.903e-02   5.067 1.57e-06 ***&#10;    NCY_BRE_3M.l1  5.096e-01  1.213e-01   4.201 5.30e-05 ***&#10;    SP.l1          1.230e-01  2.182e-01   0.564  0.57393    &#10;    INV.l1        -2.578e-06  8.847e-07  -2.914  0.00429 ** &#10;    NCY_WTI_3M.l2 -1.889e-01  8.721e-02  -2.166  0.03237 *  &#10;    NCY_BRE_3M.l2  1.613e-01  1.435e-01   1.124  0.26351    &#10;    SP.l2          5.887e-01  2.205e-01   2.669  0.00871 ** &#10;    INV.l2         1.889e-06  8.528e-07   2.215  0.02875 *  &#10;    const          1.984e-01  1.012e-01   1.961  0.05234 .  &#10;    ---&#10;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem is that both &lt;code&gt;INV.l1&lt;/code&gt; and &lt;code&gt;INV.l2&lt;/code&gt; have significance in this model, however they have differing effects. &lt;code&gt;INV.l1&lt;/code&gt; (*&lt;em&gt;) has a negative estimate, whereas &lt;code&gt;INV.l2&lt;/code&gt; (&lt;/em&gt;) has a positive estimate - this means a differing relationship over time periods, if both being significant should be believed. &lt;/p&gt;&#10;&#10;&lt;p&gt;This 'anomaly' has appeared in practically every model I have run from all 27 variables (using smaller basket models), where each time a factor has significance, the estimates provided have different signs (+/-).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I don't believe this is the case, and so my question is, is there an inherent property of VAR models that might help to explain this?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking, if the effect from &lt;code&gt;lag1&lt;/code&gt; on today is positive, then the effect from &lt;code&gt;lag2&lt;/code&gt; on &lt;code&gt;lag1&lt;/code&gt; should also be positive, so what is the expected effect from &lt;code&gt;lag2&lt;/code&gt; on today? Does the fact that the sign changes with each lag, mean that there is actually some consistency over the whole time series?&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: After speaking to other people, my current theory is that there is an ilk of overcorrection occurring. The second lagged variable has an effect on the independent variable only after the first lagged variable has been taken into account. In this train of thought, my 'problem' would then be created by a kind of mean-reversion effect, whereby the calculation always partially overshoots the mark and thus producing the positive / negative signs in front of my estimated values.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-25T01:21:18.990" FavoriteCount="1" Id="83285" LastActivityDate="2014-01-28T00:55:35.770" LastEditDate="2014-01-28T00:55:35.770" LastEditorUserId="37863" OwnerUserId="37863" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;interpretation&gt;&lt;var&gt;" Title="Alternating signs of significant estimates in VAR model" ViewCount="127" />
  
  <row Body="&lt;p&gt;A few basic issues stand out with this paper:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;It assumes correlation of search engine queries about a rising social network with the membership increases. This may have correlated in the past, but may not in the future.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are very few new large social networks. You can almost count them on one hand. Friendster, Myspace, Facebook, Google+. Also, Stack Exchange, Tumblr, and Twitter function similarly to social networks. Is anyone predicting Twitter is over? Quite to the contrary, it seems to have major momentum. There is not much mention or study of other ones to see if they fit. In a way we are talking about, does a trend exist among 5-7 data points? (The number of social networks.) It's just too little data to make any conclusion about the future.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Facebook displaced Myspace. That was the chief dynamic. It doesn't consider the idea that one infection is displacing another, it tends to consider them separately. What is displacing Facebook? Google+? Twitter? The interaction and &quot;defection&quot; of customers from one &quot;brand&quot; or &quot;product&quot; to the other is the critical phenomenon in this area.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Social networks coexist. One can be a member of multiple sites. It is true that members may tend to prefer one over the other.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It would seem a much better model is that there is a consolidation going on, like in economics, such as with automobiles, radio makers, web sites, etc. As in any new disruptive technology, there are many competitors in the beginning, and then, later, the field narrows, they tend to consolidate, there are buyouts and mergers, and some die out in the competition. We already see examples of this, e.g. Yahoo buying out Tumblr recently.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A similar concept might be with television networks consolidating and being owned by large conglomerates, e.g. major media companies owning many media assets. Indeed, Myspace was bought out by News Corporation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The way to go is to look for more analogies between economics and infections (biology). Companies acquiring customers from competitors and the uptake of products do indeed have many epidemiological parallels. There are strong parallels to evolutionary &quot;red queen&quot; races [see the book, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0060556579&quot;&gt;&lt;em&gt;Red Queen&lt;/em&gt; by Ridley&lt;/a&gt;]. There might be connections to a field called &lt;a href=&quot;http://en.wikipedia.org/wiki/Bionomics&quot;&gt;bionomics&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Another basic model is products that compete with each other and have various &quot;barriers to entry&quot; for customers to switch from one brand to another. It is true the cost of switching is very low in cyberspace. It's similar to brands of beers competing for customers, etc.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In an asymptotic model, it is much more likely that a network increases its members toward some asymptotic maximum and then it tends to &lt;strong&gt;&lt;em&gt;plateau&lt;/em&gt;&lt;/strong&gt;. Early in the plateau, it will not be apparent that it is a plateau.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;That all said, I think it has some very valid and engaging ideas and is likely to spur much further research. It's groundbreaking, pioneering, and it just needs to be adjusted a bit in its claims. I am delighted in this use of Stack Exchange and collaborative wisdom/collective intelligence analyzing this paper. (Now if only reporters researching the subject would read this whole page carefully before preparing their simplistic sound bites.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-25T03:44:06.860" Id="83294" LastActivityDate="2014-01-25T13:20:20.233" LastEditDate="2014-01-25T13:20:20.233" LastEditorUserId="32036" OwnerUserId="17493" ParentId="83136" PostTypeId="2" Score="11" />
  
  <row Body="&lt;p&gt;I believe your interval for part (1) is actually wrong (though almost correct). It's the $Z$ that looks off. Take a careful look; the subscript is wrong in at least one way and probably wrong in a couple of ways.&lt;/p&gt;&#10;&#10;&lt;p&gt;On to the next part:&lt;/p&gt;&#10;&#10;&lt;p&gt;If $g$ is a monotonic increasing function, and $(l,u)$ is a $1−α$ interval for $θ$, then $(g(l),g(u))$ will be a $1−α$ interval for $g(θ)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;(You can prove it inherits the same coverage by simple transformation of the probability statement for the original CI, but I don't see that you're expected to prove it here)&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, you can write &lt;em&gt;a&lt;/em&gt; confidence interval for $\sqrt \lambda$ immediately, though it won't be symmetric about the transformed parameter estimate (which doesn't strike me as a problem).&lt;/p&gt;&#10;&#10;&lt;p&gt;The alternative approach would be to write down a normal approximation for the square root of a Poisson, but it's a lot more work. On the other hand, the normal approximation is a lot better for the transformed Poisson.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-25T07:06:23.300" Id="83298" LastActivityDate="2014-01-25T07:20:10.307" LastEditDate="2014-01-25T07:20:10.307" LastEditorUserId="805" OwnerUserId="805" ParentId="83245" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In some bank, the time it takes from the moment a customer arrives until a clerk is available is distributed normally with $\mu=15,\sigma=2$&lt;/p&gt;&#10;&#10;&lt;p&gt;a. What's the probability for the next client to wait more than 18 minutes?&lt;/p&gt;&#10;&#10;&lt;p&gt;b. What's the prob. that the average waiting time of the next 100 customers will be greater than 15.1 minutes (assuming that the waiting times are independent)&lt;/p&gt;&#10;&#10;&lt;p&gt;c. The bank management hired a new manager for customer service and he claims that since he arrived at this job, the time it takes to be treated by a clerk decreased. To check his claim he measured the waiting time of 30 random customers and got an average of 14.3 minutes. Assuming that the s.d. hasn't changed - Check if his claim is true, with confidence level of a=0.1.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My answer&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;a. P(X&gt;18)=(normalizing) P(Z&gt;1.5)=0.0668&lt;/p&gt;&#10;&#10;&lt;p&gt;b. $P(\bar X&amp;gt;15.1)=P(Z&amp;gt;0.5)=0.3085$&lt;/p&gt;&#10;&#10;&lt;p&gt;c. $H_0:\mu=15, H_A:\mu&amp;lt;15$ so $CI=(-\infty,15-Z_0.99 \frac 2{\sqrt{30}}]=(-\infty,14.15]$&#10;but 14.13&amp;lt;14.15 so H_0 is true and not rejected. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also calculating the p-value: $P_{H_0}(\bar X&amp;lt;14.3)=P(Z&amp;lt;-1.91)=0.0281&amp;gt;0.01 $therefore $H_0$ is not rejected.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this is the first time I've been doing it on my own I'd love if someone take a look, and confirm that it's correct (or not?) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-25T11:09:58.663" Id="83309" LastActivityDate="2014-01-25T12:06:58.240" LastEditDate="2014-01-25T11:27:10.687" LastEditorUserId="805" OwnerUserId="36772" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;p-value&gt;" Title="P-value verification" ViewCount="48" />
  <row Body="&lt;p&gt;Even if changing the metric might be a solution sometimes, the problem - as you noticed - is cause by the method (centroid method in your case). So the appropriate solution is to choose a different method, if this issue occurs.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Inversions do not appear, if you use a monotonic method for clustering&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Essential monotonic hierarchical clustering methods are&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Single Linkage&lt;/li&gt;&#10;&lt;li&gt;Complete Linkage&lt;/li&gt;&#10;&lt;li&gt;Average Linkage&lt;/li&gt;&#10;&lt;li&gt;Weighted Average Linkage&lt;/li&gt;&#10;&lt;li&gt;WARD's Linkage&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;One can choose a method out of these depending on which suits the given problem most&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-25T11:22:07.507" Id="83311" LastActivityDate="2014-01-25T11:22:07.507" OwnerDisplayName="user35349" ParentId="13578" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Since this is self-study I'll just give hints.  In b, check your division. in c, check your wording&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-25T12:06:58.240" Id="83312" LastActivityDate="2014-01-25T12:06:58.240" OwnerUserId="686" ParentId="83309" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Having a bit of difficulty identifying the appropriate ARIMA model by looking at ACF/PACFs. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that AR(1) models, the ACF has a geometric progression from its highest value at lag 1 and the PACF has a spike at lag 1 and then then cuts off afterwards. &lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone possibly explain how I identify the rest? AR and MA please?!?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-25T14:50:57.063" FavoriteCount="2" Id="83322" LastActivityDate="2014-06-07T08:39:36.397" LastEditDate="2014-05-05T01:40:52.480" LastEditorUserId="805" OwnerUserId="37901" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;self-study&gt;&lt;linear-model&gt;&lt;arma&gt;" Title="ARIMA model identification" ViewCount="132" />
  
  <row Body="&lt;p&gt;Exactly. As @Vincenzo Maggio pointed out &lt;strong&gt;F&lt;sub&gt;t+1&lt;/sub&gt;&lt;/strong&gt; is nothing but the one step ahead foracast of D&lt;sub&gt;t&lt;/sub&gt; or the Demand.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you are using is called Double Exponential Smoothing, or Holt's Smoothing&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-25T16:40:04.520" Id="83334" LastActivityDate="2014-01-25T16:40:04.520" OwnerUserId="29600" ParentId="83320" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Consider a $M/E_2/1$ queueing system, where the customer arrival rate is $\lambda$ and the service time distribution has a gamma distribution with parameters $2$ and $\mu$, i.e. with p.d.f. $\mu^2te^{-\mu t}$ , $t ≥ 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) How can I determine the mean of the service time distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) What is the traffic intensity $\rho$ of the system in terms of the parameters $\lambda$ and $\mu$?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My reasoning thus far: wouldn't I simply take the mean of a Gamma(2,$\mu$) distribution and thus just say the mean service time is $\frac{2}{\mu}$ but I'm guessing there has to be something more to it than that?&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the triffic intensity $\rho$ I do not know what it would be for an erlang-2 queueing system? My thinking is (using $c=1$ for the number of servers):&lt;/p&gt;&#10;&#10;&lt;p&gt;$\rho = \frac{&quot;mean.service.time&quot;}{c*&quot;mean.customer.interarrival.time&quot;}&#10;= \frac{&quot;mean.service.time&quot;}{c*\frac{1}{&quot;arrival.rate&quot;}}&#10;=\frac{\frac{2}{\mu}}{1*\frac{1}{\lambda}}=\frac{2\lambda}{\mu}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: Follow up question: &lt;a href=&quot;http://stats.stackexchange.com/questions/83845/mean-length-of-time-spent-queueing-in-m-e-2-1-system&quot;&gt;Mean length of time spent queueing in $M/E_2/1$ system?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;I am looking at question 5 for one of the 2012 RSS exams:&lt;/em&gt; &lt;a href=&quot;http://www.rss.org.uk/uploadedfiles/userfiles/files/GD3_2012_final%20(web%20version).pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.rss.org.uk/uploadedfiles/userfiles/files/GD3_2012_final%20(web%20version).pdf&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-25T20:50:08.050" FavoriteCount="1" Id="83346" LastActivityDate="2014-01-30T22:12:13.290" LastEditDate="2014-01-30T22:12:13.290" LastEditorUserId="805" OwnerUserId="539" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;stochastic-processes&gt;&lt;gamma-distribution&gt;&lt;poisson-process&gt;&lt;queueing&gt;" Title="Mean service time of a $M/E_2/1$ queueing system?" ViewCount="119" />
  <row Body="&lt;p&gt;Simply because the coefficient switched signs and became significant from the bivariate analysis to the multivariate analysis does not guarantee that $X$ is a suppressor.  It certainly could be, what you have is the signature effect of suppression, but there are other possibilities as well.  For example, $X$ could be confounded with another of the variables in the model, causing the sign to change, but the inclusion of the remaining variables accounts for enough of the residual variance as to make the effect significant.  In general, it is hard to ultimately know for sure what the exact relationships are between variables.  The best way to determine if $X$ is a suppressor would be to run a new experiment in which you manipulate $X$ and see if there is an effect on $Y$.  If it is a suppressor, there will be no effect.  (Note that this is an equivalence test, which is more subtle than the prototypical hypothesis testing situation.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;For more information about these topics, you may want to read the following CV threads:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There is a great overview of suppression here: &lt;a href=&quot;http://stats.stackexchange.com/q/73869/7290&quot;&gt;Suppression effect in regression: definition and visual explanation/depiction&lt;/a&gt;.  &lt;/li&gt;&#10;&lt;li&gt;To understand how the sign of a covariate could change without suppression, see:   &lt;a href=&quot;http://stats.stackexchange.com/q/78828/7290&quot;&gt;Is there a difference between 'controlling for' and 'ignoring' other variables in multiple regression?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Regarding equivalence testing, see: &#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/6225/7290&quot;&gt;Is it possible to prove a null hypothesis?&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/3038/7290&quot;&gt;How to test hypothesis of no group differences?&lt;/a&gt;, and/or&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/85903/7290&quot;&gt;Why do statisticians say a non-significant result means &quot;you can't reject the null&quot; as opposed to accepting the null hypothesis?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;The partial &lt;em&gt;regression&lt;/em&gt; coefficient and the partial &lt;em&gt;correlation&lt;/em&gt; coefficient are not typically expected to agree, because they are not standardized in quite the same way, see: &#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/50156/7290&quot;&gt;Are standardized betas in multiple linear regression partial correlations?&lt;/a&gt;, and/or &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/76815/7290&quot;&gt;Multiple regression or partial correlation?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-01-25T21:02:09.320" Id="83348" LastActivityDate="2014-11-23T03:26:07.597" LastEditDate="2014-11-23T03:26:07.597" LastEditorUserId="7290" OwnerUserId="7290" ParentId="57286" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;This is true that $SS_{tot}$ will change ... but you forgot the fact that the regression sum of of squares will change as well. So let's consider the simple regression model and denote the  Correlation Coefficient as $r_{xy}^2=\dfrac{S_{xy}}{S_{xx}S_{yy}}$, where I used the sub-index $xy$ to emphasize the fact that $x$ is the independent and $y$ is the dependent variable. Obviously, $r_{xy}^2$ is unchanged if you swap $x$ with $y$. We can easily show that $SSR_{xy}=S_{yy}(R_{xy}^2)$, where $SSR_{xy}$ is the regression sum of of squares and  $S_{yy}$ is the total sum of squares where $x$ is independent and $y$ is dependent variable. Therefore: $$R_{xy}^2=\dfrac{SSR_{xy}}{S_{yy}}=\dfrac{S_{yy}-SSE_{xy}}{S_{yy}},$$ where $SSE_{xy}$ is the corresponding residual  sum of of squares  where $x$ is independent and $y$ is dependent variable. Note that in this case, we have $SSE_{xy}=b^2_{xy}S_{xx}$ with $b=\dfrac{S_{xy}}{S_{xx}}$ (See e.g. Eq. (34)-(41) &lt;a href=&quot;http://mathworld.wolfram.com/CorrelationCoefficient.html&quot;&gt;here&lt;/a&gt;.) Therefore: $$R_{xy}^2=\dfrac{S_{yy}-\dfrac{S^2_{xy}}{S^2_{xx}}.S_{xx}}{S_{yy}}=\dfrac{S_{yy}S_{xx}-S^2_{xy}}{S_{xx}.S_{yy}}.$$ Clearly above equation is symmetric with respect to $x$ and $y$. In other words: $$R_{xy}^2=R_{yx}^2.$$ To summarize when you change $x$ with $y$ in the simple regression model, both numerator and denominator of $R_{xy}^2=\dfrac{SSR_{xy}}{S_{yy}}$ will change in a way that $R_{xy}^2=R_{yx}^2.$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-26T03:01:14.440" Id="83360" LastActivityDate="2014-01-26T03:40:21.387" LastEditDate="2014-01-26T03:40:21.387" LastEditorUserId="13138" OwnerUserId="13138" ParentId="83347" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;Your approach seems like a difficult way to address the problem.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For part (a), there are two cases: $A$ either gets his own card (with probability $1/k$), or he does not.  If he gets his own card, then the probability of gaining access to the secure area is 1.  If he does not, then it is $10^{-n}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can now calculate the probabilities of the (disjoint) events:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$F$:=[$A$ gets his own card and gains access to the secure area]&lt;/li&gt;&#10;&lt;li&gt;$G$:=[$A$ gets someone else's card and nonetheless gains access to the secure area]&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Note that the event of gaining access to the secure area is $H=F\cup G$, and the answer to part (b) is therefore $P[F\vert H]$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-26T03:34:14.887" Id="83362" LastActivityDate="2014-01-26T03:34:14.887" OwnerUserId="37928" ParentId="81844" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;One sided confidence intervals are dual to one tailed hypothesis tests just as regular two sided CIs are dual to two tailed tests.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If $\theta$ is a parameter, and we say that $(a,\infty)$ is a one sided CI for $\theta$, then this means that $a$ was found by a process that will yield a value below the true value of $\theta$ $95\%$ of the time. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, the parameter of interest is the difference of means: $\mu_x-\mu_y$.  If you construct a one sided confidence interval for this parameter, of the form $(a,\infty)$, then you can say with 95% confidence that $a&amp;lt;\mu_x-\mu_y$.  Thus, if $0\leq a$, you may reject the null hypothesis.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-26T05:55:59.453" Id="83367" LastActivityDate="2014-01-26T14:10:17.063" LastEditDate="2014-01-26T14:10:17.063" LastEditorUserId="37928" OwnerUserId="37928" ParentId="53168" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am mining for association rules using arules.  I often run new transaction sets with the same code.  I dont change the list of restrictions on which items may appear in the rhs and lhs, but sometimes the transaction set does not have an item that is in my rhs or lhs list.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any way to make R ignore this?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library('arules');&#10;&amp;gt; txn = read.transactions(file=&quot;Query1.csv&quot;,rm.duplicates=FALSE,format=&quot;single&quot;,sep=&quot;,&quot;,cols=c(1,2));&#10;&amp;gt; llist &amp;lt;-scan('gpis.txt', what=&quot;&quot;, sep=&quot;\n&quot;);&#10;Read 5505 items&#10;&amp;gt; rlist &amp;lt;-scan('hccs.txt', what=&quot;&quot;, sep=&quot;\n&quot;);&#10;Read 127 items&#10;&amp;gt; rules &amp;lt;- apriori(txn, parameter = list(confidence = 0.5), appearance = list(rhs = rlist, lhs = llist));&#10;Error in asMethod(object) : &#10;  &quot;blah&quot; is an unknown item label, &quot;blab&quot; is an unknown item label..... &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-26T18:36:01.473" Id="83398" LastActivityDate="2014-01-27T12:38:26.293" OwnerUserId="10458" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;association-rules&gt;" Title="arules R - How to ignore unknown item label in appearance list?" ViewCount="388" />
  
  <row Body="&lt;p&gt;Likelihood is something that you design. &#10;Using Likelihood and prior you compute posterior and use it for inference. &lt;/p&gt;&#10;&#10;&lt;p&gt;Watch this for details: &#10;&lt;a href=&quot;http://videolectures.net/mlss09uk_bishop_ibi/&quot; rel=&quot;nofollow&quot;&gt;http://videolectures.net/mlss09uk_bishop_ibi/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-26T20:40:17.027" Id="83414" LastActivityDate="2014-01-26T20:40:17.027" OwnerUserId="17812" ParentId="83396" PostTypeId="2" Score="-1" />
  
  <row Body="&lt;p&gt;I just found something nice over here: &lt;a href=&quot;http://cran.r-project.org/web/packages/Causata/vignettes/Causata-vignette.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/Causata/vignettes/Causata-vignette.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Try this maybe when using the glmnet Package&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# extract nonzero coefficients&#10;coefs.all &amp;lt;- as.matrix(coef(cv.glmnet.obj, s=&quot;lambda.min&quot;))&#10;idx &amp;lt;- as.vector(abs(coefs.all) &amp;gt; 0)&#10;coefs.nonzero &amp;lt;- as.matrix(coefs.all[idx])&#10;rownames(coefs.nonzero) &amp;lt;- rownames(coefs.all)[idx]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-01-26T20:53:46.600" Id="83417" LastActivityDate="2014-01-26T20:53:46.600" OwnerUserId="37980" ParentId="22085" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Just to add to the discussion, this is mostly an assumption that simplifies the mathematics of inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;To take a concrete example, I am in the field of image processing and usually most algorithms will assume that the noise in the image is IID. This is hardly ever the case because most of the time we do some pre processing on the imaging (for ex: smoothing or averaging) and this will introduce correlation among neighbourhood imaging pixels. Also, pixels belonging to similar structures will have similar properties, the point spread function of the measurement device etc. will all make the IID assumption strictly not true. &lt;/p&gt;&#10;&#10;&lt;p&gt;In any real world case, it usually turns out to be an assumption but it depends on what you are trying to achieve to be able to tell whether the assumption is valid or not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-26T22:42:23.173" Id="83425" LastActivityDate="2014-01-27T00:00:59.783" LastEditDate="2014-01-27T00:00:59.783" LastEditorUserId="36540" OwnerUserId="36540" ParentId="82096" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;There are two methods for training Adaboost. Either use the weight vector directly in the training of the weak learner, or use the weight vector to sample datapoints with replacement from the original data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the latter case the sampled dataset is the same size as the original dataset, and will contain some repeated datapoints. The weight vector is usually a distribution as it makes drawing the weighted sample easier, but any weight vector will work after normalisation. The simplest way to sample the new dataset is to make the weight vector a probability distribution, calculate its cumulative distribution function, then generate N random doubles in the range $(0,1]$. Then test to see what interval the random numbers are in.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for i = 1:N&#10;    rnd = new Random(0.0,1.0)&#10;    for j = 1:N&#10;        if (cdf(j) &amp;lt; rnd)&#10;            samplePoint(i) = dataPoint(j)&#10;        end&#10;    end&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are ways which aren't $O(n^2)$, but this is easier to understand.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-27T02:02:06.993" Id="83437" LastActivityDate="2014-01-27T02:07:38.013" LastEditDate="2014-01-27T02:07:38.013" LastEditorUserId="24084" OwnerUserId="24084" ParentId="45233" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="83455" AnswerCount="1" Body="&lt;p&gt;I've seen the following question asked and answered many times on various forums:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Knowing that the odds of rolling a six with one die are one in six, what&#10;  are the odds of rolling a six in two, three, four (etc.) consecutive rolls? 1/6 + 1/6 + ... wouldn't make sense as you'd have a 100% chance of rolling a six in six rolls.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The answer I've seen is that the odds of rolling a six after n rolls are 1 - (5/6)^n.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now my question is this - lets say that instead of rolling for a six, we pick a number out of a fixed sequence of six unique numbers after each roll. So for example, lets say our sequence is 1, 2, 3, 4, 5, 6. At the first roll we're looking to roll a one, the second roll we're looking for a two, then a three etc. until finally at the seventh roll we go back to one. What are the chances of matching any of the numbers in the sequence in this way after n rolls?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-27T09:31:59.393" FavoriteCount="0" Id="83454" LastActivityDate="2014-01-27T09:37:43.623" OwnerUserId="38002" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;probability&gt;" Title="Probability of Rolling a number that changes over time on One Die" ViewCount="43" />
  
  <row Body="&lt;p&gt;I would encourage you to run this analysis in one model (in AMOS) and I do not think your data structure is problematic (see for example: Maas, CJM &amp;amp; Hox, JJ (2005) Sufficient sample sizes for multilevel modeling. Methodology, 1, 86-92.). &#10;When you run several models on the same dataset you increase the chance of making type I errors (at the minimum you will need to employ the Bonferroni Correction; which is considered to be a conservative technique).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-27T11:16:07.587" Id="83463" LastActivityDate="2014-01-27T11:16:07.587" OwnerUserId="36455" ParentId="68676" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="6" Body="&lt;p&gt;I am wondering if anyone has book references for time series. I would like something comparable (in popularity) to the 'ESL' or to 'Machine learning' from Murphy in the machine learning field. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone knows what are the most complete (in term of methods scope) books containing all about exponential smoothing (all of them), arima, sarima , arch, garch, neural network for time series, kalman filters, ... etc ? &lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2014-01-27T15:11:57.030" CreationDate="2014-01-27T15:09:02.860" FavoriteCount="1" Id="83480" LastActivityDate="2014-02-11T16:55:54.953" LastEditDate="2014-01-27T15:28:09.267" LastEditorUserId="35572" OwnerUserId="35572" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;self-study&gt;&lt;arima&gt;" Title="Good references for time series?" ViewCount="235" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Good morning/afternoon everyone,&lt;/p&gt;&#10;&#10;&lt;p&gt;first of all thanks to all of you for the valuable insights provided. I will be oulining here my current challenge, trying to provide as much detail as possible. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataset with scores associated to a list of clients of my company. These scores have been obtained through a survey designed to assess the riskiness of doing buisness with such clients, whereby every answer has a score associated with it. The higher the sum of the scores obtained by each client in each question, the higher the risk profile. Such scores range from 0 (the minimum) to several thousand. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Data quality:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The data I have is not normally distributed and does not fit a normal distribution even if standardized using Zscore and the NORM.DIST function in Excel. Smarter people than me have found distributions that could fit the data (ie: Gamma, Student T, etc.) but these distributions do not transfer over to the dataset of a different time period. That is, the distribution of the data changes every year a survey is taken and I would like to avoid trying to figure out what distribution fits the data everytime a new survey is done. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I would like to do:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in creating a way to compare the risk scores associated with each client to come up with a way of deciding what qualified to be high risk and what is low risk. Ideally, I would love to have 5 categorizations: Very High, High, Medium, Low, Very Low risk. I need to devise a way to say that a certain score is high as opposed to low or medium.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I ruled out:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I ruled out using the arythmectic mean and standard deviation. The reason is that the majority of clients has relatively low scores (0-400) and few have scores in the thousands. I fear that the mean would understate/overstate the mean risk score, being heavily affected by outliers. I need a more robust measure, so I was thinking about using the median instead. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How do you suggest I go about defining the buckets that I can associate with each of my five desired risk categories (Very High, High, Medium, Low, Very Low risk)? I know I could do it with percentiles relatively simply but that solution is not that appealing to me: it would make somewhat arbitrary to say, for instance, that the top 20 percentiles are very high risk, because it would most likely have a huge gap in between the min and max values of such bucket, whereas this would not be the case with the bottom 20 percentile, since lower score values are much more common.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A note:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would really like to avoid any solutions that involve attributing a distribution to the data, since that will change and aside from the Gaussian one, most have hard to define parameters, at least in the absense of a statistical software more evolved than Excel. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you all very much.&#10;Bernardo&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-27T20:47:36.600" Id="83510" LastActivityDate="2014-01-27T20:47:36.600" OwnerUserId="38038" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;median&gt;&lt;ranks&gt;&lt;scoring&gt;" Title="Creating a model to interpret numerical scores" ViewCount="37" />
  <row Body="&lt;p&gt;The covariances will be the common variance mutliplied by the correlations between the variables, which will be between $-1$ and $1$, though there are some restrictions on the possible values the set of three correlations might take (they can't all be $-1$ at the same time, for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;So if the variables are $Y_1,Y_2$ and $Y_3$, the common variance is $\sigma^2$ and the three correlations are $\rho_{12},\rho_{13}$ and $\rho_{23}$, then the covariance $c_{ij} = \sigma^2 \rho_{ij}$ for each $i,j$ combination.&lt;/p&gt;&#10;&#10;&lt;p&gt;The set of possible values for the three correlations is such that either the correlation matrix or the covariance matrix (either implies the other given the variances are all positive) is &lt;em&gt;positive semi-definite&lt;/em&gt;. A symmetric $n × n$ real matrix $A$ is positive semi-definite if $x^TAx \geq 0$ for every non-zero column vector $x$ of $n$ real numbers. &lt;/p&gt;&#10;&#10;&lt;p&gt;(In your case, $n=3$ of course.)&lt;/p&gt;&#10;&#10;&lt;p&gt;So, for an example of a correlation matrix that cannot occur, consider the case where all the correlations $\rho_{12},\rho_{13}$ and $\rho_{23}$ are $-0.8$. Then if you take the vector $x$ above to be all $1$'s,  $x^TAx = -1.8$, so there's at least one $x$ for which $x^TAx$ isn't at least $0$, and so the matrix $A$ isn't positive semi-definite, and cannot be a correlation matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; A&#10;     [,1] [,2] [,3]&#10;[1,]  1.0 -0.8 -0.8&#10;[2,] -0.8  1.0 -0.8&#10;[3,] -0.8 -0.8  1.0&#10;&#10;&amp;gt; x&#10;     [,1]&#10;[1,]    1&#10;[2,]    1&#10;[3,]    1&#10;&#10;&amp;gt; t(x) %*% A %*% x   #  x' A x&#10;     [,1]&#10;[1,] -1.8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So if we try to do anything that relies on it being positive semi-definite, it will fail:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; chol(A)&#10;Error in chol.default(A) : &#10;  the leading minor of order 3 is not positive definite&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;[Why does that definition of what's a possible correlation matrix work? Consider that we define a new random variable, $Z = x^TY$. Then $\text{Var}(Z) = x^T\text{Var}(Y)x = \sigma^2 x^TAx$. Clearly, if $x^TAx$ could be negative, there'd be a random variable, $Z$ with a negative variance. So we need $x^TAx\geq 0$ for every possible $x$.]&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;edit: some more-or-less related questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;The bound on a &lt;em&gt;common&lt;/em&gt; correlation of three variables (the bounds on $\rho$ for the case $\rho_{12}=\rho_{13}=\rho_{23}=\rho$) is discussed in &lt;a href=&quot;http://stats.stackexchange.com/questions/72790/bound-for-the-correlation-of-three-random-variables&quot;&gt;this question&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you specify two correlations and look at the possible values of the third, there's some relevant discussion in &lt;a href=&quot;http://stats.stackexchange.com/questions/5747/if-a-and-b-are-correlated-with-c-why-are-a-and-b-not-necessarily-correlated&quot;&gt;this question&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-27T21:52:35.903" Id="83520" LastActivityDate="2014-01-27T23:28:58.687" LastEditDate="2014-01-27T23:28:58.687" LastEditorUserId="805" OwnerUserId="805" ParentId="83517" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Uncharacteristically, I am answering my own question! I found a package in R called &lt;code&gt;poLCA&lt;/code&gt;, which is capable of handling polytomous/categorical/factor &quot;manifest&quot; variables (the outcome variables used to define the latent classes) and &quot;concomitant&quot; variables, which have effects on the probability of class membership. The package was developed by Linzer and Lewis (&lt;a href=&quot;http://cran.r-project.org/web/packages/poLCA/poLCA.pdf&quot; rel=&quot;nofollow&quot;&gt;2011&lt;/a&gt;). Oh, and to help you avoid the web search, yes, it can handle ordinal manifest variables. Happy birthday to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Linzer, DA, &amp;amp; Lewis, JB. (2011). poLCA: An R package for polytomous variable latent class analysis. Journal of Statistical Software 42(10):1-29.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-28T02:33:08.253" Id="83541" LastActivityDate="2014-01-28T02:33:08.253" OwnerUserId="7616" ParentId="83446" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I will try to expand a bit the comment of user603.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can do what you propose and usually it helps if you have a large enough sample. Indeed, this process will lead to increasing the dimensionality of the space. The increased dimensionality is the reason you need more samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;The easiest way to achieve what you want is to use a non-linear kernel (aka deploy a kernel trick). For example, using a polynomial kernel of degree 2 will &quot;create&quot; features by doing pairwise combination of your features. Similarly, for a polynomial kernel of degree 3 you will end up with combinations of triplets of features. &lt;/p&gt;&#10;&#10;&lt;p&gt;The trade-off of using a non-linear kernel is that solutions are in the dual, i.e. you end up with combinations of features that you can not interpret directly, e.g., you can't make directly statements like &quot;features A,B,C and/or their combinations are the most important&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-28T05:53:23.973" Id="83547" LastActivityDate="2014-01-28T10:51:19.187" LastEditDate="2014-01-28T10:51:19.187" LastEditorUserId="603" OwnerUserId="37188" ParentId="83442" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In Var models instead of using several dependent variable we use several independent variable and their effect on one dependent variable.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-28T06:40:54.403" Id="83551" LastActivityDate="2014-01-28T06:40:54.403" OwnerUserId="38074" ParentId="12966" PostTypeId="2" Score="-5" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there any way to estimate the bias of the estimate of the betas in a linear regression model when the actual beta values are unknown?&lt;/p&gt;&#10;&#10;&lt;p&gt;The well known Mean Square Error (MSE) criterion is used to quantify the performance of different biased estimators but to calculate the bias you need to know what the correct value of the Betas should be.&lt;/p&gt;&#10;&#10;&lt;p&gt;$MSE= var^2 +bias^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;What can you do when you don't know what the correct value is? &lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if you can use the OLS value as a proxy (as it is unbiased)? However the very situations in which biased estimators are likely to be used are those in which OLS is likely to struggle, just wondered if there were any other methods to estimate the bias?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-01-28T09:55:41.893" FavoriteCount="1" Id="83574" LastActivityDate="2014-01-28T10:39:27.413" LastEditDate="2014-01-28T10:39:27.413" LastEditorUserId="686" OwnerUserId="25973" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;bias&gt;&lt;ridge-regression&gt;" Title="Estimating Bias in a Linear Regression Model" ViewCount="132" />
  <row Body="&lt;p&gt;Since Y is bounded by 0 and 1, ordinary least squares regression is not well-suited. You could try beta regression. In &lt;code&gt;R&lt;/code&gt; there is the &lt;code&gt;betareg&lt;/code&gt; package. &lt;/p&gt;&#10;&#10;&lt;p&gt;Try something like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages(&quot;betareg&quot;)&#10;library(betareg)&#10;betamod1 &amp;lt;- betareg(y~x, data = DATASETNAME)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/betareg/betareg.pdf&quot; rel=&quot;nofollow&quot;&gt;more info&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: If you'd like a full account of beta regression, its advantages and disadvantages, see &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/16594767&quot; rel=&quot;nofollow&quot;&gt;A better lemon squeezer: Maximum likelihood regression with beta distributed dependent variables&lt;/a&gt; by Smithson and Verkuilen&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-01-28T10:55:56.023" Id="83578" LastActivityDate="2014-01-30T22:02:05.707" LastEditDate="2014-01-30T22:02:05.707" LastEditorUserId="686" OwnerUserId="686" ParentId="83554" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;(1) You will likely need some kind of autoregressive structure, simply because we expect measurements taken further apart to be less correlated than those taken closer together. Exchangeable would assume they are all equally correlated. But as with everything else, it depends. &lt;/p&gt;&#10;&#10;&lt;p&gt;(2) I think this kind of decision comes down to thinking about how the data were generated, rather than seeing how they look.&lt;/p&gt;&#10;&#10;&lt;p&gt;(4) it depends. For example, kids nested in schools should not, in most cases, be treated as independent. Due to social patterning, etc, if I know something about a kid in a given school, then I probably know at least a little bit about other kids in the schools. I once used GEE to look at relationships between different social and economic indicators and obesity prevalence in a birth cohort where participants were nested in neighborhoods. I used an exchangeable structure. You can find the &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2806889/&quot; rel=&quot;nofollow&quot;&gt;paper here&lt;/a&gt; and check some of the references, including 2 from epi journals.&lt;/p&gt;&#10;&#10;&lt;p&gt;(5) Apparently so (e.g. see &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2896994/&quot; rel=&quot;nofollow&quot;&gt;this example&lt;/a&gt;), but I can't help with the R specfics of doing this. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/3233245&quot; rel=&quot;nofollow&quot;&gt;Zeger SL, Liang KY, Albert PS. Models for longitudinal data: a generalized estimating equation approach. Biometrics. 1988;44:1049–60.&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20220526&quot; rel=&quot;nofollow&quot;&gt;Hubbard AE, Ahern J, Fleischer N, van der Laan M, Lippman S, Bruckner T, Satariano W. To GEE or not to GEE: comparing estimating function and likelihood-based methods for estimating the associations between neighborhoods and health. Epidemiology. 2009&lt;/a&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/12578807&quot; rel=&quot;nofollow&quot;&gt;Hanley JA, Negassa A, Edwardes MDB, Forrester JE. Statistical analysis of correlated data using generalized estimating equations: an orientation. Am J Epidemiol. 2003;157:364.&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-28T10:59:23.120" Id="83582" LastActivityDate="2014-01-30T13:30:23.477" LastEditDate="2014-01-30T13:30:23.477" LastEditorUserId="16049" OwnerUserId="16049" ParentId="83577" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I think you should check &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&quot; rel=&quot;nofollow&quot;&gt;Dr. LeCun's work&lt;/a&gt;, which discusses several loss functions. It uses RBF's units in the output, so I'm not sure it's exactly what you need. As for me it's quite hard to implement it with Theano, so I use softmax and it works good for my tasks. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-28T12:48:22.293" Id="83597" LastActivityDate="2014-01-28T12:48:22.293" OwnerUserId="35873" ParentId="83107" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Using the KNN classifier, given a test data-point $x$, how do we get the probability of membership of $x$ to each class $y_i$, that is the probabilities $P(y_i | x)$ for $i = 1, 2, .., n$ (where $n$ is the number of classes).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-28T13:29:21.917" Id="83600" LastActivityDate="2014-02-28T18:42:05.390" OwnerUserId="8114" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;classification&gt;&lt;k-nearest-neighbour&gt;&lt;supervised-learning&gt;" Title="How to obtain the class conditional probability when using KNN classifier?" ViewCount="327" />
  <row Body="&lt;p&gt;For text it is common, if not mandatory, to do some preprocessing steps that you don't mention: lowercasing, stemming, stop-word removal. For twitter spacial care should be taken for hashes (you probably want them as hashes, not standard words).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sure that linear SVMs will do better than naive Bays. Not sure about logistic regression. In terms of methodology you follow same approach as you did with naive Bays. &lt;/p&gt;&#10;&#10;&lt;p&gt;You should pay attention on the measurement of performance you use. Do you really want accuracy? Accuracy can be easily misleading in these problems. Probably it is better if you go for precision/recall per sentiment of interest. For example,  precision 95%, recall 50%: I am able to detect 50% of the tweets with sentiment, and from those that I detect I am gonna be correct in 95% of the cases. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-28T13:56:21.480" Id="83605" LastActivityDate="2014-01-28T13:56:21.480" OwnerUserId="37188" ParentId="83601" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I think this would depend on what you really want to know. Could you say that you are really interested in how much of the patented technologies patented eventually failed? Or is the number of failures not really a part of the number of technologies patented, and is the latter really just a control variable? &lt;/p&gt;&#10;&#10;&lt;p&gt;If your dependent variable is in reality a proportion: the proportion of technologies that failed, you could use logistic regression on these 'real proportions'. In that case, you could say that your imaginairy binary data are aggregated but therefore still binomially distributed.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-01-28T17:40:25.810" Id="83637" LastActivityDate="2014-01-28T17:40:25.810" OwnerUserId="37510" ParentId="83635" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Rounding your response variable to an integer is NOT OK.  For simplicity, lets assume you're conducting a Poisson regression.  What you're modeling is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \begin{align*}&#10;E(Y|x) &amp;amp;= \beta^{T}x + \beta_{0} \\[0.5em]&#10;\log \left( \frac{\mbox{No. of Deer}}{\mbox{Area}} \right) &amp;amp;= \beta^{T}x + \beta_{0} \\[0.5em]&#10;\log(\mbox{No. of Deer}) - \log(\mbox{Area}) &amp;amp;= \beta^{T}x + \beta_{0} \\[0.5em]&#10;\log(\mbox{No. of Deer}) &amp;amp;= \beta^{T}x + \beta_{0} + \log(\mbox{Area})&#10;\end{align*}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, this is done use the following command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(No. of Deer ~ x + offset(log(Area)), family=poisson(link=log), data=data.frame)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This allows you to use Poisson (or Quasi-Poisson or Negative Binomial) regression for a continuous response, even though the No. of Deer is still a count.  Your parameter estimates (i.e., $\beta_{0}$ and $\beta \mbox{s}$) will be on the log scale, so just exponentiate them to obtain estimates on the raw scale.  Also, be careful of the parametrization used for the negative binomial distribution, if you decide to go with negative binomial regression.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-28T18:17:21.050" Id="83642" LastActivityDate="2014-01-28T18:17:21.050" OwnerUserId="13317" ParentId="83636" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;The forms $\ln$ or $\log_\mathrm{e}$ are standard in all fields I'm familiar with.&lt;sup&gt;&amp;dagger;&lt;/sup&gt; Though $\mathrm{Ln}$ might've become a standard, it didn't; &amp;amp; though it's unlikely to cause more than a momentary hesitation on the reader's part, even that is worth taking pains to avert. Moreover, I've noticed that its use is correlated with the commission of graver mathematical solecisms, so you may want to eschew it to avoid making a bad impression among those that know and care about such things.&lt;/p&gt;&#10;&#10;&lt;p&gt;In business notation is often sloppy, &amp;amp; while you're likely to see $\mathrm{Ln}$ often enough, it doesn't constitute an alternative standard, or necessarily result from a deliberate choice&amp;mdash;could well be due to Powerpoint's autocorrect feature.&lt;/p&gt;&#10;&#10;&lt;p&gt;&amp;dagger; Complex analysis isn't one of them&amp;mdash;see @Zen's answer.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-28T21:46:15.483" Id="83666" LastActivityDate="2015-02-12T10:28:33.283" LastEditDate="2015-02-12T10:28:33.283" LastEditorUserId="17230" OwnerUserId="17230" ParentId="83663" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;Within categories of the covariates there will be a calculation of the cumulative hazard as a function of the time from beginning of the observations, summing intervals until either an event or a final censoring. As an example with your data, the &quot;winter&quot; intervals had two entrants with three intervals and 2 events,  first of which was at 118-95 time units for subject 1 and the second of which was at (21-1)+(141-118) units for subject 2. So the cumulative hazard function would be a step function within that covariate would rise to 50% at t=23 and 100% at t=43.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-28T23:25:10.280" Id="83671" LastActivityDate="2014-01-28T23:25:10.280" OwnerUserId="2129" ParentId="82087" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Google's My Track android app allows output of long/lat.&#10;However, I wrote my own client to capture the data every 5 minutes.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Time dimension - depending on how you want to do it...  I &quot;normalize&quot; the data upfront to do every hour, so the grouping makes more sense.  For example&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;2&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;37.88    -122.22    11&#10;37.88    -122.22    11&#10;37.88    -122.22    11&#10;37.88    -122.22    11&#10;37.33    -122.50    12&#10;37.33    -122.51    12&#10;37.33    -122.52    12&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The k-means algorithm, if implemented properly, can handle a matrix.  Using the coursera's machine learning exercise #8, I modified it to handle/visualize 3 dimensional data.  Not too bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think more than 3 dimensions can be visualized, though a vectorized implementation will still work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers,&#10;Simon&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-29T00:31:25.360" Id="83674" LastActivityDate="2014-01-29T00:31:25.360" OwnerUserId="37768" ParentId="83192" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This may seem like you're answering your own question because all of the following sketches were mentioned by you in the comment to another answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Logistical Regression seems to be the most analytical. It works well here, but may not do so with the other (non-logistic) patterns. Here's the visual my software gives and the p-value is &amp;lt;0.0001. Dots have random Y positions within their regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XBBBI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For less regular patterns a decision tree is better. Here's one partitioning. Dots have random X and Y positions within their regions (I think because there are usually multiple X variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/G4QVJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Composing smoothed densities:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/afIUh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Composing smoothed proportions:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tMJnQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-29T01:32:57.290" Id="83679" LastActivityDate="2014-01-29T01:32:57.290" OwnerUserId="1191" ParentId="83573" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;My understanding is that a deviate/variate a particular outcome of random variable. If I generate a 100 draws from standard normal (a random variable) using my favorite PRNG, I get 100 &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_normal_deviate&quot;&gt;standard normal deviates&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-29T02:09:16.027" Id="83680" LastActivityDate="2014-01-29T02:17:10.057" LastEditDate="2014-01-29T02:17:10.057" LastEditorUserId="7071" OwnerUserId="7071" ParentId="83677" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="83693" AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Assume $X$ and $Y$ are independent random variables. Is $Median(XY) = Median(X) \cdot Median(Y)$? If so, how would one prove this? If not, what conditions would be sufficient for this relationship to hold?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Additional question:&lt;/strong&gt; Does the relationship hold for $\alpha$-trimmed means?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Based on a conversation with @Glen_b in the comments on his answer, as well as the contribution of @nikie, it appears that sufficient conditions for the relationship to hold are:&#10;1) independence, and&#10;2) at least one of the distributions of $X$ and $Y$ has a median of zero.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-29T05:08:58.797" Id="83690" LastActivityDate="2014-01-31T00:44:30.747" LastEditDate="2014-01-31T00:44:30.747" LastEditorUserId="16319" OwnerUserId="16319" PostTypeId="1" Score="5" Tags="&lt;independence&gt;&lt;median&gt;&lt;trimmed-mean&gt;" Title="Given independence, is the median of a product equal to the product of the medians?" ViewCount="184" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;My apologies for another stats beginner question. This feels like reinventing the wheel, unfortunately I just can't find the answer myself. Googling &quot;average densities classification&quot; does not help me. &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;classifying using histogram&quot; brings up interesting pages, but &lt;a href=&quot;http://cs.ecs.baylor.edu/~hamerly/courses/5325_11s/papers/svm/chapelle1999image.pdf&quot; rel=&quot;nofollow&quot;&gt;too obscure for me&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to classify x, and find a probability that x is of Class 0 or Class 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I noticed that Class 0 and 1 have slightly different average densities:&lt;/p&gt;&#10;&#10;&lt;p&gt;here are some plots showing the &lt;strong&gt;average histograms for Class 0 (blue) and Class 1 (red)&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Aq19v.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to use the histogram to classify new images?&lt;/p&gt;&#10;&#10;&lt;p&gt;Intuitively, the closer the histogram is to the average histogram of the class, the higher the probability to belong to that class.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried using breaks of the histogram as feature, and this gives me some results, but I have a feeling I am taking the long way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any ready-made R/Python functions to classify data/images based on histogram?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-29T12:15:05.660" FavoriteCount="1" Id="83729" LastActivityDate="2014-01-31T17:31:23.670" LastEditDate="2014-01-31T16:54:26.117" LastEditorUserId="21720" OwnerUserId="21720" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;classification&gt;" Title="Image classification using histogram" ViewCount="302" />
  <row AcceptedAnswerId="83740" AnswerCount="1" Body="&lt;p&gt;I am trying to calculate chi-squared value for my fitted data using:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\chi^2 = \sum_i^n{\frac{(y-f(x))^2}{f(x)}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $f(x)$ are theoretical values from fitted function and $y$ are observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if expected values are too small, I receive (logically) very large numbers. For example if I calculate chi-squared for values that are listed below, I'll receive (in C++): -inf as a result. But statistical software which I use as a reference outputs chi-squared as 0.39.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please can you suggest what I am doing wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example data follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y:  | f(x):&#10; 0  |  0.000233516 &#10; 0  |  0.000748074  &#10; 0  |  0.00226688 &#10; 1  |  0.00649784 &#10; 1  |  0.0176183 &#10; 1  |  0.0451873 &#10; 1  |  0.109628 &#10; 0  |  0.251586 &#10; 0  |  0.546141 &#10; 0  |  1.12145 &#10; 0  |  2.17825 &#10; 1  |  4.00215 &#10; 3  |  6.9556 &#10; 6  | 11.4349 &#10;17  | 17.7821 &#10;22  | 26.1572 &#10;42  | 36.3961 &#10;41  | 47.9043 &#10;61  | 59.6417 &#10;79  | 70.2394 &#10;83  | 78.2468 &#10;82  | 82.4535 &#10;74  | 82.1877 &#10;81  | 77.4925 &#10;58  | 69.1145 &#10;73  | 58.3087 &#10;39  | 46.5322 &#10;34  | 35.1261 &#10;14  | 25.082 &#10;24  | 16.9414 &#10;19  | 10.8241 &#10;16  |  6.5417 &#10;11  |  3.73977 &#10; 4  |  2.02234 &#10; 4  |  1.03448 &#10; 4  |  0.500544 &#10; 4  |  0.229097 &#10; 2  |  0.0991863 &#10; 2  |  0.04062  &#10; 2  |  0.0157356 &#10; 1  |  0.00576612 &#10; 2  |  0.00199866 &#10; 1  |  0.000655315 &#10; 0  |  0.000203244 &#10; 0  |  5.96265e-05 &#10; 0  |  1.65469e-05 &#10; 0  |  4.34361e-06 &#10; 0  |  1.07855e-06 &#10; 0  |  2.53329e-07 &#10; 1  |  5.62841e-08  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-01-29T12:22:04.557" Id="83732" LastActivityDate="2014-01-29T13:06:36.820" LastEditDate="2014-01-29T13:06:36.820" LastEditorUserId="22047" OwnerUserId="30356" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;goodness-of-fit&gt;&lt;fitting&gt;" Title="Chi-squared Goodness of Fit - very small expected values" ViewCount="135" />
  <row Body="&lt;p&gt;There are improper priors, for example Jeffreys, which has a certain relation to Fishers Information matrix. Then it is not subjective.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-29T12:51:19.303" Id="83735" LastActivityDate="2014-01-29T12:51:19.303" OwnerUserId="28732" ParentId="83731" PostTypeId="2" Score="-2" />
  <row AnswerCount="1" Body="&lt;p&gt;Is there a close formula (or some kind of bound) on the EMD between $x_1\sim N(\mu_1, \Sigma_1)$ and $x_2 \sim N(\mu_2, \Sigma_2)$?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-29T13:24:02.530" Id="83741" LastActivityDate="2015-02-24T15:42:44.550" OwnerUserId="38175" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;distance&gt;" Title="Earth Mover's Distance (EMD) between two Gaussians" ViewCount="153" />
  
  
  
  <row Body="&lt;p&gt;Why do you want to take the logarithm of the x-axis? If one observes data over time that seems to behave exponentially, one takes the logarithm of the observed data. So, I suggest you take the logarithm of the weights and plot this against time. Then you can just click on the plotted line of the graph, click on the line an add a trend line. After taking the logarithm you should take a linear trend line.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-29T14:46:36.157" Id="83759" LastActivityDate="2014-01-29T14:46:36.157" OwnerUserId="38160" ParentId="83746" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I found one formula to calculate the pairwise distance between samples according to their SNPs in the following paper: Siu, Jin and Xiong, &lt;a href=&quot;http://www.plosone.org/article/fetchObject.action?uri=info%3adoi/10.1371/journal.pone.0029901&amp;amp;representation=PDF&quot; rel=&quot;nofollow&quot;&gt;Manifold Learning for Human Population Structure&#10;Studies&lt;/a&gt;, PLoS One (2012) 7(1), p. 13. The formula is&lt;/p&gt;&#10;&#10;&lt;p&gt;$d = 1-\frac{1}{2k}\sum_{j=1}^k (\frac{1}{P_{A_j}}IBS_{A_j}+\frac{1}{P_{a_j}}IBS_{a_j}+\frac{1}{P_{N_j}}IBS_{N_j})$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $IBS_{A_j},IBS_{a_j},IBS_{N_j}$ be the number of copies of alleles $A_j,a_j$ and missing allele $N_j$ at the $j$-th SNP shared by the pair of individuals, respectively, and $P_{A_j},P_{a_j},P_{N_j}$ are the frequencies of the alleles $A_j,a_j,$ and $N_j$, respectively, and $k$ is the number of SNPs.&lt;/p&gt;&#10;&#10;&lt;p&gt;For simplicity, let's assume that there is no missing values and $k=1$. We would have cases like AA-AA, Aa-Aa, aa-aa, AA-Aa, Aa-aa, AA-aa. &lt;/p&gt;&#10;&#10;&lt;p&gt;The distance for the first three case should be $0$. Let's use the formula to calculate the case for Aa-Aa: we should have $IBS_A=1,IBS_a=1,P_A=\frac{1}{2},P_a=\frac{1}{2}$, then the formula gives me a negative distance. How could that be possible?&lt;/p&gt;&#10;&#10;&lt;p&gt;To my understanding, $d$ can equal to $0,\frac{2}{3},1$. We have $\frac{2}{3}$ because 'weighted'? Is there any other formula to calculate the pairwise distance according to sample's SNPs?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-29T15:04:31.970" Id="83763" LastActivityDate="2014-01-29T15:57:15.477" LastEditDate="2014-01-29T15:57:15.477" LastEditorUserId="930" OwnerUserId="38184" PostTypeId="1" Score="1" Tags="&lt;distance-functions&gt;&lt;genetics&gt;" Title="Making sense of the allele frequency weighted genetic distance" ViewCount="85" />
  
  
  <row Body="&lt;p&gt;I'm unaware of any data-driven classification approaches where the class labels have any effect on the resulting class decisions; this is the case even if the training set is unbalanced.  Although not usually explicit, that the results are independent under permutations of the class labels is an essential part of how these algorithms are set up.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're not doing a strictly data-driven problem, the identification of class labels might contain information that could affect your results.  For example, if the &quot;positive&quot; cases are, say, positive for some disease, and your &quot;negative&quot; cases are from a random sample of healthy people, &lt;em&gt;and you knew that the occurence of the disease were rare&lt;/em&gt;, you may modify how you set up and use the results of any classification scheme.  However, this relies on the existence of domain specific knowledge separate from the labels themselves.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-29T20:56:35.107" Id="83801" LastActivityDate="2014-01-29T20:56:35.107" OwnerUserId="16859" ParentId="83799" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;This is &lt;em&gt;almost&lt;/em&gt; the condition for the cumulative distribution function to be &lt;em&gt;log-concave&lt;/em&gt; , which is a very useful property with many applications. But &lt;em&gt;almost&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A function $F(x)$ is log-concave if&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {\partial^2 \ln F(x)}{\partial x^2} \le 0 \Rightarrow F''(x)F(x) - \left[F'(x)\right]^2 \le 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Write $\phi(x)$ in terms of $F(x)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi(x) \equiv \frac{F'(x)}{F(x)+xF'(x)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and we want &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {\partial \phi(x)}{\partial x} \le 0 \Rightarrow F''(x)\Big(F(x)+xF'(x)\Big)-F'(x)\Big(F'(x)+F'(x) +xF''(x)\Big) \le 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow F''(x)F(x)-2\left[F'(x)\right]^2 \le 0 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;...which is not enough for log-concavity, due to the existence of the factor $2$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that the condition is satisfied. If we divide by $[F(x)]^2$ and rearrange we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {\partial \phi(x)}{\partial x} \le 0 \Rightarrow \frac {\partial^2 \ln F(x)}{\partial x^2} \le \left( \frac{F'(x)}{F(x)}\right)^2 = \left(\frac {\partial \ln F(x)}{\partial x}\right)^2$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-30T02:24:36.327" Id="83822" LastActivityDate="2014-01-30T11:05:12.837" LastEditDate="2014-01-30T11:05:12.837" LastEditorUserId="28746" OwnerUserId="28746" ParentId="19986" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;What is the difference between a traditional p-value {(b1 - b0) / se(b1)} and log-likelihood ratio between a model with the variable in question, and a model without.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Depends on what you do with the likelihood ratio. If - as is sometimes done - you work out the LR test statistic and then manipulate it in order to get a statistic whose small sample distribution can be worked out, you're rejecting the same cases as the LR test should, without using asymptotic results. But usually when people apply an LR test they usually base their likelihood ratio test on applying Wilk's theorem, yielding an asymptotic chi-square approximation. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you take the t-statistic you mentioned, it's asymptotically normal... if you square that, you have what is (that same) asymptotic chi-square test statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a result, in small samples from normal distributions, the two tests aren't identical, but as n increases they become more similar (reject exactly the same cases, as the significance level of the approximation becomes exact).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the log-likelihood case, would a significant LLR suggest that the variable had a significant impact on the model?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes, up to the fact that your test is not exactly at the desired significance level. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Assuming the LL for the full model was better than the subset model&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It's always at least as good.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example in R. I am going to use the &lt;code&gt;glm&lt;/code&gt; function because I can get the t-ratio and the asymptotic chi-square test statistic from the output. I will use the &lt;code&gt;cars&lt;/code&gt; data available in R (a few less interesting lines have been cut out).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(glm(dist~speed,cars,family=gaussian))&#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -17.5791     6.7584  -2.601   0.0123 *  &#10;speed         3.9324     0.4155   9.464 1.49e-12 ***&#10;&#10;(Dispersion parameter for gaussian family taken to be 236.5317)&#10;&#10;    Null deviance: 32539  on 49  degrees of freedom&#10;Residual deviance: 11354  on 48  degrees of freedom&#10;AIC: 419.16&#10;&#10;Number of Fisher Scoring iterations: 2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the $236.5317$ in the output is simply $11354/48$ (up to rounding error, since the actual residual deviation is closer to 11353.52)&lt;/p&gt;&#10;&#10;&lt;p&gt;LR test statistic = $\frac{(32539-11353.54)}{236.5317} = 89.565$&lt;/p&gt;&#10;&#10;&lt;p&gt;t-test statistic = $9.464$&lt;/p&gt;&#10;&#10;&lt;p&gt;$9.464^2 = 89.567$ (difference in last figure is simply rounding error in both calculations)&lt;/p&gt;&#10;&#10;&lt;p&gt;So the difference is basically the same as the difference between looking up 9.464 in $t_{48}$ tables and in normal tables.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-30T04:08:05.320" Id="83830" LastActivityDate="2014-01-30T04:13:31.007" LastEditDate="2014-01-30T04:13:31.007" LastEditorUserId="805" OwnerUserId="805" ParentId="83825" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;It was my understanding that the training data has to be labeled to create a model and then label any unseen data. As in the case of using ANN, how is Google able to identify &lt;a href=&quot;http://googleblog.blogspot.co.uk/2012/06/using-large-scale-brain-simulations-for.html&quot; rel=&quot;nofollow&quot;&gt;cats&lt;/a&gt; in Youtube videos without labeling the videos or images?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am new to ML and tried to read some literature, but couldn't get the correct explanation. Would appreciate any pointers/explanation in more of layman terms.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-01-30T09:24:35.473" Id="83842" LastActivityDate="2014-01-30T09:24:35.473" OwnerUserId="12580" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;" Title="Processing unlabelled data with ANN" ViewCount="25" />
  <row AcceptedAnswerId="83856" AnswerCount="1" Body="&lt;p&gt;For expectations of random variables (RVs) $X$ and $Y$ it is true that $$E(X+Y)=E(X)+E(Y)$$. My question is whether when conditioning on RV vector $Z_{1...J}$, it is also true that $$E(X+Y|Z_{1...J})=E(X|Z_{1...J})+E(Y|Z_{1...J})$$.&#10;Thanks for enlightening answers!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-01-30T12:21:14.180" Id="83854" LastActivityDate="2014-01-30T12:36:37.283" OwnerUserId="24515" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;random-variable&gt;&lt;conditional-expectation&gt;" Title="Is this statement about the conditional expectation of a sum true?" ViewCount="74" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let's say I have the following data on leads, monthly media spend, and clicks&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Month  Leads   Media     Clicks&#10;Jan     150    1000       500&#10;Feb     200    1000       550 &#10;March   300    1200       800&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let's say I run a linear regression where y is leads and the predictors are media and clicks. That's good, I know the relationships between these variable and can generate some lags to produce predictions. But what if I had spent 500 (or 2000 or 0) on media, what would have occurred. How do I perform this type of 'counter-factual' analysis where I attempt to find the results of a model if the actual value from one or two of the predictors was lower or higher? What is the standard approach (aka statistically proper approach)? Is it just a matter to &quot;adjusting&quot; the data to the 'new' number and rerunning the regression? or maybe simulating a regression 100+ times with 100+ different values for media?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-30T13:56:21.893" Id="83861" LastActivityDate="2014-01-30T15:40:24.613" LastEditDate="2014-01-30T14:46:23.203" LastEditorUserId="29612" OwnerUserId="29612" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;" Title="How to test counter factuals" ViewCount="60" />
  <row Body="&lt;p&gt;here is basically the model you're looking. &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;x_{j,t} = \sum_{l=1}^n \sum_{k=1}^{K} A_{i,k}x_{k, t-l}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;In this equation you write the random variable $x_j$ at time t being linearly dependent on observed value of all $K$ random variables in the past $n$ previous time steps. In a more compact matrix notation, the above equation looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;X = AT&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where matrix $T$ is a $K\times n$ matrix of your past observations for the $K$ variables. Using this, it's obvious to see the maximum likelihood estimate for your interaction matrix $A$ is:&#10;\begin{equation}&#10;A_{MLE} = X \, T^{t} \, (T T^{t})^{-1}&#10;\end{equation}&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-30T14:00:33.720" Id="83862" LastActivityDate="2014-01-30T14:00:33.720" OwnerUserId="38206" ParentId="83855" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a bunch of records, T in total. I want to know how many of these I can get away with analyzing in order to extrapolate the analysis to the entire population T.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know this is a basic question and largely depends on how much error I can accept, but can anyone tell me the math?&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-01-30T15:55:55.457" Id="83876" LastActivityDate="2014-01-30T18:06:53.140" LastEditDate="2014-01-30T18:06:53.140" LastEditorUserId="7290" OwnerUserId="38240" PostTypeId="1" Score="-2" Tags="&lt;sampling&gt;&lt;sample-size&gt;&lt;finite-population&gt;" Title="If I have T total records, how big should my sample size be for a valid analysis?" ViewCount="86" />
  
  
  <row Body="&lt;p&gt;The sample size of 30 is typically a rule of thumb for how large of a sample size you need for the sample average to be approximatelly normally distributed. This is necessary if you are doing some kind of inference on a population parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;As another user pointed out, you might need a larger sample size if you want your hypotheses test to have a particular power. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-01-30T18:01:44.970" Id="83896" LastActivityDate="2014-01-30T18:01:44.970" OwnerUserId="17661" ParentId="83893" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm looking for an approximation to the curve of a lognormal distribution, for use in non-linear regression against a dataset. As an alternative, I'm interested in an approximation to the CDF thereof.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have several goals:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Determine how closely a sample dataset drawn from a process of unknown distribution matches a lognormal distribution.&lt;/li&gt;&#10;&lt;li&gt;Given a set of samples drawn from a process determined or known to be lognormally distributed, but with unknown characteristics, determine the probability of a observing a future sample with a given value that may lie outside the range of values observed thus far.&lt;/li&gt;&#10;&lt;li&gt;Given a set of samples as above, and a new sample that may lie outside the range of values in the samples already seen, determine the likelihood of having observed that sample.&lt;/li&gt;&#10;&lt;li&gt;While computing the above, efficiency and simplicity of the implementation is quite important. A fast approximation with well-understood properties such as error bounds is better than an accurate algorithm that is difficult to implement or computationally intensive.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For these purposes, I think it is best if the approximation has a closed form so that a nonlinear regression can be done with lower computational overhead. Ideally if it is an approximation to the lognormal distribution itself, then it would be nice if it has a simple integral as well so that the CDF can be approximated too.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's possible that I'm trying to go about this the wrong way. Here's a more specific question to help figure that out: suppose I gather 1000 samples from my process. The values of the samples mostly range from 1 to 10, with occasional samples up to 20 or so. I know (based on experience) that long-term it is quite possible to see samples in the range of 10 times higher than that, but I haven't observed any from &lt;em&gt;this&lt;/em&gt; process yet. How can I determine the probability of the 1001st sample having a value greater than 100 or another arbitrary number? If the 1001st sample's value is 180, how can I determine how likely that was, based on the first 1000 samples?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-01-30T20:24:18.470" Id="83911" LastActivityDate="2014-01-31T12:16:07.290" LastEditDate="2014-01-31T12:16:07.290" LastEditorUserId="37740" OwnerUserId="37740" PostTypeId="1" Score="2" Tags="&lt;lognormal&gt;&lt;approximation&gt;" Title="Approximation to Lognormal Distribution" ViewCount="134" />
  <row Body="&lt;p&gt;The problem of missingness can be handled by a range of different methods (See &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1839993/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for a review). In general, in order to handle missingness you need to be able to make the assumption that conditional on observed covariates, the probability of missing an observation is independent of the true value of the observation. This is called the missing at random assumption. Multiple Imputation (MI) seems to be favored for many applications, but it does require the correct specification of the models used for imputation. &lt;/p&gt;&#10;&#10;&lt;p&gt;You also have structural missingness in that some variables just aren't collected in some waves. I don't think MI is necessarily appropriate for structural missingness. I've been told to use last value carried forward in that context, but I haven't seen any formal justification for that suggestion, and would be interested to hear other's opinions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-30T21:45:20.560" Id="83920" LastActivityDate="2014-01-30T21:45:20.560" OwnerUserId="38076" ParentId="83898" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I once was a research assistant for a professor who wanted me to do some regressions, but before that he wanted me to test all the sample data for the variables to ensure they were normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Today I was talking with a colleague (Mr. X) who considers himself an accomplished researcher, and he told me that he always tests sample data for normality (and insists that authors do that when he reviews papers).&lt;/p&gt;&#10;&#10;&lt;p&gt;Standing with us was an Econometrics professor who really is good at statistics, and she insisted that only the population needs to be normally distributed, the sample needn't be. I concurred.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mr. X didn't say anything then, but later when he was alone with me, he told me he didn't agree with her. I challenged him on it, and first he gave me a word jumble with &quot;type 1&quot; and &quot;type 2&quot; &quot;errors&quot; thrown in. Further challenged, he told me that if the sample data were not normally distributed, it wouldn't be possible to generalize the results. (Is he correct?)&lt;/p&gt;&#10;&#10;&lt;p&gt;My feeling is that checking sample data for normality is really a proxy for some other test (of cleanliness of data or something). And these people have forgotten or lost the reason why they were taught to test the sample data for normality. &lt;/p&gt;&#10;&#10;&lt;p&gt;What do you think? Why do some people believe that sample data need to be tested for normality?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-01-31T02:16:19.873" Id="83940" LastActivityDate="2014-01-31T02:40:14.980" LastEditDate="2014-01-31T02:40:14.980" LastEditorUserId="29742" OwnerUserId="29742" PostTypeId="1" Score="0" Tags="&lt;sample&gt;&lt;normality&gt;" Title="Why do some researchers test for normality of sample data?" ViewCount="181" />
  
  <row Body="&lt;p&gt;This book by Hardin and Hilbe, &lt;a href=&quot;http://books.google.com/books?id=tOeqO6Hs-6gC&amp;amp;lpg=PP1&amp;amp;hl=el&amp;amp;pg=PA52#v=onepage&amp;amp;q&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;available on Google Books&lt;/a&gt;, provides neat short explanations of the various types of residuals.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-31T10:18:29.610" Id="83965" LastActivityDate="2014-03-28T22:35:25.723" LastEditDate="2014-03-28T22:35:25.723" LastEditorUserId="7290" OwnerUserId="38082" ParentId="1432" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;When I conduct an a priori power analysis to calculate the required sample size with G* power for a repeated measures design with between factors, the sample size increases when I insert a higher value for correlations among repeated measures. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I choose the 'repeated measures, within-between interactions' variant, the opposite is true: the sample size decreases as a result of a higher correlation among repeated measures value.&lt;/p&gt;&#10;&#10;&lt;p&gt;What am I not understandig here? For me, it would be logical that with a higher test-retest reliability, you would need fewer participants than with a lower reliability, in general.&lt;/p&gt;&#10;&#10;&lt;p&gt;We will do a study with one within subjects variable (time: pretest vs. posttest) and one between subjects variable (condition: training vs. control). So, we are interested in the interaction between time and condition, therefore I assume we want to choose the within-between interaction option in G*power, but I am getting slightly confused here. &lt;/p&gt;&#10;&#10;&lt;p&gt;I hope you can help.&lt;/p&gt;&#10;&#10;&lt;p&gt;With kind regards,&#10;Johanna&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-01-31T12:11:46.590" Id="83979" LastActivityDate="2014-01-31T12:11:46.590" OwnerUserId="38283" PostTypeId="1" Score="2" Tags="&lt;repeated-measures&gt;&lt;sample-size&gt;&lt;power-analysis&gt;" Title="Increasing or decreasing sample size with higher correlation among repeated measures in G*Power" ViewCount="70" />
  
  
  <row Body="&lt;h2&gt;Not bayesian estimation&lt;/h2&gt;&#10;&#10;&lt;p&gt;When we suppose that all yours intervals $t_i$ have equal size $\Delta$, we can use the following asymptotically unbiased estimator proposed in &lt;a href=&quot;http://stats.stackexchange.com/questions/83998/if-maria-performs-more-observations-per-unit-of-time-than-maximilien-how-can-he&quot;&gt;another stack.exchange question&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;    \newcommand{\E}{\mathbb{E}}&#10;    \hat\lambda = - \Delta^{-1} \log \frac{W}{N}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;    \begin{aligned}&#10;      \Delta  &amp;amp; \text{ -- size of the time interval between observations}       \\&#10;      W       &amp;amp; \text{ -- number of intervals without changes} \\&#10;      N       &amp;amp; \text{ -- total number of intervals} &#10;    \end{aligned}  &#10;$$&#10;also we calculated its bias:&#10;$$&#10;\E \left[ \hat\lambda - \lambda \right] \approx &#10;\frac{1}{2N\Delta} \left(e^{\lambda\Delta} - 1\right)&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-31T16:57:02.463" Id="84010" LastActivityDate="2014-01-31T16:57:02.463" OwnerUserId="38285" ParentId="51292" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Thanks for the plug =]&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The winnowing process is erroneously removing predictors that can improve the accuracy of the model. Within the cross-validation loop, the winnowing process thinks that it is improving the accuracy, but that is not holding up once other samples are used to evaluate performance. Sometimes it helps and other times is doesn't&lt;/p&gt;&#10;&#10;&lt;p&gt;2) There is no graph of the tree yet (but it is on my list). Try using the &lt;code&gt;summary&lt;/code&gt; function: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; set.seed(1)&#10;&amp;gt; mod &amp;lt;- train(Species ~ ., data = iris, method = &quot;C5.0&quot;)&#10;&amp;gt; ## This data set liked rules over trees but it works the same for trees&#10;&amp;gt; summary(mod$finalModel)&#10;&#10;Call:&#10;&amp;lt;snip&amp;gt;&#10;-----  Trial 0:  -----&#10;&#10;Rules:&#10;&#10;Rule 0/1: (50, lift 2.9)&#10;        Petal.Length &amp;lt;= 1.9&#10;        -&amp;gt;  class setosa  [0.981]&#10;&#10;Rule 0/2: (48/1, lift 2.9)&#10;    Petal.Length &amp;gt; 1.9&#10;    Petal.Length &amp;lt;= 4.9&#10;    Petal.Width &amp;lt;= 1.7&#10;    -&amp;gt;  class versicolor  [0.960]&#10;&amp;lt;snip&amp;gt;&#10;Evaluation on training data (150 cases):&#10;&#10;Trial           Rules     &#10;-----     ----------------&#10;      No           Errors&#10;&#10;   0         4    4( 2.7%)&#10;   1         5    8( 5.3%)&#10;   2         3    6( 4.0%)&#10;   3         6   12( 8.0%)&#10;   4         4    5( 3.3%)&#10;   5         7    3( 2.0%)&#10;   6         3    8( 5.3%)&#10;   7         8   15(10.0%)&#10;   8         4    3( 2.0%)&#10;   9         5    5( 3.3%)&#10;boost             0( 0.0%)   &amp;lt;&amp;lt;&#10;&#10;&#10;   (a)   (b)   (c)    &amp;lt;-classified as&#10;  ----  ----  ----&#10;    50                (a): class setosa&#10;          50          (b): class versicolor&#10;                50    (c): class virginica&#10;&#10;&#10;Attribute usage:&#10;&#10;100.00% Petal.Length&#10; 66.67% Petal.Width&#10; 54.00% Sepal.Width&#10; 46.67% Sepal.Length&#10;&#10;&#10;Time: 0.0 secs&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;HTH,&lt;/p&gt;&#10;&#10;&lt;p&gt;Max&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-01-31T17:50:22.737" Id="84021" LastActivityDate="2014-01-31T17:50:22.737" OwnerUserId="3468" ParentId="83913" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Suppose Y is the amount of money each American spends on a new car in a given year (total purchase price).  Y will spike at 0; will have no values at all between 0 and about 12,000; and will take other values mostly in the teens, twenties and thirties of thousands.  Predictors would be proxies for the level of need and/or interest in making such a purchase.  Need or interest could hardly be said to be zero for individuals who made no purchase; on these scales non-purchasers would be much closer to purchasers than Y or even the log of Y would suggest.  In a case much like this but in health care, I found that the most accurate predictions, judged by test-set/training-set crossvalidation, were obtained by, in increasing order,&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Logistic regression on a binary version of Y,&lt;/li&gt;&#10;&lt;li&gt;OLS on Y,&lt;/li&gt;&#10;&lt;li&gt;Ordinal regression (PLUM) on Y binned into 5 categories (so as to divide purchasers into 4 equal-size groups), &lt;/li&gt;&#10;&lt;li&gt;Multinomial logistic regression on Y binned into 5 categories, &lt;/li&gt;&#10;&lt;li&gt;OLS on the log(10) of Y (I didn't think of trying the cube root), and&lt;/li&gt;&#10;&lt;li&gt;OLS on Y binned into 5 categories.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Some will recoil at this categorization of a continuous dependent variable.  But although it sacrifices some information, categorizing seems to help by restoring an important underlying aspect of the situation -- again, that the &quot;zeroes&quot; are much more similar to the rest than Y would indicate.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-01T02:37:05.990" Id="84055" LastActivityDate="2014-02-01T02:37:05.990" OwnerUserId="2669" ParentId="1444" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;They're related, but not actually so similar in form. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the beta, the variable (and its complement) is raised to some power, but in the binomial the variable &lt;em&gt;is&lt;/em&gt; the power (and it also appears in a binomial coefficient). &lt;/p&gt;&#10;&#10;&lt;p&gt;While the functional forms do look somewhat alike (there are terms in one that correspond to terms in the other), the variables that represent the parameters and the random variable in each are different. That's rather important; it's why they're actually not the same thing at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;The binomial distribution is usually used for &lt;em&gt;counts&lt;/em&gt;, or in scaled form, for count-based proportions (though you could use it for other bounded discrete random variables on a purely pragmatic basis). It's discrete.&lt;/p&gt;&#10;&#10;&lt;p&gt;The beta distribution is continuous, and so is not normally used for counts.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;By way of example, compare these two functions:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = b^x,\, x=0,1,2,3,...$ and $y = x^a,\, 0&amp;lt;x&amp;lt;1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Both these functions are defined by expressions of the same form (something of the form $c^d$), but the roles of variable and constant are interchanged and the domain is different. The relationship between the beta and the binomial is like the relationship between those two functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;- So they have different form, and different &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a simple example of a beta distribution, the $\text{beta}(1,1)$. Which binomial distribution does the same job? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VPKWR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Or consider a $\text{beta}(2,1)$; it's hard to find a binomial which looks similar. Here's one attempt:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Guj9d.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The entire beta pdf sits between the first two green spikes in the binomial pf, though they can't really be shown on the same plot because the y-axes measure different things.&lt;/p&gt;&#10;&#10;&lt;p&gt;While the shapes are vaguely similar in the sense that they're both left skew, they're really quite different, and used for different things.&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a challenge:&lt;/p&gt;&#10;&#10;&lt;p&gt;For $X_1\sim\text{beta}(1,1)$ and a $X_2\sim\text{beta(3,2)}$, find binomial distributions (presumably scaled) that can simultaneously reasonably accurately (say to within $c=(0.95, 1.05)$  times the correct probability, give or take) which have the same mean and variance or mean and range (you pick), but also approximately reproduce the probability of being in these three subintervals: (a) $(1/\pi,1/e)$, (b) $(\exp(-\frac{1}{2}),2/\pi)$, and (c) $(\exp(-3),1/\pi^2)$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The beta is used to do many things, including model continuous proportions, act as a prior on the $p$ parameter of a binomial, it is the distribution of uniform order statistics (and can be used in the derivation of the distribution of order statistics for other continuous distributions, used as a mixing distribution for the binomial $p$ (producing the beta-binomial distribution), to model task completion times in &lt;a href=&quot;http://en.wikipedia.org/wiki/Three-point_estimation&quot; rel=&quot;nofollow&quot;&gt;project management&lt;/a&gt;, and many other things. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-01T13:42:31.090" Id="84083" LastActivityDate="2014-02-02T10:08:58.043" LastEditDate="2014-02-02T10:08:58.043" LastEditorUserId="805" OwnerUserId="805" ParentId="84082" PostTypeId="2" Score="16" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I need a little help with a problem I am working on. So here's the situation, a seller can produce a apple for $1.00$ and I need to find the optimal price of selling the apple and expected profit per buyer if the distribution of the value of the apple is EXP(1).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I just need some confirmation on what I am doing. What I have is this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Expected profit = $(v-1)\cdot (1-F(v))$ &lt;/p&gt;&#10;  &#10;  &lt;p&gt;Optimal price = derivative of expected profit w.r.t. to $v$ set equal to zero then solve for $v$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I hope this makes sense guys. Like I said I just want make sure I am doing this right or else I am going to have to read the chapter for the 3rd time.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-01T21:43:15.283" FavoriteCount="0" Id="84113" LastActivityDate="2014-02-02T02:19:32.840" LastEditDate="2014-02-02T02:19:32.840" LastEditorUserId="28746" OwnerUserId="38364" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;econometrics&gt;&lt;exponential&gt;" Title="Quick Problem with maximizing profit using different distributions" ViewCount="62" />
  <row AnswerCount="2" Body="&lt;p&gt;Matrix equations of some sort or matrix decompositions often lie in foundation of many algorithms not only in data analysis but in many other fields. The wealth of already written libraries makes it tempting to formulate frequent itemsets mining problems in terms of said matrix equations and/or decompositions, yet after some search I haven't found any algorithms which would rely on linear algebra. Could some please show me if such algorithms exist or explain their absence?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-02T02:08:06.863" Id="84127" LastActivityDate="2015-02-10T08:09:27.903" LastEditDate="2014-02-02T13:50:27.587" LastEditorUserId="21104" OwnerUserId="21104" PostTypeId="1" Score="4" Tags="&lt;data-mining&gt;&lt;linear-algebra&gt;" Title="Are there frequent itemsets mining algorithms based on linear algebra?" ViewCount="88" />
  
  
  
  <row Body="&lt;p&gt;It is two names for the same thing. Both are methods that retrieve pattern highly frequent in one class while not frequent in other classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;For more information on such patterns, you can have a look at &lt;a href=&quot;http://arxiv.org/abs/1102.4104&quot; rel=&quot;nofollow&quot;&gt;Characterizing Discriminative Patterns&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are not looking for speed, you can use a standard sequential pattern mining algorithm and just compute a measure comparing the frequency of the computed sequential patterns in each class. The patterns having the highest value are then the emerging (or discriminative) patterns. This method allows to test several approaches and is very easy to implement.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are interested in performance, then the idea is to avoid exploring some part of the search space where you know that you will not be able to find any emerging pattern above a given threshold. Such algorithm may be much more efficient, but are highly dependent on your measure to discriminate pattern.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-02T13:25:57.940" Id="84160" LastActivityDate="2014-02-02T13:25:57.940" OwnerUserId="38384" ParentId="43624" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I gather you have only 2 repeated measures.  That makes this simpler.  If you had &gt;2, you would need to use a mixed effects model, which is more advanced.  Given that you have only 2, there are two basic possibilities: use differences as your response variable, or use an ANCOVA.  Which you should use has traditionally been a matter of great contention in statistics.  A basic rule is that ANCOVA makes more sense if you believe your groups were the same at baseline (i.e., this is an experiment), and using differences as your response variable makes more sense if you don't have reason to believe the groups were the same (i.e., this is an observational study).  For more on this topic, read: &lt;a href=&quot;http://stats.stackexchange.com/questions/3466/best-practice-when-analysing-pre-post-treatment-control-designs&quot;&gt;Best practice when analysing pre-post treatment-control designs&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you choose to use differences as your response variable, the approach is quite simple.  You just subtract the baseline value for each subject from the subject's followup value.  Then use those differences as Y in a multiple regression model.  In R it might be something like:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;difs = followup-baseline&#10;lm(difs~covariates)  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you choose to use ANCOVA, then you use the followup value as the response variable and include the baseline value as on of your covariates.  In R it might be something like:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(followup~covariates)  # (&quot;covariates&quot; includes &quot;baseline&quot; &amp;amp; the original covariates)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2014-02-02T17:02:12.690" Id="84181" LastActivityDate="2014-02-02T17:02:12.690" OwnerUserId="7290" ParentId="84164" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I have been trying to implement mutual information in a java program that tries to identify terms that occur often together in documents. Basically, the program does the following: There is a text collection of just over 4.000.000 summaries from Wikipedia. This is searched through based on a query, resulting in a collection of say 100 texts that are deemed as a match to the query by Lucene index. I then create bigrams (structures of two words) so that all words are paired with all the other words in the result collection. Each bigram is then calculated a mutual information value for. The idea is that the highest scoring bigrams should be (hopefully) directly relevant to the original query.&lt;/p&gt;&#10;&#10;&lt;p&gt;On to how I understand the Mutual Information equation, which for the record looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;P(x,y) * log(P(x,y) / (P(x) * P(y)),&lt;/p&gt;&#10;&#10;&lt;p&gt;where P(x,y) is the probability of term x and y occurring in a document, P(x) is the probability of just term x occurs in a document, and likewise for P(y) for term y.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am unsure if I understand correctly what this means in terms of calculating the probability values. It is calculated by finding the frequency of a term divided by the number of articles in the result set. But is it the term frequency or the document frequency for a given term?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ie. is P(x) in a result set of 100 articles the number of times term x occurs in the collection divided by 100? Or is it the number of articles in which term x occurs at least once divided by 100? And if it is the former, how then is P(x,y) calculated?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-02T20:01:40.867" Id="84190" LastActivityDate="2014-09-20T03:23:15.670" OwnerUserId="38405" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;mutual-information&gt;" Title="Details on mutual information equation" ViewCount="107" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;1) Regarding the question from letter (a), my understanding is that for either B1 or B2, they can be estimated by averaging the weights (for example, for B1 it would be adding the weights from y1, y3 and y4). Is this assumption valid?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Rather than guess, you should work it out, exactly as suggested in the question. Then you'll know for sure.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;2) Can I also assume that y3 is found by simply adding y1 and y2? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No, because they have different errors; $e_3 \neq e_1+e_2$&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I didn't think I could, because the readings would probably be different and thus have different residuals.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Correct, except its that the errors differ, not the residuals, that's the main issue.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;3) Does the 'y' in the formula correspond with the estimated column vector containing predicted B1 and B2?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No. It is the vector of observed scale readings for the 4 set-ups&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-02T23:51:26.413" Id="84215" LastActivityDate="2014-02-03T01:15:51.103" LastEditDate="2014-02-03T01:15:51.103" LastEditorUserId="805" OwnerUserId="805" ParentId="84214" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Of course it depends on the underlying data which you should always explore to find out some of its characteristics before trying to fit a model but what I've learnt as general rules of thumb are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A flexible model allows you to take full advantage of a large sample size (large n).&lt;/li&gt;&#10;&lt;li&gt;A flexible model will be necessary to find the nonlinear effect.&lt;/li&gt;&#10;&lt;li&gt;A flexible model will cause you to fit too much of the noise in the problem (when variance of the error terms is high).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-02-03T00:06:44.953" Id="84217" LastActivityDate="2014-02-03T00:06:44.953" OwnerUserId="37382" ParentId="69237" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset with approx. 13,000 cases in total, and approx. 1,000 cases have the outcome I am interested in predicting (sentenced to imprisonment). I have run a logstic regression on the whole sample and the model is terrible at predicting 'imprisonment' although quite good at predicting 'no imprisonment'. I tried drawing a random sample of 'non-imprisonment' cases and joining them with the 'imprisonment' cases to achieve a more balanced sample. When I have a matched number of cases (i.e. 50% imprisonment 50% no imprisonment) I achieve quite good prediction of both outcomes. However, when I double the number of 'non-imprisonment' cases my prediction overall increases but the ability to prdict imprisonment in particular decreases. It is valid to use a sample to best predict imprisonment or am I somehow influencing the results? Does the fact that my initial model was poor at predicting imprisonment just mean I do not have a good model? (diagnostics generally indicate it is not great but OK).&#10;Thanks &lt;/p&gt;&#10;" ClosedDate="2014-02-03T10:22:28.220" CommentCount="4" CreationDate="2014-02-03T00:08:58.803" Id="84218" LastActivityDate="2014-02-03T00:08:58.803" OwnerUserId="38417" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;sampling&gt;&lt;prediction&gt;" Title="Logistic regression and ratio cases to non-cases in sample" ViewCount="27" />
  <row Body="&lt;p&gt;Let me answer in reverse order:&lt;/p&gt;&#10;&#10;&lt;p&gt;2. Yes. If their MGFs exist, they'll be the same*.&lt;/p&gt;&#10;&#10;&lt;p&gt;see &lt;a href=&quot;http://stats.stackexchange.com/a/34958/805&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/32706/existence-of-the-moment-generating-function-and-variance/32787#32787&quot;&gt;here&lt;/a&gt; for example&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed it follows from the result you give in the post this comes from; if the MGF uniquely** determines the distribution, and two distributions have MGFs and they have the same distribution, they must have the same MGF (otherwise you'd have a counterexample to 'MGFs uniquely determine distributions'). &lt;/p&gt;&#10;&#10;&lt;p&gt;* for certain values of 'same', due to that phrase 'almost everywhere'&lt;/p&gt;&#10;&#10;&lt;p&gt;** '&lt;em&gt;almost everywhere&lt;/em&gt;'&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;No - since counterexamples exist.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Kendall and Stuart list a continuous distribution family (possibly originally due to Stieltjes or someone of that vintage, but my recollection is unclear, it's been a few decades) that have identical moment sequences and yet are different. &lt;/p&gt;&#10;&#10;&lt;p&gt;The book by Romano and Siegel (Counterexamples in Probability and Statistics) lists counterexamples in section 3.14 and 3.15 (pages 48-49). (Actually, looking at them, I think both of those were in Kendall and Stuart.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Romano, J. P. and Siegel, A. F. (1986),&lt;br&gt;&#10;&lt;em&gt;Counterexamples in Probability and Statistics.&lt;/em&gt;&lt;br&gt;&#10;Boca Raton: Chapman and Hall/CRC.&lt;/p&gt;&#10;&#10;&lt;p&gt;For 3.15 they credit Feller, 1971, p227&lt;/p&gt;&#10;&#10;&lt;p&gt;That second example involves the family of densities &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x;\alpha) = \frac{1}{24}\exp(-x^{1/4})[1-\alpha \sin(x^{1/4})], \quad x&amp;gt;0;\,0&amp;lt;\alpha&amp;lt;1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The densities differ as $\alpha$ changes, but the moment sequences are the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;That the moment sequences are the same involves splitting $f$ into the parts&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{1}{24}\exp(-x^{1/4}) -\alpha \frac{1}{24}\exp(-x^{1/4})\sin(x^{1/4})$&lt;/p&gt;&#10;&#10;&lt;p&gt;and then showing that the second part contributes 0 to each moment, so they are all the same as the moments of the first part.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's what two of the densities look like. The blue is the case at the left limit ($\alpha=0$), the green is the case with $\alpha=0.5$. The right-side graph is the same&#10;but with log-log scales on the axes.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iN8mH.png&quot; alt=&quot;example of same moments, different densities&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Better still, perhaps, to have taken a much bigger range and used a fourth-root scale on the x-axis, making the blue curve straight, and the green one move like a sin curve above and below it, something like so:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kJQCH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The wiggles above and below the blue curve - whether of larger or smaller magnitude - turn out to leave all positive integer moments unaltered. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Note that this &lt;em&gt;also&lt;/em&gt; means we can get a distribution all of whose odd moments are zero, but which is asymmetric, by choosing $X_1,X_2$ with different $\alpha$ and taking a 50-50 mix of $X_1$, and $-X_2$. The result must have all odd moments cancel, but the two halves aren't the same.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-03T02:30:40.823" Id="84220" LastActivityDate="2014-02-03T06:04:43.353" LastEditDate="2014-02-03T06:04:43.353" LastEditorUserId="805" OwnerUserId="805" ParentId="84219" PostTypeId="2" Score="8" />
  
  <row AcceptedAnswerId="84299" AnswerCount="1" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5TGJu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a graph like this what would be a smart way to go about making the error bars distinguishable without using colour coding?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-03T07:37:40.003" FavoriteCount="1" Id="84237" LastActivityDate="2014-02-03T16:01:32.707" OwnerUserId="28668" PostTypeId="1" Score="6" Tags="&lt;data-visualization&gt;" Title="How to deal with graphing overlapping error bars without color?" ViewCount="311" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Ive been trying to find a way to average negative binomial GLMM using MuMin R Package but it seems not to work for negative binomial GLMM, any alternatives? thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-03T11:15:02.603" FavoriteCount="1" Id="84260" LastActivityDate="2015-02-07T22:26:43.237" OwnerUserId="36491" PostTypeId="1" Score="0" Tags="&lt;negative-binomial&gt;" Title="Model Averging for Negative Binomial GLMM in R" ViewCount="93" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Are there many features in Eviews that R misses? I have heard that especially when dealing with time series R is less extensive than Eviews. Is this true? Which of the two packages contains most statistical tools (or are they comparable?)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-03T14:31:14.520" Id="84284" LastActivityDate="2014-07-10T00:52:35.030" LastEditDate="2014-07-10T00:52:35.030" LastEditorUserId="805" OwnerUserId="38447" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;software&gt;&lt;eviews&gt;" Title="Eviews vs R (Statistical Software)" ViewCount="479" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to find a way to compare variance within and between site with differing numbers of observations.  I have seen other posts on this, but they seem to focus on factor levels and I'm not sure that is appropriate with the number of sites (90) I have.  Can someone suggest the best way to compare within and between site variances in R? Or point me to a relevent post? &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example dataset for 10 sites with 7:10 observations at each site&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;clust&amp;lt;-rep(1:10, sample(c(7,10), 10, replace=T))  &#10;variable&amp;lt;-abs(rnorm(length(clust)))&#10;&#10;data&amp;lt;-data.frame(cbind(clust, variable))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I could (and have) used glm and anova to look at within and between cluster variance, but not sure that is appropriate here.  I have also used var() to look at variance for the whole sample and of the clust mean.  However, I'm unsure this is what I want to be doing, as there are differing numbers of observations per site/cluster.  Should I use a weighted mean/variance function?  are there other ways of going about describing the variance in this type of situation, especially where the number of sites is larger (90)?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-01-21T04:24:31.997" Id="84310" LastActivityDate="2014-02-03T19:39:04.057" OwnerDisplayName="user2296772" OwnerUserId="37635" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;variance&gt;" Title="within and between cluster variance" ViewCount="209" />
  <row Body="&lt;p&gt;One typical case for the application of density estimation is novelty detection, a.k.a. outlier detection, where the idea is that you only (or mostly) have data of one type, but you are interested in very rare, qualitative distinct data, that deviates significantly from those common cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Examples are fraud detection, detection of failures in systems, and so on. These are situations where it is very hard and/or expensive to gather data of the sort you are interested in. These rare cases, i.e. cases with low probability of occurring.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the times you are not interested on estimating accurately the exact distribution, but on the relative odds (how likely is a given sample to be an actual outlier vs. not being one).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are dozens of tutorials and reviews on the topic. This &lt;a href=&quot;http://www.siam.org/meetings/sdm10/tutorial3.pdf&quot;&gt;one&lt;/a&gt; might be a good one to start with.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: for some people seems odd using density estimation for outlier detection.&#10;Let us first agree on one thing: when somebody fits a &lt;a href=&quot;http://en.wikipedia.org/wiki/Mixture_density&quot;&gt;mixture model&lt;/a&gt; to his data, he is actually performing density estimation. A mixture model represents a distribution of probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;kNN and GMM are actually related: they are two methods of estimating such a density of probability. This is the underlying idea for many approaches in novelty detection. For example, &lt;a href=&quot;http://www.stat.washington.edu/research/reports/1996/tr305.pdf&quot;&gt;this one&lt;/a&gt; based on kNNs, this &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.4437&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;other one&lt;/a&gt; based on Parzen windows (which stress this very idea at the beginning of the paper), and many &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/cmbishop/downloads/bishop%20-%20novelty%20detection_iee%20proceedings%2094b.pdf&quot;&gt;others&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me (but it is just my personal perception) that most if not all work on this idea. How else would you express the idea of an anomalous/rare event?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-02-03T20:01:07.800" Id="84311" LastActivityDate="2014-02-06T08:29:04.920" LastEditDate="2014-02-06T08:29:04.920" LastEditorUserId="17908" OwnerUserId="17908" ParentId="82583" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to perform a CFA on data (10 indicators: $n=300$) that is severely non-normal but continuous (counts of a clinical behavior over a period of weeks): many cases are at zero, a fair few between 1 and 30, and a handful at values sometimes much higher (even $&amp;gt; 1000$). I am using lavaan in R with the MLM estimator to accommodate for this distribution. I am primarily comparing a single common factor model with a model containing three correlated factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I use all cases, I get warnings indicating a non-positive definite matrix, and negative error variance (i.e. Heywood case). Because of the severity of the outliers, in line with Bollen's (1987) recommendations, I explored whether the extreme values were the cause of the negative error variance. I removed all cases 3 standard deviations above the mean ($n=16$) and this led to admissible solutions when I re-ran my models, and sensible parameter estimates. Am I interpreting these errors appropriately, and does this sound like a reasonable procedure to have taken in these circumstances?&lt;/p&gt;&#10;&#10;&lt;p&gt;On a related note, is there a limit to how high the Satorra-Bentler scaling correction factor can be before one should be concerned? I see values of around 4 in some cases (unsurprisingly given the non-normality of the data), although I cannot locate any literature that comments on this issue. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-03T21:32:39.013" Id="84327" LastActivityDate="2014-02-04T01:06:07.077" LastEditDate="2014-02-04T01:06:07.077" LastEditorUserId="7290" OwnerUserId="14005" PostTypeId="1" Score="2" Tags="&lt;outliers&gt;&lt;confirmatory-factor&gt;&lt;error-message&gt;" Title="Outliers causing Heywood case in CFA using MLM estimator in lavaan" ViewCount="264" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;The only difference I can think of is&lt;/p&gt;&#10;&#10;&lt;p&gt;Normal dist is defined on all values (negative, zero, positive), and is symmetric&lt;/p&gt;&#10;&#10;&lt;p&gt;Gamma dist is defined on positive values only, and is skewed to the right.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarity wise, they both are continuous distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anything else am I missing?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" ClosedDate="2014-02-03T23:15:33.510" CommentCount="1" CreationDate="2014-01-30T15:58:13.273" Id="84336" LastActivityDate="2014-02-03T23:03:08.153" OwnerDisplayName="user3253931" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="difference &amp; similarity between normal distribution and gamma distribution?" ViewCount="34" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have carried out this ANOVA:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(aov(drat ~ cyl, mtcars))&#10;&#10;            Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)    &#10;cyl          1  4.342   4.342   28.81 8.24e-06 ***&#10;Residuals   30  4.521   0.151                     &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;According to the output, the sum of squares between groups is 4.342. I am trying to calculate the sum of squares using R code&lt;/p&gt;&#10;&#10;&lt;p&gt;First I have taken substracted the means of each group from the values and squared the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cyl_4_minus_mean &amp;lt;- (mtcars$drat[mtcars$cyl == 4] - mean(mtcars$drat[mtcars$cyl ==4]))^2&#10;cyl_6_minus_mean &amp;lt;- (mtcars$drat[mtcars$cyl == 6] - mean(mtcars$drat[mtcars$cyl ==6]))^2&#10;cyl_8_minus_mean &amp;lt;- (mtcars$drat[mtcars$cyl == 8] - mean(mtcars$drat[mtcars$cyl ==8]))^2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I then tried to multiply the results by the numbers of values within each group and the sum the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sum((11*cyl_4_minus_mean), (7*cyl_6_minus_mean), (14*cyl_8_minus_mean))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives 49.4459, which is different to 4.342 given by &lt;code&gt;summary(aov(drat ~ cyl, mtcars))&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Where am I going wrong here?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-04T09:55:33.130" FavoriteCount="1" Id="85375" LastActivityDate="2014-03-06T21:04:23.023" OwnerUserId="12492" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;&lt;sums-of-squares&gt;" Title="Calculating sum of squares between groups" ViewCount="1513" />
  
  <row Body="&lt;p&gt;If your data are taken from a setting where it is useful or realistic to consider that your measurements are &quot;noisy&quot;, it may be useful to pick a prior noise model for your data which means that both your distributions have support over the same space.&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple approach would be to consider that your distributions are Gaussian mixtures with means at the observed data points and a single common variance.  This &quot;smoothing&quot; gives your distribution support on the whole real line, for 1-dimensional data.  It then remains to calculate Kullback-Leibler divergences between the two Gaussian mixtures: there is no closed form solution for this, but you can compute it numerically using the approaches in Hershey and Olsen (&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.148.2502&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot; title=&quot;Hershey and Olsen&quot;&gt;1&lt;/a&gt;), e.g. by Monte Carlo sampling.  For higher-dimensional data you could use multivariate Gaussian mixtures (with full rank covariance matrices, for the reason given by Matus Telgarsky in the MathOverflow discussion &lt;a href=&quot;http://mathoverflow.net/questions/72668/how-to-compute-kl-divergence-when-pmf-contains-0s&quot; title=&quot;by Matus Telgarsky&quot;&gt;2&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course this has not removed all arbitrariness - we still have the common prior variance (and the choice of noise model).  But it should be easy to study numerically the behaviour of the Jensen-Shannon divergence as you vary the single parameter of the noise model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-04T14:18:16.867" Id="85397" LastActivityDate="2014-02-04T14:18:16.867" OwnerUserId="39510" ParentId="83885" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Examples of the filenames would be helpful to answer the question.&#10;If the filenames are such that you can know how many words are in the name, then you can only search for the movie names with the same number of word.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-04T14:40:01.193" Id="85401" LastActivityDate="2014-02-04T14:40:01.193" OwnerUserId="27088" ParentId="84178" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;In signals, variance is essentially a measure of energy.  Of course in this case they are related directly, not inversely.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-04T21:06:54.940" Id="85439" LastActivityDate="2014-02-04T21:06:54.940" OwnerUserId="36115" ParentId="85436" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="85455" AnswerCount="1" Body="&lt;p&gt;I am looking to compare regression coefficients between two regression models. Each model has the same four independent variables: two predictors of interest (we'll call them A and B) and two control variables (C and D). The only difference between the two models is that they have different dependent variables: the first model is predicting DV1, while the second model is predicting DV2. All observations are from the same sample, so the regression coefficients are dependent. &lt;/p&gt;&#10;&#10;&lt;p&gt;I believe that both A and B will more strongly predict DV1 than DV2. In other words, the regression coefficient for A predicting DV1 (controlling for B, C, and D) should be higher in magnitude than the regression coefficient for A predicting DV2 (controlling for B, C, and D). Similarly, the regression coefficient for B predicting DV1 (controlling for A, C, and D) should be higher in magnitude than the regression coefficient for B predicting DV2 (controlling for A, C, and D). &lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially, I want to test the difference between two dependent regression coefficients from two models that share all of the same IVs, but have different DVs. Is there a formal significance test I can use?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-04T21:30:24.417" FavoriteCount="2" Id="85445" LastActivityDate="2014-11-28T11:22:05.290" OwnerUserId="39538" PostTypeId="1" Score="5" Tags="&lt;regression&gt;" Title="Comparing dependent regression coefficients from models with different dependent variables" ViewCount="1199" />
  <row AcceptedAnswerId="85466" AnswerCount="3" Body="&lt;p&gt;For most of my time in stats, I have been able to ignore the marginal distribution that is usually present at the denominator of any bayesian posterior distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if we write down $L_x(\theta)\pi(\theta)$ and recognize that this function of $\theta$ looks like a distribution of $\theta$ but with an incorrect normalizing constant, I usually just mix and match till I get it since &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\pi(x|\theta) \propto L_x(\theta)\pi(\theta)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;HOWEVER, &lt;/p&gt;&#10;&#10;&lt;p&gt;why can I do this, are there any cases where this breaks down? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-05T04:25:58.873" Id="85465" LastActivityDate="2014-02-05T11:08:49.443" OwnerUserId="38348" PostTypeId="1" Score="3" Tags="&lt;mathematical-statistics&gt;" Title="Theoretically, why do we not need to compute a marginal distribution constant for finding a Bayesian posterior?" ViewCount="124" />
  <row AnswerCount="2" Body="&lt;p&gt;I measured prey capturing success (=proportion of successful captures divided by the total number of prey captured) in four fish species (species 1: n=19, species 2: n=18, species 3: n=4, species 4: n=3) and would like to compare the prey capturing success between the four species. However, as there might be an individual effect within species (some fish might be better due to slight differences in morphology) I do not think one can simple sum everything up and compare four values (proportions). Therefore I want to know if it is possible to compare multiple proportions between the four species. Note that the proportions do not show a normal distribution within the species.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks in advance!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-05T09:19:14.943" Id="85476" LastActivityDate="2014-02-05T12:02:21.340" OwnerUserId="39553" PostTypeId="1" Score="1" Tags="&lt;nonparametric&gt;&lt;multiple-comparisons&gt;&lt;proportion&gt;" Title="How to compare multiple proportions between multiple groups" ViewCount="314" />
  <row Body="&lt;p&gt;I will try to answer my own question.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Message&lt;/h2&gt;&#10;&#10;&lt;p&gt;A very important notion of factor graph is &lt;strong&gt;message&lt;/strong&gt;, which can be understood as A tells something about B, if the message is passed from A to B.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the probabilistic model context, message from factor $f$ to variable $x$ can be denoted as $\mu_{f \to x}$, which can be understood as $f$ knows something(probability distribution in this case) and tells it to $x$.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Factor summarizes messages&lt;/h2&gt;&#10;&#10;&lt;p&gt;In the &quot;factor&quot; context, to know the probability distribution of some variable, one needs to have all the messages ready from its neighboring factors and then summarize all the messages to derive the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in the following graph, the edges, $x_i$, are variables and nodes, $f_i$, are factors connected by edges. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hpfjr.png&quot; alt=&quot;Example factor graph&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To know $P(x_4)$, we need to know the $\mu_{f_3 \to x_4}$ and $\mu_{f_4 \to x_4}$ and summarize them together.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Recursive structure of messages&lt;/h2&gt;&#10;&#10;&lt;p&gt;Then how to know these two messages? For example, $\mu_{f_4 \to x_4}$. It can be seen as the message after summarizing two messages, $\mu_{x_5 \to f_4}$ and $\mu_{x_6 \to f_4}$. And $\mu_{x_6 \to f_4}$ is essentially $\mu_{f_6 \to x_6}$, which can be calculated from some other messages.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the recursive structure of messages, &lt;em&gt;messages can be defined by messages&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Recursion is a good thing, one for better understanding, one for easier implementation of computer program.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Conclusion&lt;/h2&gt;&#10;&#10;&lt;p&gt;The benefit of factors are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Factor, which summarizes inflow messages and output the outflow message, enables messages which is essential for computing marginal&lt;/li&gt;&#10;&lt;li&gt;Factors enable the recursive structure of calculating messages, making the message passing or &lt;em&gt;belief propagation&lt;/em&gt; process easier to understand, and possibly easier to implement.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-02-05T11:10:57.480" Id="85484" LastActivityDate="2014-02-05T11:10:57.480" OwnerUserId="7259" ParentId="85453" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;As I understand it, you have multiple individual fish within each species.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each individual caught a certain proportion of prey.&lt;/p&gt;&#10;&#10;&lt;p&gt;You want to model the success rate as a function of species, accounting for the fact that there were multiple individuals in each species, and each individual varied in ability.&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, there are, I think, three main approaches:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) A nonlinear multilevel model, with species at one level and individual fish at another.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) GEE (generalized estimating equations). &lt;/p&gt;&#10;&#10;&lt;p&gt;3) Accounting for the clustering with a sandwich estimator.&lt;/p&gt;&#10;&#10;&lt;p&gt;My own experience is mostly with 1) which replace the usual logistic regression equation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{logit}(Y) = BX$&lt;/p&gt;&#10;&#10;&lt;p&gt;with &lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{logit}(Y) = BX + u_{ij}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $u_{ij}$ is the effect at level 2&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference between (1) and (2) is that (1) is a conditional model and (2) is a marginal model. (3) is an attempt to deal with the clustering by making the variance estimation robust.&lt;/p&gt;&#10;&#10;&lt;p&gt;More resources:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.unc.edu/~gguo/papers/00%20Multilevel%20Models%20binary.pdf&quot; rel=&quot;nofollow&quot;&gt;Multilevel modeling for binary data&lt;/a&gt; on (1)&#10;&lt;a href=&quot;http://projecteuclid.org/euclid.aos/1291388380&quot; rel=&quot;nofollow&quot;&gt;GEE analysis of clustered binary data&lt;/a&gt; on (2) (looks good from the abstract, I haven't read it)&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/24897/difference-between-marginal-and-conditional-models&quot;&gt;This thread here on SE about the difference&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I did see a paper on (3) at a Northeast SAS Users Group; Google finds it, but the link seems to have deteriorated (or it might be my internet connection that has).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-05T12:02:21.340" Id="85489" LastActivityDate="2014-02-05T12:02:21.340" OwnerUserId="686" ParentId="85476" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Basically, if $X$ and $Y$ are independent, then also $f(X)$ and $g(Y)$ are independent if $f$ and $g$ are measurable functions:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;P(f(X) \in A,\ g(Y) \in B) &amp;amp;= P\left(X \in f^{-1}(A),\ Y \in g^{-1}(B)\right) \\&#10;&amp;amp; =  P\left(X \in f^{-1}(A)\right) \ P\left(Y \in g^{-1}(B)\right) \\&#10;&amp;amp; = P\left(f(X) \in A\right) \ P\left(g(Y) \in B\right).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular all continuous functions (like the $f(x)=1/x$ and $f(x)=x^2$ in your examples) are Borel-measurable, and hence also $X$ and $1/Y$ as well as $X^2$ and $Y^2$ are independent.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-05T12:33:43.580" Id="85492" LastActivityDate="2014-02-07T17:46:13.563" LastEditDate="2014-02-07T17:46:13.563" LastEditorUserId="38281" OwnerUserId="38281" ParentId="85487" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;As the answer from DaemonMaker points out, you need a model for the state, and a state with jumps such as your blue line is inconvenient. On the other hand, an inflation rate with instantaneous jumps and constant everywhere else doesn't make much sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think a better model would be to treat both the red and blue line as observations of an underlying, not directly observable state, which you could endow with a simple dynamic: a random walk, a local linear trend, autoregressive, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if the CPI index is released only once per month and the red line is observed, for instance, once per week, there is no problem at all: the weeks in which you have no new observation of the CPI you would fill in a NA.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-05T12:42:21.317" Id="85495" LastActivityDate="2014-02-05T12:42:21.317" OwnerUserId="892" ParentId="85373" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;Yes, covariance is &lt;a href=&quot;http://mathworld.wolfram.com/LinearOperator.html&quot; rel=&quot;nofollow&quot;&gt;linear&lt;/a&gt; in either of its arguments, which is to say:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Cov}(aX,Y) = \text{Cov}(X,aY) = a\text{Cov}(X,Y)$ (in the univariate case; an analogous formula holds for the multivariate case) and &lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Cov}(A+B,C) = \text{Cov}(A,C) + \text{Cov}(B,C)\,$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This linearity follows from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Covariance#Definition&quot; rel=&quot;nofollow&quot;&gt;definition of covariance&lt;/a&gt; and the basic properties of expectation - in particular,  &lt;a href=&quot;http://en.wikipedia.org/wiki/Expected_value#Linearity&quot; rel=&quot;nofollow&quot;&gt;linearity of expectation&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let  $\mu_X = E(X)$ and similarly for the other variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{eqnarray}&#10;\text{Cov}(X,Y+Z) &amp;amp;=&amp;amp; \text{E}[(X-\text{E}(X))(Y+Z-\text{E}(Y+Z)]\\&#10; &amp;amp;=&amp;amp; \text{E}[(X-\mu_X)(Y+Z-\{\mu_Y+\mu_Z\})]\\&#10; &amp;amp;=&amp;amp; \text{E}[(X-\mu_X)(Y-\mu_Y+Z-\mu_Z)]\\&#10; &amp;amp;=&amp;amp; \text{E}[(X-\mu_X)(Y-\mu_Y)]+\text{E}[(X-\mu_X)(Z-\mu_Z)]\\&#10; &amp;amp;=&amp;amp; \text{Cov}(X,Y)+\text{Cov}(X,Z)&#10;\end{eqnarray}&lt;/p&gt;&#10;&#10;&lt;p&gt;(which is pretty similar what you have in your own hint in comments there)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-05T22:50:21.837" Id="85552" LastActivityDate="2014-02-05T23:11:40.427" LastEditDate="2014-02-05T23:11:40.427" LastEditorUserId="805" OwnerUserId="805" ParentId="85551" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;In my opinon you should PCR &quot;confirm&quot; a subset you think is likely interesting (based on theory/guessing) either way. There is a history of problems with microarrays giving &quot;false positives&quot;. It is not just batch effects, but also position on the chips, baseline differences, etc. Also keep in mind that different expression profiles can result in the same phenotype, so only looking at averages rather than also looking for within-group clusters of similar profiles may be suboptimal. There are a number of reasons that the observed differential expression may not be clinically important. You may have reviewers who think the same. If you do the PCR verification then your conclusions are stronger regardless, just remember if you &quot;verify&quot; enough different genes some are bound to come up as significant. Since the study sounds exploratory (you have no theory predicting certain differences beforehand), the results should be presented as such.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, if both arrays are from the same two sets of subjects then it may be interesting to see which genes were not substantially different in one but were so in the other. Is there reason to expect the expression profiles to be stationary/stable over time? Out of curiosity, did you see more apparent group differences in the confounded experiment?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit in response to comments:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Interesting that nearly half the &quot;group differences&quot; can be accounted for by confounding and technical factors. I have no idea if this is the normal amount of variability seen for technical replicates of microarray data (never ran such an experiment myself). If it is much larger than usual it really calls the first study into question and really makes me wonder why there is such a large batch effect. On the other hand, if that is the normal amount of variability it makes me wonder what else is going on that affects the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also if you are using significance testing to determine which genes are differentially expressed, keep in mind this warning by Gelman and Stern:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Consider two independent studies with effect estimates and standard&#10;  errors of 25 ± 10 and 10 ± 10. The first study is statistically&#10;  significant at the 1% level, and the second is not at all&#10;  statistically significant, being only one standard error away from 0.&#10;  Thus, itwould be tempting to conclude that there is a large difference&#10;  between the two studies. In fact, however, the difference is not even&#10;  close to being statistically significant: the estimated difference is&#10;  15, with a standard error of sqrt(10^2 + 10^2) = 14.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Andrew Gelman and Hal Stern. &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf&quot; rel=&quot;nofollow&quot;&gt;The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant&lt;/a&gt;. The American Statistician, November 2006, Vol. 60, No. 4&lt;/p&gt;&#10;&#10;&lt;p&gt;You got me looking up how variable microarray results are supposed to be and I found a few papers that look of interest. Zakharkin et al reported relatively consistent results for technical replicates. Also it may be helpful to create scatter plots comparing the actual expression levels for each replicate as their figure 4:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;We found that technical replicates within a biological replicate had&#10;  higher and more consistent correlations with each other than with&#10;  other biological replicates. Generally, our correlations were higher&#10;  than those observed by Dobbin et al., 2005, for interlaboratory&#10;  correlations between tumor samples [25] and were compatible with&#10;  values for in-lab correlations obtained in another study [29].&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Zakharkin et al. &lt;a href=&quot;http://www.biomedcentral.com/content/pdf/1471-2105-6-214.pdf&quot; rel=&quot;nofollow&quot;&gt;Sources of variation in Affymetrix microarray experiments&lt;/a&gt;. BMC Bioinformatics 2005, 6:214&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is another paper that reports batch effects consistent with what you have reported:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the SMRI brain expression microarray data set, batch effects&#10;  accounted for nearly 50% of the observed variation in expression, to&#10;  which site effects contributed 42% and date effects 7.3%.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Chen C, Grennan K, Badner J, Zhang D, Gershon E, et al. (2011) &lt;a href=&quot;http://www.plosone.org/article/info%3adoi/10.1371/journal.pone.0017238&quot; rel=&quot;nofollow&quot;&gt;Removing Batch Effects in Analysis of Expression Microarray Data: An Evaluation of Six&#10;Batch Adjustment Methods.&lt;/a&gt; PLoS ONE 6(2): e17238. doi:10.1371/journal.pone.0017238&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-06T01:20:35.900" Id="85567" LastActivityDate="2014-02-06T21:24:20.863" LastEditDate="2014-02-06T21:24:20.863" LastEditorUserId="38102" OwnerUserId="38102" ParentId="85545" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The only information in the tail of the expansion of f is the coefficients $a_n$ for $n \gg 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The relationship is &#10;$$\frac{1}{r} = \limsup_{n \rightarrow \infty} \sqrt[n]{a_n}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For many standard examples the limit exists so that $$\limsup_{n \rightarrow \infty} \sqrt[n]{a_n} = \lim_{n \rightarrow \infty} \sqrt[n]{a_n} = 1/r$$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In light of whuber's comment on the OP, are you asking about the tails of $f$ as a Taylor series or the tails of the distribution $F(t) = \int_{-\infty}^t f dx$?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-06T03:25:14.103" Id="85581" LastActivityDate="2014-02-06T17:55:39.057" LastEditDate="2014-02-06T17:55:39.057" LastEditorUserId="38480" OwnerUserId="38480" ParentId="85558" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Given the plethora of things in the package, I think this is too broad to answer, in the sense of 'could take a book to answer'. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, let me give you some broad principles:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Where it involves finding the best (by some criterion of fit say) subset of variables, it will generally suffer from &lt;em&gt;pretty much exactly the same set of problems as stepwise or all subsets regression&lt;/em&gt;. [Leaps and Bounds comes in here, for example, &lt;em&gt;even if it's based on AIC or BIC&lt;/em&gt; - some of the issues may be mitigated somewhat by using such a criterion, but the main underlying problems are unaltered]&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's look at the vignette under 'Measuring quality'&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;1.2 Measuring the quality of a subset&lt;/strong&gt;&lt;br&gt;&#10;  Selecting variable subsets requires the definition of a numerical criterion which measures the quality of any given variable subset. In a univariate multiple linear regression, for example, possible measures of the quality of a subset of predictors are the coefficient of determination $R^2$, the $F$ statistic in a goodness-of-fit test, its corresponding $p$-value or Akaike's Information Criterion (AIC), to give a few examples&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yeah, like that. If this is what is happening, it's essentially going to leave you with the same shopping list of problems as stepwise (less, perhaps, the problem that stepwise often misses the 'optimal' model). &lt;/p&gt;&#10;&#10;&lt;p&gt;Broadly speaking, it doesn't matter if you use this or that algorithm to find the optimum, or this or that statistic in your criterion, it's the use of optimization itself (without properly accounting for the effects of doing that) that screws everything so badly. Use any fancy optimizer you like, it's still optimizing, so you're still screwed.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) If there's some kind of regularization (such as shrinkage, as you can get with the lasso and a number or other approaches) then many of those problems may either substantially reduced or avoided. &lt;/p&gt;&#10;&#10;&lt;p&gt;3) where there's proper out of sample assessment of the performance of competing models in the class (such as via cross validation), the inferences tend to be more 'honest' - to have closer to the required properties, like approximate coverage of confidence intervals and so on. With variable selection, this would tend to involve having a subset for identification, a subset for estimation and a subset for testing. (Cross validation would then work by looking at what happens with repeated subsetting like that.)&lt;/p&gt;&#10;&#10;&lt;p&gt;More detailed specifics depend on exactly what is being done with each function, but I think that gives a framework which should usually give you a good idea.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-06T04:45:15.747" Id="85587" LastActivityDate="2014-02-06T05:03:59.160" LastEditDate="2014-02-06T05:03:59.160" LastEditorUserId="805" OwnerUserId="805" ParentId="85586" PostTypeId="2" Score="5" />
  
  
  
  <row Body="&lt;p&gt;A. Yes this is a data mining task if you are filtering the results. But you will also need computer vision with a data mining algorithm to see which is the flag and which is not.&lt;/p&gt;&#10;&#10;&lt;p&gt;B. This can be a data mining problem depending on what variables you are using. But you CANNOT have a 100% prognosis because of the impacts of improbable events.&lt;/p&gt;&#10;&#10;&lt;p&gt;C. This is a classification problem using machine learning. IF you consider machine learning data mining then yes it is a data mining task&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-06T12:03:05.593" Id="85616" LastActivityDate="2014-02-06T12:03:05.593" OwnerUserId="36854" ParentId="85612" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Let $T^{(k)}$ be the time it takes to see the first run of $k$ successes. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X\sim\mathrm{Ber}(p)$ be independent of $T^{(k)}$ for every $k$. Then,&#10;$$&#10;  T^{(k)} = (T^{(k-1)}+1)\, X + (T^{(k-1)}+1+T^{(k)}) \, (1 - X) \, ,&#10;$$&#10;because, in words, if I see a success in the current trial, then the time to get $k$ consecutive successes is the time to get $k-1$ consecutive successes plus one (the current trial); but if I see a failure, the time to get $k$ consecutive successes is the time to get $k-1$ consecutive successes plus one (the current trial), plus itself, because the process restarted in distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Defining $a_k=\mathrm{E}[T^{(k)}]$, we find the recurrence&#10;$$&#10;  a_k = (a_{k-1}+1)\,p + (a_{k-1}+1+a_k)\,(1-p) \, ,&#10;$$&#10;or&#10;$$&#10;  a_k = \frac{a_{k-1}}{p}+\frac{1}{p} \, .&#10;$$&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-06T12:09:18.327" Id="85617" LastActivityDate="2014-02-13T01:31:04.080" LastEditDate="2014-02-13T01:31:04.080" LastEditorUserId="9394" OwnerUserId="9394" ParentId="85606" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;have you looked at &lt;a href=&quot;http://cran.r-project.org/web/packages/e1071/e1071.pdf&quot; rel=&quot;nofollow&quot;&gt;e1071&lt;/a&gt; package in R? &lt;/p&gt;&#10;&#10;&lt;p&gt;naiveBayes(...) and predict(...) functions do exactly what you're looking for. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-06T13:57:26.870" Id="85624" LastActivityDate="2014-02-06T13:57:26.870" OwnerUserId="38206" ParentId="85583" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I think this answers the short version of your question:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The &lt;a href=&quot;http://cran.r-project.org/web/packages/Cubist/index.html&quot; rel=&quot;nofollow&quot;&gt;Cubist&lt;/a&gt;  package fits rule-based models (similar to trees) with&#10;  linear regression models in the terminal leaves, instance-based&#10;  corrections and boosting.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://cran.r-project.org/web/views/MachineLearning.html&quot; rel=&quot;nofollow&quot;&gt;Cran task views: Machine Learning&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-06T14:25:31.673" Id="85631" LastActivityDate="2014-02-06T14:25:31.673" OwnerUserId="29868" ParentId="78563" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;Expectation/Expected value is an operator that can be applied to a random variable. For discrete random variables (like binomial) with $k$ possible values it is defined as $\sum_i^k x_i p(x_i)$. That is, it's the mean of the possible values weighted by the probability of those values. Continuous random variables can be thought of as the generalization of this: $\int x dP$. The mean of a random variable is a synonym for expectation. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Gaussian (normal) distribution has two parameters $\mu$ and $\sigma^2$. If $X$ is normally distributed, then $E(X)=\mu$. So the mean of a Gaussian distributed variable is equal to the parameter $\mu$.This is not always the case. Take the binomial distribution, which has parameters $n$ and $p$. If $X$ is binomially distributed, then $E(X)=np$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you saw, you can also apply expectation to functions of random variables so that for a gaussian $X$ you can find that $E(X^2)=\sigma^2+\mu^2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Wikipedia page on expected values is pretty informative: &lt;a href=&quot;http://en.wikipedia.org/wiki/Expected_value&quot;&gt;http://en.wikipedia.org/wiki/Expected_value&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-06T15:28:42.070" Id="85639" LastActivityDate="2014-02-06T15:30:55.443" LastEditDate="2014-02-06T15:30:55.443" LastEditorUserId="22047" OwnerUserId="38076" ParentId="85638" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;You can definitely do that. you can introduce your categorical variable as a factorial one. If you have decided to use R programming this following code would be fine:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;new_categ&amp;lt;-factor(categ,labels=c(0:2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then, you can interact the new categorical variable with other independent ones. You also could find examples centered around your problem in Modern Applied Statistics with S-PLUS by Venables and Ripley. However, if you are not willing to use R, you can still read its examples about regression which are beneficial for figuring out how to solve your problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-06T15:45:12.190" Id="85642" LastActivityDate="2014-02-06T15:45:12.190" OwnerUserId="38420" ParentId="85637" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am currently getting $\prod x_i$ and $\prod (1-x_i)$ but a 1 dimensional sufficient statistic is asked for and considering that there is only a single parameter ($\alpha$), I think we should get a 1-dimensional sufficient statistic. I am not sure how to go about doing so though.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-06T16:10:22.693" Id="85646" LastActivityDate="2014-02-06T16:23:53.930" LastEditDate="2014-02-06T16:23:53.930" LastEditorUserId="12005" OwnerUserId="12005" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;sufficient-statistics&gt;" Title="Find 1 dimensional sufficient statistic for $Beta(\alpha, 2\alpha)$" ViewCount="47" />
  <row Body="&lt;p&gt;I would recommend using rank correlations as some relationships may be non-linear and inspect plots of Y with all independent variables as some may be even non-monotonic. And applying regression tree as a preliminary analysis. &#10;Then pick the most promising predictors and combine them in a model - a function which can be maximized by solving for predictors.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-06T16:16:21.287" Id="85647" LastActivityDate="2014-02-06T16:16:21.287" OwnerUserId="36545" ParentId="85644" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As the data in question sounds like variables such as Temp, Conductivity, pH, etc., I would find it reasonable to normalize the variables and to place all variables on even scales first (mean$=0, SD=1)$. From there, I would use Euclidean distance as your resemblance matrix and move on with testing (PCA forces this anyway).&lt;/p&gt;&#10;&#10;&lt;p&gt;With Euclidean distance you are able to use PCA (or NMDS) to visualize things to see if anything of interest arises.  Then use PERMANOVA or ANOSIM to test for differences (PERMANOVA is much more robust to correlations and heterogeneous variances; &lt;a href=&quot;http://www.esajournals.org/doi/abs/10.1890/12-2010.1&quot; rel=&quot;nofollow&quot;&gt;Anderson &amp;amp; Walsh, 2013&lt;/a&gt;). SIMPER would still be able to identify those variables that account for a large portion of the differences between groups of interest. It won't really tell you if specific variables are significantly different between locations, but it can give you a good idea of what drives sample placement in NMDS or PCA space.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-06T16:57:24.673" Id="85651" LastActivityDate="2014-02-06T17:41:34.680" LastEditDate="2014-02-06T17:41:34.680" LastEditorUserId="39633" OwnerUserId="39633" ParentId="79688" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The Maximum Likelihood Estimator is derived as the argmax of the joint density of the sample, viewed as a function of the parameters (i.e. of the likelihood). Hence, one cannot apply maximum likelihood estimation without previously specifying (or deriving) a joint density, and hence a distribution. After all, this is one of the criticisms that ML estimation has received -that it must specify a distribution -not necessarily one of the &quot;known&quot; ones, but a distribution nevertheless-, increasing the possibility for misspecification.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If one views a distribution with unknown parameters as a family of distributions, then at the end of the process, we get a &lt;em&gt;specifically parametrized&lt;/em&gt; incarnation of this family, using the ML estimates of these unknown parameters.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-06T20:32:53.640" Id="85674" LastActivityDate="2014-02-07T08:28:16.573" LastEditDate="2014-02-07T08:28:16.573" LastEditorUserId="28746" OwnerUserId="28746" ParentId="85669" PostTypeId="2" Score="5" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm considering a Hidden Markov Model as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{n+1} = F_n(X_n,\Theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_n = G_n(X_n,\Theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $Y_n$ are the observations, the $X_n$ the hidden states and $\Theta$ the parameters. At some point, I have to deal with this probability&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(X_k = x_k | X_{k-1} = x_{k-1}, Y_{0:N} = y_{0:N})$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $0 &amp;lt; k &amp;lt; N$ and $Y_{0:N}$ denotes the $Y_0, \dots, Y_N$. If it were&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(X_k = x_k | X_{k-1} = x_{k-1}, Y_{0:k} = y_{0:k})$&lt;/p&gt;&#10;&#10;&lt;p&gt;I would know how to deal with it (I think) but I wonder what it changes to add the observations for all future times. Any idea how I could handle this probability, maybe express it with respect to usual and known probabilities? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there something I'm completely missing here?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-05T13:32:14.347" Id="85688" LastActivityDate="2015-03-04T02:49:48.547" LastEditDate="2014-03-09T10:34:11.263" LastEditorUserId="88" OwnerDisplayName="user46562" PostTypeId="1" Score="2" Tags="&lt;probability&gt;" Title="Probability in hidden Markov model" ViewCount="110" />
  <row Body="&lt;p&gt;It's nothing to do with Bayesian credible intervals vs frequentist confidence intervals. A 95% (say) confidence interval is defined as giving &lt;em&gt;at least&lt;/em&gt; 95% coverage whatever the true value of the parameter $\pi$. So when the nominal coverage is 95%, the actual coverage may be 97% when $\pi=\pi_1$, 96.5% when $\pi=\pi_2$, but for no value of $\pi$ is it less than 95%. The issue (i.e. a discrepancy between nominal &amp;amp; actual coverage) arises with discrete distributions like the binomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an illustration, consider observing $x$ successes from $n$ binomial trials with unknown success probability $\pi$:&#10;$$&#10;\begin{array}{c,c,c}&#10;x &amp;amp; \pi_\mathrm{U} &amp;amp; \Pr(X= x | \pi=0.7) &amp;amp; I(\pi_\mathrm{U}\leq 0.7)\\&#10;0 &amp;amp; 0.3930378 &amp;amp; 0.000729 &amp;amp; 0\\&#10;1 &amp;amp; 0.5818034 &amp;amp; 0.010206 &amp;amp; 0\\&#10;2 &amp;amp; 0.7286616 &amp;amp; 0.059535 &amp;amp; 1\\&#10;3 &amp;amp; 0.8468389 &amp;amp; 0.185220 &amp;amp; 1\\&#10;4 &amp;amp; 0.9371501 &amp;amp; 0.324135 &amp;amp; 1\\&#10;5 &amp;amp; 0.9914876 &amp;amp; 0.302526 &amp;amp; 1\\&#10;6 &amp;amp; 1.0000000 &amp;amp; 0.117649 &amp;amp; 1\\&#10;\end{array}&#10;$$&#10;The first column shows the possible observed values of $x$. The second shows the exact&lt;sup&gt;&amp;dagger;&lt;/sup&gt; $95\%$ upper&lt;sup&gt;&amp;ddagger;&lt;/sup&gt; confidence bound $\pi_\mathrm{U} =\pi: [\Pr(X&amp;gt;x | \pi)=0.95]$ that you would calculate in each case. Now suppose $\pi=0.7$: the third column shows the probability of each observed value of $x$ under this supposition; the fourth shows for which cases the calculated confidence interval covers the true parameter value, flagging them with a $1$. If you add up the probabilities for the cases in which the confidence interval does cover the true value you get the actual coverage, $0.989065$. For different true values of $\pi$, the actual coverage will be different:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1V7wT.png&quot; alt=&quot;coverages&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The nominal coverage is only achieved when the true parameter values coincide with the obtainable upper bounds.&lt;/p&gt;&#10;&#10;&lt;p&gt;[I just re-read your question &amp;amp; noticed that the author says the actual may be &lt;em&gt;less&lt;/em&gt; than the nominal coverage probability. So I reckon he's talking about an approximate method for calculating the confidence interval, though what I said above still goes. The graph might suggest reporting an average confidence level of about $98\%$ but&amp;mdash;averaging over values of an unknown parameter?]&lt;/p&gt;&#10;&#10;&lt;p&gt;&amp;dagger; Exact in the sense that the actual coverage  is never less than the nominal coverage for any value of $\pi$, &amp;amp; equal to it for some values of $\pi$&amp;mdash; @Unwisdom's sense, not @Stephane's.&lt;/p&gt;&#10;&#10;&lt;p&gt;&amp;ddagger; Intervals with upper &amp;amp; lower bounds are more commonly used of course; but a little more complicated to explain, &amp;amp; there's only one exact interval to consider with just an upper bound. (See Blaker (2000), &quot;Confidence curves and improved exact confidence intervals for discrete distributions&quot;, &lt;em&gt;Canadian Journal of Statistics&lt;/em&gt;, &lt;strong&gt;28&lt;/strong&gt;, 4 &amp;amp; the references.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-06T22:29:06.277" Id="85691" LastActivityDate="2014-02-13T11:00:21.763" LastEditDate="2014-02-13T11:00:21.763" LastEditorUserId="17230" OwnerUserId="17230" ParentId="85666" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The values are not exactly the same, but are very close.  &lt;strong&gt;This appears to be purely accidental&lt;/strong&gt; when we look at what goes into them:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Model    beta'*Sigma*beta  Deviance     DF    Deviance/DF   &quot;est.var&quot;&#10;    1    0.0001504468      0.0005199823 1256  4.139987e-07  0.0001508608&#10;    2    9.168255e-08      0.1896668    1256  0.0001510086  0.0001511003&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the first model the result is approximately $0.000151$ because $\beta'\Sigma\beta$ is almost this value and the deviance (divided by the DF) adds essentially nothing.  In the second model $\beta'\Sigma\beta$ contributes almost nothing but the deviance (divided by the DF) is approximately $0.000151.$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is a very good idea to look into results that seem like more than coincidence, as you have done here, but accidents do happen.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-02-06T22:33:21.487" Id="85692" LastActivityDate="2014-02-06T22:47:29.767" LastEditDate="2014-02-06T22:47:29.767" LastEditorUserId="919" OwnerUserId="919" ParentId="84277" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;$W$ must be the sample space, so that $W\setminus C = C^\complement$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed, stating $\mathbb P(B\cap C) &amp;gt;\mathbb P(B)\,\mathbb P(C)$ implies that $0&amp;lt;\mathbb P(C)&amp;lt;1$*, thus, from the definition of conditional probability,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{align}&#10;\mathbb P(B\,|\,C)\,\mathbb P(C) &amp;amp;&amp;gt; \mathbb P(B)\,\mathbb P(C)\\&#10;\mathbb P(B|C) &amp;amp;&amp;gt; \mathbb P(B)\quad.&#10;\end{align}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, adding the probability of $B\cap C^\complement$ to both sides of the inequality given, using that $\mathbb P(C^\complement)\neq0$,&#10;$$\begin{align}&#10;\mathbb P(B\cap C) + \mathbb P(B\cap C^\complement) &amp;amp;&amp;gt;\mathbb P(B)\,\mathbb P(C) + \mathbb P(B\cap C^\complement)\\&#10;\mathbb P(B) &amp;amp;&amp;gt;\mathbb P(B)\,\mathbb P(C) + \mathbb P(B\cap C^\complement)\\&#10;\mathbb P(B)[1 - \mathbb P(C)] &amp;amp;&amp;gt; \mathbb P(B\cap C^\complement)\\&#10;\mathbb P(B)\,\mathbb P(C^\complement) &amp;amp;&amp;gt; \mathbb P(B\cap C^\complement)\\&#10;\mathbb P(B) &amp;amp;&amp;gt; \mathbb P(B\,|\,C^\complement)\quad,&#10;\end{align}$$&#10;and we combine the two inequalities obtained to prove that $\mathbb P(B\,|\,C) &amp;gt; \mathbb P(B\,|\,C^\complement)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;* &lt;em&gt;If $\mathbb P(C)=0$, then $\mathbb P(B\cap C) &amp;gt; 0$, but since $B\cap C\subset C$, $\mathbb P(B\cap C)\le\mathbb P(C)$, so $\mathbb P(B\cap C) = 0$, a contradiction. On the other hand, if $\mathbb P(C)=1$, we would have $\mathbb P(B\cap C) &amp;gt; \mathbb P(C)$, which contradicts $\mathbb P(B\cap C)\le\mathbb P(C)$ as well.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-07T02:34:39.963" Id="85706" LastActivityDate="2014-02-07T02:34:39.963" OwnerUserId="10515" ParentId="77276" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Good news, I guess. This is a typo. Sent an email to the &lt;a href=&quot;http://www.mostlyharmlesseconometrics.com/tag/corrections/&quot; rel=&quot;nofollow&quot;&gt;site&lt;/a&gt; @DimitriyV.Masterov cited and got a reply sooner than I would expect. &lt;a href=&quot;http://www.mostlyharmlesseconometrics.com/2014/02/whoops/&quot; rel=&quot;nofollow&quot;&gt;Here is the answer post&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-07T03:22:26.517" Id="85707" LastActivityDate="2014-02-07T18:45:38.243" LastEditDate="2014-02-07T18:45:38.243" LastEditorUserId="31901" OwnerUserId="31901" ParentId="85699" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;From &lt;a href=&quot;http://cran.r-project.org/web/packages/epiR/epiR.pdf&quot; rel=&quot;nofollow&quot;&gt;epiR manual&lt;/a&gt; for &lt;strong&gt;&lt;em&gt;epi.2by2&lt;/em&gt;&lt;/strong&gt; function:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;units: multiplier for prevalence and incidence estimates.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In examples it is set to 100, &lt;code&gt;units=100&lt;/code&gt;. What does it mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I am working on ~5K case ~5K control data, no stratas, outcome vs exposure - 2by2 tables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; This &lt;a href=&quot;http://www.medicalbiostatistics.com/prevalenceincidence.pdf&quot; rel=&quot;nofollow&quot;&gt;PDF&lt;/a&gt; might be relevant.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-07T11:15:55.103" Id="85737" LastActivityDate="2014-02-10T17:18:22.177" LastEditDate="2014-02-10T17:18:22.177" LastEditorUserId="6454" OwnerUserId="6454" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;epidemiology&gt;" Title="What does &quot;units&quot; mean in epiR package?" ViewCount="42" />
  
  
  <row AcceptedAnswerId="85761" AnswerCount="1" Body="&lt;p&gt;I have a matrix. for each row of this matrix I made a Gaussian mixture, how can I concatenate these mixtures. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-07T13:43:01.187" Id="85754" LastActivityDate="2014-02-07T14:38:18.230" OwnerUserId="38388" PostTypeId="1" Score="1" Tags="&lt;matrix&gt;&lt;gaussian-mixture&gt;" Title="how can I concatenate these mixtures" ViewCount="27" />
  <row Body="&lt;p&gt;under independence assumption, the total likelihood of a matrix $\mathbf{M}$ given that each of its rows defined by a Gaussian mixture model $f_i(X|\vec{\theta_i})$ is equal to:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\mathcal{L}(\mathbf{M}) = \prod_{i=1}^{N} f_i(X_i|\vec{\theta_i})&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $N$ is the number of rows in matrix $\mathbf{M}$, and $X_i$ denotes the i-th row in $\mathbf{M}$. The parameter vector $\vec{\theta_i}$ are supposed to be the mixture ratio parameters, mean and variance of the Gaussian distributions in the i-th distribution.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-07T14:38:18.230" Id="85761" LastActivityDate="2014-02-07T14:38:18.230" OwnerUserId="38206" ParentId="85754" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I suggest to use either the Variation of Information or the split/join measure. These are both metric distances on the space of partitions, and have the property that they will be 0 for identical partitions and get larger as partitions become more different. Further information is available here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/24961/comparing-clusterings-rand-index-vs-variation-of-information&quot;&gt;Comparing clusterings: Rand index vs variation of information&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There is no reason at all to use some pseudo-statistical measure when in fact the space of partitions can be equipped with a metric distance (several in fact). Something to be weary of are measures that are very much affected by the size of the cluster sizes (i.e. a node change is weighted differently depending on the sizes of the clusters involved). The Rand index (and associated Mirkin distance) are especially bad in this respect.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-07T17:27:15.733" Id="85787" LastActivityDate="2014-02-07T17:27:15.733" OwnerUserId="4495" ParentId="85782" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I believe you'd do something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;f &amp;lt;- arima (USAccDeaths, order=c(0, 0, 3), fixed=c(0, 0, NA, NA))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which sets the first two MA coefficients to 0, and allows the third MA coefficient and your intercept to vary. Not sure if that will give you the same coefficients you got in EViews or not, but it's what I'd try first.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-07T17:51:31.397" Id="85791" LastActivityDate="2014-02-07T17:58:11.827" LastEditDate="2014-02-07T17:58:11.827" LastEditorUserId="1764" OwnerUserId="1764" ParentId="85773" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="85811" AnswerCount="2" Body="&lt;p&gt;Background: I'm giving a presentation to colleagues at work on hypothesis testing, and understand most of it fine but there's one aspect that I'm tying myself up in knots trying to understand as well as explain it to others.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I think I know (please correct if wrong!)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Statistics that would be normal if variance was known, follow a $t$-distribution if the variance is unknown&lt;/li&gt;&#10;&lt;li&gt;CLT: The sampling distribution of the sample mean is approximately normal for sufficiently large $n$ (could be $30$, could be up to $300$ for highly skewed distributions)&lt;/li&gt;&#10;&lt;li&gt;The $t$-distribution can be considered Normal for degrees of freedom $&amp;gt; 30$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You use the $z$-test if:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Population normal and variance known (for any sample size)&lt;/li&gt;&#10;&lt;li&gt;Population normal, variance unknown and $n&amp;gt;30$ (due to CLT)&lt;/li&gt;&#10;&lt;li&gt;Population binomial, $np&amp;gt;10$, $nq&amp;gt;10$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;You use the $t$-test if:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Population normal, variance unknown and $n&amp;lt;30$&lt;/li&gt;&#10;&lt;li&gt;No knowledge about population or variance and $n&amp;lt;30$, but sample data looks normal / passes tests etc so population can be assumed normal&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So I'm left with: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;For samples $&amp;gt;30$ and $&amp;lt;\approx 300$(?), no knowledge about population and variance known / unknown.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So my questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;At what sample size can you assume (where no knowledge about population distribution or variance) that the sampling distribution of the mean is normal (i.e. CLT has kicked in) when the sampling distribution looks non-normal? I know that some distributions need $n&amp;gt;300$, but some resources seem to say use the $z$-test whenever $n&amp;gt;30$...&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For the cases I'm unsure about, I presume I look at the data for normality. Now, if the sample data does looks normal do I use the $z$-test (since assume population normal, and since $n&amp;gt;30$)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What about where the sample data for cases I'm uncertain about don't look normal? Are there any circumstances where you'd still use a $t$-test or $z$-test or do you always look to transform / use non-parametric tests? I know that, due to CLT, at some value of $n$ the sampling distribution of the mean will approximate to normal but the sample data won't tell me what that value of $n$ is; the sample data could be non-normal whilst the sample mean follows a normal / $t$.  Are there cases where you'd be transforming / using a non-parametric test when in fact the sampling distribution of the mean was normal / $t$ but you couldn't tell?  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2014-02-07T19:29:11.853" FavoriteCount="1" Id="85804" LastActivityDate="2014-02-07T20:25:58.460" LastEditDate="2014-02-07T19:38:52.827" LastEditorUserId="7290" OwnerUserId="39704" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;&lt;normal-distribution&gt;&lt;t-test&gt;&lt;assumptions&gt;&lt;z-test&gt;" Title="Choosing between $z$-test and $t$-test" ViewCount="611" />
  
  
  <row Body="&lt;p&gt;No, the move from your penultimate equation to the final equation has an error.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general you should not say  $P(Y_1,Y_2)= P(Y_1)P(Y_2)$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-07T19:39:37.217" Id="85809" LastActivityDate="2014-02-07T19:39:37.217" OwnerUserId="2958" ParentId="72245" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The series of books by Johnson, Kotz &amp;amp; Balakrishnan (edit: which Nick has also mentioned; the original books were by the first two authors) are probably the most comprehensive. You probably want to start with Continuous Univariate Distributions, Vols I and II.&lt;/p&gt;&#10;&#10;&lt;p&gt;A couple more:&lt;/p&gt;&#10;&#10;&lt;p&gt;Evans, Hastings &amp;amp; Peacock, &lt;em&gt;Statistical Distributions&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Wimmer &amp;amp; Altmann, &lt;em&gt;Thesaurus of univariate discrete probability distributions&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There's also many other books, sometimes for more specialized applications.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2014-02-08T01:15:26.917" CreationDate="2014-02-08T01:15:26.917" Id="85853" LastActivityDate="2014-02-09T01:51:40.927" LastEditDate="2014-02-09T01:51:40.927" LastEditorUserId="805" OwnerUserId="805" ParentId="26073" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="85906" AnswerCount="1" Body="&lt;p&gt;I am calculating chi-squared in R and manually, and getting two different answers. I believe that R is correct, but I am not 100% sure.  Can someone please help me understand why?&lt;/p&gt;&#10;&#10;&lt;p&gt;In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;test           &amp;lt;- matrix(c(4203, 4218, 786, 771), ncol=2)&#10;dimnames(test) &amp;lt;- list(group = c(&quot;control&quot;,&quot;exp&quot;), click = c(&quot;n&quot;,&quot;y&quot;))&#10;print(test)&#10;print(Xsq      &amp;lt;- chisq.test(test, correct=F)) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me $\chi^2 = 0.1712$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I do this by hand, however, I get $0.339$.  Here is my calculation:&#10;\begin{align}&#10;\frac{(E1 - O1)^2}{E1}     &amp;amp;+ \frac{(E2 - O2)^2}{E2}    &amp;amp;  \\&#10;\frac{(4203-4218)^2}{4203} &amp;amp;+ \frac{(771 - 786)^2}{786} &amp;amp;= .3398&#10;\end{align}&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-08T04:41:39.523" Id="85861" LastActivityDate="2014-02-09T00:07:31.257" LastEditDate="2014-02-08T05:17:40.173" LastEditorUserId="39722" OwnerUserId="39722" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;chi-squared&gt;" Title="What is wrong with this chi-squared calculation?" ViewCount="177" />
  
  <row Body="&lt;p&gt;Consider the case where the null hypothesis is that a coin is 2 headed, i.e. the probability of heads is 1.  Now the data is the result of flipping a coin a single time and seeing heads.  This results in a p-value of 1.0 which is greater than every reasonable alpha.  Does this mean that the coin is 2 headed?  it could be, but it could also be a fair coin and we saw heads due to chance (would happen 50% of the time with a fair coin).  So the high p-value in this case says that the observed data is perfectly consistent with the null, but it is also consistent with other possibilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just like a &quot;Not Guilty&quot; verdict in court can mean the defendant is innocent, it can also be because the defendant is guilty but there is not enough evidence.  The same with the null hypothesis we fail to reject because the null could be true, or it could be we don't have enough evidence to reject even though it is false.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-08T21:51:06.820" Id="85908" LastActivityDate="2014-02-08T21:51:06.820" OwnerUserId="4505" ParentId="85903" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;In &lt;em&gt;linear&lt;/em&gt; regression, you'd certainly use an F-test. &lt;/p&gt;&#10;&#10;&lt;p&gt;In logistic regression the distribution of the corresponding likelihood ratio test statistic (that for $-2 \log \Lambda$) is asymptotically chi-square. &lt;/p&gt;&#10;&#10;&lt;p&gt;[For some kinds of GLM, some people make an argument that an F-test should be used in GLMs with small samples (presumably because it's based on a ratio of scaled deviances). Despite its intuitive appeal, I haven't yet seen a convincing argument that this has better properties (specifically, either closer to nominal significance level, or better power, or both) than the chi-square one in small samples for those cases. But those arguments are for when the variance parameter, $\phi$ is estimated. I don't think those arguments apply to logistic regression, where $\phi=1$.]&lt;/p&gt;&#10;&#10;&lt;p&gt;The basic calculation is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Compute the deviance for the full model. Compute the deviance for the reduced model. Under the null, the difference should be chi-square, with df equal to the difference in degrees of freedom in the two models (with a single restriction such as yours, that would be 1 df).&lt;/p&gt;&#10;&#10;&lt;p&gt;So the question becomes 'how do we fit the reduced model to get the deviance?'&lt;/p&gt;&#10;&#10;&lt;p&gt;Some packages may let you specify such restrictions, but it's easy enough to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say you have a model like: $\quad \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;and you want to fit a reduced model which has $4 \beta_2 = 5 \beta_3 $.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: rewrite that so one of the variables is &quot;by itself&quot;: $\beta_3 = \frac{4}{5} \beta_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: substitute for that lone variable in the original equation:&lt;br&gt;&#10;$\quad \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \frac{4}{5} \beta_2 x_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: combine the $x$'s so that the repeated $\beta$ only appears once:&lt;br&gt;&#10;$\quad \eta = \beta_0 + \beta_1 x_1 + \beta_2 (x_2 + \frac{4}{5} x_3)$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: call that &quot;combined&quot; result a new x-variable, say $x_c$, and compute that combination of the other variables: $x_c = x_2 + \frac{4}{5} x_3$, creating a new set of values for the new independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: fit the corresponding model:&lt;br&gt;&#10;$\quad \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_c$&lt;/p&gt;&#10;&#10;&lt;p&gt;That's the reduced model. If the change in deviance between that and the full model is large, you'll reject the null, and if it's small, you won't.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for how to do it in R with factor variables, there may be a much more efficient way to do it, but I'd probably generate the indicator variables (the output of applying &lt;code&gt;model.matrix&lt;/code&gt; to the model), do the above manipulations and convert to working as if  the factor or factors involving the relevant variables were 0-1 (numeric) variables (but leaving any  untouched factors as factors).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it looks like there are packages that will help with that; the package &lt;code&gt;mcprofile&lt;/code&gt; looks like it will do linear restriction fits and contrasts for you (e.g. &lt;code&gt;orglm.fit&lt;/code&gt;). I've never used this.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also &lt;a href=&quot;http://stats.stackexchange.com/questions/57312/restricting-model-parameters-in-logistic-models-in-r&quot;&gt;comments on this question&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-08T22:11:01.760" Id="85912" LastActivityDate="2014-02-09T13:48:37.940" LastEditDate="2014-02-09T13:48:37.940" LastEditorUserId="805" OwnerUserId="805" ParentId="85890" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="85962" AnswerCount="3" Body="&lt;p&gt;If $\mathbf{x}$ and $\mathbf{y}$ are two [&lt;strong&gt;Update:&lt;/strong&gt; independent] random unit vectors in $\mathbb{R}^D$, what is the distribution of dot products between them in the limit of large $D$? I guess it quickly (?) becomes almost normal (does it?) with zero mean and variance decreasing in higher dimensions $$\lim_{D\to\infty}\sigma^2(D) \to 0,$$ but is there an explicit approximate formula for $\sigma^2(D)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Well, I ran some quick simulations. First, generating 10000 pairs of random unit vectors for $D=1000$ it is easy to see that the distribution of their dot products is perfectly Gaussian (in fact it is quite Gaussian already for $D=100$), see the subplot on the left. Second, for each $D$ ranging from 1 to 10000 (with increasing steps) I generated 1000 pairs and computed the variance. Log-log plot is shown on the right, and it is clear that the formula is very well approximated by $1/D$. Note that for $D=1$ and $D=2$ this formula even gives &lt;em&gt;exact&lt;/em&gt; results (but I am not sure what happens later).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hC3FM.png&quot; alt=&quot;dot products between random unit vectors&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Why?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-02-08T22:33:28.113" FavoriteCount="2" Id="85916" LastActivityDate="2014-02-09T19:15:07.247" LastEditDate="2014-02-09T11:16:52.293" LastEditorUserId="28666" OwnerUserId="28666" PostTypeId="1" Score="7" Tags="&lt;mathematical-statistics&gt;" Title="Distribution of dot products between two random unit vectors in $\mathbb{R}^D$" ViewCount="1127" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set of 10 observations on 9 bivariate random variables (one is response &amp;amp; others are covariates). I want to fit a linear regression on the whole data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know if there is any special technique to do so.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can divide the data into two equal groups. Please suggest something for that situation also.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any kind of help/suggestion.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-09T05:13:45.097" FavoriteCount="1" Id="85935" LastActivityDate="2014-02-09T12:55:50.137" LastEditDate="2014-02-09T12:55:50.137" LastEditorUserId="88" OwnerUserId="39760" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multiple-regression&gt;" Title="What to do if the number of parameters and observations are almost equal?" ViewCount="63" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am testing the reproducibility of two drug concentrations between two sites of the same individual. I am assuming response between sites should be similar. To test the significance I created a scatter plot and did a paired t test in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The results were: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t = 1.1419, df = 44, p-value = 0.2597 &#10;alternative hypothesis: true difference in means is not equal to 0 &#10;95 percent confidence interval:&#10; -6.85025 24.76136 &#10;sample estimates: &#10;mean of the differences 8.955556&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd like to know if I am right with the assumption of alternative hypothesis of &quot;difference in means is not equal to 0&quot; since I am testing reproducibility?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-09T12:02:16.930" Id="85954" LastActivityDate="2014-07-08T22:26:31.340" LastEditDate="2014-07-08T22:26:31.340" LastEditorUserId="11887" OwnerUserId="39768" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;&lt;biostatistics&gt;&lt;equivalence&gt;" Title="Assumption with alternative hypothesis?" ViewCount="41" />
  
  <row AcceptedAnswerId="85993" AnswerCount="1" Body="&lt;p&gt;I'm trying to estimate the predicted probabilities of an observation being a particular integer, $y$, after a negative binomial regression model. Long's &lt;em&gt;Regression models for categorical and limited dependent variables&lt;/em&gt; gives this predicted probability as (pg.237):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\hat{\text{Pr}}(y \mid x) = \frac{ \Gamma(y + \hat{a}^{-1}) }{ y!\Gamma(\hat{a}^{-1}) } \left( \frac{\hat{a}^{-1}}{\hat{a}^{-1}+\hat{\mu}}\right)^{\hat{a}^{-1}} \left( \frac{\hat{\mu}}{\hat{a}^{-1}+\hat{\mu}} \right)^y &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\hat{\mu}$ is the predicted mean of the variable, $\hat{a}$ is the dispersion estimate, and $\Gamma$ is the Gamma function. Now, my question is the statistical software I use takes both a &lt;em&gt;shape&lt;/em&gt; and a &lt;em&gt;scale&lt;/em&gt; parameter for the $\Gamma$ distribution, so I am confused as to how to actually estimate the predicted probabilities for any particular integer $y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the above equation, what does Long expect me to supply as the shape and the scale for the $\Gamma$ function?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-09T17:50:55.783" Id="85974" LastActivityDate="2014-07-04T08:48:51.667" LastEditDate="2014-07-04T08:48:51.667" LastEditorUserId="805" OwnerUserId="1036" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;count-data&gt;&lt;gamma-distribution&gt;&lt;negative-binomial&gt;" Title="How do you estimate the predicted probability of an integer value from a negative binomial regression equation?" ViewCount="213" />
  <row Body="&lt;p&gt;Because (&lt;a href=&quot;http://stats.stackexchange.com/a/7984&quot;&gt;as is well-known&lt;/a&gt;) a uniform distribution on the unit sphere  $S^{D-1}$ is obtained by normalizing a $D$-variate normal distribution and the dot product $t$ of normalized vectors is their correlation coefficient, &lt;strong&gt;the answers to the three questions are:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$u= 2t-1$ has a Beta$((D-1)/2,(D-1)/2)$ distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The variance of $t$ equals $1/D$ (as speculated in the question).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The standardized distribution of $t$ approaches normality at a rate of $O\left(\frac{1}{D}\right).$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Method&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The &lt;em&gt;exact&lt;/em&gt; distribution of the dot product of unit vectors is easily obtained geometrically,&lt;/strong&gt; because this is the component of the second vector in the direction of the first.  Since the second vector is independent of the first and is uniformly distributed on the unit sphere, its component in the first direction is distributed the same as any coordinate of the sphere. (Notice that the distribution of the first vector does not matter.)&lt;/p&gt;&#10;&#10;&lt;h3&gt;Finding the Density&lt;/h3&gt;&#10;&#10;&lt;p&gt;Letting that coordinate be the last, the density at $t \in [-1,1]$ is therefore proportional to the surface area lying at a height between $t$ and $t+dt$ on the unit sphere.  That proportion occurs within a belt of height $dt$ and radius $\sqrt{1-t^2},$ which is essentially a &lt;a href=&quot;http://mathworld.wolfram.com/ConicalFrustum.html&quot;&gt;conical frustum&lt;/a&gt; constructed out of an $S^{D-2}$ of radius $\sqrt{1-t^2},$ of height $dt$, and slope $1/\sqrt{1-t^2}$.  Whence the probability is proportional to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_D(t)dt = (1 - t^2)^{(D-2)/2}(1 - t^2)^{-1/2}dt = (1 - t^2)^{(D-3)/2} dt.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Letting $u=2t-1$ and noting $f_D(u) \propto u^{(D-3)/2}(1-u)^{(D-3)/2},$ it is immediate that &lt;strong&gt;$2t-1$ has a Beta$((D-1)/2, (D-1)/2)$ distribution.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Determining the Limiting Behavior&lt;/h3&gt;&#10;&#10;&lt;p&gt;Information about the limiting behavior follows easily from this using elementary techniques: $f_D$ can be integrated to obtain the constant of proportionality $\frac{\Gamma \left(\frac{n}{2}\right)}{\sqrt{\pi } \Gamma \left(\frac{D-1}{2}\right)}$; $t^k f_D(t)$ can be integrated (using properties of Beta functions, for instance) to obtain moments, showing that &lt;strong&gt;the variance is $1/D$&lt;/strong&gt; and shrinks to $0$ (whence, by Chebyshev's Theorem, the probability is becoming concentrated near $t=0$); and the limiting distribution is then found by considering values of the density of the standardized distribution, proportional to $f_D(t/\sqrt{D}),$ for small values of $t$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;\log(f_D(t/\sqrt{D})) &amp;amp;= C(D) + \frac{D-3}{2}\log\left(1 - \frac{t^2}{D}\right) \\&#10;&amp;amp;=C(D) -\left(1/2 + \frac{3}{2D}\right)t^2 + O\left(\frac{t^4}{D}\right) \\&#10;&amp;amp;\to C -\frac{1}{2}t^2&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $C$'s represent (log) constants of integration.  Evidently &lt;strong&gt;the rate at which this approaches normality (for which the log density equals $-\frac{1}{2}t^2$) is $O\left(\frac{1}{D}\right).$&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/J2Luz.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;This plot shows the densities of the dot product for $D=4, 6, 10$, as standardized to unit variance, and their limiting density.  The values at $0$ increase with $D$ (from blue through red, gold, and then green for the standard normal density).  The density for $D=1000$ would be indistinguishable from the normal density at this resolution.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-09T19:15:07.247" Id="85977" LastActivityDate="2014-02-09T19:15:07.247" OwnerUserId="919" ParentId="85916" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;I believe it's called a process because it takes place over and can change over time. In the case of your examples, they are &lt;a href=&quot;http://en.wikipedia.org/wiki/Stochastic_process&quot; rel=&quot;nofollow&quot;&gt;stochastic processes&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: As Benjamin says in a comment, the requirement isn't time, but that there is an indexing set, which could be time. It could also be spatial.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-09T19:45:14.367" Id="85981" LastActivityDate="2014-02-10T18:40:02.873" LastEditDate="2014-02-10T18:40:02.873" LastEditorUserId="1764" OwnerUserId="1764" ParentId="85978" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Say 200 people took the interview, so that 100 received a 2nd interview and 100 did not. Out of the first lot, 95 felt they had a great first interview. Out of the 2nd lot, 75 felt they had a great first interview. So in total 95 + 75 people felt they had a great first interview. Of those 95 + 75 = 170 people, only 95 actually got a 2nd interview. Thus the probability is:&#10;$$\frac{95}{(95 + 75)}=\frac{95}{170}=\frac{19}{34}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that, as many commenters graciously point out, this computation is only justifiable if you assume that your friends form an unbiased and well distributed sampling set, which may be a strong assumption.&lt;/p&gt;&#10;" CommentCount="24" CreationDate="2014-02-10T02:25:21.713" Id="86017" LastActivityDate="2014-02-14T22:44:51.507" LastEditDate="2014-02-14T22:44:51.507" LastEditorUserId="39797" OwnerUserId="39797" ParentId="86015" PostTypeId="2" Score="130" />
  <row AcceptedAnswerId="86021" AnswerCount="1" Body="&lt;p&gt;I am attempting to use simple linear regression to construct a 95% prediction interval for a continuous response variable (Y) using a continuous input variable (X). When examining my data, I realized that if I log-transform both X and Y, the assumptions of equal variance and normality are essentially met, but if I do not log transform both variables, they are not. I'm able to run all of the R code needed to get the prediction interval for the transformed data, and I've read online and elsewhere how to interpret this interval. However, is there a way to get a 95% prediction interval on the untransformed scale using the 95% prediction interval on the log-transformed scale? In other words, I'm interested in talking about the conditional mean response instead of the median response (which, according to what I've read, is how I must interpret my prediction interval). I could exponentiate the interval endpoints, but I'm fairly sure that's incorrect because E(e^{Y}) is usually not equal to e^{E(Y)}.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T02:29:34.597" Id="86018" LastActivityDate="2014-02-10T03:30:26.387" OwnerUserId="39798" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;data-transformation&gt;&lt;prediction-interval&gt;" Title="Prediction interval on untransformed scale" ViewCount="124" />
  
  <row Body="&lt;p&gt;You can use invariance property of Maximum Likelihood estimator:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Maximum_likelihood&lt;/a&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;If alpha_hat=g(theta_hat) then theta_hat is MLE for alpha_hat.  &lt;/p&gt;&#10;&#10;&lt;p&gt;And notice that 1/2 is jacobian for transformation of alpha=2*theta and average of y is MLE for myy.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-10T06:57:21.973" Id="86033" LastActivityDate="2014-02-10T06:57:21.973" OwnerUserId="28732" ParentId="85923" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As @Clayton pointed out sample autocorrelation function cannot tell if series is non-stationary. &lt;/p&gt;&#10;&#10;&lt;p&gt;Problem is that there is various levels of non-stationarity, deterministic OR stochastic trends are just one way time series to be non-stationary. &lt;/p&gt;&#10;&#10;&lt;p&gt;Non-constant variance can be also problem even when expected value is independent of time since confidence intervals based on simple sample statistics are invalid. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T07:21:07.383" Id="86037" LastActivityDate="2014-02-10T07:21:07.383" OwnerUserId="28732" ParentId="85902" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As the main effects of &lt;em&gt;cat.var1&lt;/em&gt; and &lt;em&gt;cat.var2&lt;/em&gt; were not statistically significant and I was interested about plotting only significant interaction terms (see my question above), I made two separate models to obtain coefficients for construction of plots to show those interaction between explanatory variables, i.e. one model for each significant interaction from the &lt;strong&gt;m1&lt;/strong&gt;. Both models contain two relevant variables and their interaction term. Second factor variable was thus something like &quot;pooled&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;# 1st plot for interaction cat.var1:contin.var:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;m2&amp;lt;-geeglm(dependent.var ~ cat.var1 * contin.var, family = poisson, id&#10;  = group, corstr = &quot;exchangeable&quot;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;                                         Estimate&#10;&#10;Intercept                                  -3.464&#10;cat.var1WL                                  5.264&#10;contin.var                                  0.912&#10;cat.var1WL: contin.var                     -0.917 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(1,1))&#10;plot(contin.var, dependent.var, type=&quot;n&quot;)&#10;points(contin.var[cat.var1==&quot;WL&quot;], jitter(dependent.var[cat.var1==&quot;WL&quot;]), pch=16)&#10;points(contin.var[cat.var1==&quot;CD&quot;], jitter(dependent.var[cat.var1==&quot;CD&quot;]))&#10;x &amp;lt;- seq(3,7,0.1)&#10;y1 &amp;lt;-exp (-3.464 + 0.912*x)&#10;lines(x,y1, lty=2)                                 # CD&#10;y2 &amp;lt;-exp ((-3.464 + 5.264) + (-0.917 + 0.912)*x)&#10;lines(x,y2, lty=1)                                 # WL&#10;legend(4.82, 15, c(&quot;CD&quot;, &quot;WL&quot;), pch=c(16, 1), lty=c(2, 1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot:&#10;&lt;img src=&quot;http://i.stack.imgur.com/Q0yba.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;# 2nd plot for interaction cat.var2:contin.var:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;m3&amp;lt;-geeglm(dependent.var ~ cat.var2 * contin.var, family = poisson, id&#10;  = group, corstr = &quot;exchangeable&quot;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;                                         Estimate&#10;&#10;Intercept                                  1.1384&#10;cat.var2SKA                               -4.1958&#10;contin.var                                 0.1182&#10;cat.var2SKA: contin.var                    0.7532&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(1,1))&#10;plot(contin.var, dependent.var, type=&quot;n&quot;)&#10;points(contin.var[cat.var2==&quot;SKA&quot;], jitter(dependent.var[cat.var2==&quot;SKA&quot;]))&#10;points(contin.var[cat.var2==&quot;SAH&quot;], jitter(dependent.var[cat.var2==&quot;SAH&quot;]), &#10;       pch=16)&#10;x &amp;lt;- seq(3,7,0.1)&#10;y1 &amp;lt;-exp (1.1384 + 0.1182*x)&#10;lines(x,y1, lty=2)                                 # SAH&#10;y2 &amp;lt;-exp ((1.1384 + -4.1958) + (0.7532 + 0.1182)*x)&#10;lines(x,y2, lty=1)                                 # SKA&#10;legend(4.82, 15, c(&quot;SAH&quot;, &quot;SKA&quot;), pch=c(1, 16), lty=c(2, 1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot:&#10;&lt;img src=&quot;http://i.stack.imgur.com/sk6Eb.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T07:26:37.360" Id="86038" LastActivityDate="2014-02-10T07:26:37.360" OwnerUserId="38080" ParentId="84266" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The answer is 50%. Particularly since it was an interview question I think Amazon wanted to test the candidate to see if they could spot the obvious and not be distracted by the unimportant.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;When you hear hoofbeats, think horses, not zebras - &lt;a href=&quot;http://en.wikipedia.org/wiki/Zebra_%28medicine%29&quot;&gt;reference&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My explanation:  The first statement is all the information you need.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;50% of All People who receive first interview receive a second interview&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The other two statements are just observations. Feeling you had a good interview does not increase your chances of having a second. &lt;/p&gt;&#10;&#10;&lt;p&gt;Although statistically the observations may be correct I believe they cannot be used to predict future outcomes. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;2 shops sell lottery scratch cards&lt;/li&gt;&#10;&lt;li&gt;After selling 100 cards each a customer gets a winning card from shop 1&lt;/li&gt;&#10;&lt;li&gt;Statistically you could say that shop 1 now has a greater chance of a person getting a winning ticket, 1 in 100 compared to 0 in 100 for shop 2.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;We understand this is not true. The reason it is not true is because in this example past events will not have a bearing on future outcomes.&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2014-02-10T08:30:05.887" Id="86039" LastActivityDate="2014-02-12T08:13:01.087" LastEditDate="2014-02-12T08:13:01.087" LastEditorUserId="39816" OwnerUserId="39816" ParentId="86015" PostTypeId="2" Score="21" />
  
  <row Body="&lt;p&gt;The question contains insufficient information to answer the question:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;$x$% of all people do A&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;$y$% of your friends do B&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless we know the population size of &lt;em&gt;all people&lt;/em&gt; and &lt;em&gt;your friends&lt;/em&gt;, it is not possible to answer this question accurately, unless we make either of two assumptions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The group &lt;em&gt;your friends&lt;/em&gt; is representative of the overall population.  This results in &lt;a href=&quot;http://stats.stackexchange.com/a/86017/12615&quot;&gt;Vincent Galinas' answer&lt;/a&gt; or, equivalently, &lt;a href=&quot;http://stats.stackexchange.com/a/86016/12615&quot;&gt;Alex Williams' answer&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;The group &lt;em&gt;your friends&lt;/em&gt; is not representative, and is much smaller than the overall population.  This results in &lt;a href=&quot;http://stats.stackexchange.com/a/86039/12615&quot;&gt;CeeJeeB's answer&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Do also read the &lt;a href=&quot;http://stats.stackexchange.com/questions/86015/amazon-interview-questionprobability-of-2nd-interview#comment167951_86049&quot;&gt;comment by Kyle Strand below&lt;/a&gt;.  Another aspect we should consider is &lt;em&gt;how similar am I to my friends&lt;/em&gt;?.  This depends on whether one interprets &lt;em&gt;you&lt;/em&gt; as &lt;em&gt;the person spoken to&lt;/em&gt; or as &lt;em&gt;an unspecified individual or group of individuals&lt;/em&gt; (both usages exist).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-10T09:59:08.907" Id="86049" LastActivityDate="2014-02-11T15:46:39.973" LastEditDate="2014-02-11T15:46:39.973" LastEditorUserId="12615" OwnerUserId="12615" ParentId="86015" PostTypeId="2" Score="32" />
  
  <row Body="&lt;p&gt;You may multiply the 'Root node error' by the 'rel error'. If you do so, 0.2997x0.18726 \sim 0.056 which is the error you obtained when doing cross-validation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T10:42:41.777" Id="86055" LastActivityDate="2014-02-10T10:42:41.777" OwnerUserId="29943" ParentId="35322" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think the answer is 50% - right at the beginning of the question. It's irrelevant what percentage of your friends feel. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-10T11:48:50.603" Id="86060" LastActivityDate="2014-02-10T11:48:50.603" OwnerUserId="39836" ParentId="86015" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Expanding on @Glen_b 's comment: &lt;/p&gt;&#10;&#10;&lt;p&gt;KS works quite well here. In &lt;code&gt;R&lt;/code&gt;, first create distributions like those mentioned&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;library(moments)&#10;library(lattice)&#10;&#10;UniDist &amp;lt;- runif(10000,-1,1)&#10;NormDist &amp;lt;- rnorm(10000,0,1)&#10;TruncNormDist &amp;lt;- NormDist[NormDist &amp;gt; -1 &amp;amp; NormDist &amp;lt; 1]&#10;LowDist &amp;lt;- rnorm(5000,-.5,.1)&#10;HighDist &amp;lt;- rnorm(5000,.5,.1)&#10;TruncLowDist &amp;lt;- LowDist[LowDist &amp;lt; 0 &amp;amp; LowDist &amp;gt; -.5]&#10;TruncHighDist &amp;lt;- HighDist[HighDist &amp;gt; 0 &amp;amp; HighDist &amp;lt; .5]&#10;InvDist &amp;lt;- c(TruncLowDist, TruncHighDist)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then test with &lt;code&gt;ks.test&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ks.test(TruncNormDist, UniDist)&#10;ks.test(InvDist, UniDist)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, the D values are not intuitive (at least, not to me); ordinarily, I would suggest also using quantile plots:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qqplot(TruncNormDist, UniDist)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But that does not work well here (there is almost no divergence). Nor is a basic scatterplot much help:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xyplot(TruncNormDist~UniDist)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But Tukey mean difference plots are another matter&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tmd(xyplot(TruncNormDist~UniDist))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Shows that the difference between the two distributions is near 0 at -1 and 1, but larger in the middle.&lt;/p&gt;&#10;&#10;&lt;p&gt;Comparing the inverse distribution to the truncated normal is much easier; both qqplot and xyplot show huge differences, and tmdplot is fascinating.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-10T12:21:10.120" Id="86064" LastActivityDate="2014-02-10T12:45:23.597" LastEditDate="2014-02-10T12:45:23.597" LastEditorUserId="31221" OwnerUserId="686" ParentId="86056" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="86423" AnswerCount="2" Body="&lt;p&gt;I'm studying the Kalman Filter for tracking and smoothing. Even if I have understood the Bayesian filter concept, and I can efficiently use some of Kalman Filter implementation I'm stucked on understand the math behind it in an easy way.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I'm looking for an easy to understand derivation of Kalman Filter equations ( (1) &lt;strong&gt;update step&lt;/strong&gt;, (2) &lt;strong&gt;prediction step&lt;/strong&gt; and (3) &lt;strong&gt;Kalman Filter gain&lt;/strong&gt;) from the Bayes rules and Chapman- Kolmogorov formula, knowing that:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Temporal model is expressed by: $$ \textbf{X}_t =  A\textbf{X}_{t-1} + \mu_p + \epsilon_p$$ where $A$ is transition matrix $D_\textbf{X} \times  D_\textbf{X}$, $\mu_p$ is the $D_\textbf{X} \times 1$ control signal vector and $\epsilon_p$ is a transition gaussian noise with covariance $\Sigma_m$, and in probabilistic term could be expressed by: $$ p(\textbf{X}_t | \textbf{X}_{t-1}) = Norm_{\textbf{X}_t}[\textbf{X}_{t-1} + \mu_p, \Sigma_p] $$ and&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Measurement model is expressed by: $$ \textbf{y}_t = H\textbf{X}_t + \mu_m + \epsilon_m $$ where $H$ the $D_y \times D_x$ observation matrix, that maps real state space to observation space, $\mu_m$ is a $D_\textbf{y} \times1$ mean vector, and $\epsilon_m$ is the observation noise with covariance $\Sigma_m$ that in probabilistic term could be expressed by $$ p(\textbf{y}_t | \textbf{X}_t) = Norm_{\textbf{y}_t}[ H\textbf{X}_t + \mu_m, \epsilon_m] $$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T13:44:53.300" Id="86075" LastActivityDate="2014-02-13T04:37:52.153" OwnerUserId="2046" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;gaussian-process&gt;&lt;kalman-filter&gt;" Title="Kalman filter equation derivation" ViewCount="889" />
  <row Body="&lt;p&gt;GraphPad Prism can handle one-way ANOVA in this situation. It cannot handle missing observations for any patient, but has no trouble with different number of patients in different treatment groups. &lt;a href=&quot;http://www.graphpad.com/demos&quot; rel=&quot;nofollow&quot;&gt;Try the free demo.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T14:15:16.560" Id="86077" LastActivityDate="2014-02-10T14:15:16.560" OwnerUserId="25" ParentId="67267" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think you want $p(\boldsymbol{X}_t|\boldsymbol{X}_{t-1} = N(A\boldsymbol{X}_{t-1} + \mu_p,\ldots)$ in your second equation. Regarding easy-to-follow derivations of the filter, there are many in textbooks such as &#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B008YGWPBO&quot; rel=&quot;nofollow&quot;&gt;Durbin &amp;amp; Koopman&lt;/a&gt; or &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0486439380&quot; rel=&quot;nofollow&quot;&gt;Anderson-Moore&lt;/a&gt;. You might also look &lt;a href=&quot;http://en.wikipedia.org/wiki/Kalman_filter&quot; rel=&quot;nofollow&quot;&gt;the Kalman filter at the Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-10T15:03:20.263" Id="86082" LastActivityDate="2014-02-10T15:03:20.263" OwnerUserId="892" ParentId="86075" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="86111" AnswerCount="2" Body="&lt;p&gt;I'm not sure if I fully understand the meaning of the symbol, I've seen this symbol in various articles but haven't managed to understand what they implied.&#10;I did some reading and it looks like $A \sim B$ means B-Distribution of random variable $A$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then how would it apply in continuous case as this &#10;$$\epsilon_{ij}\sim F$$&#10;$$F\sim DP$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\epsilon$ is noise and DP means Dirichlet process?&lt;/p&gt;&#10;&#10;&lt;p&gt;Then it gets more complicated with  $A | B \sim C$ such as Dirichlet process mixture models. Like is the left-hand side denoting $\theta$ given $G$?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\theta_i|G \sim G$$&#10;$$x_i|\theta_i \sim F(\theta_i)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/papers/hdp.pdf&quot;&gt;source&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-10T17:55:55.023" Id="86105" LastActivityDate="2014-02-10T19:08:07.447" LastEditDate="2014-02-10T19:08:07.447" LastEditorUserId="21958" OwnerUserId="21958" PostTypeId="1" Score="6" Tags="&lt;terminology&gt;" Title="What does &quot;$\sim$&quot; mean and $A | B \sim C$?" ViewCount="239" />
  
  <row AcceptedAnswerId="86172" AnswerCount="1" Body="&lt;p&gt;I want to compute Mean Average Precision (MAP) from Average Precision (AP) over all users and over every 5 minute intervals of a day. How do I compute MAP from AP for both these dimensions [user, time interval].&lt;/p&gt;&#10;&#10;&lt;p&gt;That is I get an AP per user, per 5 minute. I want MAP for whole day averaged across all users.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a &quot;MMAP&quot; (for lack of better word)? Does it even make sense as I am computing average over an average? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-10T20:39:45.713" Id="86122" LastActivityDate="2014-02-19T18:21:46.387" LastEditDate="2014-02-10T22:55:20.897" LastEditorUserId="39939" OwnerUserId="39939" PostTypeId="1" Score="0" Tags="&lt;average-precision&gt;" Title="Mean Average Precision (MAP) in two dimensions" ViewCount="150" />
  
  <row AcceptedAnswerId="86252" AnswerCount="1" Body="&lt;p&gt;What is &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;conditional quantile of $Y(t)$ given $Y(t-1)$ where $Y(t)$ is a univariate time series &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(they call it conditional value at risk in risk management).&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody explain this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-10T23:21:59.943" Id="86140" LastActivityDate="2014-02-11T21:38:28.450" LastEditDate="2014-02-10T23:50:06.837" LastEditorUserId="17590" OwnerUserId="17590" PostTypeId="1" Score="0" Tags="&lt;econometrics&gt;&lt;conditional-probability&gt;&lt;quantiles&gt;&lt;risk&gt;" Title="What is a conditional quantile function?" ViewCount="59" />
  
  <row Body="&lt;p&gt;jMetrik is more powerful than you may think. It is designed for operational work where researchers need multiple procedures in a single unified framework. Currently you can estimate IRT parameters for the Rasch, partial credit and rating scale models. It also allows for IRT scale linking via the Stocking-Lord, Haebara and other methods. Because it includes an integrated database, the output from the IRT estimation can be used in scale linking without the need to reshape data files. Moreover, all output can be stored in the database for use with other methods in jMetrik or external programs like R. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can also run it with scripts instead of the GUI. For example, the follwing code will (a) import data into the database, (b) score items with an answer key, (c) estimate Rasch model parameters, and (d) export data as a CSV file. You can use the final output file as input into R for further analysis, or you can use R to connect directly to the jMetrik database and work with the results. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#import data into database&#10;import{&#10;     delimiter(comma);&#10;     header(included);&#10;     options(display);&#10;     description();&#10;     file(C:/exam1-raw-data.txt);&#10;     data(db = testdb1, table = EXAM1);&#10;}&#10;&#10;#conduct item scoring with the answer key&#10;scoring{&#10;     data(db = mydb, table = exam1);&#10;     keys(4);&#10;     key1(options=(A,B,C,D), scores=(1,0,0,0), variables=  (item1,item9,item12,item15,item19,item21,item22,item28,item29,item30,item34,item38,item42,item52,item55));&#10;     key2(options=(A,B,C,D), scores=(0,1,0,0), variables=(item4,item6,item16,item18,item24,item26,item32,item33,item35,item43,item44,item47,item50,item54));&#10;     key3(options=(A,B,C,D), scores=(0,0,1,0), variables=(item3,item5,item7,item11,item14,item20,item23,item25,item31,item40,item45,item48,item49,item53));&#10;     key4(options=(A,B,C,D), scores=(0,0,0,1), variables=(item2,item8,item10,item13,item17,item27,item36,item37,item39,item41,item46,item51,item56));&#10;}&#10;&#10;#Run a Rasch models analysis.&#10;#Item parameters saved as database table named exam1_rasch_output&#10;#Residuals saved as a databse table named exam1_rasch_resid&#10;#Person estimates saved to original data table. Person estimate in variable called &quot;theta&quot;&#10;rasch{&#10;     center(items);&#10;     missing(ignore);&#10;     person(rsave, pfit, psave);&#10;     item(isave);&#10;     adjust(0.3);&#10;     itemout(EXAM1_RASCH_OUTPUT);&#10;     residout(EXAM1_RASCH_RESID);&#10;     variables(item1, item2, item3, item4, item5, item6, item7, item8, item9, item10, item11, item12, item13, item14, item15, item16, item17, item18, item19, item20, item21, item22, item23, item24, item25, item26, item27, item28, item29, item30, item31, item32, item33, item34, item35, item36, item37, item38, item39, item40, item41, item42, item43, item44, item45, item46, item47, item48, item49, item50, item51, item52, item53, item54, item55, item56);&#10;     transform(scale = 1.0, precision = 4, intercept = 0.0);&#10;     gupdate(maxiter = 150, converge = 0.005);&#10;     data(db = testdb1, table = EXAM1);&#10;}&#10;&#10;#Export output table for use in another program like R&#10;export{&#10;     delimiter(comma);&#10;     header(included);&#10;     options();&#10;     file(C:/EXAM1_RASCH_OUTPUT.txt);&#10;     data(db = testdb1, table = EXAM1_RASCH_OUTPUT);&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The software is still in its early stages of development. I am currently adding exploratory factor analysis and more advanced item response models. Unlike many other IRT programs, jMetrik is open source. all of the measurement procedures use the psychometrics library which is currently available on GitHub, &lt;a href=&quot;https://github.com/meyerjp3/psychometrics&quot; rel=&quot;nofollow&quot;&gt;https://github.com/meyerjp3/psychometrics&lt;/a&gt;. Anyone interested in contributing is welcomed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-11T00:36:31.710" Id="86151" LastActivityDate="2014-02-11T00:48:33.783" LastEditDate="2014-02-11T00:48:33.783" LastEditorUserId="39970" OwnerUserId="39970" ParentId="15565" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;what does it mean for a variable to be statistically significant??&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone give a complete answer to this question?&lt;/p&gt;&#10;" ClosedDate="2014-02-11T10:09:04.403" CommentCount="1" CreationDate="2014-02-11T04:14:59.820" Id="86162" LastActivityDate="2014-02-11T07:32:04.153" OwnerUserId="39991" PostTypeId="1" Score="-2" Tags="&lt;regression&gt;" Title="what does it mean for a variable to be statistically significant?" ViewCount="600" />
  <row AnswerCount="1" Body="&lt;p&gt;This is from a Bayesian problem I'm working on. I have worked out&#10;\begin{align}&#10;f(y_1,...,y_T|\varphi)=f(y_1|\varphi)f(y_2|y_1,\varphi)...f(y_T|y_1,y_2,...,y_{T-1},\varphi),&#10;\end{align}&#10;and all terms in the equation above are known.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let $X=(x_1,...,x_T)$ with $x_i=y_i$ if $y_i&amp;gt;0$ and $x_i=0$ if $y_i\le0$. How do I calculate $f(x_i,...,x_T|\varphi)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a bit like the tobit model but $f(y_i)$ is unknown in my case, I only have their conditional density..&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-11T05:51:17.030" Id="86168" LastActivityDate="2014-09-16T20:54:02.280" LastEditDate="2014-02-11T09:23:15.270" LastEditorUserId="2116" OwnerUserId="38426" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;bayesian&gt;&lt;multivariate-analysis&gt;&lt;censoring&gt;&lt;tobit-regression&gt;" Title="Left-censoring in time series data" ViewCount="75" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to build a content-control (web filtering) application using machine learning (just for training purposes). For example define gaming sites.  I'm somewhat familiar with machine learning algorithms, but I have almost no idea how to get features from HTML pages. Can anyone suggest any good material? Maybe some resource with training data? I will be glad of any advice!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-11T08:19:38.067" Id="86175" LastActivityDate="2014-04-29T04:51:40.080" LastEditDate="2014-03-17T00:21:05.433" LastEditorUserId="32036" OwnerUserId="28377" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;data-mining&gt;&lt;filter&gt;" Title="Content-control (web filtering) using machine learning" ViewCount="79" />
  <row AnswerCount="1" Body="&lt;p&gt;I was ask to do a linear regression analyses of my variables. I have Reaction times. Can you recommend me a linear regression analysis (hierarchical?) and tell me whether I should transform my raw data into logarithms before that analysis? &lt;/p&gt;&#10;&#10;&lt;p&gt;My reaction times are in milliseconds.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-11T10:04:21.043" Id="86182" LastActivityDate="2014-10-21T06:25:52.473" LastEditDate="2014-02-11T13:22:23.833" LastEditorUserId="88" OwnerUserId="40009" PostTypeId="1" Score="-1" Tags="&lt;regression&gt;&lt;anova&gt;&lt;t-test&gt;&lt;matlab&gt;&lt;hierarchical&gt;" Title="Hierarchical linear regression analyses, but with logarithms?" ViewCount="85" />
  <row AnswerCount="0" Body="&lt;p&gt;This problem occurred to me a couple of days ago, in the context of a game with a leaderboard.  I wondered, given only the leaderboard, could I estimate parameters for the distribution of scores?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Estimation of order statistics is a reasonably well understood problem, but as far as I can see all require knowledge of n, the total number of games giving rise to the leaderboard.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If we know properties of the order statistics (for example, if we know that they tend to a uniform distribution) we might estimate the parameters of the order statistics distribution and work back from there.  However, this feels rather unsatisfying.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-11T10:16:30.210" Id="86184" LastActivityDate="2014-02-11T10:16:30.210" OwnerUserId="40007" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;order-statistics&gt;" Title="Estimating distribution given top k order statistics and unknown n" ViewCount="62" />
  
  
  <row Body="&lt;p&gt;Assuming $y_t$ is stationary and $|a_1|&amp;lt;1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;First calculate the joint distribution of $f(y_t,y_{t-1})$. This you can calculate using the conditions&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(y_t,y_{t-1}) = f(y_t|y_{t-1}) f(y_{t-1})$&lt;/p&gt;&#10;&#10;&lt;p&gt;You already know about this part $f(y_t|y_{t-1})$ and  $f(y_{t-1})$ is the unconditional density function (The distribution will be normal and you can  calculate the mean and variance).&lt;/p&gt;&#10;&#10;&lt;p&gt;From the joint distribution (bivariate normal) you can calculate the probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Eg: &#10;$f(y_t | y_{t-1}&amp;gt;c) =  \dfrac{\int_{c}^{\infty} f(y_t, y_{t-1})dy_{t-1} } {\int_{c}^{\infty} f(y_{t-1})dy_{t-1}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I think these are good hints to proceed your problem.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-02-11T12:56:10.697" Id="86204" LastActivityDate="2014-02-11T12:56:10.697" OwnerUserId="7788" ParentId="86185" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am dealing with sentiment results of web articles. Sentiment is represented with two int values, +ve and -ve.&lt;/p&gt;&#10;&#10;&lt;p&gt;For given article &quot;xyz&quot; I am getting different sentiments while testing at different time period. Each result has +Ve and -ve value and value are close to each other. I represent them using 2-D Graph on x-y axis.&lt;/p&gt;&#10;&#10;&lt;p&gt;values are like this:&#10;    x=10, y=100 - 10&lt;/p&gt;&#10;&#10;&lt;p&gt;For my given point (x,y) &lt;code&gt;Euclidean distance&lt;/code&gt; for each point can be calculated but how it is useful? Because mypurpose is not to get nearest neightbour. But getting most relevant and eligible point from all available. &lt;/p&gt;&#10;&#10;&lt;p&gt;My purpose is to get sentiment result with highest density. Is KNN also is suitable for this kind of scenario? Or any othe approach if someone can suggest ..&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to understand this doc: &lt;a href=&quot;http://www.statsoft.com/textbook/k-nearest-neighbors&quot; rel=&quot;nofollow&quot;&gt;http://www.statsoft.com/textbook/k-nearest-neighbors&lt;/a&gt; but not clear whether it can do what I want or not.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-11T14:33:20.950" Id="86214" LastActivityDate="2014-02-11T14:33:20.950" OwnerUserId="39727" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;probability&gt;&lt;estimation&gt;&lt;data-mining&gt;" Title="Density estimation using knn" ViewCount="42" />
  
  <row Body="&lt;p&gt;It sounds like you're trying to create an active learning algorithm, if I interpreted the setting correctly. If so, have you already looked at other active learning methods and found them unsatisfactory for some reason?&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether 500 documents is too small a set or not of course depends on the problem, but there are other issues too. First, your stopping criterion can't simply be a threshold of the accuracy on the set H, since you don't know the Bayes-optimal error for the problem at hand (unless you &lt;em&gt;do&lt;/em&gt;...). Worse, if H happens to be a bad sample (not representative), then you could be stuck getting more and more labels from the oracle (which I'm guessing is expensive, which is why you're trying to avoid it).&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: on second thought, it's not &lt;em&gt;really&lt;/em&gt; active learning since you're not focusing on how to pick which samples to give to the oracle. It's more of a stopping criterion problem for online/active learning, e.g.:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1753784&quot; rel=&quot;nofollow&quot;&gt;Zhu et al. &quot;Confidence-based stopping criteria for active learning for data annotation&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://webdocs.cs.ualberta.ca/~greiner/RESEARCH/PAPERS/seqpac-colt95.pdf&quot; rel=&quot;nofollow&quot;&gt;Schuurmans, Dale, and Russell Greiner. &quot;Sequential PAC learning.&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/early/2004/08/05/bioinformatics.bth461.full.pdf&quot; rel=&quot;nofollow&quot;&gt;Fu et al. &quot;How Many Samples Are Needed to Build A Classiﬁer: A General Sequential Approach&quot;&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-11T14:45:11.320" Id="86216" LastActivityDate="2014-02-11T18:01:10.260" LastEditDate="2014-02-11T18:01:10.260" LastEditorUserId="23664" OwnerUserId="23664" ParentId="86117" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;What i am trying to do is to take a machine learning algorithm that is already trained.Run the algorithm several times and collect the output.&lt;/p&gt;&#10;&#10;&lt;p&gt;Conduct a statistical analysis on the output and try to create an inference regarding the parameters of the algorithm &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this possible ? Are there known statistical tests which does this or which does something similar to this ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Adding from the comment&lt;/p&gt;&#10;&#10;&lt;p&gt;I was not sure whether to add statistical bias or not.I ll try to explain my thought flow when i added that tag. If in fact we can learn something about the algorithm (it could be the parameters used or something else) from the output, then it should be(i am not sure) because the output is biased in some way. My aim to find out if there is some way to learn something about the algorithm used, using the properties of this output.Have you heard of such a thing ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-11T16:53:16.363" Id="86235" LastActivityDate="2014-02-12T10:12:00.903" LastEditDate="2014-02-12T10:12:00.903" LastEditorUserId="40041" OwnerUserId="40041" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;statistical-bias&gt;" Title="Are there known statistical tests to determine the parameters of a machine learning algorithm?" ViewCount="83" />
  
  <row AnswerCount="0" Body="&lt;p&gt;A friend is looking at how closely related genes cluster together on a section of DNA. In particular, he is looking at genes that code for mitochondrial phenotypes (what I will call &quot;mitochondrial genes&quot;) vs ones that don't.&lt;/p&gt;&#10;&#10;&lt;p&gt;He has picked a distance, say D = 500,000 base pairs, and for each branch of each chromosome, computed -&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The average density of genes within D base pairs of another gene.&lt;/li&gt;&#10;&lt;li&gt;The average density of genes on the entire branch.&lt;/li&gt;&#10;&lt;li&gt;The average density of mitochondrial genes with D base pairs of another mitochondrial gene.&lt;/li&gt;&#10;&lt;li&gt;The average density of mitochondrial genes on the entire branch.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The hypothesis is that mitochondrial genes cluster together more than non-mitochondrial genes do.&lt;/p&gt;&#10;&#10;&lt;p&gt;What statistical test is appropriate for this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-11T18:16:05.013" Id="86244" LastActivityDate="2014-02-11T18:16:05.013" OwnerUserId="2425" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;genetics&gt;" Title="Gene location clustering" ViewCount="20" />
  <row AcceptedAnswerId="86303" AnswerCount="3" Body="&lt;p&gt;I've been attempting to forecast natural gas power demand and how it is affected by temperature and price. I'm not sure if I have done everything correctly (relatively new to R), but I do seem to get relevant data other than I can't seem to change my forecast period, nor am I sure this is an appropriate model for this data. Hopefully someone can provide me with some guidance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data: &lt;a href=&quot;https://www.dropbox.com/s/g9uytz3guyjrbq2/demand.csv&quot; rel=&quot;nofollow&quot;&gt;demand.csv&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(forecast)&#10;data = read.csv(&quot;demand.csv&quot;)&#10;&#10;# Create matrix of numeric predictors&#10;xreg &amp;lt;- cbind(weather=data$Weather,price=data$Price,m1=data$M1,&#10;    m2=data$M2,m3=data$M3,m4=data$M4,m5=data$M5,m6=data$M6,&#10;m7=data$M7,m8=data$M8,m9=data$M9,m10=data$M10,m11=data$M11)&#10;&#10;# Rename columns&#10;colnames(xreg) &amp;lt;- c(&quot;Weather&quot;,&quot;Price&quot;,&quot;Jan&quot;,&quot;Feb&quot;,&quot;Mar&quot;,&quot;Apr&quot;,&#10;&quot;May&quot;,&quot;Jun&quot;,&quot;Jul&quot;,&quot;Aug&quot;,&quot;Sep&quot;,&quot;Oct&quot;,&quot;Nov&quot;)&#10;&#10;# Variable to be modelled&#10;demandTS &amp;lt;- ts(data$Demand, frequency=12)&#10;&#10;# Find ARIMAX model&#10;demandArima &amp;lt;- auto.arima(demandTS, xreg=xreg)&#10;demand.fcast &amp;lt;- forecast(demandArima, xreg=xreg)&#10;plot(demand.fcast)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you for any help.&lt;/p&gt;&#10;&#10;&lt;p&gt;References:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r&quot;&gt;How to setup xreg argument in auto ARIMA in R&lt;/a&gt;&#10;&lt;a href=&quot;http://stackoverflow.com/questions/10606295/from-auto-arima-to-forecast-in-r&quot;&gt;From auto ARIMA to forecast in R&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-11T18:55:00.240" FavoriteCount="1" Id="86248" LastActivityDate="2014-02-22T21:47:05.113" LastEditDate="2014-02-11T20:38:29.363" LastEditorUserId="36284" OwnerUserId="36284" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;arima&gt;" Title="Performing a time series ARIMA model on natural gas power demand using the forecast package from R" ViewCount="723" />
  
  <row Body="&lt;p&gt;The &quot;response&quot; or &quot;outcome&quot; variables are commonly used terms.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-11T19:41:01.753" Id="86253" LastActivityDate="2014-02-11T19:41:01.753" OwnerUserId="2425" ParentId="35255" PostTypeId="2" Score="0" />
  
  
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;Neither of your fitted models can be correct for your original equation, and your original equation cannot be a correct model for the random variables you observe.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's address some issues one at a time.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Your original equation is $A=kC$, but of course data is not observed at population values (otherwise you'd only need one $x$ and $y$ value, since $k=y/x$. Clearly this is not the right model for the data or you wouldn't be trying to fit regressions. We'll see how to rewrite it shortly.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) If both variables are observed with error, you need more specialized techniques than simple linear regression models.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) If $C$ is observed with no error (or very low error relative to $A$), you presumably mean a model something like say $E(A|C) = kC$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question then is &quot;how to model the variation about the mean&quot; - we need a distribution for $A$ at each $C$, or at least some information relating to the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;a) As you suggested, you could fit a linear model directly to the data on the original scale:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{E}(A|C)=kC$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This might suitable if the variability is near constant on this scale ($\text{Var}(A|C)$ near-constant).&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this model, based on your population model, has no intercept term; it goes through the origin. &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, if you have some other model for the variation about the line, such as $\text{Var}(A|C)\propto C^p$, then you could fit a weighted regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;b) As you mention, another possible model may be fitted on the log-scale:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{E}(\log_{10}A|\log_{10}C)=\log_{10}k+\log_{10}C$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This might suitable if the variation is near constant on the log scale ($\text{Var}(\log_{10}A|\log_{10}C)$ near-constant) ... which you'd tend to see if the variance-power in (a) were 2 or close to 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this model has an intercept, but has a slope coefficient of 1. A way to fit this model is to fit:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{E}(\log_{10}A|\log_{10}C)-\log_{10}C=\log_{10}k$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(That said, you may want to check a more general model that your original one, such as considering a non-unit slope in (b) for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;The fits in (a) and (b) weight the data differently (though with $p=2$ in (a) the results will be quite close), so they won't give identical answers. On your data they differ by about 26%, which illustrates that for such a sample the choice is quite important.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't have any prior knowledge to guide the choice, residual analysis might be a way to arrive at a model (especially if you have another data set on which you can base that choice); alternatively, you could make $p$ a parameter of a larger model.&lt;/p&gt;&#10;&#10;&lt;p&gt;(As it happens, a little residual analysis suggests to me there may be potential problems with both (a) with constant variance and with the model in (b); neither model, nor even, perhaps, the more general models (a)-with0intercept and (b)-with-non-unit slope are necessarily suitable (there's a suggestion of curvature about the line). One of the concerns is leverage in (a), which stems from the 'many orders of magnitude')&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Note that there are numerous &lt;em&gt;other&lt;/em&gt; models that might be fitted.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, consider $E(A^q|C) = k^qC^q$ for some specified constant $q$ (the log-model corresponds to $q=0$).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-12T10:05:02.793" Id="86319" LastActivityDate="2014-02-12T11:32:41.463" LastEditDate="2014-02-12T11:32:41.463" LastEditorUserId="805" OwnerUserId="805" ParentId="86302" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Why not use a simple linear regression, in the form&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = ax + b&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;total_sales = a*sample + b&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;since your correlation is so high?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-12T13:40:05.617" Id="86338" LastActivityDate="2014-02-12T13:40:05.617" OwnerUserId="32065" ParentId="74002" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Can you nominate which are the steps which I need to follow in order to get a basic grasp into the field of forecasting? which are the &quot;must to know things&quot;? &lt;/p&gt;&#10;" ClosedDate="2014-02-12T16:51:54.120" CommentCount="0" CommunityOwnedDate="2014-02-12T16:51:04.320" CreationDate="2014-02-12T16:22:20.210" FavoriteCount="0" Id="86357" LastActivityDate="2014-02-12T16:34:40.187" OwnerUserId="3856" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;forecasting&gt;" Title="Basics of forecasting" ViewCount="28" />
  
  <row Body="&lt;p&gt;A risk factor is typically a variable thought to be related to the incidence of an outcome. Anylyses of risk factors usually center on trying to make reasonable causal inferences, to facilitate subsequent preventive efforts. &lt;/p&gt;&#10;&#10;&lt;p&gt;A prognostic factor is typically a variable thought to be related to how a disease progresses, given you already have the disease. Analyses of prognostic factors tend to focus on making predictions, irrespective of any causal inferences.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-13T01:31:57.830" Id="86413" LastActivityDate="2014-02-13T01:31:57.830" OwnerUserId="16049" ParentId="83509" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;If $X_1, X_2, \ldots, X_n \sim {\rm Bernoulli}(p)$ are IID, then $S = \sum_{i=1}^n X_i \sim {\rm Binomial}(n,p)$.  Therefore, $${\rm Var}[S/n] = \frac{1}{n^2}\cdot np(1-p) = \frac{p(1-p)}{n}.$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-13T05:25:15.980" Id="86425" LastActivityDate="2014-02-13T05:25:15.980" OwnerUserId="36771" ParentId="86422" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I often use the reduced chi squared as a quick goodness of fit test when fitting histograms. Is there an analogous method that works more generally for interpreting the absolute value of the best fit likelihood when data are not normal? I'm thinking of binomial and poisson with noise here but the more general the better. I guess what I might be fishing for is an answer to this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/37678/relation-between-reduced-chi-square-and-akaike-criterion&quot;&gt;Relation between reduced chi square and Akaike criterion&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-13T10:05:52.873" Id="86439" LastActivityDate="2014-02-13T11:44:39.897" LastEditDate="2014-02-13T11:44:39.897" LastEditorUserId="2116" OwnerUserId="7734" PostTypeId="1" Score="2" Tags="&lt;goodness-of-fit&gt;" Title="Reduced chi square goodness of fit" ViewCount="89" />
  
  
  
  <row Body="&lt;p&gt;Following the discussion in the comments above and your edits, it is clear that the data is not ordinal, but simply discrete. It makes everything rather easy: e.g. you can run the PCA.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to do that, you need to convert your count table into a data matrix with two columns (X and Y) and as many rows as you have observations (1070). I always prefer to look at a scatter plot before doing PCA; in this case scatter plot will be hard to interpret because almost all the points overlap, so a better way to plot it would be with some random jitter:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7KlHU.png&quot; alt=&quot;PCA on discrete data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we see that the data points are obviously spread along the main diagonal. Indeed, PCA tells us that the first PC explains 72% of the total variance, and is pointing in the direction (0.69, 0.72). This is &lt;em&gt;very&lt;/em&gt; close to the diagonal (0.71, 0.71), so you can as well take the diagonal itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you only need to project your data onto the first PC (or on the diagonal). Which in case of the diagonal is equivalent to taking the average between X and Y, as @NickCox suggested above. So if I were you, I would center the data (subtract mean X and mean Y) and take the average. But it would be a good idea to mention in the methods that the first PC was almost exactly the diagonal.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;@PaulM made a very good point in the comments: the square form of the grid might bias the covariance matrix towards the diagonal. Indeed, looking at these data, one might suspect that if the measurement scale (0-8) somehow had a broader range, then some of the points that fall on the boundaries of the square grid would actually be located further away from its centre. Constraining all the data points to the square grid can introduce a bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think it is a valid concern, and I don't know a principled way to deal with it. However, I can suggest an ad-hoc method. Let us cut out chunks of the grid that have a form that is not diagonally elongated. A &quot;cross&quot; seems to be a natural choice: one could take 5 neighbouring grid positions that form a cross, but I decided to take 12 positions that form a larger cross (see figure below). It is easy to select points belonging to such a cross, as they have distance less than 2 to the centre of the cross, and all other grid locations are further away.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the following picture I show 10 cross positions that I used (black dots), and the red circle shows the boundary for the first cross. [Note that blue points were jittered only for visualization, whereas all the analysis was done on the unjittered points.] For each cross I computed the PCA and plotted the direction of the first PC as a black line.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ThRdh.png&quot; alt=&quot;PCA on a grid&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One can see that all black lines are roughly diagonal, confirming that the bias does not play a big role in the above conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Still, to try to correct the possible bias, one could take the average over the 10 &quot;bootstrapped&quot; PCs: (0.79, 0.61), which is 7.5 degrees below the diagonal, or maybe the weighted average with number of points in each cross serving as a weight: (0.80, 0.59), which is 8.5 degrees below.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-02-13T16:02:47.080" Id="86464" LastActivityDate="2014-02-14T18:32:47.993" LastEditDate="2014-02-14T18:32:47.993" LastEditorUserId="28666" OwnerUserId="28666" ParentId="86456" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="88766" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt;&#10;Consider a scenario where you observe the inputs ($X$) to and outputs ($Y$) from a process ($B$). If I have a model describing how $X$ evolves over time, and a similar model for $Y$, how do I combine the two?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example + Details&lt;/strong&gt;&#10;To make this concrete, consider the coupled $B$iological process of photosynthesis + respiration ($B = B_p + B_r$), where, to a rough approximation, for every atom of Carbon ($X$) taken up, 1 atom of Oxygen ($Y$) is released. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are other factors ($F_x$, $F_y$) that also play minor roles in the abundance of $X$ or $Y$, but these &quot;factors&quot; are deterministic processes based on $X$ or $Y$ at the previous time step and 2 known control vectors ($K$, $Z$), and the parameters of interest are associated with the process $B$. $B$ is a function of 2 control variables (in a Tx2 matrix, $U$), and 2 parameters (say, $\beta_1$ and $\beta_2$, in a 2x1 matrix, $\beta$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The model for $X$ might look something like $$X_t = X_{t-1} + B_{t} + F_{x,t}$$ $$B_{t} = U_t\beta$$ $$F_{x,t}=K_{x,t}(Z_{t-1}-X_{t-1})$$ Similarly, $Y$ is driven by $B$, but in the opposite direction $$Y_t = Y_{t-1} + -B_{t} + F_{y,t}$$$$B_{t} = U_t\beta$$ $$F_{y,t}=K_{y,t}(Z_{t-1}-Y_{t-1})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Current Framework, Objective&lt;/strong&gt;&#10;I already have both models for $X$ and $Y$ working in several frameworks (as a dlm fit with maximum likelihood, in a Kalman filter, and in a Bayesian framework). But I want to put the two analyses (for $X$ and $Y$) together.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;JAGS Example&lt;/strong&gt;&#10;In JAGS, the main loop of my current model for $X$ looks something like this, and would look identical for $Y$ (the signs of estimated parameters reversed):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for(i in 2:N){  &#10;    X[i] ~ dnorm(a[i], tauV) # observations (X) are distributed with mean of true values, but with precision tauV (1/tauV is variance of observation error)&#10;    Kx[i] ~ dnorm(KxP[i-1, 1], 1/KxP[i-1, 2]) #distributin on Kx, from input KxP containing means and variances of Kx at each time step&#10;    Fx[i] &amp;lt;- Kx[i]*(Z[i-1]-X[i-1]) # Z is just an input control vector of know values&#10;    aHat[i] &amp;lt;- a[i-1] + U[i,]%*%Beta + Fx[i] # the process&#10;    a[i] ~ dnorm(aHat[i], tauW) # true values have a mean of estimated values, but accompanied by process error (process precision is tauW)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&#10;It seems that I am not making use of valuable information when I estimate $\beta$ independently from $X$ and $Y$, especially b/c both time series are prone to different sources of observation and process error (as well as some of the same sources, especially for the process error).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&#10;How do I couple the models? Do I model the observations as multivariate normal?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-13T17:41:11.530" FavoriteCount="1" Id="86474" LastActivityDate="2014-03-04T19:12:20.627" LastEditDate="2014-02-14T16:36:14.373" LastEditorUserId="25184" OwnerUserId="25184" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;bayesian&gt;&lt;multivariate-analysis&gt;&lt;jags&gt;" Title="Is this multivariate normal? 2 time series linked by a common process" ViewCount="57" />
  
  
  <row Body="&lt;p&gt;1: The standard deviation stands for the standard deviation of the fitted random intercept respectively the fitted random slope. I.e. the assumption of your fitted model is  &lt;/p&gt;&#10;&#10;&lt;p&gt;$\left(\matrix{b_0 \\ b_1}\right) \sim \mathcal{N} \left(\left(\matrix{0\\0}\right), \left(\matrix{\tau_0 &amp;amp; \tau_{01} \\ \tau_{10} &amp;amp; \tau_1}\right)\right)$, &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\sqrt{\tau_0}$ estimates to 42.215374, $\sqrt{\tau_1}$ estimates to 4.059765 and the correlation resulting from the estimated covariance  $\tau_{01}$ of $b_0$ and $b_1$ estimates to 0.455. In other words, the values provide information on the variance of your data, which can be explained by the random effects. StdDev of Residual as the name suggests stands for the estimated residual standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;2: A similar question on stack overflow: &lt;a href=&quot;http://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output&quot;&gt;How do I interpret the &amp;#39;correlations of fixed effects&amp;#39; in my glmer output?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;3: To evaluate whether your model is &quot;a good model&quot;, you will need to consider the aim of your analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the aim of your model is to explain the variability of your data as much as possible, you may have a look at &lt;a href=&quot;http://stats.stackexchange.com/questions/7240/proportion-of-explained-variance-in-a-mixed-effects-model&quot;&gt;Proportion of explained variance in a mixed-effects model&lt;/a&gt;. In this case, you could maybe improve your model by adding additional covariates / random effects or check, if any of your covariates has a non-linear influence on Tumor.Volume. But be aware when comparing mixed models - changing the fixed effects in a mixed model will result in a different calculation of the likelihood and thus comparisons based on the likelihood produce invalid results (with REML estimation).&lt;/p&gt;&#10;&#10;&lt;p&gt;If your aim is to predict tumor volumes with further observations of covariates, you'll have to chose a criterion (e.g. some sort of prediction error) , which evaluates your prediction performance and, based on this, compare your models.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your model is based on theoretical assumptions (i.e. you don't need to evaluate the fit or prediction), you just have to interpret the results (relevance / significance of estimations and so on). &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-02-13T21:06:37.223" Id="86496" LastActivityDate="2014-02-13T21:06:37.223" OwnerUserId="39838" ParentId="86475" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have the following Cox Model: $$g(t,d,x ,\beta) = \ln[h_{0}(t)]+d_{1}\beta_{1}+d_{2}\beta_{2} + d_{1}d_{2} \beta_{3}$$ where $d_1$ and $d_2$ are binary variables that take $1$ or $0$. What is the difference between the following hazard ratios:&#10;$$\exp(\beta_{1}+\beta_3)$$&#10;$$ \exp(\beta_1+\beta_2 +\beta_3)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we are interested in comparing $d_1 = 1$ vs. $d_1=0$ given that $d_2=1$, why is $\exp(\beta_1+\beta_3)$ the correct hazard ratio? What happens to the $d_{2}\beta_{2}$ term?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-13T21:19:41.373" Id="86500" LastActivityDate="2014-04-19T00:40:32.077" LastEditDate="2014-02-14T01:59:37.957" LastEditorUserId="88" OwnerUserId="40122" PostTypeId="1" Score="0" Tags="&lt;survival&gt;" Title="Interaction term in survival analysis" ViewCount="37" />
  
  
  
  
  <row Body="&lt;p&gt;Note that t.test is for a difference of means, when you actually want to test for a difference of variances based on the null and alternative hypotheses you set up. See:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;?var.test&#10;var.test(x, y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-02-14T02:50:39.197" Id="86531" LastActivityDate="2014-02-14T02:50:39.197" OwnerUserId="21654" ParentId="86530" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You do neither a T-test nor a $\chi^{2}$ test when testing $H_0: \sigma^{2}_X = \sigma^2_Y$ against $H_a: \sigma^{2}_X \neq \sigma^2_Y$. For testing the equality of variances between two normally distributed populations you use the &lt;a href=&quot;http://en.wikipedia.org/wiki/F-test_of_equality_of_variances&quot;&gt;F-test of equality of variances&lt;/a&gt;, which reformulates your test as $H_0: \frac{\sigma^{2}_X}{\sigma^2_Y} = 1$ against $H_a: \frac{\sigma^{2}_X}{\sigma^2_Y} \neq 1$. In R, you should run &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; X=c( 11.4, 9.7, 11.4, 13.3, 7.4, 8.5, 13.4, 17.4, 12.7)&#10;&amp;gt; Y=c(3.2, 2.7, 5.5, -0.9, -1.8)&#10;&amp;gt; var.test(x,y)&#10;&#10;F test to compare two variances&#10;&#10;data:  X and Y&#10;F = 0.979, num df = 8, denom df = 4, p-value = 0.9033&#10;alternative hypothesis: true ratio of variances is not equal to 1&#10;95 percent confidence interval:&#10;0.109 4.947&#10;sample estimates:&#10;ratio of variances &#10;         0.979&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-02-14T02:56:41.703" Id="86534" LastActivityDate="2014-02-14T02:56:41.703" OwnerUserId="19788" ParentId="86530" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="86553" AnswerCount="2" Body="&lt;p&gt;I have a toy dataset and want to eval the AUC - ROC with bootstrap&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    set.seed(117)&#10;    test &amp;lt;- c(rnorm(70, 40, 10), rnorm(30, 55, 10))&#10;    dx &amp;lt;- c(rep(0, 65), rep(1,35) )&#10;    df &amp;lt;- data.frame(cbind(test, dx)) &#10;    r &amp;lt;- 1000&#10;    boot.f &amp;lt;- function(d, i){&#10;      data &amp;lt;- d[i,]&#10;      p &amp;lt;- unlist(subset(data, data$dx==1)[&quot;test&quot;])&#10;      n &amp;lt;- unlist(subset(data, data$dx==0)[&quot;test&quot;])&#10;      mean(sample(p, r, replace=T) &amp;gt; sample(n, r, replace=T))&#10;    } &#10;    roc.boot&amp;lt;-boot(df, boot.f, r) &#10;    roc.boot &#10;    boot.ci(roc.boot, type=&quot;bca&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It seems that bootstrapping needs two samplings (in &lt;code&gt;mean&lt;/code&gt; and in &lt;code&gt;boot&lt;/code&gt;). Is there a more efficient way? I'd like to obtain a &lt;code&gt;boot&lt;/code&gt; object, so prefer to use the function.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-14T03:04:02.263" FavoriteCount="1" Id="86535" LastActivityDate="2014-02-14T16:23:04.233" OwnerUserId="20657" PostTypeId="1" Score="1" Tags="&lt;bootstrap&gt;&lt;roc&gt;" Title="Bootstrap to evaluate variance of AUC ROC" ViewCount="362" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;The only thing that came to mind was using $g(x)=1$  No better distributions come to mind that are easy to sample from. Is this a good option?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&quot;Good&quot; is relative. Simulation often requires large samples, so you may have to balance ease of calculation and rejection rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's a good option if your rejection rate's not too high (I wouldn't use it with $\pi(x)=100x^{99}$, for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;As for better choices, it depends on what you know:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;are you familiar with the Probability Integral Transform approach to generating random numbers? If so, generating from $g(x)=\alpha x^{\alpha-1}$ might be fast enough (if you can compute roots reasonably quickly) and likely to work well (in terms of a good $c$) for suitable $\alpha$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;do you know any way to generate from a triangular distribution ($\pi(x) = kx$, say)? Then you might be slightly better off using that (there's a way to organize it that you can get two triangular values for every two uniforms you generate, for example). It also depends on whether you require independent values, or whether say some negative dependence is acceptable (or even, in some cases, desirable).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;That said, the uniform will work reasonably well for this example, and I wouldn't ignore it simply because there are 'closer' $g$'s. Depending on the relative speed of various computations on your machine it may actually be faster to just use the uniform.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also two conceptual things - what makes a distribution easy to sample from?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Knowing a method by which to sample from it. There are &lt;em&gt;so many&lt;/em&gt; sampling algorithms that it's really just limited to what you know, and some particulars of what the sampling situation is (not just $\pi$, but for example whether you are sampling many values at once, or one at a time, whether this is something you - or someone else - will be using again and again, or just on this one thing, and much else besides).&lt;/p&gt;&#10;&#10;&lt;p&gt;[As an illustration of the various ways in which something might be 'easy', one can, for example, construct several different kinds of adaptive forms of rejection sampling that construct simple upper and lower envelopes (&quot;g&quot;-type bounds) on $\pi$ &lt;em&gt;on the fly&lt;/em&gt;, in such a way that if you sample a lot, your average number of evaluations of $\pi$ per random variate obtained is quite small. This can lead to very efficient ways to sample from $\pi$s that are very difficult/expensive to evaluate - as long as you need large enough samples to build up good envelopes.]&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Why do we want to pick c as small as possible. Why not use c=50? Is this for algorithm efficiency? i.e. more of the randomly drawn samples from cg(x) will fall under the curve of π(x)?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes, for a given $g$, the larger $c$ is the more often you need to sample from $g$ to get one value from $\pi$ (in fact, $c$ times on average).&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, even with different competing $g$'s, small $c$ is still desirable, but there's a tradeoff between how fast it is to compute a 'better' $g$ (one with a smaller $c$) and how fast it is to sample from an 'easy' $g$. A uniform $g$ is the easiest of all, of course.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;All things considered, I think your idea of trying the uniform is likely to be reasonable - an acceptance rate of 1/3.75 isn't high but it's not too bad and the calculations will be both fast and simple.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-14T03:55:35.893" Id="86541" LastActivityDate="2014-02-14T04:45:08.447" LastEditDate="2014-02-14T04:45:08.447" LastEditorUserId="805" OwnerUserId="805" ParentId="86533" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  <row AcceptedAnswerId="86606" AnswerCount="1" Body="&lt;p&gt;I am trying to understand what makes estimating the posterior distribution such a hard problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, imagine I need to estimate the posterior distribution over a set of parameters given the data y, so a quantity $P(\theta|y)$ and $\theta$ is generally high dimensional.&lt;/p&gt;&#10;&#10;&lt;p&gt;The prior over $\theta$ is a multivariate Gaussian i.e. $P(\theta) \sim N(\theta; 0, \Sigma)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;The likelihood i.e. $P(y|\theta)$ can be written down as product over Gaussian likelihoods.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, it seems to be that the posterior distribution will also be Gaussian. Is that correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, going through Bishop's book, it seems that the conditional posterior distributions and the marginal distributions will be Gaussian as well (assuming that the joint distribution over the parameters and data is Gaussian) and should have a closed form solution. If that is the case, why is this problem intractable? &lt;/p&gt;&#10;&#10;&lt;p&gt;If I need to find the parameters of this posterior distribution, can this not be set as an optimisation problem where I estimate the mean and covariance of the posterior Gaussian? I am basically having trouble visualising why this problem is complicated?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-02-14T13:59:35.907" Id="86595" LastActivityDate="2014-02-14T15:37:49.663" OwnerUserId="36540" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;normal-distribution&gt;&lt;optimization&gt;&lt;basic-concepts&gt;&lt;posterior&gt;" Title="On the tractability of posterior distributions" ViewCount="109" />
  
  
  <row Body="&lt;p&gt;In general, we're referring to the distribution of the &lt;em&gt;residuals&lt;/em&gt; in these models. The distribution of $X$ in a linear regression or ANOVA model may be highly irregular, or sampled in a sequential fashion (e.g. a stratified sample of 10 people within each age group defined as 10-19, 20-29, ...). When we fit such models, we think of the $Y$s as being &lt;em&gt;conditional upon&lt;/em&gt; the $X$s.&lt;/p&gt;&#10;&#10;&lt;p&gt;Homogeneity means a lack of mean variance relationship. This is assessed with the usual residual vs fitted plot to examine whether there is a trend. It seems you've already grasped this.&lt;/p&gt;&#10;&#10;&lt;p&gt;These &quot;assumptions&quot; are overplayed in applied courses, though. The only reason why we care about the normality of residuals in such models is because, when the residuals are found to be normally distributed, we can think of the tests as being obtained from a maximum likelihood estimate of the correlation/LS slope/group means.&lt;/p&gt;&#10;&#10;&lt;p&gt;When residuals &lt;em&gt;aren't&lt;/em&gt; normal, we can rely on results from the Central Limit Theorem to tell us that the sampling distribution of the mean has an approximate normal distribution for even modest sample sizes. Even when sample sizes are &lt;em&gt;very&lt;/em&gt; small, the only problem that we have with non-normality is that there is less power to detect a difference in means. If you obtain significant results in such cases, this hardly means that such results are &lt;em&gt;wrong&lt;/em&gt;. The basic ANOVA, t-test, and linear regression models are calibrated at all sample sizes and distributions (i.e. the test is of accurate size: 0.05 cut off for p implies that 5% spurious false positives when generating data from the null hypothesis). &lt;/p&gt;&#10;&#10;&lt;p&gt;That's not to say that we wouldn't care about &lt;em&gt;high leverage/high influence&lt;/em&gt; points as being obtained from an apparently skewed distribution of residuals. We might observe one or more points whose values are very inconsistent with the spread expected in a normal distribution. About 99% of the time, I leave such points in the analysis, but carefully describe what they are and how they impact the results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-14T14:35:42.780" Id="86599" LastActivityDate="2014-02-14T14:35:42.780" OwnerUserId="8013" ParentId="86597" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Yes the posterior will be Gaussian, because normal priors are conjugate with normal likelihoods that you probably know. I will ask you  to go back to your book and verify the meaning of intractability, because though the posterior has a known form (multivariate normal), sampling from the multivariate normal distribution requires MCMC techniques, which may simply be why the say it is intractable. In any case i will want to know what intractability means in your case.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-14T15:37:49.663" Id="86606" LastActivityDate="2014-02-14T15:37:49.663" OwnerUserId="31261" ParentId="86595" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Given $X\sim N(0,\Omega)$. Suppose that we can construct a sequence $\{X_n\}$ based on the observation such that $\{X_n\}\to X$ in distribution. My problem is to estimate $\Omega$ consistently using this sequence.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\operatorname{var} X_n$ converges to a &quot;finite&quot; matrix, then of course $\operatorname{var}X_n$ is the estimator that we want using the dominated convergence theorem. But, as we know $\operatorname{var} X_n$ can go to infinity, so in this case, what should I do? Any help will be appreciated. Thanks&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2014-02-14T16:31:03.317" FavoriteCount="2" Id="86611" LastActivityDate="2014-02-20T12:15:53.490" LastEditDate="2014-02-20T12:15:53.490" LastEditorUserId="14675" OwnerUserId="40104" PostTypeId="1" Score="3" Tags="&lt;estimation&gt;&lt;multivariate-analysis&gt;&lt;random-variable&gt;&lt;convergence&gt;" Title="Estimating variance from sequence of random variable" ViewCount="115" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have an assignment to implement the &lt;a href=&quot;http://en.wikipedia.org/wiki/Adaptive_resonance_theory&quot; rel=&quot;nofollow&quot;&gt;adaptive resonance theory&lt;/a&gt; (ART) type network (as part of a bigger project). I have red a lot of Internet resources on the topic and I think I've got the essence of it, but I am not sure. So far I have the following findings and questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The ART1 network has two layers (input and output) which are fully connected in both directions.&lt;/li&gt;&#10;&lt;li&gt;The input passes through the bottom-up connections, and the output neuron with highest value is the winner. Than the input is somehow compared against the prototype stored in the top-down connection weights (not sure how this is done, does the out-value of the winning output neuron pass through the top-down connections or not, it looks to me that the top-down connections are not really proper connections by definition)&lt;/li&gt;&#10;&lt;li&gt;Does the ART1 network have a learning phase, and operating phase (like a multi-layer perceptron) or is it adjusting its weights constantly and it is up to the user to decide when is the &quot;learning&quot; phase over?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-02-14T18:25:26.427" Id="86619" LastActivityDate="2014-02-14T18:48:57.877" LastEditDate="2014-02-14T18:48:57.877" LastEditorUserId="7290" OwnerUserId="40270" PostTypeId="1" Score="1" Tags="&lt;neural-networks&gt;" Title="ART neural network disambiguation" ViewCount="10" />
  
  <row Body="&lt;p&gt;It is hard to answer without knowing more about what &lt;code&gt;mod&lt;/code&gt; is.  That is why we suggest a reproducible example.&lt;/p&gt;&#10;&#10;&lt;p&gt;If &lt;code&gt;mod&lt;/code&gt; is a glm fit with a 'gaussian' family (the default) then it is just a linear model and you can use &lt;code&gt;predict.lm&lt;/code&gt; instead which has the &lt;code&gt;interval&lt;/code&gt; argument that can be set to &quot;prediction&quot; to compute prediction intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;If &lt;code&gt;mod&lt;/code&gt; is a glm fit with a non-Gaussian family then the concept of a standard error of prediction may not even make sense (what is the prediction interval when the predictions are all TRUE/FALSE?).  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you can give more detail (a reproducible example and a clear statement of what you want) then we will have a better chance of giving a useful answer.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-14T19:49:57.403" Id="86626" LastActivityDate="2014-02-14T19:49:57.403" OwnerUserId="4505" ParentId="86624" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have the following question to answer:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The total thickness X of the four pads of 36 half-ring (H-R) mounts&#10;  for aircraft engines, taken periodically from the production line,&#10;  were found to be as shown below. Determine whether the total number of&#10;  runs above and below the median of this sequence of values of X is&#10;  significantly different from its expected value at the 5% level of&#10;  significance, under the hypothesis that X is under statistical&#10;  control. &lt;img src=&quot;http://i.stack.imgur.com/We6N2.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I believe this involves Chi Square but I honestly don't know where to begin.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-14T21:38:53.737" Id="86639" LastActivityDate="2014-02-14T22:19:49.173" LastEditDate="2014-02-14T22:19:49.173" LastEditorUserId="24808" OwnerUserId="40284" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;statistical-significance&gt;&lt;median&gt;" Title="Determine number of runs is significant at 5% level of significance" ViewCount="69" />
  <row Body="&lt;p&gt;Not in any usual way; the chi-square test (at least, as that is commonly meant) is for categorical variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please tell us what you are trying to do.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-14T22:37:30.367" Id="86644" LastActivityDate="2014-02-14T22:37:30.367" OwnerUserId="686" ParentId="86632" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I would explain it saying that sometimes I need things predicted. For instance, the price of a house given some information about it. Say, its size, location, how old the construction is, etc. I want to factor that into a model that takes into account the influence of these factors to predict the price. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now taking a sub-example, lets say, I consider only the size of the house. That would imply that nothing else affects the price. It could be a case where I am comparing houses which are in the same locality, were constructed around the same time etc. Or it could be that I don't want to complicate matters for myself and hence want the real life to conform to how far I can think. Moving on, I make a model where I have a list of sizes and corresponding prices of similar properties (say, from sales that have been happening recently... but that would have serious bias from houses that are not for sale and hence affect price of houses that are. but lets ignore that).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I see that a 100 sq feet house costs $1m(get over yourself, this is a simplified example). So, naturally you would expect a 200sq feet house to cost double. And that is what we would call a &quot;linear pattern&quot;. Of course when we collect the data and plot size vs price, we see that it is not exactly double. But there is definitely an increasing trend. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I try to quantify the trend. How much increase for every increased sq foot? That is linear regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;INSERT terminology map and continue with statistical concepts. One way of explaining random and systematic component could be that whatever you forgot to model, or couldn't possibly gauge, is random. Whatever you could is systematic. (For instance, say it is 2008 and you want to sell a house.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Assumptions that underlie this model are that the scatterplot should look like a rod. Which is that Both X and Y are &quot;Normal&quot;. and all have similar variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;If that is not the case, enter GLM. and now explain link function n all that. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is simplified, but it should work as an introduction. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can put in history of GLMs and factorial models. Where Fisher required things to start varying together and this framework was suitable for that kind of complexity. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-14T22:59:52.343" Id="86646" LastActivityDate="2014-02-14T23:03:47.197" LastEditDate="2014-02-14T23:03:47.197" LastEditorUserId="7290" OwnerUserId="40291" ParentId="27651" PostTypeId="2" Score="-2" />
  <row Body="&lt;p&gt;Your ANOVA was significant, implying you either made a Type I error or the means are not all equal (in which case the null is false).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the chance of making a Type I error was (presumably) set fairly low, the second option becomes a relatively plausible explanation for the size of the test statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;In that sense, the research hypothesis you stated is indicated.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, your multiple comparisons were unable to clearly identify any specific 'cause' of that difference - likely there are several small effects that are enough for yout to conclude there's a difference, even though none alone are large enough to 'stand out' by themselves for you to say &quot;this pair of groups differ on X&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Such a thing happens not infrequently, especially when samples size calculations are based on only just achieving a moderate power at some overall effect size. If the effect sizes are all a little smaller than that, you may be unlikely to find them.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: To address the specific phrasing of the research hypothesis being 'partially accepted' -&lt;/p&gt;&#10;&#10;&lt;p&gt;It depends on what you mean by &quot;correct&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;I&lt;/em&gt; would not use such a phrase - either accepting the alternative or 'partial' in reference to it. You rejected the null, and there was nothing partial about that. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the important thing is to convey exactly what null was rejected. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd also draw clear displays of means and (ANOVA-based) standard errors of the mean (likely along with the raw data on the same display) in order that the effect sizes relative to the uncertainty was clear to the readership.&lt;/p&gt;&#10;&#10;&lt;p&gt;I certainly have never used such phrasing and don't imagine I ever will, but that doesn't make it objectively &lt;em&gt;wrong&lt;/em&gt;. What matters most is that the audience of such a phrase clearly understand the intended meaning.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-02-15T02:30:26.443" Id="86658" LastActivityDate="2014-02-15T02:58:38.640" LastEditDate="2014-02-15T02:58:38.640" LastEditorUserId="805" OwnerUserId="805" ParentId="86656" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You have two sources of uncertainty: the uncertainty in the historical data, and the uncertainty in producing the forecasts given the historical data. The simulation distribution of point forecasts is capturing the uncertainty in the historical data only. &lt;/p&gt;&#10;&#10;&lt;p&gt;To capture the joint uncertainty, I suggest you simulate a future value from the forecast distribution for each of the synthetic time series. That is, for each synthetic time series compute the point forecast and the forecast variance, and then simulate a value from this distribution. These simulated future values then include the uncertainty in both the forecast distribution and in the historical data.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could compute a prediction interval from the percentiles of the future values and compare its width with the size of the prediction interval produced for each synthetic series. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-15T08:00:45.107" Id="86666" LastActivityDate="2014-02-15T08:00:45.107" OwnerUserId="159" ParentId="86665" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Tukey multiple comparisons control for multiplicity in the strong sense, meaning they control for every paired comparison simultaneously. Assuming there are no differences, then the probability of any Tukey comparison rejecting H0 is 5% (with 5% significance level). Therefore the Tukey test can be performed directly, without conducting the overall ANOVA F test first. On the other hand, the overall F test from ANOVA controls type I error in the weak sense, meaning it only controls for the overall hypothesis. &#10;Strong control is recommended over weak control, since most of the time we are interested in the specific paired hypotheses in practice, so you could adopt a practice of only doing the Tukey tests. &lt;/p&gt;&#10;&#10;&lt;p&gt;Rejecting the overall test implies rejection of the overall null that all means are equal. That is the only conclusion that can be made this this test. So you can claim there are differences, but you do not have enough evidence to say which &lt;em&gt;specific&lt;/em&gt; ones are different without risking higher Type I error rate due to the multiplicity involved.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-15T13:25:54.573" Id="86672" LastActivityDate="2014-02-15T13:25:54.573" OwnerUserId="13586" ParentId="86656" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am struggling at the moment with how to determine links between different sets of variables. I have data on land use/cover changes in a number of regions in a period of 20 years. They are all expressed in percentages of area surface changed. I have data on so called drivers of change (population change, employment levels, education structure changes, slope, altitude etc) also expressed in percentages or indices. I have a third type of variables - a questionnaire survey.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically what I did was multiple regression analysis on each independent variable (land cover change - forest change, grassland expansion or reduction, arable land expansion or reduction etc.) with all of the drivers of change. So I would get R for the connection between e.g. deforestation on one side and population change and altitude on the other (other drivers were eliminated through backward stepwise regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;So after I would determine that forest cover change is affected by changes in population number or altitude, I would use data from the questionnaire survey to interpret the reasons for such connection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any better way to model these variables? I am dabbling into canonical correlation - could it be useful? Land cover variables on one side, and the drivers on the other? And could I include the data from the questionnaire survey in the canonical correlation in any way? Thank you very much for your help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-15T13:39:04.353" Id="86674" LastActivityDate="2014-02-15T14:21:58.950" OwnerUserId="40315" PostTypeId="1" Score="2" Tags="&lt;multiple-regression&gt;&lt;survey&gt;&lt;canonical-correlation&gt;" Title="Statistical modeling of land use - 3 types of variables" ViewCount="118" />
  
  <row Body="&lt;p&gt;I've never heard that and, to me, it makes little sense. &lt;/p&gt;&#10;&#10;&lt;p&gt;First there are all the usual problems with p values and cutoffs. But, if anything, I think the p value cutoff for interactions ought to be more lenient than that for main effects, since they can be harder to find if either or both main effects are measured less than perfectly reliably. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-15T16:26:40.590" Id="86684" LastActivityDate="2014-02-15T16:26:40.590" OwnerUserId="686" ParentId="86683" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;When referring to Box-Cox transformations there are really 2 concepts that look like they are being mixed up.  The first is what the original paper was about, the methodology of finding a transformation within a family of transformations that gives the &quot;best&quot; transformation assuming the truth results in normal residuals with equal variance and a linear relationship.  This is what you already did with the response (dependent) variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the paper Box and Cox presented a family of transformations (actually the paper has more than 1, but variations on the main one are what people mainly refer to), this family of transformations (or variations on it) is also often called the Box-Cox transformations.  If you want to apply one of this family of transformations to a predictor variable, then just plug the variable into the formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;To determine what types of transformations to apply to predictor variables, the most important thing to use is knowledge about your data and the science behind it.  How do you expect weight to change with age, height, or income?  (I would be surprised if a Box-Cox transform is the best for any of these).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are tools like spline fits, ACE, AVAS, and others that can suggest transformations, but you need to use knowledge and common sense to convert these into meaningful transformations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-15T17:55:00.867" Id="86687" LastActivityDate="2014-02-15T17:55:00.867" OwnerUserId="4505" ParentId="86681" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;@EngrStudent illuminating answer tells us that we should expect &lt;strong&gt;&lt;em&gt;different results&lt;/em&gt;&lt;/strong&gt; when the distribution is &lt;strong&gt;&lt;em&gt;continuous&lt;/em&gt;&lt;/strong&gt;, and when it is &lt;strong&gt;&lt;em&gt;discrete&lt;/em&gt;&lt;/strong&gt; (the &quot;red&quot; graphs, where the asymptotic distribution of the sample median fails spectacularly to look like normal, correspond to the distributions Binomial(3), Geometric(11), Hypergeometric(12), Negative Binomial(14), Poisson(18), Discrete Uniform(22).  &lt;/p&gt;&#10;&#10;&lt;p&gt;And indeed this is the case. When the distribution is discrete, things get complicated. I will provide the proof for the Absolutely Continuous Case, essentially doing no more than detailing the answer already given by @Glen_b, and then I will discuss a bit what happens when the distribution is discrete, providing also a recent reference for anyone interested in diving in.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;ABSOLUTELY CONTINUOUS DISTRIBUTION&lt;/strong&gt;&lt;br&gt;&#10;Consider a collection of i.i.d. absolutely continuous random variables $\{X_1,...X_n\}$ with distribution function (cdf) $F_X(x) = P(X_i\le x)$ and density function $F'_X(x)=f_X(x)$. Define $Z_i\equiv I\{X_i\le x\}$ where $I\{\}$ is the indicator function. Therefore $Z_i$ is a Bernoulli r.v., with &#10;$$E(Z_i) =  E\left(I\{X_i\le x\}\right) = P(X_i\le x)=F_X(x),\;\; \text{Var}(Z_i) = F_X(x)[1-F_X(x)],\;\; \forall i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y_n(x)$ be the sample mean of these i.i.d. Bernoullis, defined for fixed $x$ as&#10;$$Y_n(x) =  \frac 1n\sum_{i=1}^nZ_i$$&#10;which means that&#10;$$E[Y_n(x)] = F_X(x),\;\; \text{Var}(Y_n(x)) = (1/n)F_X(x)[1-F_X(x)]$$&#10;The Central Limit Theorem applies and we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\Big(Y_n(x) - F_X(x)\Big) \rightarrow_d \mathbb N\left(0,F_X(x)[1-F_X(x)]\right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that $Y_n(x) = \hat F_n(x)$ i.e. non else than the empirical distribution function.&#10;By applying the &quot;Delta Method&quot; we have that for a continuous and differentiable function $g(t)$ with non-zero derivative $g'(t)$ at the point of interest, we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\Big(g[\hat F_n(x)] - g[F_X(x)]\Big) \rightarrow_d \mathbb N\left(0,F_X(x)[1-F_X(x)]\cdot\left(g'[F_X(x)]\right)^2\right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, choose $g(t) \equiv F^{-1}_X(t),\;\; t\in (0,1)$ where $^{-1}$ denotes the inverse function. This is a continuous and differentiable function (since  $F_X(x)$ is), and by the Inverse Function Theorem we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$g'(t)=\frac {d}{dt}F^{-1}_X(t) = \frac 1{f_x\left(F^{-1}_X(t)\right)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Inserting these results on $g$ in the delta-method derived asymptotic result we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\Big(F^{-1}_X(\hat F_n(x)) - F^{-1}_X(F_X(x))\Big) \rightarrow_d \mathbb N\left(0,\frac {F_X(x)[1-F_X(x)]}{\left[f_x\left(F^{-1}_X(F_X(x))\right)\right]^2} \right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and simplifying,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\Big(F^{-1}_X(\hat F_n(x)) - x\Big) \rightarrow_d \mathbb N\left(0,\frac {F_X(x)[1-F_X(x)]}{\left[f_x(x)\right]^2} \right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;.. for any fixed $x$. Now set $x=m$, the (true) median of the population. Then we have $F_X(m) = 1/2$ and the above general result becomes, for our case of interest,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\Big(F^{-1}_X(\hat F_n(m)) - m\Big) \rightarrow_d \mathbb N\left(0,\frac {1}{\left[2f_x(m)\right]^2} \right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;But $F^{-1}_X(\hat F_n(m))$ is (or converges strongly to, I forget that part) the sample median, $\hat m$. So we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\Big(\hat m - m\Big) \rightarrow_d \mathbb N\left(0,\frac {1}{\left[2f_x(m)\right]^2} \right) $$&#10;which is the Central Limit Theorem for the sample median for absolutely continuous distributions.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;DISCRETE DISTRIBUTIONS&lt;/strong&gt;&lt;br&gt;&#10;When the distribution is discrete (or when the sample contains ties) it has been argued that &lt;em&gt;the &quot;classical&quot; definition of sample quantiles, and hence of the median also, may be misleading in the first place&lt;/em&gt;, as the theoretical concept to be used in order to measure what one attempts to measure by quantiles.&lt;br&gt;&#10;In any case it has been simulated that under this classical  definition (the one we all know), the asymptotic distribution of the sample median is non-normal and a discrete distribution.  &lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative definition of sample quantiles is by using the concept of the &quot;mid-distribution&quot; function, which is defined as &#10;$$F_{mid}(x) = P(X\le x) - \frac 12P(X=x)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The definition of sample quantiles through the concept of mid-distribution function can be seen as a generalization that can cover as special cases the continuous distributions, but also, the not-so-continuous ones too.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the case of discrete distributions, among other results, it has been found that the sample median as defined through this concept has an asymptotically normal distribution with an ...elaborate looking variance.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Most of these are recent results. The reference is &lt;a href=&quot;http://link.springer.com/content/pdf/10.1007/s10463-008-0215-z.pdf&quot;&gt;Ma, Y., Genton, M. G., &amp;amp; Parzen, E. (2011). Asymptotic properties of sample quantiles of discrete distributions. Annals of the Institute of Statistical Mathematics, 63(2), 227-243.&lt;/a&gt;, where one can find a discussion and links to the older relevant literature.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-16T02:11:00.903" Id="86725" LastActivityDate="2014-02-16T02:11:00.903" OwnerUserId="28746" ParentId="45124" PostTypeId="2" Score="8" />
  <row AnswerCount="0" Body="&lt;p&gt;I am about to receive a dataset for multivariate regression where we are trying to find $X\in\Re^{m\times n}$ such that it best fits the loss function $$\|AX-B\|_F^2$$ where $A\in \Re^{l\times m},B\in\Re^{l\times n}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For some other purposes, what I was wondering was, what are the typical proportions for $A,X,B$ in most datasets?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for ballpark estimates; I would assume $A$ is usually very tall but how many columns would it have? How many columns would $B$ have?&lt;/p&gt;&#10;&#10;&lt;p&gt;If there are any freely available datasets for me to play with, if you can, please give link. I was not able to find any purely numerical datasets on the UCI ML repository. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-16T03:02:05.650" FavoriteCount="1" Id="86728" LastActivityDate="2014-02-16T03:35:22.723" LastEditDate="2014-02-16T03:35:22.723" LastEditorUserId="32036" OwnerUserId="40341" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;" Title="What are the typical scales of multivariate regression?" ViewCount="28" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider a finite set $A$. Let the sample space be $A\times A$. We have an unknown probability distribution $f$ on this sample space. Now this probability distribution has a &quot;blocky&quot; property, which I am going to explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let a block be a function (not necessarily probability distribution) over the sample space $b_{(B,C,r)}$ where $B,C\subset A$ and $r$ be any real number (not necessarily in $[0,1]$). Then $b_{(B,C,r)}(x,y)=r$ if $(x,y)\in B\times C$ and $b_{(B,C,r)}(x,y)=0$ if not. Then $f$ can be written as a finite sum of those blocks (at most $|A|^{2}$ of them in fact). Let $X$ be the minimum number of blocks needed to sum to $f$. If prior to observing any trials, we know that $X$ follows a geometric distribution with unknown $p$ truncated to $[1,|A|^{2}]$ then we say that $f$ is &quot;blocky&quot;. (Note that despite the analogy to reconstructing a picture, a block need not be a rectangle: there is no relationship among elements in $A$)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we are given the outcome of a number of trials, where each trial consists of picking an element in the sample space with probability given by that unknown but &quot;blocky&quot; probability distribution $f$. The task is to make estimate of $f$ of each possible outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what is a good algorithm for that?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;I'm sorry that I am new to statistics and thus I was not able to phrase this in a more proper way, and I do not even know if this problem has a solution or not, so it might be trivial or unsolved for all I know. Hope I can get some help on this. Thank you for any help you can give.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-16T04:18:10.933" Id="86733" LastActivityDate="2014-02-16T05:01:55.033" LastEditDate="2014-02-16T05:01:55.033" LastEditorUserId="805" OwnerUserId="40346" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;bayesian&gt;&lt;estimation&gt;&lt;algorithms&gt;" Title="Reconstruct a &quot;blocky&quot; picture?" ViewCount="50" />
  
  <row Body="&lt;p&gt;An AR can be inverted to produce an infinite MA.&lt;/p&gt;&#10;&#10;&lt;p&gt;Take an autoregressive model of order $p$, which &lt;a href=&quot;http://en.wikipedia.org/wiki/Autoregressive_model#Definition&quot; rel=&quot;nofollow&quot;&gt;by definition&lt;/a&gt; is of the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{t}=\mu_0+\phi_1X_{t-1}+\phi_2X_{t-2}+\ldots+\phi_pX_{t-p}+\varepsilon _{t}\,$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\,\,=\mu_0+\phi_1B^1X_{t}+\phi_2B^2X_{t}+\ldots+\phi_pB^pX_{t}+\varepsilon _{t}\,$, where $B$ is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Lag_operator#Lag_polynomials&quot; rel=&quot;nofollow&quot;&gt;backshift operator&lt;/a&gt;, so  &lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{t}-\phi_1B^1X_{t}-\phi_2B^2X_{t}-\ldots-\phi_pB^pX_{t}=\mu_0+\varepsilon _{t}\,$, or&lt;/p&gt;&#10;&#10;&lt;p&gt;$\phi(B) X_t = \mu_0 + \varepsilon_t$, where $\phi_p(B) = 1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $X_t = \phi(B)^{-1}\mu_0 +  \phi(B)^{-1} \varepsilon_t$, which is an infinite MA.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-16T04:49:44.370" Id="86737" LastActivityDate="2014-02-16T07:42:06.820" LastEditDate="2014-02-16T07:42:06.820" LastEditorUserId="805" OwnerUserId="805" ParentId="86729" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;So I have a set of 9 x,y values, and I need to find the gradient/slope of the data, AND its associated error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without the error, I would've used Excels LINEST function, but as the errors in my y values are large (no x value errors) I need LINEST to take these into account.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would I do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-16T11:40:29.683" Id="86756" LastActivityDate="2014-02-16T11:40:29.683" OwnerUserId="40366" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;error&gt;&lt;uncertainty&gt;" Title="Uncertainty in gradient of data" ViewCount="43" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a data sample of ~5000 observations, ~700 predictors and 2 classes. I've built a classification model based on RF with 500 trees using randomForest R library. Than I've estimated the performance based on a hold-out sample (I understand it's better to use the out-of-bag, but I'm going to combine this with another model going forward). The mean forecast probability of the first class is 10.98%, whereas the real probability on the hold-out sample is 8.64%. KS is 17.6%.&#10;Then I've also tried a single CART tree for a comparison, and the performance of this tree appeared to be better booth in KS and mean probabilities (KS=21%, mean prob=9.56%)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a logical explanation why a single decision tree could be so much better than a RF? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-16T13:10:19.023" Id="86762" LastActivityDate="2014-02-16T13:10:19.023" OwnerUserId="31158" PostTypeId="1" Score="1" Tags="&lt;random-forest&gt;&lt;cart&gt;" Title="Random forest performs worse than single CART tree?" ViewCount="143" />
  
  
  <row Body="&lt;p&gt;For a layperson, the most salient feature is that as you buy more tickets, your chances increase in a better-than-linear fashion. You could say, &quot;we square the number of tickets you buy. That means if you buy 2 tickets instead of 1, your chances of winning are quadrupled. If you buy 4 tickets, your chances are 16 times higher!&quot; You could illustrate this with some pictures of squares with sides of 1, 2, and 4 units, broken down into unit squares. It's a geometric progression, though I think laypeople tend to (incorrectly) call it &quot;exponential&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-16T17:23:04.083" Id="86781" LastActivityDate="2014-02-16T17:23:04.083" OwnerUserId="9816" ParentId="86771" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="86797" AnswerCount="1" Body="&lt;p&gt;let $X,Y$ are two independent random variables Bernoulli with probability $P$. how can calculate $E(X^Y|X+Y=1)$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-16T18:37:41.443" Id="86790" LastActivityDate="2014-02-18T16:09:25.923" LastEditDate="2014-02-17T18:44:49.843" LastEditorUserId="919" OwnerUserId="10286" PostTypeId="1" Score="1" Tags="&lt;expected-value&gt;" Title="how can calculate $E(X^Y|X+Y=1)$" ViewCount="149" />
  <row AnswerCount="1" Body="&lt;p&gt;Having come across &lt;a href=&quot;http://stats.stackexchange.com/questions/59177/how-to-calculate-confidence-intervals-of-1-sqrtx-transformed-data-after-run&quot;&gt;this discussion&lt;/a&gt; I'm raising the question on the back-transformed confidence intervals conventions.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to &lt;a href=&quot;http://www.amstat.org/publications/jse/v13n1/olsson.html&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt; the nominal coverage back-transformed CI for the mean of a log-normal random variable is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ UCL(X)= \exp\left(Y+\frac{\text{var}(Y)}{2}+z\sqrt{\frac{\text{var}(Y)}{n}+\frac{\text{var}(Y)^2}{2(n-1)}}\right)$&#10;$\ LCL(X)= \exp\left(Y+\frac{\text{var}(Y)}{2}-z\sqrt{\frac{\text{var}(Y)}{n}+\frac{\text{var}(Y)^2}{2(n-1)}}\right)$&lt;/p&gt;&#10;&#10;&lt;p&gt;/and not the naive $\exp((Y)+z\sqrt{\text{var}(Y)})$/&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what are such CIs for the following transformations:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$\sqrt{x}$ and $x^{1/3}$&lt;/li&gt;&#10;&lt;li&gt;$\text{arcsin}(\sqrt{x})$&lt;/li&gt;&#10;&lt;li&gt;$\log(\frac{x}{1-x})$&lt;/li&gt;&#10;&lt;li&gt;$1/x$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;How about the tolerance interval for the random variable itself (I mean a single sample value randomly drawn from the population)? Is there the same issue with the back-transformed intervals, or will they have the nominal coverage?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-17T00:11:22.853" Id="86808" LastActivityDate="2014-07-29T15:58:37.183" LastEditDate="2014-05-19T12:33:24.817" LastEditorUserId="88" OwnerUserId="36545" PostTypeId="1" Score="7" Tags="&lt;confidence-interval&gt;&lt;data-transformation&gt;&lt;back-transformation&gt;" Title="Back-transformed confidence intervals" ViewCount="424" />
  
  
  
  
  
  <row Body="&lt;p&gt;Bit late to the party, but I think the chosen answer is wrong, so here's my reasoning:&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the confusion comes from using the &quot;a classifier&quot; with two different meanings:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The ROC of &quot;a classifier&quot; considers a function that yields a continuous score as output which indicates the class. The ROC is generated by varying a threshold above which the label of the positive class is assigned.   &lt;/li&gt;&#10;&lt;li&gt;On contrast, recall (or sensitivity or true positive rate TPR) implies that this threshold is fixed. So the recall of &quot;a classifier&quot; implies one further step: the dichotomization or hardening of the continuous output of &quot;a classifier&quot; in the ROC meaning.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The 2nd meaning classifier is one working point of a set of points (not entirely sure about term: the German term would be Schar) that together form the classifier in the first meaning, the parametrization being the threshold.&lt;/p&gt;&#10;&#10;&lt;p&gt;To generate the ROC, the threshold is varied from below the lowest observed score (&quot;start&quot;) to above the highest observed score (&quot;end&quot;). This makes the curve start at (TPR 1.0; TNR 0.0) and end at (TPR 0.0; TNR 1.0). Note that these two points correspond to trivital classifiers (2nd meaning) that always predict the positive and negative class, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;So &lt;strong&gt;each ROC contains at least one point&lt;/strong&gt; (and &quot;a classifier&quot; in the 2nd sense)  &lt;strong&gt;with recall 0&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a graph produced from 4 cases with &quot;predicted&quot; scores 1, 2, 3, 4 and the first 2 cases and &quot;reference&quot; classes negative, negative, positive, positive. If you look at classifiers (2nd meaning) with threshold anywhere &gt; 4, you end up at the red operating point which has recall 0, although the AUC is 1.0 for the classifier (1st meaning).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wQXoa.png&quot; alt=&quot;ROC&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So far my example may look artificially constructed. After all, why would one in practice choose a threshold outside the predicted range?&lt;br&gt;&#10;However here are some points that come to my mind how one could be faced with such a situation in practice.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;There are classifiers where the predicted scores actually have an interpretable meaning. Therefore, the threshold may be chosen according to some &quot;external&quot; principle. Consider a classifier like logistic regression which predicts class membership probability. Now consider a situation where you need to have rather &lt;em&gt;specific&lt;/em&gt; predictions. For example, you decided that you'll raise the hurdles a bit and yell &quot;positive&quot; only if the predicted probability is above 90 %. Now if for some reason the predicted probabilities never reach the 90 % mark, you end up with the situation in question. &lt;/p&gt;&#10;&#10;&lt;p&gt;The same would apply if the classifier was set up first as a chemical calibration, e.g. a quantitation of fasting blood glucose level. Prediction should be &quot;diabetic&quot; if the quantitation yields more than 126 mg/dl. Now assume that unfortunately, your &lt;a href=&quot;http://goldbook.iupac.org/R05206.html&quot;&gt;recovery&lt;/a&gt; is bad: it predicts systematically far too low and 126 mg/dl is never predicted. A similar situation would result if you looked up the official threshold as 126 mg/dl, but calibrated mmol/l (126 mg/dl = 7,0 mmol/l)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Number two is a classical programming error. Consider programming the ROC by actually looping over a series of increasing threshold values to construct the ROC, e.g. setting some &lt;code&gt;threshold&lt;/code&gt; variable accordigly in the loop. If then next step the recall is calculated using &lt;code&gt;threshold&lt;/code&gt; which is still set to a value outside the predicted score range, the recall will be 0 (or 1, if high scores predict the negative class).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## calculate and plot the ROC&#10;roc &amp;lt;- data.frame (&#10;         threshold = seq (min (score) - 1, max (score + 1), length.out = 100),&#10;         tpr = NA, fpr = NA&#10;       )&#10;&#10;for (i in 1 : 100) {&#10;   threshold &amp;lt;- roc$threshold [i]&#10;           roc$tpr [i] &amp;lt;- mean (score [class == &quot;positive&quot;] &amp;gt; threshold)&#10;   roc$fpr [i] &amp;lt;- mean (score [class == &quot;negative&quot;] &amp;gt; threshold )&#10;}&#10;&#10;plot (roc$fpr, roc$tpr, col = rainbow (100))&#10;&#10;## now the recall&#10;&#10;# **FORGET TO SET A SENSIBLE THRESHOLD VALUE**&#10;&#10;recall &amp;lt;-  mean (score [class == &quot;positive&quot;] &amp;gt; threshold)&#10;recall&#10;## [1] 0&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I used this R code to produce the graph:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require (ROCR)&#10;perf &amp;lt;- performance (prediction (1:4, 1:4 &amp;gt; 2.5), &quot;tpr&quot;, &quot;fpr&quot;)&#10;plot (perf, asp = 1)&#10;points (perf@x.values [[1]], perf@y.values[[1]], pch = 19, col = c(2, 1,1,1,1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-02-17T15:16:45.280" Id="86867" LastActivityDate="2014-02-17T22:47:11.207" LastEditDate="2014-02-17T22:47:11.207" LastEditorUserId="4598" OwnerUserId="4598" ParentId="38141" PostTypeId="2" Score="8" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;Try user-written cmp in Stata.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-17T18:34:20.260" Id="86903" LastActivityDate="2014-02-17T18:34:20.260" OwnerUserId="7071" ParentId="86863" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The OLS estimator is&#10;$$&#10;\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{Y}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The class of linear unbiased estimators is $\tilde{\boldsymbol{\beta}} = \mathbf{C}\boldsymbol{Y}$, for $\mathbf{C} = \mathbf{f}(\mathbf{X})$ (that is $\mathbf{C}$ is a matrix valued function of $\mathbf{X}$), such that&#10;$$&#10;\begin{align}&#10;\mathbb{E}(\tilde{\boldsymbol{\beta}} \mid \mathbf{X}) &amp;amp;= \boldsymbol{\beta} \\&#10;\mathbf{C}\mathbb{E}(\boldsymbol{Y} \mid \mathbf{X}) &amp;amp;= \boldsymbol{\beta} \\&#10;\mathbf{C}\mathbf{X}\boldsymbol{\beta}  &amp;amp;=\boldsymbol{\beta}&#10;\end{align}&#10;$$&#10;where the last step follows from the linear regression model $\mathbb{E}(\boldsymbol{Y} \mid \mathbf{X}) = \mathbf{X}\boldsymbol{\beta}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, it follows that&#10;$$&#10;\mathbf{C}\mathbf{X} = \boldsymbol{\iota}_K&#10;$$&#10;where $\boldsymbol{\iota}_K$ is the identity matrix of size $K$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-02-17T18:44:04.590" Id="86907" LastActivityDate="2014-02-17T18:44:04.590" OwnerUserId="8141" ParentId="86904" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="86959" AnswerCount="1" Body="&lt;p&gt;I have 4 questions in a survey with Likert-style answers (i.e. 1–5) – and the participants have also answered the demographic questions (country and study-years) – which test do I need to find if the demographics, i.e. country and study-years, have each affected answers to each of the questions significantly? Many thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-17T20:54:30.740" Id="86923" LastActivityDate="2014-02-21T14:04:54.663" LastEditDate="2014-02-17T21:29:13.327" LastEditorUserId="32036" OwnerUserId="40437" PostTypeId="1" Score="2" Tags="&lt;survey&gt;&lt;likert&gt;" Title="Effect of two demographic IVs on survey answers (Likert scale)" ViewCount="447" />
  
  <row AcceptedAnswerId="86939" AnswerCount="2" Body="&lt;p&gt;I'm reading Fawcett's 2004 paper on ROC graphs for machine learning algorithms, which can be found &lt;a href=&quot;http://binf.gmu.edu/mmasso/ROC101.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;On page 7-8 he shows a simple ROC example and makes some interpretations that I don't understand. Below is the ROC graph:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vE5u4.png&quot; alt=&quot;example ROC graph&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And here is what he wrote:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Although the test set is very small, we can make some tentative&#10;  observations about the classifier. It appears to perform better in the&#10;  more conservative region of the graph; the ROC point at (0.1,0.5)&#10;  produces its highest accuracy (70%). This is equivalent to saying that&#10;  the classifier is better at identifying likely positives than at&#10;  identifying likely negatives. Note also that the classifier’s best&#10;  accuracy occurs at a threshold of ≥ .54, rather than at ≥ .5 as we&#10;  might expect with a balanced distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't understand how he derived his interpretations.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;the ROC point at (0.1,0.5) produces its highest accuracy (70%)&lt;/code&gt;  How is the highest accuracy of 70% for point (0.1, 0.5) found from that graph, and how do we know it's the highest accuracy?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;This is equivalent to saying that the classifier is better at identifying likely positives than at identifying likely negatives.&lt;/code&gt; I don't see how that interpretation is determined.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;Note also that the classifier’s best accuracy occurs at a threshold of ≥ .54&lt;/code&gt; How was this found?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;rather than at ≥ .5 as we might expect with a balanced distribution&lt;/code&gt; Why would we expect that?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you for any help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-17T22:31:12.637" FavoriteCount="3" Id="86936" LastActivityDate="2014-02-26T16:31:09.673" OwnerUserId="29072" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;roc&gt;" Title="ROC graph interpretation" ViewCount="127" />
  <row Body="&lt;p&gt;Here are some examples:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Descriptive Parametric&lt;/strong&gt;: &lt;em&gt;(I'm having trouble coming up with an example here. I guess the idea is that &quot;descriptive&quot; statistics, by nature, are non-parametric, hence they let us see the shape of the data prior to our making assumptions about it.)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Descriptive Non-Parametric&lt;/strong&gt;: A histogram of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Inferential Parametric&lt;/strong&gt;: A first order ordinary least squares linear regression, which assumes a particular shape in the data (i.e. a linear fit) is an appropriate model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Inferential Non-Parametric&lt;/strong&gt;: Fitting the data using an ensemble of regression trees to develop a predictive model. The model does not make any assumptions about the shape of the data.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-17T22:45:07.190" Id="86937" LastActivityDate="2014-02-18T03:33:39.703" LastEditDate="2014-02-18T03:33:39.703" LastEditorUserId="8451" OwnerUserId="8451" ParentId="82552" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Model:&#10;$y_{it} = \alpha_{i}+\beta_1 D_{it} + \beta_2 G_i + \beta_3 (G_{i}\times D_{it}) + \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;G is a group dummy&lt;/p&gt;&#10;&#10;&lt;p&gt;D is a treatment&lt;/p&gt;&#10;&#10;&lt;p&gt;y is an outcome&lt;/p&gt;&#10;&#10;&lt;p&gt;data is a panel&lt;/p&gt;&#10;&#10;&lt;p&gt;$\alpha$ is a fixed effect&lt;/p&gt;&#10;&#10;&lt;p&gt;My question:  if the only regressor in the model that is shared between the two groups is interacted by the group dummy, is there any sense in keeping the data pooled, or should it be subset and modeled separately?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-17T22:58:08.013" Id="86942" LastActivityDate="2014-02-17T22:58:08.013" OwnerUserId="40441" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;interaction&gt;&lt;panel-data&gt;&lt;fixed-effects-model&gt;" Title="Subsets vs pooling in regressions with interactions" ViewCount="97" />
  <row Body="&lt;p&gt;Yes this situation can arise and is a feature of your modeling assumptions specifically normality in the prior and sampling model (likelihood). If instead you had chosen a Cauchy distribution for your prior, the posterior would look much different. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;prior = function(x) dcauchy(x, 1.5, 0.4)&#10;like = function(x) dnorm(x,6.1,.4)&#10;&#10;# Posterior&#10;propto = function(x) prior(x)*like(x)&#10;d = integrate(propto, -Inf, Inf)&#10;post = function(x) propto(x)/d$value&#10;&#10;# Plot&#10;par(mar=c(0,0,0,0)+.1, lwd=2)&#10;curve(like, 0, 8, col=&quot;red&quot;, axes=F, frame=T)&#10;curve(prior, add=TRUE, col=&quot;blue&quot;)&#10;curve(post, add=TRUE, col=&quot;seagreen&quot;)&#10;legend(&quot;bottomleft&quot;, c(&quot;Prior&quot;,&quot;Likelihood&quot;,&quot;Posterior&quot;), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;seagreen&quot;), lty=1, bg=&quot;white&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZjZ19.png&quot; alt=&quot;Cauchy prior, normal sampling model&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-17T22:58:29.723" Id="86943" LastActivityDate="2014-02-17T22:58:29.723" OwnerUserId="40440" ParentId="86472" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;In the comments of &lt;a href=&quot;http://stats.stackexchange.com/questions/86887&quot;&gt;this question&lt;/a&gt;, it was pointed out that, when comparing two distributions, it is more natural and more general use the cumulative distribution (CDF) instead of the distribution (PDF).&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, why? I.e. what are the advantages (and/or disadvantages) of using the CDF instead of the PDF that make it more &quot;more natural and general&quot;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-18T08:48:51.483" Id="86983" LastActivityDate="2014-02-18T08:48:51.483" OwnerUserId="12100" PostTypeId="1" Score="1" Tags="&lt;pdf&gt;&lt;distance&gt;&lt;cdf&gt;" Title="Why it is better to use the cumulative distribution to compute distances?" ViewCount="39" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a question regarding application of &lt;a href=&quot;http://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot; rel=&quot;nofollow&quot;&gt;tf-idf&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume I have a &lt;a href=&quot;http://en.wikipedia.org/wiki/Document_classification&quot; rel=&quot;nofollow&quot;&gt;document classification&lt;/a&gt; task, there is a training set of documents that are multi-labeled, such that one document can have multiple labels. I use bigrams and unigrams as a features and tf-idf as features values.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, how to calculate td-idf values.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example I have &lt;strong&gt;document 1&lt;/strong&gt; that classified as &lt;strong&gt;class1&lt;/strong&gt; and &lt;strong&gt;class2&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;tf&lt;/strong&gt; should be just the frequency of feature &lt;strong&gt;f&lt;/strong&gt; in &lt;strong&gt;document 1&lt;/strong&gt; or frequency of feature &lt;strong&gt;f&lt;/strong&gt; in all documents of type &lt;strong&gt;class1&lt;/strong&gt; and &lt;strong&gt;class2&lt;/strong&gt; or frequency of feature &lt;strong&gt;f&lt;/strong&gt; in the entire corpus?&lt;/p&gt;&#10;&#10;&lt;p&gt;The same question regarding &lt;strong&gt;idf&lt;/strong&gt;. Should I consider the class of dicument when calculating it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-18T13:25:51.060" Id="87010" LastActivityDate="2014-06-17T08:06:17.320" OwnerUserId="30761" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;" Title="tf-idf in multi-label classification task" ViewCount="122" />
  <row AcceptedAnswerId="87025" AnswerCount="1" Body="&lt;p&gt;I collected data on one sample, the DV could be separated into two groups (success yes vs. no) and then I have several IVs with interval scale. I just don't know if to use Wilcoxon or Man-Whitney test. Also I don't know if it's necessary to use Bonferroni correction or if that is just important for parametric tests. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-18T14:34:16.170" Id="87021" LastActivityDate="2014-02-18T14:54:58.433" LastEditDate="2014-02-18T14:41:35.377" LastEditorUserId="805" OwnerUserId="40114" PostTypeId="1" Score="0" Tags="&lt;nonparametric&gt;&lt;wilcoxon&gt;&lt;mann-whitney-u-test&gt;&lt;bonferroni&gt;" Title="what kind of non-parametric test to use?" ViewCount="45" />
  <row AcceptedAnswerId="87568" AnswerCount="1" Body="&lt;p&gt;I have a dataset of varying sequence lengths and want to calculate descriptive measured for it. The main cause of differing lengths is that some cases are right censored, and are dealt properly when defining the sequence through the &lt;code&gt;right=&quot;DEL&quot;&lt;/code&gt; argument in TraMineR's &lt;code&gt;seqdef&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Will the entropy of state distribution calculated by &lt;code&gt;seqstatd&lt;/code&gt; with the option &lt;code&gt;with.missing=FALSE&lt;/code&gt;, &lt;code&gt;weighted=FALSE&lt;/code&gt; and &lt;code&gt;norm=TRUE&lt;/code&gt; be affected by the variation in length? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Will the sequence turbulence calculated by &lt;code&gt;seqST&lt;/code&gt; be affected by the variation in length?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Will the complexity index calculated with &lt;code&gt;seqici()&lt;/code&gt; with the option &lt;code&gt;with.missing=FALSE&lt;/code&gt; be affected by the varying sequence lengths?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-02-18T15:20:46.540" Id="87034" LastActivityDate="2014-02-23T11:29:34.877" LastEditDate="2014-02-23T11:29:34.877" LastEditorUserId="88" OwnerUserId="29707" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;traminer&gt;&lt;sequence-analysis&gt;" Title="Entropy of state distribution, complexity index and turbulence for sequences of varying length" ViewCount="143" />
  
  <row AcceptedAnswerId="87042" AnswerCount="4" Body="&lt;p&gt;Using this data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;head(USArrests)&#10;nrow(USArrests)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can do a PCA as thus:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(USArrests)&#10;otherPCA &amp;lt;- princomp(USArrests)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can get the new components in &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;otherPCA$scores&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the proportion of variance explained by components with&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(otherPCA)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But what if I want to know which variables are mostly explained by which principal components? And vice versa: is e.g. PC1 or PC2 mostly explained by &lt;code&gt;murder&lt;/code&gt;? How can I do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I say for instance that PC1 is 80% explained by &lt;code&gt;murder&lt;/code&gt; or &lt;code&gt;assault&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the loadings help me here, but they show the directionality not the variance explained as i understand it, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;otherPCA$loadings&#10;&#10;Loadings:&#10;         Comp.1 Comp.2 Comp.3 Comp.4&#10;Murder                         0.995&#10;Assault  -0.995                     &#10;UrbanPop        -0.977 -0.201       &#10;Rape            -0.201  0.974   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-02-18T15:50:48.067" Id="87037" LastActivityDate="2015-01-13T11:36:09.337" LastEditDate="2015-01-12T23:30:25.457" LastEditorUserId="28666" OwnerUserId="19744" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;pca&gt;&lt;dimensionality-reduction&gt;&lt;regression-strategies&gt;" Title="Which variables explain which PCA components, and vice versa?" ViewCount="382" />
  <row Body="&lt;p&gt;Before undertaking a more complex time series approach like VAR, I have attempted some more basic approaches. Again, the aim is to determine the relationship of Hg prices and Au prices after taking into account a generic metals price index. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, I ran a cross-correlation of the mercury and gold price series. The highest correlation  was a lag-0, so I decided to ignore time lags for now. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then, I ran correlations (pearson) on each pair of variables (e.g. mercury~gold). I found that mercury and gold prices were correlated more strongly (~0.9) than either mercury or gold were correlated with the price index (~0.6 and ~0.5 respectively). &lt;/p&gt;&#10;&#10;&lt;p&gt;Next, I created 3 linear regression models: (hg~au), (hg~price index), and (hg~au + price index). The last mode showed the highest R2, with au contributing most to the fit, but the price index also making a statistically significant contribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I concluded that mercury and gold prices are correlated much more strongly to each other than to a generic price index, and that a linear model including both gold price and metals price index provided the best prediction for mercury prices. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very grateful for any feedback or criticism. Would VAR add significant value to this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-18T17:10:57.950" Id="87047" LastActivityDate="2014-02-18T17:10:57.950" OwnerUserId="39525" ParentId="85431" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;I think you can make such estimation. since different model use the same dataset, so the accuracy can be used to be compared. However, one important question, maybe, you need take the parameters of the models. whether these parameter will influence your conclusion.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-18T22:00:41.107" Id="87079" LastActivityDate="2014-07-10T18:49:57.050" LastEditDate="2014-07-10T18:49:57.050" LastEditorUserId="49071" OwnerUserId="40524" ParentId="57173" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;The following answer is based on:  (1) my interpretation of Willett and Singer (1988) &lt;em&gt;Another Cautionary Note about R-squared: It's use in weighted least squates regression analysis.&lt;/em&gt; The American Statistician. 42(3). pp236-238, and (2) the premise that robust linear regression is essentially weighted least squares regression with the weights estimated by an iterative process.&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula I gave in the question for r2w needs a small correction to correspond to equation 4 in Willet and Singer (1988) for r2wls: the SSt calculation should also use a weighted mean: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;the correction is SSt &amp;lt;- sum((x$w*observed-mean(x$w*observed))^2)].&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What is the meaning of this (corrected) weighted r-squared?&lt;/strong&gt; Willett and Singer interpret it as: &quot;the coefficient of determination in the transformed [weighted] dataset. It is a measure of the proportion of the variation in &lt;em&gt;weighted&lt;/em&gt; Y that can be accounted for by &lt;em&gt;weighted&lt;/em&gt; X, and is the quantity that is output as R2 by the major statistical computer packages when a WLS regression is performed&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is it meaningful as a measure of goodness of fit?&lt;/strong&gt; This depends on how it is presented and interpreted. Willett and Singer caution that it is typically quite a bit higher than the r-squared obtained in ordinary least squares regression, and the high value encourages prominent display... but this display may be deceptive IF it is interpreted in the conventional sense of r-squared (as the proportion of &lt;em&gt;unweighted&lt;/em&gt; variation explained by a model). Willett and Singer propose that a less 'deceptive' alternative is pseudoR2wls (their equation 7), which is equivalent to my function r2 in the original question. In general, Willett and Singer also caution that it is not good to rely on any r2 (even their pseudor2wls) as a sole measure of goodness of fit. Despite these cautions, the whole premise of robust regression is that some cases are judged 'not as good' and don't count as much in the model fitting, and it may be good to reflect this in part of the model assessment process. &lt;strong&gt;The weighted r-squared described, can be &lt;em&gt;one&lt;/em&gt; good measure of goodness of fit - as long as the correct interpretation is clearly given in the presentation and it is not relied on as the sole assessment of goodness of fit.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-18T22:51:07.780" Id="87085" LastActivityDate="2014-02-18T22:51:07.780" OwnerUserId="36766" ParentId="83826" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;Salam Mona:   &lt;/p&gt;&#10;&#10;&lt;p&gt;A1)   &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;beef=read.table(&quot;C:/.../beef.txt&quot;,header=T)&#10;&amp;gt;attach(beef)&#10;&amp;gt; beeflm = lm(PBE ~., data=beef)&#10;&amp;gt; summary(beeflm)&#10;&#10;Call:&#10;lm(formula = PBE ~ ., data = beef)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-1.1637 -0.3655 -0.1406  0.6686  1.0238 &#10;&#10;Coefficients:&#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 2613.5927  1211.6305   2.157 0.074375 .  &#10;YEAR          -1.2285     0.6317  -1.945 0.099794 .  &#10;CBE           -1.8901     0.2577  -7.335 0.000328 ***&#10;PPO           -1.1175     0.3739  -2.988 0.024369 *  &#10;CPO           -1.6758     0.5155  -3.251 0.017452 *  &#10;PFO            4.6485     2.7613   1.683 0.143276    &#10;DINC          -6.8875     3.6191  -1.903 0.105716    &#10;CFO            0.4570     1.0339   0.442 0.673989    &#10;RDINC          4.0028     2.2843   1.752 0.130269    &#10;RFP           -0.1926     0.1501  -1.283 0.246658    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 1.12 on 6 degrees of freedom&#10;Multiple R-squared:  0.9871,    Adjusted R-squared:  0.9678 &#10;F-statistic: 51.12 on 9 and 6 DF,  p-value: 5.528e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you look at the p-values for the t-test, you will see three stars for the CBE variable. It means that, taking into account other variables in the model, the CBE is the most significant variable at $\alpha=0.001$ confidence level. See the Signif. codes: below the table. There are no other variables that are even significant at 1%.&lt;/p&gt;&#10;&#10;&lt;p&gt;A2)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; m2=lm(PBE ~CBE, data=beef)&#10;&amp;gt; summary(m2)&#10;&#10;Call:&#10;lm(formula = PBE ~ CBE, data = beef)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-5.7529 -3.1370 -0.6019  2.2417 11.8345 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 126.4851    15.8134   7.999 1.37e-06 ***&#10;CBE          -1.0751     0.2995  -3.590  0.00296 ** &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 4.663 on 14 degrees of freedom&#10;Multiple R-squared:  0.4793,    Adjusted R-squared:  0.4421 &#10;F-statistic: 12.89 on 1 and 14 DF,  p-value: 0.002959&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A3) &amp;amp; A5)   &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; plot(CBE,PBE,xlab=&quot;CBE&quot;,ylab=&quot;PBE&quot;,main=&quot;Beef PBE vs CBE with the fitted line&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A4)    &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; points(CBE,fitted(m2),type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To answer your last question ... First let me denote the true slope and intercept parametesr (for CBE) in the simple regression model by $\beta_1$ and $\beta_0$, respectively. The corresponding estimates for $\beta_1$ and $\beta_0$ in the multiple regression model are -1.8901 and 2613.5927. Basically, you need to test $H_0: \beta_1=-1.8901$ vs $H1: \beta_1\neq-1.8901$ and $H_0: \beta_0=2613.5927$ vs $H1: \beta_0\neq2613.5927$. I will give you a hint, you need to use the t-test and the standard errors reported in the simple regression model.&#10;&lt;img src=&quot;http://i.stack.imgur.com/J8OGW.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-19T05:14:45.827" Id="87108" LastActivityDate="2014-02-19T05:58:32.537" LastEditDate="2014-02-19T05:58:32.537" LastEditorUserId="13138" OwnerUserId="13138" ParentId="87105" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Well, I have these values: $\mathrm{Mean}=5.77$; $95\%\mathrm{CL}=5.30-6.11$&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I simulate a normal distribution using these parameters? I do not have standard deviations.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I obtain random values (for example, 1000 of them) that fit to these parameters? (like using the &lt;code&gt;rnorm&lt;/code&gt; function when having Standard deviations except $95\%CLs$)&lt;/p&gt;&#10;" ClosedDate="2014-02-19T15:51:41.227" CommentCount="5" CreationDate="2014-02-19T11:33:59.350" Id="87139" LastActivityDate="2014-02-19T12:15:35.067" LastEditDate="2014-02-19T12:04:31.200" LastEditorUserId="686" OwnerUserId="40547" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;confidence-interval&gt;" Title="How can I simulate a normal distribution from means and 95% confidence limits?" ViewCount="51" />
  
  <row AcceptedAnswerId="87163" AnswerCount="2" Body="&lt;p&gt;I have a dataset and I would like to see how the dataset is organized via a hierarchy.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have thought of using a divisive method as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1. Cluster the data into 2 classes using K-means (k=2)&#10;2. Cluster Each of the 2 classes using K-means (with k=2) to create a total of 4 classes&#10;3. And so on,&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Would this be a good approach to constructing a dendogram for my data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach I have considered was &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1. Create 15 clusters of my data&#10;2. Cluster the 15 centroids into 5 classes&#10;3. Cluster the 5 centroids into 2 classes&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then we can see a formed hierarchy for my data. Is this approach better?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-19T14:52:09.627" Id="87154" LastActivityDate="2014-02-19T15:33:26.087" OwnerUserId="34588" PostTypeId="1" Score="0" Tags="&lt;k-means&gt;&lt;hierarchical&gt;" Title="Using K-means for hierarchy clustering, a good approach?" ViewCount="319" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;One of the steps in Expectation Propagation based inferencing is to be able to divide out a factor which has a normal form from the current posterior distribution (which is a high dimensional Gaussian). The idea is that the division removes the influence of that factor from the posterior distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, assume that the posterior is multivariate normal with a n-dimensional mean and a $n$ $\times$ $n$ covariance matrix. Now, my likelihood depends on a subset of this n-dimensional parameters (actually 3. So, displacement along the 3 spatial dimensions and has a variance of $\sigma$). Just to make it clear, the likelihood is defined in terms of the residual as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;L(e) \propto \exp^{-0.5 e \sigma e}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $e$ is the residual that depends non-linearly on sub-set of posterior parameters $w$ and has a precision given by $\sigma$ ($\sigma$ is a global noise precision). Just to explain, regardless of the fact that the likelihood is normally distributed wrt to the residuals it should still be seen as a function of $w$ for EP to work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, my question is how can I generate such a factor? So, the factor I need to be designed should be a normal distribution that is a function of all of $w$ but depends on only a subset of $w$ (those 3 elements). Also, I need to somehow generate a precision matrix that will be equivalent to the $\sigma$ and more importantly if I divide the posterior distribution by this Gaussian factor, it would be equivalent to removing the influence of this factor from the posterior distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope I have managed to explain the problem clearly. Please let me know if I can make it clearer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-19T21:59:30.280" Id="87221" LastActivityDate="2014-02-19T21:59:30.280" OwnerUserId="36540" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;normal-distribution&gt;&lt;basic-concepts&gt;" Title="Dividing/Multiplying Gaussians" ViewCount="40" />
  <row AnswerCount="1" Body="&lt;p&gt;Here we have a arrival process. The inter-arrival time follows a shifted negative exponential distribution. The density function of the distribution is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(t)=\lambda e^{-\lambda(t-\theta)},\quad\text{ where }t\ge\theta$$&lt;/p&gt;&#10;&#10;&lt;p&gt;How to derive the variance of the number of arrivals in time period of $T$?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-19T22:12:50.097" FavoriteCount="1" Id="87223" LastActivityDate="2014-12-29T10:50:37.380" LastEditDate="2014-02-20T01:07:57.467" LastEditorUserId="7290" OwnerUserId="40577" PostTypeId="1" Score="5" Tags="&lt;variance&gt;&lt;poisson&gt;&lt;exponential&gt;" Title="Variance of arrival process with shifted exponential distribution" ViewCount="257" />
  
  <row Body="&lt;p&gt;I would not use the guidance in that video.  It is extremely poor advice to use automatic model selection strategies.  To help understand this point, it may help you to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856&quot;&gt;Algorithms for automatic model selection&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That having been said, the answer to your specific question is that you are misunderstanding what is being shown in the video.  What the &lt;code&gt;R&lt;/code&gt; output displayed in the video means is that the AIC listed on the far right is what the model &lt;em&gt;would have&lt;/em&gt; if you dropped the variable in question.  Lower AIC values are still better, both in the Wikipedia article and in the video.  In the middle of the video, the presenter walks through reading the output and shows that dropping &lt;code&gt;C2004&lt;/code&gt; would lead to a new model with &lt;code&gt;AIC = 16.269&lt;/code&gt;.  This is the lowest AIC possible, so it is the best model, so the variable you should drop is &lt;code&gt;C2004&lt;/code&gt;.  The presenter is not saying that you should drop that model, but that you should drop &lt;code&gt;C2004&lt;/code&gt; from the current model to get that model.  The second model, under &lt;code&gt;step&lt;/code&gt; can be seen on the same screen.  You can see that model does not include the variable &lt;code&gt;C2004&lt;/code&gt; and has &lt;code&gt;AIC=16.27&lt;/code&gt;. (Again, for the record, using the AIC in this way is invalid, I'm just explaining what the video is recommending.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IVINw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-20T00:39:12.180" Id="87235" LastActivityDate="2014-02-20T00:46:20.790" LastEditDate="2014-02-20T00:46:20.790" LastEditorUserId="7290" OwnerUserId="7290" ParentId="87234" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="87254" AnswerCount="2" Body="&lt;p&gt;I have pre- and post-treatment continuous data for a large number of variables that I am analyzing for treatment effect. Normally I would obtain the P values and then adjust them for multiple testing with a method such as Benajmini-Hochberg.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the statistical test that I am using is somewhat computationally intensive. To reduce this load, I am thinking of first filtering the data by removing variables for which the average effect size is less than 2-fold in either direction; i.e., &lt;code&gt;mean(absolute(log2(post/pre))) &amp;lt;1&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Will such filtering of data by effect size violate some assumption of P value adjustment methods? It seems that the filtering is likely to enrich for variables for which a truly positive treatment effect exists.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-20T07:30:16.850" Id="87252" LastActivityDate="2014-02-22T06:04:42.040" LastEditDate="2014-02-22T03:24:23.263" LastEditorUserId="4045" OwnerUserId="4045" PostTypeId="1" Score="1" Tags="&lt;multiple-comparisons&gt;&lt;p-value&gt;&lt;effect-size&gt;&lt;filter&gt;&lt;adjustment&gt;" Title="Does filtering of data by effect size violate some assumption of P value adjustment methods?" ViewCount="72" />
  <row Body="Data organized into discrete categories or *classes* may present problems for certain analyses if the number of observations ($n$) belonging to each class is not constant across classes. Classes with unequal $n$ are *unbalanced*." CommentCount="0" CreationDate="2014-02-20T08:49:19.050" Id="87259" LastActivityDate="2014-02-20T09:29:11.930" LastEditDate="2014-02-20T09:29:11.930" LastEditorUserId="32036" OwnerUserId="32036" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Short answer: Yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Longer answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a dependent variable $y$ consisting $J$ categories, than a multinomial logit model would model the probability that $y$ falls in category $m$ as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\mathrm{Pr}(y=m | x) = \frac{\exp(x\beta_m)}{\sum_{j=1}^J \exp(x\beta_j)}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\beta_1 = 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if $y$ has three categories (1,2,3), you could get the three probabilities as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\mathrm{Pr}(y=1 | x) = \frac{\exp(x0)}{\exp(x0) + \exp(x\beta_2) + \exp(x\beta_3)} = \frac{1}{1 + \exp(x\beta_2) + \exp(x\beta_3)}&#10;$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\mathrm{Pr}(y=2 | x) = \frac{\exp(x\beta_2)}{1 + \exp(x\beta_2) + \exp(x\beta_3)}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\mathrm{Pr}(y=3 | x) = \frac{\exp(x\beta_3)}{1 + \exp(x\beta_2) + \exp(x\beta_3)}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;In your special case where $y$ has two categories this condences to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\mathrm{Pr}(y=1 | x)  = \frac{1}{1 + \exp(x\beta_2) }&#10;$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\mathrm{Pr}(y=2 | x) = \frac{\exp(x\beta_2)}{1 + \exp(x\beta_2) }&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is exactly a binary logistic regression.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-20T10:39:56.820" Id="87270" LastActivityDate="2014-02-20T10:39:56.820" OwnerUserId="23853" ParentId="87248" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="87299" AnswerCount="1" Body="&lt;p&gt;I have generated a list of 3 random number where each summed to 1. I would like to access the quality of randomness. What is the best mechanism to access this randomness? E.g my random numbers are. Any idea what tools can I use excel for this?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.4 0.5 0.1&#10;0.2 0.3 0.5&#10;0.6 0.2 0.2&#10;0.5 0.2 0.3&#10;0.2 0.4 0.4&#10;0.2 0.1 0.7&#10;0.3 0.3 0.4&#10;0.8 0.1 0.1&#10;0.1 0.5 0.4&#10;0.4 0.4 0.2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-02-20T10:42:15.313" Id="87271" LastActivityDate="2014-02-20T14:51:06.147" OwnerUserId="40605" PostTypeId="1" Score="1" Tags="&lt;uniform&gt;&lt;randomness&gt;" Title="Quality of randomness in generated random number" ViewCount="48" />
  
  
  
  
  <row AcceptedAnswerId="87391" AnswerCount="1" Body="&lt;p&gt;When we have to make a forecast, the books tell us that the main method is the autoregressive moving average model. In my opinion there is another big tool, the feed forward neural network  (FFNN). So I think that we could use two main tools:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Autoregressive moving average&lt;/li&gt;&#10;&lt;li&gt;Feed forward neural network&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Of course there must be differences, but I am not an expert.&#10;Who, having sufficient experience in these two methods, can explain to me the differences between these two methods on making predictions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-20T14:53:48.377" Id="87300" LastActivityDate="2014-02-21T08:16:03.583" LastEditDate="2014-02-21T08:14:31.107" LastEditorUserId="805" OwnerUserId="10017" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;forecasting&gt;&lt;neural-networks&gt;&lt;arma&gt;" Title="Autoregressive moving average or feed-forward neural network" ViewCount="150" />
  
  <row Body="&lt;p&gt;You can just use the loadings matrix, PCA$rotation, to extract a matrix with the number of components (columns) you want to use. Rather than use predict(), just multiply your test data matrix by this loading matrix to get the transformed data returned by predict. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-20T16:23:58.553" Id="87318" LastActivityDate="2014-02-20T16:23:58.553" OwnerUserId="27190" ParentId="87307" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="87331" AnswerCount="2" Body="&lt;p&gt;I am trying to study predictors of companies' pollution output of some specific chemicals. The data I am using have many 0's (i.e., the company did not pollute at all with those chemicals) and then are continuous with a long right tail. I have seen others model this data by logging the dependent variable after adding 1. My sense is that this is wrong, but I don't understand why. Could someone explain? This approach is much simpler than what I think I should be doing - using zero-inflated two-part models for semi-continuous data - so I'd be thrilled if it turned out simply adding 1 and logging is right. &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, I have found a Stata ado file to run zero-inflated two-part models for semi-continuous data. Is there a way to incorporate fixed effects into this type of model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-20T17:14:04.013" Id="87329" LastActivityDate="2014-02-20T19:00:02.040" LastEditDate="2014-02-20T18:41:01.837" LastEditorUserId="7071" OwnerUserId="40622" PostTypeId="1" Score="3" Tags="&lt;data-transformation&gt;&lt;panel-data&gt;&lt;fixed-effects-model&gt;&lt;zero-inflation&gt;&lt;point-mass-at-zero&gt;" Title="Zero-inflated two-part models for semi-continuous data" ViewCount="355" />
  
  
  <row Body="&lt;p&gt;In item (3), what the problem probably means is that $X_1,X_2$ are conditionally independent and identically distributed, given $Y=y$, such that $X_1\mid Y=y\sim\mathrm{Ber}(y)$. Then, it is easy to prove that&#10;$$&#10;    P(X_2=1\mid X_1 = 1) = \int_0^1 P(X_2=1\mid Y=y)\,f_{Y\mid X_1}(y\mid 1)\,dy \, .&#10;$$&#10;To compute $f_{Y\mid X_1}(y\mid 1)$, notice that $Y\sim\mathrm{Beta}(2,1)$ and use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes%27_theorem&quot; rel=&quot;nofollow&quot;&gt;most beautiful theorem ever&lt;/a&gt; (the answer is $Y\mid X=1\sim\mathrm{Beta}(3,1)$, but please do it). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-20T19:11:44.597" Id="87340" LastActivityDate="2014-02-20T19:28:40.603" LastEditDate="2014-02-20T19:28:40.603" LastEditorUserId="9394" OwnerUserId="9394" ParentId="87242" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_%28information_theory%29&quot; rel=&quot;nofollow&quot;&gt;Shannon entropy&lt;/a&gt; is a general concept for entropy for &lt;em&gt;any&lt;/em&gt; probability distribution. And depending what probability you choose, you get something different. &lt;/p&gt;&#10;&#10;&lt;p&gt;In particular:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Source entropy is Shannon entropy of probability distribution associated with some source of signals - for example zeros and ones or letter over an alphabet (see &lt;a href=&quot;https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication&quot; rel=&quot;nofollow&quot;&gt;C. Shannon, A Mathematical Theory of Communication (1948)&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula&quot; rel=&quot;nofollow&quot;&gt;Boltzmann entropy&lt;/a&gt; is Shannon entropy of the probability distribution of statistical microstates (same: of probability distribution in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Phase_space&quot; rel=&quot;nofollow&quot;&gt;phase space&lt;/a&gt;, up to a constant factor)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;And about entropies I am not deep into:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://mathworld.wolfram.com/KolmogorovEntropy.html&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov-Sinai Entropy&lt;/a&gt;, is (as far I understand) a particular quantity derived from entropy, related to chaotic behavior of dynamic systems (it also happens in the phase space); as you see its formula, its formula involves Shannon entropy for a particular set related to trajectories, but also involves other operations&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Topological_entropy&quot; rel=&quot;nofollow&quot;&gt;Topological entropy&lt;/a&gt; is a related concept&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-02-20T20:09:49.273" Id="87348" LastActivityDate="2014-02-20T20:09:49.273" OwnerUserId="6552" ParentId="82865" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;My teacher likes to give online quizzes that are about 20-30 questions long. Every student has the same questions in the same order. We are not told after taking the quiz which questions we got wrong, but the system does tell us our score.&lt;/p&gt;&#10;&#10;&lt;p&gt;That made me curious as to inferring the answers statistically. (Note that this is not &lt;em&gt;cheating&lt;/em&gt;, as the teacher encourages heavy collaboration.) My idea is this: when a courageous person is confident of his answers, he submits the quiz. Each answer for each question has a &lt;em&gt;fitness score&lt;/em&gt;. Each time someone gets the result after taking the quiz, the square of this quiz score is added to the fitness score of every answer he selected. After some tries, we will have a good prediction of the right answers for all of the questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this statistically rigorous? If not, are there better procedures?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-02-20T20:58:52.627" FavoriteCount="1" Id="87352" LastActivityDate="2014-02-20T20:58:52.627" OwnerUserId="35280" PostTypeId="1" Score="2" Tags="&lt;inference&gt;&lt;causal-inference&gt;" Title="Guessing test question answers from scores" ViewCount="39" />
  <row AnswerCount="1" Body="&lt;p&gt;If the density $f(x,y) = c$  when $x&amp;gt;0$, $y&amp;gt;0$, $x+y &amp;lt; 1$ and $0$ otherwise, find $$E((X+Y)^2 | X = x)\text{ for } x \in (0,1).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;How to approach this question?&lt;/p&gt;&#10;&#10;&lt;p&gt;Can we approach it by letting it equal to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E( (X+Y)^2 | X = x) = E (E( (X+Y)^2 | X = x))$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Just not sure...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-20T21:02:30.023" Id="87353" LastActivityDate="2014-02-21T03:55:48.700" LastEditDate="2014-02-21T03:55:48.700" LastEditorUserId="2970" OwnerUserId="40633" PostTypeId="1" Score="1" Tags="&lt;conditional-probability&gt;&lt;conditional-expectation&gt;" Title="Conditional expectation of $(X+Y)^2 | X = x$" ViewCount="111" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In ridge regression we know that&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3YeSy.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;as an estimate of $\beta$ and this gives the minimum sum of squares of the residuals:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1vkIm.jpg&quot; alt=&quot;&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;And we know that&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6rmkv.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is how to demonstrate that &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UwgBB.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;" ClosedDate="2014-02-21T00:10:42.947" CommentCount="9" CreationDate="2014-02-20T23:31:29.330" Id="87367" LastActivityDate="2014-02-21T00:08:55.410" LastEditDate="2014-02-21T00:08:55.410" LastEditorUserId="24808" OwnerUserId="40639" PostTypeId="1" Score="0" Tags="&lt;ridge-regression&gt;" Title="On distance between parameters in Ridge regression" ViewCount="34" />
  <row AnswerCount="0" Body="&lt;p&gt;Imagine we have &lt;em&gt;C&lt;/em&gt; cars and &lt;em&gt;D&lt;/em&gt; drivers, and each driver takes a large subset of these &lt;em&gt;C&lt;/em&gt; cars in order to test the rate of fuel consumption for some fixed amount of fuel (let's assume that the number of cars not tested by any given reviewer is relatively small, e.g. &amp;lt;20%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Every driver has their own preferred way of testing, which means that given the same car each driver will get different consumption rates. For example, perhaps Driver 1 takes the cars down a highway as fast as possible, Driver 2 drives through an inner-city route with the headlights on, Driver 3 just sits in the car and turns the air-con to full power, and so on. So we end up with a matrix of fuel consumption values &lt;strong&gt;F&lt;/strong&gt; of size &lt;em&gt;C&lt;/em&gt; x &lt;em&gt;D&lt;/em&gt; with some missing values, and even though the values in each row measure the same underlying notion, there may be a large amount of variation because the testing methods are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: how can we use this information covering a variety of usage types - but also with missing data - to find a general consensus about which car has best/worst fuel consumption?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-21T02:34:44.173" Id="87375" LastActivityDate="2014-02-21T02:54:57.883" LastEditDate="2014-02-21T02:54:57.883" LastEditorUserId="40631" OwnerUserId="40631" PostTypeId="1" Score="2" Tags="&lt;missing-data&gt;&lt;dimensionality-reduction&gt;&lt;measurement&gt;" Title="Creating consensus from multiple methods of measuring the same entity with some missing values" ViewCount="26" />
  
  <row Body="&lt;p&gt;Feed forward neural networks, as opposed to ARMA models, attempt to capture the fundamental patterns present in your data. In other words, ARMA predicts the next step based on the moving average and its current tendency. FFNNs try to see the bigger picture and approximate a non-linear function that maps previous steps to the next step. Theoretically, FFNNs may give you better long-term predictions. However, training FFNNs is no easy task; the number of different FFNN training algorithms and corresponding parameters to be optimised is somewhat overwhelming. The major difference between the two approaches can be formulated as follows: ARMA produces linear models; FFNN produces non-linear models. If your data is simple and can be fitted to a linear model, you are better off using ARMA. If you suspect non-linear relationships between variables are present in your data, or if ARMA does not produce a model of sufficient quality, go for FFNNs.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-21T07:24:59.403" Id="87391" LastActivityDate="2014-02-21T08:16:03.583" LastEditDate="2014-02-21T08:16:03.583" LastEditorUserId="805" OwnerUserId="13128" ParentId="87300" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a set of values and I wish to check if they are piecewise uniform. I hope I'm using the correct terms, but I'll explain what I mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following values - 100,105,100,103,98. &lt;/p&gt;&#10;&#10;&lt;p&gt;We can &quot;see&quot; they are close to each other. We can define this uniformity by normalizing the Standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's look at the following values - 100,105,100,103,98,198,200,203,205,100,105,100,103,98&lt;/p&gt;&#10;&#10;&lt;p&gt;Standard deviation will not work here, but it's clearly uniform in each segment. To check the uniformity here we can check how many of the sample deviate by no more than d% from their predecessor. This seems to work but (and here's the question) - is there a better approach? I don't even know how to call this test to try and search for articles.&lt;/p&gt;&#10;&#10;&lt;p&gt;This doesn't handle outliers very well. I can use a moving average or a moving median, and again - is there something better?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2014-02-21T14:31:14.117" Id="87421" LastActivityDate="2014-12-29T22:25:18.797" LastEditDate="2014-02-21T15:56:12.743" LastEditorUserId="919" OwnerUserId="40668" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;change-point&gt;" Title="Checking that values are piecewise uniform" ViewCount="95" />
  <row AnswerCount="1" Body="&lt;p&gt;In a questionnaire I conducted for my dissertation, i asked respondents to rate the frequency (5 point scale from never to always) to which they resorted to certain unethical behaviour during their work. I also asked them to rate their acceptance of such behaviour using a reliable scale (5 point scale from Strongly Disagree to Strongly Agree).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, as expected, the mean scores of the self-admitted unethical behaviour is lower than the mean of the behaviour acceptance scale. Is there an appropriate statistical test to use in this case? &lt;/p&gt;&#10;&#10;&lt;p&gt;What test is there to prove that the behaviour acceptance scale is a good measure/representative of actual behaviour? Is correlation enough?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-21T16:02:59.313" Id="87431" LastActivityDate="2014-08-07T19:58:11.593" OwnerUserId="40183" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;" Title="How to analyse means between two similar scales?" ViewCount="46" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have fitted random coefficient Poisson analysis in R. I have obtained the following results: &lt;/p&gt;&#10;&#10;&lt;p&gt;Generalized linear mixed model fit by maximum likelihood ['glmerMod']&#10;Family: poisson ( log )&lt;/p&gt;&#10;&#10;&lt;p&gt;Formula: frequency ~ 1 + cc + ageveh + make + (1 | AREA) &lt;/p&gt;&#10;&#10;&lt;p&gt;Data: x &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  AIC       BIC    logLik  deviance &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;1359.1477 1389.7370 -672.5739 1345.1477 &lt;/p&gt;&#10;&#10;&lt;p&gt;Random effects:&lt;/p&gt;&#10;&#10;&lt;p&gt;Groups Name        Variance Std.Dev.&lt;/p&gt;&#10;&#10;&lt;p&gt;AREA   (Intercept) 1.323    1.15 &lt;/p&gt;&#10;&#10;&lt;p&gt;Number of obs: 584, groups: AREA, 8&lt;/p&gt;&#10;&#10;&lt;p&gt;Fixed effects:&#10;            Estimate Std. Error z value Pr(&gt;|z|) &lt;/p&gt;&#10;&#10;&lt;p&gt;(Intercept) -0.12902    0.44432  -0.290   0.7715 &lt;/p&gt;&#10;&#10;&lt;p&gt;ccL          0.05656    0.12371   0.457   0.6475&lt;/p&gt;&#10;&#10;&lt;p&gt;agevehO      0.02136    0.09264   0.231   0.8177&lt;/p&gt;&#10;&#10;&lt;p&gt;make2       -0.45454    0.20632  -2.203   0.0276 *&lt;/p&gt;&#10;&#10;&lt;p&gt;make3       -0.31799    0.21422  -1.484   0.1377 &lt;/p&gt;&#10;&#10;&lt;h2&gt;make4       -0.29708    0.14469  -2.053   0.0401 *&lt;/h2&gt;&#10;&#10;&lt;p&gt;Signif. codes:  0 ‘&lt;strong&gt;&lt;em&gt;’ 0.001 ‘&lt;/strong&gt;’ 0.01 ‘&lt;/em&gt;’ 0.05 ‘.’ 0.1 ‘ ’ 1&lt;/p&gt;&#10;&#10;&lt;p&gt;Correlation of Fixed Effects:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    (Intr) ccL    agevhO make2  make3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;ccL      0.052    &lt;/p&gt;&#10;&#10;&lt;p&gt;agevehO -0.179 -0.232   &lt;/p&gt;&#10;&#10;&lt;p&gt;make2   -0.171 -0.007 -0.001 &lt;/p&gt;&#10;&#10;&lt;p&gt;make3   -0.156  0.022 -0.078  0.366 &lt;/p&gt;&#10;&#10;&lt;p&gt;make4   -0.300 -0.235  0.167  0.544  0.522&lt;/p&gt;&#10;&#10;&lt;p&gt;However I am unable to interpret the results. &lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-02-21T19:11:41.860" Id="87445" LastActivityDate="2014-02-23T14:25:11.817" LastEditDate="2014-02-23T14:25:11.817" LastEditorUserId="40494" OwnerUserId="40494" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;interpretation&gt;&lt;glmm&gt;" Title="How to interpret the results from random coefficient Poisson data analysis" ViewCount="96" />
  
  
  <row AcceptedAnswerId="87463" AnswerCount="1" Body="&lt;p&gt;The first thing that comes to mind when comparing some property of two samples is probably the independent samples &lt;em&gt;t&lt;/em&gt;-test. Sometimes, people point out that this has the inherent assumption of the &lt;em&gt;t&lt;/em&gt;-distribution. Does any one know of a non-parametric alternative to the independent samples &lt;em&gt;t&lt;/em&gt;-test?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-21T21:46:03.083" Id="87462" LastActivityDate="2014-02-21T23:02:55.307" LastEditDate="2014-02-21T22:07:36.810" LastEditorUserId="32036" OwnerUserId="25186" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;nonparametric&gt;" Title="Non-parametric test for comparing a function of two samples" ViewCount="65" />
  <row Body="&lt;p&gt;Let me first point out that you appear to have a common misunderstanding about the meaning of &lt;a href=&quot;http://en.wikipedia.org/wiki/P_value&quot; rel=&quot;nofollow&quot;&gt;p-values&lt;/a&gt;.  In conventional (&lt;a href=&quot;http://en.wikipedia.org/wiki/Frequency_probability&quot; rel=&quot;nofollow&quot;&gt;frequentist&lt;/a&gt;) statistical analysis, the p-value is the probability of getting a sample statistic (say a sample mean) as far or further from the proposed null value as yours, if the null value is the true value.  Importantly, there is no such thing as (e.g.) &quot;bananas cure cancer with probability at least 99.99%&quot;.  The fact that a p-value might be $&amp;lt; 0.0001$ very much does not imply that there is a 99.99% probability the alternative hypothesis is true (or a 0.01% probability the null hypothesis is true).  For more on this topic, it may help you to read this CV thread: &lt;a href=&quot;http://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests&quot;&gt;What is the meaning of p values and t values in statistical tests?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That having been said, it is possible to assert a (subjective) probability associated with the null hypothesis within the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayesian_probability&quot; rel=&quot;nofollow&quot;&gt;Bayesian framework&lt;/a&gt;.  &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes%27_rule&quot; rel=&quot;nofollow&quot;&gt;Bayes' rule&lt;/a&gt; is:&lt;br&gt;&#10;$$&#10;Pr(H_0|D) = \frac{Pr(D|H_0)}{Pr(D)}Pr(H_0)&#10;$$&#10;In words, the probability that the null hypothesis is true that you should believe after having seen some data is equal to the distinctiveness of the data with respect to the null hypothesis (indexed by the quotient on the RHS) multiplied by the probability that the null hypothesis is true that you believed before having seen the data in question.  To make this easier, consider the following example&lt;sup&gt;1&lt;/sup&gt;:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;MAMMOGRAPHY&lt;br&gt;&#10;  A reporter for a women's monthly magazine would like to write an article about breast cancer.  As a part of her research, she focuses on mammography as an indicator of breast cancer.  She wonders what it really means if a woman tests positive for breast cancer during her routine mammography examination.  She has the following data:&lt;br&gt;&#10;  The probability that a woman who undergoes a mammography will have breast cancer is 1%.&lt;br&gt;&#10;  If a woman undergoing a mammography has breast cancer, the probability that she will test positive is 80%.&lt;br&gt;&#10;  If a woman undergoing a mammography does not have breast cancer, the probability that she will test positive is 10%.&lt;br&gt;&#10;  What is the probability that a woman who has undergone a mammography actually has breast cancer, if she tests positive?  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;How can we figure out that probability?  We must revise the a priori probability that a woman who undergoes a mammography has breast cancer, p(cancer) which according to the text is 1% or p=.01, in light of the new information that the test was positive.  That is, we are looking for the conditional probability of p(cancer|positive).  The probability of a positive result given breast cancer, p(positive|cancer), is 80% or p=.8, and the probability of a positive result given no breast cancer, p(positive|no cancer), is 10% or p=.1.  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Thus, we have:&lt;br&gt;&#10;$$&#10;Pr({\rm cancer|positive}) = \frac{0.80}{\underbrace{0.80\!\times\! 0.01}_{Pr(D)\text{ w/ cancer}}\;+\;\underbrace{0.10\!\times\! 0.99}_{\Pr(D)\text{ w/o cancer}}} 0.01 = 0.075&#10;$$&#10;(The denominator of the fraction in Bayes' rule is often hard for people to understand.  In this case, it is possible to enumerate the possible probabilities of the data, and $Pr(D)$ is simply the sum of all the individual enumerated probabilities.  For greater clarity, I annotated them here.  Often, the set of possible probabilities is much harder to determine.  In practice, people often ignore the denominator and replace the equals sign with $\propto$, 'proportional to'.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now in this example, the cancer rate is known beforehand.  To make this example more like your new research finding example, let's imagine that no one knows exactly what the cancer rate is, but two different doctors believe the cancer rate is 1%, and 5% respectively.  If we use the latter value in the equation above, we get:&lt;br&gt;&#10;$$&#10;Pr({\rm cancer|positive}) = \frac{0.80}{0.80\!\times\! 0.05\;+\;0.10\!\times\! 0.95} 0.05 = 0.296&#10;$$&#10;The probability is now 29.6%, which is very different from the 7.5% above.  So who is right?  We don't really know, but the important part is that both doctors are rational in believing their (very different) probabilities that their patient has breast cancer.  &lt;strong&gt;To put this a different way, what is rational isn't the probability that each believes, but rather the manner in which they &lt;em&gt;change&lt;/em&gt; their belief in light of new evidence.&lt;/strong&gt;  Since both doctors changed their belief using a correct application of Bayes' rule, both are rational, even though they came to different conclusions.  The reason they didn't end up with the same probability is because they didn't believe in the same probability beforehand; this is what @AlecosPapadopoulos meant by 'they do not have &quot;same information&quot;'.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;1. This example is copied from: Sedlmeier, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0805832823&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Improving Statistical Reasoning&lt;/em&gt;&lt;/a&gt;, pp. 8-9.&lt;/sub&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-21T22:24:08.180" Id="87464" LastActivityDate="2014-02-22T16:49:23.860" LastEditDate="2014-02-22T16:49:23.860" LastEditorUserId="7290" OwnerUserId="7290" ParentId="87447" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am conducting a multiple regression with 1 DV and 6 IVs. I am trying to test Homoscedasticity on SPSS using a scatterplot since all my variables are scales. I conducted a the residual vs predictor value scatterplot and I think it might be a little heteroscadestic. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I know which variable is the one causing the problem? And what should the next step be to try to make my data homoscedastic? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-22T09:59:43.603" Id="87486" LastActivityDate="2014-02-22T18:01:07.677" LastEditDate="2014-02-22T18:01:07.677" LastEditorUserId="40183" OwnerUserId="40183" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;" Title="Testing homoscedasticity for multiple regression in SPSS" ViewCount="1227" />
  
  
  <row Body="&lt;p&gt;Based on the comments, first of all try to decide whether there is a reason to suspect an autoregressive generative process. You could do this, for example, by looking at correlation between outputs at varying delays. If you are modeling some kind of physical system you can use information about the structure. Since you mention it's a black-box I assume you have no such information.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is a reason to assume some kind of autoregressive process, I would focus on fitting an AR model. From a fitted model you can typically get confidence intervals on predictions which you can then use to assess whether a certain output falls within expectations. In case you get a terrible fit, (i) maybe there really is no underlying autoregressive process or (ii) you may need to use a nonlinear technique like SVM to adequately model the process. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-22T11:02:32.737" Id="87495" LastActivityDate="2014-02-22T11:02:32.737" OwnerUserId="25433" ParentId="87492" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;How do I show the Weibull distribution $f:(y; \lambda, \rho)$ can be transformed to the exponential family using the transformation $z=y^\lambda$?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I know the form I need to express it in is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\exp\lbrace(y.\theta - b(\theta)/a(\phi))+c(y,\phi)\rbrace$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;but am unsure how to get there.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-22T13:07:17.040" FavoriteCount="2" Id="87501" LastActivityDate="2014-12-23T04:11:15.623" LastEditDate="2014-12-23T04:08:39.347" LastEditorUserId="805" OwnerUserId="40124" PostTypeId="1" Score="4" Tags="&lt;data-transformation&gt;&lt;weibull&gt;&lt;exponential-family&gt;" Title="Show that Weibull distribution can be transformed to exponential family" ViewCount="289" />
  <row Body="&lt;p&gt;The error you got has nothing to do with Kruskal-Wallis test.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[P,ANOVATAB,STATS]=kruskalwallis([rand(10,1) rand(30,1)])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This results in an error because you are trying to horizontally join a column of 10 elements with a column of 30 elements. This is impossible in matlab.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you type &quot;&lt;a href=&quot;http://www.mathworks.com/help/stats/kruskalwallis.html&quot; rel=&quot;nofollow&quot;&gt;help kruskalwallis&lt;/a&gt;&quot;, you will immediately see that the first input parameter should  be &lt;em&gt;all&lt;/em&gt; groups joined together, and the second -- dummy coding of a group. So you should write&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[P,ANOVATAB,STATS]=kruskalwallis([rand(10,1); rand(30,1)], [ones(10,1); 2*ones(30,1)])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and it works just fine.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-22T13:58:31.310" Id="87503" LastActivityDate="2014-02-22T13:58:31.310" OwnerUserId="28666" ParentId="54699" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;For this non-linear equation&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = a\left(1-\exp\left(-\left(x/b\right)\right)\right)^c$$&lt;/p&gt;&#10;&#10;&lt;p&gt;by using least square estimated values for the parameters are obtained and&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(3, 33, 146, 227, 342, 351, 353, 444, 556, 571, 709, 759, 836, &#10;860, 968, 1056, 1726, 1846, 1872, 1986, 2311, 2366, 2608, 2676, &#10;3098, 3278, 3288, 4434, 5034, 5049, 5085, 5089, 5089, 5097, 5324, &#10;5389, 5565, 5623, 6080, 6380, 6477, 6740, 7192, 7447, 7644, 7837, &#10;7843, 7922, 8738, 10089, 10237, 10258, 10491, 10625, 10982, 11175, &#10;11411, 11442, 11811, 12559, 12559, 12791, 13121, 13486, 14708, &#10;15251, 15261, 15277, 15806, 16185, 16229, 16358, 17168, 17458, &#10;17758, 18287, 18568, 18728, 19556, 20567, 21012, 21308, 23063, &#10;24127, 25910, 26770, 27753, 28460, 28493, 29361, 30085, 32408, &#10;35338, 36799, 37642, 37654, 37915, 39715, 40580, 42015, 42045, &#10;42188, 42296, 42296, 45406, 46653, 47596, 48296, 49171, 49416, &#10;50145, 52042, 52489, 52875, 53321, 53443, 54433, 55381, 56463, &#10;56485, 56560, 57042, 62551, 62651, 62661, 63732, 64103, 64893, &#10;71043, 74364, 75409, 76057, 81542, 82702, 84566, 88682)&#10;&#10;y &amp;lt;- c(1:136)&#10;df &amp;lt;- data.frame(x,y)&#10;&#10;fit &amp;lt;- nls(y ~ a*(1-exp(-x/b))^c, data=df, start = list( a=100,b=1000,c=0.5),  &#10;     algorithm=&quot;port&quot;,lower=list(a=100,b=100,c=0.5),upper=list(a=200,b=10000,c=2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How to do this using Maximum Likelihood Estimation (MLE) for this non-linear equation?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-23T10:32:08.810" Id="87570" LastActivityDate="2014-02-23T11:28:15.527" LastEditDate="2014-02-23T11:28:15.527" LastEditorUserId="88" OwnerUserId="40741" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;maximum-likelihood&gt;&lt;nonlinear-regression&gt;" Title="Maximum likelihood estimation for custom equation in R" ViewCount="106" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was thinking this may be similar to a Mark and recapture problem where there is a known upper bound, hence the title.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am doing a proportion but sometimes the estimate is larger than the known upper bound. Is there a way to restrict the possible answers to be between the two bounds?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: Assume out of a known population (U=10,000) a total of (X=8,505) people made (T=25,916) purchases. Of those, only (A=4,697) of them recorded anything with (R=8,632) recorded purchases. &lt;/p&gt;&#10;&#10;&lt;p&gt;The only unknown value is X and we wish to estimate it. &lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the time the ratio estimate works great but does usually over-estimate the true size - and in some cases it is greater than the known population. In this case (X_est/T)=(A/R) which yielded X_est=14,102 which is larger than the known population.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to incorporate the known population size to limit the estimate between 0 and U? Currently I am just censoring anything above U as U which seems ad hoc as the bounds should be 'built in' to the formula (hopefully).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-23T15:45:45.593" Id="87584" LastActivityDate="2014-02-23T15:45:45.593" OwnerUserId="16623" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;proportion&gt;&lt;capture-mark-recapture&gt;" Title="Mark and recapture with known upper bound" ViewCount="31" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a text string containing unstructured data and I would like to analyze it in order to extract structured information. In particular, this text string specifies when a service is operational (the days and the hours). This text string may be written in different ways:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a list containing one or more day names;&lt;/li&gt;&#10;&lt;li&gt;hours may be written next to the names of the days to which they refer;&lt;/li&gt;&#10;&lt;li&gt;the string may contain one or more range of days (each range of days could have a certain range of hours);&lt;/li&gt;&#10;&lt;li&gt;the name of the days may not be present, and in this case it could be replaced by &quot;every day&quot; or the words &quot;workdays&quot; or by &quot;holidays&quot;;&lt;/li&gt;&#10;&lt;li&gt;etc..&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What approach could I use to be able to extract structured information from such a text string? Are there specific algorithms to achieve this purpose? Should I use a classifier (artificial neural networks, random forests, etc.)?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-23T16:02:24.570" FavoriteCount="1" Id="87586" LastActivityDate="2014-02-23T16:02:24.570" OwnerUserId="16937" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;text-mining&gt;&lt;natural-language&gt;" Title="How to extract structured information from a text string?" ViewCount="60" />
  <row Body="&lt;p&gt;The numbers in the problem are small enough that you can solve this problem by simply listing all possible cases and counting.  Here is something to start you off&#10;for part (b). Call the 5 novels N1, N2, N3, N4, N5, and make a list of all possible&#10;pairs of novels starting with&lt;/p&gt;&#10;&#10;&lt;p&gt;N1, N2,&lt;/p&gt;&#10;&#10;&lt;p&gt;N1, N3,&lt;/p&gt;&#10;&#10;&lt;p&gt;.....&lt;/p&gt;&#10;&#10;&lt;p&gt;and ending with&lt;/p&gt;&#10;&#10;&lt;p&gt;N4, N5.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your list has more than (or less than!)  $\binom{5}{2} = 10$ pairs,&#10;go back and correct it.  Now add in P1 or P2 or P3 into each of the&#10;10 pairs of novels.....&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-23T20:43:48.823" Id="87593" LastActivityDate="2014-02-23T20:43:48.823" OwnerUserId="6633" ParentId="87572" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="128745" AnswerCount="1" Body="&lt;p&gt;I have a time series that shows a non stationary seasonal autoregressive component as well ask known heteroshedasticity. In order to model the series, I have fit a seasonal arima model for the mean with the auto.arima model in forecast R package and a GARCH model on residuals of the arima model. Is the procedure of sequentially estimating ARIMA and GARCH model correct or it would have been better to jointly model the mean and the variance of the series? In this were correct is there and (possibly R) function to do it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-23T22:19:01.930" FavoriteCount="1" Id="87600" LastActivityDate="2014-12-11T21:38:55.100" OwnerUserId="6547" PostTypeId="1" Score="1" Tags="&lt;arma&gt;&lt;garch&gt;" Title="ARMA/GARCH estimation in sequence" ViewCount="64" />
  
  <row AcceptedAnswerId="87780" AnswerCount="1" Body="&lt;p&gt;I just have a quick question about MLE.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes, when doing MLE problems, I see that the variance expression gotten from the inverse of the Fisher Information is exactly like what it should be and sometimes it isn't. Is there a reason that for some distributions this method is exact? Also, why is it that for some distributions, for example like $geom0(\pi)$, is it difficult to find an exact formula for $var(\tilde\pi)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks so much for your help!&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Geom0(\pi)$ is the geometric distribution where the PMF is $(1-\pi)^y\pi$ where $y=0,1,2,...\infty$ as opposed to $Geom1(\pi)$ where the PMF is $(1-\pi)^{(y-1)}\pi$ where $y=1,2,...\infty$. Sorry for the confusion!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-24T03:18:00.007" Id="87616" LastActivityDate="2014-02-25T09:36:25.140" LastEditDate="2014-02-25T02:41:36.127" LastEditorUserId="24243" OwnerUserId="24243" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;maximum-likelihood&gt;" Title="When is the inverse of the Fisher Information exact? (MLE)" ViewCount="82" />
  <row AnswerCount="1" Body="&lt;p&gt;This question regards the basic statistics of a normal distribution, but I can't figure it out. I have been given the mean and 95% confidence intervals for a distribution, but would like to know the standard deviation. In my example:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\mu=53.4\quad&#10;95\%\ c.i.=(52.3, 54.3)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I had thought that the solution for $\sigma$ would be something like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$54.3=53.4+(SE*1.96)$$&#10;$$SE=(54.3-53.4)/1.96=0.46$$&#10;and then,&#10;$$SE=\frac{\sigma}{\sqrt{n}}$$&#10;$$\sigma=0.46*\sqrt{n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if I don't know the $n$, is this possible?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-24T10:52:02.327" Id="87641" LastActivityDate="2014-02-24T13:33:33.193" OwnerUserId="10675" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;confidence-interval&gt;&lt;standard-deviation&gt;" Title="Given a mean and 95% confidence interval, do I need to know the sample size to calculate the standard deviation?" ViewCount="123" />
  
  <row Body="&lt;p&gt;I would suggest it's not possible. There are two unknowns in one equation, implying that there can be more than one unique answer. The following graph shows a curve that lies on some combinations that can give rise to an SE of 0.46, and you can see that without assuming what the sample size is, there will be more than one possible SD.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tfdPJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In very rare cases, one may look at what the variable actually is and come up with a guess of the SD. Is it safe to assume a normal distribution? Does the variable go into the negative domain? etc. From these assumption, you can sometimes guess what the SD may be. I need to use this very occasionally when calculating sample size and power. In those situations, being able to name some conservative SD is a lot more important that nailing down the exact SD.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-24T13:33:33.193" Id="87656" LastActivityDate="2014-02-24T13:33:33.193" OwnerUserId="13047" ParentId="87641" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The between-cluster heterogeneity induced by the frailty term can be depicted by the spread in the median time to event (or any other quantile) from cluster to cluster  or in the $5$-year survival rate (or any other rate) over clusters [&lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1198/000313005X43236#.UwtzhoX_1qI&quot; rel=&quot;nofollow&quot;&gt;Duchateau and Janssen (2005)&lt;/a&gt;, &lt;a href=&quot;http://ctj.sagepub.com/content/3/1/10.abstract&quot; rel=&quot;nofollow&quot;&gt;Legrand et al. (2006)&lt;/a&gt;]. &lt;/p&gt;&#10;&#10;&lt;p&gt;The first paper develops the idea while the second paper illustrates it by providing, using a real data set, recommendations on how to explain heterogeneity between clusters, and how to depict it beyond the plot of the Kaplan-Meier curve stratified by cluster.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Some details&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $S_i(t) = \exp(-H_0(t) \, u_i)$ be the model-based survival function in cluster $i$ (with no covariate for ease of presentation). The median time to event in cluster $i$, $t_{M,i}$, is such that &#10;$$&#10;\exp(-H_0(t_{M,i}) \, u_i) = 0.5&#10;$$&#10;that is,&#10;$$&#10;t_{M,i} = H_0^{-1} \left( \frac{\log(2)}{u_i} \right)&#10;$$&#10;As $u_i$ is the actual value of a random variable $U$, $t_{M,i}$ is also the actual value of a random variable, say $T_M$. The density of $T_M$, $f_{T_M}$, can be worked out using results on transformations of random variables:&#10;$$&#10;f_{T_M}(t_{M,i}) = f_{U} \left( \frac{\log(2)}{H_0(t_{M,i})} \right) \left|\frac{\mathrm{d}U}{\mathrm{d}T_{M}}\right|&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I take $H_0(t) = \lambda t^\rho$ (Weibull), with $\lambda = 0.7$ and $\rho = 1.5$. Then $\left|\dfrac{\mathrm{d}U}{\mathrm{d}T_{M}}\right| = \dfrac{\rho\log(2)}{\lambda \,T_M^{\rho+1}}$.&#10;I consider that $U$ follows a gamma distribution with mean $1$ and variance $\theta$. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta = 0.5$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8D0Oh.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;$$$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta = 1$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ecF6k.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-24T16:39:54.163" Id="87679" LastActivityDate="2014-02-25T06:00:15.120" LastEditDate="2014-02-25T06:00:15.120" LastEditorUserId="3019" OwnerUserId="3019" ParentId="87672" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I often hear about evaluating a classification model's performance by holding out the test set and training a model on the training set.  Then creating 2 vectors, one for the predicted values and one for the true values.  Obviously doing a comparison allows one to judge the performance of the model by its predictive power using things like F-Score, Kappa Statistic, Precision &amp;amp; Recall, ROC curves etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;How does this compare to evaluating numeric prediction like regression?  I would assume that you could train the regression model on the training set, use it to predict values, then compare these predicted values to the true values sitting in the test set.  Obviously the measures of performance would have to be different since this isn't a classification task.&#10;The usual residuals and $R^2$ statistics are obvious measures but are there more/better ways to evaluate the performance for regression models?  It seems like classification has so many options but regression is left to $R^2$ and residuals.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-24T16:41:00.813" FavoriteCount="1" Id="87680" LastActivityDate="2014-02-24T17:16:34.710" LastEditDate="2014-02-24T17:16:34.710" LastEditorUserId="24808" OwnerUserId="40808" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;performance&gt;" Title="Evaluating a regression model's performance using training and test sets?" ViewCount="297" />
  
  <row Body="&lt;p&gt;Because your conclusions might depend on how you model the relationship between people and stores, let's be explicit about that.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;One reasonable approach&lt;/strong&gt; that seems in the spirit of the question is to suppose that stores emerge randomly as a function of the population: each new person creates a chance $\lambda$ of a new store appearing.  More explicitly (and slightly more generally) let us posit that the distribution of the number of stores in an area with $n$ people is indeed Poisson with a parameter equal to $\lambda n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the null hypothesis of no difference between the two areas we would estimate $\lambda$ as the mean number of stores per person:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\lambda} = \frac{43 + 27}{28137 + 35167}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How consistent are the data with this hypothesis?&lt;/strong&gt;  One way to tell, without any further analysis, would be to simulate data for the numbers of stores given these two populations.  In such a simulation the numbers of stores in the first area would have a Poisson$(28137\hat{\lambda})$ distribution and the numbers in the second area would have a Poisson$(35167\hat{\lambda})$ distribution.  The simulation results would therefore look like the histograms in the figure below.  That figure marks the actual observations with vertical red lines: the numbers of stores appear to be high for the high deprivation area and low for the low deprivation area.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;We can formalize and quantify this appearance&lt;/strong&gt; by constructing a statistic that appropriately measures the differences in stores per capita.  A good one would be a Student T-like statistic that divides the difference in stores per capita (compared to the expected difference) by its standard deviation.  Denote the two areas by indexes $1$ and $2$, their populations by $n_1$ and $n_2,$ the (true but unknown) underlying rates $\lambda_1$ and $\lambda_2,$ and the total numbers of stores in the two areas by $X_1$ and $X_2.$   Assuming these numbers are statistically independent (which is a useful starting hypothesis), it follows that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(X_2 - X_1) = \lambda_2 n_2 - \lambda_1 n_1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}(X_2 - X_1) = \text{Var}(X_1) + \text{Var}(X_2) = n_1 \lambda_1 + n_2 \lambda_2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the null hypothesis that $\lambda_1 = \lambda_2,$ the mean $\mathbb{E}(X_2-X_1)$ is estimated as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\lambda} (n_2 - n_1) \approx 7.7736$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the variance will be estimated just as simply by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\widehat{\text{Var}}(X_2 - X_1) = X_1 + X_2 = 70$$&lt;/p&gt;&#10;&#10;&lt;p&gt;whence the T statistic is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$T = \frac{27-43 - 7.7736}{\sqrt{70}} \approx -2.8415.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;A normal approximation to its distribution is accurate because both store counts are fairly large. Because the alternative hypothesis is only that the two rates might differ, this &lt;em&gt;two-sided test&lt;/em&gt; gives a p-value of $2\Phi(-|-2.8415|) \approx 0.00449,$ which is small: &lt;strong&gt;the difference looks significant&lt;/strong&gt;.   The simulation provides a check: in $10,000$ iterations, I found only $40$ of them, or $0.004$ of the total, exhibited a T statistic larger in absolute value than the one actually observed.  (The values of $\pm 2.8415$ are shown in the histogram of these statistics.)  This is close agreement with the Normal approximation.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, &lt;strong&gt;these data are not consistent with the model and the additional hypotheses made in the foregoing analysis.&lt;/strong&gt;  Therefore at least one of the following is not true:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The store counts do not even approximately have Poisson distributions with rates proportional to the populations; and/or&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The store counts are not independent.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In either case it would be fair to say there is a &quot;significant difference&quot; in the per-capita numbers of stores in these two regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WS1cM.png&quot; alt=&quot;Figures&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; code to produce the calculations and figures follows.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# Data&#10;#&#10;people &amp;lt;- c(High=28137, Low=35167)&#10;stores &amp;lt;- c(High=43, Low=27)&#10;#&#10;# Estimates and tests.&#10;#&#10;lambda &amp;lt;- sum(stores)/sum(people)&#10;n &amp;lt;- 10^4&#10;sim &amp;lt;- matrix(rpois(n*length(stores), lambda*people), ncol=length(stores), byrow=TRUE)&#10;colnames(sim) &amp;lt;- c(&quot;High&quot;, &quot;Low&quot;)&#10;&#10;t.stat &amp;lt;- function(stores, people) {&#10;  m = diff(sum(stores)/sum(people) * people)&#10;  (diff(stores) - m) / sqrt(sum(stores))&#10;}&#10;&#10;t.sim &amp;lt;- apply(sim, 1, t.stat, people=people)&#10;sum(abs(t.sim) &amp;gt; abs(t.stat(stores, people))) / n # The simulation p-value&#10;2 * pnorm(-abs(t.stat(stores, people)))           # The normal approximation&#10;#&#10;# Figures&#10;#&#10;par(mfrow=c(1,3))&#10;plot.sim &amp;lt;- function(s) {&#10;  hist(sim[, s], freq=FALSE, main=paste(s, &quot;deprivation simulation&quot;), xlab=&quot;Stores&quot;)&#10;  #abline(v=(lambda * people)[s], col=&quot;Blue&quot;)&#10;  abline(v=stores[s], col=&quot;Red&quot;, lwd=2)&#10;}&#10;plot.sim(&quot;High&quot;); plot.sim(&quot;Low&quot;)&#10;hist(t.sim, main=&quot;T statistic in simulation&quot;, xlab=&quot;T&quot;, freq=FALSE)&#10;abline(v = t.stat(stores, people)*c(-1,1), col=&quot;Red&quot;, lwd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-02-24T18:49:30.823" Id="87707" LastActivityDate="2014-02-24T19:00:25.523" LastEditDate="2014-02-24T19:00:25.523" LastEditorUserId="919" OwnerUserId="919" ParentId="87691" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;Say I have a bag with 10 balls (either red or blue), and I know that there are 8 balls that are red. How do I calculate the probability of drawing a a red ball $&amp;gt;75\%$ of the time if the first ball I picked without replacements is red? Hope this is clear. Thanks! &lt;/p&gt;&#10;" ClosedDate="2014-02-24T20:50:03.063" CommentCount="2" CreationDate="2014-02-24T19:53:00.677" Id="87714" LastActivityDate="2014-02-24T19:58:57.847" LastEditDate="2014-02-24T19:58:57.847" LastEditorUserId="32036" OwnerUserId="40818" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;bayesian&gt;&lt;binomial&gt;&lt;conditional-probability&gt;" Title="Is there a way to calculate the probability of drawing X+ &gt; y% of time given X-" ViewCount="25" />
  <row AnswerCount="1" Body="&lt;p&gt;When doing a multiple binomial regression, is it possible to do the regression with a categorical variable as x at the same time as y is categorical? Most of my x-variables are continuous except for two, while my y-variables is all categorical.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-24T20:16:15.313" Id="87717" LastActivityDate="2014-02-24T21:34:19.597" LastEditDate="2014-02-24T21:34:19.597" LastEditorUserId="22468" OwnerUserId="40116" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;binomial&gt;" Title="Binomial regression with categorical explanatory variable" ViewCount="31" />
  <row Body="&lt;p&gt;&quot;nodesize: Minimum size of terminal nodes.  Setting this number larger&#10;          causes smaller trees to be grown (and thus take less time).&#10;          Note that the default values are different for classification&#10;          (1) and regression (5).&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Check out this parameter in the randomForest package.  By upping the number of observations in terminal nodes you will have smaller (in terms of depth) trees.  &quot;Over-fit&quot; trees are generally big (deep in terms of depth).  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-24T20:18:11.487" Id="87718" LastActivityDate="2014-02-24T20:18:11.487" OwnerUserId="40582" ParentId="87662" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have about 900 flies that either like sugar or are neutral (about 50% in each group). However, on closer inspection I do notice a trend that sugar liking flies are from bins 1-10 (each bin has about 45 flies), while the neutral ones tend to be from 11-20 bins. I'm not sure why, but is there a statistical test I can do to show that sugar liking flies tend to belong to bins where there are flies that also likes sugar and vice-versa? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-24T20:53:43.107" Id="87721" LastActivityDate="2014-02-25T01:08:25.750" LastEditDate="2014-02-25T01:08:25.750" LastEditorUserId="183" OwnerUserId="40818" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;statistical-significance&gt;&lt;conditional-probability&gt;" Title="Is there a statistical test to show that sugar liking flies tend to belong to bins where there are flies that also likes sugar and vice-versa?" ViewCount="52" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I've conducted a multiple linear regression with 3 predictors, A, B, and C, all three on the same scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the three predictors displays a smaller range than the other two (range A=0.53; B=0.98; C=0.86), as well as a smaller standard deviation (SD A=0.18; B=0.30; C=0.25).This is precisely the predictor that does not show a significant effect, while the other two (B and C) do.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to correct the coefficient (or standardized coefficient) of A for its restricted range.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have only found corrections for the correlation coefficient, such as Thorndike's case 2 (see, e.g., here: &lt;a href=&quot;http://pareonline.net/pdf/v14n5.pdf&quot; rel=&quot;nofollow&quot;&gt;http://pareonline.net/pdf/v14n5.pdf&lt;/a&gt; ).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How can coefficients in multiple regression be corrected?&#10;Could I perhaps apply Thorndike's correction to the partial correlation between A and Y, while controlling for B and C?&#10;(ideally, however, I would like to have a correction of the regression coefficient)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The correction requires knowing the unrestricted SD. This is not possible, but I can use the SD of one of the other predictors. The argument would go: &quot;The lack of an effect for A, as opposed to B and C might be due to its restricted range; with this correction we show that assuming for A as large an SD as for B, then A's coefficient would still be small (or is now as large as that for B)&quot;. Is this at all sound?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-24T21:12:24.450" Id="87724" LastActivityDate="2014-02-26T01:00:29.360" LastEditDate="2014-02-26T01:00:29.360" LastEditorUserId="40821" OwnerUserId="40821" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;multiple-regression&gt;" Title="Correcting for restriction of range in multiple regression" ViewCount="180" />
  
  <row AcceptedAnswerId="88283" AnswerCount="1" Body="&lt;p&gt;I am applying $\epsilon$- and $\nu$-regression to sample data, and I discovered I had different results in terms of the count of support vectors. &lt;/p&gt;&#10;&#10;&lt;p&gt;When I have fewer support vectors, does it mean that the model is simpler?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-25T00:40:57.843" Id="87743" LastActivityDate="2014-02-28T19:04:54.380" LastEditDate="2014-02-28T16:09:44.627" LastEditorUserId="35099" OwnerUserId="16107" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;svm&gt;&lt;model&gt;" Title="Do fewer support vectors imply a simpler model?" ViewCount="86" />
  <row Body="&lt;p&gt;If the model for your data is&#10;$$&#10;y_{ij} = \mu_i + \varepsilon_{ij}, \quad {\rm E}_\xi \varepsilon_{ij} = 0, {\rm V}_\xi \varepsilon_{ij} = \sigma^2&#10;$$&#10;where the subindex $\xi$ stands for the model expectations, then the cluster totals $T_i[y] = \sum_j y_{ij}$ have the model moments&#10;$$&#10;{\rm E}_\xi T_i[y] = M_i \mu, \quad {\rm V}_\xi T_i[y] = M_i \sigma^2,&#10;$$&#10;so there clearly is proportionality to the cluster size.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to understand design-based inference, think in totals. These are the only linear statistics; everything else is a ratio or another non-linear statistic that requires a delta-method.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-25T02:58:57.567" Id="87752" LastActivityDate="2014-02-25T02:58:57.567" OwnerUserId="5739" ParentId="86726" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;This is not homework. Just practicing for an upcoming exam. Question is taken from a web pdf : &lt;a href=&quot;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following process. We have two coins, one of which is fair, and the&#10;other of which has heads on both sides. We give these two coins to our friend,&#10;who chooses one of them at random (each with probability 1/2). During the&#10;rest of the process, she uses only the coin that she chose. She now proceeds&#10;to toss the coin many times, reporting the results. We consider this process&#10;to consist solely of what she reports to us.&lt;/p&gt;&#10;&#10;&lt;p&gt;(a) Given that she reports a head on the nth toss, what is the probability&#10;that a head is thrown on the (n + 1)st toss?&lt;/p&gt;&#10;&#10;&lt;p&gt;(b) Consider this process as having two states, heads and tails. By computing&#10;the other three transition probabilities analogous to the one in part (a),&#10;write down a \transition matrix&quot; for this process.&lt;/p&gt;&#10;&#10;&lt;p&gt;(c) Now assume that the process is in state &quot;heads&quot; on both the (n - 1)st&#10;and the nth toss. Find the probability that a head comes up on the&#10;(n + 1)st toss.&lt;/p&gt;&#10;&#10;&lt;p&gt;(d) Is this process a Markov chain?&lt;/p&gt;&#10;&#10;&lt;p&gt;Solution is given here: Chapter 11 question 19 :&lt;a href=&quot;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Answersodd-10-14-08.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Answersodd-10-14-08.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, what is the best approach in assigning state i a state? &quot;state 1=...&quot;?&#10;Also, what is the difference between the Markov process and Markov chain?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-25T03:01:50.590" Id="87753" LastActivityDate="2014-02-25T03:31:08.490" LastEditDate="2014-02-25T03:31:08.490" LastEditorUserId="40837" OwnerUserId="40837" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;markov-process&gt;&lt;summary-statistics&gt;&lt;markov-chain&gt;" Title="Markov chain and process" ViewCount="80" />
  
  <row Body="&lt;p&gt;Update: I just see Charles's reply above. I think the documents he pointed to are very helpful. Thanks  Patrick Coulombe for discussion.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your &lt;code&gt;color&lt;/code&gt; variable has only two possible values 'white' and 'red', you will have only &lt;code&gt;coefficients&lt;/code&gt; for &lt;code&gt;colorwhite&lt;/code&gt; and &lt;code&gt;colorwhite:bar&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The point is that you have &lt;code&gt;intercept&lt;/code&gt;. And the base case is &lt;code&gt;color&lt;/code&gt; = &lt;code&gt;red&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The&lt;code&gt;coefficient&lt;/code&gt; for &lt;code&gt;colorwhite&lt;/code&gt; actually reflect the effect size of &lt;code&gt;white&lt;/code&gt;, being compared to that of &lt;code&gt;red&lt;/code&gt;, given the &lt;code&gt;bar&lt;/code&gt; as 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;The&lt;code&gt;coefficient&lt;/code&gt; for &lt;code&gt;colorwhite:bar&lt;/code&gt; actually reflect the effect size of the combination of (interaction between) &lt;code&gt;white&lt;/code&gt; and a unit of &lt;code&gt;bar&lt;/code&gt; value, being compared to that of the combination of (interaction between) &lt;code&gt;red&lt;/code&gt; and a unit of &lt;code&gt;bar&lt;/code&gt; value, given the same &lt;code&gt;bar&lt;/code&gt; value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please check this &lt;a href=&quot;http://en.wikipedia.org/wiki/Effect_size&quot; rel=&quot;nofollow&quot;&gt;wikipedia page&lt;/a&gt;: &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Always present effect sizes for primary outcomes...If the units of measurement are meaningful on a practical level (e.g., number of cigarettes smoked per day), then we usually prefer an unstandardized measure (&lt;em&gt;regression coefficient&lt;/em&gt; or mean difference) to a standardized measure (r or d).&lt;/p&gt;&#10;&#10;&lt;p&gt;— L. Wilkinson and APA Task Force on Statistical Inference (1999, p. 599)&lt;/p&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;p&gt;The term effect size can refer to a standardized measures of effect (such as r, Cohen's d, and odds ratio), or to an unstandardized measure (e.g., the raw difference between group means and unstandardized regression coefficients). &quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;And also &lt;a href=&quot;http://dss.princeton.edu/online_help/analysis/interpreting_regression.htm&quot; rel=&quot;nofollow&quot;&gt;this page&lt;/a&gt;:&#10;&quot;Remember that regression analysis is used to produce an equation that will predict a dependent variable using one or more independent variables. This equation has the form&lt;/p&gt;&#10;&#10;&lt;p&gt;Y = b1X1 + b2X2 + ... + A&#10;where Y is the dependent variable you are trying to predict, X1, X2 and so on are the independent variables you are using to predict it, b1, b2 and so on are the coefficients or multipliers that describe &lt;em&gt;the size of the effect&lt;/em&gt; the independent variables are having on your dependent variable Y, and A is the value Y is predicted to have when all the independent variables are equal to zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;p&gt;In simple or multiple linear regression, the &lt;em&gt;size of the coefficient&lt;/em&gt; for each independent variable gives you &lt;em&gt;the size of the effect&lt;/em&gt; that variable is having on your dependent variable, and the sign on the coefficient (positive or negative) gives you the direction of the effect. In regression with a single independent variable, the coefficient tells you how much the dependent variable is expected to increase (if the coefficient is positive) or decrease (if the coefficient is negative) when that independent variable increases by one. In regression with multiple independent variables, the coefficient tells you how much the dependent variable is expected to increase when that independent variable increases by one, &lt;em&gt;holding all the other independent variables constant&lt;/em&gt;. Remember to keep in mind the units which your variables are measured in.&#10;&quot;&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-02-25T03:27:40.217" Id="87754" LastActivityDate="2014-02-25T04:42:47.973" LastEditDate="2014-02-25T04:42:47.973" LastEditorUserId="18265" OwnerUserId="18265" ParentId="87751" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="87865" AnswerCount="3" Body="&lt;p&gt;The normal distribution can be generalized into the multivariate normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can the logistic distribution also be generalized into a similar multivariate distribution? &#10;Is there a multivariate generalization of the logistic distribution which depends on the covariance matrix $\Sigma$, similar to the multivariate normal distribution? The multivariate distribution should be such that its marginals are univariate logistic distributions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-25T10:19:17.900" Id="87787" LastActivityDate="2015-02-12T17:01:58.717" LastEditDate="2014-02-26T12:00:17.757" LastEditorUserId="31221" OwnerUserId="31221" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;&lt;covariance&gt;&lt;multivariate&gt;" Title="Multivariate logistic distribution" ViewCount="282" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a number of categorical variables, each with varying numbers of levels:&lt;/p&gt;&#10;&#10;&lt;p&gt;gender of client (m,f)&#10;age group (0-8, etc.)&#10;diagnosis (mild, moderate, severe, profound)&#10;location (community, residential)&#10;restrictive interventions used (physical restraint, chemical restraint, seclusion, etc.)&#10;(and more)&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to see if certain group characteristics, e.g. a client's age or gender (or both combined) are more predictive than others of receiving a particular restrictive intervention. &lt;/p&gt;&#10;&#10;&lt;p&gt;E.g. Does gender predict type of restrictive intervention received more strongly than range of diagnosis? What is the strongest predictor of seclusion?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm pretty much at a loss in terms of what test to use to answer these questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm at a basic level of statistics, so any help would be appreciated. Cheers! &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-25T12:58:13.320" Id="87799" LastActivityDate="2014-02-25T12:58:13.320" OwnerUserId="26232" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;categorical-data&gt;&lt;predictor&gt;" Title="How to analyse the relationship between many categorical variables" ViewCount="65" />
  <row Body="&lt;p&gt;Still don't have a good grasp of what you're looking for. But if you're comparing binary classification models I wouldn't recommend just using AUC (a measure of discrimination)            &lt;/p&gt;&#10;&#10;&lt;p&gt;Usually one uses three metrics together, choosing one from each of the three categories below. :&lt;br&gt;&#10;(1) Global measure:  scaled Brier or N's R-sq. The difference between Brier and N's R-sq may be significant due to different penalty functions&lt;br&gt;&#10;The Brier score is often preferred, but should usually be converted to a scale Brier score (thought this is less of an issue of all analysis on same dataset).&lt;br&gt;&#10;(2) Discrimination: AUC, discrimination slope&lt;br&gt;&#10;(3) Calibration:  calibration slope with calibration or validation graph                   &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-25T14:22:04.467" Id="87813" LastActivityDate="2014-02-26T19:46:09.280" LastEditDate="2014-02-26T19:46:09.280" LastEditorUserId="34658" OwnerUserId="34658" ParentId="87774" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a multivariate Gaussian parameterised by a mean vector $\mu$ and a precision matrix $\Sigma$. Now, I want to set the Gaussian along a given dimension $i$ to a point mass i.e. I set the corresponding diagonal entry i.e. $\Sigma_{ii}$ to infinity (zero variance) and also set all the entries in the ith row and column to 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what I would like to know is how should the mean vector be adjusted to account for this? Also, if someone would be kind enough to describe an intuitive reason of how changing the covariance parameters affects the mean vector, that would be great.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-25T18:36:44.713" Id="87846" LastActivityDate="2014-02-26T17:28:39.213" LastEditDate="2014-02-26T17:28:39.213" LastEditorUserId="2970" OwnerUserId="36540" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;mean&gt;&lt;covariance&gt;&lt;basic-concepts&gt;&lt;multivariate&gt;" Title="Changing a multivariate Gaussian along a dimension" ViewCount="46" />
  <row AnswerCount="0" Body="&lt;p&gt;I am interested in estimating a break point in response to one explanatory variable, while also including other variables as terms in the linear model. One reason to include additional terms is to account for spatial dependency/autocorrelation. The following code was adapted from code provided by @jbowman &lt;a href=&quot;http://stats.stackexchange.com/questions/19772/&quot;&gt;here&lt;/a&gt;, and also referred to &lt;a href=&quot;http://stats.stackexchange.com/questions/29327/&quot;&gt;here&lt;/a&gt;.  It finds the break point for the model &lt;code&gt;lm(Rare25~epi.i)&lt;/code&gt;.  What I would like to do is find a break point for &lt;code&gt;Rare25&lt;/code&gt; at a value of &lt;code&gt;epi.i&lt;/code&gt;, while including the variable &lt;code&gt;l.dist.mrc&lt;/code&gt; in a model such as &lt;code&gt;lm(Rare25 ~ epi.i + l.dist.mrc)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Data:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;epi.i&amp;lt;-c(7, 8, 9, 10, 8.01, 6, 7.01, 5.999, 4.003, 12, 2.01, 7.02, 10.01, 8.2, 5.9, 3.9, 6.999, 4.0001, 3.99, 6.001, 8.001, 5.99, 7.9, 6.99, 3.98)&#10;Rare25&amp;lt;-c(4.471429, 4.551474, 4.204894, 4.456710, 3.504348, 4.175824, 4.298193, 4.406838, 3.058707, 4.451128, 1.000000, 4.327893, 3.580541, 4.082432, 3.630869, 4.352130, 3.919075, 3.795205, 3.380952, 2.993347, 3.775886, 3.766723, 3.852396, 3.923977, 4.308840)&#10;l.dist.mrc&amp;lt;-c(2.7839, 2.7839, 2.7839, 3.0232, 3.0232, 2.5051, 2.5051, 2.7041, 2.9965, 3.0773, 3.4457, 3.1202, 3.1936, 3.4362, 3.3718, 2.7641, 2.7641, 3.7904, 3.4457, 3.7904, 3.1202, 2.5302, 2.5302 ,3.1242, 3.1241)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Basis functions&#10;bp = 4&#10;b1 &amp;lt;- function(x, bp) ifelse(x &amp;lt; bp, bp - x, 0)&#10;b2 &amp;lt;- function(x, bp) ifelse(x &amp;lt; bp, 0, x - bp)&#10;&#10;#Wrapper&#10;foo &amp;lt;- function(bp)&#10;{&#10;mod &amp;lt;- lm(Rare25 ~ b1(epi.i, bp) + b2(epi.i, bp))&#10;deviance(mod)&#10;}&#10;&#10;search.range &amp;lt;- c(min(epi.i),max(epi.i))&#10;foo.opt &amp;lt;- optimize(foo, interval = search.range)&#10;bp &amp;lt;- foo.opt$minimum&#10;bp&#10;&#10;# confidence interval&#10;foo.root &amp;lt;- function(bp, tgt)&#10;{&#10;foo(bp) - tgt&#10;}&#10;tgt &amp;lt;- foo.opt$objective + qchisq(0.95,1)&#10;&#10;lb95 &amp;lt;- uniroot(foo.root, lower=(search.range[1]), upper=bp, tgt=tgt)&#10;ub95 &amp;lt;- uniroot(foo.root, lower=bp, upper=search.range[2], tgt=tgt)&#10;lb95$root&#10;ub95$root&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-02-25T18:38:55.040" FavoriteCount="0" Id="87847" LastActivityDate="2014-02-25T18:48:30.883" LastEditDate="2014-02-25T18:48:30.883" LastEditorUserId="32036" OwnerUserId="38474" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;change-point&gt;&lt;threshold&gt;&lt;piecewise-linear&gt;" Title="Estimating break point in a broken stick / piecewise linear model for one variable while including other as model terms" ViewCount="170" />
  
  
  <row Body="&lt;p&gt;Here is an example in R that shows the theoretical concepts in action.  10,000 trials of flipping a coin 10,000 times that has a probability of heads of .0001 compared to 10,000 trials of flipping a coin 10,000 times that has a probability of heads of .00011&lt;/p&gt;&#10;&#10;&lt;p&gt;t.test(rbinom(10000, 10000, .0001), rbinom(10000, 10000, .00011))&lt;/p&gt;&#10;&#10;&lt;p&gt;t = -8.0299, df = 19886.35, p-value = 1.03e-15&#10;alternative hypothesis: true difference in means is not equal to 0 &#10;95 percent confidence interval:&#10; -0.14493747 -0.08806253 &#10;sample estimates:&#10;mean of x mean of y &#10;   0.9898    1.1063 &lt;/p&gt;&#10;&#10;&lt;p&gt;The difference in the mean is relatively closed to 0 in terms of human perception, however it is very statistically different than 0.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-25T22:00:14.940" Id="87869" LastActivityDate="2014-02-25T22:00:14.940" OwnerUserId="40582" ParentId="87856" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The definition of exponential family is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p(x|\theta) = h(x)\exp(\theta^T\phi(x) - A(\theta)),&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $A(\theta)$ is the log partition function. Now one can prove that the following three things hold for 1D case (and they generalize to higher dimensions--you can look into properties of exponential families or log partition):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$ \frac{dA}{d\theta} = \mathbb{E}[\phi(x)]$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$ \frac{d^2A}{d\theta^2} = \mathbb{E}[\phi^2(x)] -\mathbb{E}[\phi(x)]^2 = {\rm var}(\phi(x)) $&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$ \frac{ \partial ^2A}{\partial\theta_i\partial\theta_j} = \mathbb{E}[\phi_i(x)\phi_j(x)] -\mathbb{E}[\phi_i(x)] \mathbb{E}[\phi_j(x)] = {\rm cov}(\phi(x)) \Rightarrow \Delta^2A(\theta) = {\rm cov}(\phi(x))$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The above result prove that $A(\theta)$ is convex(as ${\rm cov}(\phi(x))$ is positive semidefinite). Now we take a look at likelihood function for MLE:  &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;p(\mathcal{D}|\theta)                 &amp;amp;= \bigg[\prod_{i=1}^{N}{h(x_i)}\bigg]\ \exp\!\big(\theta^T[\sum_{i=1}^{N}\phi(x_i)] - NA(\theta)\big)  \\&#10;\log\!\big(p(\mathcal{D}|\theta)\big) &amp;amp;= \theta^T\bigg[\sum_{i=1}^{N}\phi(x_i)\bigg] - NA(\theta)  \\&#10;                                      &amp;amp;=  \theta^T[\phi(\mathcal{D})] - NA(\theta)&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Now $\theta^T[\phi(\mathcal{D})]$ is linear in theta and $-A(\theta)$ is concave. Therefore, there is a unique global maximum.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a generalized version called curved exponential family which would also be similar. But most of the proofs are in canonical form. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-02-25T23:33:30.430" Id="87884" LastActivityDate="2014-05-14T21:53:15.297" LastEditDate="2014-05-14T21:53:15.297" LastEditorUserId="7290" OwnerUserId="29568" ParentId="87615" PostTypeId="2" Score="7" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using &lt;a href=&quot;http://en.wikipedia.org/wiki/RapidMiner&quot; rel=&quot;nofollow&quot;&gt;RapidMiner&lt;/a&gt; to perform linear regression with ridge parameter 1$\text{E}$ -8, min tolerance 0.05 and M5 prime for feature elimination. The &lt;code&gt;std coefficient&lt;/code&gt; parameters appear to be excessively high. What are some possible causes of this?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;property        user_created   user_question_count    intercept&#10;coefficient     -0.076         0.046                  0.767&#10;std. error      0.012          0.012                  0.004&#10;std coefficient 5.59000E12     -9.271701831E12        ?&#10;tolerance       0.984          0.984                  ?&#10;t-Stat          -6.523         3.962                  199.67&#10;p-value         0              0.000                  0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-02-25T23:34:07.497" Id="87885" LastActivityDate="2014-02-26T02:48:54.637" LastEditDate="2014-02-26T02:48:54.637" LastEditorUserId="32036" OwnerUserId="31208" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;ridge-regression&gt;" Title="What can be a cause of a extremely high standard coefficient?" ViewCount="52" />
  <row AcceptedAnswerId="87893" AnswerCount="1" Body="&lt;p&gt;While reading, I came across the puzzling statement that the sample mean and variance &lt;em&gt;are uncorrelated only in symmetric distributions&lt;/em&gt; and there is strong correlation if the distribution is heavily skewed.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, is it true? I already know this holds for the case of a &lt;em&gt;normal&lt;/em&gt; population, but can that proved for &lt;em&gt;every&lt;/em&gt; symmetric distribution? Although I think the result is counter-intuitive, I would appreciate simple arguments.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-25T23:36:47.893" Id="87886" LastActivityDate="2014-02-26T00:31:01.733" LastEditDate="2014-02-26T00:29:48.437" LastEditorUserId="31420" OwnerUserId="31420" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;sampling&gt;&lt;references&gt;" Title="Why is a symmetric distribution sufficient for the sample mean and variance to be uncorrelated?" ViewCount="69" />
  <row Body="&lt;p&gt;Have you tried plotting this using &lt;a href=&quot;http://cran.r-project.org/web/packages/ggplot2/index.html&quot; rel=&quot;nofollow&quot;&gt;ggplot2&lt;/a&gt; in R? It has a nice semi transparency feature with the &lt;a href=&quot;http://cran.r-project.org/web/packages/Cairo/index.html&quot; rel=&quot;nofollow&quot;&gt;Cairo&lt;/a&gt; package which makes guesstimating the mean for such residual plots easy. For example you could have each point semi transparent and you could visually check if they are centered around 0. But overall by looking at the image you posted, no reason to think otherwise.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-26T00:01:10.327" Id="87889" LastActivityDate="2014-02-26T00:01:10.327" OwnerUserId="13516" ParentId="87793" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Too long for a comment:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;It is not true that sample mean and variance are always independent if the distribution is symmetric.  For example, take a sample from a distribution which takes values $\pm1$ with equal probability: if the sample mean is $\pm1$ then the sample variance will be $0$, while if the sample mean is not $\pm1$ then the sample variance will be positive.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It is true that the distributions of the sample mean and variance have zero correlation (if they have a correlation) if the distribution is symmetric.  This is because $E(s_X^2|\bar{X}-\mu=k)=E(s_X^2|\bar{X}-\mu=-k)$ by symmetry.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Neither of these points deal with the statement in the book, which says &lt;em&gt;only if&lt;/em&gt; but not &lt;em&gt;if&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For an example of the final statement, if most of a distribution is closely clustered but there can be the occasional particular very large value, then the sample mean will be largely determined by the number of very large values in the sample, and the more of them there are, the higher the sample variance will be too, leading to high correlation. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-26T00:25:26.597" Id="87893" LastActivityDate="2014-02-26T00:31:01.733" LastEditDate="2014-02-26T00:31:01.733" LastEditorUserId="2958" OwnerUserId="2958" ParentId="87886" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to figure out the relationship between $\text{Cov}(x_i^2, e_i^2), V$ and $V_0$, where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$V=$ asymptotic variance of $\sqrt{n(\hat{\beta}-β)}$ under heteroskedasticity, and&lt;br&gt;&#10;$V_0=$ asymptotic variance of $\sqrt{n(\hat{\beta}-β)}$ under homoscedasticity&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I am given that that the expected value of x squared, E(x_i^2), is equal to one.&lt;/p&gt;&#10;&#10;&lt;p&gt;From this, I get that: &lt;/p&gt;&#10;&#10;&lt;p&gt;$V = 1/N ∑〖(x_i^2 \cdot e_i^2)〗$&lt;br&gt;&#10;$V_0=σ^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am certain that this is right. Now, using this I get that  &lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Cov}(x_i^2, e_i^2) = V - 1/N ∑〖(e_i^2)〗$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now to get $V_0$ into this equation, by mathematical manipulation, I obtained $1/N ∑〖(e_i^2)〗= V_0$, and thus that $\text{Cov}(x_i^2, e_i^2) = V-V_0$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct? I am afraid that the substitution of $1/N ∑〖(e_i^2)〗$ for $V_0$ is only possible if I assume homoscedasticity. Is this worry justified? If so, how do I proceed? &#10;Grateful for a response.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case anyone has the energy to read this, here is how I derived the above (I apologize for the notation, but I do not know how to change it): &lt;/p&gt;&#10;&#10;&lt;p&gt;Cov(x_i^2 〖,e〗_i^2 )=E(x_i^2 e_i^2 )-E(x_i^2 )E(e_i^2 )&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                  =V-E(x_i^2 )E(e_i^2 )&#10;&#10;                  =V-E(e_i^2 ),  as E(x_i^2 )=1&#10;&#10;                  =V-1/N ∑〖(e_i^2)〗&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now,&lt;/p&gt;&#10;&#10;&lt;p&gt;e_i=y_i-X_i'β ̂ = X_i'β+ε_i-X_i'β ̂=ε_i+X_i'β-X_i'β ̂,&lt;/p&gt;&#10;&#10;&lt;p&gt;e_i^2=ε_i^2+(β-β ̂ )'X_iX_i'(β-β ̂ )+2ε_i X_i'(β-β ̂ ),&lt;/p&gt;&#10;&#10;&lt;p&gt;σ ̂^2=1/N ∑(e_i^2 )=1/N ∑(ε_i^2)+(β-β ̂ )'1/N ∑(X_iX_i') (β-β ̂) +2 1/N ∑ε_iX_i'(β-β ̂ ) &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;              (1)              (2)                               (3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(2) and (3) converge in probability to zero, as it is assumed that beta-hat is a consistent estimator for beta. (1) converges in probability to σ^2. Therefore, &lt;/p&gt;&#10;&#10;&lt;p&gt;1/N ∑(e_i^2 ) converges in probability to σ^2 &lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, we have that &#10;Cov(x_i^2,e_i^2 )=E(x_i^2 e_i^2 )-E(x_i^2 )E(e_i^2 )=V-1/N ∑(e_i^2 ) =V-V0&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-26T01:10:29.287" Id="87897" LastActivityDate="2014-02-26T04:59:31.320" LastEditDate="2014-02-26T04:59:31.320" LastEditorUserId="40908" OwnerUserId="40908" PostTypeId="1" Score="3" Tags="&lt;covariance&gt;&lt;heteroscedasticity&gt;&lt;asymptotics&gt;" Title="Relationship between $\text{Cov}(x_i^2, e_i^2)$, the asymptotic variance of b under homoscedasticity and heteroscedasticity?" ViewCount="39" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am now working with Stata and I found out I have a unit root in my regression. How can I correct for this, because I can read everywhere what the test does but if you have a unit root, what to do next?? Sorry, it is one of my first times working with Stata so it is all really confusing..&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-26T11:18:33.200" Id="87943" LastActivityDate="2014-02-26T11:33:55.307" LastEditDate="2014-02-26T11:33:55.307" LastEditorUserId="22047" OwnerUserId="40929" PostTypeId="1" Score="0" Tags="&lt;stata&gt;&lt;unit-root&gt;" Title="Augmented Dickey-Fuller test" ViewCount="97" />
  <row Body="&lt;p&gt;As the norm is applied to vectors in dimension $n$, $$|| x^{(i)} - \mu_{c^{(i)}}||^2 = \sum^n_{j=1} (x^{(i)}_j - \mu_{c^{(i)},j})^2$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;with $x^{(i)} = (x^{(i)}_1,..., x^{(i)}_n)$ and $\mu_{c^{(i)}}=(\mu_{c^{(i)},1},..., \mu_{c^{(i)},n})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The summation is first on the $m$ points of the sample ($x^{(i)}$) and then on their $n$ components, sothat:&#10;$$J(c,\mu) =\sum^m_{i=1}\sum^n_{j=1} (x^{(i)}_j - \mu_{c^{(i)},j})^2$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-26T12:31:48.227" Id="87953" LastActivityDate="2014-02-26T12:31:48.227" OwnerUserId="12865" ParentId="87950" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have been reading on cook's distance to identify outliers which have high influence on my regression. In Cook's original study he says that a cut-off rate of 1 should be comparable to identify influencers. However, various other studies use 4/n or 4/(n-k-1) as a cut-off.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my study, none of my residuals have a D higher than 1. However, if I use 4/n as a cutoff (4/149= .026), then there are various data points which are considered influencers. I decided to test whether removing these data points would make a difference to my general linear regression. All my IVs retained their significance and no obvious change was apparent. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, should I retain all my data points and use the cut-off rate of 1? Or remove them? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-26T13:31:54.867" Id="87962" LastActivityDate="2014-02-26T13:31:54.867" OwnerUserId="40183" PostTypeId="1" Score="3" Tags="&lt;outliers&gt;" Title="Cook's distance cut-off value" ViewCount="564" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Are there any major potential problems with using negative binomial regression (xtnbreg) with random effects and lagged dependent/independent variables. (Time-series cross-section data)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm analyzing a piece of research that uses time-series cross section data for 138 countries and looks to explain the number of yearly labour disputes by fdi flows, levels of democracy, etc. Due to overdispersion, xtnbreg is used. RE is justified on the basis that there is a large panel. Lagged dependent variable is included (as it is assumed strikes lead to more strikes),as are lagged fdi flows. I'm thinking more of problems to do with assumptions and fit, as I am trying to understand the methods used&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-26T14:17:12.793" Id="87967" LastActivityDate="2014-02-26T16:10:25.910" LastEditDate="2014-02-26T16:10:25.910" LastEditorUserId="40939" OwnerUserId="40939" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;stata&gt;&lt;random-effects-model&gt;&lt;negative-binomial&gt;&lt;dynamic-regression&gt;" Title="What are the potential problems associated with using negative binomial regression with random effects?" ViewCount="80" />
  <row Body="&lt;p&gt;John has already given a spot on answer. Just as an addendum (which is just a bit too long as a comment), let me just add a quote from Cohen himself (from &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt;, 1988):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The terms &quot;small,&quot;, &quot;medium,&quot;, and &quot;large&quot; are relative, not only to each other, but to the area of behavioral science or even more particularly to the specific content and research method being employed in any given investigation [...]. In the face of this relativity, there is a certain risk inherent in offering conventional operational definitions for these terms [...]. This risk is nevertheless accepted in the belief that more is to be gained than lost by supplying a common conventional frame of reference &lt;strong&gt;which is recommended for use only when no better basis for estimating the ES index is available&lt;/strong&gt;. (p. 25).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The emphasis is mine. In many cases, there is a better basis, since effects are often measured with scales for which we have some prior knowledge/intuition about the meaning of the raw units and the amount of variability in the outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;In his famous 1994 paper (&quot;The earth is round (p &amp;lt; .05)&quot;), Cohen himself also recommended moving away from 'standardized' measures of effect and instead advocated working with raw measures of effect. Another quote:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To work constructively with &quot;raw&quot; regression coefficients and confidence intervals, &lt;strong&gt;psychologists have to start respecting the units they work with&lt;/strong&gt;, or develop measurement units they can respect enough so that researchers in a given field or subfield can agree to use them. In this way, there can be hope that researchers' knowledge can be cumulative. (p. 1001).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Again, emphasis is mine. It is unfortunate that Cohen got his name attached to these 'canned' values, when in fact he was quite careful not to overemphasize their meaning.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-26T14:17:27.770" Id="87968" LastActivityDate="2014-02-26T14:17:27.770" OwnerUserId="1934" ParentId="23775" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;$R^2$ doesn't have to be equal to $\beta$. you either have a rare coincidence, or reading the same field from the fit object somehow.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-02-26T14:34:07.197" Id="87973" LastActivityDate="2014-02-26T14:34:07.197" OwnerUserId="36041" ParentId="87963" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;It is not enterely correct. You should find:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(\frac{b}{a}\right)^2 \operatorname{var}[ B(a-b)]+(-b) ^2 \operatorname{var}[ B(b)]+ 2 \frac{b}{a}(-b)\operatorname{Cov}(B(a-b), B(b))$$&lt;/p&gt;&#10;&#10;&lt;p&gt;because $\operatorname{var}(aX+bY) = a^2\operatorname{var}(X) + b^2\operatorname{var}(Y) + 2ab\operatorname{Cov}(X,Y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;You have now to use classical properties of the covariance in a Browian motion: $$\operatorname{Cov}(B(t),B(s)) = \min(s,t).$$ You're almost done ;)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-26T14:46:21.200" Id="87977" LastActivityDate="2014-05-05T18:28:23.160" LastEditDate="2014-05-05T18:28:23.160" LastEditorUserId="919" OwnerUserId="12865" ParentId="87974" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If your response is a proportion, percentage or anything similiar that can only take values in $(0,1)$ you would typically use beta regression, not binomial&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-26T14:47:06.793" Id="87979" LastActivityDate="2014-02-26T14:52:19.863" LastEditDate="2014-02-26T14:52:19.863" LastEditorUserId="30201" OwnerUserId="30201" ParentId="87956" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $x[n]$ be some time series in 1D or $x[m,n]$ in 2D, of length $N$ (resp. $N^2$) How can I assess whether it is &lt;em&gt;stationary&lt;/em&gt;? At least in the weak sense. I can check whether the &lt;em&gt;stdev&lt;/em&gt; remains constant, but this is only a necessary condition.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-02-26T15:37:15.940" Id="87986" LastActivityDate="2014-02-26T15:59:56.190" LastEditDate="2014-02-26T15:59:56.190" LastEditorUserId="8580" OwnerUserId="40946" PostTypeId="1" Score="1" Tags="&lt;stationarity&gt;" Title="Checking whether a given sample is stationary" ViewCount="31" />
  
  
  <row AcceptedAnswerId="88057" AnswerCount="1" Body="&lt;p&gt;I am new to R, and don't see these questions answered anywhere in documentation (though I could be wrong).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I am using the following nomenclature to run my mixed-effects logistic regression, based on instructions from another site:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;output &amp;lt;- glmer(DV ~ IV1 + IV2 + (1 | RE), family = binomial, nAGQ = 10)&lt;/code&gt;&lt;br&gt;&#10;RE is a factor with several levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;This works. But I'm wondering why it's necessary to use the &lt;code&gt;(1 | RE)&lt;/code&gt; syntax instead of just &lt;code&gt;DV~IV1+IV2 | RE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am running two mixed effects logistic regressions. On one of them I can view the random effects intercepts using &lt;code&gt;ranef()&lt;/code&gt;. But I get all 0s when I run ranef on the output of the other one. Both regressions/data are ostensibly the same. What do all 0s for the random effects intercepts mean?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-02-26T20:01:09.063" Id="88036" LastActivityDate="2014-02-26T22:26:34.163" LastEditDate="2014-02-26T20:35:55.643" LastEditorUserId="4485" OwnerUserId="37742" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;mixed-model&gt;" Title="Mixed-effect logistic regression in R - questions" ViewCount="107" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two continuous stochastic Markov processes: the concentration readout of two proteins in a cell over time. These are shown in this figure, where the blue line is the unbounded protein, and all other lines are zero bounded proteins, but I am interested in just one of them. (The y axis is concentration, and sorry for the poor quality image).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vV5DX.png&quot; alt=&quot;Protein concentrations over time&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In an oversimplified model, the two proteins are produced under the same regime and therefore their concentration distributions (over the whole simulation period) should be identical. However the concentration of protein 1 is close to zero, so is zero-inflated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe there is a proper terminology which I am neglecting because I don't know the field, but I want to say that the distributions of the driving processes for the two proteins are the same, but the resultant protein concentration distributions are different, (Gaussian and Gamma maybe?) seen in the figure below. This is the case for my simplistic model containing these two proteins.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kzVYj.png&quot; alt=&quot;These are the concentration distributions I get&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now in the real biological system, experiments show a similar pattern, the concentration distribution over the observation period looks Gaussian for the non-bounded protein, but long-tailed for the zero-bounded protein. I want to know whether they are driven by the same process. Is there some way I can compare their distributions, accounting for the fact that one of them is bounded at zero?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;I.e.&lt;/em&gt; I want to ask, is the difference in the two distributions explained fully by the fact that one of them is close to zero, or is there another difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this possible? Please let me know if I have not phrased this clearly, or need to include more information.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2014-02-26T20:15:46.880" Id="88038" LastActivityDate="2014-04-05T21:24:35.940" OwnerUserId="9170" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;markov-process&gt;&lt;zero-inflation&gt;" Title="Comparing the distributions of two processes, one of which is constrained by zero" ViewCount="125" />
  
  
  
  <row Body="&lt;p&gt;As shown in &lt;strong&gt;Tong, Y. L. (1990). Multivariate normal distribution. Springer-Verlag., ch. 6&lt;/strong&gt;, for the setup described in the question and for &lt;em&gt;non-negative&lt;/em&gt; correlation coefficient $\rho\in [0,\;1)$, the distribution function (cdf) and the density of an order statistic $X_{(i)}$ are (where $\phi()$ and $\Phi()$ are the standard normal pdf and cdf)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$G_{(i)}(x) = \int_{-\infty}^{\infty}F_{(i)}\left(\frac{x+\sqrt{\rho}z}{\sqrt{1-\rho}}\right)\phi(z)dz$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and differentiating,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$g_{(i)}(x) = \int_{-\infty}^{\infty}\frac 1{\sqrt{1-\rho}}f_{(i)}\left(\frac{x+\sqrt{\rho}z}{\sqrt{1-\rho}}\right)\phi(z)dz$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{(i)}(y) = \frac{n!}{(i-1)!(n-i)!}[\Phi(y)]^{i-1}[\Phi(-y)]^{n-i}\phi(y)$$&#10;and&#10;$$F_{(i)}(y) = \sum_{j=i}^n {n \choose j}[\Phi(y)]^{j}[\Phi(-y)]^{n-j}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;i.e $f_{(i)}(y)$ and $F_{(i)}(y)$ are the pdf and cdf of the order statistic $(i)$ from an i.i.d. standard normal random sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the corresponding results when the correlation coefficient is negative, the author refers to the book &lt;strong&gt;&quot;Order Statistics&quot;, by H.A. David &amp;amp; H.N. Nagaraja ch. 5&lt;/strong&gt; (now in its 3d edition, 2003).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T03:26:45.303" Id="88077" LastActivityDate="2014-02-27T22:44:13.263" LastEditDate="2014-02-27T22:44:13.263" LastEditorUserId="28746" OwnerUserId="28746" ParentId="53368" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I know this is the &lt;a href=&quot;http://heapol.oxfordjournals.org/content/early/2011/01/27/heapol.czr004.full.pdf+html&quot; rel=&quot;nofollow&quot;&gt;general specification&lt;/a&gt; for a single group:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_i$ = $\beta_0$ + $\beta_1$*time + $\beta_2$*post + $\beta_3$*timepost&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;time&lt;/code&gt; is a continuous variable indicating the time (e.g., month) of every observation in the study; &lt;code&gt;post&lt;/code&gt; is an indicator for whether the observation is after the introduction of the intervention; and &lt;code&gt;timepost&lt;/code&gt; is time since the introduction of the intervention (0 if pre-intervention). &lt;/p&gt;&#10;&#10;&lt;p&gt;What is the correct specification for a segmented regression of interrupted time series data with a non-equivalent comparison group?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T04:54:32.547" Id="88080" LastActivityDate="2014-02-27T04:54:32.547" OwnerUserId="23607" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;segmented-regression&gt;" Title="What is the correct specification for a segmented regression of interrupted time series data with a non-equivalent comparison group?" ViewCount="40" />
  
  <row Body="&lt;p&gt;I think you want to address the &quot;cohesive subgroup&quot; literature rather than &quot;clustering.&quot; k-cores are part of the cohesive subgroup literature and it sounds like they're might be the measurement you want. If you read some of the early literature on them (e.g., Seidman [1983] in &lt;em&gt;Social Networks&lt;/em&gt;), you'll understand the theory behind them better. I have no idea why you arrived at the conclusion that they can only be used to &quot;visualize&quot; a network; your post is the first time I have ever heard that.&lt;/p&gt;&#10;&#10;&lt;p&gt;While k-cores might be the way to go, I would suggest reading more broadly into cohesive subgroups, as you might find cliques, k-cliques, cutpoints, etc more appropriate. To start reading into this literature, I would recommend the Chapter 7 in Wasserman and Faust (1994).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T07:28:54.593" Id="88091" LastActivityDate="2014-02-27T07:28:54.593" OwnerUserId="36673" ParentId="87984" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two (unpaired) distributions. The distributions are far from being normal, so I'm using the Wilcoxon test to compare their medians: the Wilcoxon test does not reject the null-hypothesis (p-value = 0.5584). Then I'm comparing variances of these distributions using the Siegel-Tukey test, which turns out to reject the null-hypothesis (p-value = 0.03351) suggesting difference in variances.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, since the literature says that difference in medians might have affected the results of the Siegel-Tukey test, I've decided to rerun the Siegel-Tukey test with median adjustment (despite the previous result of the Wilcoxon test). Surprisingly, after the median adjustment the Siegel-Tukey test can no longer reject the null hypothesis (p-value = 0.1144), i.e., we can no longer claim difference in medians.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this mean that I should have relied on the Wilcoxon tests and that adjusting the means was wrong in this case? Does this mean that the Siegel-Tukey test is not powerful enough to distinguish difference in the variances in the second case? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6zWAb.jpg&quot; alt=&quot;Violin plots and boxplots of the two distributions.&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-02-27T09:10:22.323" Id="88095" LastActivityDate="2014-02-27T20:41:40.323" LastEditDate="2014-02-27T20:41:40.323" LastEditorUserId="11058" OwnerUserId="11058" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;wilcoxon&gt;&lt;variability&gt;" Title="Adjustment of medians in the Siegel-Tukey test vs. the Wilcoxon test" ViewCount="128" />
  
  
  <row Body="&lt;p&gt;The Wilcoxon test is no more a test of medians than it is a test of means. &lt;/p&gt;&#10;&#10;&lt;p&gt;If means exist, then under certain assumptions, it can be a test of means. Under the same assumptions, it's a test of medians.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, under location shift alternatives, the Wilcoxon test tests whether the median of pairwise cross-population differences is different from zero. More generally still, it's a test of whether $P(X&amp;lt;Y)\neq \frac{1}{2}$.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Then I'm comparing variances of these distributions using the Siegel-Tukey test, which turns out to reject the null-hypothesis (p-value = 0.03351) suggesting difference in variances.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It suggests a difference in spread (in some sense). The population variance needn't exist; this test would still work.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's difficult to guess what it is about the samples that lead to the outcomes you describe (certainly not without the samples, but even then it may be tricky), but you'd never expect two different ways of treating the samples to yield identical p-values. [Note also that adjusting for the difference in median no longer means all sample arrangements are equally likely; I don't think the test is exactly distribution free any more.]&lt;/p&gt;&#10;&#10;&lt;p&gt;It may be that there's just enough location shift in the unadjusted data to tip the Siegel-Tukey over the 5% critical value. Or it may be that the significance level of the median-adjusted version of the test is affected by that adjustment just enough to push the otherwise significant difference a little way the other side of your p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or it might be something else. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T09:27:33.687" Id="88101" LastActivityDate="2014-02-27T09:27:33.687" OwnerUserId="805" ParentId="88095" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You are absolutely correct in observing that even though $\mathbf{u}$ (one of the eigenvectors of the covariance matrix, e.g. the first one) and $\mathbf{X}\mathbf{u}$ (projection of the data onto the 1-dimensional subspace spanned by $\mathbf{u}$) are two different things, both of them are often called &quot;principal component&quot;, sometimes even in the same text.&lt;/p&gt;&#10;&#10;&lt;p&gt;In most cases it is clear from the context what exactly is meant. In some rare cases, however, it can indeed be quite confusing. Particularly confusing it can become when some generalizations of PCA (e.g. sparse PCA, or related techniques, e.g. CCA) are discussed, where different directions $\mathbf{u}_i$ do not have to be orthogonal. In this case a statement like &quot;components are orthogonal&quot; has very different meanings depending on whether it refers to axes or projections.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I would advocate calling $\mathbf{u}$ a &quot;principal axis&quot; or a &quot;principal direction&quot;, and $\mathbf{X}\mathbf{u}$ a &quot;principal component&quot;.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have also seen $\mathbf u$ called &quot;principal component vector&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T14:38:29.297" Id="88137" LastActivityDate="2015-02-25T14:20:16.300" LastEditDate="2015-02-25T14:20:16.300" LastEditorUserId="28666" OwnerUserId="28666" ParentId="88118" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;Check &lt;a href=&quot;http://sites.stat.psu.edu/~jls/aaps_schafer.pdf&quot; rel=&quot;nofollow&quot;&gt;Missing Data in Longitudinal Studies: A Review&lt;/a&gt; presentation. There are books on this subject. It's a very broad subject. I think that MLE is more common approach when dealing with missing data than OLS though.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-27T14:49:37.040" Id="88140" LastActivityDate="2014-02-27T14:49:37.040" OwnerUserId="36041" ParentId="88129" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;First of all, you won't find a proof of this in the general case. Proofs of convergence in batch/stochastic gradient descent algorithms rely on convexity/strong convexity hypotheses. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of stochastic gradient descent, a theorem is that if the objective function is convex and the learning rate &#10;$\eta_t$ is such that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_t \eta_t = +\infty \quad \text{and} \quad \sum_t \eta_t^2 &amp;lt; + \infty$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then stochastic gradient converges almost surely to the global minimum. (Robbins-Siegmund theorem if I recall). The proof is nontrivial and makes use of results in the theory of stochastic process &amp;amp; martingale theory. This is the case for any convergence results for SGD. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your stepsize clearly checks this condition, although typically one chooses a step of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\sigma}{(1 + \sigma \lambda t)^{3/4}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\sigma$ is the initial learning rate and $\lambda$ governs asymptotic convergence speed.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-27T15:40:19.433" Id="88144" LastActivityDate="2014-02-27T15:40:19.433" OwnerUserId="37148" ParentId="87838" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="88236" AnswerCount="1" Body="&lt;p&gt;I am using the ar() function to fit an AR model to some data, and this object will return the in sample residuals. I also know the syntax for how to get the corresponding predicted values, but I want to compute these predicted values manually (just to check my own understanding) for a simple AR(1) example. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that my manually computed residuals (based on my manually computed predictions) do not match the in sample residuals stored in the ar object (well the 1st residual does match, but not the rest).&lt;/p&gt;&#10;&#10;&lt;p&gt;From the documentation, I see that&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x[t]  = m + a[1]*(x[t-1] - m) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where m is the sample mean of the series. Here is an example of what I am doing manually.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Create some true AR(1) data&#10;set.seed(123)&#10;x = w = rnorm(30) + 3 ; for (t in 2:30) x[t] = .60*x[t-1] + w[t]&#10;# Fit an ar model&#10;x.model = ar(x) # coefficient is .49, mean value of x is 6.98&#10;# Manually create predictions&#10;x.MyPred = rep(0,30) ; x.MyPred[1] = x[1]&#10;for (t in 2:30) x.MyPred[t] = 6.984234 + .4988327*(x.MyPred[t-1] - 6.984234)&#10;MyResid = x - x.MyPred&#10;cbind(MyResid, x.model$res) # does not match&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And interestingly, the first residual (at observation 2) does match, but the rest do not. Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer given below is basically highlighting the difference between so called static forecasting and dynamic forecasting, here are a few more details. &lt;/p&gt;&#10;&#10;&lt;p&gt;The two possible choices to make the fitted values are &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Method 1:&#10;for (t in 2:30) x.MyPred[t] = 6.984234 + .4988327*(x.MyPred[t-1] - 6.984234)&#10;&#10;# Method 2:&#10;for (t in 2:30) x.MyPred[t] = 6.984234 + .4988327*(x[t-1] - 6.984234)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Method 1 is taking the forecasted value from the previous step and using this to plug in for the lagged values on the RHS.&#10;This could feasibly be used to create out of sample predictions forever. Also, depending on what time point you start to make predictions, the resulting predictions for a given time period can be different.&lt;/p&gt;&#10;&#10;&lt;p&gt;Method 2 is taking the actual known historical values to plug in for the lagged values on the RHS. It will never be able to forecast more than 1 step out of sample. Also, the forecast values will always be the same no matter where you start.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both of these will give the same 1 step ahead forecast value. Method 2 is what produces the residuals returned in the ar() object.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Eviews software documentation has a good discussion of this. This R documentation was problematic in not specifying which of the x values are fitted values versus known historical values. A better expression to show in the documentation would be&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;\hat{x}[t]  = m + a[1]*(x[t-1] - m)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and, dare I say, possibly a few sentences on this very topic. But good documentation is hardly something to expect in R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T15:52:30.423" Id="88145" LastActivityDate="2014-02-28T17:40:11.850" LastEditDate="2014-02-28T17:40:11.850" LastEditorUserId="31973" OwnerUserId="31973" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;" Title="ar() time series function in R, manually checking the residuals/predicted values" ViewCount="287" />
  <row Body="&lt;p&gt;I have never heard of this name.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, the plot looks just like any Bland Altman plot I have seen, other than there are two sets of data overlaid on the plot. I guess the &quot;unpaired&quot; indicates that you cannot tell which MLEM and ST-MLEM data are coming from the same patient, because there is no linkage between the blue and the pink data points on the plot.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T16:01:51.693" Id="88149" LastActivityDate="2014-02-27T16:01:51.693" OwnerUserId="13047" ParentId="87179" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;See the following paper:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt; Murdock, D, Tsai, Y, and Adcock, J (2008) _P-Values are Random&#10; Variables_. The American Statistician. (62) 242-245.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;When the null hypothesis is true then the p-values follow a uniform (or something approaching a uniform when there is a finite number of possible test statistics) distribution, so you would expect to see p-values &quot;all over the place&quot;.  When the null is false then the distribution of p-values is more heavily weighted towards 0 (hopefully) which would be the more consistent results.  So my guess is that when you test with 0.5 then the null is true, and for the other cases it is false and you have enough power that there is little variation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also see the &lt;code&gt;Pvalue.norm.sim&lt;/code&gt; and &lt;code&gt;Pvalue.binom.sim&lt;/code&gt; functions in the TeachingDemos package for R for a quick way to simulate this for yourself to help see what is going on.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-27T18:05:17.137" Id="88170" LastActivityDate="2014-02-27T18:05:17.137" OwnerUserId="4505" ParentId="88166" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="96090" AnswerCount="2" Body="&lt;p&gt;We have a bivariate normal process where $X \sim N(\mu_x, \sigma), \, Y \sim N(\mu_y, \sigma)$, with no covariance.&lt;/p&gt;&#10;&#10;&lt;p&gt;$(\mu_x, \mu_y)$ are unknown.&lt;/p&gt;&#10;&#10;&lt;p&gt;(For convenience we can assert that $\sigma = 1$, or that we have a good estimate for its value.)&lt;/p&gt;&#10;&#10;&lt;p&gt;We are trying to characterize the distance between our sample center and the true center $(\mu_x, \mu_y)$ as a function of shots sampled &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because we don't care about the location of the true center, only our distance from it, we assert that $\mu_x = \mu_y = 0$ and look at the random variable $R(n) =  \sqrt{\overline{x_i}^2 + \overline{y_i}^2}$ -- the distance between sample center and true center.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How can we characterize the confidence interval of &lt;em&gt;R(n)&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that $R(n) \ge 0$ and $E[R(n)] \to 0$ as $n \to \infty$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have Monte Carlo estimates of both the mean and standard deviation of &lt;em&gt;R(n)&lt;/em&gt; for small &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to calculate confidence levels and intervals for &lt;em&gt;R(n)&lt;/em&gt;.  I.e., given &lt;em&gt;n&lt;/em&gt; and confidence level 90% what is the confidence interval of a sample &lt;em&gt;R(n)&lt;/em&gt; about its population mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't believe this is amenable to CLT analysis because the values are bounded at 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suppose I could Monte Carlo the edf since I'm only interested in $n \in [2, 30]$, and the edf must scale with $\sigma$ or $\sigma^2$.  But first I want to make sure I'm not missing something obvious or a known closed-form expression.&lt;/p&gt;&#10;" CommentCount="20" CreationDate="2014-02-27T18:20:00.943" FavoriteCount="1" Id="88171" LastActivityDate="2014-05-02T12:50:56.603" LastEditDate="2014-03-01T03:59:24.883" LastEditorUserId="34792" OwnerUserId="34792" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;confidence-interval&gt;&lt;distance&gt;&lt;bivariate&gt;" Title="Confidence interval for distance from center" ViewCount="163" />
  
  
  <row Body="&lt;p&gt;If you have no idea about the weights of bead types, then you can't combine them meaningfully. Period.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine two types of beads one with mass 1 tonn, and the other with a mass 1 mg. Unless you have some additional information about the total weight, there's no way to devise the combined variable. Of course, you always have some idea about the total weight, maybe typical values of total wight. In this case you can come up with a reasonable weights to get a combined variables.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-27T21:13:06.343" Id="88194" LastActivityDate="2014-02-27T21:13:06.343" OwnerUserId="36041" ParentId="88192" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;How can you determine the direction of the test by looking at a pair of hypotheses? How can you tell which direction (or no direction) to make the hypothesis by looking at the problem statement (research question)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-27T22:12:39.177" Id="88200" LastActivityDate="2014-03-26T15:15:22.820" LastEditDate="2014-02-27T22:13:15.953" LastEditorUserId="919" OwnerUserId="41043" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;" Title="How can I determine which of two complementary hypotheses should be the null?" ViewCount="467" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data that essentially correspond to Euclidean distances in some space. More specifically, I have dimensionality-reduced user data (quantified by their behavior), and a distance matrix in this dimensionality-reduced space from user to user.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to come up with a 'Similarity Percentage', to determine that, say, User A is 85% similar to User B. I then want to analyze the distribution of 'Similarity Percentage's for a particular user. If I pass these Euclidean distances through some sort of squashing function like the sigmoid, could I then model these squashed distances as a distribution on [0,1]?&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically what I'm asking is, what information would I lose (if any) by transforming my data through the sigmoid function?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-27T22:31:37.273" Id="88203" LastActivityDate="2014-02-27T22:31:37.273" OwnerUserId="37760" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;data-transformation&gt;" Title="Using squashing functions to impose finite bounds on data" ViewCount="41" />
  
  <row Body="&lt;p&gt;in banking you have to use also the external data. the regulators will insist that you don't come up with distribution strictly on your own dataset. so this becomes rather complicated. for instance take a look at &lt;a href=&quot;http://fic.wharton.upenn.edu/fic/papers/12/p1215.htm&quot; rel=&quot;nofollow&quot;&gt;change of measure&lt;/a&gt; approach. the paper also discusses the scenarios - another part of oprisk estimation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-28T02:22:10.197" Id="88222" LastActivityDate="2014-02-28T02:22:10.197" OwnerUserId="36041" ParentId="74845" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The Fisher Information is &lt;em&gt;defined&lt;/em&gt; as&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\left(\mathcal{I} \left(\theta \right) \right)}_{i, j}&#10;=&#10;\operatorname{E}&#10;\left[\left.&#10; \left(\frac{\partial}{\partial\theta_i} \log f(X;\theta)\right)&#10; \left(\frac{\partial}{\partial\theta_j} \log f(X;\theta)\right)&#10;\right|\theta\right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(the question in the post you linked to states mistakenly otherwise, and the answer politely corrects it). &lt;/p&gt;&#10;&#10;&lt;p&gt;Under the following regularity conditions:&lt;br&gt;&#10;1) The support of the random variable involved does not depend on the unknown parameter vector&lt;br&gt;&#10;2) The derivatives of the loglikelihood w.r.t the parameters exist up to 3d order&lt;br&gt;&#10;3) The expected value of the squared 1st derivative is finite  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;and&lt;/em&gt; under the assumption that&#10;the specification is correct&lt;/strong&gt; (i.e. the specified distribution family includes the actual distribution that the random variable follows)&lt;br&gt;&#10;&lt;em&gt;then&lt;/em&gt; the Fisher Information equals the (negative of the) Hessian of the loglikelihood for one observation. This equality is called the &quot;Information Matrix Equality&quot; for obvious reasons.  &lt;/p&gt;&#10;&#10;&lt;p&gt;While the three regularity conditions are relatively &quot;mild&quot; (or at least can be checked), the assumption of correct specification is at the heart of the issues of statistical inference, especially with observational data. It simply is too strong a condition to be accepted easily. And this is the reason why it is a major issue to &lt;em&gt;prove&lt;/em&gt; that the log-likelihood is concave in the parameters (which leads to consistency and asymptotic normality &lt;em&gt;irrespective&lt;/em&gt; of whether the specification is correct -the quasi-MLE case), and not just assume it by assuming that the Information Matrix Equality holds.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So you were absolutely right in thinking &quot;too good to be true&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;On the side, you neglected the presence of the minus sign -so the Hessian of the log-likelihood (for one observation) would be &lt;em&gt;negative&lt;/em&gt;-semidefinite, as it should since we seek to maximize it, not minimize it.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-02-28T03:26:52.447" Id="88224" LastActivityDate="2014-02-28T04:07:25.073" LastEditDate="2014-02-28T04:07:25.073" LastEditorUserId="28746" OwnerUserId="28746" ParentId="88182" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="88229" AnswerCount="1" Body="&lt;p&gt;I recently began learning about OLS estimation of multiple regression models and came across the following formulas explaining the calculations:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/tuNO5GU.jpg&quot; alt=&quot;formulas&quot; title=&quot;formulas&quot;&gt;&#10;What would the formulas be for an OLS regression model with more than two independent variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm asking this because I'm trying to make an application in JavaScript that will perform the calculations. Alternatively, I would be amazing if anyone knew of a free API that could be called from JavaScript to calculate OLS multiple regressions. &lt;/p&gt;&#10;" ClosedDate="2014-02-28T11:49:29.507" CommentCount="1" CreationDate="2014-02-28T04:34:26.083" Id="88228" LastActivityDate="2014-02-28T05:03:18.230" LastEditDate="2014-02-28T05:00:37.847" LastEditorUserId="24808" OwnerUserId="41058" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;least-squares&gt;&lt;javascript&gt;" Title="How do you calculate the Ordinary Least Squares estimated coefficients in a Multiple Regression Model?" ViewCount="250" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have a dataset $\{y_i,x_i\}$ $i=1,2,...n$. For the response variable, $y_i$ as per quantile regression I have the following likelihood:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(y_i|\beta,\alpha_i,\sigma) =\frac{\alpha_i(1-\alpha_i)}{\sigma}\exp\left(-\frac{\epsilon_i(\alpha_i-I(\epsilon_i&amp;lt;0))}{\sigma}\right)  $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where, $\epsilon_i=y_i-\beta^Tx_i$, and $\alpha_i$ is the percentile. This is an asymmetric laplacian.&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually for a synthetic dataset you can generate the data and know what the $\alpha_i$'s are (because you generated them).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Question is for a real dataset how would you know what the $\alpha_i$ is. My first thought was to put a uniform prior on $\alpha_i$ and look at the posterior, however this would symmetricise the likelihood (atleast intuitively it does). Is ML an option? This would highly depend on $\beta$ which in turn would depend on $\alpha_i$'s.&lt;/li&gt;&#10;&lt;li&gt;If above estimation of $\alpha_i$ is not possible what datasets would give you the quantile directly (with corresponding $\alpha_i$).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-02-28T05:17:23.557" Id="88230" LastActivityDate="2014-02-28T05:17:23.557" OwnerUserId="29537" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;maximum-likelihood&gt;&lt;prior&gt;&lt;quantiles&gt;&lt;quantile-regression&gt;" Title="Figuring out quantiles in quantile regression" ViewCount="53" />
  
  
  <row Body="&lt;p&gt;If your goal is to look at the data and get a feel for what's going on in the plot, you may be stuck. Exponents varying by that much I don't think are going to plot nicely. Your best bet might be to plot $K$ instead, as it's at least close to being the log probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your goal is to draw samples though, have you considered using &lt;a href=&quot;http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&quot; rel=&quot;nofollow&quot;&gt;metropolis hastings&lt;/a&gt;? I ask, as the only part of that which interacts with the distribution in question (i.e. the part where the algorithm decides whether to accept a new sample $x'$ given the prior sample $x$) depends on the &lt;em&gt;ratio&lt;/em&gt; of probabilites $a$, where $a = P(x')/P(x)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;But to get the ratio, you don't need to calculate the actual probabilities &#10;$P(x')$ and $P(x)$. Instead you can use&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;a = \frac{P(x')}{P(x)} = e^{\log(P(x')) - \log(P(x))}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That means you can just calculate the log probabilities directly (which should be scaled much more nicely), subtract them (getting rid of any huge nasty constant factors between the two), and exp the answer. You won't be free of numeric issues, but they ought to be lessened.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-02-28T10:46:37.537" Id="88248" LastActivityDate="2014-02-28T10:46:37.537" OwnerUserId="16765" ParentId="88247" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;my name is vincenzo and i have that type of problem with zinb that you intorduce in this discussion (&lt;a href=&quot;http://stats.stackexchange.com/questions/54898/zip-converges-but-zinb-does-not-should-i-drop-this-model&quot;&gt;ZIP converges but ZINB does not. Should I drop this model?&lt;/a&gt;): the iterations continues to be not concave. If I take out one variable from my model, the second iterationis ot concave, the third is &quot;backed up&quot;, then the model run normally.&#10;What is going on? can you help me please?&lt;/p&gt;&#10;&#10;&lt;p&gt;thank you in advance!&#10;Regards,&#10;Vincenzo&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-28T14:02:12.137" Id="88266" LastActivityDate="2014-02-28T14:25:36.740" OwnerUserId="41101" PostTypeId="1" Score="0" Tags="&lt;zero-inflation&gt;" Title="not concave iterations on a zinb model" ViewCount="139" />
  
  
  <row Body="&lt;p&gt;Assuming you use Stata (I base that assumption on your use of terminology) you can use your second model. It is important that the final iteration does not show such messages, but such messages in the middle of the iteration log are not a problem and not surprising for a zero-inflated negative binomial (zinb) model.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing you can do is make sure that the value 0 on all explanatory/independent/right-hand-side/x-variables are within or near the range of the data. For example, in most case you don't want to use year of birth, as the year 0 is usually way outside the range of your data. Instead you would create and use a different variable containing something like (year of birth - 1960). Also you want to scale your variables such that their effects are of the same order of magnitude. For example don't add annual income in euros, as a single euro increase in annual income isn't going to do much. Instead add annual income in 1000s of euros. Similarly, I usually enter year of birth measured in decades since 1960. Most of the time I do both because it makes the coefficients easier to interpret, but in complicated models like zinb it can actually help attain convergence.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-28T14:25:36.740" Id="88272" LastActivityDate="2014-02-28T14:25:36.740" OwnerUserId="23853" ParentId="88266" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="88487" AnswerCount="2" Body="&lt;p&gt;I have a time series that is affected by two (or more) kinds of events. When event $A$ happens, some signal is linearly added to the time series (the signal lasts, for example, for 100 time points). When event $B$ happens, another signal (also of 100 time points length) is linearly added to the time series. The noise is not completely normal - there are some very strong outliers here and there. My goal is to model the two signals.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the responses to the two events didn't overlap in time, I could simply average all the time points delayed t time points after each event kind. The standard deviation of the values in this subset of time points divided by $\sqrt{n}$ (n is the number of event occurrences) performs well enough in estimating the error: if in one of event $A$'s occurrences there was a strong outlier at time $x$, it will increase the local estimate of error for that particular delay. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that the two events do overlap in time. Therefor, I have to use regression in order to deconvolve the two responses. This is done by creating a set of pulse predictors of 100 delays from the occurrences of each event type (a Finite Impulse Response model). By OLS I can get a good estimation of the shapes of the two signals. However, I can't get a good estimate of the error: the standard OLS error estimator is assuming homoscedastic error. Hence, a time point at time $t_1$ that was distorted by a strong outlier has the same error estimation as time point $t_2$, which wasn't noised so much. This makes the error estimator useless in telling a true increase in the signal from a momentary noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a crude approximation, I thought about estimating the error for each delay $t$ (and each event kind) by the standard deviation of the particular residuals at that specific delay, divided by $\sqrt{n}$ (n is the number of event occurrences, not total time points as in the usual OLS SE).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd appreciate any help with this problem. And in particular, answering these two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is the latter solution biased? And if it is, in what way? &lt;/li&gt;&#10;&lt;li&gt;Is there a better way to estimate the noise in this case? Due to the limitations of my scientific field, it should not involve rich parametric assumptions.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-02-28T14:39:05.103" Id="88276" LastActivityDate="2014-03-03T20:49:53.730" LastEditDate="2014-02-28T17:29:33.613" LastEditorUserId="26338" OwnerUserId="20587" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;multiple-regression&gt;&lt;least-squares&gt;&lt;heteroscedasticity&gt;&lt;normality&gt;" Title="Standard error of regression coefficients without an assumption of homoscedastic normal noise" ViewCount="183" />
  <row Body="&lt;p&gt;The decision boundary seems already optimal to me, and I assume you would like to observe the effect of parameter changes on the decision boundary. If that is the case, first make sure you scale the data in advance. Then I would suggest you implement the grid search on &lt;code&gt;C&lt;/code&gt; values on a wide range ($10^{-5},...,10^5)$), and observe the training accuracy. If &lt;code&gt;C&lt;/code&gt; increases slightly, it is possible that you still form all of the linear models the same as before, since the upper bound on the allowable norm of the weights along with the margin may not influence on their choice very much within a small range of &lt;code&gt;C&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-28T14:48:01.953" Id="88277" LastActivityDate="2014-02-28T15:40:00.850" LastEditDate="2014-02-28T15:40:00.850" LastEditorUserId="35099" OwnerUserId="35099" ParentId="88000" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a large word-frequency matrix (~6m unique words X ~4k documents) and I'm trying to use truncated singular value decomposition (SVD) to project it onto a matrix with fewer dimensions. I know how to get to $[U_{k}, S_{k}, V_{k}']$, but I don't know what to do then.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each tutorial I find gives me a different answer. &lt;a href=&quot;http://www.site.uottawa.ca/~diana/csi4107/LSI.pdf&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt;, for instance, I'm told to retain only the first $k$ rows of $S_{k}$ and $V_{k}'$ and then multiply $S_{k}V_{k}'$. In the scikit-learn implementation, however, they do it by multiplying $U_{k}S_{k}'$ (&lt;a href=&quot;http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis&quot; rel=&quot;nofollow&quot;&gt;formula&lt;/a&gt; and &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/truncated_svd.py&quot; rel=&quot;nofollow&quot;&gt;code&lt;/a&gt;). (The &lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot; rel=&quot;nofollow&quot;&gt;book chapter they mention&lt;/a&gt; as source doesn't say anything about that.) In &lt;a href=&quot;http://stats.stackexchange.com/questions/57467/how-to-perform-dimension-reduction-after-doing-pca-in-r&quot;&gt;this previous answer&lt;/a&gt; they suggest $X'U_{k}$ instead (though they are talking about PCA, so maybe that doesn't apply here). Other answers haven't helped (&lt;a href=&quot;http://stats.stackexchange.com/questions/64370/how-to-use-svd-for-dimensionality-reduction&quot;&gt;here&lt;/a&gt;, for instance, they say we need to apply truncated SVD to the reduced matrix - which doesn't sound right, we use truncated SVD precisely to get to the reduced matrix, no?).&lt;/p&gt;&#10;&#10;&lt;p&gt;So, bottom line: how do I go from $[U_{k}, S_{k}, V_{k}']$ to a reduced data matrix? Which of the above formulas is the correct one (and why)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to be clear: I'm not looking to reduce rank only (that I can do by multiplying $U_{k}S_{k}V_{k}'$ but it gives me the exact same dimensions I had before, which doesn't help me); I want to reduce the actual dimensions of the data matrix.&lt;/p&gt;&#10;" ClosedDate="2015-01-30T18:16:12.903" CommentCount="0" CreationDate="2014-02-28T15:28:40.017" Id="88282" LastActivityDate="2015-01-30T18:16:03.180" LastEditDate="2014-02-28T16:30:05.627" LastEditorUserId="88" OwnerUserId="41053" PostTypeId="1" Score="0" Tags="&lt;svd&gt;" Title="Truncated SVD: how do I go from [Uk, Sk, Vk'] to low-dimension matrix?" ViewCount="139" />
  <row AcceptedAnswerId="88306" AnswerCount="1" Body="&lt;p&gt;I'm trying to find a reasonable way to measure &lt;strong&gt;retention&lt;/strong&gt; of a site (news site). I'm trying to do cohort analysis: I've grouped all visits by year/week, and measured the percent of users that kept coming back in the following weeks.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: these users are first-time visitors, who have been tracked along the weeks following their first time visit.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've got the following data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;yyyy/ww 1       2       3       4       5       6       7       8       9       10      11      12      13&#10;201348  27.33   7.51    5.71    6.61    3       5.41    4.8     4.5     4.2     5.41    4.2     3.9     2.1&#10;201349  7.88    5.23    3.03    2.53    4.02    3.16    3.13    2.83    2.63    2.16    2.06    1.25    &#10;201350  7.91    3.79    3.01    4.59    3.57    3.48    3.08    2.93    2.45    2.25    1.4     &#10;201351  6.64    4.1     5.69    4.41    4.13    3.68    3.36    2.8     2.56    1.56            &#10;201352  6.09    6.99    5.05    4.79    3.99    3.79    3.24    2.94    1.84                &#10;201401  11.66   6.69    6.28    5.45    4.67    3.74    3.4     1.99                    &#10;201402  8.45    6.14    5.11    4.61    3.49    3.1     1.77                        &#10;201403  9.45    6.27    5.38    4.12    3.57    2.08                            &#10;201404  9.78    6.78    4.9     4.18    2.29                                &#10;201405  8.96    5.37    4.45    2.47                                    &#10;201406  8.88    5.85    3.07                                        &#10;201407  8.46    3.76                                            &#10;201408  6.29                                                &#10;&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The row labels are a week in time, the column labels are the number of weeks that passed since the first visit. The cell values are the &lt;strong&gt;percent&lt;/strong&gt; of inital users that came back.&lt;/p&gt;&#10;&#10;&lt;p&gt;The two funky values at&lt;code&gt;week+1&lt;/code&gt; account for christmas and new year. My question is: how should I interpret these values? I don't have a very deep statisical knowledge beyond the basics; I tried to calculate the slope of each row, and I got this (oldest to newest):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;-4.956&#10;-1.042&#10;-0.788&#10;-0.471&#10;-0.64&#10;-1.522&#10;-1.145&#10;-1.391&#10;-1.758&#10;-2.039&#10;-2.905&#10;-4.7&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My rationale is that a good performing site would have an &lt;code&gt;increasing&lt;/code&gt;slope value over time. In this case it seems the opposite. &#10;&lt;strong&gt;Clarification&lt;/strong&gt;: What I mean with increasing slope is that the slope value would be less farther away from zero for the next first-time visitors; over time, the percentage of people who kept coming back would be higher (but still &amp;lt; 100% of course) than the previous cohorts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Something like this, where you can see the fraction of retained users is bigger than the previous month: &lt;img src=&quot;http://i.stack.imgur.com/s33qF.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions and/or critiques are welcome. Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-28T15:54:56.923" FavoriteCount="1" Id="88284" LastActivityDate="2014-03-03T18:25:52.373" LastEditDate="2014-03-03T18:25:52.373" LastEditorUserId="41087" OwnerUserId="41087" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;survival&gt;&lt;case-cohort&gt;" Title="Cohort analysis for media site" ViewCount="82" />
  
  <row AnswerCount="0" Body="&lt;p&gt;From Elements of Statistical Learning, on page 49 (in the context of least square regression), we are given the approximate confidence set (shown in image) for the parameter vector $\beta$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Xx4Ak.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to implement this in R. However, I have no idea how to make $\beta$ as the subject,i.e. I could not construct the confidence set in the following form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\beta \leq \hat{\sigma}^2 \chi^2_{p+1} \times something$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't really know how to rearrange the terms (of the confidence set from the image). &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone kindly help me out here please ? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-02-28T16:35:35.373" Id="88291" LastActivityDate="2014-02-28T16:35:35.373" OwnerUserId="40761" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;confidence-interval&gt;&lt;references&gt;&lt;least-squares&gt;" Title="Constructing the approximate confidence set for parameter vector beta in least square regression" ViewCount="31" />
  
  
  
  <row Body="&lt;p&gt;Yes, I suspect that is the gist of it.  I have provided similar exercises to students.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Another thing you can do is see how the sampling distribution of the sample mean gets narrower as the number of data in your sample increases (although that does not seem to have been a part of your assignment).  For example, if you sampled only $2$ data from that population, the highest sample mean you could get is $8.5$, and the lowest is $2.5$.  Consider how the range of possible sample means changes as a function of sample size:&lt;br&gt;&#10;$$&#10;2\quad 3\quad 4\quad 5\quad 6\quad 7\quad 8\quad 9  \\&#10;\underbrace{\ 2.5\quad\quad\quad\quad\quad\quad\quad\;\  8.5}  \\&#10;\underbrace{3\quad\quad\quad\quad\quad\quad\quad 8}  \\&#10;\underbrace{3.5\quad\quad\quad\quad\;\ 7.5}  \\&#10;\underbrace{4\quad\quad\quad\quad 7}  \\&#10;\underbrace{4.5\quad\ \ \  6.5}  \\&#10;\underbrace{5\quad 6}  \\&#10;\underbrace{5.5}_{\mu_Y}  &#10;$$&#10;When your sample is smaller, there are a wider range of possible estimates that you can get.  As your sample gets larger, the range narrows.  In the end, when you have the entire population, you necessarily get the population mean.  (In fancy terms, the sample mean is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Consistent_estimator&quot; rel=&quot;nofollow&quot;&gt;consistent estimator&lt;/a&gt; of the population mean.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, it isn't simply that the &lt;em&gt;possible&lt;/em&gt; range narrows, but you are increasingly likely to get an estimate that is closer to the true population mean.  You can see this in histograms that display all possible sample means as a function of sample size:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dQhiZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-28T17:16:07.670" Id="88299" LastActivityDate="2014-02-28T17:16:07.670" OwnerUserId="7290" ParentId="88253" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You are doing a cohort analysis which suggests that you are following a specific set of users, but you say the data represents &quot;all visits by year/week.&quot; Can you describe the data more? If you're not limiting this to a specific set of users, then over time wouldn't you expect to see more one-time visitors? If you're not limiting this to a specific set of users, then you are not doing a cohort analysis. But I suspect that's not the issue. Also, if you're not already, it might be a good idea to constrain your analysis (or do a separate analysis) focusing on cohorts of first-time visitors to the site.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a kind of survival analysis: you say that you would expect an increasing slope over time, but I think it makes sense to expect the slope to be generally decreasing for any site. An increasing slope would mean that you have more people coming back for week two than week one, but presumably your best performance for week two would be to &lt;em&gt;retain&lt;/em&gt; everyone who visited in week one, and realistically you're going to lose a few people. Looking further down stream, an increasing slope means that, looking at a specific group of people, more people from that group visited the site 5 months later than the week after their first visit. That'd be a pretty unusual expectation for a website. &lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: at what value does the slope begin to plateau? This describes your consistent userbase.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding those two rows with high values in week one, I'd suggest the following possibilities:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Those weeks your website presented especially interesting articles and brought a large percentage of users back the next week due to interest in your site.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Those were actually &quot;slow news&quot; weeks that brought fewer one-time visitors to the site than normal, and those high percentages indicate that a disproportionate number of the visitors that week are users of the site who would have come back again the following week anyway.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You had a slow week followed by a high traffic week due to some interesting content. Many users were brought back to your website not due to the week 0 content but due to the week 1 content, i.e. their week 1 visit was actually independent of their week 0 visit. If you shift the values for 201401 over 1 and treat week 1 as week 0, the values line up reasonably well with the other weeks. This does not hold for 201348.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I originally thought option 2 was more likely, but if it were the case we'd anticipate that high percentage to trickle over into the following weeks, not just week one. So I'd check the traffic you got those weeks and see if maybe your site published any particularly popular content those weeks. It's going to take some domain knowledge to really interpret this data, and it's your website not mine.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your purposes, I don't think it would be unreasonable to take the mean down each column, giving the average retention N weeks out for each week. If you discount the first week as an outlier and then consider only out to a time period where you have at least 5 data points to consider (ignoring week 1), you'll observe a decay in retention out to week 8 where it hits about 2.7% percent. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think taking the linear slope for each week as you have done makes much sense since your data isn't linear. If you really want you can fit a non-linear regression to your data, but I'm not sure how useful such a model would be to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you could keep tabs on the week 1, week 2 (... however far back you want to go) retention rates over time to try and determine the overall change in (new user?) retention for your site. It looks like the trend in these statistics is generally a decline, so maybe you should be concerned. If you wanted to plot a linear regression down each week (i.e. treating each column as a data series) I think that would make more sense than across rows. If you do this, you might want to start your regression at 201352 or 201402 since these weeks seem to be the start of your most consistent trend.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-02-28T17:49:43.513" Id="88306" LastActivityDate="2014-02-28T18:10:53.380" LastEditDate="2014-02-28T18:10:53.380" LastEditorUserId="8451" OwnerUserId="8451" ParentId="88284" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="88359" AnswerCount="2" Body="&lt;p&gt;Let's suppose that I have a function like below, but I don't know what it is. However, as I choose &lt;code&gt;x&lt;/code&gt;, I know what corresponding &lt;code&gt;f(x)&lt;/code&gt; is. What is the optimal way to choose &lt;code&gt;x&lt;/code&gt; such that sum of &lt;code&gt;f(x)&lt;/code&gt; is maximized for some finite &lt;code&gt;n&lt;/code&gt; tries (let's say 50 tries for the following example)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xLbbX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea so far:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Choose some &lt;code&gt;x&lt;/code&gt;s at random in grid search type fashion e.g, 0,25,50,75,100.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) a) Choose &lt;code&gt;x&lt;/code&gt;s that correspond to higher &lt;code&gt;f(x)&lt;/code&gt; with higher probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;b) Introduce some kind of noise factor around &lt;code&gt;x&lt;/code&gt;, so we are able to search around &lt;code&gt;x&lt;/code&gt;. For example if 75 is chosen, in 2.a, in 2.b 78.33 can be chosen. This noise factor should decrease as number of iterations increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Keep doing step 2 for a number of values of &lt;code&gt;x&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a better way to do the above? Is there a known algorithm to tackle this problem? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-01T03:21:22.630" FavoriteCount="1" Id="88340" LastActivityDate="2014-03-02T20:23:47.827" LastEditDate="2014-03-01T12:02:29.323" LastEditorUserId="22047" OwnerUserId="11708" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;mathematical-statistics&gt;&lt;algorithms&gt;" Title="Maximize sum of f(x), where f(x) is unknown, but we learn as each x is chosen" ViewCount="116" />
  
  <row Body="&lt;p&gt;First you need to decide whether you need model/parameter selection, or just model.  Once your model is fixed, bootstrap seems make more sense to determine how your modeling procedure performs. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are implementing cross validation on multiple dataset, just randomly partition the data without considering their labels. It is possible sometimes that one label in test data set does not even gets trained, and it counts into the validation error. Usually a 10-fold cross-validation is highly recommended to repeat 50-100 times for stability.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may try to avoid class imbalance issue (thus indirectly reduce the odds of the excluded label event mentioned above), but if your data really suffers from this problem, there are several re-sampling strategies in my previous answer in &lt;a href=&quot;http://stats.stackexchange.com/questions/81111/classification-problem-using-imbalanced-dataset/81171#81171&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-03-01T06:04:49.923" Id="88344" LastActivityDate="2014-03-01T06:04:49.923" OwnerUserId="35099" ParentId="88012" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="88351" AnswerCount="2" Body="&lt;p&gt;&lt;sup&gt;This is my first question on Cross Validated here, so please help me out even if it seems trivial :-) First of all, the question might be an outcome of language differences or perhaps me having real deficiencies in statistics. Nevertheless, here it is:&lt;/sup&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In population statistics, are variation and variance the same terms? If not, what is the difference between the two?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that variance is the square of standard deviation. I also know that it is a measure of how sparse the data is, and I know how to compute it. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I've been following a Coursera.org course called &quot;Model Thinking&quot;, and the lecturer clearly described variance but was constantly calling it variation. That got me confused a bit.&lt;/p&gt;&#10;&#10;&lt;p&gt;To be fair, he always talked about computing variation of some particular instance in a population.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone make it clear to me if those are interchangeable, or perhaps I'm missing something?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-01T07:28:46.633" FavoriteCount="2" Id="88348" LastActivityDate="2014-03-01T08:49:23.820" LastEditDate="2014-03-01T08:17:51.893" LastEditorUserId="32036" OwnerUserId="41079" PostTypeId="1" Score="10" Tags="&lt;variance&gt;&lt;summary-statistics&gt;&lt;basic-concepts&gt;&lt;definition&gt;" Title="Is variation the same as variance?" ViewCount="4327" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using a double model with log transformed independent variables and have calculated average partial effects. Now I am now not sure how to interpret the coefficients; particularly those from the first stage which is supposed to be capturing the probability of participation using a probit model. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-01T09:19:18.910" Id="88353" LastActivityDate="2014-03-01T20:34:40.563" LastEditDate="2014-03-01T20:33:06.037" LastEditorUserId="7071" OwnerUserId="41157" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;probit&gt;&lt;logarithm&gt;" Title="interpreting the coefficient on a logged independent variable from a probit model" ViewCount="266" />
  
  <row AnswerCount="1" Body="&lt;pre&gt;&lt;code&gt;I have fitted a glm Poisson to my frequency data and obtained the result:&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Null deviance: 657.49  on 583  degrees of freedom&lt;/p&gt;&#10;&#10;&lt;p&gt;Residual deviance: 575.00  on 571  degrees of freedom&lt;/p&gt;&#10;&#10;&lt;p&gt;AIC: 1534.4&lt;/p&gt;&#10;&#10;&lt;p&gt;Is the high AIC value and relatively high residual deviance a serious matter to be taken into consideration? Thanks&lt;/p&gt;&#10;" ClosedDate="2014-03-01T20:30:57.927" CommentCount="2" CreationDate="2014-03-01T13:25:26.880" Id="88368" LastActivityDate="2014-03-01T13:44:46.483" LastEditDate="2014-03-01T13:34:46.903" LastEditorUserId="40494" OwnerUserId="40494" PostTypeId="1" Score="-3" Tags="&lt;poisson-regression&gt;&lt;deviance&gt;" Title="Interpreting AIC and deviance values" ViewCount="199" />
  <row Body="&lt;p&gt;If there is not theory and you just want to find the best fit, you can have a look at the AIC values of the models.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# run the models&#10;m1 &amp;lt;- lm(mpg ~ wt*cyl + I(wt^2) , mtcars)&#10;m2 &amp;lt;- lm(mpg ~ wt + I(wt^2)*cyl , mtcars)&#10;m3 &amp;lt;- lm(mpg ~ wt*cyl + I(wt^2)*cyl , mtcars)&#10;&#10;# compare AIC values&#10;AIC(m1, m2, m3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   df      AIC&#10;m1  6 153.7035&#10;m2  6 154.2784&#10;m3  7 153.5813&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The third model has the lowest AIC score and is therefore the &quot;best&quot; model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, this model selection depends on using AIC scores as a criterion. If you use another criterion, e.g., BIC scores, the result can be different:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;BIC(m1, m2, m3)&#10;&#10;   df      BIC&#10;m1  6 162.4979&#10;m2  6 163.0729&#10;m3  7 163.8415&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You have to choose a criterion for the model selection.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-01T13:31:21.300" Id="88369" LastActivityDate="2014-03-01T13:31:21.300" OwnerUserId="13680" ParentId="88349" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;pre&gt;&lt;code&gt;Heart                  Group-1                         Group-2&#10;               Case1    Case2   Case3          Case1    Case2   Case3&#10;İnflammation    -       -       -              +++      ++      ++&#10;Hemorrhage      +       -       -              ++       +++     ++&#10;Fibrosis        +       +       -              ++       +++     ++&#10;Necrosis        ++      ++      +              ++       +++     ++&#10;&#10;(-;absent, +;mild, ++;moderate, +++;severe)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A sample table is seen above. I have 3 cases in each groups. How can I calculate statistical significance between two groups. Should I calculate statistical significance for each lesion?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-01T20:50:08.203" Id="88413" LastActivityDate="2014-03-01T21:35:59.370" LastEditDate="2014-03-01T21:01:42.423" LastEditorUserId="4598" OwnerUserId="41178" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;" Title="Statistical significance between two groups" ViewCount="55" />
  
  
  
