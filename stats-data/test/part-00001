&#10;\int_0^1\left[\frac{(m+1)!}{y!(m-y)!}\int_0^{1-p} (1-q)^{y}q^{m-y}dq\right] p^{x}(1-p)^{n-x} dp
&#10;\int_0^1\left[\sum_{k=0}^{y}\frac{(m+1)!}{(m+1-k)!k!}p^{k}(1-p)^{m+1-k}\right] p^{x}(1-p)^{n-x} dp
  
  <row Body="&lt;p&gt;Just take partial derivatives.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you increase $x_1$ by one unit, then the expected value of $y$ increases by $\beta_1 + 2\beta_2 x_1$ units. Note that this effect depends upon the level of $x_1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you increase $x_3$ by one unit, then the expected value of $y$ increases by $\beta_3$ units. Note that this effect does not depend upon the level of $x_3$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As someone else mentioned, $\beta_0$ is the average value of $y$ when $x_1$ and $x_3$ are 0. I like to interpret the intercept as putting the regression  through the point of means, the combination of the average $x_1$, the average $x_1^2$ (&lt;em&gt;not&lt;/em&gt; the (average of $x_1$)-quantity squared), the average $x_3$, and the average $y$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-01T16:24:26.793" Id="22086" LastActivityDate="2012-02-01T16:24:26.793" OwnerUserId="401" ParentId="22054" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I have revised my answer from earlier today. I have now generated some example data on which to run the code. Others have rightly suggested that you look into using the caret package, which I agree with. In some instances, however, you may find it necessary to write your own code. Below I have attempted to demonstrate how to use the sample() function in R to randomly assign observations to cross-validation folds. I also use for loops to perform variable pre-selection (using univariate linear regression with a lenient p value cutoff of 0.1) and model building (using stepwise regression) on the ten training sets. You can then write your own code to apply the resultant models to the validation folds. Hope this helps!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;################################################################################&#10;## Load the MASS library, which contains the &quot;stepAIC&quot; function for performing&#10;## stepwise regression, to be used later in this script&#10;library(MASS)&#10;################################################################################&#10;&#10;&#10;################################################################################&#10;## Generate example data, with 100 observations (rows), 70 variables (columns 1&#10;## to 70), and a continuous dependent variable (column 71)&#10;Data &amp;lt;- NULL&#10;Data &amp;lt;- as.data.frame(Data)&#10;&#10;for (i in 1:71) {&#10;for (j in 1:100) {&#10;Data[j,i]  &amp;lt;- rnorm(1) }}&#10;&#10;names(Data)[71] &amp;lt;- &quot;Dependent&quot;&#10;################################################################################&#10;&#10;&#10;################################################################################&#10;## Create ten folds for cross-validation. Each observation in your data will&#10;## randomly be assigned to one of ten folds.&#10;Data$Fold &amp;lt;- sample(c(rep(1:10,10)))&#10;&#10;## Each fold will have the same number of observations assigned to it. You can&#10;## double check this by typing the following:&#10;table(Data$Fold)&#10;&#10;## Note: If you were to have 105 observations instead of 100, you could instead&#10;## write: Data$Fold &amp;lt;- sample(c(rep(1:10,10),rep(1:5,1)))&#10;################################################################################&#10;&#10;&#10;################################################################################&#10;## I like to use a &quot;for loop&quot; for cross-validation. Here, prior to beginning my&#10;## &quot;for loop&quot;, I will define the variables I plan to use in it. You have to do&#10;## this first or R will give you an error code.&#10;fit &amp;lt;- NULL&#10;stepw &amp;lt;- NULL&#10;training &amp;lt;- NULL&#10;testing &amp;lt;- NULL&#10;Preselection &amp;lt;- NULL&#10;Selected &amp;lt;- NULL&#10;variables &amp;lt;- NULL&#10;################################################################################&#10;&#10;&#10;################################################################################&#10;## Now we can begin the ten-fold cross validation. First, we open the &quot;for loop&quot;&#10;for (CV in 1:10) {&#10;&#10;## Now we define your training and testing folds. I like to store these data in&#10;## a list, so at the end of the script, if I want to, I can go back and look at&#10;## the observations in each individual fold&#10;training[[CV]] &amp;lt;- Data[which(Data$Fold != CV),]&#10;testing[[CV]]  &amp;lt;- Data[which(Data$Fold == CV),]&#10;&#10;## We can preselect variables by analyzing each variable separately using&#10;## univariate linear regression and then ranking them by p value. First we will&#10;## define the container object to which we plan to output these data.&#10;Preselection[[CV]] &amp;lt;- as.data.frame(Preselection[CV])&#10;&#10;## Now we will run a separate linear regression for each of our 70 variables.&#10;## We will store the variable name and the coefficient p value in our object&#10;## called &quot;Preselection&quot;.&#10;for (i in 1:70) {&#10;Preselection[[CV]][i,1]  &amp;lt;- i&#10;Preselection[[CV]][i,2]  &amp;lt;- summary(lm(Dependent ~ training[[CV]][,i] , data = training[[CV]]))$coefficients[2,4]&#10;}&#10;&#10;## Now we will remove &quot;i&quot; and also we will name the columns of our new object.&#10;rm(i)&#10;names(Preselection[[CV]]) &amp;lt;- c(&quot;Variable&quot;, &quot;pValue&quot;)&#10;&#10;## Now we will make note of those variables whose p values were less than 0.1.&#10;Selected[[CV]] &amp;lt;- Preselection[[CV]][which(Preselection[[CV]]$pValue &amp;lt;= 0.1),] ; row.names(Selected[[CV]]) &amp;lt;- NULL&#10;&#10;## Fit a model using the pre-selected variables to the training fold&#10;## First we must save the variable names as a character string&#10;temp &amp;lt;- NULL&#10;for (k in 1:(as.numeric(length(Selected[[CV]]$Variable)))) {&#10;temp[k] &amp;lt;- paste(&quot;training[[CV]]$V&quot;,Selected[[CV]]$Variable[k],&quot; + &quot;,sep=&quot;&quot;)}&#10;variables[[CV]] &amp;lt;- paste(temp, collapse = &quot;&quot;)&#10;variables[[CV]] &amp;lt;- substr(variables[[CV]],1,(nchar(variables[[CV]])-3))&#10;&#10;## Now we can use this string as the independent variables list in our model&#10;y &amp;lt;- training[[CV]][,&quot;Dependent&quot;]&#10;form &amp;lt;- as.formula(paste(&quot;y ~&quot;, variables[[CV]]))&#10;&#10;## We can build a model using all of the pre-selected variables&#10;fit[[CV]] &amp;lt;- lm(form, training[[CV]])&#10;&#10;## Then we can build new models using stepwise removal of these variables using&#10;## the MASS package&#10;stepw[[CV]] &amp;lt;- stepAIC(fit[[CV]], direction=&quot;both&quot;)&#10;&#10;## End for loop&#10;}&#10;&#10;## Now you have your ten training and validation sets saved as training[[CV]]&#10;## and testing[[CV]]. You also have results from your univariate pre-selection&#10;## analyses saved as Preselection[[CV]]. Those variables that had p values less&#10;## than 0.1 are saved in Selected[[CV]]. Models built using these variables are&#10;## saved in fit[[CV]]. Reduced versions of these models (by stepwise selection)&#10;## are saved in stepw[[CV]].&#10;&#10;## Now you might consider using the predict.lm function from the stats package&#10;## to apply your ten models to their corresponding validation folds. You then&#10;## could look at the performance of the ten models and average their performance&#10;## statistics together to get an overall idea of how well your data predict the&#10;## outcome.&#10;################################################################################&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Before performing cross-validation, it is important that you read about its proper use. These two references offer excellent discussions of cross-validation:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Simon RM, Subramanian J, Li MC, Menezes S. Using cross-validation to evaluate predictive accuracy of survival risk classifiers based on high-dimensional data. Brief Bioinform. 2011 May;12(3):203-14. Epub 2011 Feb 15. &lt;a href=&quot;http://bib.oxfordjournals.org/content/12/3/203.long&quot; rel=&quot;nofollow&quot;&gt;http://bib.oxfordjournals.org/content/12/3/203.long&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Richard Simon, Michael D. Radmacher, Kevin Dobbin and Lisa M. McShane. Pitfalls in the Use of DNA Microarray Data for Diagnostic and Prognostic Classification. JNCI J Natl Cancer Inst (2003) 95 (1): 14-18. &lt;a href=&quot;http://jnci.oxfordjournals.org/content/95/1/14.long&quot; rel=&quot;nofollow&quot;&gt;http://jnci.oxfordjournals.org/content/95/1/14.long&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;These papers are geared toward biostatisticians, but would be useful for anyone.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, always keep in mind that using stepwise regression is dangerous (although using cross-validation should help to alleviate overfitting). A good discussion of stepwise regression is available here: &lt;a href=&quot;http://www.stata.com/support/faqs/stat/stepwise.html&quot; rel=&quot;nofollow&quot;&gt;http://www.stata.com/support/faqs/stat/stepwise.html&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me know if you have any additional questions!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-01T16:39:19.773" Id="22088" LastActivityDate="2012-02-01T22:58:19.113" LastEditDate="2012-02-01T22:58:19.113" LastEditorUserId="8724" OwnerUserId="8724" ParentId="22085" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I think you will need to impute values for the missing variables.  There are several ways of doing this - if you're lucky the choice won't matter that much, so I'd try a couple.  One obvious way is to just assign the column mean of V3 (and other missing variables) to your new observation (as chl implies in his comment); another way would be to create a statistical model (linear or more complicated) to predict V3 etc using V1 and  V2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have your imputed values you can then project the new datapoint onto your space.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-01T18:08:00.673" Id="22095" LastActivityDate="2012-02-01T18:08:00.673" OwnerUserId="7972" ParentId="21677" PostTypeId="2" Score="1" />
  
&#10;$$&#10;Unfortunately, the marginal distribution of $V_1+V_2$ is not so straightforward since it is the sum of two inverse gamma random variables with different scale and shape parameters... Even the sum $V_1^{-1}+V_2^{-1}$ does not enjoy a simple expression when the scale parameters are not the same!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-01T21:28:12.603" Id="22113" LastActivityDate="2012-02-01T21:28:12.603" OwnerUserId="7224" ParentId="22106" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;Are density and dot gain both measures of quality?  Are they highly correlated?  If yes and yes it might be easier to create a single quality index and see what happens to that over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whatever the answer to that, it sounds like you want something like &lt;a href=&quot;http://en.wikipedia.org/wiki/Local_regression&quot; rel=&quot;nofollow&quot;&gt;locally weighted scatterplot smoothing&lt;/a&gt;.  There is a good implementation in R (free) and in some commercial stats packages.  This technique would be effective if what you are most worried about is a locally consistent downwards trend (where &quot;local&quot; means recent observations).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-02T10:36:55.887" Id="22158" LastActivityDate="2012-02-02T10:36:55.887" OwnerUserId="7972" ParentId="22144" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Actually I am preparing a paper in which I am using your approach of treating a response on a likert item as if it is the overt aggregate of a covert series of binomial trials. &lt;/p&gt;&#10;&#10;&lt;p&gt;In my paper the binomial distribution is used in order to explain the shape &#10;of the observed frequency distributions. The rationale behind this approach is given by two assumptions. In many applets, showing how the binomial distribution comes into existence, one has repeated independent Bernoulli trials by a single ball falling through an array of pins. Each time a ball falls onto a pin, it will bounce to the right (i.e. a success) with probability p or to the left (i.e. a failure) with probability 1-p. After the ball falls through the array, it lands in a bin labeled by the corresponding number of successes. In my paper the process of decision making is also seen as a series of repeated independent Bernoulli trials in which, at each trial, the subject decide to agree or not to agree to the statement in question. The two assumptions read as follows. &lt;/p&gt;&#10;&#10;&lt;p&gt;(i) At each independent Bernoulli trial the subject makes a decision to agree with probability p or not to agree (disagree) with probabiliity 1-p. &lt;/p&gt;&#10;&#10;&lt;p&gt;(ii) If five categories of response are available for the statement, then the number of times a Bernoulli decision is made regarding the decision to agree or not to agree (disagree) is equal to 4 (5-1).&lt;/p&gt;&#10;&#10;&lt;p&gt;The final choice for a specific response category is given by the following rules.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;If in all (four) cases a Bernoulli decision of agreement is made, then the response 'strongly agree' will be given.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If in three cases a Bernoulli decision of agreement is made, then the response 'agree' will be given.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If in two cases a Bernoulli decision of agreement is made, then the response 'undecided' will be given.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If in only one case a Bernoulli decision of agreement is made, then the response 'disagree' will be given.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If in no case a Bernoulli decision of agreement is made, then the response 'stronglly disagree' will be given.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A similar reasoning can be given using 'disagree' decisions. &#10;In order to obtain a binomial distribution, the scoring of the response categories is as follows.&lt;/p&gt;&#10;&#10;&lt;p&gt;strongly disagree = 0, disagree = 1, neutral = 2, agree = 3, strongly agree = 4&lt;/p&gt;&#10;&#10;&lt;p&gt;These two assumptions lead to a binomial distribution for the response frequencies provided that there are no systematic differences between the respondents. &lt;/p&gt;&#10;&#10;&lt;p&gt;I hope you can agree. I would appriciate very much if you could improve my english in the above text.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-02T11:28:44.513" Id="22159" LastActivityDate="2012-02-02T13:23:00.393" LastEditDate="2012-02-02T13:23:00.393" LastEditorUserId="930" OwnerUserId="8914" ParentId="2401" PostTypeId="2" Score="-1" />
  
  
  
  <row Body="&lt;p&gt;It looks like your survey 2 is a convenience sample. I don't know what it can be useful for. Without a clear sampling strategy, you cannot generalize to the population in any meaningful way. At best, you might be able to utilize Survey 2 to build a model of how variables are interrelated, and then try to improve your estimates from Survey 1 using some sort of &lt;a href=&quot;http://www.google.com/search?q=model+assisted+survey+sampling&quot; rel=&quot;nofollow&quot;&gt;generalized regression estimation&lt;/a&gt; but to get there, you need to make sure your sample is not biased.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, suppose you want to predict how much income taxes can be collected in the economy. Suppose you use something like the US Current Population Survey as your survey 1. This is a very well designed survey, with weights, poststratification, bells and whistles, whatever have you. Then you also have a survey that you hand out in the local unemployment offices, and only hope that most people will write something in. This is your survey 2. You don't know how well it reaches to your population -- in all likelihood, you are more likely to reach those who are seeking jobs more actively, and show up in the local offices more often. You won't reach the frustrated workers who quit looking for a job, or those who are not eligible for the unemployment benefits but would be looking for a job otherwise, some seasonal workers, and a number of other people. You don't know any of that though: somebody just handed you Survey 2 and said, &quot;This is our rich data base, make sense out of it&quot;. Well, this is a biased sample to begin with. If you fit a regression model of individual's earnings using this data set, you will likely get wrong estimates: the sample censors out those with higher earnings in full time, permanent jobs, and probably has way more people with low education than there are in general population. So what's the use of Survey 2 for you? As I said, I doubt it has much value in this purpose.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is not helping at all that you give zero background information about what the survey is about, what sampling units are, etc. I understand that you are probably bound by your employer, or your client, or whatever form of supervisor you have. But without more detail, all we can give you is some sort of handwaving advice. I can point you to technical literature about combining information from several surveys (using Bayesian or empirical likelihood methods), but I am not sure it will help much at this point.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-02T20:05:05.233" Id="22186" LastActivityDate="2012-02-02T20:05:05.233" OwnerUserId="5739" ParentId="22182" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;An ROC curve is not useful in this setting, although the generalized ROC area (c-index, which does not require any dichotomization at all) is.  The R &lt;code&gt;rms&lt;/code&gt; package will compute the c-index and cross-validated or bootstrap overfitting-corrected versions of it.  You can do this without holding back any data if you fully pre-specify the model or repeat a backwards stepdown algorithm at each resample.  If you truly want to do external validation, i.e., if your validation sample is enormous, you can use the following &lt;code&gt;rms&lt;/code&gt; functions: &lt;code&gt;rcorr.cens&lt;/code&gt;, &lt;code&gt;val.surv&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-02-02T21:02:30.270" Id="22190" LastActivityDate="2012-02-02T21:02:30.270" OwnerUserId="4253" ParentId="22177" PostTypeId="2" Score="5" />
  <row AnswerCount="2" Body="&lt;p&gt;I am working for two signals. One is a dataset with 10 equally spaced impulses. Another is a dataset with randomly spaced impulses of identical length. The higher order moments, such as kurtosis, skewness or standard deviation are not sensitive to the two signals. Who can help look for an indicator to identify it?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-02T21:17:47.370" Id="22191" LastActivityDate="2012-05-03T22:14:48.407" LastEditDate="2012-02-03T07:35:35.353" LastEditorUserId="930" OwnerUserId="8928" PostTypeId="1" Score="0" Tags="&lt;signal-detection&gt;" Title="How to distinguish between periodic and random impulse?" ViewCount="373" />
  <row AnswerCount="0" Body="&lt;p&gt;I've written a user defined splitting function to use with rpart, its returning a 'vector of goodness', but the tree that is returned never has any splits, just one node. &lt;/p&gt;&#10;&#10;&lt;p&gt;using the anova method on the same data provides a tree with many nodes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been interrogating the code, but rpart feels like a black box so far, I can't figure out why there wouldn't be any splits or how to trouble shoot the problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure whether this belongs here or @stackoverflow.  I'll include my function&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;distance &amp;lt;- function(y, wt, x, parms, continuous) {&#10;    # return a vector of goodness using a distance measure&#10;    # find distance between treatment and control in left and right,&#10;    # gain = difference between the two, normalized &#10;&#10;    if (missing(parms) || is.null(parms['treated']))&#10;        stop(&quot;requires vector indicating treatment&quot;)&#10;    treated &amp;lt;- which(parms['treated'] == max(parms['treated']))&#10;    #treated &amp;lt;- which(parms$treated == max(parms$treated))&#10;&#10;    goodness &amp;lt;- numeric(length(x) - 1)&#10;&#10;    for (i in 1:(length(x)-1)) {&#10;        left &amp;lt;- which(x &amp;gt;= x[i])&#10;        left.data &amp;lt;- y[left]&#10;        right.data &amp;lt;- y[-left]&#10;&#10;        left.distance &amp;lt;- dist(rbind(left.data[treated], left.data[-treated])) &#10;        right.distance &amp;lt;- dist(rbind(right.data[treated], right.data[-treated]))  &#10;&#10;        treated.frac &amp;lt;- length(left.data[treated])/length(left)&#10;&#10;        normalization &amp;lt;- gini(c(treated.frac, 1-treated.frac)) * left.distance&#10;        + treated.frac * gini(left.data[treated])&#10;        + (1 - treated.frac) * gini(left.data[-treated]) + .5&#10;&#10;        goodness[i] &amp;lt;- (left.distance - right.distance)/normalization&#10;    }&#10;    list(goodness = goodness, direction=rep(1, length(x) - 1))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-02-02T21:39:39.760" FavoriteCount="1" Id="22193" LastActivityDate="2012-02-02T21:39:39.760" OwnerUserId="2149" PostTypeId="1" Score="1" Tags="&lt;cart&gt;&lt;rpart&gt;" Title="decision tree using user defined split function in rpart: No splits returned when tree is run" ViewCount="431" />
  
  
  
  <row Body="&lt;p&gt;Here is an explanation of how &lt;a href=&quot;http://www.bonziniusa.com/foosball/tournament/TournamentRankingSystem.html&quot; rel=&quot;nofollow&quot;&gt;Elo rankings are used in an actual foosball league&lt;/a&gt;. It goes into quite a lot of depth.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-03T14:24:38.403" Id="22221" LastActivityDate="2013-07-29T07:01:54.207" LastEditDate="2013-07-29T07:01:54.207" LastEditorUserId="6029" OwnerUserId="8920" ParentId="17246" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am writing a hierarchical BUGS model that involves both linear and angle variables. I want the hyper-parameters to be normally distributed, which is straight-forward for the linear variables, but I'm not sure what to do about the angle variables. I would gladly use a von Mises distribution or a truncated normal, but neither is available in the BUGS language. My angle variables range from 0 to pi. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model {&#10;&#10;   # parameters&#10;   for ( i in 1:P ) {&#10;      Y[i] ~ dnorm(Yhat[i], eachtau[plot[i]])&#10;      Yhat[i] &amp;lt;- C[plot[i]] + a[plot[i]] * exp(-v[plot[i]] * year[i]) * cos(w[plot[i]] * year[i] - phi[plot[i]])&#10;   }&#10;&#10;   # hyperparameters&#10;   for ( i in 1:M ) {&#10;      C[i] ~ dnorm(mu.C, tau.C)&#10;      a[i] ~ dnorm(mu.z, tau.z)&#10;      v[i] ~ dnorm(mu.v, tau.v)&#10;&#10;      # what do we do with these angles?!&#10;      w[i] ~ ?&#10;      phi[i] ~ ?&#10;&#10;      eachtau[i] ~ dgamma(0.001, 0.001)&#10;   }&#10;&#10;   # priors&#10;   mu.C ~ dnorm(0.0, 0.001)&#10;   tau.C ~ dgamma(0.001, 0.001)&#10;   mu.z ~ dnorm(0.0, 0.001)&#10;   tau.z ~ dgamma(0.001, 0.001)&#10;   mu.v ~ dnorm(0.0, 0.001)&#10;   tau.v ~ dgamma(0.001, 0.001)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If it's important, I'm actually using JAGS (rather than OpenBUGS or WinBUGS), using the rjags package in R, but I believe the model syntax is the same for both.&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the simplest way to model the angle variables with something like a normal in BUGS?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-03T19:29:26.760" Id="22234" LastActivityDate="2012-09-02T14:20:03.637" LastEditDate="2012-09-02T14:20:03.637" LastEditorUserId="1036" OwnerUserId="8951" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;multilevel-analysis&gt;&lt;bugs&gt;&lt;jags&gt;&lt;directional-statistics&gt;" Title="How do I specify priors for angle parameters in BUGS/JAGS?" ViewCount="376" />
  <row AcceptedAnswerId="22272" AnswerCount="1" Body="&lt;p&gt;If $\text{RMSEA} = 0$, it means $\chi^2 &amp;lt; df$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does it disqualify RMSEA as a criterion to evaluate the model fit, or is it just the explanation why it is zero?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-04T07:31:01.060" FavoriteCount="2" Id="22252" LastActivityDate="2012-02-04T18:59:44.223" LastEditDate="2012-02-04T18:59:44.223" LastEditorUserId="930" OwnerUserId="8781" PostTypeId="1" Score="2" Tags="&lt;goodness-of-fit&gt;&lt;sem&gt;" Title="When can RMSEA be zero?" ViewCount="1253" />
  
  <row AcceptedAnswerId="22262" AnswerCount="1" Body="&lt;p&gt;I'm looking for a reference for the Naive bayse classifier to put in my work. Not sure what I'm missing but a &lt;a href=&quot;http://scholar.google.com/scholar?hl=en&amp;amp;q=Naive%20Bayes%20classifier&amp;amp;btnG=Search&amp;amp;as_sdt=0,5&amp;amp;as_ylo=&amp;amp;as_yhi=&amp;amp;as_vis=1&quot; rel=&quot;nofollow&quot;&gt;scholar search&lt;/a&gt;  didn't yield any meaningful results. any idea ?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;: I meant for a reference to put in the bibliography...&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-02-04T08:11:58.467" FavoriteCount="1" Id="22255" LastActivityDate="2012-02-04T15:19:25.477" LastEditDate="2012-02-04T09:38:57.363" LastEditorUserId="930" OwnerUserId="6637" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;naive-bayes&gt;&lt;references&gt;" Title="Canonical reference for Naive Bayes classifier" ViewCount="145" />
  <row Body="&lt;p&gt;It's not a peer-reviewed source, but I like &lt;a href=&quot;http://cscs.umich.edu/~crshalizi/notebooks/power-laws.html&quot; rel=&quot;nofollow&quot;&gt;this note&lt;/a&gt; by &lt;a href=&quot;http://www.cmu.edu/index.shtml&quot; rel=&quot;nofollow&quot;&gt;CMU&lt;/a&gt; stats professor &lt;a href=&quot;http://cscs.umich.edu/~crshalizi/&quot; rel=&quot;nofollow&quot;&gt;Cosma Shalizi&lt;/a&gt;. He's also an author on &lt;a href=&quot;http://arxiv.org/abs/0706.1062&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;, about estimating such things from data.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-04T17:09:55.640" Id="22268" LastActivityDate="2012-02-04T17:09:55.640" OwnerUserId="8485" ParentId="22238" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In simple linear regression, $t = \frac{\hat\beta_1 - \beta_1}{\hat\sigma \sqrt{S_{xx}}} $ is the test statistic for the null hypothesis $H_0 : \beta_1 = 0$. How can I express $t^2$ as an F-distribution, i.e. as the ratio of two independent chi-squared random variables divided by their respective degrees of freedom?&lt;/p&gt;&#10;&#10;&lt;p&gt;So far I have $t^2 = \frac{\hat\beta_1^2}{\hat\sigma^2 / S_{xx}}$ since $\beta_1 = 0$ and then I've shown that the denominator $\hat\sigma^2 = \frac{\sum (y_i - (\hat\beta_0 + \hat\beta_1x_1)^2 }{n} = \frac{\sum e_i^2}{n}$ is a chi-squared distribution divided by its degrees of freedom, since $e_i$ is the $i$th error and the $e_i$s are independent, normally distributed random variables. But I have no idea how to show that the numerator $\hat\beta_1^2 S_{xx}$ is also a chi-squared distribution... how could I go about doing that?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-02-04T23:40:16.817" Id="22284" LastActivityDate="2012-02-05T02:48:20.890" LastEditDate="2012-02-05T02:48:20.890" LastEditorUserId="8974" OwnerUserId="8974" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;linear-model&gt;&lt;f-distribution&gt;" Title="In simple linear regression, how do I show that the squared test statistic for the null hypothesis has an F-distribution?" ViewCount="349" />
  <row AcceptedAnswerId="22475" AnswerCount="3" Body="&lt;p&gt;OK, I am not a statistician (Not even close). I am a High Performance Computing researcher and I wanted a few test cases for &lt;strong&gt;Large&lt;/strong&gt; (Greater than 5000x5000) Dense Matrices. I had asked &lt;a href=&quot;http://scicomp.stackexchange.com/questions/1108/where-do-dense-matrices-occur&quot;&gt;here&lt;/a&gt; and a few other places but never got any reply from a statistician. I am very much interested in trying out my codes on a statistics problem. Could you suggest an application in statistics where one needs to solve $Ax=b$ for x where $A$ is dense and square.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would highly appreciate it if you could also give me applications where A has no structure i.e. No symmetry, No Positive-Definiteness etc. But thats not entirely necessary. A large dense matrix with a good application suffices.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm sorry if this question appears open or vague but I can't imagine a better place to ask this question.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2012-02-05T12:07:27.243" FavoriteCount="1" Id="22294" LastActivityDate="2012-02-13T09:37:11.383" LastEditDate="2012-02-08T08:59:23.030" LastEditorDisplayName="user8968" OwnerDisplayName="user8968" PostTypeId="1" Score="8" Tags="&lt;large-data&gt;&lt;matrix&gt;" Title="What are some use of dense matrices in statistics?" ViewCount="474" />
&#10;$$&#10;is a convergent (in $n$) approximation to the true cdf, $F$. Therefore, any quantity depending on $F$, e.g. an expectation, $\mathbb{E}_F[h(X)]$, or the distribution of a statistic $\psi(X_1,\ldots,X_n)$, can be approximated by the corresponding quantity under $\hat F_n$. Which can only be evaluated by simulation, except for special cases. For instance, determining the bias of &#10;$$
&#10;\beta= \hat \sigma^2_n (x^*_1,\ldots,x^*_n) - \hat \sigma^2_n (x_1,\ldots,x_n) 
  
  <row Body="&lt;p&gt;The brief answer is &lt;em&gt;random sampling&lt;/em&gt;, but the more difficult issue is determining the size of the random sample that you should use.  One efficient solution to that problem is provided by &lt;em&gt;progressive sampling&lt;/em&gt;—a method that Foster Provost, Tim Oates, and I developed in the late 1990s [1].  The approach begins with a small sample size and increases sample size according to a &lt;em&gt;sampling schedule&lt;/em&gt;, checking whether model accuracy increases at each iteration.  We show that a geometric schedule (e.g., doubling the sample size on each iteration) is asymptotically no worse than knowing the correct sample size in advance.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;[1] F. Provost, D. Jensen, and T. Oates (1999). Efficient progressive sampling. Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.&#10;&lt;a href=&quot;http://pages.stern.nyu.edu/~fprovost/Papers/progressive.ps&quot; rel=&quot;nofollow&quot;&gt;http://pages.stern.nyu.edu/~fprovost/Papers/progressive.ps&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-05T16:27:26.143" Id="22303" LastActivityDate="2012-02-05T16:27:26.143" OwnerUserId="8981" ParentId="22291" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;In the 1980s dynamic time warping was the method used for template matching in speech recognition. The aim was to try to match time series of analyzed speech to stored templates, usually of whole words. The difficulty is people speak at different rates. DTW was used to register the unknown pattern to the template. It was called &quot;rubber sheet&quot; matching. Basically you search through some constrained possibilities of how the time series can locally be stretched to optimize the global fit. This approach was shown to be pretty much the same thing as hidden Markov models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-06T10:11:06.733" Id="22335" LastActivityDate="2012-02-06T10:11:06.733" OwnerUserId="8996" ParentId="22209" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I know there're at least two approaches for nonparametric curve estimation: kernel and orthogonal basis.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are their advantages and disadvantages over each other ?&lt;/p&gt;&#10;&#10;&lt;p&gt;And what are the typical application scenarios of each approach ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone give a hopefully not-too-long explaination ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-06T14:19:07.777" Id="22345" LastActivityDate="2012-02-07T02:54:50.403" LastEditDate="2012-02-07T02:54:50.403" LastEditorUserId="5708" OwnerUserId="5708" PostTypeId="1" Score="4" Tags="&lt;estimation&gt;&lt;nonparametric&gt;&lt;kernel&gt;" Title="Compare two nonparametric curve estimation approaches: kernel and orthogonal basis" ViewCount="88" />
  
  <row AcceptedAnswerId="22360" AnswerCount="4" Body="&lt;p&gt;How can one obtain standardized (fixed effect) regression weights from a multilevel regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;And, as an &quot;add-on&quot;: What is the easiest way to obtain these standardized weights from a &lt;code&gt;mer&lt;/code&gt;-object (from the &lt;code&gt;lmer&lt;/code&gt; function of the &lt;code&gt;lme4&lt;/code&gt;package in &lt;code&gt;R&lt;/code&gt;)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-06T14:20:47.137" FavoriteCount="3" Id="22346" LastActivityDate="2012-12-20T17:01:02.183" LastEditDate="2012-02-06T16:42:42.247" LastEditorUserId="6082" OwnerUserId="6082" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;multilevel-analysis&gt;&lt;lmer&gt;&lt;regression-coefficients&gt;&lt;standardization&gt;" Title="Standardized beta weights for a multilevel regression" ViewCount="6467" />
  
  <row Body="&lt;p&gt;To be explicit, let's make some assumptions and go from there. Assumptions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;During each of the two periods the publication rate 'r' is constant (possibly different between periods.&lt;/li&gt;&#10;&lt;li&gt;Data from the two periods is independent.&lt;/li&gt;&#10;&lt;li&gt;Data is drawn iid.&lt;/li&gt;&#10;&lt;li&gt;The deviation from the rate 'r' is modeled as Gaussian noise. (easily generalized, but lets keep it simple for now)&lt;/li&gt;&#10;&lt;li&gt;The variances in these Gaussians are sufficiently close to the sample variances and we can thus assume them to be equal to the sample variances. (This leaves us with one parameter families of random variables indexed by $r_1$ and $r_2$.)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Okay, suppose you had two data sets (one for each publication period) $x_j^{(1)}$ and $x^{(2)}_k$ drawn from the random variables $X^{(1)}$ and $X^{(2)}$ for every $j = 1..M$ and $k = 1..N$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We model these random variables as $$X^{(1)} \sim r_1 + N(0,\sigma^2) \text{ and } X^{(2)} \sim r_2 + N(0,\sigma^2) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;First, lets compute the likelihood of each of the data sets:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$l^{(1)}(r_1) = p(x^{(1)}_1,...,x^{(1)}_M;r_1) = \prod^M_{j=1}p(x^{(1)}_j;r_1)$$ with $l^{(2)}(r_2)$ defined similarly. &lt;/p&gt;&#10;&#10;&lt;p&gt;From what you said, the quantity you were interested in was the probability that one of the rates was greater than the other. This can be computed, given our assumptions, as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(r_1&amp;gt;r_2) = \int^\infty_0\int^\infty_0 1_{r_1 &amp;gt; r2}l^{(1)}(r_1)l^{(2)}(r_2)dr_1dr_2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously, some of the assumptions can be changed according to personal taste. This generative model wouldn't make much sense if we used it actually generate new data since the normal allows for negative values of $X$, but, for these purposes, this should be a good approximation to what you're looking for, philosophical differences aside. All that is left is picking your favourite numerical methods to compute these values. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-06T15:34:48.110" Id="22348" LastActivityDate="2012-02-06T15:34:48.110" OwnerUserId="8242" ParentId="22266" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The chi-squared test is essentially &lt;em&gt;always a one-sided test&lt;/em&gt;.  Here is a loose way to think about it: the chi-squared test is basically a 'goodness of fit' test.  Sometimes it is explicitly referred to as such, but even when it's not, it is still often in essence a goodness of fit.  For example, the chi-squared test of independence on a 2 x 2 frequency table is (sort of) a test of goodness of fit of the first row (column) to the distribution specified by the second row (column), and vice versa, simultaneously.  Thus, when the realized chi-squared value is way out on the right tail of it's distribution, it indicates a poor fit, and if it is far enough, relative to some pre-specified threshold, we might conclude that it is so poor that we don't believe the data are from that reference distribution.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If we were to use the chi-squared test as a two-sided test, we would also be worried if the statistic were too far into the &lt;em&gt;left&lt;/em&gt; side of the chi-squared distribution.  This would mean that we are worried the fit might be &lt;em&gt;too good&lt;/em&gt;.  This is simply not something we are typically worried about.  (As a historical side-note, this is related to the controversy of whether Mendel fudged his data.  The idea was that his data were too good to be true.  See &lt;a href=&quot;http://www.amjbot.org/content/88/5/737.full&quot;&gt;here&lt;/a&gt; for more info if you're curious.)&lt;/p&gt;&#10;" CommentCount="17" CreationDate="2012-02-06T16:46:55.497" Id="22350" LastActivityDate="2012-05-20T16:57:35.477" LastEditDate="2012-05-20T16:57:35.477" LastEditorUserId="7290" OwnerUserId="7290" ParentId="22347" PostTypeId="2" Score="26" />
  <row Body="&lt;p&gt;For standard linear models regressed with lm() you can either scale() your predictors data or just use this simple formula:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm.results = lm(mydata$Y ~ mydata$x1)&#10;&#10;sd.y = sd(mydata$Y)&#10;sd.x1 = sd(mydata$x1)&#10;x1.Beta = coef(lm.results)[&quot;mydata$x1&quot;] * (sd.x1 / sd.y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-02-06T17:02:22.040" Id="22351" LastActivityDate="2012-02-06T17:09:48.847" LastEditDate="2012-02-06T17:09:48.847" LastEditorUserId="7795" OwnerUserId="7795" ParentId="22346" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Simply scale your explanatory variables to having mean of zero and variance of one before you put them in the model.  Then the coefficients will all be comparable.  The mixed effects nature of the model doesn't impact on this issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to do it, and least likely to go wrong, is to use scale() before you fit the model.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-06T18:55:09.017" Id="22360" LastActivityDate="2012-02-06T18:55:09.017" OwnerUserId="7972" ParentId="22346" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;MAE is more intuitive than MSE to simply evaluate the overall error.&lt;/p&gt;&#10;&#10;&lt;p&gt;MSE is easier to handle mathematically for variance analysis. For example, MSE is used to calculate the error variance $s_e^2$, which is a recurring value in regression statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-07T00:54:47.590" Id="22370" LastActivityDate="2012-02-07T00:54:47.590" OwnerUserId="7795" ParentId="22344" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I think the following papers will address your question:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Walsh SJ. Goodness-of-fit issues in ROC curve estimation. Med Decis Making. 1999 Apr-Jun;19(2):193-201. PMID: 10231082.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Zou, K.H., et al. Receiver-Operating Characteristic Analysis for Evaluating Diagnostic Tests and Predictive Models. Circulation.2007; 115: 654-657doi: 10.1161/​CIRCULATIONAHA.105.594929&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Zou, K.H., et al. A global goodness-of-fit test for receiver operatingcharacteristic curve analysis via the bootstrap method. Journal of Biomedical Informatics. &lt;a href=&quot;http://www.spl.harvard.edu/archive/spl-pre2007/pages/papers/zou/Zou2005GOF.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.spl.harvard.edu/archive/spl-pre2007/pages/papers/zou/Zou2005GOF.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Here is an example of how to do a goodness of fit analysis using SAS: &lt;a href=&quot;http://www.sfu.ca/sasdoc/sashtml/stat/chap39/sect49.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.sfu.ca/sasdoc/sashtml/stat/chap39/sect49.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-07T12:15:35.157" Id="22384" LastActivityDate="2012-02-07T12:15:35.157" OwnerUserId="8724" ParentId="22376" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;I need to analyse a dataset of clinical rehabilitation data. I am interested in hypothesis-driven relationships between quantified &quot;input&quot; (amount of therapy) and changes in health status. Although the dataset is relatively small (n~70) we have repeated data reflecting temporal changes in both. I am familiar with non-linear mixed effects modelling in R however am interested in potential &quot;causal&quot; relationships between input and output here and thus am considering repeated measures applications of SEM&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd appreciate advice on which if any of the SEM packages for R (sam, lavaan, openmx?) are best suited to repeated measures data, and particularly recommendations for textbooks (is there a &quot;Pinheiro and Bates&quot; of the field?).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-07T14:22:25.950" FavoriteCount="1" Id="22388" LastActivityDate="2012-02-25T17:55:48.070" LastEditDate="2012-02-07T14:38:42.930" LastEditorUserId="930" OwnerUserId="9016" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;longitudinal&gt;&lt;sem&gt;" Title="Repeated measures structural equation modeling" ViewCount="1211" />
  <row Body="&lt;p&gt;The use of the CLT for justifying the use of the Gaussian distribution is a common fallacy because the CLT is applied to the sample mean, not to individual observations. Therefore, increasing your sample size, does not mean that the sample is closer to normallity.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Gaussian distribution is commonly used because:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Maximum likelihood estimation is straightforward.&lt;/li&gt;&#10;&lt;li&gt;Bayesian inference is simple (using conjugate priors or Jeffreys-type priors).&lt;/li&gt;&#10;&lt;li&gt;It is implemented in most of the numerical packages.&lt;/li&gt;&#10;&lt;li&gt;There is a lot of theory about this distribution in terms of hypothesis testing.&lt;/li&gt;&#10;&lt;li&gt;Lack of knowledge about other options (more flexible).&#10;...&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Of course, the best option is to use a distribution that takes into account the characteristics of your context,  but this can be challenging.  However, is something that people should do &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Everything should be made as simple as possible, but not simpler.&quot; (Albert Einstein)&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope this helps.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best wishes.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-07T15:11:02.230" Id="22390" LastActivityDate="2012-02-07T15:30:09.890" LastEditDate="2012-02-07T15:30:09.890" LastEditorUserId="9018" OwnerUserId="9018" ParentId="22387" PostTypeId="2" Score="-1" />
  
  <row Body="&lt;p&gt;Method 1 doesn't work.  Method 2 has hope depending on how you do it.  It's better to enter principal components in descending order of variance explained.  A more interpretable approach is to do variable clustering, then reducing each cluster to a single score (not using Y), then fit a model with the cluster scores.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-02-07T17:36:43.803" Id="22395" LastActivityDate="2012-02-07T17:36:43.803" OwnerUserId="4253" ParentId="22393" PostTypeId="2" Score="5" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;We wish to know if a gambler will enter a lottery and what he expects to win if he does enter, based upon our uncertain knowledge about his beliefs regarding the lottery.&lt;/p&gt;&#10;&#10;&lt;p&gt;The gambler believes that his probability of winning the lottery is p and also believes that the lottery prize will be W if he wins.  But we do not know the gamblers belief’s p or W, only that they are distributed with density functions f(p) and f(W) and cumulative distribution functions F(p) and F(W) respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variables p and W are independent of each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;We assume that f(p) is positive in the interval (a,b) and is zero outside this interval.  &lt;/p&gt;&#10;&#10;&lt;p&gt;We assume that 0 &amp;lt; a &amp;lt; b &amp;lt;1.&lt;/p&gt;&#10;&#10;&lt;p&gt;f(p) is continuous and differentiable throughout. There is some neighbourhood of b where f(p) is nonincreasing.&lt;/p&gt;&#10;&#10;&lt;p&gt;We also assume that f(W) is positive in the interval (c,d) and is zero outside this interval.  c and d are both large dollar amounts where d&gt;c.&lt;br&gt;&#10;f(W) is continuous and differentiable throughout and there is some neighbourhood of d where f(W) is nonincreasing.&lt;/p&gt;&#10;&#10;&lt;p&gt;We assume that distributions f(p) and f(W) are both triangular in shape for the sake of simplicity.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The gambler will only invest in the lottery if pW is &gt; S&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two questions I need help with:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the probability that the gambler will invest in the lottery?&lt;/li&gt;&#10;&lt;li&gt;What is the expected value of the lottery if he chooses to invest?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-08T11:18:42.530" Id="22444" LastActivityDate="2012-02-08T15:36:39.410" LastEditDate="2012-02-08T15:36:39.410" LastEditorUserId="88" OwnerUserId="9043" PostTypeId="1" Score="1" Tags="&lt;probability&gt;" Title="Compound risk lottery" ViewCount="88" />
  
  
  <row Body="&lt;p&gt;This is a harder question if you don't have the $n\gg k$ and assuming that this makes them 'close enough' to independent to not affect the answer non-trivially. Lets proceed with these assumptions. Let $X_j \sim Binomial(n,\frac{1}{m})$ $\forall j = 1,..,m$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(\max_j X_j \geq k) = 1 - P(\max_j X_j &amp;lt; k)$$&#10;$$ = 1  - P(X_1 &amp;lt; k,...,X_m &amp;lt; k)$$&#10;and, assuming independence of the $m$ random variables,&#10;$$ = 1  - \prod^m_{j=1}P(X_j &amp;lt; k)$$&#10;$$ = 1  - [P(X_1 &amp;lt; k)]^m$$&#10;$$ = 1  - [\sum^{k-1}_{i=0} {n \choose i}(\frac{1}{m})^i(1-\frac{1}{m})^{n-i}]^m$$&lt;/p&gt;&#10;&#10;&lt;p&gt;or, if you have the binomial cdf function in the language you are using:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ = 1  - [Binomial\_cdf(k-1;n,\frac{1}{m})]^m$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-08T15:34:37.600" Id="22457" LastActivityDate="2012-02-08T16:37:17.700" LastEditDate="2012-02-08T16:37:17.700" LastEditorUserId="8242" OwnerUserId="8242" ParentId="22450" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;I would plot the values and perform a least products regression to see if there is anything interesting going on in the data, but there do seem to be statistical tests that would be appropriate. Comparing methods of measurement is an interest of my friend John Ludbrook. He has a review of statistical methods that you can get here: &lt;a href=&quot;http://www.cytel.com/papers/Ludbrook-Special-2002.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cytel.com/papers/Ludbrook-Special-2002.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-08T23:07:41.370" Id="22485" LastActivityDate="2012-02-08T23:07:41.370" OwnerUserId="1679" ParentId="22479" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;As you seem comfortable with generalized linear mixed models, and you don't seem to imply that you're interested in latent variables, perhaps you might want to take a piecewise approach using &lt;code&gt;lmer&lt;/code&gt; which you can then evaluate using a D-Sep test.  See Shipley, B. (2009). Confirmatory path analysis in a generalized multilevel context. Ecology, Ecology, 90, 363–368.  &lt;a href=&quot;http://dx.doi.org/10.1890/08-1034.1&quot; rel=&quot;nofollow&quot;&gt;http://dx.doi.org/10.1890/08-1034.1&lt;/a&gt; for an example.  He also provides R code in the appendix for how to calculate the test of D-Separation.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really want to get into latent variable modeling and SEM using maximum likelihood, check out &lt;a href=&quot;http://lavaan.org&quot; rel=&quot;nofollow&quot;&gt;http://lavaan.org&lt;/a&gt; - there's a great tutorial there that covers its capabilities as well as a section on latent growth curve models which may well be what you're after.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-09T00:10:59.813" Id="22487" LastActivityDate="2012-02-25T17:55:48.070" LastEditDate="2012-02-25T17:55:48.070" LastEditorUserId="5739" OwnerUserId="101" ParentId="22388" PostTypeId="2" Score="2" />
  
&#10;= \sum_{ {\bf t} \in \mathcal{T} } 
  
  <row AnswerCount="1" Body="&lt;p&gt;Does anyone know a way to calculate the effect size of a Fisher's exact test? Does that even make sense? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-09T10:18:52.247" FavoriteCount="1" Id="22508" LastActivityDate="2013-07-21T19:59:14.120" LastEditDate="2013-07-21T19:59:14.120" LastEditorUserId="7290" OwnerUserId="9077" PostTypeId="1" Score="4" Tags="&lt;chi-squared&gt;&lt;effect-size&gt;&lt;fishersexact&gt;" Title="Effect size for Fisher's exact test?" ViewCount="1251" />
  <row Body="&lt;p&gt;As an add-on to whuber's answer: The presented formula&lt;/p&gt;&#10;&#10;&lt;p&gt;$1 + 2 \rho \sigma \tau - \left(\rho^2 + \sigma^2 + \tau^2\right) \ge 0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;can be transformed into following inequality (Olkin, 1981):&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sigma\tau - \sqrt{(1-\sigma^2)(1-\tau^2)} \le \rho \le \sigma\tau + \sqrt{(1-\sigma^2)(1-\tau^2)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;A &lt;strong&gt;graphical representation&lt;/strong&gt; of the upper and lower limits for $\rho$ looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/akuNs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Olkin, I. (1981). Range restrictions for product-moment correlation matrices. Psychometrika, 46, 469-472. doi:10.1007/BF02293804&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-09T13:27:23.727" Id="22522" LastActivityDate="2012-02-09T13:27:23.727" OwnerUserId="6082" ParentId="5747" PostTypeId="2" Score="6" />
&#10;\leq a
  
  <row AnswerCount="1" Body="&lt;p&gt;While I was reading about  Bayesian networks, I run into &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_blanket&quot; rel=&quot;nofollow&quot;&gt;Markov blanket&lt;/a&gt;&quot; term and got severely confused with its independency in a Bayesian network graph. &lt;/p&gt;&#10;&#10;&lt;p&gt;Markov blanket briefly says that every node is only dependent on its parents, children and children's parents [it is gray area for node A in the picture].&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://upload.wikimedia.org/wikipedia/commons/e/ea/MarkovBlanket.png&quot; alt=&quot;Markov blanket&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the joint probability of this BN, $P(M,S,G,I,B,R)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://www.aiqus.com/upfiles/IMG_0095_copy.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If I follow the step parent only independency rule, it is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(M | S)P(S | G,I)P(I | B)P(R | B)P(G)P(B)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if I follow the &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_blanket&quot; rel=&quot;nofollow&quot;&gt;Markov Blanket independency&lt;/a&gt;, I end up with this (notice $P(I|\mathbf{G},B)$ is different):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(M | S)P(S | G,I)P(I | \mathbf{G},B)P(R | B)P(G)P(B)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So which is the correct joint probability of this BN?&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: &lt;a href=&quot;http://www.aiqus.com/questions/36374/what-is-the-joint-probability-of-this-bayesian-network&quot; rel=&quot;nofollow&quot;&gt;Crosslink of this question in AIQUS&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;Respective chapter and diagrams are below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://img828.imageshack.us/img828/9783/img0103s.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://img406.imageshack.us/img406/3788/img0104l.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-09T19:36:22.733" FavoriteCount="1" Id="22539" LastActivityDate="2012-02-11T11:11:19.070" LastEditDate="2012-02-11T09:03:40.093" LastEditorUserId="3419" OwnerUserId="3419" PostTypeId="1" Score="5" Tags="&lt;bayesian&gt;&lt;bayes-network&gt;" Title="Markov blanket vs normal dependency in a Bayesian network" ViewCount="1593" />
  <row AcceptedAnswerId="22545" AnswerCount="1" Body="&lt;p&gt;Let $X\,$ be a non-negative r.v. with known pdf $f(x|\theta)$ but with a single unknown parameter $\theta$. Suppose that the mean $\mu$ can be used to uniquely determine the value of $\theta$, i.e. if given the value of $\mu$, then the value of $\theta$ can be calculated directly. Thus, an alternative way of writing the pdf is $f(x|\mu)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the loss function $L(z,\mu)=\int_z^\infty (x-z)f(x|\mu)\,\mathrm{d}x = \mathbb{E}_{\mu}\{[X-z]^+\}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, is it possible to shown that this function is convex in $\mu$ for general $f(\cdot)$?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;By way of example, consider the exponential distribution with $f(x|\theta)=\theta e^{-x\theta}$. Then clearly $\theta=\frac{1}{\mu}$, and so write $f(x|\mu)=\frac{1}{\mu}e^{-x/\mu}$. Observing that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\int_z^\infty (x-z)f(x|\mu)\,\mathrm{d}x =\int_z^\infty xf(x|\mu)\,\mathrm{d}x-z(1-F(z|\mu))$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $F(x|\mu)$ is the cdf of $X$ given $\mu$, and using the fact that $\int_z^\infty xf(x|\mu)\,\mathrm{d}x=(z+\mu)e^{-z/\mu}$ and that $F(z|\mu)=1-e^{-z/\mu}$ for the exponential distribution, then clearly&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{dL(z,\mu)}{d\mu}=\left(\frac{z^2+z\mu+\mu^2}{\mu^2}\right)e^{-z/\mu}-\left(\frac{z}{\mu}\right)^2e^{-z/\mu}=\left(\frac{z}{u}+1\right)e^{-z/\mu}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{d^2L(z,\mu)}{d\mu^2}=\frac{z^2}{\mu^3}e^{-z/\mu} \geq 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the second derivative is non-negative, the loss function for the exponential distribution is convex with respect to the mean, $\mu$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Many thanks in advance for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-09T20:23:25.930" Id="22542" LastActivityDate="2014-11-23T17:01:24.107" LastEditDate="2014-11-23T17:01:24.107" LastEditorUserId="7224" OwnerUserId="8562" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;decision-theory&gt;&lt;loss-functions&gt;" Title="Convexity of loss function with respect to the mean" ViewCount="86" />
  
  <row AcceptedAnswerId="22560" AnswerCount="1" Body="&lt;p&gt;Seeking a textbook or other publication describing the Mantel-Haenszel test of a $2 \times 2 \times r$ table that provides at least one numeric example.  Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-10T00:41:43.207" Id="22555" LastActivityDate="2012-02-10T04:20:45.353" LastEditDate="2012-02-10T03:55:32.190" LastEditorUserId="2970" OwnerUserId="8137" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;books&gt;" Title="Seeking text describing the Mantel-Haenszel test" ViewCount="97" />
  
  
  <row Body="&lt;p&gt;I don't entirely agree with the premise of the question, i.e. I think there is no way in which computers could ever hope to replace statisticians, but to put a concrete example to why I think that:&lt;/p&gt;&#10;&#10;&lt;p&gt;The work which statisticians do with scientists, particularly, in the design and interpretation of experiments, requires not only a human mind but even a philosophical bent which it is inconceivable that computers could ever show.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless we end up in some sort of Skynet type situation, of course, in which case I reckon all bets are probably off as far as the future of all humanity, never mind about just the statisticians, is concerned :-)&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-02-10T21:49:04.137" CreationDate="2012-02-10T09:17:46.937" Id="22576" LastActivityDate="2012-02-10T09:17:46.937" OwnerUserId="199" ParentId="22572" PostTypeId="2" Score="6" />
  
  
  
  
&#10;&amp;amp;= \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(y - \mu)^2}{2 \sigma^2}}\ dy
  <row Body="&lt;p&gt;If all you want to do is open R from the command line, then just type the letter $\verb!R!$, but I think you might be asking how to run a script at the command line. &lt;/p&gt;&#10;&#10;&lt;p&gt;To run an R script, $\verb!example.txt!$ at the command line and write the log to a file, &#10;$\verb!out.txt!$, type&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R CMD BATCH example.r out.txt&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are a few other options you can pass with the '$\verb!-!$' flag. See &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/utils/html/BATCH.html&quot; rel=&quot;nofollow&quot;&gt;http://stat.ethz.ch/R-manual/R-devel/library/utils/html/BATCH.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;for more information.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-11T03:26:09.770" Id="22616" LastActivityDate="2012-02-11T03:32:13.117" LastEditDate="2012-02-11T03:32:13.117" LastEditorUserId="4856" OwnerUserId="4856" ParentId="22615" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="22703" AnswerCount="4" Body="&lt;p&gt;I have a ton of univariate samples ($x_i \in \mathbb{R}^+$).  I'd like an automated method to check for outliers and identify the outliers, if any are present.  A reasonable model for the distribution of the non-outliers is a mixture of Gaussians.  The number of Gaussians in the mixture and their parameters are not known a priori.  Can you suggest a simple method for identifying outliers?  Do you have any recommendations?  It'd be nice if it were simple to code up in Python.&lt;/p&gt;&#10;&#10;&lt;p&gt;Something quick and dirty -- say, easy to understand, easy to implement, and pretty effective-- beats something complex but optimal.  For example, I'm a bit reluctant to wade into something fancy based upon expectation maximization.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example parameters: I might have 10,000 samples or so.  The distribution of non-outliers might be a mixture of 2 Gaussians; or I might have a mixture of a few hundred Gaussians.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; People have asked how anything could possibly be an outlier, given these assumptions.  (Presumably, the unstated concern is that this problem may be unsolvable: if every data set is always explainable by some mixture model, then there's no basis to ever identify anything as an outlier.)  That's a fair question, so let me try to respond.  In my application domain, I can reasonably assume that there will be dozens of samples from each component Gaussian.  e.g., I might have 40,000 samples from a mixture of 100 Gaussians, where each Gaussian component has a probability no lower than 0.001 (so it is almost guaranteed that I have at least 10 samples from each Gaussian).  I realize I didn't state this assumption earlier, and I apologize for that.  However, with this additional assumption, I believe the problem is solvable.  There exist examples of data sets where one or more points can be considered outliers (they cannot reasonably be explained by any mixture model).  For example, consider a data set that has a single isolated point that is very far from all others: if it's far enough away, it can't be explained by the Gaussian mixture model and thus can be recognized as an outlier.  In conclusion, I believe that the problem is well-defined and is solvable (given the additional assumption stated here): there do exist example situations where some points can reasonably be identified as outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I'm not trying to propose a special or unusual definition of outlier.  I am happy to use the standard notion of outlier (e.g., a point that cannot reasonably be explained as having been generated by the hypothesized process, because it is too unlikely to have been generated by that process).&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-02-11T08:16:19.757" FavoriteCount="2" Id="22627" LastActivityDate="2012-04-25T20:41:43.810" LastEditDate="2012-02-12T21:09:12.217" LastEditorUserId="2921" OwnerUserId="2921" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;&lt;outliers&gt;&lt;mixture&gt;" Title="Detect outliers in mixture of Gaussians" ViewCount="1118" />
  
  <row Body="&lt;p&gt;I'm not sure I understand the issue here, but the MAD-Median rule:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{|X-M|}{MADN}&amp;gt;2.24$, where $M$ is the median and $MADN$ is the $\frac{\text{median absolute deviation from the median}}{0.6745}$&lt;/p&gt;&#10;&#10;&lt;p&gt;is pretty commonly used.  Wilcox's WRS package in R has an &lt;code&gt;out()&lt;/code&gt; function that fits this and returns the cases to keep and cases to drop, and I'm sure it would be easy to code in other languages.  On the face of it this would be an answer to your question - one of many of course because there is a vast literature on outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may need a more restrictive definition of &quot;outlier&quot;, of course.  If you are happy with any observations that are consistent with a mixed distribution of 100s of Gaussian variables it is hard to imagine anything being ruled an outlier.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-02-11T09:49:11.033" Id="22629" LastActivityDate="2012-02-11T10:44:59.337" LastEditDate="2012-02-11T10:44:59.337" LastEditorUserId="7972" OwnerUserId="7972" ParentId="22627" PostTypeId="2" Score="3" />
  
  
&#10;    \notin D}$ to $\sum_x$, i.e. $x$ is not necessarily restricted to be&#10;outside the training set, will both parts in NFL theorem still be&#10;true?&lt;/li&gt;&#10;&lt;li&gt;If the true relation between $x$ and $y$ are not assumed to be a&#10;deterministic function $F$ as $y=F(x)$, but instead conditional&#10;distributions $P(y|x)$, or a joint distribution $P(x,y)$ which is&#10;equivalent to knowing $P(y|x)$ and $P(x)$ (also see &lt;a href=&quot;http://stats.stackexchange.com/questions/22654/is-the-true-relation-between-independent-and-dependent-variables-assumed-to-be-a&quot;&gt;my another question&lt;/a&gt;), then I can change&#10;$\mathcal{E}_k (E|F,n)$ to be $$ \mathcal{E}_k(E|P(x,y),n) =
  
  
  <row Body="&lt;p&gt;thanks for updating your question with the scatterplot, it does give us some information we didn't have before. Eyeballing the scatterplot, it looks like 1v1 performance and 3v3 (adjusted) performance aren't related. What this tells us is that there is no simple relationship between 1v1 and 3v3 performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;That sounds like we're stuck, however assuming that the 3v3 mixes the players around a bit, so  the players don't have the same team mates for every 3v3, the compositional changes to the 3v3 teams may be masking overall individual performance within teams. The scatterplot could be telling us that, when matched with a lower skilled player, the presence of a higher skilled player on a 3v3 team does not automatically lift team performance (and vice versa).&lt;/p&gt;&#10;&#10;&lt;p&gt;To answer your second question, when you have enough data to change the rankings from 1500, look at those players who score low on the 1v1 ELO axis and have a higher ranking on the 3v3 ELO axis - this tells you the players with poorer individual skills who make large contributions to teams &lt;em&gt;assuming that the teams are matched overall on terms of mix of skills.&lt;/em&gt;  For example, if one poorer player keeps being matched with the two top players in a team, then the team result is likely due to the top players with the poorer player having probably little effect, and therefore the team result won't be an accurate reflection of how well the poorer player works in a team generally.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your first question, experience and skill will be highly correlated simply because practice tends to increase skill, so both factors are unstable over time. Could you further define how you wish to examine recent playing experience? Do you mean:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the number of games played over the last week/fortnight, so each time you look at this, you will use the same week/fortnight measure and ignore earlier games, or&lt;/li&gt;&#10;&lt;li&gt;whether, as the season progresses, does experience tend to mean that initial skill doesn't matter so much?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;These are two quite different questions and will require different approaches.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-11T18:59:01.267" Id="22651" LastActivityDate="2012-02-11T18:59:01.267" OwnerUserId="8605" ParentId="22195" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/1529/what-type-of-post-fit-analysis-of-residuals-do-you-use&quot;&gt;This&lt;/a&gt; looks like a similar question and didn't get many responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;Omitting tests such as Cook's D, and just looking at residuals as a group, I am interested in how others use residuals when assessing goodness-of-fit. I use the raw residuals:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;in a QQ-plot, for assessing normality&lt;/li&gt;&#10;&lt;li&gt;in a scatterplot of $y$ versus residuals, for eyeball check of (a) hetereoscedasticity and (b) serial autocorrelation.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For plotting $y$ versus residuals to examine the values for $y$ where outliers may occur, I prefer to use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Studentized_residual&quot; rel=&quot;nofollow&quot;&gt;studentized residuals&lt;/a&gt;. The reason for my preference is that it allows easy viewing of which residuals at which $y$-values are problematic, although &lt;a href=&quot;http://www-stat.wharton.upenn.edu/~waterman/Teaching/701f99/Class04/class04.pdf&quot; rel=&quot;nofollow&quot;&gt;standardised residuals&lt;/a&gt; provide an extremely similar result. My theory on which is used is that it depends on which university one went to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this similar to how others use residuals? Do others use this number of graphs in combination with summary statistics? &lt;/p&gt;&#10;" CommentCount="7" CommunityOwnedDate="2012-02-16T15:26:53.123" CreationDate="2012-02-11T21:15:38.577" FavoriteCount="5" Id="22653" LastActivityDate="2014-08-04T21:51:58.097" LastEditDate="2012-12-18T19:56:18.700" LastEditorUserId="5003" OwnerUserId="8605" PostTypeId="1" Score="16" Tags="&lt;goodness-of-fit&gt;&lt;residuals&gt;" Title="Raw residuals versus standardised residuals versus studentised residuals - what to use when?" ViewCount="4375" />
&#10;f_a(z_1) = \frac{2}{a^2}(a-z_1) ,\quad 0 &amp;lt; z_1 &amp;lt; a \&amp;gt; .
&#10;\Pr(D \leq d) = \iint_{\{z_1^2+z_2^2 \leq d^2\}} f_a(z_1) f_b(z_2) \rd z_1 \rd z_2 \&amp;gt; .
  
  <row Body="&lt;p&gt;Variance is one of true distribution characteristics indicating how widely values of given random variable are distributed. In a sense, it is a similar concept to width of range (difference between min and max) So in almost all cases, you can't say &quot;A percent of the variance in X is due to the Var(X) for female drivers and B percent is the rest. A + B should be 100 percent&quot; whether that variable is independent of gender.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to run conventional ANOVA, then Peter Ellis's answer is the right approach. Otherwise, describe what you want to achieve in detail.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-12T05:13:43.257" Id="22663" LastActivityDate="2012-02-12T15:19:12.010" LastEditDate="2012-02-12T15:19:12.010" LastEditorUserId="5597" OwnerUserId="5597" ParentId="22626" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;A brief answer is yes an inappropriate link function can lead to residuals appearing &quot;non-random&quot;.   however it can also be due to a predictor which needs to be transformed.  I am assuming that you are refering to a plot of residuals against the linear predictor.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to check that it isn't t a predictor is to create pseudo &quot;partial residuals&quot;.  These are done by $u_{ij}=\hat{g}(y_{i})-x_{i}^T\hat{\beta}+x_{ij}\hat{\beta}_{j}$ where i indexes observations and j indexes variables.  The &quot;hat&quot; on the link function refers to the first order taylor series of g commonly refered to as the working response in a irls algorithm.  you plot $u_{ij}$ against $x_{ij}$ (ie one plot per covariate) and it should look like a straight line.  If this passes then i would consider changing the link function.  Of course this is only feasible when you only have a small number of variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that there are no hard and fast rules for using diagnostics - this is one approach of many.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-12T05:23:35.027" Id="22664" LastActivityDate="2012-02-12T05:23:35.027" OwnerUserId="2392" ParentId="22661" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;See my comment above re whether variables 2 and 3 really can be used as a basis for stratification (they can't unless the survey you refer to there is a different survey to the one you are discussing the sampling method for now).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you try to select your sample based on three categorical variables you quickly end up with a lot of strata and complex sampling and weighting problems.  You would need to calculate the population in each cell of a three dimensional array, where each cell is a particular combination of the three variables; then specify a proportion of that population you are going to include in your survey (doesn't need to be the same proportion for each cell).  You also need to know each potential samplee's values on those three variables as part of your sample selection process.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative to using all three for sampling might be to select your sample on the basis of just one of your variables as strata, and bring the other two in through &lt;strong&gt;post-stratification&lt;/strong&gt; weighting.  Further, if you use the &lt;strong&gt;raking&lt;/strong&gt; technique you can get around the problem of so many &quot;cells&quot; in your population array, while still making sure that the weights for each total category of each variable (ie the marginal totals in your three dimensional array) add up to the correct amount, and this can help in keeping the standard errors down to a reasonable size.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're doing post-stratification (raking or otherwise) you still need to know the population values for your categorical variables - essential for calculating the right weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I'm right in my suspicion that you don't really know the population values of your variables 2 and 3 (which need to be measured by survey), your best bet will be just to stratify on the basis of previous examination results, and then calculate weights to population based just on that variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've found Thomas Lumley's &lt;code&gt;survey&lt;/code&gt; package for R relatively straightforward to use and it has the advantage of being free.  I would say this or something equivalent is essential for decent survey analysis.  It has a good &lt;a href=&quot;http://faculty.washington.edu/tlumley/survey/&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt; and an even better &lt;a href=&quot;http://faculty.washington.edu/tlumley/svybook/&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt; - you probably need to get hold of the book or an equivalent for all this to make sense&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-12T05:46:56.400" Id="22667" LastActivityDate="2012-02-12T05:46:56.400" OwnerUserId="7972" ParentId="22662" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I imagine this is history by now, but just in case...&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Yes, this seems appropriate.  Your research question must be &quot;are teacher attitudes/behaviours at a school related to student attitudes/behaviours at that school?&quot;  If this is your question, a school is the appropriate unit of analysis (and there would be no way to match up individual teachers to students anyway).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would just add caveats on the use of Pearson's correlation coefficient, unrelated to the question of the unit of analysis or sampling strategy.  The correlation coefficient cannot pick up non-linear relationships, can be misleading to interpret, is easily distorted by a few outliers, and classical inference based on it depends on Normality (which won't hold exactly with your proportion data, although it may be a reasonable approximation).  At a minimum I would carefully use graphical methods to check that this is a sensible approach and there is not a better way of inferring the relationship between the two variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) I don't think you &lt;em&gt;need&lt;/em&gt; to weight the data but I would certainly try it (and hope it doesn't change the results).  But I would weight by your &lt;em&gt;sample size&lt;/em&gt; in the school, not by the enrollment size.  The reason would be about estimation rather than either your unit of analysis or any need to &quot;weight to population&quot;.  You only have an estimate of the true teacher and student responses in each school, drawing on your finite sample.  Schools where you had a larger sample you are more confident in your estimate, and hence it would be good if they were taken more seriously in fitting your correlation or linear regression.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-12T06:43:19.070" Id="22670" LastActivityDate="2012-02-12T06:43:19.070" OwnerUserId="7972" ParentId="11883" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;In your Nov. 16 comment you say &quot;How would I know which classification is 'best' using cost of each group as the measure.&quot;  This seems to simplify matters dramatically and perhaps to make unnecessary some of the considerations described in your original question.  If the main thing you want to do is to compare the success of each clustering arrangement in explaining the variance in &lt;em&gt;cost&lt;/em&gt;, that would in most cases be pretty straightforward.  Since cost is a continuous variable and group is a nominal/categorical one, you could ordinarily run an ANOVA to see to what extent the clustering accounts for cost.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that you refer to 1500 groups.  If this is the end-result of clustering, then you've got your cases much too splintered to deal with in a statistical test (unless you are dealing with vast amounts of cases--say, from the entire U.S.).  You've got a statistical power problem.  So if I have understood you correctly the 1500 groups may themselves need to be reclustered into just a few before you can formally test their performance in accounting for the variance in cost.  Just how few would be ascertainable through a power analysis that drew on the typical mean differences among the final groups; the typical amount of variance within each group; the N-size within each group; and your threshold for statistical significance.  Assuming you got the reclustering done (no small feat), such a power analyis could be facilitated using the free program GPower. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-12T15:15:03.643" Id="22687" LastActivityDate="2012-02-12T15:15:03.643" OwnerUserId="2669" ParentId="18423" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Jbowman's answer is correct but to add to the &quot;real life&quot; dimension he or she adverts to:  You really should think about &quot;real life&quot; here because the basic answer to your question is: &quot;Impossible to say; it depends on what you are modeling.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer to the main question -- can there be a &quot;significant&quot; interaction between two &quot;nonsignificant&quot; predictors -- is &quot;of course.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine, e.g., a disease that is equally likely to be terminal for members of two subpopulations &amp;amp; that can be effectively treated with an intervention in only 1. Membership in the groups will not predict death from the disease; and the main effect of the treatment -- which will be a (sample-size weighted) average of the effect on the two groups might well be nonsignificant too if the sample size of the treatment-responsive population or the effect size of the intervention is small. But add a cross-product interaction term -- &amp;amp; voila, you see that the effect of treatment is &quot;significant&quot; for the treatment-responsive group.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you can see from this example that your questions about the relative &quot;likelihood&quot; &amp;amp; &quot;theoretical possibility&quot; etc. of signficant interactions conditional on the predictor &amp;amp; moderator being significant can't be answered in a meaningful way. Everything depends on how the predictor &amp;amp; moderator are related to the outcome being modeled. &lt;/p&gt;&#10;&#10;&lt;p&gt;For a phenomenon in which it is not meaningful or plausible to envision the two variables interacting, there's no point asking about how &quot;likely&quot; or &quot;theoretically possible,&quot; whether or not the predictor and moderator are significant or nonsignificant (likely the interaction will be nonsignificant in that case, but if it turns out otherwise, it's likely a coincidence or a reflection of &quot;significant&quot; but meaningless relations between variables when you have large sample, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If such a relationship is plausible, then by definition a &quot;significant&quot; interaction is &quot;theoretically possible&quot; &amp;amp; whether one would expect the predictor and moderator to be significant or nonsignificant on their own in that situation necessarily depends on &lt;em&gt;what&lt;/em&gt; you are modeling. (Because the universe of things you might investigate is infinite, there's no way to say what's more likely -- both variables, one, or neither &quot;significant&quot; )&lt;/p&gt;&#10;&#10;&lt;p&gt;Statistics won't help anyone who doesn't known &lt;em&gt;what&lt;/em&gt; &amp;amp; &lt;em&gt;why&lt;/em&gt; he or she is using them to understand a particular phenomenon.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-12T15:19:51.470" Id="22688" LastActivityDate="2012-02-12T15:19:51.470" OwnerUserId="11954" ParentId="22680" PostTypeId="2" Score="9" />
  
  <row AcceptedAnswerId="22713" AnswerCount="1" Body="&lt;p&gt;$X_1, X_2, \ldots, X_n$ is a random sample from $\mathrm{Bernoulli}(\theta)$, $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are independent $\mathcal N(0, \sigma^2)$, independent of $X_i$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Define $Y_i = \theta X_i + \epsilon_i$, for $i = 1, 2, \ldots,n$. Define estimating function &#10;$$
  
  <row Body="&lt;p&gt;I'm still not quite sure how part (a) is different from part (b) but, from your comment above it appears you are now only asking about part (b), so: &lt;/p&gt;&#10;&#10;&lt;p&gt;If $\psi[\hat{\theta};(X,Y)] = 0$, then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sum_{i=1}^{n} Y_i = \hat{\theta} \cdot \sum_{i=1}^{n} X_i $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore $\hat{\theta} = \overline{Y}/\overline{X}$, the ratio of the sample means, satisfies  $\psi[\hat{\theta};(X,Y)] = 0$. Regarding unbiasedness, it is easy to see that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E( \hat{\theta} ) = E\left( \frac{ \sum_{i=1}^{n} \theta X_i + \varepsilon_{i} }{ \sum_{i=1}^{n} X_i }\right) = \theta + E \left( \frac{ \sum_{i=1}^{n} \varepsilon_i }{
  <row AnswerCount="1" Body="&lt;p&gt;I am currently playing around with the MNIST dataset (&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot; rel=&quot;nofollow&quot;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt;) in R. The training set size is 60000x748 and it seems to drain all my memory even when constructing simple models like logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: how do you guys usually deal with big datasets in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;And tangent: is it feasible to break the dataset into smaller chunks, construct a model on each, then perform a weighted average on the predicted values?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-13T05:02:31.073" Id="22716" LastActivityDate="2012-02-13T10:14:13.173" LastEditDate="2012-02-13T10:14:13.173" LastEditorUserId="930" OwnerUserId="9164" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;dataset&gt;&lt;large-data&gt;" Title="How to deal with RAM limitations when working with big datasets in R?" ViewCount="446" />
&#10;D(p_1||p_2) =\int p_1 \log p_1 \text{d}\lambda - \int p_1 \log p_2 \text{d}\lambda
&#10;$$&#10;$$
&#10;$$&#10;$$
&#10;$$&#10;leads to a minimum in $\Sigma$ for $\Sigma=\Sigma_1$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-13T08:37:57.200" Id="22722" LastActivityDate="2012-02-13T08:37:57.200" OwnerUserId="7224" ParentId="22694" PostTypeId="2" Score="7" />
  
  
  
  <row Body="&lt;p&gt;If there are not too many non-zero values, why not simply present the cross-table? Or graphically, the two bar charts?&lt;/p&gt;&#10;&#10;&lt;p&gt;@Peter -- the description of the book Nonparametric Measures of Association on Amazon mentions that it is for ordinal data, and you too mention using ordinal methods for count data. But are they really sufficiently similar?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-13T16:43:21.527" Id="22739" LastActivityDate="2012-02-13T16:43:21.527" OwnerUserId="1945" ParentId="7512" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="22747" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;OP EDIT&lt;/strong&gt;: There where no problem with this. The problem was with the method I was using for obtaining the PACF. Apparently it doesn't work quite well in this case (I was using the scikits/tsa python package to obtain the PACF via the YW equations). Testing the coefficients in R worked like a charm.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to simulate an AR(2) process, but I seem to be getting awful results.&lt;/p&gt;&#10;&#10;&lt;p&gt;The way I'm doing it is as follows: if I want to simulate 1000 points of an AR(2) process with coefficients, say, a1=0.1 and a2=0.5, I simulate a realization of 2000 points of a white noise process (in my case I simulated 2000 points drawn from a normal, ~N(0,1) distribution), where I'll use the first 1000 points as burn-in points. Suppose I store this realization in a vector W[t]. Then, I simulate the actual AR(2) process by iterating on a new vector, X[t], as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X[0]=W[0]&#10;&#10;X[1]=W[1]&#10;&#10;X[2]=a1*X[1]+a2*X[0]+W[2]&#10;&#10;X[3]=a1*X[2]+a2*X[1]+W[3]&#10;&#10;...&#10;&#10;X[i]=a1*X[i-1]+a2*X[i-2]+W[i]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Finally, I return the last 1000 values of the vector X[i]. The problem is that when I plot the actual Partial Autocorrelation Function (i.e. when I estimate the coefficients of the AR(2) process I generated), I get wrong coefficients for a1 (I get an acceptable value for coefficient a2, though). What am I missing?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's my Python function for the simulation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from numpy.random import normal&#10;from pylab import *&#10;&#10;# INPUT: &#10;# a:        Is the array with coefficients, e.g. a=array([a1,a2]).&#10;# sigma:    The white noise (zero-mean normal in this case) standard deviation.&#10;# n:        Number of points to generate.&#10;&#10;def ARgenerator(a,sigma,n,burnin=0):&#10;&#10;  if(burnin==0):&#10;    burnin=100*len(a) # Burn-in elements!&#10;  w=normal(0,sigma,n+burnin)&#10;  AR=array([])&#10;  s=0.0&#10;  warning=0&#10;  for i in range(n+burnin):&#10;      if(i&amp;lt;len(a)):&#10;        AR=append(AR,w[i])&#10;      else:&#10;        s=0.0&#10;        for j in range(len(a)):&#10;            s=s+a[j]*AR[i-j-1]&#10;        AR=append(AR,s+w[i])&#10;  print 'Measured standard deviation: '+str(sqrt(var(w[burnin:])))&#10;  return AR[burnin:]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; by an MCMC simulation (where I use the autocovariance matrix for an AR(2) process with a multi-variate gaussian likelihood), I get right the value of sigma (the std. deviation of the white noise process) and the a2 coefficient. However, the a1 value I obtain has nothing to do with the true one.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-02-13T17:45:11.730" Id="22742" LastActivityDate="2012-02-15T07:58:42.550" LastEditDate="2012-02-15T07:58:42.550" LastEditorUserId="930" OwnerUserId="9174" PostTypeId="1" Score="2" Tags="&lt;simulation&gt;&lt;python&gt;&lt;autoregressive&gt;" Title="Problem simulating AR(2) process" ViewCount="1190" />
  
&#10;    \right| }} \quad\text{on}\quad\space [0,2]
  <row Body="&lt;p&gt;Surname frequencies, like the relative frequencies for many types of words, tend to follow a zeta distribution, possibly produced by an effect like &lt;a href=&quot;http://en.wikipedia.org/wiki/Zipf%27s_law&quot; rel=&quot;nofollow&quot;&gt;Zipf's law&lt;/a&gt; (See &lt;a href=&quot;http://www.jstor.org/stable/1402733&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for more details)&lt;/p&gt;&#10;&#10;&lt;p&gt;So the sort of distribution you've suggested seems reasonable. Probably the empirical distribution differs quite a bit though, and fluctuates from year to year. If you're a young person, who mostly interacts with people ages 5-10, for example, then this could quite strongly affect the probability of meeting 8 out of 10 people with a J.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, I'm not certain that the distribution of given names matches that for surnames.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-13T20:33:05.060" Id="22752" LastActivityDate="2012-02-13T20:33:05.060" OwnerUserId="6446" ParentId="22736" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to work out how to propagate standard deviations in a biological experiment, but I have some difficulties. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following (semi)fictitious data originating from an image analysis resulting in optical density (OD) measurements:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ds &amp;lt;- data.frame(&#10;    Sample = c(rep(seq(1,6),2)),&#10;    Control.OD = c(7000, 7100, 5600, 5200, 5900, 7000, &#10;                   8100, 7700, 6100, 5500, 6600, 7500), &#10;    Target.OD  = c(1000,  330,   35, 9300, 5570, 8700, &#10;                   1300,  400,   62, 9100, 5817, 9000),&#10;    Group = c(1,1,1,2,2,2,1,1,1,2,2,2)&#10;)&#10;&#10;print(ds)&#10;&#10;   Sample Control.OD Target.OD Group&#10;1       1       7000      1000     1&#10;2       2       7100       330     1&#10;3       3       5600        35     1&#10;4       4       5200      9300     2&#10;5       5       5900      5570     2&#10;6       6       7000      8700     2&#10;7       1       8100      1300     1&#10;8       2       7700       400     1&#10;9       3       6100        62     1&#10;10      4       5500      9100     2&#10;11      5       6600      5817     2&#10;12      6       7500      9000     2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The data represents duplicate measurements from one experiment with 6 biological samples from two different groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Aggregate technical duplicates&#10;ds.Agg &amp;lt;- aggregate(cbind(Control.OD, Target.OD) ~ Sample + Group, &#10;                    data = ds, mean)&#10;&#10;# Rename columns for easier reading&#10;names(ds.Agg)[names(ds.Agg)==&quot;Control.OD&quot;] &amp;lt;- &quot;Control.mean&quot;&#10;names(ds.Agg)[names(ds.Agg)==&quot;Target.OD&quot;] &amp;lt;- &quot;Target.mean&quot;&#10;&#10;# Calculate initial ratio between the target and the control&#10;# This is the value that represents each biological sample&#10;ds.Agg$Ratio &amp;lt;- ds.Agg$Target.mean / ds.Agg$Control.mean &#10;&#10;# Calculate the group means and normal sd (no propagation)&#10;ds.group &amp;lt;- aggregate(Ratio ~ Group, data = ds.Agg, mean)&#10;ds.group$sd &amp;lt;- aggregate(Ratio ~ Group, data = ds.Agg, sd)$Ratio&#10;ds.group$sd.rel &amp;lt;- ds.group$sd / ds.group$Ratio * 100&#10;&#10;# Print results for classical way of doing this (no propagation)&#10;format(ds.group, digits=3)&#10;&#10;  Group Ratio     sd sd.rel&#10;1     1  0.07 0.0742  106.0&#10;2     2  1.28 0.4080   31.8    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A final read-out would be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2) # version 0.9.0   &#10;&#10;png(type = &quot;windows&quot;, file = &quot;c:/temp/propagation1.png&quot;,width=140,height=200)&#10;ggplot(ds.group, mapping = aes(x = factor(Group), y = Ratio)) +&#10;  geom_bar(width = 0.8, position=&quot;dodge&quot;) +&#10;  geom_errorbar(mapping = aes(ymin = Ratio - sd, ymax = Ratio + sd), &#10;                width = 0.40, position=position_dodge(width = 0.8)) +&#10;  scale_x_discrete(&quot;Group&quot;) +&#10;  scale_y_continuous(limits = c(-0.1, 1.7)) +&#10;  theme_bw()&#10;message(dev.off())&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RLQzm.png&quot; alt=&quot;No propagation&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then an attempt on propagation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Calculate sd for technical duplicates which will be propagated&#10;sdTmp &amp;lt;- aggregate(cbind(Control.OD, Target.OD) ~ Sample + Group, data=ds, sd) &#10;ds.Agg$Control.sd &amp;lt;- sdTmp$Control.OD&#10;ds.Agg$Target.sd &amp;lt;- sdTmp$Target.OD&#10;&#10;# Calculate the relative propagated sd associated with the calculated Ratio&#10;fltRelativeSd &amp;lt;- with(ds.Agg, &#10;    sqrt( (Control.sd / Control.mean)^2 + (Target.sd / Target.mean)^2 ) &#10;)&#10;ds.Agg$Ratio.sd &amp;lt;- ds.Agg$Ratio * fltRelativeSd&#10;ds.Agg$Ratio.sd.rel &amp;lt;- ds.Agg$Ratio.sd / ds.Agg$Ratio * 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The follow code is my biggest concern when it comes to calculation of the propagated standard deviation:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Calculate propagated sd for the groups&#10;ds.group$sd.prop &amp;lt;- aggregate(Ratio.sd ~ Group, data = ds.Agg, &#10;    function(x) { &#10;        sqrt( sum((x)^2) ) / length(x);&#10;    })$Ratio&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;to here.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ds.group$sd.prop.rel &amp;lt;- ds.group$sd.prop / ds.group$Ratio * 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Re-print the results with the propagated standard deviations:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;format(ds.group, digits=3)&#10;&#10;  Group Ratio     sd sd.rel sd.prop sd.prop.rel&#10;1     1  0.07 0.0742  106.0  0.0111       15.79&#10;2     2  1.28 0.4080   31.8  0.0418        3.26&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And a graph:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;png(type = &quot;windows&quot;, file = &quot;c:/temp/propagation2.png&quot;,width=140,height=200)&#10;ggplot(ds.group, mapping = aes(x = factor(Group), y = Ratio)) +&#10;  geom_bar(width = 0.8, position=&quot;dodge&quot;) +&#10;  geom_errorbar(mapping = aes(ymin = Ratio - sd.prop, ymax = Ratio + sd.prop), &#10;                width = 0.40, position=position_dodge(width = 0.8)) +&#10;  scale_x_discrete(&quot;Group&quot;) +&#10;  scale_y_continuous(limits = c(-0.1, 1.7)) +&#10;  theme_bw()&#10;message(dev.off())&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So a read-out for the propagation:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1vFrk.png&quot; alt=&quot;With propagation&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would have expected standard deviations in the same neighborhood as seen on the first graph, however they seem a factor 10 off.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my thoughts/questions at this point:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Am I doing the calculations right?&lt;/li&gt;&#10;&lt;li&gt;Does it make sense to do this calculation (it seems that the biological variance is ignored and only the initial duplicate sd is actually propagated and presented)?  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any comments are most welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional info: The experiment uses a technique called &lt;a href=&quot;http://en.wikipedia.org/wiki/Western_blot&quot; rel=&quot;nofollow&quot;&gt;Western Blotting&lt;/a&gt; and the image analysis was done in ImageJ in a similar fashion as described &lt;a href=&quot;http://lukemiller.org/index.php/2010/11/analyzing-gels-and-western-blots-with-image-j/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-13T22:47:55.090" FavoriteCount="2" Id="22765" LastActivityDate="2012-02-14T13:57:20.547" LastEditDate="2012-02-14T13:57:20.547" LastEditorUserId="9182" OwnerDisplayName="Kim N" OwnerUserId="9182" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;standard-deviation&gt;&lt;biostatistics&gt;&lt;error-propagation&gt;" Title="Propagation of uncertainty/standard deviations in biological experiments" ViewCount="642" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In an experiment with continuous-value random variable, e.g. measuring length of cucumbers. I would like to compare the probability of getting a particular length range in two different conditions. e.g. $P1=P(0.1&amp;lt;L&amp;lt;0.2, fertilizer A)$, $P2=P(0.1&amp;lt;L&amp;lt;0.2, fertilizer B)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I create 2 histograms for the lengths corresponding to each of the 2 fertilizers, $H_A$ and $H_B$.&#10;From this histograms, probabilities over ranges of L are calculated from bin counts, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;My aim is to show that only for some ranges (or even 1 range) of L the probabilities (or counts of the above histogram) differ while in most other length ranges the probabilities do not differ. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) what is a good way to express this difference? The usual ratio $\frac{P1}{P2}$ suffers when $P1$ or $P2$ is zero. a good one is $P1-P2$ but then I would like this to be normalised over P so as to have something like a percentage change for comparison, I thought something along: $\frac{P1-P2}{P1+P2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;2) I would like to plot these probability differences for various L and if possible to use a statistical test which tests the significance of probability differences for just one range and not overall (what a t-test does). I guess the significance of difference depends also on the counts for that particular range of lengths.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-14T15:04:39.420" Id="22796" LastActivityDate="2012-08-14T06:54:42.530" LastEditDate="2012-08-14T06:54:42.530" LastEditorUserId="3826" OwnerUserId="9195" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;statistical-significance&gt;" Title="Compare the difference of two probabilities or a ratio of probabilities?" ViewCount="1826" />
  <row Body="&lt;p&gt;Absence of homoscedasticity may give unreliable standard error estimates of the parameters. Parameter estimates are unbiased.  But the estimates may not efficient(not BLUE).  You can find some more in the following &lt;a href=&quot;http://www.nd.edu/~rwilliam/stats2/l25.pdf&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-14T16:07:36.443" Id="22801" LastActivityDate="2012-02-14T16:07:36.443" OwnerUserId="7788" ParentId="22800" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;I would try modeling the &quot;expected&quot; behavior (like time-of-day fluctuations) with some model such as regression or exponential smoothing. Then, since you are interested in detection, I'd monitor the forecast errors using a statistical control chart. The control chart is based on putting threshold(s) on the forecast error, such that an extreme forecast will trigger an alert.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-14T18:23:42.727" Id="22817" LastActivityDate="2012-02-14T18:23:42.727" OwnerUserId="1945" ParentId="22807" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let´s say that my PCA analysis extracted 2 components, which explain 80% of the variance before rotation. The components were then rotated using oblique (Direct Oblimin) rotation, so SPSS cannot compute how much percentage of variance each component explain. When I plot the graph, usually editors require these percentages in brackets after the components. So I want to compute relative percentage of variance for each component. So if data looks like this&lt;/p&gt;&#10;&#10;&lt;p&gt;BEFORE ROTATION:&lt;/p&gt;&#10;&#10;&lt;p&gt;PC1 accounts for 60% of variance, eigenvalue 6.000;&#10;PC2 accounts for 20% of variance, eigenvalue 2.000;&#10;Total - 80% variance,  eigenvalue 8.000;&lt;/p&gt;&#10;&#10;&lt;p&gt;AFTER ROTATION:&lt;/p&gt;&#10;&#10;&lt;p&gt;PC1: eigenvalue 5.000;&#10;PC2: eigenvalue 4.000;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;can I compute these percentages this way?&#10;I expect, that after rotation total variance explained by both components doesn´t change, so it should be 80%. Am I right?&lt;/p&gt;&#10;&#10;&lt;p&gt;In unrotated solution, you can compute variance explained by component 2 this way: eigenvalue of component 2/total eigenvalue * total percentage explained = 2/8*80%=20%&lt;/p&gt;&#10;&#10;&lt;p&gt;We can also compute, that 1.000 eigenvalue = 10% of variance&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I use this equation (1.000 eigenvalue=10%) to compute variance explained by each component after rotation, so variance explained by PC1 after rotation should be 50% (eigenvalue=5 so 50%) and variance explained by PC2 after rotation should be 40%(eigenvalue=4 so 40%)? Of course in this case we cannot compute total variance by PC1+PC2 because components are correlated and total variance should be still 80% (maybe).&lt;/p&gt;&#10;&#10;&lt;p&gt;I don´t think that it is so easy, because SPSS would give me these numbers. So can I somehow compute (from data above line) how much % of variance is explained by PC1 after oblique rotation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2012-02-14T21:16:52.883" Id="22843" LastActivityDate="2014-05-06T04:03:23.743" LastEditDate="2012-02-15T16:23:18.730" LastEditorUserId="8705" OwnerUserId="8705" PostTypeId="1" Score="3" Tags="&lt;spss&gt;&lt;pca&gt;&lt;factor-analysis&gt;&lt;rotation&gt;" Title="Can I somehow compute variance explained by PC after Oblique rotation in PCA?" ViewCount="1824" />
  <row Body="&lt;p&gt;Make it optional to answer the gender question. That way, it's more likely to be accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;You will never know for sure the exact number of males and females because there will always be some sort of sampling error in your data -- you can't avoid that unless you interviewed every single customer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Selecting how many samples you need for the correct confidence interval requires a calculator (like this one &lt;a href=&quot;http://www.macorr.com/sample-size-calculator.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.macorr.com/sample-size-calculator.htm&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-14T13:32:57.720" Id="22847" LastActivityDate="2012-02-14T13:32:57.720" OwnerDisplayName="Jarie Bolander" ParentId="22846" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we are using logistic regression on a binary outcome variable $Y$ with two continuous predictors $X_1$ and $X_2$. Suppose the observation $Y = 1, X_1 = 10, X_2 = 15$ occurs 1000 times. In SAS, how would you account for this frequency in the regression without having to manually type the observation 1000 times?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit.&lt;/strong&gt; I figured it out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-15T03:48:14.797" Id="22861" LastActivityDate="2012-02-15T05:38:28.203" LastEditDate="2012-02-15T04:05:59.683" LastEditorUserId="9194" OwnerUserId="9194" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;sas&gt;" Title="Expressing observations in regression" ViewCount="52" />
  
&#10;AX+BY=\left(\begin{matrix}A&amp;amp; B \end{matrix}\right)
&#10;$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-15T11:43:48.293" Id="22883" LastActivityDate="2012-02-15T20:19:59.253" LastEditDate="2012-02-15T20:19:59.253" LastEditorUserId="7224" OwnerUserId="7224" ParentId="22879" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You should definitely learn some LaTeX before starting on beamer. &lt;/p&gt;&#10;&#10;&lt;p&gt;How much LaTeX you want to learn before adding Sweave (or while learning Sweave) depends on what you will do with LaTeX &lt;em&gt;other&lt;/em&gt; than write things from R code. LaTeX is huge. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-15T13:28:01.077" Id="22887" LastActivityDate="2012-02-15T13:28:01.077" OwnerUserId="686" ParentId="22880" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;While &quot;The Elements of Statistical Learning&quot; is a brilliant book, it requires a relatively high level of knowledge to get the most from it. There are many other resources on the web to help you to understand the topics in the book. &lt;/p&gt;&#10;&#10;&lt;p&gt;Lets take a very simple example of linear discriminant analysis where you want to group a set of two dimensional data points into K = 2 groups. The drop in dimensions will be only be K-1 = 2-1 = 1. As @deinst explained, the drop in dimensions can be explained with elementary geometry. &lt;/p&gt;&#10;&#10;&lt;p&gt;Two points in any dimension can be joined by a line, and a line is one dimensional. This is an example of a K-1 = 2-1 = 1 dimensional subspace.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in this simple example, the set of data points will be scattered in two-dimensional space. The points will be represented by (x,y), so for example you could have data points such as (1,2), (2,1), (9,10), (13,13). Now, using linear discriminant analysis to create two groups A and B will result in the data points being classified as belonging to group A or to group B such that certain properties are satisfied. Linear discriminant analysis attempts to maximize the variance between the groups compared to the variance within the groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, groups A and B will be far apart and contain data points that are close together. In this simple example, it is clear that the points will be grouped as follows. Group A = {(1,2), (2,1)} and Group B = {(9,10), (13,13)}.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, the centroids are calculated as the centroids of the groups of data points so&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Centroid of group A = ((1+2)/2, (2+1)/2) = (1.5,1.5), (11,11.5)&#10;&#10;Centroid of group B = ((9+13)/2, (10+13)/2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The Centroids are simply 2 points and they span a 1-dimensional line which joins them together.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7oWlH.png&quot; alt=&quot;Figure 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can think of linear discriminant analysis as a projection of the data points on a line so that the two groups of data points are as &quot;separated as possible&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you had three groups (and say three dimensional data points) then you would get three centroids, simply three points, and three points in 3D space define a two dimensional plane. Again the rule K-1 = 3-1 = 2 dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suggest you search the web for resources that will help explain and expand on the simple introduction I have given; for example &lt;a href=&quot;http://www.music.mcgill.ca/~ich/classes/mumt611_07/classifiers/lda_theory.pdf&quot;&gt;http://www.music.mcgill.ca/~ich/classes/mumt611_07/classifiers/lda_theory.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-15T14:15:52.067" Id="22891" LastActivityDate="2012-02-15T15:00:10.477" LastEditDate="2012-02-15T15:00:10.477" LastEditorUserId="919" OwnerUserId="9233" ParentId="22884" PostTypeId="2" Score="9" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Is it possible to define any useful entropy or &lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_entropy&quot; rel=&quot;nofollow&quot;&gt;conditional entropy&lt;/a&gt; which is based on the distance between datapoint(s) and cluster center(s), instead of basing on the number of points assigned to cluster like it is defined in for example to compute the &lt;a href=&quot;http://acl.ldc.upenn.edu/D/D07/D07-1043.pdf&quot; rel=&quot;nofollow&quot;&gt;v-measure&lt;/a&gt; ? I am not implying that I want an equivalent to v-measure, but I just wonder if it is possible and maybe useful to define a conditional entropy based on distances.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, What I'm looking for is a kind of conditional entropy based on distances, which allow to have an idea about the &quot;homogeneity&quot; and &quot;completeness&quot; of clusters, with respect to distances.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-15T21:53:43.490" Id="22920" LastActivityDate="2012-02-20T16:46:24.530" LastEditDate="2012-02-16T10:24:08.197" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;distance-functions&gt;&lt;entropy&gt;&lt;maximum-entropy&gt;" Title="Entropy based on euclidian distances between datapoints / clusters centers?" ViewCount="749" />
  <row AnswerCount="1" Body="&lt;p&gt;I am a beginner in statistics, and am self-studying from &quot;Information Theory, Inference, and Learning Algorithms&quot; by David MacKay. I've hit a wall with one of the questions, and was wondering if any of you could be so kind as to point me in the right direction. Admittedly, my title may be wrong or totally misleading as well - if you found it was, please let me know what would have been a better title for this question!&lt;/p&gt;&#10;&#10;&lt;p&gt;The example which the questions are based on is as follows:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Bill tosses a bent coin N times, obtaining a sequence of heads and tails. We assume that hte coin has a probability fH of coming up heads; we do not know fH. If nH heads have occurred in N tosses, what is the probability distribution of fH? What is the probability that the N+1th outcome will be a head, given nH heads in N tosses.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The question is (this is Exercise 2.8 on p. 30):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Assuming a uniform prior on fH, P(fH) = 1, solve the problem in the example (given above). Sketch the posterior distribution of fH and compute the probability that the N+1th outcome will be a head for&lt;/p&gt;&#10;  &#10;  &lt;ol&gt;&#10;  &lt;li&gt;N = 3 and nH = 0; &lt;/li&gt;&#10;  &lt;li&gt;N = 3 and nH = 2;&lt;/li&gt;&#10;  &lt;li&gt;N = 10 and nH = 3;&lt;/li&gt;&#10;  &lt;li&gt;N = 300 and nH = 29;&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;He also states:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;You will find the beta integral useful:&#10;  (Since I'm new I guess I can only give a link to the image - sorry about that):&#10;  &lt;a href=&quot;http://i.stack.imgur.com/7MxQt.png&quot; rel=&quot;nofollow&quot;&gt;http://i.stack.imgur.com/7MxQt.png&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now I'm guessing that my difficulty has a lot to do with the fact that I'm new to statistics, and still learning the parlance so to speak. So let me break my question into a few pieces:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;When he says &quot;Sketch the posterior distribution of fH,&quot; what exactly does he mean by that? Does he mean it literally? (Yes...this is how ignorant I am...)&lt;/li&gt;&#10;&lt;li&gt;Since he doesn't give us fH, is he expecting us to come up with it on our own? Would it be something like (fH + 1)/(N + 2)?&lt;/li&gt;&#10;&lt;li&gt;First of all, how do I read the beta integral (what exactly is meant by Fa, Fb, pa) and secondly, how is it useful for this problem?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Like I said, I'm a beginner, and trying to grok as much of what he said, but I think at this point I need some help. Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-15T22:54:44.493" Id="22924" LastActivityDate="2012-02-16T02:23:08.517" LastEditDate="2012-02-15T23:45:17.277" LastEditorUserId="88" OwnerUserId="9244" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;bayesian&gt;" Title="Posterior distribution and computation of probability of a future event" ViewCount="137" />
  
  <row Body="&lt;p&gt;The responses by @Henry and @Zach both work, but I think the most straight-forward way to do what you want is to use &lt;code&gt;lmList&lt;/code&gt; in the &lt;code&gt;nlme&lt;/code&gt; package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat &amp;lt;- data.frame(&#10;  GRP = sample(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 100, replace=TRUE), &#10;  X = runif(100), &#10;  Y = runif(100)&#10;)&#10;require(nlme)&#10;lmList(Y ~ X | GRP, data=dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2012-02-16T03:22:16.617" Id="22936" LastActivityDate="2012-02-16T16:34:54.057" LastEditDate="2012-02-16T16:34:54.057" LastEditorUserId="9249" OwnerUserId="9249" ParentId="22925" PostTypeId="2" Score="9" />
  
&#10;\end{cases}
  
  <row Body="&lt;p&gt;Here is a simple solution that should give you a taste of how to solve the problem. Whether this solution is satisfactory would depend on your actual application. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to determine how usual or unusual your distance matrix is compared to other arrangements of the points then you can use the following approach. You will need a measure of how well any set of red and green points are clustered. A very simple metric would be as follows. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Initialise the metric to zero.&#10;Foreach data point&#10;    If the nearest neighbour of a point is in a different group (ie different colour) then add this distance to the metric.  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, you can label the data points randomly with red or green and work out the metric for a random labelling.&#10;Repeat this random labelling many times and record the metric each time, allowing you to determine distribution statistics for the metric.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also calculate the metric for your actual set of data points and compare this to the distribution of metric data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some things to note. If you have two tightly clustered groups that are well separated then the metric would be zero....ie all neighbours belong to the same group.&lt;/p&gt;&#10;&#10;&lt;p&gt;The metric can be as simple as I described or more complex, depending on the application of the results. This sort of random reladelling is common. If you have a relatively small number of points you could work out the metric for all possible permutations.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can find more on this topic here &lt;a href=&quot;http://en.wikipedia.org/wiki/Resampling_(statistics&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Resampling_(statistics&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-16T13:18:44.083" Id="22963" LastActivityDate="2012-02-16T13:18:44.083" OwnerUserId="9233" ParentId="22835" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Perhaps 'calibration models' would give you some insights.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-16T13:45:13.150" Id="22966" LastActivityDate="2012-02-16T13:45:13.150" OwnerUserId="9080" ParentId="22912" PostTypeId="2" Score="1" />
  
  
&#10;e(N,0) &amp;amp;= \frac{1-p^N}{1-p} + (1-p^N)e(N,0); \\
  
  <row Body="&lt;p&gt;There are two ways of seeing how the density arises.  First, from the definition of the density as the derivative of the distribution, we have that&#10;$$
&#10;$$&#10;(a simple application of the chain rule.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Second (and less rigorously) is to note that $f_v(v)$ is the limit of probability that the minimum occurs between $v$ and $v+\Delta v$ divided by $\Delta v$ as $\Delta v$ goes to $0$. Now there are $n$ choices for which statistic is the minimum, that statistic has probability $f(v)\Delta v$ of occurring between $v$ and $v+\Delta v$, and the other $n-1$ statistics have probability $[1-F(v)]^{n-1}$ of all being greater than $n$.  Multiplying all these terms and dividing by $\Delta(v)$ gives us&#10;$$
  
  <row Body="&lt;p&gt;@ChrisTaylor has provided the basic answer to your question.  I just want to add another detail.  You ask, &quot;how can I calculate the... linear regression line so that I can plot... [the] &quot;prediction&quot; line if we were to extend the graph?&quot;  Doing this is called &lt;em&gt;extrapolation&lt;/em&gt;, and it is generally considered to be a very dangerous practice.  See &lt;a href=&quot;http://en.wikipedia.org/wiki/Regression_analysis#Interpolation_and_extrapolation&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for a simple overview of the topic.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-17T04:27:41.983" Id="23021" LastActivityDate="2012-02-17T04:27:41.983" OwnerUserId="7290" ParentId="22999" PostTypeId="2" Score="0" />
&#10;$$&#10;then a scaled error is $q_t = (y_t-\hat{y}_t)/Q$, where $\hat{y}_t$ is a forecast of $y_t$ using whatever forecasting method you are implementing for that item. Take the mean absolute value of the scaled errors to get the MASE. For example, you might use a rolling origin (aka &lt;a href=&quot;http://robjhyndman.com/researchtips/crossvalidation/&quot;&gt;time series cross-validation&lt;/a&gt;) and take the mean absolute value of the resulting one-step (or $h$-step) errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Series that are easy to forecast should have low values of MASE. Here &quot;easy to forecast&quot; is interpreted relative to the seasonal naive forecast. In some circumstances, it may make more sense to use an alternative base measure to scale the results.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-02-17T11:45:26.680" Id="23033" LastActivityDate="2012-02-17T11:45:26.680" OwnerUserId="159" ParentId="23007" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;[UPDATE: I improved on the code below and made a small R package hosted on GitHub: &lt;a href=&quot;https://github.com/djhocking/qicpack&quot; rel=&quot;nofollow&quot;&gt;https://github.com/djhocking/qicpack]&#10;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I figured out a solution for calculating QIC from geepack package output. My code is below. This is one of the first functions I've ever written, so I apologize if it's messy but hopefully others find it useful. I definitely recommend reading gung's thoughts on model selection (linked above) before using this or any other information criterion model selection techniques (e.g. AIC, BIC, DIC). Also much of this code was pieced together from other sources, which I tried to reference at the start. I also received valuable input from Jun Yan, the geepack author.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;######################################################################################&#10;# QIC for GEE models&#10;# Daniel J. Hocking&#10;# 07 February 2012&#10;# Refs:&#10;  # Pan (2001)&#10;  # Liang and Zeger (1986)&#10;  # Zeger and Liang (1986)&#10;  # Hardin and Hilbe (2003)&#10;  # Dornmann et al 2007&#10;  # # http://www.unc.edu/courses/2010spring/ecol/562/001/docs/lectures/lecture14.htm&#10;######################################################################################&#10;&#10;# Poisson QIC for geeglm{geepack} output&#10;# Ref: Pan (2001)&#10;QIC.pois.geese &amp;lt;- function(model.R, model.indep) {&#10;  library(MASS)&#10;  # Fitted and observed values for quasi likelihood&#10;  mu.R &amp;lt;- model.R$fitted.values&#10;      # alt: X &amp;lt;- model.matrix(model.R)&#10;          #  names(model.R$coefficients) &amp;lt;- NULL&#10;      #  beta.R &amp;lt;- model.R$coefficients&#10;          #  mu.R &amp;lt;- exp(X %*% beta.R)&#10;      y &amp;lt;- model.R$y&#10;&#10;  # Quasi Likelihood for Poisson&#10;  quasi.R &amp;lt;- sum((y*log(mu.R)) - mu.R) # poisson()$dev.resids - scale and weights = 1&#10;&#10;  # Trace Term (penalty for model complexity)&#10;  AIinverse &amp;lt;- ginv(model.indep$geese$vbeta.naiv) # Omega-hat(I) via Moore-Penrose generalized inverse of a matrix in MASS package&#10;  # Alt: AIinverse &amp;lt;- solve(model.indep$geese$vbeta.naiv) # solve via indenity&#10;  Vr &amp;lt;- model.R$geese$vbeta&#10;  trace.R &amp;lt;- sum(diag(AIinverse %*% Vr))&#10;  px &amp;lt;- length(mu.R) # number non-redunant columns in design matrix&#10;&#10;  # QIC&#10;  QIC &amp;lt;- (-2)*quasi.R + 2*trace.R&#10;  QICu &amp;lt;- (-2)*quasi.R + 2*px    # Approximation assuming model structured correctly&#10;  output &amp;lt;- c(QIC, QICu, quasi.R, trace.R, px)&#10;  names(output) &amp;lt;- c('QIC', 'QICu', 'Quasi Lik', 'Trace', 'px')&#10;  output&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-02-17T17:03:27.820" Id="23050" LastActivityDate="2014-01-14T03:08:33.100" LastEditDate="2014-01-14T03:08:33.100" LastEditorUserId="8289" OwnerUserId="8289" ParentId="21771" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;To quote from the intro in the Efron/Tibshirani text on the subject:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The message of this book can therefore be summarized by paraphrasing&#10;  Tukey: &quot;The bootstrap, like a shotgun, can blow the head off any&#10;  problem if the statistician can stand the resulting mess''.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It sounds like you're trying to make inference based on the assumption that the asymptotic bootstrap distribution of the estimated breakpoint is unimodal. What if the &quot;true trend&quot;, in fact, had two break points?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-17T20:34:48.577" Id="23058" LastActivityDate="2012-02-17T20:34:48.577" OwnerUserId="8013" ParentId="23057" PostTypeId="2" Score="2" />
  
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Showing that we do not have unique estimates unless $4AC\neq B^2$.  Now we have:&#10;$$\begin{array}{c c}
  
  
  <row AcceptedAnswerId="23082" AnswerCount="1" Body="&lt;p&gt;I have 168 rows of patient data: 104 controls and 64 cases. I want to know if albumin status (low or high) is related to case/control status. I made a table using R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; table(Albumin, Status, useNA = &quot;ifany&quot;)&#10;Albumin    Control  Case&#10;    Low    51       16&#10;    High   39       32&#10;    &amp;lt;NA&amp;gt;   14       16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, I have missing data. I did a chi-squared test on the entire table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; chisq.test(table(Albumin, Status, useNA = &quot;ifany&quot;))$p.value&#10;[1] 0.006222513&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Should I perform the test on the 3x2 table above that includes the missing data? Or should I perform it on a 2x2 table that excludes the missing data, as shown below?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; chisq.test(table(Albumin, Status))$p.value&#10;[1] 0.01496166&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: In this example, both approaches yield significant p-values. However, I have other variables for which the difference is insignificant when missing values are excluded, but significant when they are included. I have some variables with only one missing value, as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: How should I apply the chi-squared test in those situations? Is my choice of test correct, or should I be using Fisher's exact test or some other test? And are there any diagnostics that I need to do before even applying these tests?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-18T14:51:15.057" FavoriteCount="1" Id="23079" LastActivityDate="2013-07-21T18:39:37.997" LastEditDate="2013-07-21T18:39:37.997" LastEditorUserId="7290" OwnerUserId="8724" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;missing-data&gt;&lt;epidemiology&gt;&lt;fishersexact&gt;" Title="How to handle missing data when determining differences between groups using chi-squared or Fisher's exact test" ViewCount="1812" />
  
  
  
  <row Body="&lt;p&gt;Another option seems to be &lt;a href=&quot;http://en.wikipedia.org/wiki/Dataverse&quot; rel=&quot;nofollow&quot;&gt;Dataverse&lt;/a&gt;, which is available as a service and as open source software. I did not try it, though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-19T18:28:17.717" Id="23113" LastActivityDate="2012-02-19T18:28:17.717" OwnerUserId="573" ParentId="17850" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;A trivial way to do this is just to rescale the $\lambda_i$ by the input $\lambda$ value, and return the appropriate rescaled $\lambda_i$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;new_lambda &amp;lt;- function(beginning_lambda, current_minute)&#10;{&#10;  fval &amp;lt;- c(2.1, 2.08097540957931, 2.06192246297887, 2.04284081233169, &#10;     2.02373010167247, 2.00458996665794, 1.98542003427428, 1.96621992253097,&#10;     ..., &#10;     0.0759036727240167, 0.041819322644209)&#10;  fval[current_minute] * beginning_lambda / fval[1]&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since your second list appears (from a test of the first, second, and last values) to be exactly 1.5 times your first list, you don't need both lists.  This is all you really need, at least given your stated objective.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-19T19:14:47.530" Id="23115" LastActivityDate="2012-02-19T19:14:47.530" OwnerUserId="7555" ParentId="21192" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Here's a second idea based on stl. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could fit an stl decomposition to each series, and then compare the standard error of the remainder component to the mean of the original data ignoring any partial years. Series that are easy to forecast should have a small ratio of se(remainder) to mean(data).&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I suggest ignoring partial years is that seasonality will affect the mean of the data otherwise. In the example in the question, all series have seven complete years, so it is not an issue. But if the series extended part way into 2012, I suggest the mean is computed only up to the end of 2011 to avoid seasonal contamination of the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;This idea assumes that mean(data) makes sense -- that is that the data are mean stationary (apart from seasonality). It probably wouldn't work well for data with strong trends or unit roots.&lt;/p&gt;&#10;&#10;&lt;p&gt;It also assumes that a good stl fit translates into good forecasts, but I can't think of an example where that wouldn't be true so it is probably an ok assumption.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-19T22:27:56.523" Id="23125" LastActivityDate="2012-02-19T22:27:56.523" OwnerUserId="159" ParentId="23007" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;If the $X_i$ are iid each with positive finite variance $v$ then $$\text{var}\left(\sum_i a_i X_i\right) = \sum_i \text{var}\left( a_i X_i\right) = \sum_i a_i^2 \text{var}\left( X_i\right) = \sum_i a_i^2 v = v \sum_i a_i^2$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;so you want to minimise $v \sum_i a_i^2$ subject to $\sum_i a_i =1$ (since it has to be unbiased).  You can ignore the positive constant $v$ and deduce this happens when each $a_i=1/n$; for example the Cauchy–Schwarz inequality will do this.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-02-19T23:28:17.760" Id="23127" LastActivityDate="2012-02-19T23:46:09.233" LastEditDate="2012-02-19T23:46:09.233" LastEditorUserId="2958" OwnerUserId="2958" ParentId="23120" PostTypeId="2" Score="2" />
&#10;\frac{1}{2} \frac{1}{\pi} \frac{1}{1+x^2} + \frac{1}{2}\frac{1}{4}\frac{\mathbb{I}_{[0,2]}(x)}{\sqrt{|1-x|}}
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Regarding &quot;Set Role&quot;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;prediction&quot; is referring to the predicted label AFTER the application of a model to an exampleset. By default, all attributes with role &quot;regular&quot; are used as predictors. So the operator &lt;code&gt;Set Role&lt;/code&gt; can be skipped here.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Regarding application of a model&lt;/strong&gt;&#10;You have to load the test-set separately and apply the model to it. Something like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&amp;gt;&#10;&amp;lt;process version=&quot;5.2.000&quot;&amp;gt;&#10;  &amp;lt;context&amp;gt;&#10;    &amp;lt;input/&amp;gt;&#10;    &amp;lt;output/&amp;gt;&#10;    &amp;lt;macros/&amp;gt;&#10;  &amp;lt;/context&amp;gt;&#10;  &amp;lt;operator activated=&quot;true&quot; class=&quot;process&quot; compatibility=&quot;5.2.000&quot; expanded=&quot;true&quot; name=&quot;Process&quot;&amp;gt;&#10;    &amp;lt;process expanded=&quot;true&quot; height=&quot;443&quot; width=&quot;636&quot;&amp;gt;&#10;      &amp;lt;operator activated=&quot;true&quot; class=&quot;retrieve&quot; compatibility=&quot;5.2.000&quot; expanded=&quot;true&quot; height=&quot;60&quot; name=&quot;load_train&quot; width=&quot;90&quot; x=&quot;45&quot; y=&quot;30&quot;&amp;gt;&#10;        &amp;lt;parameter key=&quot;repository_entry&quot; value=&quot;//Samples/data/Golf&quot;/&amp;gt;&#10;      &amp;lt;/operator&amp;gt;&#10;      &amp;lt;operator activated=&quot;true&quot; class=&quot;naive_bayes&quot; compatibility=&quot;5.2.000&quot; expanded=&quot;true&quot; height=&quot;76&quot; name=&quot;Naive Bayes&quot; width=&quot;90&quot; x=&quot;179&quot; y=&quot;30&quot;/&amp;gt;&#10;      &amp;lt;operator activated=&quot;true&quot; class=&quot;retrieve&quot; compatibility=&quot;5.2.000&quot; expanded=&quot;true&quot; height=&quot;60&quot; name=&quot;load_test&quot; width=&quot;90&quot; x=&quot;45&quot; y=&quot;210&quot;&amp;gt;&#10;        &amp;lt;parameter key=&quot;repository_entry&quot; value=&quot;//Samples/data/Golf-Testset&quot;/&amp;gt;&#10;      &amp;lt;/operator&amp;gt;&#10;      &amp;lt;operator activated=&quot;true&quot; class=&quot;apply_model&quot; compatibility=&quot;5.2.000&quot; expanded=&quot;true&quot; height=&quot;76&quot; name=&quot;Apply Model&quot; width=&quot;90&quot; x=&quot;313&quot; y=&quot;120&quot;&amp;gt;&#10;        &amp;lt;list key=&quot;application_parameters&quot;/&amp;gt;&#10;      &amp;lt;/operator&amp;gt;&#10;      &amp;lt;operator activated=&quot;true&quot; class=&quot;performance_binominal_classification&quot; compatibility=&quot;5.2.000&quot; expanded=&quot;true&quot; height=&quot;76&quot; name=&quot;Performance&quot; width=&quot;90&quot; x=&quot;447&quot; y=&quot;120&quot;/&amp;gt;&#10;      &amp;lt;connect from_op=&quot;load_train&quot; from_port=&quot;output&quot; to_op=&quot;Naive Bayes&quot; to_port=&quot;training set&quot;/&amp;gt;&#10;      &amp;lt;connect from_op=&quot;Naive Bayes&quot; from_port=&quot;model&quot; to_op=&quot;Apply Model&quot; to_port=&quot;model&quot;/&amp;gt;&#10;      &amp;lt;connect from_op=&quot;load_test&quot; from_port=&quot;output&quot; to_op=&quot;Apply Model&quot; to_port=&quot;unlabelled data&quot;/&amp;gt;&#10;      &amp;lt;connect from_op=&quot;Apply Model&quot; from_port=&quot;labelled data&quot; to_op=&quot;Performance&quot; to_port=&quot;labelled data&quot;/&amp;gt;&#10;      &amp;lt;connect from_op=&quot;Performance&quot; from_port=&quot;performance&quot; to_port=&quot;result 1&quot;/&amp;gt;&#10;      &amp;lt;connect from_op=&quot;Performance&quot; from_port=&quot;example set&quot; to_port=&quot;result 2&quot;/&amp;gt;&#10;      &amp;lt;portSpacing port=&quot;source_input 1&quot; spacing=&quot;0&quot;/&amp;gt;&#10;      &amp;lt;portSpacing port=&quot;sink_result 1&quot; spacing=&quot;0&quot;/&amp;gt;&#10;      &amp;lt;portSpacing port=&quot;sink_result 2&quot; spacing=&quot;0&quot;/&amp;gt;&#10;      &amp;lt;portSpacing port=&quot;sink_result 3&quot; spacing=&quot;0&quot;/&amp;gt;&#10;    &amp;lt;/process&amp;gt;&#10;  &amp;lt;/operator&amp;gt;&#10;&amp;lt;/process&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Simply copy and paste the code to the XML tab in the design-perspective to make it work.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in order to make a solid statement about the accuracy of your classifier you should perform a xvalidation. See e.g. the process under /Samples/03_Validation/XValidation_Nominal&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, in order to reduce the confusion about the two types of model-applications, i.e. XValidation on the first dataset and application of the final model on the second /holdout-set, I recommend this question:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/9357/why-only-three-partitions-training-validation-test&quot;&gt;Why only three partitions? (training, validation, test)&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-20T12:02:46.727" Id="23148" LastActivityDate="2012-02-20T12:02:46.727" OwnerUserId="264" ParentId="23099" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="23156" AnswerCount="1" Body="&lt;p&gt;Question:&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a bit of an abstract question, but bear with me. I am averaging images, to try and deduce what the average image of a specific subject looks like (just out of curiosity, it might produce some interesting results).&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to know what kind of averaging method is permitted, i am trying to determine as what kind of data images can be classified (nominal, ordinal, interval or ratio). I would think a collection of images from a specific subject (creative commons portraits scraped from flickr for example) can be classified as nominal data, since there is no sense in ordering portraits (which would make it ordinal data, or higher).&lt;/p&gt;&#10;&#10;&lt;p&gt;In my opinion the images are on the same level as names, but instead of a linguistic representation of something, they are a visual representation of something (thereby functioning on the same level as a name). Which would make it nominal data.&lt;/p&gt;&#10;&#10;&lt;p&gt;But then again: digital images are in essence an array of numbers (rgb values), which could mean that it's measured at the ratio level (or at least the colors are). &lt;strong&gt;So a mean average is &lt;em&gt;possible&lt;/em&gt;, but is it sensible?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My other question should clarify what it is i am trying to achieve: &lt;a href=&quot;http://stats.stackexchange.com/questions/23193/averaging-images&quot;&gt;Averaging images&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-20T13:04:13.653" Id="23154" LastActivityDate="2012-02-20T21:44:37.023" LastEditDate="2012-02-20T21:44:37.023" LastEditorDisplayName="user9318" OwnerDisplayName="user9318" PostTypeId="1" Score="2" Tags="&lt;image-processing&gt;&lt;functional-data-analysis&gt;&lt;nominal&gt;" Title="What is the level of measurement of image data?" ViewCount="309" />
  
  <row Body="&lt;p&gt;I'm going to have to agree with pmgjones. If you need to ask this question, and do what the reviewer asks, its time to seek help beyond the power of most online calculators. Especially given the proper control of BMI should &lt;em&gt;probably&lt;/em&gt; not be done with a purely linear term (or heaven forbid a dichotomous obese/not obese variable).&lt;/p&gt;&#10;&#10;&lt;p&gt;Go seek the advice of colleagues and collaborators with a stronger background in statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-20T16:09:39.443" Id="23166" LastActivityDate="2012-02-20T16:09:39.443" OwnerUserId="5836" ParentId="23134" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Here is the head of my data set (&lt;code&gt;tjornres&lt;/code&gt;): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       Fish.1 Fish.2  MORPHO      DIET &#10;1         1      2        0.03768       0.1559250 &#10;2         1      3        0.05609       0.7897060 &#10;3         1      4        0.03934       0.4638010 &#10;4         1      5        0.03363       0.1200480 &#10;5         1      6        0.05629       0.4390760 &#10;6         1      8        0.08366       0.1866750 &#10;7         1      9        0.04892       0.0988235 &#10;8         1     10       0.04427       0.2637140 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;MORPHO&lt;/code&gt; and &lt;code&gt;DIET&lt;/code&gt; refer to the morphological and diet distances between fish 1 and fish 2. My original data set has over 2400 pairs of fish. My goal  is to resample this dataset by selecting only 435. &#10;I would like to do this 999 times and get a distribution of the correlation coefficients &lt;code&gt;MORPHO~DIET&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I went on and wrote this code: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;head(tjornres) &#10;&#10;essayres = tjornres                  # copy of the data             &#10;R = 999                                         # the number of replicates             &#10;cor.values = numeric(R)         # store the data             &#10;for (i in 1:R) {                              # loop &#10;+ group1 = sample(essayres, size=435, replace=F) &#10;+ group2 = sample(essayres, size=435, replace=F) &#10;+ cor.values[i] = cor.test(group1,group2)$cor &#10;+ } &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have a syntax error in this code. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also if I run one resampling, &lt;code&gt;sample(essayres, size=435, replace=F)&lt;/code&gt;, I get this error &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;message: Error in `[.data.frame`(x, .Internal(sample(length(x), size, replace,  &#10;:cannot take a sample larger than the population when 'replace = FALSE'.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does anyone know why this code is not working? Are there any other ways to resample (without replacement) ? &#10;Thank you for your help, &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-20T16:49:03.597" Id="23173" LastActivityDate="2012-02-20T17:57:27.053" LastEditDate="2012-02-20T16:58:03.330" LastEditorUserId="1036" OwnerUserId="9278" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;correlation&gt;&lt;resampling&gt;" Title="Resampling without replacement in R, with a loop" ViewCount="971" />
  <row Body="&lt;p&gt;In the past I have used Median Absolute Deviation to test for outliers in a static series of data. This is a simple measure to implement and is related to standard deviation for normally distributed data points. It is very robust against outliers. I have used it for very large static data sets rather than with a time series but it should still be an efficient measure to detect outliers. &lt;/p&gt;&#10;&#10;&lt;p&gt;The median absolute deviation is defined as the median of deviations from the median &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{MAD} = \text{median}_i\left(|X_i - \text{median}_j(X_j)|\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and you can find out more about it here &lt;a href=&quot;http://en.wikipedia.org/wiki/Median_absolute_deviation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Median_absolute_deviation&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-20T17:00:35.697" Id="23174" LastActivityDate="2012-02-20T17:36:23.820" LastEditDate="2012-02-20T17:36:23.820" LastEditorUserId="919" OwnerUserId="9233" ParentId="23158" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;For each fold, a new model should be completely developed on a training set. Then the samples in the separate test should be classified to determine the error rate. The samples in the test set should &lt;strong&gt;not&lt;/strong&gt; be used for &lt;strong&gt;any&lt;/strong&gt; aspect of model development. This includes variable selection, selection of cutpoints, etc. I haven't worked with neural networks before but I assume this advice also holds true for them. If you are doing ten-fold cross-validation, you should make ten different, totally independent models.&lt;/p&gt;&#10;&#10;&lt;p&gt;See this paper by Refaeilzadeh, et al., for an accessible overview:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.public.asu.edu/~ltang9/papers/ency-cross-validation.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.public.asu.edu/~ltang9/papers/ency-cross-validation.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-02-20T20:30:41.233" Id="23190" LastActivityDate="2012-02-20T20:30:41.233" OwnerUserId="8724" ParentId="23189" PostTypeId="2" Score="4" />
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;Lets' $p$ and $q$ are probability mass functions of two discrete random variables. I need examples of functions $F(p,q)$ that $r = F(p,q)$ and $r$ is a probability mass function for some random variable. Thank you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I mean that $r$ should be probability mass function for arbitrary distributions $p$ and $q$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-21T04:35:17.837" Id="23210" LastActivityDate="2012-03-09T02:52:48.860" LastEditDate="2012-02-21T11:45:12.417" LastEditorUserId="930" OwnerUserId="8361" PostTypeId="1" Score="-2" Tags="&lt;probability&gt;&lt;function&gt;" Title="Find a mapping of two probability mass function to another probability mass function" ViewCount="237" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;em&gt;Is it valid to train a neural network over and over again with new arriving data (including pruning after each new training)?&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I plan to collect data for a period of time, train/cv/test the networ, then again collect new data and train the existing and already trained network with the new data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The time series prediction setup, e.g. described &lt;a href=&quot;http://neuroph.sourceforge.net/tutorials/StockMarketPredictionTutorial.html&quot;&gt;here&lt;/a&gt;, doesn't fit I think, as the input and the output isn't the same data type, i.e. the input features are volume based count of a data stream and the output is a class label, so I cannot just &quot;move the data to the left&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;My searches on Google yielded mainly results regarding &quot;incremental vs. batch&quot; training of the weights of a model or incremental growing of the hidden layer. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9724&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;This paper&lt;/a&gt; seems to be what I'm looking for, but I'm still not completely confident about the usage of recurrent neural networks.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could also create a new network for each time period, but thus I'd lose the knowledge gathered from the previous time periods.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;So what do you suggest?&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-21T06:21:08.257" FavoriteCount="1" Id="23214" LastActivityDate="2012-09-01T03:26:07.947" LastEditDate="2012-02-21T07:55:16.657" LastEditorUserId="264" OwnerUserId="8116" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;continuous-data&gt;" Title="Incremental training of Neural Networks" ViewCount="454" />
  <row Body="&lt;p&gt;The interaction term in a moderated regression is either significant or it's not. When it comes to simple slopes, however, you can test whether each single simple slope differs significantly from zero. These significance tests are dependent on which value of the moderator you probe the slope.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using +/- 1 SD simply is just a convention - but it makes sense. It shows the slopes of two hypothetical groups, one 1 SD below the mean (which equals a percentage rank of 16% under the assumption of normality) and one 1 SD above the mean (PR = 84%). If you take 2 SD it would characterize groups at PR = 2% and 98% - these are rather extreme values and do not characterize a typical member of the sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I would say it is &lt;em&gt;not&lt;/em&gt; OK to use 2 SD, unless you have a good argument why you want to show simple slopes for very extreme groups.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-21T07:32:09.713" Id="23215" LastActivityDate="2012-02-21T07:32:09.713" OwnerUserId="6082" ParentId="23169" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am interested in a measurement or index that will tell me if a curve is more convex or concave versus a straight line. &lt;/p&gt;&#10;&#10;&lt;p&gt;This curve is sampled in n points and I know the x and y coordinates of these points. &lt;/p&gt;&#10;&#10;&lt;p&gt;The straight line I am comparing this curve with is a line that goes through the first and last point of the curve. Let's suppose that the straight line is oriented bottom left to top right with a certain angle and if the curve is on the left side of the straight line or north of it (top) then it is convex and if it is on the right side or south (bottom) it is concave. &lt;/p&gt;&#10;&#10;&lt;p&gt;If this curve never intersects the straight line – then I can imagine a way to maybe deal with this problem, but if it intersects the straight line then... I am at a loss. Any suggestions will be very much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-21T15:03:02.310" Id="23232" LastActivityDate="2012-02-21T19:56:14.670" LastEditDate="2012-02-21T18:18:06.010" LastEditorUserId="88" OwnerUserId="9349" PostTypeId="1" Score="2" Tags="&lt;curves&gt;" Title="Convex vs. concave curve - measurement or index" ViewCount="290" />
  
  
  <row Body="&lt;p&gt;Yes, I agree, sounds like you could use a ROC curve/AUC approach (sounds a lot like what you're describing, actually). Plenty of info on ROC curves here. You could use the ROCR R package and there are also some SAS macros at &lt;a href=&quot;http://www2.sas.com/proceedings/sugi27/p226-27.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www2.sas.com/proceedings/sugi27/p226-27.pdf&lt;/a&gt;. Analyse-It has a commercial package as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-21T19:56:14.670" Id="23255" LastActivityDate="2012-02-21T19:56:14.670" OwnerUserId="9207" ParentId="23232" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;What you get as your bottom line is of the form&#10;$$
  <row Body="&lt;p&gt;Note that the normalising constant for a IG variable is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{b^a}{\Gamma(a)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is equal to the reciprical of the integral over $\sigma^{2}$ of the kernel of the pdf.  hence we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_0^{\infty}(\sigma^{2})^{-(a+1)}\exp\left(-\frac{b}{\sigma^2}\right)d\sigma^2=\frac{\Gamma(a)}{b^a}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Your integral is of this form for certain choice of $a$ and $b$.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-02-21T21:10:36.200" Id="23262" LastActivityDate="2012-02-21T21:10:36.200" OwnerUserId="2392" ParentId="23181" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;It depends on what you are measuring. If the different runs are on different input data, it makes sense to compare the mean values to get average expected run times (i.e., the $\Theta(n)$ in algorithm analysis). Or may be the maximum to get the worst case analysis (the big Oh notation $O(n)$).&lt;/p&gt;&#10;&#10;&lt;p&gt;If the different runs are on identical data, and you are measuring wall-time rather then CPU time used by the process, minimums might be useful to get rid of the effect of other processes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-21T22:53:02.177" Id="23272" LastActivityDate="2012-02-21T22:53:02.177" OwnerUserId="2728" ParentId="21512" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I would suggest taking a look at NetworkX in Python, to add to the tremendous list of recommendations you're getting. I've found it to be quite flexible. It expressly allows things like images to be nodes:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;You might notice that nodes and edges are not specified as NetworkX objects. This leaves you free to use meaningful items as nodes and edges. The most common choices are numbers or strings, but a node can be any hashable object (except None), and an edge can be associated with any object x using &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://groups.google.com/group/networkx-discuss/browse_thread/thread/aa11e38740863fc0&quot; rel=&quot;nofollow&quot;&gt;This thread&lt;/a&gt; on their message list suggests people have been thinking about problems similar to yours, and I've found their community quite helpful. &lt;/p&gt;&#10;&#10;&lt;p&gt;If nothing else, NetworkX can absolutely calculate almost any graph measure you could care to think about, and lay out the graph for you using &quot;avatar sized squares&quot; which you could use to bring in the actual images.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-21T23:59:39.400" Id="23279" LastActivityDate="2012-02-21T23:59:39.400" OwnerUserId="5836" ParentId="23226" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AcceptedAnswerId="23312" AnswerCount="1" Body="&lt;p&gt;My question originated from Xi'an's &lt;a href=&quot;http://stats.stackexchange.com/questions/23218/posterior-mean-of-exponential-functions-how-to-do-it-with-mcmc&quot;&gt;suggestion&lt;/a&gt; to check integrability against the posterior in my nonlinear hierarchical model. I did not check it, but had possible infinity in mind and found out that one of my conditionals (which is inverse gamma distribution) have infinite variance. So that sampling resulted in chains often being far away at the tails.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, this raised a question for me:&lt;/p&gt;&#10;&#10;&lt;p&gt;How to properly sample from distributions with infinite variances like inverse gamma distribution (with $ \alpha = 2 $) or Levy distribution or any other distribution with infinite variance? What does MCMC offer? I tried to search for papers dealing with such issues, but still no luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-22T10:38:54.970" Id="23303" LastActivityDate="2012-02-28T14:14:02.530" LastEditDate="2012-02-28T14:14:02.530" LastEditorUserId="9343" OwnerUserId="9343" PostTypeId="1" Score="4" Tags="&lt;variance&gt;&lt;mcmc&gt;&lt;inverse-gamma&gt;" Title="MCMC for infinite variance posteriors" ViewCount="200" />
  <row Body="&lt;p&gt;You have explicitly asked for type-1 computations, which skip the use of car::Anova and therefore miss out on its computation of assumption tests. If your data are balanced with regards to the between-Ss variable, then you can omit the &lt;code&gt;type=1&lt;/code&gt; argument and receive the same results plus assumption tests. If your data aren't balanced with regards to the between-Ss variable I'm afraid you're out of luck.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-02-22T12:23:36.687" Id="23307" LastActivityDate="2012-02-22T12:23:36.687" OwnerUserId="364" ParentId="23305" PostTypeId="2" Score="2" />
&#10;$$&#10;when the $x_i$'s are Cauchy leads to an infinite variance estimate. See, e.g., &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; expl=matrix(abs(rcauchy(10^6)),ncol=1000)&#10;&amp;gt; est=apply(expl,2,mean)/2&#10;&amp;gt; quantile(est,c(.9,.99,.999))&#10;      90%       99%     99.9% &#10;   6.484375  37.393755 160.869406 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which shows that the estimator can get very large! And away from the true value&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; integrate(function(x){sqrt(x)*dcauchy(x)},low=0,up=Inf)&#10;0.7071078 with absolute error &amp;lt; 2e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this case, you need to use importance sampling.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-22T13:21:37.287" Id="23312" LastActivityDate="2012-02-22T13:41:47.967" LastEditDate="2012-02-22T13:41:47.967" LastEditorUserId="7224" OwnerUserId="7224" ParentId="23303" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Cross correlation assumes a linear relationship between 2 sets of data. Whereas mutual information only assumes that one value of one dataset says something about the value of the other dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;So mutual information makes much weaker assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;A traditional problem solved with mutual information is aligning (registration) of two types of medical images, for example an ultrasound and a x-ray image.&#10;(typically, the types of images are called modalities, so the problem is named multi-modal image registration).&lt;/p&gt;&#10;&#10;&lt;p&gt;For both X-ray and ultrasound, a specific material, say bone, leads to a certain 'brightness' in the image. Whereas some materials lead to a bright x-ray and ultrasound image, for other materials (e.g. fat) it might be the opposite, one is bright, the other is dark. &#10;Therefore, it is not the case that bright parts of the X-ray image are also bright parts of the ultrasound. &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, mutual information is still a useful criterion for aligning the images, but cross correlation is not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-22T16:43:40.563" Id="23322" LastActivityDate="2012-02-22T16:43:40.563" OwnerUserId="3867" ParentId="23308" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;(You should probably cite the source for your naming conventions and explain in more detail why this question is being posed. If this a case of trying to match the documentation for SAS or SPSS we might have cross-cultural difficulties.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The quick answer to your specific question about how to get a &quot;Peto test&quot; is to use rho=1, but it will be an approximation. Referring to the one-sample and two-sample sections of chapter 7 in Klein and Moeschberger's &quot;Survival Analysis&quot;, we read that the Peto-Peto version and the Gehan versions were both two-sample (censored) versions of the Mann-Whitney Wilcoxon two-sample test but used different versions of the survival function estimator. There is no single 'Fleming-Harrington test' since that term refers to a family of tests which reduce to the log-rank and the Wilcoxon-type  tests at specified values of &lt;code&gt;rho&lt;/code&gt;. (The R/S &lt;code&gt;surv.diff&lt;/code&gt; function has the q-parameter of the Fleming-Harrington family fixed at 0 and only varies the p-parameter which it names rho.)&lt;/p&gt;&#10;&#10;&lt;p&gt;A meta-question is whether you should be focusing on the names and not on the mathematical substance? Choosing p=rho=0 (with q fixed at 0) in the Fleming-Harrington family weights the (O-E) or cross-group differences equally across the range of times, whereas both the Gehan-Wilcoxon and Peto-Peto tests weight the early deaths more strongly. My opinion (as a physician) is that it's sensible to have a weighting the considers early differences more probative for the typical case, but can imagine specific instances where the other choice could be defended.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-22T17:01:47.877" Id="23327" LastActivityDate="2012-02-22T17:23:07.647" LastEditDate="2012-02-22T17:23:07.647" LastEditorUserId="2129" OwnerUserId="2129" ParentId="23323" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Does it make sense to define $Z_{i}=Y_{i1}-Y_{i2}$ and to repeat your regression analysis? That makes a strong assumption about the tradeoff between throughput and the cost of bandwith, which is that they can be compared one for one. That may be like comparing oranges and orangutangs and I don't know enought about your problem to tell. A more sophisticated approach would be to convert both throughput and the bandwith into dollars (or another currency, or just a common scale), with $Z_{i}=Benefit(Y_{i1})-Cost(Y_{i2})$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-22T18:20:28.650" Id="23330" LastActivityDate="2012-02-22T18:20:28.650" OwnerUserId="7071" ParentId="23263" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It's funny that the most upvoted answer doesn't really answer the question :) so I thought it would be nice to back this up with a bit more theory - mostly taken from &lt;em&gt;&quot;Data Mining: Practical Machine Learning Tools and Techniques&quot;&lt;/em&gt; and &lt;em&gt;Tom Mitchell's &quot;Machine Learning&quot;&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Introduction.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So we have a classifier and a limited dataset, and a certain amount of data must go into training set and the rest is used for testing (if necessary, a third subset used for validation). &lt;/p&gt;&#10;&#10;&lt;p&gt;Dilemma we face is this: to find a good classiﬁer, the &quot;training subset&quot; should be as big as possible, but to get a good error estimate the &quot;test subset&quot; should be as big as possible - but both subsets are taken from the same pool.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's obvious that the training set should be bigger than the test set - that is, the split should not be 1:1 (main goal is to &lt;em&gt;train&lt;/em&gt;, not to &lt;em&gt;test&lt;/em&gt;) - but it's not clear where the split should be.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Holdout procedure.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The procedure of splitting the &quot;superset&quot; into subsets is called &lt;em&gt;holdout method&lt;/em&gt;. Note that you may easily get unlucky and examples of a certain class could be missing (or overpresented) in one of the subsets, which can be addressed via&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;random sampling, which guarantees that each class is properly represented in all data subsets - the procedure is called &lt;em&gt;stratiﬁed holdout&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;random sampling with repeated training-testing-validation process on top of it - which is called &lt;em&gt;repeated stratified holdout&lt;/em&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In a single (nonrepeated) holdout procedure, you might consider swapping the roles of the&#10;testing and training data and average the two results, but this is only plausible with a 1:1 split between training and test sets which is not acceptable (see &lt;em&gt;Introduction&lt;/em&gt;). But this gives an idea, and an improved method (called &lt;em&gt;cross-validation&lt;/em&gt; is used instead) - see below!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Cross-validation.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In cross-validation, you decide on a ﬁxed number of &lt;em&gt;folds&lt;/em&gt; (partitions of the data). If we use three folds, the data is split into three equal partitions and &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;we use 2/3 for training and 1/3 for testing &lt;/li&gt;&#10;&lt;li&gt;and repeat the procedure three times so that, in the end, every instance has been used exactly once for testing. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This is called &lt;em&gt;threefold cross-validation&lt;/em&gt;, and if stratiﬁcation is adopted as well (which it often true) it is called &lt;em&gt;stratified threefold cross-validation&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, lo and behold, the standard way is &lt;strong&gt;not&lt;/strong&gt; the 2/3:1/3 split. Quotting &lt;em&gt;&quot;Data Mining: Practical Machine Learning Tools and Techniques&quot;&lt;/em&gt;,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The standard way [...] is to use stratiﬁed 10-fold cross-validation. The data is divided randomly into 10 parts in which the class is represented in approximately the same proportions as in the full dataset. Each part is held out in turn and the learning scheme trained on the remaining nine-tenths; then its error rate is calculated on the holdout set. Thus the learning procedure is executed a total of 10 times on different training sets (each of which have a lot in common). Finally, the 10 error estimates are averaged to yield an overall error estimate.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Why 10? Because &lt;em&gt;&quot;..Extensive tests on numerous datasets, with different learning techniques, have shown that 10 is about the right number of folds to get the best&#10;estimate of error, and there is also some theoretical evidence that backs this up..&quot;&lt;/em&gt; I haven't found which extensive tests and theoretical evidence they meant but this one seems like a good start for digging more - if you wish.&lt;/p&gt;&#10;&#10;&lt;p&gt;They basically just say &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Although these arguments are by no means conclusive, and debate continues to&#10;  rage in machine learning and data mining circles about what is the best scheme&#10;  for evaluation, 10-fold cross-validation has become the standard method in&#10;  practical terms. [...] Moreover, there is nothing magic about the exact number&#10;  10: 5-fold or 20-fold cross-validation is likely to be almost as good.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Bootstrap, and - finally! - the answer to the original question.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But we haven't yet arrived to the answer as to, why the 2/3:1/3 is often recommended. My take is that it's inherited from &lt;em&gt;bootstrap&lt;/em&gt; method.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's based on sampling with replacement. Previously, we put a sample from the &quot;grand set&quot; into exactly one of the subsets. Bootstraping is different and a sample can easily appear in both training and test set. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's look into one particular scenario where we take a dataset &lt;em&gt;D1&lt;/em&gt; of &lt;em&gt;n&lt;/em&gt; instances and sample it &lt;em&gt;n&lt;/em&gt; times with replacement, to get another dataset &lt;em&gt;D2&lt;/em&gt; of &lt;em&gt;n&lt;/em&gt; instances. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now watch narrowly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because some elements in &lt;em&gt;D2&lt;/em&gt; will (almost certainly) be repeated, there must be some instances in the original dataset that have not been picked: we will use these as test instances.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the chance that a particular instance wasn't picked up for &lt;em&gt;D2&lt;/em&gt;? The probability of being picked up on each take is &lt;em&gt;1/n&lt;/em&gt; so the opposite is &lt;em&gt;(1 - 1/n)&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;When we multiply these probabilities together, it's &lt;em&gt;(1 - 1/n)^n&lt;/em&gt; which is &lt;em&gt;e^-1&lt;/em&gt; which is about 0.3. This means our test set will be about 1/3 and the training set will be about 2/3.&lt;/p&gt;&#10;&#10;&lt;p&gt;I &lt;em&gt;guess&lt;/em&gt; this is the reason why it's recommended to use 1/3:2/3 split: this ratio is taken from the bootstrapping estimation method.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Wrapping it up.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to finish off with a quote from the data mining book (which I cannot prove but assume correct) where they generally recommend to prefer 10-fold cross-validation:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The bootstrap procedure may be the best way of estimating error for very&#10;  small datasets. However, like leave-one-out cross-validation, it has disadvantages&#10;  that can be illustrated by considering a special, artiﬁcial situation [...] a completely random dataset with two classes. The true error rate is 50% for any prediction rule.But a scheme that memorized the training set would give a perfect resubstitution score of 100%&#10;  so that etraining instances= 0, and the 0.632 bootstrap will mix this in with a weight&#10;  of 0.368 to give an overall error rate of only 31.6% (0.632 ¥ 50% + 0.368 ¥ 0%),&#10;  which is misleadingly optimistic.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2012-02-15T16:56:56.760" Id="23334" LastActivityDate="2012-02-15T16:56:56.760" OwnerDisplayName="andreister" OwnerUserId="9013" ParentId="23331" PostTypeId="2" Score="10" />
  
  <row Body="&lt;p&gt;Now that I understand your data I would certainly use the original set in your analysis rather than the four averages.  The &quot;noise&quot; that you describe is actually a crucial part of your data.  The higher $R^2$ in the model with fewer data points is because you have taken out the variation around the means.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First, which of your two plots and regressions to use?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;By using a technique like linear regression on your original data (the 120 points) you can simultaneously look at the average perceived velocity for each of your four levels of actual velocity, and conduct inference based on a full understanding of the level of variation in the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you conduct it on just the four means, you have split your analysis into two phases.  The first phase finds the average for each level of actual velocity; then the second (where you conduct regression) tries to do inference to the general population.  You can't really do what you want to in this second phase because you have lost all the information about the randomness in your first phase.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to come back to your picture of the four averages as part of a result-reporting summary of your data, but all inference should be based on a model fitted to the original set.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second, what analysis to do&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because you have repeated observations with characteristics in common (eg different trials; and each observer with different goes) you can't just fit a regression to all 120 points.  You need to somehow control for observer effects.  I would fit a model like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;perceived ~ actual * observer&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;while hoping that can be reduced to&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;perceived ~ actual + observer&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For this to work properly &lt;code&gt;actual&lt;/code&gt; needs to be an ordered factor.   R will then automatically apply the correct contrasts to it in fitting a model (sorry I don't have time to explain that better but there would be material on the web about what that means).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there are lots of potential fishhooks.  Can perceived velocity really be treated as continuous?  Does it have an approximately normal distribution (obviously not really, but it may be close enough)?  Is the variance independent of the mean?  Evading these will be tricky and you should use R's graphic capabilities fully in exploring your data.  If the assumptions behind linear regression do not hold, you may be able to use the polr() function from library(MASS) to fit an ordinal response regression instead.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-22T18:51:35.620" Id="23345" LastActivityDate="2012-02-22T21:28:22.503" LastEditDate="2012-02-22T21:28:22.503" LastEditorUserId="7972" OwnerUserId="7972" ParentId="23282" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;This is how I would approach it. You actually need to check if a text is in class I or III (else it would be class II). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;First, define a bag of words for classes I and III. You can manually do this&lt;/li&gt;&#10;&lt;li&gt;For each text, calculate the tf-idf for the words in these two classes and sum it (get two sums).&lt;/li&gt;&#10;&lt;li&gt;If some of these two sums is above some predefined threshold then it belongs in that class.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If you have a learning dataset big enough, you can easily find out what are the two bags of words, as well as the two thresholds for them.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-19T02:16:26.990" Id="23370" LastActivityDate="2012-02-19T02:16:26.990" OwnerDisplayName="vonPetrushev" OwnerUserId="7819" ParentId="23369" PostTypeId="2" Score="4" />
  
  <row AnswerCount="2" Body="&lt;p&gt;The most well-known bandit algorithm is upper confidence bound (UCB) which popularized this class of algorithms.  Since then I presume there are now better algorithms.  What is the current best algorithm (in terms of either empirical performance or theoretical bounds)?  Is this algorithm optimal in some sense?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-16T23:06:00.480" FavoriteCount="2" Id="23382" LastActivityDate="2012-07-09T16:33:12.443" LastEditDate="2012-07-09T16:33:12.443" LastEditorUserId="4872" OwnerDisplayName="yeewhye" PostTypeId="1" Score="13" Tags="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;theory&gt;&lt;reinforcement-learning&gt;&lt;multiarmed-bandit&gt;" Title="Best bandit algorithm?" ViewCount="1118" />
  
  <row Body="&lt;p&gt;In boosting we use weak learners mostly since they are trained faster compared to strong learners. Think about it. If I use Multi Layer Neural Network as learner, then I need to train lots of them. On the otherhand a decision tree may be a lot faster , then I can train lots of them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say I use 100 learners. I train NN in 100 seconds and decision tree in 10 seconds. My first boosting with NN will take 100*100 seconds  while second boosting with decision tree will take 100*10 seconds. &lt;/p&gt;&#10;&#10;&lt;p&gt;That said I have seen articles, which uses strong learners in boosting. But in that problems that strong learners was fast in my opinion.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to train MLP on KDD99 Intrusion Detection Dataset, (4+ Million) using Weka. It took more than 72 hours on my machine. But boosting  (AdaBoostM1 with Decision Tree - Decision Stump) took only 3 hours. In this problem it is clear that I can not use boosting with strong learner, that is a learner which takes too much time.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-16T15:21:40.547" Id="23389" LastActivityDate="2012-02-16T15:21:40.547" OwnerDisplayName="Atilla Ozgur" OwnerUserId="7170" ParentId="23388" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Dimensionality reduction via something like PCA would be helpful to get an idea of the number of dimensions that are critical to represent your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;To check for misclassified instances, you can do a rudimentary k-means clustering of your data to get an idea of how well your raw data would fit your proposed categories.  While not automatic, visualizing at this stage would be helpful, as your visual brain is a powerful classifier in and of itself.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of data that are outright missing, statistics has &lt;a href=&quot;http://en.wikipedia.org/wiki/Missing_data&quot;&gt;numerous techniques&lt;/a&gt; to deal with that situation already, including imputation, taking data from the existing set or another set to fill in the gaps.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-16T07:53:52.263" Id="23404" LastActivityDate="2012-02-16T08:00:41.750" OwnerDisplayName="jonsca" OwnerUserId="3826" ParentId="23403" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;If you know that your data is not quite good, it is always good to check for outliers as well. Most of the time there are anomalies.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a lot of features, dimensionality reduction is a must. PCA is quite efficient for that. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you have missing data, you can use imputation or interpolation, but if your needs allows it, the winning case is to use collaborative filtering.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-19T02:38:38.183" Id="23405" LastActivityDate="2012-02-19T02:38:38.183" OwnerDisplayName="vonPetrushev" OwnerUserId="7819" ParentId="23403" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="23416" AnswerCount="1" Body="&lt;p&gt;Suppose we measure the classifier error on a test set and obtain a certain success rate - say, 75%. Now, of course, this is only one measurement - how to calculate the &quot;true&quot; success rate? Sure it will be close to 75% but how close? &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand it's related to confidence intervals but now I'm lost in confidence intervals. I think my example is similar to &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval#Practical_example&quot;&gt;this one on wikipedia&lt;/a&gt;  where they look at weight distribution of margarine cups. (Sorry, math is not rendering here so I created a screenshot - you might also want to flick through the corresponding &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval#Practical_example&quot;&gt;section&lt;/a&gt; in the wikipedia article). &lt;/p&gt;&#10;&#10;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img src=&quot;http://i.stack.imgur.com/YZhf3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why they use the above standard error formula?&lt;/li&gt;&#10;&lt;li&gt;Where does this Ф^{-1}(0.975)=1.96 come from?&lt;/li&gt;&#10;&lt;li&gt;To solve my &quot;true success rate&quot; problem, should I repeat the estimation N times and then apply the same reasoning as they do with margarine cups?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-02-15T15:22:03.887" Id="23415" LastActivityDate="2012-02-16T08:53:37.530" OwnerDisplayName="andreister" OwnerUserId="9013" PostTypeId="1" Score="5" Tags="&lt;classification&gt;" Title="Classifier success rate and confidence intervals" ViewCount="591" />
  <row AnswerCount="3" Body="&lt;p&gt;A &lt;a href=&quot;http://en.wikipedia.org/wiki/Recommender_system&quot;&gt;Recommender System&lt;/a&gt; would measure the correlation between ratings of different users and yield recommendations for a given user about the items which may be of interest to him.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, tastes change over time so &lt;em&gt;old ratings&lt;/em&gt; might not reflect &lt;em&gt;current preferences&lt;/em&gt; and vice versa. You may once have put &quot;excellent&quot; to a book you would now rate as &quot;not too disgusting&quot; and so on. Moreover, the interests themselves do change as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How should recommender systems work in a changing environment?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;One option is to cut off the &quot;old&quot; ratings, which may work just fine assuming you correctly define &quot;old&quot; (you can even say ratings never expire and pretend that the problem doesn't exist). But it's not the best possible option: of course tastes evolve, it's a normal life flow, and there's no reason why we cannot use the extra knowledge of once correct past ratings. &lt;/li&gt;&#10;&lt;li&gt;Another option is to somehow accommodate this extra knowledge. Thus we could not just find an &quot;instant match&quot; for your current interests but suggest you the things you may like &lt;em&gt;next&lt;/em&gt; (as opposed to the things you may like &lt;em&gt;now&lt;/em&gt;).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I'm not sure if I'm explaining this well enough. Basically I'm in favor of the second approach and am talking about a Recommender System which would measure the correlations of taste &lt;em&gt;trajectories&lt;/em&gt; and yield recommendations which will cater for.. well, let's call it personal growth - because they will be coming from people whose &quot;tastes trajectory&quot; (and not just &quot;tastes snapshot&quot;) is similar to yours.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Now the question:&lt;/strong&gt; I wonder if something similar to the &quot;option 2&quot; already exists and, if it does, I wonder how it works. And if it doesn't exist, you're welcome to discuss how it should work! :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-15T14:09:55.393" FavoriteCount="4" Id="23417" LastActivityDate="2012-09-18T04:54:59.747" OwnerDisplayName="andreister" OwnerUserId="9013" PostTypeId="1" Score="10" Tags="&lt;recommender-system&gt;" Title="Dynamic recommender systems" ViewCount="353" />
  <row Body="&lt;p&gt;There are multiple papers by Hinton et al. which deal with temporal data and also audio (&lt;a href=&quot;http://www.cs.toronto.edu/~hinton/papers.html&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.toronto.edu/~hinton/papers.html&lt;/a&gt;). For example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Acoustic Modeling using Deep Belief Networks, 2012.&lt;/li&gt;&#10;&lt;li&gt;Learning a better Representation of Speech Sound Waves using&#10;Restricted Boltzmann Machines, 2011.&lt;/li&gt;&#10;&lt;li&gt;Deep Belief Networks using Discriminative Features for Phone&#10;Recognition, 2011. The Recurrent Temporal Restricted Boltzmann&#10;Machine, 2009.&lt;/li&gt;&#10;&lt;li&gt;Factored Conditional Restricted Boltzmann Machines for Modeling&#10;Motion Style, 2009.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I haven't read the more recent papers, but the 2009 papers should give you a good sense of how temporal data can be modeled using RBMs and DBNs.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-15T19:30:51.453" Id="23425" LastActivityDate="2012-02-15T19:30:51.453" OwnerDisplayName="Lucas" OwnerUserId="7733" ParentId="23423" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="23430" AnswerCount="1" Body="&lt;p&gt;When doing natural language processing, one can take a corpus and evaluate the probability of the next word occurring in a sequence of n.  n is usually chosen as 2 or 3 (bigrams and trigrams).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a known point at which tracking the data for the nth chain becomes counterproductive, given the amount of time it takes to classify a particular corpus once at that level?  Or given the amount of time it would take to look up the probabilities from a (data structure) dictionary?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-14T00:54:09.033" FavoriteCount="2" Id="23429" LastActivityDate="2012-02-24T09:14:10.480" LastEditDate="2012-02-24T09:14:10.480" LastEditorUserId="7365" OwnerDisplayName="jonsca" OwnerUserId="3826" PostTypeId="1" Score="5" Tags="&lt;text-mining&gt;&lt;natural-language&gt;" Title="At what n do n-grams become counterproductive?" ViewCount="334" />
  <row AcceptedAnswerId="23432" AnswerCount="1" Body="&lt;p&gt;I was looking at the &lt;a href=&quot;http://en.wikipedia.org/wiki/Fiducial_inference&quot; rel=&quot;nofollow&quot;&gt;Fiducial Inference&lt;/a&gt; page on wikipedia, which is an alternative to the traditional Frequentist and Bayesian standpoints. Although it was out of favour in mainstream statistics for many years, there seems to have been a resurgence in interest in recent years (see for example &lt;a href=&quot;http://www.unc.edu/~hannig/r_publications.html&quot; rel=&quot;nofollow&quot;&gt;Jan Hannig's recent publications on the subject&lt;/a&gt;). Does anyone know of anyone in the ML community looking at these ideas from a theoretical point of view, or who has successfully created an algorithm based on them?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-13T11:34:39.367" Id="23431" LastActivityDate="2012-02-23T22:10:33.557" LastEditDate="2012-02-23T22:10:33.557" LastEditorUserId="5594" OwnerDisplayName="tdc" OwnerUserId="7365" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;inference&gt;" Title="Fiducial Inference in Machine Learning" ViewCount="80" />
  
  <row Body="&lt;p&gt;We did a bit of work on this at one point. The set of features we extracted are given in this &lt;a href=&quot;http://eprints.pascal-network.org/archive/00003416/01/Diethe.pdf&quot;&gt;NIPS workshop paper&lt;/a&gt;. I have to admit we couldn't replicate the results of some other authors in the field, although there were some doubts about the datasets used in these (note that the datasets used by authors in this field tend to be hand-picked and not released to the public, for copyright reasons, although this not always the case). Essentially they were all &lt;strong&gt;short-term spectral features&lt;/strong&gt; with Autoregression coefficients thrown in too. We were looking at classification of genre, which we know can be done by humans (although not with wonderful accuracy, and not with consistent agreement ....) in very short timespans (&amp;lt;1s), which validates the use of short term features. If you're interested in doing more complicated things than the typical genre/artist/album/producer classification then you might need more long-range features, otherwise these short-term spectral features tend to perform best.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-13T10:53:50.433" Id="23434" LastActivityDate="2012-02-13T10:53:50.433" OwnerDisplayName="tdc" OwnerUserId="7365" ParentId="23433" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Cool question!&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems like this question is about the natural technique for this type of&#10;problem. I think think the natural technique for this type of problem is&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;reinforcement learning&lt;/a&gt; (RL). RL is about how an agent ought to take actions in&#10;an environment so as to maximize some notion of cumulative reward. Perhaps the&#10;best known algorithm for RL is &lt;a href=&quot;http://en.wikipedia.org/wiki/Q-learning&quot;&gt;Q-learning&lt;/a&gt;. I think this is the first question in&#10;this site about reinforcement learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think what you're asking is true if you try to approach this as&#10;classification/regression, but those do not seem like the right tool for this&#10;problem. This is naturally a RL problem where sequences of actions and outcomes&#10;need to be taken into account.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-13T04:31:57.973" Id="23436" LastActivityDate="2012-02-13T04:31:57.973" OwnerDisplayName="carlosdc" OwnerUserId="1540" ParentId="23435" PostTypeId="2" Score="6" />
  <row AnswerCount="5" Body="&lt;p&gt;A limitations of standard neural net algorithms (like backprop) is that you have to make a design decision of how many hidden layers and neurons-per-layer you want. Usually, the learning rate and generalization is highly sensitive to these choices. This has been the reason, why neural net algorithms like &lt;a href=&quot;http://en.wikipedia.org/wiki/Cascade_correlation_algorithm&quot;&gt;cascade correlation&lt;/a&gt; have been generating interest. It starts with a minimal topology (just input and output unit) and recruit new hidden units as learning progresses. &lt;/p&gt;&#10;&#10;&lt;p&gt;The CC-NN algorithm was introduced by Fahlman in 1990, and the recurrent version in 1991. &lt;strong&gt;What are some more recent (post 1992) neural net algorithms that start with a minimal topology?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Related questions&lt;/h3&gt;&#10;&#10;&lt;p&gt;CogSci.SE: &lt;a href=&quot;http://cogsci.stackexchange.com/q/214/29&quot;&gt;Neural networks with biologically plausible accounts of neurogenesis&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-12T01:42:55.523" FavoriteCount="7" Id="23439" LastActivityDate="2012-02-16T00:06:11.173" OwnerDisplayName="Artem Kaznatcheev" OwnerUserId="4872" PostTypeId="1" Score="14" Tags="&lt;neural-networks&gt;" Title="Modern neural networks that build their own topology" ViewCount="803" />
  
  
  
  <row Body="&lt;p&gt;There is no need for a neural network approach, collaborative filtering is an algorithm on itself. For your problem specifically, there is a good description of cf and recomender system on:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ml-class.org/course/video/preview_list&quot; rel=&quot;nofollow&quot; title=&quot;ml-class.org&quot;&gt;ml-class.org&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(look for XVI: Recommender Systems). It is elegant, simple, and if you do it right (that is, use vectorized form, fast minimizers, and prepared gradients) it can be quite fast.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-19T00:59:59.673" Id="23450" LastActivityDate="2012-02-19T00:59:59.673" OwnerDisplayName="vonPetrushev" OwnerUserId="7819" ParentId="23448" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Your pollutant problem probably doesn't need much of a language at all. It looks like a symbolic regression rather than a control problem, in which case you could just use standard tree GP, with features and a few useful constants as the terminal set and relevant operators in the function set. The GP system will weed out irrelevant features and there are techniques to handle very large datasets. Generally, specify the smallest function set that you estimate could solve the problem, and expand it with care if necessary.&lt;/p&gt;&#10;&#10;&lt;p&gt;You'll need to choose between tree and linear GP early on.  Lisp is tree, Slash/A is linear. Read up on both to understand the pros &amp;amp; cons, but from what you wrote I'd suggest a simple tree GP system. It's not too hard to write your own, but there are existing Python implementations. These ones below are for evolutionary algorithms in Python in general but not all do GP and some are inactive:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;PyGressionGP (GP for symbolic regression in Python) --&#10;&lt;a href=&quot;http://code.google.com/p/pygressiongp/&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/pygressiongp/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;PyGene -- &lt;a href=&quot;https://github.com/blaa/PyGene&quot; rel=&quot;nofollow&quot;&gt;https://github.com/blaa/PyGene&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;A Simple Genetic Programming in Python --&#10;&lt;a href=&quot;http://zhanggw.wordpress.com/2009/11/08/a-simple-genetic-programming-in-python-4/&quot; rel=&quot;nofollow&quot;&gt;http://zhanggw.wordpress.com/2009/11/08/a-simple-genetic-programming-in-python-4/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Pyevolve -- &lt;a href=&quot;https://github.com/perone/Pyevolve&quot; rel=&quot;nofollow&quot;&gt;https://github.com/perone/Pyevolve&lt;/a&gt; -- also see blog --&#10;&lt;a href=&quot;http://blog.christianperone.com&quot; rel=&quot;nofollow&quot;&gt;http://blog.christianperone.com&lt;/a&gt; -- and this post --&#10;&lt;a href=&quot;http://blog.christianperone.com/?p=549&quot; rel=&quot;nofollow&quot;&gt;http://blog.christianperone.com/?p=549&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;esec (Evolutionary Computation in Python) --&#10;&lt;a href=&quot;http://code.google.com/p/esec/&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/esec/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Peach -- &lt;a href=&quot;http://code.google.com/p/peach/&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/peach/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;PyBrain (does a lot, not just NN) -- &lt;a href=&quot;http://pybrain.org/&quot; rel=&quot;nofollow&quot;&gt;http://pybrain.org/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;dione -- &lt;a href=&quot;http://dione.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;http://dione.sourceforge.net/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;PyGEP (Genetic Expression Programming) --&#10;&lt;a href=&quot;http://code.google.com/p/pygep/&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/pygep/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;deap (Distributed Evolutionary Algorithms) --&#10; &lt;a href=&quot;http://code.google.com/p/deap/&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/deap/&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Also, see the (free) introductory book on GP by well-known GP authors Poli, Langdon and McPhee:&lt;/p&gt;&#10;&#10;&lt;p&gt;A Field Guide to Genetic Programming -- &lt;a href=&quot;http://www.gp-field-guide.org.uk/&quot; rel=&quot;nofollow&quot;&gt;http://www.gp-field-guide.org.uk/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-13T10:08:46.377" Id="23453" LastActivityDate="2015-01-20T14:51:24.663" LastEditDate="2015-01-20T14:51:24.663" LastEditorUserId="46095" OwnerDisplayName="Graham Jones" OwnerUserId="9336" ParentId="23451" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;How about: &lt;a href=&quot;http://www.ml-class.org/&quot;&gt;http://www.ml-class.org/&lt;/a&gt;? It has good introduction and some programming excersises. AFAIK Euler has much more sophisticated examples, but ml-class is still a good beginning.&lt;/p&gt;&#10;&#10;&lt;p&gt;As it was pointed in the comments this course has next edition: &lt;a href=&quot;http://jan2012.ml-class.org/#&quot;&gt;http://jan2012.ml-class.org/#&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2012-02-22T23:48:17.327" CreationDate="2012-02-12T19:20:03.223" Id="23462" LastActivityDate="2012-02-13T12:34:16.090" OwnerDisplayName="jb." OwnerUserId="7425" ParentId="23459" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="23464" AnswerCount="2" Body="&lt;p&gt;Recently there was a ML-like question over on cstheory stackexchange, and I posted an answer recommending Powell's method, gradient descent, genetic algorithms, or other &lt;a href=&quot;http://en.wikipedia.org/wiki/Approximation_algorithm&quot; rel=&quot;nofollow&quot;&gt;&quot;approximation algorithms&quot;.&lt;/a&gt; In a comment someone told me these methods were &quot;heuristics&quot; and &lt;em&gt;not&lt;/em&gt; &quot;approximation algorithms&quot; and frequently did not come close to the theoretical optimum (because they &quot;frequently get stuck in local minima&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Do others agree with that? Also, it seems to me there is a sense of which heuristic algorithms can be guaranteed to come close to theoretical optimums if they are set up to explore a large part of the search space (eg setting parameters/step sizes small), although I haven't seen that in a paper. Does anyone know if this has been shown or proven in a paper? (if not for a large class of algorithms maybe for a small class say NNs etc.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-10T19:03:03.517" FavoriteCount="2" Id="23463" LastActivityDate="2015-03-03T09:54:22.567" LastEditDate="2015-03-03T09:54:22.567" LastEditorUserId="70237" OwnerDisplayName="vzn" OwnerUserId="17493" PostTypeId="1" Score="15" Tags="&lt;machine-learning&gt;&lt;optimization&gt;&lt;approximation&gt;" Title="Are machine learning techniques &quot;approximation algorithms&quot;?" ViewCount="628" />
  <row Body="&lt;p&gt;I use the &lt;strong&gt;Elbow method&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Start with K=2, and keep increasing it in each step by 1, calculating your clusters and the cost that comes with the training. At some value for K the cost drops dramatically, and after that it reaches a plateau when you increase it further. This is the K value you want.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The rationale is that after this, you increase the number of clusters but the new cluster is very near some of the existing.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-19T01:13:42.223" Id="23478" LastActivityDate="2012-02-19T01:13:42.223" OwnerDisplayName="vonPetrushev" OwnerUserId="7819" ParentId="23472" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;For your two specific examples:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt;&#10;The paper &lt;a href=&quot;http://research.yahoo.com/pub/2293&quot;&gt;&quot;Online Linear Regression and Its Application to Model-Based Reinforcement Learning&quot;&lt;/a&gt; by Alexander Strehl and Michael Littman describes an algorithm called &quot;KWIK Linear Regression&quot; (see algorithm 1) which provides an approximation to the linear regression solution using incremental updates. Note that this is &lt;a href=&quot;http://en.wikipedia.org/wiki/Tikhonov_regularization&quot;&gt;&lt;em&gt;not regularised&lt;/em&gt;&lt;/a&gt; (i.e. it is not Ridge Regression). I'm pretty sure that the method of Strehl &amp;amp; Littman cannot extend to that setting.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.talkstats.com/showthread.php/15743-Incremental-update-to-%2aexisting%2a-Logistic-Regression-model&quot;&gt;This thread&lt;/a&gt; sheds some light on the matter. Quoting:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Even without a regularization constraint, logistic regression is a nonlinear optimization problem. Already this does not have an analytic solution, which is usually a prerequisite to deriving an update solution. With a regularization constraint, it becomes a constrained optimization problem. This introduces a whole new set of non-analytic complications on top of the ones that the unconstrained problem already had.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There are however other online (or incremental) methods for regression that you might want to look at, for example &lt;a href=&quot;http://wcms.inf.ed.ac.uk/ipab/slmc/research/software-lwpr&quot;&gt;Locally Weighted Projection Regression (LWPR)&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-09T09:45:19.630" Id="23482" LastActivityDate="2012-02-09T09:45:19.630" OwnerDisplayName="tdc" OwnerUserId="7365" ParentId="23481" PostTypeId="2" Score="7" />
  
  <row AcceptedAnswerId="23485" AnswerCount="1" Body="&lt;p&gt;I have come across some basic ways to measure the complexity of neural networks:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Naive and informal: count the number of neurons, hidden neurons, layers, or hidden layers&lt;/li&gt;&#10;&lt;li&gt;VC-dimension (Eduardo D. Sontag [1998] &quot;VC dimension of neural networks&quot; [&lt;a href=&quot;http://mathsci.kaist.ac.kr/~nipl/mas557/VCD_ANN_3.pdf&quot;&gt;pdf&lt;/a&gt;].)&lt;/li&gt;&#10;&lt;li&gt;A course grained and asymptotic &lt;a href=&quot;http://cstheory.stackexchange.com/a/2699/1037&quot;&gt;computational complexity measure by equivalence to $TC^0_d$&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Are there other alternatives?&lt;/p&gt;&#10;&#10;&lt;p&gt;It is preferred:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If the complexity metric could be used to measure neural networks from different paradigms (to measure backprop, dynamics neural nets, cascade correlation, etc) on the same scale. For instance, VC-dimension can be used for different types on networks (or even things other than neural networks) while number of neurons is only useful between very specific models where the activation function, signals (basic sums vs. spikes), and other properties of the network are the same.&lt;/li&gt;&#10;&lt;li&gt;If it has nice correspondences to standard measures of complexity of functions learnable by the network&lt;/li&gt;&#10;&lt;li&gt;If it is easily to compute the metric on specific networks (this last one is not a must, though.)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Notes&lt;/h3&gt;&#10;&#10;&lt;p&gt;This question is based on a &lt;a href=&quot;http://cogsci.stackexchange.com/q/252/29&quot;&gt;more general question&lt;/a&gt; on CogSci.SE.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-08T23:38:31.747" FavoriteCount="4" Id="23484" LastActivityDate="2012-02-24T09:23:56.257" LastEditDate="2012-02-24T09:23:56.257" LastEditorUserId="7365" OwnerDisplayName="Artem Kaznatcheev" OwnerUserId="4872" PostTypeId="1" Score="10" Tags="&lt;neural-networks&gt;&lt;theory&gt;" Title="What are alternatives to VC-dimension for measuring the complexity of neural networks?" ViewCount="230" />
  
  <row AnswerCount="4" Body="&lt;p&gt;Excuse me I'm not an English speaker and I have no idea whatsoever whether there is any standard naming scheme. And since I don't want to use names I have read in some books I will be overly descriptive. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's suppose I want to describe a dataset containing data for prediction of weather pollutants. Each row of this dataset consists of daily averages of some weather parameters for a particular (current) day, daily average of pollutants for the previous day, daily average of pollutants on the current day (this value will be predicted) &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the name of single row in this dataset;&lt;/li&gt;&#10;&lt;li&gt;What is the name of single cell in single row;&lt;/li&gt;&#10;&lt;li&gt;What is the name of part of this dataset that will be input of this model (in this case weather parameters, and pollutant for the previous day);&lt;/li&gt;&#10;&lt;li&gt;What is the name of part of the dataset that model will predict based on the part in latter point;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have some guesses but I'll post them when I get some answers. References to books/articles would be appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;PS. If anyone knows proper Polish translations I would also be very interested :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-08T19:05:38.533" Id="23493" LastActivityDate="2012-02-22T22:45:45.133" LastEditDate="2012-02-22T22:09:45.040" LastEditorUserId="930" OwnerDisplayName="jb." OwnerUserId="7425" PostTypeId="1" Score="5" Tags="&lt;dataset&gt;&lt;terminology&gt;&lt;prediction&gt;" Title="What is the proper naming scheme for dataset parts?" ViewCount="47" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Let $\mathbb P(X=1) = \mathbb P(X=-1) = 1/2$. Define &lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$X_n = \begin {cases} X &amp;amp; \text{with probability } 1- \frac{1}{n}\\
  
  <row Body="&lt;p&gt;The distributions are clearly positively skewed, so a normal distribution wouldn't be appropriate. Economists often seem to assume that income has a log-normal distribution, so that would probably be a good choice if it fits OK. To check that, you could log the data and then construct a &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_probability_plot&quot; rel=&quot;nofollow&quot;&gt;normal probability plot&lt;/a&gt; for each group by plotting the logged percentiles (ignore the mean but include the median as the 50th percentile) against the percentiles of a standard normal distribution. If the points lie roughly on a straight line then the log-normal distribution is a reasonable fit. You could then estimate its parameters by fitting a straight line by least squares - that's not the optimal method, but it's simple and probably good enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Just tried that myself:&#10;&lt;img src=&quot;http://i.stack.imgur.com/XnMmT.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Log-normal seems an reasonable fit in group 2, but not so good in group 1. I don't know if it might still be good enough for your purposes. If not you might need to go to some three-parameter distribution, but that could get a fair bit more complicated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-23T13:21:56.183" Id="23543" LastActivityDate="2012-02-23T13:56:39.993" LastEditDate="2012-02-23T13:56:39.993" LastEditorUserId="449" OwnerUserId="449" ParentId="23531" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I don't understand why predictions from &lt;code&gt;lm.ridge()&lt;/code&gt; are so far out, when using the &quot;best&quot; lambda, based upon GCV. Can anyone help me to obtain better predictions? Or, at least, does anyone have a good ridge example with a simple explanation of the results?&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is my R code for wine quality data (from &lt;a href=&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot; rel=&quot;nofollow&quot;&gt;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;wine_all &amp;lt;- read.table(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot;, sep=&quot;;&quot;, header = T)&#10;#wine_all &amp;lt;- read.table(&quot;winequality-red.csv&quot;, sep=&quot;;&quot;, header = T)&#10;wine_train &amp;lt;- wine_all[1:1400,]&#10;wine_test &amp;lt;- wine_all[-(1:1400),]&#10;&#10;train.lm &amp;lt;- lm.ridge(quality~., wine_train, lambda = seq(0, 100, 0.1))&#10;plot(x=train.lm$lambda, y=train.lm$GCV)&#10;&#10;pred.test &amp;lt;- scale(wine_test[,1:11], center = F, scale = train.lm$scales) %*% train.lm$coef[, which.min(train.lm$GCV)] + train.lm$ym&#10;&#10;pred.all &amp;lt;-  scale(wine_all[,1:11], center = F, scale = train.lm$scales) %*% train.lm$coef[, which.min(train.lm$GCV)] + train.lm$ym&#10;cor(wine_test[, 12], pred.test)^2&#10;cor(wine_all[, 12], pred.all)^2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-02-22T23:18:11.537" Id="23548" LastActivityDate="2014-06-29T23:44:32.877" LastEditDate="2014-06-29T23:44:32.877" LastEditorUserId="19762" OwnerDisplayName="Andrew Dempsey" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;ridge-regression&gt;" Title="Poor predictions from lm.ridge?" ViewCount="2038" />
  
  
  
  <row AcceptedAnswerId="23557" AnswerCount="1" Body="&lt;p&gt;Whenever boosting is brought up, Adaboost is the first algorithm to be listed.  What are the most popular boosting algorithms that &lt;em&gt;aren't&lt;/em&gt; Adaboost?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-23T18:13:14.507" Id="23556" LastActivityDate="2012-02-23T18:34:36.503" OwnerUserId="9391" PostTypeId="1" Score="2" Tags="&lt;boosting&gt;&lt;ensemble&gt;" Title="What are the strongest boosting alternatives to Adaboost?" ViewCount="180" />
  <row Body="&lt;p&gt;I'm not sure about FPCA, but one thing to remember, is that in &lt;em&gt;extremely&lt;/em&gt; high dimensions, there is a lot more &quot;space&quot;, and points within the space start to look uniformly distributed (i.e. everything is far from everything else). At this point the covariance matrix will start to look essentially uniform, and will be very highly sensitive to noise. It therefore becomes a bad estimate of the &quot;true&quot; covariance. Perhaps FPCA gets round this somehow, but I'm not sure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-17T11:03:46.217" Id="23567" LastActivityDate="2012-02-17T11:03:46.217" OwnerDisplayName="tdc" OwnerUserId="7365" ParentId="23566" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Exactly, as you state in the question and as @tdc puts in his answer, in case of extremely high dimensions even if the geometric properties of PCA remain valid, the covariance matrix is no longer a good estimate of the real population covariance.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;There's a &lt;em&gt;very&lt;/em&gt; interesting paper &lt;em&gt;&quot;Functional Principal Component&#10;Analysis of fMRI Data&quot;&lt;/em&gt; (&lt;a href=&quot;http://spin.ecn.purdue.edu/fmri/PDFLibrary/VivianiR_HBM_2005_24_109_129.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;) where they use functional PCA to visualize the variance:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;...As in other explorative techniques, the objective is that of providing an initial assessment that will give the data a chance “to speak for themselves” before an appropriate model is chosen. [...]&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In the paper they explain how exactly they've done it, and also provide theoretical reasoning:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The decisive advantage of this approach consists in the possibility of specifying a set of assumptions in the choice of the basis function set and in the error functional minimized by the fit. These assumptions will be weaker than the speciﬁcation of a predeﬁned hemodynamic function and a set of events or conditions as in F-masking, thus preserving the exploratory character of the procedure; however, the assumptions might remain stringent enough to overcome the difﬁculties of ordinary PCA.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2012-02-17T12:23:24.463" Id="23568" LastActivityDate="2012-02-17T12:23:24.463" OwnerDisplayName="andreister" OwnerUserId="9013" ParentId="23566" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If the complexity of your model or classifier trained on those n features scales badly (e.g. the number of parameters grows as O(n^3)), then even 500 features can be a problem. Not only because the optimization takes longer, but also because you might not have enough data to constrain your parameters, which would lead to overfitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;By reducing model complexity, dimensionality reduction can therefore also act as a means of &lt;strong&gt;regularization&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-16T08:07:11.180" Id="23573" LastActivityDate="2012-02-16T08:07:11.180" OwnerDisplayName="Lucas" OwnerUserId="7733" ParentId="23569" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;If the kernel is radial, you can use &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/kernlab/html/sigest.html&quot; rel=&quot;nofollow&quot;&gt;this heuristic&lt;/a&gt; to get a proper $\sigma$ -- C optimisation is way easier then.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-24T11:09:33.113" Id="23603" LastActivityDate="2012-02-24T11:09:33.113" OwnerUserId="88" ParentId="23509" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="23608" AnswerCount="2" Body="&lt;p&gt;If I have a binomial maximum likelihood function&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ L(\theta | k) = \Pi_i^N p(\theta)^{k_i}(1-p(\theta))^{(n_i-k_i)}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the index terms can be made into a sum, simplifying the expression for calculation purposes. I have a model which predicts the probability of success as a function of a variable (x) and a parameter. I have measured k (knowing n) for a number of values of x and now I want to find the best fit value for my parameter which I am trying to determine experimentally. Am I right in thinking my likelihood function is in this case&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ L(\theta | x, k) = \Pi_i^N p(\theta, x_i)^{k_i}(1-p(\theta, x_i))^{(n_i-k_i)}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming this is the case (if not can someone correct me), is there anyway I can convert my product to a sum, or, do I have to keep it in product form?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-24T12:29:31.770" Id="23607" LastActivityDate="2013-12-02T19:46:36.267" LastEditDate="2012-02-24T13:13:15.603" LastEditorUserId="88" OwnerUserId="7734" PostTypeId="1" Score="0" Tags="&lt;binomial&gt;&lt;maximum-likelihood&gt;" Title="Binomial maximum likelihood expressed as a sum" ViewCount="253" />
  <row Body="&lt;p&gt;A lot of this comes down to what question you are actually asking, how you design your study, and even what you mean by equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;I ran accros an interesting little insert in the British Medical Journal once that talked about what people interpreted certain phases to mean.  It turns out that &quot;always&quot; can mean that something happens as low as 91% of the time (BMJ VOLUME 333 26 AUGUST 2006 page 445).  So maybe equal and equivalent (or within X% for some value of X) could be thought to mean the same thing.  And lets ask the computer a simple equality, using R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; (1e+5 + 1e-50) == (1e+5 - 1e-50)&#10;[1] TRUE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now a pure mathematician using infinite precision might say that those 2 values are not equal, but R says they are and for most practical cases they would be (If you offered to give me $\$$(1e+5 + 1e-50), but the amount ended up being $\$$(1e+5 - 1e-50) I would not refuse the money because it differed from what was promised).&lt;/p&gt;&#10;&#10;&lt;p&gt;Further if our alternative hypothesis is $H_a: \mu &amp;gt; \mu_0$ we often write the null as $H_0: \mu=\mu_0$ even though technically the real null is $H_0: \mu \le \mu_0$, but we work with the equality as null since if we can show that $\mu$ is bigger than $\mu_0$ then we also know that it is bigger than all the values less than $\mu_0$.  And isn't a two-tailed test really just 2 one-tailed tests? After all, would you really say that $\mu \ne \mu_0$ but refuse to say which side of $\mu_0$ $\mu$ is on?  This is partly why there is a trend towards using confidence intervals in place of p-values when possible, if my confidence interval for $\mu$ includes $\mu_0$ then while I may not be willing to believe that $\mu$ is exactly equal to $\mu_0$, I cannot say for certain which side of $\mu_0$ $\mu$ lies on, which means they might as well be equal for practical purposes.&lt;/p&gt;&#10;&#10;&lt;p&gt;A lot of this comes down to asking the right question and designing the right study for that question.  If you end up with enough data to show that a practically meaningless difference is statistically significant, then you have wasted resources getting that much data.  It would have been better to decide what a meaningful difference would be and designed the study to give you enough power to detect that difference but not smaller.&lt;/p&gt;&#10;&#10;&lt;p&gt;And if we really want to split hairs, how do we define what parts of the lamb are on the right and which are on the left?  If we define it by a line that by definition has equal number of hairs on each side then the answer to the above question becomes &quot;Of Course it is&quot;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-24T18:25:38.327" Id="23622" LastActivityDate="2012-02-24T18:33:35.127" LastEditDate="2012-02-24T18:33:35.127" LastEditorUserId="919" OwnerUserId="4505" ParentId="23617" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;From an organisational perspective, be it government with policy options or a company looking to roll out a new process/product, the use of a simple cost-benefit analysis can help too. I have argued in the past that (ignoring political reasons) given the known cost of a new initiative, what is the break even point for numbers of people who must be affected positively by that initiative? For example, if the new initiative is to get more unemployed people into work, and the initiative costs &lt;code&gt;$100,000&lt;/code&gt;, does it achieve a reduction in unemployment transfers of at least &lt;code&gt;$100,000&lt;/code&gt;? If not, then the effect of the initiative is not practically significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;For health outcomes, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Value_of_life#Treatment_in_Economics_-_The_Value_of_a_Statistical_Life_.28VSL.29&quot; rel=&quot;nofollow&quot;&gt;value of a statistical life&lt;/a&gt; takes on importance. This is because health benefits are accrued over a lifetime (and therefore the benefits are adjusted downwards in value based on a &lt;a href=&quot;http://en.wikipedia.org/wiki/Discount_rate&quot; rel=&quot;nofollow&quot;&gt;discount rate&lt;/a&gt;). So then instead of statistical significance, one gets arguments over how to estimate the value of a statistical life, and what discount rate should apply.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-24T18:40:03.390" Id="23625" LastActivityDate="2012-02-24T18:50:10.360" LastEditDate="2012-02-24T18:50:10.360" LastEditorUserId="8605" OwnerUserId="8605" ParentId="23617" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;As far as significance testing goes (or anything else that does &lt;em&gt;essentially the same thing&lt;/em&gt; as significance testing), I have long thought that the best approach in most situations is likely to be estimating a standardized effect size, with a 95% confidence interval about that effect size.  There's nothing really new there--mathematically you can shuffle back and forth between them--if the p-value for a 'nil' null is &amp;lt;.05, then 0 will lie outside of a 95% CI, and vise versa.  The advantage of this, in my opinion, is &lt;em&gt;psychological&lt;/em&gt;; that is, it makes salient information that exists but that people can't see when only p-values are reported.  For example, it is easy to see that an effect is wildly 'significant', but ridiculously small; or 'non-significant', but only because the error bars are huge whereas the estimated effect is more or less what you expected.  These can be paired with raw values and their CI's.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in many fields the raw values are intrinsically meaningful, and I recognize that raises the question of whether it's still worthwhile to compute effect size measures given that we already have values like means and slopes.  An example might be looking at stunted growth; we know what it means for a 20 year old, white male to be 6 +/- 2 inches shorter (i.e. 15 +/- 5 cm), than they would otherwise, so why mention $d=-1.6\pm.5$?  I tend to think that there can still be value in reporting both, and functions can be written to compute these so that it's very little extra work, but I recognize that opinions will vary.  At any rate, I argue that point estimates with confidence intervals replace p-values as the first part of my response.  &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, I think a bigger question is 'is the thing that significance testing does what we really want?'  I think the real problem is that for most people analyzing data (i.e., practitioners not statisticians), significance testing can become the entirety of data analysis.  It seems to me that the most important thing is to have a principled way to think about what is going on with our data, and null hypothesis significance testing is, at best, a very small part of that.  Let me give an imaginary example (I acknowledge that this is a caricature, but unfortunately, I fear it is somewhat plausible):  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Bob conducts a study, gathering data on something-or-other.  He&#10;  expects the data will be normally distributed, clustering tightly&#10;  around some value, and intends to conduct a one-sample t-test to see&#10;  if his data are 'significantly different' from some pre-specified&#10;  value.  After collecting his sample, he checks to see if his data are&#10;  normally distributed, and finds that they are not.  Instead, they do&#10;  not have a pronounced lump in the center but are relatively high over a given&#10;  interval and then trail off with a long left tail.  Bob worries about&#10;  what he should do to ensure that his test is valid.  He ends up doing&#10;  something (e.g., a transformation, a non-parametric test, etc.), and&#10;  then reports a test statistic and a p-value.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I hope this doesn't come off as nasty.  I don't mean to mock anyone, but I think something like this does happen occasionally.  Should this scenario occur, we can all agree that it is poor data analysis.  However, the problem isn't that the test statistic or the p-value is wrong; we can posit that the data were handled properly &lt;em&gt;in that respect&lt;/em&gt;.  I would argue that the problem is Bob is engaged in what Cleveland called &quot;rote data analysis&quot;.  He appears to believe that the only point is to get the right p-value, and thinks very little about his data outside of pursuing that goal.  He even could have switched over to my suggestion above and reported a standardized effect size with a 95% confidence interval, and it wouldn't have changed what I see as the larger problem (this is what I meant by doing &quot;essentially the same thing&quot; by a different means).  In this specific case, the fact that the data didn't look the way he expected (i.e., weren't normal) is real information, it's &lt;em&gt;interesting&lt;/em&gt;, and very possibly important, but that information is essentially just thrown away.  Bob doesn't recognize this, because of the focus on significance testing.  To my mind, that is the &lt;em&gt;real&lt;/em&gt; problem with significance testing.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let me address a few other perspectives that have been mentioned, and I want to be very clear that I am not criticizing anyone.  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;It is often mentioned that many people don't really understand&#10;p-values (e.g., thinking they're the probability the null is&#10;true), etc.  It is sometimes argued that, if only people would use&#10;the Bayesian approach, these problems would go away.  I believe that people&#10;can approach Bayesian data analysis in a manner that is just as&#10;incurious and mechanical.  However, I think that misunderstanding the meaning of p-values would be less harmful if no one thought getting a p-value was the goal.  &lt;/li&gt;&#10;&lt;li&gt;The existence of 'big data' is generally unrelated to this issue.  Big data only make it obvious that organizing data analysis around 'significance' is not a helpful approach.  &lt;/li&gt;&#10;&lt;li&gt;I do not believe the problem is with the hypothesis being tested.  If people only wanted to see if the estimated value is outside of an interval, rather than if it's equal to a point value, many of the same issues could arise.  (Again, I want to be clear &lt;em&gt;I know you are not 'Bob'&lt;/em&gt;.)  &lt;/li&gt;&#10;&lt;li&gt;For the record, I want to mention that my own suggestion from the first paragraph, does &lt;em&gt;not&lt;/em&gt; address the issue, as I tried to point out.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For me, this is the core issue:  What we really want is &lt;em&gt;a principled way to think about what happened&lt;/em&gt;.  What that means in any given situation is not cut and dried.  How to impart that to students in a methods class is neither clear nor easy.  Significance testing has a lot of inertia and tradition behind it.  In a stats class, it's clear what needs to be taught and how.  For students and practitioners it becomes possible to develop a conceptual schema for understanding the material, and a checklist / flowchart (I've seen some!) for conducting analysis.  Significance testing can naturally evolve into rote data analysis without anyone being dumb or lazy or bad.  &lt;em&gt;That&lt;/em&gt; is the problem.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-24T20:34:34.893" Id="23631" LastActivityDate="2012-02-24T20:34:34.893" OwnerUserId="7290" ParentId="23617" PostTypeId="2" Score="19" />
  <row Body="&lt;p&gt;You can use the &lt;code&gt;qqplot&lt;/code&gt; function like @Peter Flom suggests as one way to compare visually.  Another visual approach is to create a set of graphs where one is the real data and the others are simulated data, but you don't know which is the real data (or a sample of the real data), then see if you can pick out the real data.  The &lt;code&gt;vis.test&lt;/code&gt; function in the &lt;code&gt;TeachingDemos&lt;/code&gt; package for R helps with this.  This gives more of a close enough test than an exact match test.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;ks.test&lt;/code&gt; function in R can take 2 vectors of numbers and tests the null hypothesis that they come from the same distribution.  Note however that with large sample sizes this test can find significance for differences so small that most would not care about and for small sample sizes it may not have the power to differentiate between distributions that we would care about the difference of.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-24T21:44:45.567" Id="23639" LastActivityDate="2012-02-24T21:44:45.567" OwnerUserId="4505" ParentId="23634" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;Does rpart function require normalized data when using the &quot;anova&quot; method? Im assuming so, but I do not like to assume. I have looked and no where has it said it needs to be, but the method makes me think it does. Thanks &lt;/p&gt;&#10;&#10;&lt;p&gt;Scott&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-25T05:32:51.827" Id="23647" LastActivityDate="2012-02-25T07:14:25.643" OwnerUserId="9423" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="r :Does rpart require normalized data set for the input variable?" ViewCount="230" />
&#10;\end{array}$$&#10;showing that autocorrelation actually depends on $i$ as opposed to $j-i$ (where $\phi(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ is the pdf of the Gaussian distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know where to go from here?  Are there closed-form expressions for $\{\hat{X}_i\}_{i=1}^n$ from $\{Y_i\}_{i=1}^n$ as well as the squared error $\sum_{i=1}^n(X_i-\hat{X}_i)^2$ given $\sigma^2$, $N_0$, and $n$? This seems to me like a problem that someone thought about before.  Unfortunately, I am not well-versed in filtering/signal-processing -- perhaps there is a version of a Wiener filter that could work.  I would appreciate any guidance in figuring this out.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: This may be soluble using the first principles from which the Wiener filter is derived, i.e. via the orthogonality principle and using the expectation operator as a dot-product.  I'm fearful that this might get me stuck with a nasty integral equation, requiring the use of transforms (which I am not too familiar with to begin with) but I'll try it in the morning.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-25T06:14:44.310" Id="23648" LastActivityDate="2012-02-25T07:48:49.700" LastEditDate="2012-02-25T07:48:49.700" LastEditorUserId="6946" OwnerUserId="6946" PostTypeId="1" Score="2" Tags="&lt;estimation&gt;&lt;normal-distribution&gt;&lt;gaussian-process&gt;" Title="Estimating a 1-D Brownian motion process using noisy observations" ViewCount="106" />
  <row AcceptedAnswerId="23667" AnswerCount="1" Body="&lt;p&gt;I'm hoping someone can explain this bit of R code for me related to &lt;code&gt;glm()&lt;/code&gt;. I don't understand the diagnostic plot that has been suggested. It seems a more informative plot would be to plot against the fitted values, but maybe I don't understand something.&#10;Here's the code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;result &amp;lt;- glm(survive~age, data=donner, family=binomial)&#10;# Why is this plotted against the respondent index?&#10;plot(residuals(result,type=&quot;pearson&quot;), main=&quot;pearson residual plot&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Data to reproduce the above example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dput(donner)&#10;structure(list(age = c(23L, 40L, 40L, 30L, 28L, 40L, 45L, 62L, &#10;65L, 45L, 25L, 28L, 28L, 23L, 22L, 23L, 28L, 15L, 47L, 57L, 20L, &#10;18L, 25L, 60L, 25L, 20L, 32L, 32L, 24L, 30L, 15L, 50L, 21L, 25L, &#10;46L, 32L, 30L, 25L, 25L, 25L, 30L, 35L, 23L, 24L, 25L), sex = c(1L, &#10;0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, &#10;0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, &#10;1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L), survive = c(0L, &#10;1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, &#10;1L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, &#10;0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L)), .Names = c(&quot;age&quot;, &#10;&quot;sex&quot;, &quot;survive&quot;), class = &quot;data.frame&quot;, row.names = c(NA, -45L&#10;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-02-25T19:59:03.453" Id="23666" LastActivityDate="2012-02-26T08:00:09.313" LastEditDate="2012-02-26T08:00:09.313" LastEditorDisplayName="user5644" OwnerUserId="9429" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;self-study&gt;&lt;generalized-linear-model&gt;&lt;logistic&gt;&lt;diagnostic&gt;" Title="Explanation of R diagnostic plot for logistic regression" ViewCount="1845" />
&#10;\theta)$$ for parameter vector $\theta$, and data points $Z_{i}$, where $\mathcal{H}(Z_{i},\theta)$ denotes the Hessian matrix of the log-likelihood function, evaluated at the single datum $Z_{i}$ and the parameter vector estimate $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then the matrix given by $I(\theta)^{-1}$ contains the variance/covariance structure of the MLE estimates, and its diagonal entries in particular are the variances of the estimated MLE parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my specific application, the $Z_{i}$ are consumer data about price preferences for different heating options, and the model is a logit probability model. I've done all the work of estimating the MLE and there are standard formulas for the derivative vector and Hessian matrix in this logit scenario. I have about 250 data samples, which is large enough that I expect the MLE estimate to be fairly accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I numerically compute the information matrix and the corresponding variances, I am seeing very large numbers. The estimated coefficients range in absolute value from 0.5 to 22, but the smallest standard error is about 7.2, which makes me question what I'm doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the empirical estimate of the Hessians are correct, as I've used my code on a separate data set to confirm that I'm computing the derivatives and second-derivatives correctly, so I think it's unlikely to be a coding mistake.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have any advice on what might yield such large standard deviations in practice?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-25T21:13:38.053" Id="23668" LastActivityDate="2012-02-25T21:13:38.053" OwnerUserId="8927" PostTypeId="1" Score="1" Tags="&lt;maximum-likelihood&gt;&lt;standard-error&gt;&lt;logit&gt;&lt;information&gt;" Title="What can be going wrong when Maximum Likelihood standard errors are high?" ViewCount="661" />
  <row Body="&lt;p&gt;A very common solution for this very common problem (ie, over-weighting variables) is to &lt;strong&gt;&lt;em&gt;standardize&lt;/em&gt;&lt;/strong&gt; your data. &lt;/p&gt;&#10;&#10;&lt;p&gt;To do this, you just perform two successive &lt;em&gt;column-wise&lt;/em&gt; operations on your data:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;subtract the &lt;strong&gt;mean&lt;/em&gt;&lt;/strong&gt; and&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;divide by the &lt;strong&gt;standard deviation&lt;/em&gt;&lt;/strong&gt;&#10;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For instance, in &lt;em&gt;NumPy&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; # first create a small data matrix comprised of three variables &#10;&amp;gt;&amp;gt;&amp;gt; # having three different 'scales' (means and variances)&#10;&#10;&amp;gt;&amp;gt;&amp;gt; a = 10*NP.random.rand(6)&#10;&amp;gt;&amp;gt;&amp;gt; b = 50*NP.random.rand(6)&#10;&amp;gt;&amp;gt;&amp;gt; c = 2*NP.random.rand(6)&#10;&amp;gt;&amp;gt;&amp;gt; A = NP.column_stack((a, b, c))&#10;&amp;gt;&amp;gt;&amp;gt; A   # the pre-standardized data&#10;    array([[  1.753,  37.809,   1.181],&#10;           [  1.386,   8.333,   0.235],&#10;           [  2.827,  40.5  ,   0.625],&#10;           [  5.516,  47.202,   0.183],&#10;           [  0.599,  27.017,   1.054],&#10;           [  8.918,  35.398,   1.602]])&#10;&#10;&amp;gt;&amp;gt;&amp;gt; # mean center the data (columnwise)&#10;&amp;gt;&amp;gt;&amp;gt; A -= NP.mean(A, axis=0)&#10;&amp;gt;&amp;gt;&amp;gt; A&#10;    array([[ -1.747,   5.099,   0.368],&#10;           [ -2.114, -24.377,  -0.578],&#10;           [ -0.673,   7.79 ,  -0.189],&#10;           [  2.016,  14.493,  -0.631],&#10;           [ -2.901,  -5.693,   0.24 ],&#10;           [  5.418,   2.688,   0.789]])&#10;&#10;&amp;gt;&amp;gt;&amp;gt; # divide by the standard deviation&#10;&amp;gt;&amp;gt;&amp;gt; A /= NP.std(A, axis=0)&#10;&amp;gt;&amp;gt;&amp;gt; A&#10;    array([[-0.606,  0.409,  0.716],&#10;           [-0.734, -1.957, -1.125],&#10;           [-0.233,  0.626, -0.367],&#10;           [ 0.7  ,  1.164, -1.228],&#10;           [-1.007, -0.457,  0.468],&#10;           [ 1.881,  0.216,  1.536]])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-02-26T10:53:05.493" Id="23683" LastActivityDate="2012-02-26T19:51:22.660" LastEditDate="2012-02-26T19:51:22.660" LastEditorUserId="438" OwnerUserId="438" ParentId="23680" PostTypeId="2" Score="3" />
&#10;\textrm{cat 2}  &amp;amp; O_{21}          &amp;amp; O_{22}          &amp;amp; O_{2.}\\
&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;there are $O_{11}$ observations that fall within the first category of $X$ $\underline{\textrm{and}}$ within the first category of $Y$;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are $0_{2.}$ observations that fall within the second category of $X$;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the total sample size is $O_{..}$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1st step&lt;/strong&gt;: Compute the &lt;em&gt;expected&lt;/em&gt; contingency table&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{cc}
&#10;\hline
  
  <row AnswerCount="1" Body="&lt;p&gt;Prove that FGLS  is asymptotically efficient. Does one have to use Cramer Rao to do this?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-26T21:34:57.163" Id="23703" LastActivityDate="2012-02-27T00:20:24.993" LastEditDate="2012-02-26T22:50:26.947" LastEditorUserId="9448" OwnerUserId="9448" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;asymptotics&gt;&lt;proof&gt;&lt;gls&gt;" Title="proof FGLS asymptotically efficient" ViewCount="368" />
  
  <row AnswerCount="1" Body="&lt;p&gt;There seem to be several options available for working with Gaussian Mixture Models (GMMs) in Python. At first glance there are at least: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;PyMix - &lt;a href=&quot;http://www.pymix.org/pymix/index.php&quot;&gt;http://www.pymix.org/pymix/index.php&lt;/a&gt; Tools for mixture modeling&lt;/li&gt;&#10;&lt;li&gt;PyEM - &lt;a href=&quot;http://www.ar.media.kyoto-u.ac.jp/members/david/softwares/em/&quot;&gt;http://www.ar.media.kyoto-u.ac.jp/members/david/softwares/em/&lt;/a&gt; which is part of the Scipy toolbox and seems to focus on GMMs &lt;strong&gt;Update: Now known as &lt;a href=&quot;http://scikit-learn.github.com/scikit-learn.org/dev/modules/mixture.html&quot;&gt;sklearn.mixture&lt;/a&gt;&lt;/strong&gt;&#10;.&lt;/li&gt;&#10;&lt;li&gt;PyPR - &lt;a href=&quot;http://pypr.sourceforge.net/&quot;&gt;http://pypr.sourceforge.net/&lt;/a&gt; pattern recognition and related tools including GMMs&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;... and perhaps others. They all seem to provide the most basic needs for GMMs, including creating and sampling, parameter estimation, clustering, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;What's the difference between them, and how should one go about determining which is best suited for a particular need? &lt;/p&gt;&#10;&#10;&lt;p&gt;Ref: &lt;a href=&quot;http://www.scipy.org/Topical_Software&quot;&gt;http://www.scipy.org/Topical_Software&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-27T03:12:01.750" FavoriteCount="2" Id="23713" LastActivityDate="2013-04-05T22:55:43.357" LastEditDate="2012-03-11T09:59:37.250" LastEditorUserId="88" OwnerUserId="5387" PostTypeId="1" Score="6" Tags="&lt;normal-distribution&gt;&lt;python&gt;&lt;mixture&gt;" Title="Python packages for working with Gaussian mixture models (GMMs)" ViewCount="3590" />
  
  
  <row Body="&lt;p&gt;I would personally favor cross-validated score evaluation because:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;it is easily interpretable by the analyst provided that the underlying score function (accuracy, f1-score, RMSE...) is interpretable too,&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;it gives an idea of the uncertainty by looking at the stdev of the score values across CV folds,&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;it gives a way to decompose the error into bias (error measured on train folds) and variance (difference of errors measured on train and  test folds).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Model size is not a factor with the computing power &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is not always true: deep learning machine learning models for instance have a model size that is often limited by the hardware (typically the amount of RAM on the GPU card).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-27T09:03:17.943" Id="23732" LastActivityDate="2012-02-27T09:03:17.943" OwnerUserId="2150" ParentId="23729" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;I recommend calculating AIC model weights for each of these. This gives you the flexibility of being able to choose the AIC-best model for inference, or if there is considerable structural uncertainty, to obtain a model-weighted estimate of the effect of x2. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-02-27T16:19:55.727" Id="23752" LastActivityDate="2012-02-27T16:19:55.727" OwnerUserId="6829" ParentId="23750" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I presume the z_k are not probabilities but sample frequencies. This is because, otherwise, Q_i(z_0, ..., z_i) is not a random variable. In that case, computing the variance of the Q_i's is straightforward algebra. Define, first, the event indicators Z_i which is 1 if Z == i, 0 otherwise. It's a Bernoulli random variable with probability p_i. You can compute the first and second moments of any of these variables and they should give you all the necessary terms for computing the variance of the Q_i's.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-27T22:54:19.830" Id="23769" LastActivityDate="2012-02-27T22:54:19.830" OwnerUserId="8013" ParentId="23761" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="23776" AnswerCount="1" Body="&lt;p&gt;I ran &lt;a href=&quot;http://www.maizegenetics.net/tassel/&quot; rel=&quot;nofollow&quot;&gt;Tassel3&lt;/a&gt; and I filtered results with p-value &#10;not more than 0.05. This way, it is ok to draw a Manhattan plot. However, for &#10;a QQ-plot there is a problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;Say I have 40 thousands SNPs, after filtering, about 1,800 SNPs were &#10;kept in the output file. Now how do I draw QQ-plot because of partial p-values? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;ppoints&lt;/code&gt; only generate points within (0,1). I think the resulting points &#10;should be in the filtered range (0,0.05) with the same number (1,800) too. &#10;&lt;code&gt;runif&lt;/code&gt; can do the trick, but it is random, each time it generates different &#10;results. I think this would affect the QQ-plot line.&lt;/p&gt;&#10;&#10;&lt;p&gt;I figured out three ways to solve it but I am not sure if it is right. &#10;Hope you guys can give some advices.&lt;/p&gt;&#10;&#10;&lt;p&gt;the following are three methods:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;code&gt;0.05*ppoint(1800)&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;ppoints(1800*100/0.05, a=0.05)&lt;/code&gt;, then I only get the former 1/20  results.&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;ppoints(40k)&lt;/code&gt;, and use those p-values below 0.05 in plotting.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-02-28T01:43:12.157" Id="23773" LastActivityDate="2014-11-20T14:18:25.170" LastEditDate="2014-11-20T14:18:25.170" LastEditorUserId="805" OwnerUserId="9470" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;uniform&gt;&lt;qq-plot&gt;" Title="How to use ppoints to generate points within 0 and 0.05 for qq plotting in R?" ViewCount="331" />
  <row Body="&lt;p&gt;You are right not to use random points. Version 3 will provide what you want. But as &lt;code&gt;ppoints(40k)&lt;/code&gt; is just producing 40k equally-spaced values between 0 and 1, it's not too difficult to directly code what you want. To bypass the use of &lt;code&gt;ppoints()&lt;/code&gt; altogether, use e.g. &lt;code&gt;(-0.5+1:1800)/40000&lt;/code&gt;; this gives the expected position (under the null) of the 1800 smallest p-values, from a sample of 40000 p-values.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-28T02:42:50.397" Id="23776" LastActivityDate="2012-02-28T02:42:50.397" OwnerUserId="7497" ParentId="23773" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="23964" AnswerCount="2" Body="&lt;p&gt;How are (linear) mixed effects models normally compared against each other? I know likelihood ratio tests can be used, but this doesn't work if one model is not a 'subset' of the other correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is the estimation of the models df always straightforward? Number of fixed effects + number of variance components estimated? Do we ignore the random effects estimates?&lt;/p&gt;&#10;&#10;&lt;p&gt;What about validation? My first thought is cross validation, but random folds might not work given the structure of the data. Is a methodology of 'leave one subject/cluster out' appropriate? What about leave one observation out?&lt;/p&gt;&#10;&#10;&lt;p&gt;Mallows Cp can be interpreted as an estimate of the models prediction error. Model selection via AIC attempts to minimize the prediction error (So Cp and AIC should pick the same model if the errors are Gaussian I believe). Does this mean AIC or Cp can be used to pick an 'optimal' linear mixed effects model from a collection of some un-nested models in terms of prediction error? (provided they are fit on the same data) Is BIC still more likely to pick the 'true' model amongst candidates?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am also under the impression that when comparing mixed effects models via AIC or BIC we only count the fixed effects as 'parameters' in the calculation, not the actual models df.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any good literature on these topics? Is it worth investigating cAIC or mAIC? Do they have specific application outside of AIC?&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-28T03:33:59.977" FavoriteCount="9" Id="23778" LastActivityDate="2012-03-01T22:08:56.873" LastEditDate="2012-03-01T11:35:51.727" LastEditorUserId="845" OwnerUserId="845" PostTypeId="1" Score="15" Tags="&lt;hypothesis-testing&gt;&lt;mixed-model&gt;&lt;cross-validation&gt;&lt;aic&gt;&lt;degrees-of-freedom&gt;" Title="How should mixed effects models be compared and or validated?" ViewCount="4366" />
  
&#10;     $ V3 : Factor w/ 1 level &quot;m01xrfn2 Effective resolution&quot;: 1&#10;     $ V4 : Factor w/ 1 level &amp;quot;5.1&amp;quot;: 1
&#10;     $ V11: Factor w/ 1 level &quot;nextag&quot;: 1&#10;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-02-28T10:43:33.613" FavoriteCount="1" Id="23793" LastActivityDate="2012-02-29T13:50:55.690" LastEditDate="2012-02-29T13:50:55.690" LastEditorUserId="264" OwnerUserId="9485" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;predictive-models&gt;" Title="Why does GBM predict different values for the same data" ViewCount="2018" />
  
  
  
  
  <row AcceptedAnswerId="23823" AnswerCount="2" Body="&lt;p&gt;I am looking for a good software system to teach students on response surface designs with. Few things I can think of are SAS Proc RSReg or SAS ADX. Is there any other software?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-28T16:27:45.300" Id="23811" LastActivityDate="2012-02-28T21:27:06.470" LastEditDate="2012-02-28T21:27:06.470" LastEditorUserId="930" OwnerUserId="5597" PostTypeId="1" Score="2" Tags="&lt;experiment-design&gt;&lt;software&gt;&lt;teaching&gt;" Title="Good statistical software system for teaching response surface design?" ViewCount="402" />
  <row Body="&lt;p&gt;One practical approach (in case of supervised learning at least) is to include all possibly relevant features and use a (generalized) linear model (logistic regression, linear svm etc.) with regularization (L1 and/or L2). There are open source tools (e.g. Vowpal Wabbit) that can deal with trillions of example/feature combinations for these types of models so scalability is not an issue (besides, one can always use sub-sampling). The regularization helps to deal with feature selection.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-28T18:20:54.937" Id="23824" LastActivityDate="2012-02-28T21:27:48.160" LastEditDate="2012-02-28T21:27:48.160" LastEditorUserId="6129" OwnerUserId="6129" ParentId="23426" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I used MATLAB neural network toolbox to train a self-organizing map for a given data set. The obtained &quot;weight-position&quot; plot is given as follows. I do not think this plot looks good  in comparison to the sample plot given in the &lt;a href=&quot;http://www.mathworks.com/help/toolbox/nnet/ref/plotsompos.html&quot; rel=&quot;nofollow&quot;&gt;MathWorks website&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AlZEr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I followed the standard MATLAB routine to train this SOM. Thus, I am having three questions on this case study.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Which kind of information can I obtain from this plot?&lt;/li&gt;&#10;&lt;li&gt;Given the shape of this plot, what can I say for the input?&lt;/li&gt;&#10;&lt;li&gt;Since the training method is trivial and standard, I am wondering whether SOM can fit to my data set? My data set is composed of 3,000 data points, and each data points has 20,000 dimensions. The example given by MATLAB is only for 4 dimensions.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-02-28T19:26:09.560" Id="23826" LastActivityDate="2012-02-28T21:48:32.287" LastEditDate="2012-02-28T21:48:32.287" LastEditorUserId="930" OwnerUserId="3269" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;matlab&gt;&lt;neural-networks&gt;&lt;self-organizing-maps&gt;" Title="How to interpret &quot;weight-position&quot; plot when using self-organizing map for clustering?" ViewCount="640" />
  <row Body="&lt;p&gt;None of the answers are entirely baseless. But they ALL assume significant external knowledge and can't be taken to be correct strictly on the basis of the statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;A, B, D, and E all require assumptions about the factors the cause patients to choose one hospital over another; the process by which doctors and patients are matched up, the extent to which success rates are attributable to specific classes of operations vs. shared factors like ICU, and on an on.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In the real world we could legitimately consider many alternate factors such as the payment providers the hospital officially accepts, the socioeconomics and obesity rates of the neighborhood, whether this is a teaching hospital (in which case the success rate plummets when new interns arrive and we have to consider monthly mix), and on and on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously we can and do make reasonable assumptions about these factors, but without specifically addressing or excluding them from the problem, it's impossible to say if an answer is &quot;right&quot; or not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-28T23:04:40.737" Id="23837" LastActivityDate="2012-02-28T23:04:40.737" OwnerUserId="3331" ParentId="21896" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;How do I compute a confidence level for a forced ranking survey?  For example, say I have 10 categories, each with four associated statements that must be ranked either 1, 2, 3 or 4 with no duplicate ranking within the category (forced ranking).  Say I get fifty responses.  How do I compute the confidence level/margin of error? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-02-28T23:41:27.320" Id="23838" LastActivityDate="2012-02-29T08:39:27.307" LastEditDate="2012-02-29T08:39:27.307" LastEditorUserId="88" OwnerUserId="9501" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;sample-size&gt;&lt;ranking&gt;" Title="How do I compute a confidence level for a forced ranking survey?" ViewCount="324" />
  <row Body="&lt;p&gt;Just a brief point...&#10;When I read a stat book, I like when the data sets are made available so that the examples are reproducible. &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-02-29T08:43:18.220" CreationDate="2012-02-29T06:21:15.180" Id="23844" LastActivityDate="2012-02-29T06:21:15.180" OwnerUserId="3019" ParentId="23841" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="24201" AnswerCount="3" Body="&lt;p&gt;I am trying to forecast the sales of products in vending machine. The problem is that the machine is filled at irregular intervals and at every fill we only can record the aggregated sales since the last fill of the machine (i.e. we don't have daily sales data). So basically we have data for aggregated sales at irregular intervals. The intervals usually are between 2 days and 3 weeks. Here is example data for one  vending machine and one product:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;27/02/2012 48&#10;17/02/2012 24&#10;09/02/2012 16&#10;02/02/2012 7&#10;25/01/2012 12&#10;16/01/2012 16&#10;05/01/2012 16&#10;23/12/2011 4&#10;16/12/2011 14&#10;09/12/2011 4&#10;02/12/2011 2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Our current naive algorithm is to calculate average sales per day by dividing the total quantity sold during the last 90 days by 90.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have any idea how to improve the forecast of the sales per day? I need to forecast what will be sold at the next visit of the machine. Is it possible to use some sort of exponential smoothing algorithm given the nature of our data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: Thanks a lot for all the answers and comments. Let me try to give a bit more context (the business case behind the question - very simplified of course). We have hundreds of vending machines. Every day we have to decide which 20 of them to visit for refill. In order to do so we are trying to predict what is the current status of the machines and to select the &quot;emptiest&quot; 20 machines. For each machine and product we are calculating the average sales per day (SPD) using the naive algorithm described above. Then we multiply the SPD by the number of days since the last fill of the machine and the result is the predicted quantity sold.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-29T10:08:16.663" FavoriteCount="3" Id="23860" LastActivityDate="2012-03-06T18:26:07.247" LastEditDate="2012-03-06T16:12:39.020" LastEditorUserId="9506" OwnerUserId="9506" PostTypeId="1" Score="7" Tags="&lt;time-series&gt;&lt;forecasting&gt;" Title="How to forecast based on aggregated data over irregular intervals?" ViewCount="2177" />
  <row Body="&lt;p&gt;I found an answer to my question on this thread: &lt;a href=&quot;http://stats.stackexchange.com/q/13784/442&quot;&gt;Repeated measures ANOVA with lme in R for two within-subject factors&lt;/a&gt; (somehow this thread was already one of my favorites, I must have forgotten about it). The specification is a little unhandy.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m6 &amp;lt;- lme(mean ~ condition*group*problem*topic, &#10;   random = list(code=pdBlocked(list(~1, pdIdent(~problem-1), pdIdent(~topic-1)))), data = d)&#10;anova(m6)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, the denominator dfs are still wrong, as noted in the &lt;a href=&quot;http://stats.stackexchange.com/q/13784/442&quot;&gt;thread&lt;/a&gt; and apparent in comparisons between the ANOVA and &lt;code&gt;lme&lt;/code&gt; dfs.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data.frame(effect = rownames(anova(m6)), denDf= anova(m6)$denDF)&#10;&#10;m4$ANOVA[,c(&quot;Effect&quot;, &quot;DFd&quot;)]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As long as there are no other ideas, I think I will need to do the analysis in &lt;code&gt;lme4&lt;/code&gt;, for which I wil need to post another question.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-02-29T10:51:07.953" Id="23861" LastActivityDate="2012-02-29T10:51:07.953" OwnerUserId="442" ParentId="23833" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I'd suggest using a generalized linear model for binomial data, i.e. &lt;em&gt;grouped&lt;/em&gt; binary data, with the score 0-8 as the outcome and 8 as the binomial denominator.&lt;/p&gt;&#10;&#10;&lt;p&gt;As the 8 trials for the same participant aren't independent it's very possible you'll have &lt;a href=&quot;http://en.wikipedia.org/wiki/Overdispersion&quot; rel=&quot;nofollow&quot;&gt;over- (or under-) dispersion&lt;/a&gt;. The simplest way to deal with that within a GLM is to estimate the scale parameter from the data rather than fixing it at the theoretical value.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-02-29T12:56:49.713" Id="23866" LastActivityDate="2012-02-29T12:56:49.713" OwnerUserId="449" ParentId="23839" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;:Bruder the Box-Jenknins approach incorporates all well-known forecasting models except multiplicative models like the Holt-Winston Multiplicative Seasonal Model where the expected value is based upon a multiplicand . The multiplicative seasonal model can be used for to model time series where one has the following ( in my opinion a very unusual) case . If the amplitude of the seasonal component/pattern is proportional to the average level of the series, the series can be referred to as having multiplicative seasonality. Even in the case of multiplicative models, one can often represent these as ARIMA models &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_tffordet_sect014.htm&quot; rel=&quot;nofollow&quot;&gt;http://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_tffordet_sect014.htm&lt;/a&gt; thus completing the &quot;umbrella&quot;. Furtermore since a Transfer Function is a Generalized Least Squares Model it can reduce to a standard regression model by omitting the ARIMA component and assuming a set of weights needed to homogenize the error structure.  &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-02-29T13:22:56.327" Id="23867" LastActivityDate="2012-02-29T13:30:55.927" LastEditDate="2012-02-29T13:30:55.927" LastEditorUserId="3382" OwnerUserId="3382" ParentId="23864" PostTypeId="2" Score="4" />
&#10;$$&#10;then you already know, by simple inspection of the formula of the beta density, that $\theta\mid x\sim Beta(6,8)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can you see why?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a pattern that you should keep in your mind when solving similar problems.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-29T15:49:49.477" Id="23883" LastActivityDate="2012-02-29T16:25:13.513" LastEditDate="2012-02-29T16:25:13.513" LastEditorUserId="9394" OwnerUserId="9394" ParentId="23852" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Do you just report that MANOVA significance test is significant, or do you have to include contrast estimates (SPSS &lt;code&gt;K MATRIX&lt;/code&gt;)? In the latter case, what are recommended formatting guidelines?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-01T00:15:19.253" Id="23914" LastActivityDate="2012-04-19T13:07:41.363" LastEditDate="2012-03-08T11:32:21.633" LastEditorUserId="930" OwnerUserId="9525" PostTypeId="1" Score="3" Tags="&lt;manova&gt;&lt;contrasts&gt;" Title="How do you report significant contrast results for a MANOVA in APA style?" ViewCount="12342" />
  
  <row Body="&lt;p&gt;A reasonable and useful method, in this situation, is Beta regression. You can see my answer to &lt;a href=&quot;http://stats.stackexchange.com/questions/23921/why-are-my-p-values-so-high&quot;&gt;Why are my p-values so high?&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-01T08:56:04.233" Id="23926" LastActivityDate="2012-03-01T08:56:04.233" OwnerUserId="7251" ParentId="23908" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a list of items that represent cells, now i query each cell value to get a set of possible categories that this cell might belong to, these categories are weighted. For Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Microsoft is: {Software=83.543266, Video Game Platform=132.297455, Degree=71.624733, Organization=208.17038, Programming Language=66.998901, File Format=67.159828}&#10;&#10;apple is: {Physicist=50.008327, Organism Classification=145.239532, Video Game Platform=86.5653, Computer=60.457172, Organization=199.53299, Record label=53.477039}&#10;&#10;Banana is: {Musical Group=27.214769, Organism Classification=136.063583, Writer=27.566528, Musical instrument=29.285137, Organization=36.962833, TV Program=32.693085}&#10;&#10;Blackberry is: {Software=23.69421, Organism Classification=64.572891, Operating System=90.257011, City/Town/Village=16.836142, Computer=34.484653}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The weights that i get for each result are independent at cell level, so Microsoft having a very high confidence as an organization does not relate to the confidence score of Apple organization confidence. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now i have two problems, the first is to compare two different cells and compute a similarity score, for example how similar apple is to Microsoft having these possible categories for each.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, i want to deduce a global common category for the column that represents these cells, for example if i run a simple algorithm that will calculate the number of categories occurrences and the average of their confidences will not be accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will appreciate any help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-01T09:06:34.343" Id="23927" LastActivityDate="2013-10-30T13:44:50.950" LastEditDate="2012-08-06T08:51:41.350" LastEditorUserId="930" OwnerUserId="9532" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;classification&gt;&lt;algorithms&gt;&lt;ranking&gt;" Title="Ranking algorithm of different weighted resources" ViewCount="217" />
  <row Body="&lt;p&gt;There are no paradoxes in statistics, only puzzles waiting to be solved.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nevertheless, my favourite is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Two_envelopes_problem&quot;&gt;two envelope &quot;paradox&quot;&lt;/a&gt;. Suppose I put two envelopes in front of you and tell you that one contains twice as much money as the other (but not which is which). You reason as follows. Suppose the left envelope contains $x$, then with 50% probability the right envelope contains $2x$ and with 50% probability it contains $0.5x$, for an expected value of $1.25x$. But of course you can simply reverse the envelopes and conclude instead the left envelope contains $1.25$ times the value of the right envelope. What happened?&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2012-03-01T10:30:36.057" CreationDate="2012-03-01T10:30:36.057" Id="23929" LastActivityDate="2012-03-01T10:30:36.057" OwnerUserId="9533" ParentId="23779" PostTypeId="2" Score="20" />
  <row Body="&lt;p&gt;Yes, you need to separate the categories into 0/1 variables, omitting one of them. In R, this would be done with &lt;code&gt;as.factor(paymentmode)&lt;/code&gt;. In Stata, it is done with &lt;code&gt;i.paymentmode&lt;/code&gt; (which may have to be prefixed by &lt;code&gt;xi:&lt;/code&gt; in older versions). Some people believe in &lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter5/sasreg5.htm&quot; rel=&quot;nofollow&quot;&gt;different coding schemes&lt;/a&gt; for these categorical variables, but really it is just a matter of how you are going to read your output, and has no effect on estimation procedure itself.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-01T12:12:00.763" Id="23933" LastActivityDate="2012-03-01T12:12:00.763" OwnerUserId="5739" ParentId="23931" PostTypeId="2" Score="3" />
  
  
  
  
  <row Body="&lt;p&gt;The question asks for ways to &lt;em&gt;display&lt;/em&gt; bivariate discrete data using Excel.  Although that software is notoriously limited in graphical capabilities, it is still able to generate many different useful graphics.  Let's use the example to illustrate.  Here are the data, laid out as they might be in a spreadsheet:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0&#10;Y, 4, 4, 2, 4, 3, 4, 1, 4, 3, 3, 3, 3, 3, 3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Scatterplot&lt;/h3&gt;&#10;&#10;&lt;p&gt;By default, a good way to display bivariate data is with a scatterplot: distance along one coordinate corresponds to X, distance along the other coordinate to Y, and a standard symbol is drawn at the Cartesian location (X,Y) (one symbol for each (X,Y) pair).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with discrete data is that many points may coincide on the scatterplot.  For instance, the example data would show seven points stacked at the location (0,3).  A solution is to &lt;em&gt;jitter&lt;/em&gt; the points: move them randomly a little bit.  Do this by adding a small multiple of &lt;code&gt;RAND() - 1/2&lt;/code&gt; to the values.  Here, I chose 1/3 for the multiple because it disaggregates the stacked points but keeps them visually clustered near their correct locations:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9f9AD.png&quot; alt=&quot;Scatterplot, jittered&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Bubbleplot&lt;/h3&gt;&#10;&#10;&lt;p&gt;When jittering might confuse the audience, or when many points might be included in clusters, it may help to summarize the count of points at each location (X,Y) with a &quot;bubble plot.&quot;  This one is annotated with the count, because people usually do not compare areas of circles correctly:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jt4Py.png&quot; alt=&quot;Bubbleplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Producing this required creating a table of (X, Y, Count) triples: do this using &lt;code&gt;COUNTIF&lt;/code&gt;.  For this example, the X values were originally placed in cells A2:A15 and the Y values next to them in cells B2:B15.  To obtain the table, I computed&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;10*A2 + B2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;in cell C2 and copied it down to cell C15, creating a &quot;code&quot; column.  Its values are&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;14  14  2   14  3   4   1   14  3   3   3   3   3   3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;They could then be counted after setting up parallel columns of all possible X and Y values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X, Y&#10;0, 1&#10;0, 2&#10;...&#10;1, 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I put these into columns M and N.  To the right of each entry the count is computed with a formula like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;=COUNTIF($C$2:$C$15, 10*M2+N2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In effect, each entry is converted into a unique numeric code with &lt;code&gt;10*M2+N2&lt;/code&gt; and then that code is looked up in the &quot;Code&quot; column and counted.  (One begins to understand why databases and statistical software are usually used for such visualizations rather than spreadsheets.)&lt;/p&gt;&#10;&#10;&lt;h3&gt;Point plot&lt;/h3&gt;&#10;&#10;&lt;p&gt;We can &quot;slice&quot; the data by conditioning on one value, such as the binary X value.  For X=0 this produces a sequence of (Y, Count) pairs and for X=1 it produces another sequence of (Y, Count) pairs: use &lt;code&gt;COUNTIF&lt;/code&gt; to compute these, laying them out in a rectangular array like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;,  1, 2, 3, 4&#10;0, 1, 1, 7, 1&#10;1, 0, 0, 0, 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The rows provide the X values (labeled by the left column) and the columns provide the Y values (labeled by the top row).  The table entries are the counts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Parallel point plots display these values:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/LLoqP.png&quot; alt=&quot;Point plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note the need for a legend and to distinguish the values of X by means of visually different symbols (solid dark circles and hollow light squares here).&lt;/p&gt;&#10;&#10;&lt;p&gt;Point plots provide accurately readable displays of the counts: people can compare the heights quickly and easily.  The faint grid makes reading off the counts more reliable without intruding on the visual display.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Bar chart&lt;/h3&gt;&#10;&#10;&lt;p&gt;Bars can &quot;lie&quot; when they are based at arbitrary values, but when they are based at zero, their lengths accurately reflect the values they are intended to depict.  As such, we can use bars instead of points:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kyL3H.png&quot; alt=&quot;Bar chart&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In some applications care is needed to distinguish zeros (shown as the absence of a bar) from missing values (also shown as the absence of a bar).  That's not necessary in this example, because a missing count for an (X,Y) pair is not possible.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Pseudo 3D chart&lt;/h3&gt;&#10;&#10;&lt;p&gt;3D representations of data are difficult to interpret and read quantitatively, but with certain complex datasets they can sometimes be useful.  In this instance, we erect a 3D bar at each location (X,Y) whose pseudo-height is directly proportional to the count:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tX546.png&quot; alt=&quot;3D bar chart&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Quickly, now: does the tallest bar have a height of 6, 7, or 8?  Imagine trying to figure this out when there are many possible values of X and Y in the image.  Usually, such charts are reserved for annual reports and other slick, glossy materials intended for propaganda instead of presenting information.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-01T16:56:40.607" Id="23949" LastActivityDate="2012-03-01T16:56:40.607" OwnerUserId="919" ParentId="23945" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to be able to compare the angles of neighbours in a herd of hippos. I have data for the x and y coordinates and the angles that they are facing (using &lt;a href=&quot;http://rsbweb.nih.gov/ij/&quot; rel=&quot;nofollow&quot;&gt;imageJ&lt;/a&gt;, angles are between -180 and 180 with 0 being a horizontal straight line across the middle of the image). I have plotted the $g(r)$ function to see whether my hippos 'like' to orient themselves towards individuals that are nearby, but I would love to be able &lt;strong&gt;to compute a visual representation of their orientation in the herd&lt;/strong&gt; using &lt;code&gt;R&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be much appreciated, I'm not really &lt;code&gt;R&lt;/code&gt; literate yet!&lt;/p&gt;&#10;" ClosedDate="2013-11-14T07:50:25.727" CommentCount="4" CreationDate="2012-03-01T17:28:58.257" FavoriteCount="2" Id="23954" LastActivityDate="2013-11-14T01:38:00.543" LastEditDate="2013-11-14T01:38:00.543" LastEditorUserId="12786" OwnerUserId="9543" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;directional-statistics&gt;" Title="How can I produce a plot showing the directional angles of my points?" ViewCount="631" />
  
  <row AnswerCount="0" Body="&lt;p&gt;This question is in the context of time-delay estimation. Say I have a stationary Gaussian stochastic process $g$, and I know its autocorrelation function $R_g(\tau)$. To do time-delay estimation, I'm computing a windowed cross correlation between $g$ and a delayed version of it. In other words,&#10;$$
&#10;$$&#10;and I'm going to determine the delay by finding the maximum of $\phi$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, is it possible to get an expression for the probability distribution of $\phi$? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-01T19:53:11.717" Id="23962" LastActivityDate="2012-03-01T19:53:11.717" OwnerUserId="9544" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;stochastic-processes&gt;&lt;cross-correlation&gt;" Title="Probability distribution of windowed cross-correlation" ViewCount="135" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was running PLS regression on the data which is weighted and gives the following error message: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error: The dataset is weighted, but this procedure cannot be used with weights&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I do not understand the reason for this as earlier with weights on, there was an outcome.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-02T10:55:51.403" Id="23985" LastActivityDate="2012-03-11T21:25:28.383" LastEditDate="2012-03-11T21:25:28.383" LastEditorUserId="930" OwnerUserId="9079" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;pls&gt;" Title="PLS regression is not working on weighted data" ViewCount="107" />
&#10;   &amp;amp;= \prod_c{P(B^c|A^c)}
&#10;\end{align*}
  
  
  <row Body="&lt;p&gt;Total number of possible events = 2^5 = 32&lt;/p&gt;&#10;&#10;&lt;p&gt;Frequency of exactly 3 heads (HHHT*, THHHT, *THHH) = 2+1+2 = 5&lt;/p&gt;&#10;&#10;&lt;p&gt;Frequency of exactly four consecutive heads (HHHHT, THHHH) = 2&lt;/p&gt;&#10;&#10;&lt;p&gt;Frequency of five consecutive heads = 1&lt;/p&gt;&#10;&#10;&lt;p&gt;Frequency of required events = 5+2+1 = 8&lt;/p&gt;&#10;&#10;&lt;p&gt;Required probability = 8/32 = 1/4&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-02T19:43:22.523" Id="24010" LastActivityDate="2012-03-02T21:20:51.497" LastEditDate="2012-03-02T21:20:51.497" LastEditorUserId="9583" OwnerUserId="9583" ParentId="24006" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;In baseball statistics, there is a statistic called &quot;luck&quot; which is the difference between a team's win-loss record and their &lt;a href=&quot;http://en.wikipedia.org/wiki/Pythagorean_expectation&quot; rel=&quot;nofollow&quot;&gt;Pythagorean win-loss record&lt;/a&gt;. This statistic is supposed to measure how lucky or unlucky a team was to win however many games they did in a season. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose one has a big data set which, for each year n, includes &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;team winning percentage $P(n)$&lt;/li&gt;&#10;&lt;li&gt;team winning percentage the previous year $P(n-1)$&lt;/li&gt;&#10;&lt;li&gt;team luck the previous year $L(n-1)$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;and wants to create a linear regression model using $P(n-1)$ and $L(n-1)$ to estimate $P(n)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;There's no apparent relationship between $L(n-1)$ and $P(n)$, but it seems as though we could use $L(n-1)$ in conjunction with $P(n-1)$ to better predict $P(n)$ based on how &quot;flukey&quot; $P(n-1)$ was and in what way. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, the question is, how could one incorporate a luck-type measure into a linear regression model like I've discussed? I'm not concerned with this particular luck-type measure, but rather any measure which does something similar to what this one is supposed to do. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-02T21:40:24.627" FavoriteCount="1" Id="24021" LastActivityDate="2012-06-01T04:17:17.383" LastEditDate="2012-03-02T22:47:14.050" LastEditorUserId="930" OwnerUserId="9571" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;multivariate-analysis&gt;" Title="How to make sense of &quot;luck&quot; in a multilinear regression?" ViewCount="187" />
  
  <row Body="&lt;p&gt;You &lt;em&gt;could&lt;/em&gt; use an F test to assess the variance of two groups, but the using F to test for differences in variance strictly requires that the distributions are normal.  Using Levene's test (i.e., absolute values of the deviations from the mean) is more robust, and using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Brown%E2%80%93Forsythe_test&quot;&gt;Brown-Forsythe test&lt;/a&gt; (i.e., absolute values of the deviations from the &lt;em&gt;median&lt;/em&gt;) is even more robust.  SPSS is using a good approach here.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; In response to the comment below, I want to clarify what I'm trying to say here.  The question asks about using &quot;a simple F ratio of the ratio of the variances of the two groups&quot;.  From this, I understood the alternative to be what is sometimes known as &lt;a href=&quot;http://en.wikipedia.org/wiki/Hartley%27s_test&quot;&gt;Hartley's test&lt;/a&gt;, which is a very intuitive approach to assessing heterogeneity of variance.  Although this does use a ratio of variances, it is not the same as that used in Levene's test.  Because sometimes it is hard to understand what is meant when it is only stated in words, I will give equations to make this clearer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hartley's test: $$
&#10;F=\frac{MS_{b/t-levels}}{MS_{w/i-levels}}
  
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Two of them, say X1 and X2, are a priori reasonable approximations of the target variable, though obtained with different assumptions&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This was something of a red flag for me.  You'll need to decide how much use it is to predict WPCT using these 2 variables that sound almost like different versions of WPCT itself.  You don't want to stray too close to tautology:  &quot;teams that are successful tend to win a lot of games.&quot;  Maybe you can tell us more about how these 2 are constructed.  Are they truly alternate indicators of team success?  I can imagine variables such as % of games in which team is leading after a certain portion of the game, or number of winning streaks of at least 3 games, that might constitute overly &quot;incestuous&quot; predictors of this type. On the spectrum from exogenous to endogenous, these would be too close to the latter to make for a useful model.&lt;/p&gt;&#10;&#10;&lt;p&gt;I like where you're going with crossvalidation--I think your plan is sound, though of course one can always get more rigorous, more intricate.  If you're interested in more advanced methods you could look up k-fold crossvalidation, jackknifing, or bootstrapping, for instance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-03T18:18:00.610" Id="24056" LastActivityDate="2012-03-03T20:56:46.870" LastEditDate="2012-03-03T20:56:46.870" LastEditorUserId="2669" OwnerUserId="2669" ParentId="24055" PostTypeId="2" Score="0" />
  <row AnswerCount="4" Body="&lt;p&gt;I am concerned about how unequipped most people are (both within and without academia) to properly employ standard model building methods such as linear regression and to interpret the results of these models. Both from my own observations and the literature, it is clear that most people are being poorly served by the standard statistical tools they have available.&lt;/p&gt;&#10;&#10;&lt;p&gt;To improve this situation, I would like to propose a different set of model evaluation output tables (examples shown below in Table 2 and Table 3) to replace the standard output table that are almost universally used today (an example shown in Table 1). This new output format is robust to human error (both in model specification and analysis) to a much greater extent that is our current format.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like ask people for their critique of this proposed method and whether there was interest in using it.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Sorry for the length of this, it got much longer than expected)&lt;/p&gt;&#10;&#10;&lt;h2&gt;Approach Overview&lt;/h2&gt;&#10;&#10;&lt;p&gt;First, calculate an &quot;honest $R^2$&quot; using leave one out cross validation (LOOCV) to get an overall measure of the model's fit. Simply put, remove each data point in turn, estimate the model without that data point, then use the reduced model to calculate the value of the removed data point and use these errors to estimate your $R^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then for each coefficient in the model, remove that parameter from the model and calculate an &quot;honest $R^2$&quot; for that submodel. Calculate the effect of the adding the coefficient to the model as the overall model's $R^2$ minus the $R^2$ of the submodel when the coefficient is removed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus a positive value for this coefficient $R^2$ indicates the fit improves when it is added to the model; a negative value indicates the fit decreases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Results would be reported as a simple table of honest $R^2$ values (see examples in Table 2 and Table 3 below).&lt;/p&gt;&#10;&#10;&lt;h2&gt;Discussion&lt;/h2&gt;&#10;&#10;&lt;p&gt;This approach is designed to be straightforward and to do three things:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Provide end users with the information they need to interpret the (practical) significance of models and coefficients&lt;/li&gt;&#10;&lt;li&gt;Provide end users with this information in a form they are already familiar with&lt;/li&gt;&#10;&lt;li&gt;Minimize the effect of user error both in developing the models and interpreting results&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I think this approach does all three. The use of the universally understood $R^2$ metric should mean it is readily understandable and can plug right into users' existing mental frameworks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, the use of this approach is robust: both to misspecified models and to poor interpretation. Most models depend on certain assumptions that users often fail to check are met. Of course, p-values for coefficients in linear regressions depend on certain parametric assumptions that are generally violated to a greater or a lesser extent. This proposed approach is resilient to such issues (the main issue it is susceptible to is, as with regular output statistics, correlation between observations, which could lead to an overestimation of the honest $R^2$).&lt;/p&gt;&#10;&#10;&lt;p&gt;In regards to the interpretation of results, p-values or often misinterpreted (as has been extensively noted). I believe this honest $R^2$ approach is much more resilient to these issues as it is focused on effect size (which is often what people incorrectly take the p-value as a proxy for). It emphasizes practical significance in place of statistical significance which is a concept people have a lot of trouble with.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, this method allows the direct comparison of results to those generated by other methods (even those that don't generate likelihoods) such as machine learning techniques such as random forests.&lt;/p&gt;&#10;&#10;&lt;p&gt;One issue with this approach is computational burden. Where LOOCV is computationally infeasible I would suggest 10- or 5-fold CV. Since these methods should result in higher error (as the model is training on smaller data sets), the honest $R^2$ values reported by them would be &lt;em&gt;conservative&lt;/em&gt;. Therefore they can be used and reported in placed of the LOOCV with any risk of mis-comparison being of a conservative nature.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another issue is the LOOCV estimate is known to have high bias. I'm not really sure how this could be dealt with or if it is a serious problem. One last point is the LOOCV is asymptotically equivalent to AIC, so this could fit into that paradigm a bit.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Applied Example&lt;/h2&gt;&#10;&#10;&lt;p&gt;Taking a set of housing data ( &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Housing&quot; rel=&quot;nofollow&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Housing&lt;/a&gt; ) we try to predict average house value in a suburb based on average house age, number of rooms in the house, pollutant levels, pupil to teacher ratio, and proximity to a highway. We start with a linear regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is similar to what most software will currently output (this specific table was generated by R).&lt;/p&gt;&#10;&#10;&lt;h3&gt;Table 1. Standard regression table.&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&#10;Coefficients:&#10;               Estimate Std. Error t value Pr(|t|)    &#10;(Intercept)     7.76739    4.98881   1.557 0.120112    &#10;AGE            -0.01509    0.01378  -1.096 0.273773    &#10;ROOMS           7.00565    0.41172  17.015    2e-16 ***&#10;NOX           -13.31418    3.90262  -3.412 0.000698 ***&#10;PUPIL.TEACHER  -1.11645    0.14799  -7.544 2.17e-13 ***&#10;HIGHWAY        -0.02487    0.04257  -0.584 0.559341    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 5.819 on 500 degrees of freedom&#10;Multiple R-squared: 0.6037, Adjusted R-squared: 0.5997 &#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The more honest statistics results are shown below. Note that these results aren't surprising given what we saw in table 1. The parameters that were not significant in Table 1, result in worse models as measured by the change in the honest $R^2$ (again the coefficient Honest $R^2$ value indicate how the honest $R^2$ changed when that coefficient was added to the model).&lt;/p&gt;&#10;&#10;&lt;h3&gt;Table 2. Proposed &quot;honest&quot; statistics table.&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&#10;          Item Coefficient Honest.R2  &#10;  -Full Model-              0.593080  &#10;   (Intercept)      7.7674 -0.000998  &#10;           AGE     -0.0151 -0.000409&#10;         ROOMS      7.0056  0.228901&#10;           NOX    -13.3142  0.008123&#10; PUPIL.TEACHER     -1.1165  0.045506&#10;       HIGHWAY     -0.0249 -0.002018&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Finally, this approach allows us to directly compare more exotic algorithms such as Random Forests. Unlike Bayes factor, AIC, BIC or some other methods, likelihoods are not required for comparison. Anything that creates predictions can be compared. Classification algorithms can also fit into this scheme if you use one of the common pseudo $R^2$ approaches for calculating $R^2$.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Table 3. Comparison between algorithms.&lt;/h3&gt;&#10;&#10;&lt;p&gt;(In this case Support Vector Machines and Random Forests)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;        SVM               |         Random Forest&#10;          Item Honest.R2  |             Item Honest.R2&#10;  -Full Model-  0.71052   |     -Full Model-   0.7450&#10;           AGE  0.00834   |              AGE  -0.0128&#10;         ROOMS  0.34722   |            ROOMS   0.2068&#10;           NOX  0.02338   |              NOX   0.0654&#10; PUPIL.TEACHER  0.01813   |    PUPIL.TEACHER  -0.0127&#10;       HIGHWAY -0.00381   |          HIGHWAY  -0.0255&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Side note: It is interesting to interpret these results as we can see in the linear model pupil per teacher has a statistically significant effect on housing prices. While in the random forest model, it does not. Since the random forest model is the most predictive of the models, I would have to conclude that this was evidence that this coefficient did not significantly (practical significance) affected housing prices. This is a small illustration of the weakness of blindly applying linear models to everything and using that to carry out hypothesis testing.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Implementation&lt;/h2&gt;&#10;&#10;&lt;p&gt;I have uploaded (rough-hewn, unoptimized) code in R to implement this technique at:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dl.dropbox.com/u/94002/HonestStats.R&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/94002/HonestStats.R&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is interest, I can optimize this to be much faster for linear models and implement things like k-Fold CV.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-03T20:44:59.953" FavoriteCount="3" Id="24061" LastActivityDate="2012-03-06T22:42:16.070" OwnerUserId="9573" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;modeling&gt;&lt;predictive-models&gt;&lt;model-selection&gt;&lt;diagnostic&gt;" Title="Communicating Regression Model Results" ViewCount="488" />
  <row AnswerCount="7" Body="&lt;p&gt;How can I create &lt;a href=&quot;http://en.wikipedia.org/wiki/Sankey_diagram&quot; rel=&quot;nofollow&quot;&gt;Sankey diagrams&lt;/a&gt;? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-04T03:35:23.593" FavoriteCount="2" Id="24074" LastActivityDate="2015-02-22T12:21:17.670" LastEditDate="2014-10-08T18:06:34.840" LastEditorUserId="22047" OwnerUserId="1421" PostTypeId="1" Score="9" Tags="&lt;data-visualization&gt;&lt;software&gt;" Title="What's a good tool to create Sankey diagrams?" ViewCount="5329" />
&#10;X_{spline} &amp;amp;= 0 &amp;amp;\text{if } X\le{.7}  \\
  
  <row Body="&lt;p&gt;This should probably be a comment, but getting nicely formatted TeX in the comment field drives me nuts.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your problem is generating a truncated normal distribution in java you have a number of choices.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Easiest, if your interval has high enough probability is just to generate normal random variables until one falls into the range.  This is the essence of guest's selection.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Use accept-reject with some easy to simulate distribution, say a uniform over the interval in question, or an exponential if you are dealing with a tail.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Use the fact that the distribution function for the truncated normal is just a linear function of the distribution function for the normal.  If $\Phi(x)$ is the distribution function of the normal distribution and $\phi(x)$ is the density, and you are truncating so that $a\le x \le b$ then the density function is&#10;$$\phi_{[a,b]}(x) = \frac{\phi(x)}{\Phi(b)-\Phi(a)}$$&#10;and &#10;$$\Phi_{[a,b]}(x) = \frac{\Phi(x)-\Phi(a)}{\Phi(b)-\Phi(a)}.$$&#10;both for $a\le x\le b$. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So if we want to simulate a truncated normal random variable, we choose a random $[0,1]$ uniform random variable $u$ and compute&#10;$$\Phi^{-1}(\Phi(A)+u(\Phi(b)-\Phi(a)) $$&#10;This is what @joint_p was doing.  In R $\Phi(x)$ is &lt;code&gt;pnorm(x)&lt;/code&gt; and $\Phi^{-1}(x)$ is &lt;code&gt;qnorm(x)&lt;/code&gt;.  What these are in the java library you are using, I have no idea.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-03-04T18:54:19.360" Id="24091" LastActivityDate="2012-03-04T18:54:19.360" OwnerUserId="72" ParentId="24069" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="24147" AnswerCount="1" Body="&lt;p&gt;Does there exists a Box-Cox method for linear Gaussian models with random effects ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-05T11:05:46.850" FavoriteCount="1" Id="24115" LastActivityDate="2012-10-10T21:51:35.517" LastEditDate="2012-10-10T21:51:35.517" LastEditorUserId="686" OwnerUserId="8402" PostTypeId="1" Score="5" Tags="&lt;mixed-model&gt;&lt;data-transformation&gt;" Title="Box-Cox transformation for mixed models" ViewCount="826" />
  <row Body="&lt;p&gt;How about:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;AIC helps you find the best-fitting model that uses the fewest&#10;  variables.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If that is too far in the non-technical direction, let me know in comments and I'll come up with another.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-05T11:35:42.350" Id="24117" LastActivityDate="2012-03-05T11:35:42.350" OwnerUserId="8462" ParentId="24116" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;If you like something short and easy to digest, then have a look at the following paper from the psychological literature:&lt;/p&gt;&#10;&#10;&lt;p&gt;Erceg-Hurn, D. M., &amp;amp; Mirosevich, V. M. (2008). &lt;a href=&quot;http://www.unt.edu/rss/class/mike/5700/articles/robustAmerPsyc.pdf&quot; rel=&quot;nofollow&quot;&gt;Modern robust statistical methods: An easy way to maximize the accuracy and power of your research.&lt;/a&gt; &lt;em&gt;American Psychologist&lt;/em&gt;, 63(7), 591–601. doi:10.1037/0003-066X.63.7.591&lt;/p&gt;&#10;&#10;&lt;p&gt;They mainly rely on the books by Rand R Wilcox (which are admittedly also not too mathematical):&lt;/p&gt;&#10;&#10;&lt;p&gt;Wilcox, R. R. (2001). Fundamentals of modern statistical methods : substantially improving power and accuracy. New York; Berlin: Springer.&lt;br&gt;&#10;Wilcox, R. R. (2003). Applying contemporary statistical techniques. Amsterdam; Boston: Academic Press.&lt;br&gt;&#10;Wilcox, R. R. (2005). Introduction to robust estimation and hypothesis testing. Academic Press.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-05T15:29:01.280" Id="24134" LastActivityDate="2012-03-05T15:29:01.280" OwnerUserId="442" ParentId="24054" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;Here's a definition that locates AIC in the menagerie of techniques used for model selection. &lt;strong&gt;AIC is just one of several reasonable ways to capture the trade-off between goodness of fit (which is improved by adding model complexity in the form of extra explanatory variables, or adding caveats like &quot;but only on Thursday, when raining&quot;) and parsimony (simpler==better) in comparing non-nested models.&lt;/strong&gt; Here's the fine print: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I believe the OP's definition only applies to linear models. For things like probits, the AIC are usually defined in terms of log-likelihood.&lt;/li&gt;&#10;&lt;li&gt;Some other criteria are adjusted $R^{2}$ (which has the least adjustment for extra explanatory variables), Kullback-Leibler IC, BIC/SC, and even more exotic ones, like Amemiya's prediction criterion, rarely seen in the wilds of applied work. These criteria differ on how steeply they penalize model complexity. Some have argued that the AIC tends to select models that are overparameterized, because the model-size penalty is pretty low. The BIC/SC also increases the penalty as the sample size increases, which seems like a handy-dandy feature.&lt;/li&gt;&#10;&lt;li&gt;A nice way to sidestep participating in America's Top Information Criterion, is to admit that these criteria are arbitrary and considerable approximations are involved in deriving them, especially in the non-linear case. In practice, the choice of a model from a set of models should probably depend on the intended use of that model. If the purpose is to explain the main features of a complex problem, parsimony should be worth its weight in gold. If prediction is the name of the game, parsimony should be less dear. Some would even add that theory/domain knowledge should also play a bigger role. In any case, what you plan to do with the model should determine what criterion you might use.&lt;/li&gt;&#10;&lt;li&gt;For &lt;strong&gt;nested&lt;/strong&gt; models, the standard hypothesis test restricting the parameters to zero should suffice.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-03-05T21:53:23.267" Id="24158" LastActivityDate="2013-08-12T16:14:20.987" LastEditDate="2013-08-12T16:14:20.987" LastEditorUserId="17230" OwnerUserId="7071" ParentId="24116" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;By p-value I presume you mean &quot;the probability of seeing a value of my test statistic as large as this, if it really comes from a $\chi^2$ distribution with one degree of freedom&quot;.  So if you get a low p-value it means it is an improbable value to have seen.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1-pchisq(0.1185, 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pchisq(0.1185, 1, lower=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In Excel&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;=1-CHISQ.DIST(0.1185,1, TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;=CHISQ.DIST.RT(A2,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The p-value is very large because your value of the test statistic is actually considerably less than the average value of such a $\chi^2$ random variable.  If anything, it is unusually &lt;em&gt;small&lt;/em&gt;. So basically, you do not have a large value and hence there is not evidence to dismiss the hypothesis that it genuinely comes from a $\chi^2$ distribution.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-05T22:30:16.460" Id="24162" LastActivityDate="2012-03-06T00:23:59.070" LastEditDate="2012-03-06T00:23:59.070" LastEditorUserId="7972" OwnerUserId="7972" ParentId="24160" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="24326" AnswerCount="1" Body="&lt;p&gt;A regression in which all the IVs are replaced by their residuals after regressing them with the rest of the IVs. I have seen (for example in Elements of Statistical Learning) how doing this in a multivariate regression yields the multivariate coefficients from univariate regressions. But other than this, why would one benefit from using orthogonalized IVs rather than the regular ones? What are some reasons to be doing this in a multivariate regression?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-05T22:50:40.193" FavoriteCount="1" Id="24164" LastActivityDate="2012-03-08T15:45:15.597" OwnerUserId="6355" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;" Title="Why would one want to carry out an orthogonalized (gram schmit) regression?" ViewCount="196" />
  <row Body="&lt;p&gt;I think if you do PCA on all the variables, the first PC is very likely to be a &quot;general size measure&quot; so the idea of doing PCA on residuals may be to get rid of that. But, as far as I can see, the 2nd, 3rd etc. PCs on the original variables will be very similar to the 1st, 2nd, etc PCs on the residuals.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-05T23:09:56.753" Id="24167" LastActivityDate="2012-03-05T23:09:56.753" OwnerUserId="686" ParentId="24146" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Say I have 5 binary variables and 2 normal variables. I want to get the probability of success, say one of the variable 1 or 0, 1 for success. How can I do that?&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried &lt;code&gt;glm(A~B+C+D+E, binomial, data)&lt;/code&gt; in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;$A$ is a binary variable for which I want to know the probability of success. &#10;$B$, $C$, $D$, $E$ are all binary variables. Am I doing right?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-06T03:46:32.870" Id="24181" LastActivityDate="2012-03-06T12:25:51.507" LastEditDate="2012-03-06T12:25:51.507" LastEditorUserId="686" OwnerUserId="9633" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;binary&gt;" Title="Coding the regresssion model with several binary variables" ViewCount="105" />
  
  <row Body="&lt;p&gt;Let's focus on the business problem, develop a strategy to address it, and begin implementing that strategy in a simple way.  Later, it can be improved if the effort warrants it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;business problem&lt;/strong&gt; is to maximize profits, of course.  That is done here by balancing the costs of refilling machines against the costs of lost sales.  In its current formulation, the costs of refilling the machines are fixed: 20 can be refilled each day.  The cost of lost sales therefore depends on the frequency with which machines are empty.&lt;/p&gt;&#10;&#10;&lt;p&gt;A conceptual &lt;strong&gt;statistical model&lt;/strong&gt; for this problem can be obtained by devising some way to estimate the costs for each of the machines, based on previous data.  The &lt;em&gt;expected&lt;/em&gt; cost of not servicing a machine today approximately equals the chance it has run out times the rate at which it is used.  For example, if a machine has a 25% chance of being empty today and on average sells 4 bottles per day, its expected cost equals 25% * 4 = 1 bottle in lost sales.  (Translate that into dollars as you will, not forgetting that one lost sale incurs intangible costs: people see an empty machine, they learn not to rely on it, etc.  You can even adjust this cost according to a machine's location; having some obscure machines run empty for a while might incur few intangible costs.)  It's fair to assume that refilling a machine will immediately reset that expected loss to zero--it should be rare that a machine will get emptied every day (don't you wish...).  As time goes by, the expected loss starts to rise until eventually it reaches a limiting value equal to the expected daily sales: an example is shown in the second figure below.&lt;/p&gt;&#10;&#10;&lt;p&gt;A &lt;em&gt;simple&lt;/em&gt; statistical model along these lines proposes that fluctuations in a machine's use appear random.  This suggests a &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_regression&quot;&gt;Poisson model&lt;/a&gt;.  Specifically, we may posit that a machine has an underlying daily sales rate of $\theta$ bottles and that the number sold during a period of duration $x$ days has a Poisson distribution with parameter $\theta x$.  (Other models can be formulated to handle the possibility of clusters of sales; this one supposes that sales are individual, intermittent, and independent of each other.)&lt;/p&gt;&#10;&#10;&lt;p&gt;In the present example, the observed durations are $x=(7, 7, 7, 13, 11, 9, 8, 7, 8, 10)$ and the corresponding sales were $y=(4, 14, 4, 16, 16, 12, 7, 16, 24, 48)$.  Maximizing the likelihood gives $\hat{\theta} = 1.8506$: this machine has been selling about two bottles per day.  The data history is not long enough to suggest that any more complicated model is needed; this is an adequate description of what has been observed so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/O0CvZ.png&quot; alt=&quot;Actual vs fit&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The red dots show the sequence of sales; the blue dots are estimates based on the maximum likelihood estimate of the typical sales rate.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Armed with an estimated sales rate, we can go on to compute the chance that a machine may be empty after $t$ days: it is given by the complementary cumulative distribution function (CCDF) of the Poisson distribution, as evaluated at the machine's capacity (presumed to be 50 in the next figure and the examples below).  Multiplying by the estimated sales rate gives a &lt;strong&gt;plot of the expected daily loss in sales &lt;em&gt;versus&lt;/em&gt; time since last refill:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RmQWE.png&quot; alt=&quot;Loss over time&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Naturally this curve is rising fastest near the time at $50/1.85 = 27$ days when the machine is most likely to run out.  What it adds to our understanding is to show that an appreciable rise actually begins a week earlier than that.  Other machines with other rates will have steeper or shallower rises: that will be useful information.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a chart like this for each machine (of which it seems there are a couple hundred), you can easily &lt;strong&gt;identify the 20 machines currently experiencing the greatest expected loss: servicing them is the optimal business decision.&lt;/strong&gt;  (Note that each machine will have its own estimated rate and will be at its own point along its curve, depending on when it was last serviced.)  Nobody actually has to look at these charts: identifying the machines to service on this basis is easily automated with a simple program or even with a spreadsheet.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is just the beginning.  Over time, additional data may suggest modifications to this simple model: you might account for weekends and holidays or other anticipated influences on sales; there may be a weekly cycle or other seasonal cycles; there may be long-term trends to include in the forecasts.  You might want to track outlying values representing unexpected one-time runs on the machines and incorporate this possibility in the loss estimates, etc.  I doubt, though, that it will be necessary to worry much about serial correlation of sales: it's hard to think of any mechanism to cause such a thing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Oh, yes: how does one obtain the ML estimate?  I used a numerical optimizer, but in general you will get very close simply by dividing total sales over a recent period by the length of the period.  For these data that's 163 bottles sold from 12/9/2011 through 2/27/2012, a period of 87 days: $\hat{\theta} = 1.87$ bottles per day.  Close enough to $1.8506$, and extremely simple to implement, so anyone can start these calculations right away. (R and Excel, among others, will readily compute the Poisson CCDF: model the calculations after  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1-POISSON(50, Theta * A2, TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for Excel (&lt;code&gt;A2&lt;/code&gt; is a cell containing the time since last refill and &lt;code&gt;Theta&lt;/code&gt; is the estimated daily sales rate) and&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 - ppois(50, lambda = (x * theta))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for R.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The fancier models (which incorporate trends, cycles, etc) will need to use Poisson regression for their estimates.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; For the aficionados: I am purposely avoiding any discussion of uncertainties in the estimated losses.  Handling these can significantly complicate the calculations.  I suspect that directly using these uncertainties would not add appreciable value to the decision.  However, being aware of the uncertainties and their sizes could be useful; that might be depicted by means of error bands in the second figure.  In closing, I just want to re-emphasize the nature of that figure: it plots numbers that have a direct and clear business meaning; namely, expected losses; it does not plot more abstract things such as confidence intervals around $\theta$, which may be of interest to the statistician but will just be so much noise to decision maker.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-06T18:26:07.247" Id="24201" LastActivityDate="2012-03-06T18:26:07.247" OwnerUserId="919" ParentId="23860" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;If you want to encompass categorical data analysis, then you might want to treat continuous variables as categorical variables. For example, if you want to perform Mantel-Haenszel chi-square test, an appropriate categorization is needed. &lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of categorization, you need to categorize them in a sensible way because we don't have too few records for &lt;code&gt;T=1&lt;/code&gt; and too many records for &lt;code&gt;T=2&lt;/code&gt;, for instance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-06T23:00:38.367" Id="24219" LastActivityDate="2012-03-06T23:00:38.367" OwnerUserId="9651" ParentId="24202" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;Suppose products A, B and C are sold on markets 1, 2 and 3.&#10;I have access to three years worth of monthly historical data.&#10;I'd like to detect/foresee possible shifts in sales mix for the three products over the three markets (e.g. I'd like to know if market 1 is buying more of product A than it used to, or if product B is cannibalizing the others on market 3).&lt;/p&gt;&#10;&#10;&lt;p&gt;Does a 5/10/15% increase mean that I'm facing a shift in consumer behaviour? Does it depend on the intrinsic variability of the behavioural pattern? Also how do I choose the optimal time-frame on which to perform such analysis (i.e. how do I know if comparing semester over semester increase makes more sense than comparing quarter over quarter increase)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would t-test/ANOVA work here? Or maybe a control chart? Or maybe some intervention detection method to check if there was a level shift in my time series? If so, how do I go about it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any input is greatly appreciated!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-03-07T09:13:05.897" FavoriteCount="1" Id="24244" LastActivityDate="2012-05-06T19:31:07.163" LastEditDate="2012-03-07T12:44:42.670" LastEditorUserId="7972" OwnerUserId="9251" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;variance&gt;" Title="How do I detect shifts in sales mix?" ViewCount="451" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have a categorical variable $X_1$ with $100$ levels. Should we &lt;strong&gt;not&lt;/strong&gt; test interaction between $X$ and some other variable $X_2$? &lt;/p&gt;&#10;&#10;&lt;p&gt;Because if, for example, $X_1(3)*X_2$ is not significant then the interaction between $X_1$ and $X_2$ is not significant?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-07T10:41:57.660" Id="24246" LastActivityDate="2012-03-07T12:40:58.870" LastEditDate="2012-03-07T12:12:46.017" LastEditorUserId="686" OwnerUserId="9648" PostTypeId="1" Score="2" Tags="&lt;interaction&gt;" Title="Categorical variable with a lot of levels and interaction" ViewCount="207" />
  
&#10;  &amp;amp;+ 0.534440 &amp;amp;\text{ if }f=f_3  \\
&#10;  &amp;amp; -0.085658\ a &amp;amp;\text{ if }h=h_1  \\
  
  <row Body="&lt;p&gt;This is most probably going to be achieved using simulation. Using normal priors for the &lt;em&gt;log&lt;/em&gt; odds ratios with &quot;almost 0&quot; precision, estimating the logistic regression model, and computing the parameters' posterior mode gives you approximate frequentist inference. Increasing this precision can be used to represent the strength of prior information. The power of your Bayesian test will, of course, depend on both the precision and the location of the priors so be certain to state explicitly how this information is used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-07T16:22:46.360" Id="24262" LastActivityDate="2012-03-07T16:22:46.360" OwnerUserId="8013" ParentId="24136" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;The situation is as follows. There are 400 examples in the training set and 200 discrete classes (each class has two examples). There are a few thousand attributes.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I run dimensionality reduction to 2D or 3D, I would like to see (optimally) 200 clusters of 2 points each - one cluster for each class. However in practice that's not the case.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, how do you go about choosing the set of attributes that will give optimal results with clustering/classification (whether with dimensionality reduction or without it).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-07T20:59:35.603" Id="24281" LastActivityDate="2012-04-06T23:54:50.307" OwnerUserId="8760" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;classification&gt;" Title="Choosing attributes for clustering/classification" ViewCount="80" />
  
  <row Body="&lt;p&gt;Even if you would see &quot;clusters&quot; of 2 points each, it would not be statistically meaningful. You need more data. Two examples per class is just too little.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-07T23:04:07.623" Id="24289" LastActivityDate="2012-03-07T23:04:07.623" OwnerUserId="7828" ParentId="24281" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&quot;Exponential Random Graph Models&quot; - precisely deal with the first case in your question.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-03-08T06:31:31.197" Id="24307" LastActivityDate="2012-03-08T06:31:31.197" OwnerUserId="6897" ParentId="24306" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Using &lt;code&gt;unique&lt;/code&gt; in the right way ought to do the trick:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(2)&#10;limit &amp;lt;- 3&#10;myindex &amp;lt;- seq(0,limit)&#10;&#10;endDim&amp;lt;-factorial(limit)&#10;permutations&amp;lt;-sample(myindex)&#10;&#10;while(is.null(dim(unique(permutations))) || dim(unique(permutations))[1]!=endDim) {&#10;    permutations &amp;lt;- rbind(permutations,sample(myindex))&#10;}&#10;# Resulting permutations:&#10;unique(permutations)&#10;&#10;# Compare to&#10;set.seed(2)&#10;permutations&amp;lt;-sample(myindex)&#10;for(i in 1:endDim)&#10;{&#10;permutations&amp;lt;-rbind(permutations,sample(myindex))&#10;}&#10;permutations&#10;# which contains the same permutation twice&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-03-08T08:37:16.517" Id="24309" LastActivityDate="2012-03-08T08:37:16.517" OwnerUserId="8507" ParentId="24300" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The key insight to understand this is that correlation is somehow equivalent to predictability with linear functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine you have two (mean zero) random variables $X$ and $Y$ and you try to predict $Y$ from $X$. The optimal linear predictor (in a least squares sense) is &#10;$$y = \frac{\sigma_{yx}}{\sigma_x^2}x = \frac{\rho_{yx} \sigma_y}{\sigma_x}x,$$&#10;which basically means that you divide $x$ by the variance of $X$ multiply it with the correlation coefficient and rescale it with the variance of $Y$. If the correlation is $\rho_{yx}=0$, then there's no meaningful prediction of $Y$ from $X$ (except $Y$ is zero all the time).&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, if you have to uncorrelated (mean zero) random variables $X$ and $Y$ and you project them on two linearly dependent vectors $\mathbf w$ and $\mathbf v$, then the results will be correlated: Assume that $\|\mathbf v\|=\|\mathbf w\|=1$, $W=(\mathbf v,\mathbf w)^\top$, $\mathbf z = (x,y)^\top$, and $\tilde{\mathbf z} = W \mathbf z$, then&#10;$$\Sigma_{\tilde z\tilde z} = E\left[\tilde{\mathbf z} \tilde{\mathbf z}^\top \right] =  WE\left[\mathbf z \mathbf z^\top \right]W^\top = WDW^\top \not= I$$&#10;where $D$ is a diagonal matrix (the covariance matrix of $X$ and $Y$).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you can also see that $\tilde{Z_1}$ and $\tilde{Z_2}$ would still have been uncorrelated if $\mathbf v$ and $\mathbf w$ were orthogonal and $D=I$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you have several IVs and want to do linear regression, you want your IVs be uncorrelated because then you can use them one at a time. If they are uncorrelated, then you cannot gain information about one IV from another through a linear function, so-in some sense-each of the uncorrelated IVs gives you a fresh look on the dependent variable. Since linear dependence means correlation, you use orthogonalized IVs in regression. In general, however, orthogonalization is not enough to get decorrelation. For that, you must use the principal components (from PCA) which are an orthogonal system. If you additionally equalize the variances in the PCA system (which is called whitening then), you can rotate the IVs again, since they will stay uncorrelated then (but &lt;em&gt;only&lt;/em&gt; after whitening and &lt;em&gt;only&lt;/em&gt; for orthogonal transforms).&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, decorrelation means removing linear predictability between the IVs.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-08T15:45:15.597" Id="24326" LastActivityDate="2012-03-08T15:45:15.597" OwnerUserId="6000" ParentId="24164" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I previously asked this question: &lt;a href=&quot;http://stats.stackexchange.com/questions/24194/winning-percentage-logistic-regression-or-linear-regression&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've generated the two logistic regression models described in the link using all available data (8 seasons for all 30 teams), and now I want to decide which is a better predictor of win percentage in the coming season, winning percentage in the previous season or Pythagorean expectation from the previous season. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that if you want to evaluate the predictive abilities of a model, it's often good to evaluate on data different from that which generated the model. I wonder how important that is here, given that the models are of the same type, created from the same data. It seems as though we could say that the model which was a better fit with respect to the data which created both models is better in this case. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, the question is: 1) do I need to see how the models compare on a different data set, and 2) what's the standard measure for model comparison in a situation like that I've described. &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, how do I know which is better? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-08T20:32:14.917" FavoriteCount="1" Id="24343" LastActivityDate="2012-04-13T19:55:03.050" OwnerUserId="9571" PostTypeId="1" Score="2" Tags="&lt;error&gt;&lt;logistic&gt;" Title="Comparing logistic regression models of winning percentage" ViewCount="218" />
  <row AcceptedAnswerId="32230" AnswerCount="2" Body="&lt;p&gt;Gelman &amp;amp; Hill (2006) explain how a multilevel regression model can compensate for data sparsity by pooling parameter estimates. But what happens if the data are not sparse at random?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, suppose we are evaluating a new blood pressure medication. On every visit we measure a patient's blood pressure, administer the treatment, and measure blood pressure again. We may have fewer opportunities to observe the effect of a treatment in some patients than in others, so we may want to use pooling. But the number of observations per patient could be related to the fact that the treatment is ineffective for some patients (so they do not want to come in), or causes side effects two hours later (so they do not want to come in), or the medication puts patients in a coma and they drop out of the study.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is pooling no longer appropriate in such a study?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-08T21:19:43.703" Id="24346" LastActivityDate="2012-07-13T15:28:56.197" OwnerUserId="8207" PostTypeId="1" Score="2" Tags="&lt;multilevel-analysis&gt;&lt;missing-data&gt;" Title="Hierarchical modeling when data not missing at random" ViewCount="234" />
  
  
  <row AcceptedAnswerId="24451" AnswerCount="1" Body="&lt;p&gt;I am using the mlogit package in R to run a multinomial logistic regression on pooled discrete choice data collected using two different questionnaire formats. I want to test whether the format had a significant effect on choices. When I run the basic model I get a result. But when I run the same model with a dummy variable indicating which format the respondents saw, I get an error: &quot;Error in solve.default(H, g[!fixed]) : Lapack routine dgesv: system is exactly singular&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I was able to replicate the error using Train's Electricity dataset in the mlogit package, setting a dummy based on whether the respondent ID was odd or even:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mlogit)&#10;data(&quot;Electricity&quot;, package = &quot;mlogit&quot;)&#10;Electr &amp;lt;- mlogit.data(Electricity, id = &quot;id&quot;, choice = &quot;choice&quot;, &#10;                      varying = 3:26, shape = &quot;wide&quot;, sep = &quot;&quot;)&#10;Electr$odd.dummy &amp;lt;- ifelse(Electr$id %% 2 == 0, 0, 1) # As example, set dummy if ID is odd&#10;summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas | 0, data=Electr)) # Basic model&#10;summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas + odd.dummy | 0, data=Electr)) # Basic + dummy&#10;summary(mlogit(choice ~ odd.dummy | 0, data=Electr)) # Only dummy&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As with my data, the first model runs, but the second two are singular.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that a result will be singular if there is perfect colinearity between variables, but I don't see how this is the case here.  Respondents were randomly assigned to one format or the other, and the underlying experimental design was the same in both formats, so there shouldn't be any colinearity between the dummy and the other variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be grateful if someone could explain why adding the dummy leads to a singular result, and even more grateful if they could suggest a solution to avoid it.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-09T05:50:28.550" Id="24365" LastActivityDate="2012-03-10T21:29:24.677" OwnerUserId="6857" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;conjoint-analysis&gt;" Title="Why do I get exact singularity when I add a dummy variable to a multinomial logistic using R mlogit?" ViewCount="1367" />
  
  <row AcceptedAnswerId="24671" AnswerCount="3" Body="&lt;p&gt;Imagine a (reasonably large) household survey where all persons in every household have been questioned. For the purpose of microsimulation, this survey needs to be expanded to a &lt;a href=&quot;http://stats.stackexchange.com/q/23900/6432&quot;&gt;full population&lt;/a&gt;. In a first step, weights are attached to each observation so that external control totals are obeyed (calibration).&lt;/p&gt;&#10;&#10;&lt;p&gt;If we only have control totals that describe how many households of this-and-that type are in a zone, we can use &lt;a href=&quot;http://en.wikipedia.org/wiki/Iterative_proportional_fitting&quot; rel=&quot;nofollow&quot;&gt;IPF&lt;/a&gt; (also known as raking) which gives a maximum-likelihood estimate of the weights. Minimizing the relative entropy is equivalent to raking/IPF. &lt;strong&gt;EDIT&lt;/strong&gt;: But what if we have control totals at person and household level? Like, telling us how many households of which type &lt;em&gt;and&lt;/em&gt; how many persons of which sex/age/education level/... there are. I was unable to find a &quot;standard&quot; approach here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is raking/IPF the &quot;correct&quot; approach from a statistical point of view? Are there other options? What would be, from a statistical point of view, the most reasonable approach to calibrate the weights in the presence of control totals at household and person level?&lt;/p&gt;&#10;&#10;&lt;p&gt;See the &lt;a href=&quot;http://stats.stackexchange.com/q/23529&quot;&gt;original question&lt;/a&gt; for more context. (It was probably too big, I'm splitting it into parts.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-09T12:24:27.650" Id="24373" LastActivityDate="2012-03-15T00:32:33.033" LastEditDate="2012-03-14T13:33:56.287" LastEditorUserId="6432" OwnerUserId="6432" PostTypeId="1" Score="1" Tags="&lt;survey&gt;&lt;multilevel-analysis&gt;&lt;population&gt;&lt;finite-population&gt;&lt;calibration&gt;" Title="Calibrating a household survey to household-level and person-level control totals" ViewCount="322" />
  
  
  
  
&#10;\Pr[0] &amp;amp;= 24/100 \\
  
  <row Body="&lt;p&gt;As per my comment I am not certain what you are looking for, but when I am fitting time series after a bit of a hiatus from them I tend to grab my copy of Time Series Analysis and Its Applications for more theory questions and I look at a few different sites online (also do some googling to see if there are any sweet new ones):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/views/TimeSeries.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/views/TimeSeries.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The CRAN taskview on time series gives you a good look at just how many things you can do&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is a nice walk through of some time series analysis in R. I personally do much of my statistical learning through example (which generally means following guides like this in R), so this guide is a favorite of mine.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.duke.edu/~rnau/411arim.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.duke.edu/~rnau/411arim.htm&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This link is a decent look at ARIMA outside of R, it walks you through what different models mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you can always check out wikipedia if you are just looking for statements for formulas. These are just the ones I have book marked, so maybe some other folks will contribute their favorites. As I said in comments, if you expand on what you are looking for more specifically you can probably get better links from me or one of the folks that follow time series closer than I do.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-05-10T11:54:42.550" CreationDate="2012-03-09T21:53:06.143" Id="24408" LastActivityDate="2012-04-02T02:21:25.383" LastEditDate="2012-04-02T02:21:25.383" LastEditorUserId="9007" OwnerUserId="9497" ParentId="24398" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The question suggests that a repeated measures &lt;code&gt;ANOVA&lt;/code&gt; could work, where &lt;code&gt;technique&lt;/code&gt; defines the groups. For how frequently you should measure, that depends on what is important, i.e. the research hypothesis. If you're interested in minute outcomes, measure in minute lots. However, that will give you a lot of data points if you're doing this even over the space of a couple of hours, let alone days. If you end up with a lot of data points, the question is likely to be one of practical significance rather than statistical significance, as you are likely to end up with a statistically significant result regardless of how small the difference is between the techniques, just due to sheer volume of data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-09T22:38:05.863" Id="24413" LastActivityDate="2012-03-09T22:38:05.863" OwnerUserId="8605" ParentId="24371" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;There are several methods in Biostatistics that used to test pre-post treatment effects, i.e., change scores, percentage change scores, analysis of covariance (ANCOVA) or random effects models. If your underlying interest is mean performance change in pre-post treatment as well as treatment vs. control group, random-effects model (random intercept model) might be helpful, as change score or ANCOVA take the difference between pre-post performances or take pre-performance as a covariate to test the treatment effect.  It is equivalent to the repeated measure analysis of variance if you data is balanced. &#10;Also due to the skewness of the distribution in the different groups, bootstrapping may be helpful, depending on whether you wanna test the hypothesis or you wanna a reliable &quot;effect size&quot; and confidence interval. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-11T01:37:49.677" Id="24460" LastActivityDate="2012-03-11T01:37:49.677" OwnerUserId="7159" ParentId="24438" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;I'd calculate a similarity matrix using jaccard distance and then run k-means&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-11T20:34:08.827" Id="24486" LastActivityDate="2012-03-11T20:34:08.827" OwnerUserId="7568" ParentId="24477" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I will use established notations, when initial TF-IDF matrix stores documents at columns, and rows correspond to term occurrences or term's tf/idfs.&#10;Let $A$ be $(n \times d)$ matrix with $d$ domuments and $n$ dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The standart method to do feature reduction in text mining is &lt;a href=&quot;http://en.wikipedia.org/wiki/Latent_semantic_indexing&quot; rel=&quot;nofollow&quot;&gt;latent semantic indexing&lt;/a&gt;. The key idea is applying a little modification of &lt;a href=&quot;http://en.wikipedia.org/wiki/SV_decomposition&quot; rel=&quot;nofollow&quot;&gt;SVD&lt;/a&gt; decomposition for $n \times d$ TF-IDF matrix (or just word occurrence matrix).&lt;/p&gt;&#10;&#10;&lt;p&gt;Particularly, let our initial matrix $A$ be decomposed:&#10;$$A = S\times D \times T^t,$$ where $S, D, T^t$ have dimensions $(n \times r), (r \times r), (r \times d)$ respectively and $D$ is a diagonal matrix with $A$'th sorted singular values on diagonal: $D = diag(d_1,d_2,\dots,d_r), d_1 \le \dots d_r$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The modification performed in latent semantid indexing is truncating the matrix $D$ so that only $k \le r$ largest singular values remained. It can be shown, that&#10;$A \approx A_k = S_k \times D_k \times T_k^t$, where $S_k$ is $n \times k$ matrix of first $k$ columns of $S$, $T_k^t$ is $k \times d$ matrix of first $k$ rows of $T^t$, and $D_k = diag(r_1,\dots,r_k)$. The matrix $T_k$ is a &lt;em&gt;concept document matrix&lt;/em&gt;, which rows store reduced description of document. Further you apply text-mining algorithms to that matrix as you could apply them to initial TF-IDF matrix.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-12T08:06:56.603" Id="24498" LastActivityDate="2012-03-12T08:35:16.337" LastEditDate="2012-03-12T08:35:16.337" LastEditorUserId="2789" OwnerUserId="2789" ParentId="24493" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Just a simple example to see why data can be transformed in any suitable way for the analysis:&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine we want to find out the dependency between the size of a car and its price. For some reason you measure the area (or volume), whereas the effect perhaps can be explained better when looking at the length. Let's assume also that the marginal utility of additional income &lt;a href=&quot;http://en.wikipedia.org/wiki/Marginal_utility#Diminishing_marginal_utility&quot; rel=&quot;nofollow&quot;&gt;decreases&lt;/a&gt;, hence we would expect a dependency between $\log(income)$ and the length. The only way to capture this correctly is to apply different transformations to the variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-12T09:55:26.160" Id="24501" LastActivityDate="2012-03-12T13:38:57.713" LastEditDate="2012-03-12T13:38:57.713" LastEditorUserId="6432" OwnerUserId="6432" ParentId="24467" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="24517" AnswerCount="3" Body="&lt;p&gt;I have multiclass unbalanced data (4 class with 15% 25% 45% 15% data in each class). Which method is good for classification of such data- SVM or ANN? &lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE- Let me make the question little more general. @Dikran Marsupial said in one answer &quot;choice of classifier depends on the nature of the particular dataset&quot; but what are the factors that one should consider before choosing a classifier. I understand the first chose may not give best answer all the time but it can be a good starting point. So what properties of data I should consider before choosing a classifier??&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-12T13:49:03.627" FavoriteCount="4" Id="24514" LastActivityDate="2012-03-14T09:54:29.133" LastEditDate="2012-03-14T09:54:29.133" LastEditorUserId="8924" OwnerUserId="8924" PostTypeId="1" Score="3" Tags="&lt;classification&gt;&lt;svm&gt;&lt;neural-networks&gt;&lt;unbalanced-classes&gt;" Title="SVM vs. artificial neural network" ViewCount="1812" />
  
  <row Body="&lt;p&gt;I can give you few hints:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You could use dynamic time warping to extract similarity between your sequences. Please see : &lt;a href=&quot;http://stats.stackexchange.com/questions/22209/can-someone-please-explain-dynamic-time-warping-for-determining-time-series-simi&quot;&gt;Can someone please explain dynamic time warping for determining time series similarity?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Cave plot (visualization as you have stated) is another option. &lt;a href=&quot;http://cm.bell-labs.com/cm/ms/departments/sia/project/misc/caveplot.html&quot; rel=&quot;nofollow&quot;&gt;http://cm.bell-labs.com/cm/ms/departments/sia/project/misc/caveplot.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;If you use R, &lt;a href=&quot;http://www.rdatamining.com/examples/ts-mining&quot; rel=&quot;nofollow&quot;&gt;http://www.rdatamining.com/examples/ts-mining&lt;/a&gt; may give you some hint&lt;/li&gt;&#10;&lt;li&gt;You may want to see a discussion on similar topic at &lt;a href=&quot;http://stats.stackexchange.com/questions/3238/time-series-clustering-in-r&quot;&gt;Time series &amp;#39;clustering&amp;#39; in R&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-03-12T17:17:14.283" Id="24525" LastActivityDate="2012-03-12T17:17:14.283" OwnerUserId="9583" ParentId="24518" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am currently doing a project which involves pothole detection and neural networks. So far, I have an Android phone that reads Accelerometer readings and writes the X,Y,Z Axis aswell as the Amplitude and current timestamp into a CSV file. The data is then normalized using min-max normalization and uses the Y axis readings from the CSV file. The problem I am facing for the neural network to learn a pothole is the fact that what data should I feed to the Back Propagation Neural Network? Shall I set a threshold and when the Y axis reaches this point, get the 5 previous points and 5 points after and then feed the network with 11 inputs? I don't want to overtrain the network nor feed it with data in different positions each time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Training - I am also starting to gather the data collected and create a training dataset - should I put things such as readings for normal/bumpy roads/speed bumps as well as potholes? How large should a training set be? or is 'the more data the better it is' actually true?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what the pothole data looks like.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZKGth.png&quot; alt=&quot;http://i.stack.imgur.com/4cSzt.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is how the speedbump data looks like.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/m38eG.png&quot; alt=&quot;http://i.stack.imgur.com/7BLjq.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A sample of the data collected:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; X-Axis     Y-Axis    Z-Axis   Timestamp&#10;&#10;-0.371827, 8.513097, 5.441484, 165401&#10;-0.601749, 7.976613, 5.326523, 165601&#10;-0.333506, 8.053253, 5.441484, 165801&#10;-0.256866, 8.206534, 5.364844, 166001&#10;0.049697, 8.398136, 5.364844, 166202&#10;-0.371827, 8.436457, 5.211563, 166400&#10;-0.256866, 8.551417, 5.709726, 166601&#10;-0.256866, 8.513097, 5.403164, 166801&#10;-0.333506, 8.474776, 5.709726, 167000&#10;-0.563428, 8.628057, 5.594766, 167201&#10;-0.563428, 7.401808, 4.713398, 167402&#10;-1.981280, 5.447472, 4.406836, 167602    POTHOLE&#10;-0.180225, 5.600753, 5.403164, 167800    POTHOLE&#10;-0.984952, 8.053253, 4.445156, 168001&#10;-1.214874, 8.666378, 5.671406, 168201&#10;-0.525108, 7.210207, 3.870352, 168401&#10;-1.138233, 7.286847, 5.824687, 168600&#10;-0.601749, 10.045910, 5.288203, 168801&#10;-0.180225, 8.206534, 5.173242, 169001&#10;0.279619, 7.861651, 5.518125, 169200&#10;0.202978, 8.934620, 5.824687, 169401&#10;-0.065264, 8.321495, 5.364844, 169601&#10;-0.065264, 8.628057, 5.709726, 169800&#10;-0.716710, 8.014933, 5.748047, 170001&#10;-0.141905, 8.513097, 5.441484, 170200&#10;-0.026944, 8.206534, 5.594766, 170401&#10;-0.601749, 8.168214, 5.058281, 170601&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Algorithm&lt;/p&gt;&#10;&#10;&lt;p&gt;My proposed algorithm is to set a certain threshold such as line 12 on the sample data when the Y axis hits a certain threshold such as &amp;lt;7 then pass the previous 5 points and the 5 points after that to the NN.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-03-12T18:08:10.917" Id="24527" LastActivityDate="2013-04-25T15:58:21.307" LastEditDate="2012-09-27T12:45:52.613" LastEditorUserId="7290" OwnerUserId="9794" PostTypeId="1" Score="2" Tags="&lt;dataset&gt;&lt;algorithms&gt;&lt;neural-networks&gt;&lt;validation&gt;" Title="Back propagation neural network data input advice" ViewCount="847" />
  <row AnswerCount="2" Body="&lt;p&gt;How is it possible to identify quickly (without doing many tests) an approximative number of clusters from a dataset which is not vary large, even if this value is not the correct number of clusters, I just want to identify a reasonable value representing the number of clusters from this small dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note1: that I don't want to do many tests and/or cross-validate just to find an optimal number of clusters (time is important for me).&lt;/p&gt;&#10;&#10;&lt;p&gt;Note2: I know that there is no way to automatically set the &quot;right&quot; K nor is there a definition of what &quot;right&quot; is. I just want a reasonably approximative value.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-12T18:57:11.013" Id="24530" LastActivityDate="2013-02-11T06:03:26.697" OwnerUserId="8114" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;k-means&gt;" Title="How to identify quickly an aproximative number of clusters from a relatively small dataset" ViewCount="123" />
  <row AcceptedAnswerId="24555" AnswerCount="1" Body="&lt;p&gt;I'm having a little trouble early on using the convert() function in RTAQ to convert .csv taq intraday trade data into an .RData format. I type this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; from &amp;lt;- &quot;2012-02-01&quot;&#10;&amp;gt; to &amp;lt;- &quot;2012-02-29&quot;&#10;&#10;&amp;gt; ###convert data to .RData format&#10;&amp;gt; convert(from, to, datasource = &quot;/home/taylor/Desktop/trading&quot;, &#10;datadestination = &quot;/home/taylor/Desktop/trading&quot;, trades = T, quotes = F, &#10;ticker = &quot;AAPL&quot;, dir = F, extention = &quot;csv&quot;, header = F)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and get this error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[1] &quot;no trades for stock AAPL&quot;&#10;There were 42 warnings (use warnings() to see them)&#10;&amp;gt; warnings()&#10;Warning messages:&#10;1: In file(file, &quot;rt&quot;) :&#10;cannot open file '/home/taylor/Desktop/trading/2012-02-01 &#10;AAPL_trades.csv':     &#10;No such file or directory&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or i type this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; convert(from, to, datasource = &quot;/home/taylor/Desktop/trading/aapl &#10;2012-02.csv&quot;, datadestination = &quot;/home/taylor/Desktop/trading&quot;, trades = T,&#10;quotes = F, ticker = &quot;AAPL&quot;, dir = F, extention = &quot;csv&quot;, header = F)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and get this error&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in setwd(datasource) : cannot change working directory&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My hunch is that the raw data isn't in the anticipated format. In the documentation for RTAQ it mentions that the raw trade data has 9 columns (with no sample picture), however my data has 15 columns. An example row:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;20120201,40000,793,AAPL,P,T...,100,457,,00,1,N,,AAPL,&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The columns are Date, Time (HHMMSS), MS, Symbol, Exchange, Sale Condition, Trade Volume, Trade Price, Trade Stop Stock Indicator, Trade Correction Indicator, Trade Sequence Number, Source of Trade, Trade Reporting Facility, Symbol Root, and Symbol Suffix, respectively. If I have to change the format of the raw data, which columns do i drop/combine?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any help. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-12T19:01:39.807" Id="24531" LastActivityDate="2012-03-13T07:55:26.877" LastEditDate="2012-03-13T07:55:26.877" LastEditorUserId="2116" OwnerUserId="8336" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;dataset&gt;&lt;large-data&gt;&lt;finance&gt;" Title="Trouble using convert() in RTAQ package" ViewCount="259" />
  
  
  
  <row Body="&lt;p&gt;Given your comment, I am guessing that what distinguishes a pothole from a speed bump is largely the vehicle's speed going into the event. I think your idea of looking for outlier Y positions and then passing a surrounding window of data is a great place to start. What this means then is that your training data set will have to have 11 values for each known pattern you have. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your neural network library has a &lt;a href=&quot;http://en.wikipedia.org/wiki/Softmax_activation_function&quot; rel=&quot;nofollow&quot;&gt;softmax activation function&lt;/a&gt; for the output layer, you can perhaps use a single network to learn and identify potholes, speedbumps, and normal roads.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you can train two separate networks: one network would learn/identify good vs potholes, the other good vs speedbumps. Each new pattern would be given to both networks and you'd consider the output pairs (ideally [0,1] or [1,0]).&lt;/p&gt;&#10;&#10;&lt;p&gt;As for how large the training set needs to be, start small (e.g. 10 examples of each case), assess out of sample accuracy (so you'll need to reserve some additional known patterns as a hold out set), and then iteratively improve by adding more training patterns and/or changing the network layout and optimization settings. If you happen to be coding this in Python, &lt;a href=&quot;http://pybrain.org/docs/tutorial/fnn.html&quot; rel=&quot;nofollow&quot;&gt;PyBrain&lt;/a&gt; has some nice example documentation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-13T05:58:33.620" Id="24561" LastActivityDate="2012-03-13T05:58:33.620" OwnerUserId="1080" ParentId="24527" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="24581" AnswerCount="1" Body="&lt;p&gt;In my previous &lt;a href=&quot;http://stats.stackexchange.com/questions/24380/how-to-get-ellipse-region-from-bivariate-normal-distributed-data&quot;&gt;question&lt;/a&gt; I needed to help with ellipse region extraction and determine if point lies in that region or not.&#10;I ended up with this code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ellipse)&#10;library(mvtnorm)&#10;require(spatstat)&#10;&#10;netflow &amp;lt;- read.csv(file=&quot;data.csv&quot;,head=FALSE,sep=&quot; &quot;)&#10;#add headers&#10;names(netflow)&amp;lt;-c('timestamps','flows','flows_tcp','flows_udp','flows_icmp','flows_other','packe ts','packets_tcp','packets_udp','packets_icmp','packets_other','octets','octets_tcp','octets_udp','octets_icmp','octets_other')&#10;attach(netflow)&#10;&#10;#load library&#10;library(sfsmisc)&#10;#plot&#10;plot(packets,flows,type='p',xlim=c(0,500000),ylim=c(0,50000),main=&quot;Dependence number of flows on number of packets&quot;,xlab=&quot;packets&quot;,ylab=&quot;flows&quot;,pch = 16, cex = .3,col=&quot;#0000ff22&quot;,xaxt=&quot;n&quot;)&#10;#Complete the x axis&#10;eaxis(1, padj=-0.5, cex.axis=0.8)&#10;&#10;pktsFlows=subset(na.omit(netflow),select=c(packets,flows))&#10;head(pktsFlows)&#10;#plot(pktsFlows,pch = 16, cex = .3,col=&quot;#0000ff22&quot;)&#10;&#10;cPktsFlows &amp;lt;- apply(pktsFlows, 2, mean)&#10;elpPktsFlows=ellipse::ellipse(var(pktsFlows),centre=cPktsFlows,level=0.8)&#10;&#10;png(file=&quot;graph.png&quot;)&#10;plot(elpPktsFlows,type='l',xlim=c(0,500000), ylim=c(0,50000))&#10;points(pktsFlows,pch = 19, cex = 0.5,col=&quot;#0000FF82&quot;)&#10;grid(ny=10,nx=10)&#10;dev.off()&#10;&#10;W &amp;lt;- owin(poly=elpPktsFlows)&#10;inside.owin(100000,18000,W)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This produces this &lt;a href=&quot;https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png&quot; rel=&quot;nofollow&quot;&gt;graph&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/ellipse.png&quot; alt=&quot;graph ellipse&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the same data with the &lt;a href=&quot;https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png&quot; rel=&quot;nofollow&quot;&gt;regression line plotted&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/density/linRegAll.png&quot; alt=&quot;Plot all with linear regression line&quot;&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you explain me, why the ellipse has this shape? I expected that main axe of ellipse will have the same direction with linear regression line, but it hasn't.&lt;/p&gt;&#10;&#10;&lt;p&gt;Btw. &lt;a href=&quot;https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png&quot; rel=&quot;nofollow&quot;&gt;kernel density estimation&lt;/a&gt; also points to 100000 althought there are no points...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;https://github.com/matejuh/doschecker_wiki_images/raw/master/linear_regression/kernel/kernelPoints.png&quot; alt=&quot;kernel density estimation&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-13T15:14:56.240" FavoriteCount="1" Id="24572" LastActivityDate="2012-03-13T17:20:42.480" LastEditDate="2012-03-13T15:30:17.370" LastEditorUserId="1036" OwnerUserId="9722" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;regression&gt;&lt;density&gt;&lt;bivariate&gt;" Title="Ellipse region shape from bivariate normal distributed data?" ViewCount="723" />
  <row Body="&lt;p&gt;1) The goal of exponential smoothing is to estimate the level.  Since the level changes randomly from one period to the next, but the change has zero mean, this means that estimating the current value of the level, predicting the next value of the level, and predicting all future values of the level, all result in the same estimate / prediction - although of course the standard errors of those three estimations / predictions will be different.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Estimating the level&quot; is equivalent to &quot;predicting the next value of $Y$&quot;, as the difference between the level and the next value of $Y$ is a zero-mean random number, so you can't make a better prediction than just using the estimate of the level.  (Well, if you don't use squared-error loss, and your error distribution isn't normal, you often can, but that's not on this question's topic - and typically it's assumed that you are using squared-error loss and do have normal distributions.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference between your two equations is essentially notational; when using exponential smoothing, you'll always use your most recent data point, and depending upon how you think about the problem, that can be denoted $Y_t$ or $Y_{t-1}$; in either case, though, it's the most recent observation, which is what really matters.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Your approach of jointly determining $y_0$ and $\alpha$ via least squares is just fine - much better than ignoring $y_0$ if you don't have much data.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-13T16:11:02.300" Id="24575" LastActivityDate="2012-03-13T16:11:02.300" OwnerUserId="7555" ParentId="17033" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;For unimodal, more-or-less symmetric distributions, HPD- and quantile-based credible intervals won't be too different. But consider a bimodal posterior distribution with well-separated modes: the HPD-based credible region will be two disjoint intervals whereas the central quantile-based credible region is a single interval by construction.&lt;/p&gt;&#10;&#10;&lt;p&gt;From a decision theory perspective, the two different kinds of intervals correspond to two different loss functions. The big difference is &lt;s&gt;that the HPD corresponds to a loss function that includes a penalty for the length of the credible region(s).&lt;/s&gt; (um, no, &lt;a href=&quot;http://stats.stackexchange.com/questions/24681/what-is-the-decision-theoretic-justification-for-bayesian-credible-interval-proc&quot;&gt;as guest implicitly points out&lt;/a&gt;, it's) that if the interval fails to cover the true value, the loss function for the quantile-based credible interval penalizes you for how wrong you are whereas in the loss function for the HPD interval, &quot;&lt;a href=&quot;http://idioms.thefreedictionary.com/miss+is+as+good+as+a+mile&quot; rel=&quot;nofollow&quot;&gt;a miss is as good as a mile&lt;/a&gt;&quot;.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-13T19:58:34.757" Id="24589" LastActivityDate="2012-03-16T18:51:22.753" LastEditDate="2012-03-16T18:51:22.753" LastEditorUserId="9738" OwnerUserId="9738" ParentId="24588" PostTypeId="2" Score="6" />
&#10;\text{Total probability} &amp;amp;= \sum_{p \in P}\text{area}(p)\text{density}(p) \\
  <row AnswerCount="1" Body="&lt;p&gt;What range of sample size would increase the likelihood of finding significance in a 2x2 chi square? I did not find significant relations between artists (n=36) and non artists (n=20) on 2 levels of mood (mild or severe disorder) with more than 5 in each cell.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-13T22:06:58.237" Id="24593" LastActivityDate="2012-03-13T23:03:55.843" OwnerUserId="9825" PostTypeId="1" Score="-1" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;" Title="What range of sample size would increase the likelihood of finding significance in a 2x3 chi square?" ViewCount="304" />
  <row AcceptedAnswerId="24603" AnswerCount="2" Body="&lt;p&gt;I am currently arguing with someoe on how to correctly treat data with multiple measurements for each subject. In this case data was gathered for each subject within a short time for different conditions within each subject. All measurements gather exactely the same variable, just multiple.&lt;/p&gt;&#10;&#10;&lt;p&gt;One option now is to just group the data by conditions and not care that multiple data points come from one subject. However the data points from each subject are probably not completely independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other alternative is, to first take the mean of all measurements for each condition from each subject and then compare the means. However this will probably impact the significance, since in the final analysis it is not taken into account, that the means have less error.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can you correctly analyse such data? Is this somehow taken care of in SPSS? In principle it should be possible to calculate the error margin when calculating a mean and than considering this in the final analysis, but I do not guess that SPSS is somehow doing this calculation behind my back.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-13T22:53:45.067" FavoriteCount="4" Id="24595" LastActivityDate="2012-03-14T13:27:07.493" LastEditDate="2012-03-13T22:59:45.137" LastEditorUserId="7194" OwnerUserId="7194" PostTypeId="1" Score="4" Tags="&lt;repeated-measures&gt;&lt;spss&gt;&lt;within-subjects&gt;" Title="How to correctly treat multiple data points per each subject" ViewCount="2913" />
  
  <row AcceptedAnswerId="24639" AnswerCount="1" Body="&lt;p&gt;Is there a function in R to calculate the generalized determinant of a singular matrix? (similar to the &lt;code&gt;ginv()&lt;/code&gt; used to compute the generalized inverse)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-14T00:34:03.147" Id="24600" LastActivityDate="2012-03-15T15:25:43.880" LastEditDate="2012-03-15T07:54:09.633" LastEditorUserId="930" OwnerUserId="9828" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;matrix-inverse&gt;" Title="Moore-Penrose generalized determinant" ViewCount="281" />
  
  <row AnswerCount="0" Body="&lt;p&gt;At &lt;a href=&quot;https://github.com/OpenMDAO/OpenMDAO-Framework/issues/599&quot; rel=&quot;nofollow&quot;&gt;https://github.com/OpenMDAO/OpenMDAO-Framework/issues/599&lt;/a&gt; it is stated that non-square Latin Hypercube experimental design is not well defined (I assume that for higher dimensions that means hypercube must have the same length in every dimension). &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering why that is. Would it not be possible to, for example, have a LHC scheme where each row in each dimension had &lt;em&gt;one or more&lt;/em&gt; samples, and the sample space was relatively evenly sampled, with no correlation between variables? I guess this is actually making a hypercube out of a hyperrectangle, but is there any reason that wouldn't work?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I have a bunch of parameters that I want to change in a physical model. Some have 4 states, some only two. Could I make a hypercube where the Parameters with only two states just have those states duplicated (so that there are two model runs with each of those two states)? how about doing the same with 4-, 3- and 2-state parameters? Could I make an LHC scheme where each dimension has length equal to the smallest common multiple of the states count for each parameter (ie. 12)? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-14T01:44:29.000" Id="24604" LastActivityDate="2012-03-31T14:19:28.583" LastEditDate="2012-03-28T23:08:14.203" LastEditorUserId="9007" OwnerUserId="9007" PostTypeId="1" Score="4" Tags="&lt;experiment-design&gt;&lt;latin-square&gt;" Title="Are non-square latin hypercubes viable?" ViewCount="164" />
  <row Body="&lt;p&gt;A simple example would be if you bought a light-bulb with a lifetime which was exponentially distributed with a mean of 1000 days. &lt;/p&gt;&#10;&#10;&lt;p&gt;With a 95% credible region: would you tend to see it as likely to last for between 25 and 3689 days (based on the quantiles at 0.025 and 0.975), or would you see it as likely to last fewer than 2996 days (based on the Highest Density Interval)? &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, would it surprise you if it died almost as soon as you bought it, even though the mode of the distribution is at zero? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-14T01:53:27.790" Id="24606" LastActivityDate="2012-03-14T01:53:27.790" OwnerUserId="2958" ParentId="24588" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I'm pretty sure this is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Secretary_problem&quot;&gt;marriage problem&lt;/a&gt;. The idea is: You need to find a spouse. Researching information about a spouse is hard, and you can only look at one at a time. After some time spent looking (which we assume is constant), you can estimate a SpouseValue, which is how happy you would be married to this person. Then you must either marry the candidate, or move on and look for someone new.&lt;/p&gt;&#10;&#10;&lt;p&gt;The one difference between this and the problem you specify is that in the marriage problem, you have a predefined maximum number of candidates $n$ (after all, you'll have to settle eventually!). Probably this applies to your housing problem too (after all, you need to live somewhere!).&lt;/p&gt;&#10;&#10;&lt;p&gt;The optimal policy for making a decision under this condition is to first assess $\frac{n}{e}$ (that is $e$ the numeric constant) applicants at random, and accept none of them. Then keep interviewing. For each new candidate, determine if they are the best one seen so far. If they are, stop. This is your spouse (or house). Otherwise, keep going until you have to settle.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's it. Keep in mind that the policy is optimal, but not guaranteed to pick the best candidate. You get the best one about a third of the time though, even for large $n$, so it's not half bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Also, this assumes you care more about finding the best house than about the cost of looking...&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-14T04:22:32.250" Id="24612" LastActivityDate="2012-03-14T04:22:32.250" OwnerUserId="6446" ParentId="24611" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;Check out &lt;a href=&quot;http://www.cs205.org/downloads/lectures/Slides/19-ROC_PR_Clustering.pdf&quot; rel=&quot;nofollow&quot;&gt;these slides from cs205.org at Harvard&lt;/a&gt;. Once you get to the section on Error Measures, there is discussion of precision and recall in multi-class settings (e.g., one-vs-all or one-vs-one) and confusion matrices. Confusion matrices is what you really want here.&lt;/p&gt;&#10;&#10;&lt;p&gt;FYI, in the Python software package &lt;a href=&quot;http://scikit-learn.org/stable/&quot; rel=&quot;nofollow&quot;&gt;scikits.learn&lt;/a&gt;, there are built-in methods to automatically compute things like the confusion matrix from classifiers trained on multi-class data. It can probably directly compute precision-recall plots for you too. Worth a look.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-14T14:25:09.140" Id="24630" LastActivityDate="2012-03-14T14:25:09.140" OwnerUserId="8927" ParentId="21551" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;code&gt;glmnet&lt;/code&gt; is more setup to solve the first formulation of the L1 penalty instead of the size constraint.  &lt;code&gt;glmnet&lt;/code&gt; has a fairly unique method of solving the problem, but is probably the best out there.  For most applications, you just feed it your design matrix in the &lt;code&gt;x&lt;/code&gt; parameter and the response in the &lt;code&gt;y&lt;/code&gt; parameter.  The &lt;code&gt;x&lt;/code&gt; needs to already have factors built into dummies/contrasts.  I would usually recommend doing full dummy coding instead of the default treatment coding unless you have a true reference category to shrink towards.&lt;/p&gt;&#10;&#10;&lt;p&gt;You need to then set your alpha to choose the balance between Lasso vs Ridge.  Alpha = 1 will give you full LASSO.  Usually you would want alpha=0.95 to have a touch of ridging to handle any lingering multicollinearity and just give a bit better conditioning to the problem in general.  If you're more focused on predictive accuracy than parsimony then setting alpha much further towards the ridge side of things is usually better (Alpha ~ 0.05).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;glmnet&lt;/code&gt; will then fit a whole string of lambda values.  It starts with the smallest lambda that produces an intercept only model and then slowly lowers the lambda by ~8 orders of magnitude, making a handful of discrete estimates along the way.  You can control how deep it goes and how many stops via some lambda specific parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, you'll probably want to use the &lt;code&gt;cv.glmnet&lt;/code&gt; function to pick a good value for lambda.  The cross validation will give you a good estimate of lambda to use when making predictions on new data.  Make sure the resulting lambda sequence reaches a minimum.  It usually produces a nice visual of the bias-variance tradeoff, but sometimes you'll have to create a longer sequence of lambdas if not much penalty is really needed.  Just be sure your custom sequence starts with an intercept-only model and progresses smoothly like the default sequence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of minor notes, do be sure you choose the correct distribution for your response.  And do know that the design matrix can be passed in sparse format if you have some factors with many levels.  And read the useful vignettes available on the CRAN &lt;code&gt;glmnet&lt;/code&gt; webpage. Good luck!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-14T14:31:54.340" Id="24631" LastActivityDate="2012-03-14T14:31:54.340" OwnerUserId="8120" ParentId="24623" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I browsed through lots of literature on &lt;strong&gt;Linear Discriminant Analysis&lt;/strong&gt; on the web. I can say that, I have understood most of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I understood. You take the data, calculate mean of the &lt;strong&gt;data&lt;/strong&gt;, &lt;strong&gt;mean per class&lt;/strong&gt;, &lt;strong&gt;scatter between classes&lt;/strong&gt;, &lt;strong&gt;scatter within a class&lt;/strong&gt;. Use this information to project multi-dimensional data onto a single line and hence reduce dimensionality.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I don't understand how do I use it for &lt;strong&gt;training and testing of the data sets&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I know that &lt;strong&gt;Y = X dot W&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Y is projected data&#10;X is original data and&#10;W is the calculated vector used for projection.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, do I save &lt;strong&gt;W&lt;/strong&gt; and use it to project new data? &lt;/p&gt;&#10;&#10;&lt;p&gt;I hope I am on the right track!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-14T17:56:39.937" Id="24641" LastActivityDate="2012-03-14T17:56:39.937" OwnerUserId="7918" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;pattern-recognition&gt;" Title="Linear Discriminant Analysis for Training and Testing of data sets" ViewCount="211" />
  <row Body="&lt;p&gt;Lorenz curve is also known under the name of &quot;&lt;a href=&quot;http://mrvar.fdv.uni-lj.si/pub/mz/mz3.1/vuk.pdf&quot; rel=&quot;nofollow&quot;&gt;lift curve&lt;/a&gt;&quot; when applied to classification/ranking. For a given range of predicted probability values, the lift represents a multiplicative increase in the positive class's rate (due to a given predictive model) over a random guess.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rocr.bioinf.mpi-sb.mpg.de/ROCR.pdf&quot; rel=&quot;nofollow&quot;&gt;rocr package&lt;/a&gt; can calculate lift values/curves (the manual also has a concise definition of the lift). The Gini index can be calculated from the area under the lift curve (I typically use cumulative lift value at a given predicted probability threshold instead since it is easier to relate to business metrics) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-14T18:36:10.327" Id="24647" LastActivityDate="2012-03-14T18:36:10.327" OwnerUserId="6129" ParentId="24325" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="24686" AnswerCount="1" Body="&lt;p&gt;This question is a follow up to &lt;a href=&quot;http://stats.stackexchange.com/questions/24595/how-to-correctly-treat-multiple-data-points-per-each-subject&quot;&gt;the question I asked yesterday&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the current situation again. I have data for a within subject measurement, where each variable is measured repeatedly. The repetitions are used to cancel out random fluctuation of the measured variable. Each subject was exposed to each of the conditions, so I have multiple datapoints per condition and subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently the data is organized like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Subject; Cond1_rep1; Cond1_rep2; ... Cond1_rep20; Cond1_mean; Cond2_rep1; Cond2_rep2; ... Cond2_rep20; Cond2_mean ... Cond8_rep1; Cond8_rep2 ... Con8_rep20; Cond8_mean&#10;1      ; ....&#10;2      ; ....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I have eight Conditions, and around 20 repetitions per condition per subject. Currently there are about 20 subjects to be included in the analysis (no removal of outliers and other data cleanup is done yet). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now for analysis the first attempt would be to do a multivariate ANOVA on all the means for all different conditions to see if there is any difference in the distributions. However if I use means like this I will use the fact, that each value probably has a very low error and the likelihood of a false result is actualy lower than estimated by the ANOVA. So I should somehow include the repetitions in the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;The suggestion I was given in the previous question was to use a generalized linear model with repetitions and include all the repetitions in the analysis. However in this case I would need an additional variable identifying the conditions to be used as an independent variable in the generalized linear model. So I would have to somehow add this in my data.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I would try would be first to split up the eight conditions. The eight conditions come from variations of three independent factors, so I can easily split them up. Then I would separate my data into multiple rows, so I can introduce a condition variable. This way I would organize like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Subject; Factor1; Factor2; Factor3; Rep1; Rep2; ... Rep20&#10;1      ; 0      ; 0      ; 0      ; ...&#10;1      ; 1      ; 0      ; 0      ; ...&#10;1      ; 0      ; 1      ; 0      ; ...&#10;1      ; 1      ; 1      ; 0      ; ...&#10;...&#10;1      ; 1      ; 1      ; 1      ; ...&#10;2      ; 0      ; 0      ; 0      ; ...&#10;2      ; 1      ; 0      ; 0      ; ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This way I would get eight rows per subject with a different combination of factors. In this case I can do a repeated measures GLM with the factors as independent variables and the 20 repeated measurements as repeated dependent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would this setup be correct or are there any drawbacks? If there are, is there any better way of analysing this data without loosing the fact that each measurement was repeated multiple time (as when you take the mean first).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not so much interested in a way to restructure the data like this, but my main question if there could be a problem when I do a GLM like this, without taking into account that some data points where gathered from the same person. The final analysis in SPSS should not be a problem.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-03-14T21:37:55.870" FavoriteCount="1" Id="24662" LastActivityDate="2012-03-15T15:41:02.797" LastEditDate="2012-03-15T09:41:19.720" LastEditorUserId="7194" OwnerUserId="7194" PostTypeId="1" Score="2" Tags="&lt;repeated-measures&gt;&lt;dataset&gt;&lt;generalized-linear-model&gt;&lt;within-subjects&gt;" Title="How to organize the data for this repeated-measure within-subject setup?" ViewCount="2673" />
  <row Body="&lt;p&gt;With Latin hypercube samples, you have to decide on the number of samples, so that you break your range into either 10 or 20 bins to begin with. Otherwise, you will likely miss some parts of your space. My understanding is that the only quasi-Monte Carlo method that allows to take the next sample (or next $N$ samples) easily without trying to figure out their dependence on the previously collected samples is &lt;a href=&quot;http://en.wikipedia.org/wiki/Halton_sequence&quot; rel=&quot;nofollow&quot;&gt;Halton sequence&lt;/a&gt;. See the encompassing treatment in &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0898712955&quot; rel=&quot;nofollow&quot;&gt;Niederreiter (1992)&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, as far as I recall from computational physics literature (most likely, it was in &lt;a href=&quot;http://dx.doi.org/10.1006/jcph.1995.1209&quot; rel=&quot;nofollow&quot;&gt;Morokoff and Caflisch (1995)&lt;/a&gt;), for the sequence of length up to about $6^d$ where $d$ is the dimension of your space, quasi Monte Carlo sequences do not show appreciable gains over the standard pseudo-random number generators. So you may not have to bother with LHC and agonize over the choice between 10 and 20 samples -- you can just start with any random number generator you have at hand, and keep adding new ones if you are not satisfied with the achieved precision.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-15T02:11:37.713" Id="24674" LastActivityDate="2012-03-20T02:07:34.410" LastEditDate="2012-03-20T02:07:34.410" LastEditorUserId="9007" OwnerUserId="5739" ParentId="24605" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Another trick in addition to the Randomized Response method whuber mentioned is a case where the respondent can effectively conceal sensitive information in the form of an aggregate response. The researcher can then back out some measures of the sensitive information across the whole population, but not at the individual level. I can't remember what this is called either, but here's how you would do it:&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, say you were trying to measure the incidence of erectile dysfunction. This is something many people would be reluctant to admit and a yes/no question would be ineffective. You could randomly include/not include ED in a list of, say, 7 other ailments, some common, some uncommon. You could then ask a question like &quot;Out of the following list of 7 ailments, &lt;em&gt;how many&lt;/em&gt; have you personally experienced within the last 90 days?&quot; Having kept track of the respondents who were given the list with/without ED, you might find that people listed on average 3.2 ailments in the ED-less list, and 3.7 ailments in the ED-containing list. This would give an expected incidence of 50%.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can see that one of the drawbacks is the error. If everybody had ED then the mean would be expected to be 1 higher in the longer list, and if nobody had ED the means would be expected to  be identical. So you are measuring a difference of two means on the range of 0 to 1, so it is necessary to have a large sample size, so the confidence interval for each mean is much smaller than 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be important to carefully select the other ailments - you would not want hardly anybody to need to give a &quot;1&quot; for only the sensitive response, because then it may not feel as concealed for the respondent, and similarly, you would not want anybody to need to give a &quot;7&quot;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-15T02:33:58.707" Id="24675" LastActivityDate="2012-03-15T02:33:58.707" OwnerUserId="3331" ParentId="24657" PostTypeId="2" Score="7" />
  
  
  <row Body="&lt;p&gt;If your interviewers have completed more than 20 interviews you might consider the Wald-Wolfowitz test for randomness. Falsified data has a tendency to mix or cluster whereas randomly sampled data should exhibit a random order. Note that this only works for dichotomous variables. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-15T13:20:56.250" Id="24696" LastActivityDate="2012-03-15T13:20:56.250" OwnerUserId="9872" ParentId="22514" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;While studying ANOVA analysis, I meet an example related to the ANOVA table given in the attached figure&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VuoaG.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$Y$ stands for change in hemoglobin (%)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$X_1$ stands for duration of the operation (min)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$X_2$ stands for blood loss (ml)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Based on this ANOVA table, several arguments are made:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$X_2$ has a significant linear association with $Y$ with or without including $X_1$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$X_1$ has a significant linear association&#10;with $Y$ after adjusting for linear effects of blood loss X2 on both $X_1$ and $Y$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Without adjusting for linear effects of $X_2$, the linear relationship between $Y$ and $X_1$ was not quite significant&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I am quite confusing on how to generate these three arguments based on the information of ANOVA table? I think it should have connection with those P-values, but do not know how?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-15T15:15:47.947" Id="24701" LastActivityDate="2014-04-27T09:38:58.513" LastEditDate="2014-04-27T09:38:58.513" LastEditorUserId="930" OwnerUserId="3125" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;interpretation&gt;" Title="Capturing the relationships among model variables based on ANOVA table" ViewCount="76" />
  <row AnswerCount="1" Body="&lt;p&gt;I have the following hypothesis:&lt;/p&gt;&#10;&#10;&lt;p&gt;$h_0 : \frac {\sum {a_{i}}} {\sum {b_{i}}} = x \\
  
  
  <row AcceptedAnswerId="25118" AnswerCount="2" Body="&lt;p&gt;I have independent observations $Y_1, Y_2, ...., Y_n$ that take values 1 or 0 with &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(Y_i = 1) = \frac{1}{1 + \exp\{-(\alpha + \beta x_i)\}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\alpha$ and $\beta$ are unknown constant and $x_1, x_2, ....,x_n$ are known, I have to show that $(\sum Y_i,\sum x_i Y_i)$is a minimal sufficient statistics for $(\alpha,\beta)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We know that any statistics $T(Y)$ is minimal sufficient if $\frac{\prod_{i=1}^nf(y_i;\alpha, \beta)}{\prod_{i=1}^nf(z_i;\alpha, \beta)}$ is independent of $\alpha$ and $\beta$ if and only if $T(Y) = T(Z)$. But here the probability density of $Y$ is independent of $y$. Is there any other method to show the minimal statistics? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-16T00:52:33.747" Id="24729" LastActivityDate="2013-01-17T09:33:45.393" LastEditDate="2012-03-23T11:39:38.383" LastEditorDisplayName="user9410" LastEditorUserId="88" OwnerUserId="9158" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;mathematical-statistics&gt;" Title="Sufficient statistics" ViewCount="417" />
  
  
  <row Body="&lt;p&gt;i) Yes, you are doing right.&#10;ii) the coefficient for the missing level is simply &lt;strong&gt;0&lt;/strong&gt;.&#10;iii) because you don't have intercept in your model (the -1 does this in your formula), one level becomes your base (psex1 in your first model and bgrp1 in your second model). If you allow intercept (remove -1 from your formula), you will see 2 levels 'missing'.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-16T06:34:23.790" Id="24741" LastActivityDate="2012-03-16T06:34:23.790" OwnerUserId="5461" ParentId="24740" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here's an example of how to get the results of the calculation and add the equation of the fitted line.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x&amp;lt;-1:5&#10;y&amp;lt;-2*x+rnorm(5)&#10;plot(x, y)&#10;&#10;# Regression:&#10;m&amp;lt;-lm(y~x)&#10;abline(coef(m), col=2)&#10;&#10;# Summary of calculated values:&#10;summary(m)&#10;&#10;# Add equation of fitted line to the plot using the text command&#10;# Regression coefficients rounded to three decimal places&#10;text(2.5,8,paste(&quot;y=&quot;,round(coef(m)[1],3),&quot;+&quot;,round(coef(m)[2],3),&quot;x&quot;,sep=&quot;&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-03-16T07:33:39.040" Id="24743" LastActivityDate="2012-03-16T07:33:39.040" OwnerUserId="8507" ParentId="24731" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Actually in most cases it's best not to convert the usage numbers into ratings, but to work with the original numbers instead.  For one thing, you lose information in categorizing, e.g. you can no longer make the distinction between someone 'very interested' and someone 'extremely interested.'  More than that, the categorization process will involve some arbitrary decisions:  where exactly should the line be between one category and another?  And any arbitrariness in your data means their reliability will be undercut.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you still feel you need to create these categories, there is no single best method for doing so.  You just need to come up with one that seems sound, logical, and consistent to those who will be working with the data and those who will be viewing the results of any analyses.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's some good information &lt;a href=&quot;http://stats.stackexchange.com/questions/16565/what-is-the-effect-of-dichotomising-variables&quot;&gt;here&lt;/a&gt; that relates to this question, although it deals with the specific topic of categorizing into just 2 levels.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-16T09:45:49.547" Id="24747" LastActivityDate="2012-03-16T09:45:49.547" OwnerUserId="2669" ParentId="24742" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Do you mean the coefficients?&lt;/p&gt;&#10;&#10;&lt;p&gt;Extending @MånsT example.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x&amp;lt;-1:5&#10;y&amp;lt;-2*x+rnorm(5)&#10;&#10;# Regression:&#10;m&amp;lt;-lm(y~x)&#10;names(m)&#10;m$coefficients&#10;&#10;&amp;gt; m$coefficients&#10;(Intercept)           x &#10; -0.7811257   2.2952913&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-03-16T14:58:41.647" Id="24761" LastActivityDate="2012-03-16T14:58:41.647" OwnerUserId="3847" ParentId="24731" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;(A cross &lt;a href=&quot;http://math.stackexchange.com/questions/120970/bayesian-inference-on-partitioned-multivariate-gaussian&quot;&gt;post&lt;/a&gt; after finding more appropriate tags here.)&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is on Bayesian inference of partitioned multivariate Gaussian. To make things easier, suppose there is a 2-dimensional Guassian,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we have an observation $x_1$ for $X_1$. By Bayesian inference we get,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and by partitioned Gaussian we have,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  <row Body="&lt;p&gt;You can easily turn this into an odds ratio by calculating &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\; \frac{16}{10000-16} \;}{ \;\frac{10}{10000-10} \; }$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;and although this is not exactly $1.6$, it is close at about $1.60096$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As to whether &quot;men have a much higher risk of heart disease than women&quot;, I would not say so: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;because I think it sometimes misleading to say that two very small numbers are very different (similarly I would not say that somebody with net assets worth $\$10$ was &quot;much richer&quot; than somebody with net assets of $\$5$); and &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;because the fact that out of $26$ people found to have heart disease $16$ were men is not significantly different from a hypothesis that half might be male, using for example the &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_test&quot; rel=&quot;nofollow&quot;&gt;binomial test&lt;/a&gt;  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-03-17T01:20:48.867" Id="24790" LastActivityDate="2012-03-17T01:20:48.867" OwnerUserId="2958" ParentId="24788" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have been looking into the random trees and decision trees in OpenCv and something that still confuses me is the differences between regression and classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone outline what the differences are, when would one use one over the other?&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason for me asking is that i am using classification for labeling a multilabel class of a dataset, and then i ran across the code for the article &quot;Object Classification using Hough Forest&quot; and its source code. In their code, the methods are called regression and its also doing a multiclass labeling, so now i am not sure if I know the difference?&lt;/p&gt;&#10;&#10;&lt;p&gt;A little more about my case:&#10;I am extracting N descriptors from images, each descriptor are different computed features from an image patch, forming a 1x8096 descriptor. Each descriptor are labelled with a label 0:4. I then use OpenCV RandomTrees to train and classify. Looking at the code, its using the methods for ordered classification. It also have categorical classification/regression and categorical regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;So from what i read, would the right term not be that im using a regression, my values can be ordered continuous: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In brief: &lt;/p&gt;&#10;  &#10;  &lt;ol&gt;&#10;  &lt;li&gt;Classification trees have dependent&#10;  variables that are categorical and unordered&lt;/li&gt;&#10;  &lt;li&gt;Regression trees have&#10;  dependent variables that are continuous values or ordered whole&#10;  values.&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.differencebetween.com/difference-between-classification-and-vs-regression/&quot; rel=&quot;nofollow&quot;&gt;http://www.differencebetween.com/difference-between-classification-and-vs-regression/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-17T14:03:08.340" Id="24804" LastActivityDate="2012-03-17T16:37:25.923" LastEditDate="2012-03-17T16:37:25.923" LastEditorUserId="88" OwnerUserId="9923" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;classification&gt;&lt;cart&gt;" Title="Regression vs classification within decision trees in OpenCV" ViewCount="1027" />
  
  <row Body="&lt;p&gt;+1 to @PeterFlom for an excellent (and concise) answer that hits the main points. As Peter correctly points out, using backward elimination as a model selection technique will lead to a lot of problems.  It may be helpful to you to understand why that is true.  My answer to a similar question &lt;a href=&quot;http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856&quot;&gt;here&lt;/a&gt; might be useful in that regard.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to say a couple other things to complement Peter's answer.  First, since your predictors are counts, it may be worthwhile to take the log before using them.  This is often done with counts.  Basically, the idea would be that there are diminishing returns: going from 5 mentions in a given day to 10 is a bigger jump than going from 155 to 160.  It is always possible that this isn't true, but it's something to think about.  There is a lot of good information about the use and effects of the log transformation &lt;a href=&quot;http://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va&quot;&gt;here&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My other note is that reducing the number of predictor variables in the manner you go about it may not be strictly necessary.  A different approach is to combine predictors &lt;em&gt;before&lt;/em&gt; looking at the response variable.  For example, it may be possible to conduct a &lt;a href=&quot;http://www.psych.cornell.edu/darlington/factor.htm&quot; rel=&quot;nofollow&quot;&gt;factor analysis&lt;/a&gt; on your predictor variables and then use just a handful of robust factors as the predictors in your model.  This could provide some of the benefits you hope for without the problems Peter mentions.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-17T18:13:01.033" Id="24811" LastActivityDate="2012-03-17T18:13:01.033" OwnerUserId="7290" ParentId="24752" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This will be an R-centric answer.  One approach is to wrap the call to &lt;code&gt;lm&lt;/code&gt; in a function which is passed the breakpoint and constructs a regression conditional upon that breakpoint, then minimize the deviance of the fitted model conditional upon the breakpoint by just iterating over the possible values for the breakpoint. This maximizes the &lt;a href=&quot;http://www.stat.tamu.edu/~suhasini/teaching613/profile_likelihood.pdf&quot; rel=&quot;nofollow&quot;&gt;profile log likelihood&lt;/a&gt; for the breakpoint, and, in general (i.e., not just for this problem) if the function interior to the breakpoint iteration (lm in this case) finds maximum likelihood estimates conditional upon the parameter passed to it, the whole procedure finds the joint maximum likelihood estimates for all the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# True model: y = a + b*(obs. no &amp;gt;= shift) + c*x&#10;# a = 0, b = 1, c = 1, shift at observation 31&#10;&#10;# Construct sample data&#10;x &amp;lt;- rnorm(100)&#10;shift &amp;lt;- c(rep(0,30),rep(1,70))&#10;y &amp;lt;- shift + x + rnorm(100)&#10;&#10;# Find deviance conditional upon breakpoint&#10;lm.shift &amp;lt;- function(y, x, shift.obs) {&#10;  shift.var &amp;lt;- c(rep(0, (shift.obs-1)), rep(1, length(y)-shift.obs+1))&#10;  deviance(lm(y~x+shift.var))&#10;}&#10;&#10;# Find deviance of all breakpoint values &#10;dev.value &amp;lt;- rep(0, length(y))&#10;for (i in 1:length(y)) {&#10;  dev.value[i] &amp;lt;- lm.shift(y, x, i)&#10;}&#10;&#10;# Calculate profile-ll based confidence interval&#10;estimate &amp;lt;- which.min(dev.value)&#10;profile.95.dev &amp;lt;- min(dev.value) + qchisq(0.95,1)&#10;est.lb.95 &amp;lt;- max(which(dev.value[1:estimate] &amp;gt; profile.95.dev))&#10;est.ub.95 &amp;lt;- est -1 + min(which(dev.value[estimate:length(y)] &amp;gt; profile.95.dev))&#10;&#10;&amp;gt; estimate&#10;[1] 30&#10;&amp;gt; est.lb.95&#10;[1] 28&#10;&amp;gt; est.ub.95&#10;[1] 33&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So our estimate is 30 with a 95% confidence interval of 28 - 33.  Pretty tight, but that was a pretty big shift relative to the standard deviation of the error term too.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note some messiness is involved in calculating the profile log-likelihood based confidence interval, but the basic idea is to find the largest index less than the estimate with a deviance greater than the cutoff level for the lower bound and the smallest index larger than the estimate with a deviance greater than the cutoff level for the upper bound.&lt;/p&gt;&#10;&#10;&lt;p&gt;One really should plot the deviance curve out, just to make sure you don't have multiple local minima that are close to as good as each other, which might tell you something interesting about the assumed model (or the data):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ttksv.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-17T20:42:43.517" Id="24816" LastActivityDate="2012-03-17T20:42:43.517" OwnerUserId="7555" ParentId="24810" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There's some ambiguity. Suppose they separate all the Justin Bieber tweets (JBT), and then they give you exactly 10%.  Then you divide that value (50k) by the sample rate (10%), get 500k, the original number, with NO statistical error.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the filter is applied to the tweets, and then they evaluate whether they are JBT (which is what is implied by your question) then you have a simple form of a Horvitz–Thompson estimator. The reason for mentioning that is that the HT estimators can handle various probabilities; that info can come in handy if you change the sampling rate over time (for budget purposes, for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;On page 27 of &lt;a href=&quot;http://www.math.umt.edu/patterson/549/Horvitz-Thompson.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.math.umt.edu/patterson/549/Horvitz-Thompson.pdf&lt;/a&gt; there is a formula for the variance of the total (i.e. around the 500,000 JBT).&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's look at that formula: &#10;&lt;img src=&quot;http://i.stack.imgur.com/qK0Tj.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the first term (to the left of the +) we know pi=.1 (probability of selection 10%). Since y=1 if it's a JBT and 0 otherwise, the fact that we don't know v (the sample including both JBT and non-JBT) doesn't matter. Further, the second term (to the right of the +) is the covariance term, but but with a 10% sample this can be ignored.  This gives a standard error for the population estimate of 500,000 of 2,225.&lt;/p&gt;&#10;&#10;&lt;p&gt;We can check the reasonableness of this estimate by assuming various number of total Tweets on any topic (from say, 5e6 to 5e10) and seeing what we get from a simple binomial. This varies from 2,121 to 2,236 -- i.e. it doesn't vary much at all. (which is good because we don't know that number).&lt;/p&gt;&#10;&#10;&lt;p&gt;Working this through to a sampling rate for your requested confidence interval gives a sampling rate of 1.9%&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-17T23:03:58.657" Id="24817" LastActivityDate="2012-03-19T15:07:35.920" LastEditDate="2012-03-19T15:07:35.920" LastEditorUserId="3919" OwnerUserId="3919" ParentId="24815" PostTypeId="2" Score="2" />
&#10;1\\
  <row Body="&lt;p&gt;1): Including derived features is a way to inject expert knowledge into the training process, and so to accelerate it. For example, I work with physicists a lot in my research. When I'm building an optimization model, they'll give me 3 or 4 parameters, but they usually also know certain forms that are supposed to appear in the equation. For example, I might get variables $n$ and $l$, but the expert knows that $n*l$ is important. By including it as a feature, I save the model the extra effort of finding out that $n*l$ is important. Granted, sometimes domain experts are wrong, but in my experience, they usually know what they're talking about.&lt;/p&gt;&#10;&#10;&lt;p&gt;2): There are two reasons I know of for this. First, if you have thousands of features supplied (as often happens in real world data), and are short on CPU time for training (also a common occurrence), you can use a number of different feature selection algorithms to pare down the feature space in advance. The principled approaches to this often use information-theoretic measures to select the features with the highest predictive power. Second, even if you can afford to train on all the data and all the features you have, neural networks are often criticized for being 'black box' models. Reducing the feature space in advance can help to mitigate this issue. For example, a user looking at the NN cannot easily tell whether a weight of 0.01 means &quot;0, but the optimization process didn't quite get there&quot; or &quot;This feature is important, but has to be reduced in value prior to use&quot;. Using feature selection in advance to remove useless features makes this less of an issue.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-18T03:50:10.267" Id="24831" LastActivityDate="2012-03-18T03:50:10.267" OwnerUserId="6446" ParentId="24825" PostTypeId="2" Score="3" />
&#10;  f_{X\mid C}(x\mid 2) = \frac{1}{4\pi\sqrt{2}} \exp\left(-\frac{1}{2}\left(\frac{(x_1-2)^2}{4} + \frac{(x_2-4)^2}{2} \right)\right) \, .
&#10;  \log f_{X\mid C}(x\mid 2) - \log f_{X\mid C}(x\mid 1) &amp;lt; 0 \, ,
  <row AcceptedAnswerId="24846" AnswerCount="2" Body="&lt;p&gt;I need to determine if there is any relationship between two count variables. &#10;I have 60+ observations for 4 variables and I want to see if any of the pairs of these variables are significantly correlated with one another. &lt;/p&gt;&#10;&#10;&lt;p&gt;Mostly I use R, so forgive me if you're not familiar.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been using the &lt;code&gt;cor(...,method=&quot;pearson&quot;)&lt;/code&gt; and &lt;code&gt;cor.test()&lt;/code&gt; functions to test each pair, but now I'm not so sure that this is the right approach/test. &#10;Would a non-linear regression like &lt;code&gt;glm(...,family=&quot;poisson&quot;)&lt;/code&gt; be more appropriate?&lt;/p&gt;&#10;&#10;&lt;p&gt;I started thinking like this because when I looked at a histogram of the counts across my observations, I noticed that there seemed to be a slight tendency for the pink and green variables to go up and down to together.&lt;/p&gt;&#10;&#10;&lt;p&gt;I produced a scatter plot of each of the variables plotted against each of the other variables. I used the tests mention above to try and quantify this relationship and to test weather it was real or just noise. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rNFTE.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/iZ3hW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-18T13:59:33.303" Id="24840" LastActivityDate="2012-03-19T09:46:51.660" LastEditDate="2012-03-19T09:46:51.660" LastEditorDisplayName="user5644" OwnerUserId="9928" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;correlation&gt;" Title="What type of test to use to determine correlation/relationship between two non-continuous varaibles" ViewCount="2090" />
  
  <row Body="&lt;p&gt;Here's a reformulation of your model broken down by component assumptions.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;We have an unknown matrix of win probabilities, $P=(p_{ij})$, indexed from 1 to 5, where $0 \leq p_{ij} \leq 1$ and $p_{ij}=1-p_{ji}$ for all $i$, $j$.  For every $(i,j)$ with $i \leq j$, we observe $n_{ij}$ independent draws from each corresponding Bernoulli($p_{ij}$) distribution. &lt;/li&gt;&#10;&lt;li&gt;There is some ranking $r(1),...,r(5)$ of the indices where $r(1)$ is the index of the &quot;best&quot; character and $r(5)$ is the index of the worst character. More precisely, we have (i) $p_{r(i)r(j)}\leq 1/2$ for all $i \leq j$ and (ii) $p_{r(i)r(j)} \geq p_{r(i)r(k)}$ for all $j \leq k$.&lt;/li&gt;&#10;&lt;li&gt;We observe &quot;size&quot; covariates $s_i$, and would like to find a relationship of the form $p_{ij}= f(s_i-s_j) + \epsilon$ where $f$ is some monotonic function and $\epsilon$ is a small error term.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I will describe a simple way of doing it using Maximum Likelihood, before outlining a more technically difficult Bayesian approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Maximum Likelihood&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Background: see &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_function&quot; rel=&quot;nofollow&quot;&gt;definition of logit&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/EM_algorithm&quot; rel=&quot;nofollow&quot;&gt;EM Algorithm&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\tilde{P}=(p_{r^{-1}(i)r^{-1}(j)})$ be the matrix $P$ permuted so that the indices correspond to the true ranking.  Reparameterize $\tilde{P}$ by writing&#10;$$
&#10;$$&#10;for $i &amp;lt; j$. This is done so that your transformed variables $u_{21},u_{31},u_{32},...,u_{54}$ have a 1-1 relationship with valid $\tilde{P}$ but can take any real values.&lt;br&gt;&#10;For every possible ranking, use the EM algorithm to find the likelihood of the ranking, and keep the ranking which has the highest likelihood.  For this ranking, use the maximum likelihood estimates of $u_{21},...,u_{54}$ to convert back to the ML estimate of the matrix $P$, which we call $\hat{P}=(\hat{p}_{ij})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, to determine the &quot;consistency&quot; of the relationship between $p_{ij}$ and $s_i-s_j$, carry out a logistic regression $\hat{p}_{ij} \sim \text{logit}(\beta_0 + \beta_1 (s_i-s_j))$ for all $i &amp;lt; j$.  The residuals give you an idea of the consistency.  You can try high-order regression formulae, (eg quadratic) but then you may not preserve monotonicity.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Bayesian Approach&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Specify a prior on $P$ by assigning uniform (or optionally, beta-weighted) probability to all 5 by 5 matrices with entries in $[0,1]$ which satisfy condition 2, and zero probability to all such matrices which violate condition 2; this gives you the prior density $p(P)$.  Sample from the posterior be using resampling (see any intro text on Markov Chain Monte Carlo).  From each posterior draw, compute the posterior of the sum or residuals (or whatever measure of consistency you use) based on priors for the regression coefficients $\beta_0, \beta_1$, weighting by the likelihood&#10;$$
  
  
  
  <row Body="&lt;p&gt;It is called &lt;a href=&quot;http://en.wikipedia.org/wiki/Distributed_lag&quot; rel=&quot;nofollow&quot;&gt;distributed lag&lt;/a&gt; model. The model from of your example can be estimated with simple linear regression. You can use &lt;em&gt;dynlm&lt;/em&gt; package for easier lag handling:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(dynlm)&#10;&amp;gt; n&amp;lt;-4 ##Number of lag used in a model&#10;&amp;gt; data(&quot;USDistLag&quot;, package = &quot;lmtest&quot;)&#10;&amp;gt; dynlm(consumption~L(gnp,0:4))&#10;&#10;Time series regression with &quot;ts&quot; data:&#10;Start = 1967, End = 1982&#10;&#10;Call:&#10;dynlm(formula = consumption ~ L(gnp, 0:4), data = USDistLag)&#10;&#10;Coefficients:&#10;        (Intercept)  L(gnp, 0:4)Series 1  L(gnp, 0:4)Series 2  L(gnp, 0:4)Series 3  &#10;          -30.54113              0.46253              0.09632             -0.06309  &#10;L(gnp, 0:4)Series 4  L(gnp, 0:4)Series 5  &#10;            0.13511              0.03176  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-03-19T07:08:25.663" Id="24873" LastActivityDate="2012-03-19T07:08:25.663" OwnerUserId="2116" ParentId="24869" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I have an array of $n$ real values, which has mean $\mu_{old}$ and standard deviation $\sigma_{old}$. If an element of the array $x_i$ is replaced by another element $x_j$, then new mean will be&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$\mu_{new}=\mu_{old}+\frac{x_j-x_i}{n}$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Advantage of this approach is it requires constant computation regardless of value of $n$. Is there any approach to calculate $\sigma_{new}$ using $\sigma_{old}$ like the computation of $\mu_{new}$ using $\mu_{old}$? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-19T11:24:52.187" FavoriteCount="2" Id="24878" LastActivityDate="2014-05-27T14:21:04.887" LastEditDate="2012-03-19T12:51:42.700" LastEditorUserId="4319" OwnerUserId="4319" PostTypeId="1" Score="12" Tags="&lt;standard-deviation&gt;&lt;online&gt;" Title="Computation of new standard deviation using old standard deviation after change in dataset" ViewCount="2181" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have seen it claimed in Hosmer &amp;amp; Lemeshow (and elsewhere) that least squares parameter estimation in logistic regression is suboptimal (does not lead to a minimum variance unbiased estimator).  Does anyone know other books/articles that show/discuss this explicitly?  Google and my personal library have not helped me here...&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-19T21:40:41.537" FavoriteCount="3" Id="24904" LastActivityDate="2012-12-22T10:04:41.737" OwnerUserId="9968" PostTypeId="1" Score="18" Tags="&lt;least-squares&gt;&lt;logistic&gt;" Title="Least squares logistic regression" ViewCount="1616" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been trying to get answers for a while, and have looked up every help section and tried other stats sites. I have a thesis that I need to be completing shortly, but one set of data has been completely out of my league due to a small sample size. &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I have four broods (birds with chicks - precocial so they can peck at hatch e.g. chickens). I measured the proportion of time each brood fed every other day (approximately) until they flew. I had three broods in the ocean front and one brood in the mudflat habitats. &lt;/p&gt;&#10;&#10;&lt;p&gt;Each data point within a brood is non-independent, so the only way to compare the mf brood against the OF broods in terms of time spent feeding (and other behaviours) would be to use the mean and use a Kruskal-Wallis or a one-sample Wilcoxon (mu=mean for MF brood). Of course the sample is too small to get anywhere near significance even though there is a clear pattern of MF feeding more and being less disturbed when I look at all the data points. &lt;/p&gt;&#10;&#10;&lt;p&gt;It was suggested that I could use a mixed effects model so that each data point for a brood could be used and each brood would be considered a block. The person typed out some ideas for models but no one could tell me how to do it step-by-step and I don't really understand them. I found a source which would let me do it in Minitab (&lt;a href=&quot;https://onlinecourses.science.psu.edu/stat502/node/72&quot; rel=&quot;nofollow&quot;&gt;https://onlinecourses.science.psu.edu/stat502/node/72&lt;/a&gt;) but when I tried it it told me that it is unbalanced and I think it is because the first block is in one 'treatment' and the other three in the other 'treatment' but there is no overlapping of either.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another issue is that now I may have figured what the models do, I think they just try to explain the reasons why things are as they are, but I would still like to be able to say that MF feed significantly more than OF broods and that MF brood is disturbed less often. The model would only try to explain the reasons why rather than if they are significantly different (which is all I am really trying to figure out).&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is an easier way to compare two groups that have independent and non-independent data then I would greatly appreciate it. I actually thought of using a point biserial correlation using the means with MF coded as 0 and OF coded as 1. It works but is it valid? (&lt;a href=&quot;http://faculty.vassar.edu/lowry/pbcorr.html&quot; rel=&quot;nofollow&quot;&gt;http://faculty.vassar.edu/lowry/pbcorr.html&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anyone out there who could possibly help me with this? I am reaching my deadline and know that if I could just do this part then the rest should be okay. I am also a foreign student and my professor has said he can't help me long distance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also have flush rates vs number of days to hatch. I wanted to see if birds were less likely to flush off their nest closer to hatch so used correlation. I have data for 7 nests and plotted each data point (n=77). The issue again is that not all points are independent but I need to show the flush rate for nest age (and not all nests were measured at the same age). Should I do a correlation for each nest and then is there someway of combining them to give me a p-value, or should I leave it as it is?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you SO much. I am sorry if my questions were too long.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rachel&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-03-20T00:26:24.620" Id="24910" LastActivityDate="2012-03-20T04:08:39.573" LastEditDate="2012-03-20T04:08:39.573" LastEditorUserId="7972" OwnerUserId="9976" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;small-sample&gt;" Title="Mixed effects models issue and correlation question. Getting desperate" ViewCount="364" />
  <row Body="&lt;p&gt;There are 2 things that you are trying to achieve:&#10;1. Summarise each of the 6 variables from their individual items&#10;2. Summarise the 6 variables&lt;/p&gt;&#10;&#10;&lt;p&gt;For 1, you used the sum of the items under each variable as a representative score. This provides an uneven basis to compare the scores across the variables. A better way would be to use the mean of the items belonging to each of the 6 variables as the representative score for that variable. Also, have tou tried running a factor analysis on the items itself to determine if your 6 factor model is a good fit for your data?&lt;/p&gt;&#10;&#10;&lt;p&gt;For 2, I am unclear as to how you intend to use factor analysis to create a social capital index. Factor analysis would outline the relationships between latent factors and the variables. Here you would be assuming that they all belong to 1 latent variable which might or might not be the most appropriate factor model for the 6 variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;For summation of the factor scores, you are assuming equidistant between the likert scales and unidimensionality of all the 6 variables which might be a stretch. An alternative would be to use the 6 factor scores in your multiple regression.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-20T02:29:01.103" Id="24916" LastActivityDate="2012-03-20T02:29:01.103" OwnerUserId="7448" ParentId="24905" PostTypeId="2" Score="2" />
  
&#10;\sum\frac{(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2_{(n-1)}
  <row Body="&lt;p&gt;You might consider a permutations test.&lt;/p&gt;&#10;&#10;&lt;p&gt;A permutations test assumes that the observations are drawn from one population and then treatments are randomly allocated. Thus in the context of a permutations test for a difference in the means the null hypothesis (no treatment effect) becomes equivalent to a statement that any difference between the groups under the null hypothesis is a consequence of only the random allocation of the values into the groups. The significance of the observed differences between the treatment groups is thus just a measure of how unusual the observed allocation is relative to all possible random allocations.&lt;/p&gt;&#10;&#10;&lt;p&gt;The significance is thus calculable by enumerating all possible allocations and finding from that list the distribution of random differences between group means. The probability under the null hypothesis of obtaining a difference as great as that observed or greater is equal to the proportion of the population of differences that is as as that observed or greater.&lt;/p&gt;&#10;&#10;&lt;p&gt;More detail, some references and free software (a bit archaic...) can be had from my webpage: &lt;a href=&quot;http://www.pharmacology.unimelb.edu.au/statboss/permutations%20test.html&quot; rel=&quot;nofollow&quot;&gt;http://www.pharmacology.unimelb.edu.au/statboss/permutations%20test.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-03-20T05:32:01.250" Id="24923" LastActivityDate="2012-03-22T01:47:50.287" LastEditDate="2012-03-22T01:47:50.287" LastEditorUserId="1679" OwnerUserId="1679" ParentId="24911" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;For one random variable with k parents the number of parameters = $$ d \cdot d^k $$&lt;br&gt;&#10;If you need independent parameters: $$ (d-1) \cdot d^k $$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-20T16:40:43.380" Id="24947" LastActivityDate="2012-03-20T16:40:43.380" OwnerUserId="4763" ParentId="17416" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You always convert the values last if you're going to convert them.  Of course they're asymmetrical when converted to odds (if not near 1).  And they'd be asymmetrical converted to proportions (if not near 0.5).  The regression was done in logit space and will only be linear with symmetrical confidence intervals there.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It's much easier to see the necessity of the asymmetry with proportions because the values cannot be greater than 1 or less than 0.  For similar reasons it should be asymmetrical with odds because of the exponential shape of increasing odds.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-20T20:47:53.977" Id="24965" LastActivityDate="2012-03-20T20:47:53.977" OwnerUserId="601" ParentId="24950" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I have problems in using the &lt;code&gt;cor()&lt;/code&gt; and &lt;code&gt;cor.test()&lt;/code&gt; functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I just have two matrices (only numerical values, and the same number of&#10;row and columns) and I want to have the correlation number and the &#10;corresponding p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I use &lt;code&gt;cor(matrix1, matrix2)&lt;/code&gt; I get the correlation coefficients for all the cells.&#10;I just want a single number as result of cor.&lt;/p&gt;&#10;&#10;&lt;p&gt;In additon when I do &lt;code&gt;cor.test(matrix1, matrix2)&lt;/code&gt; I get the following error &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in cor.test.default(matrix1, matrix2) : 'x' must be a numeric vector&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I get p-values for matrices?&lt;/p&gt;&#10;&#10;&lt;p&gt;You find the simple tables I want to correlate here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dl.dropbox.com/u/3288659/table_exp1_offline_MEANS.csv&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/3288659/table_exp1_offline_MEANS.csv&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dl.dropbox.com/u/3288659/table_exp2_offline_MEANS.csv&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/3288659/table_exp2_offline_MEANS.csv&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-21T01:30:33.857" FavoriteCount="2" Id="24980" LastActivityDate="2012-05-02T08:17:11.227" LastEditDate="2012-03-21T07:19:50.350" LastEditorUserId="930" OwnerUserId="4701" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;correlation&gt;" Title="Correlation between matrices in R" ViewCount="10280" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Suppose, we have a set of measurements of some quantity in some units of measurement. We also have a nice model that heavily relies on the properties of the Gaussian distribution. The model is tailored for data in some specific units of measurement with some physical meaning behind (like watt, ohm, etc.). It turns out that the distribution of the data does not exactly follow the normal distribution and has some undesired features (like skewness). We apply the popular Box-Cox transformation and obtain a more or less normally distributed data set. The problem now is that we have logarithms, powers, etc. of the original measurements, which contradicts with our nice model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, what can one do in such a situation? I need to change the model such that it can handle the new data? And in general, if I got everything correctly, why do people what to study transformed data that have lost their physical meaning? Because, at the end of the day, one will, probably, have to return back to the original units of measurement.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-21T09:11:46.193" FavoriteCount="1" Id="24992" LastActivityDate="2012-04-21T07:08:54.400" LastEditDate="2012-03-21T10:50:36.287" LastEditorUserId="9222" OwnerUserId="9222" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;data-transformation&gt;" Title="Life after the Box-Cox transformation" ViewCount="749" />
  <row Body="&lt;p&gt;Plainly there's a difference between the groups - the locations are completely different, with no overlap in the groups. Any sensible test will reject even at this small sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume you want to ask a different question of the data than that.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-21T11:45:45.983" Id="25002" LastActivityDate="2012-03-21T11:45:45.983" OwnerUserId="805" ParentId="24994" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You might want to refer to this and the forecasted upper-lower limit plot in it.&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/24521/problem-in-discrete-valued-time-series-forecasting&quot;&gt;Problem in discrete valued time series forecasting&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-21T11:48:04.120" Id="25004" LastActivityDate="2012-03-21T11:48:04.120" OwnerUserId="10015" ParentId="24776" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Given an arbitrary discrete distribution and an observed distribution coming from a Monte Carlo simulation, my goal is to be able to say whether or not the observed distribution is the same is the given distribution such that I correctly identify a distribution which is different with probability greater than 1 in 10,000.  I should say that the Monte Carlo simulation is run at 10^10 iterations.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, I have been using a combination of confidence intervals (derived from the given distribution) to test the observed mean as well as the chi-square test.  Originally, it seemed to me that this combination would provide, over multiple iterations, the desired precision for which I am aiming.  Upon further thought, however, it has occurred to me that these tests are not independent since I am using them on the same observation.  Consequently, I can only do as well as the most precise test.  Is this true?  I have several other questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my interpretation of confidence intervals correct when I say, when using a 95% confidence interval (for instance), that the probability that the observed mean falls outside of the confidence interval when the observed distribution is the same as the given distribution is 5%?&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, if the above is correct, is the interpretation of the chi-square result similar?  In other words, with an alpha value of 0.05, is it true that the probability that the chi-square result falls outside of this bound when the observed distribution is the same as the given distribution is 10%?&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, is there a good method for achieving my desired precision?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-21T20:09:43.100" Id="25037" LastActivityDate="2012-03-21T20:09:43.100" OwnerUserId="9247" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;chi-squared&gt;" Title="Proper interpretation for error analysis" ViewCount="124" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to sort out two different uses of the term &quot;compound distribution&quot; and figure out the relationship.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Wikipedia article on &lt;a href=&quot;http://en.wikipedia.org/wiki/Compound_distribution&quot; rel=&quot;nofollow&quot;&gt;compound distribution&lt;/a&gt; -- which I wrote -- defines a compound distribution as an infinite mixture, i.e. if $p(x|a)$ is a distribution of type F, and $p(a|b)$ is a distribution of type G, then $p(x|b) = \int_a p(x|a) p(a|b) da$ is a compound distribution that results from compounding F with G.  This is the distribution of prior and posterior predictive distributions in Bayesian statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the term &quot;compound distribution&quot; has another meaning as a random sum, i.e. a sum of i.i.d. variables where the number of variables is random.&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the relation between the two?  And am I using &quot;compound distribution&quot; correctly for the first definition?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-22T02:05:08.207" FavoriteCount="1" Id="25051" LastActivityDate="2012-03-22T07:40:16.610" LastEditDate="2012-03-22T07:40:16.610" LastEditorUserId="930" OwnerUserId="9862" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;bayesian&gt;" Title="Compound distribution in Bayesian sense vs. compound distribution as random sum" ViewCount="145" />
  
&#10;1/6 &amp;amp;5/6 &amp;amp;0 &amp;amp;0 &amp;amp;0 &amp;amp;0 \\
&#10;0   &amp;amp;2/6 &amp;amp;4/6 &amp;amp;0 &amp;amp;0 &amp;amp;0\\
&#10;\end{pmatrix}
&#10;\end{pmatrix}
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Yes, repeated CV is a popular resampling technique.&lt;/li&gt;&#10;&lt;li&gt;The sample standard deviation of your metric of interest (where one measurement corresponds to one repeat/fold combination) divided by the square root of the number of repeat-fold combinations minus one (i.e. standard error of the mean). This is done for each tuning parameter combination and then the &quot;best&quot; tuning parameter combination is chosen according to a certain rule (max, &quot;one sigma rule&quot;, etc)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;R package &lt;a href=&quot;http://cran.r-project.org/web/packages/caret/vignettes/caretTrain.pdf&quot; rel=&quot;nofollow&quot;&gt;caret&lt;/a&gt; supports all that (including the &quot;one sigma rule&quot;) and much more.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-22T14:48:57.283" Id="25062" LastActivityDate="2012-03-22T19:52:53.447" LastEditDate="2012-03-22T19:52:53.447" LastEditorUserId="6129" OwnerUserId="6129" ParentId="24875" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="26337" AnswerCount="2" Body="&lt;p&gt;I have a question on the difference-in-differences estimator. Suppose my data consists of two periods and the treatment is administered to some of the individuals in period $t = 2$. I estimate this model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_{it} = \beta_0 + \beta_1 treatment_i + \beta_2 year_t + \beta_3 treatment_i \times year_t + \beta_4 x_{it} + u_{it}$&lt;/p&gt;&#10;&#10;&lt;p&gt;for individuals $i = 1, \ldots, N$ and $t = 1, 2$. $treatment_i$ is the treatment dummy (1 for treated individuals, 0 otherwise), $year_t$ is a period dummy (1 for second period, 0 otherwise). I am interested in estimating $\beta_3$, the difference-in-differences coefficient. $x_{it}$ is a control variable and $u_{it}$ is the error term. Panel is balanced, so no missing observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's my question. If I run this regression without any control variables (so dropping $x_{it}$) then OLS and fixed effects give &lt;strong&gt;exactly&lt;/strong&gt; the same estimate for $\beta_3$. However, in a model that includes one (or more than one) control variable(s) (so, including $x_{it}$ now), this is no longer the case. I.e., in that case, the fixed effects and OLS estimator of $\beta_3$ are no longer exactly the same. I've checked this in two datasets as well as using simulated data. The estimates are usually very similar, but nonetheless I'm interested in figuring out why fixed effects and OLS do not give exactly the same estimate for $\beta_3$ in a two-period DiD model with control variables (especially since I'm worried that I may be missing something really obvious). Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-22T15:09:09.570" FavoriteCount="2" Id="25069" LastActivityDate="2012-04-12T06:13:23.330" OwnerUserId="10044" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;econometrics&gt;&lt;least-squares&gt;&lt;fixed-effects-model&gt;" Title="2 period difference-in-differences fixed effects versus OLS" ViewCount="3313" />
  
  
  <row AcceptedAnswerId="25108" AnswerCount="1" Body="&lt;p&gt;I have survey data from students from two Universities. One of the questions in the survey had 4 response categories and students were allowed to check only one (categorical variable).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to make a table that shows the odds ratio and 95% CI for OR with University arranged in columns and all 4 possible answers in rows. I also want to see if there is a statistical difference between ORs in rows.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible? If so, how should I do it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-22T17:20:45.867" Id="25080" LastActivityDate="2012-03-22T23:26:07.980" LastEditDate="2012-03-22T18:20:50.463" LastEditorUserId="10050" OwnerUserId="10050" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;odds-ratio&gt;" Title="How to get odds ratio in a table in SPSS?" ViewCount="11385" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let's say I have some kind of survival data - i.e. I'm giving a drug that may cause mortality. So I have three patients: A, B and C. All are given the drug at Time t1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say patient A dies at time t2. Patient B dies at time t3. And Patient C dies at time t100.&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly, the likelihood that the drug caused the death of patient A and B is higher than the likelihood that the drug cause patient C's death (i.e. patient C likely died from natural causes whereas patients A and B probably died because of the drug).&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any techniques in survival analysis/related fields that allow one to quantify the probability/likelihood that the treatment caused the effect. For example, if I could calculate the probability that the treatment caused the death given that patient was treated at time X and died at time Y.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-22T17:49:47.993" Id="25083" LastActivityDate="2012-10-28T21:03:24.593" LastEditDate="2012-07-30T20:43:50.367" LastEditorUserId="88" OwnerUserId="5464" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;bayesian&gt;&lt;survival&gt;&lt;conditional-probability&gt;" Title="Time-wise treatment effect / survival analysis" ViewCount="120" />
  <row Body="&lt;p&gt;The &lt;code&gt;loess.demo&lt;/code&gt; function in the TeachingDemos package for R will interactively demonstrate the ideas behind a loess fit.  It will plot a set of data and the loess fit, then when you click on a point it will show the window used to fit at that point, the relative weights of the points within the window, and the &quot;linear model&quot; fit to that weighted data.  Clicking on additional points will then update the display to show the general concept of the loess fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;This may help explain what loess does and may help in an explanation of the difference. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-22T21:34:09.320" Id="25098" LastActivityDate="2012-03-22T21:34:09.320" OwnerUserId="4505" ParentId="25092" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;It sounds like you're looking for a robust measure of central tendency, of which there are many. The Wikipedia page on &lt;a href=&quot;http://en.wikipedia.org/wiki/Robust_statistics&quot; rel=&quot;nofollow&quot;&gt;robust statistics&lt;/a&gt; is probably worth a read. The simplest approach is probably to use the median of the samples, rather than the mean; you'll have to do this calculation for $x$ and $y$ coordinates separately.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-22T22:54:31.357" Id="25106" LastActivityDate="2012-03-22T22:54:31.357" OwnerUserId="9975" ParentId="24906" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The $p$-value is the area under the $\chi^2$ density to the right of the observed test statistic. Therefore, to calculate the $p$-value by hand you need to calculate an integral. &lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, a $\chi^2$ random variable with $k$ degrees of freedom has probability density &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x;\,k) =
  
  
  
  <row Body="&lt;p&gt;Here's a slightly out of left-field answer, that only touches the &lt;em&gt;&quot;best-practices around combining multiple models&quot;&lt;/em&gt; part of your question. This is basically exactly my honours thesis, except that I'm dealing with complex, highly non-linear models that exhibit chaos and noise - climate models. This isn't likely to be broadly applicable to many fields, but might be useful in ecology or econometrics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Until fairly recently in the climate modelling community, models were largely just smashed together in an &lt;strong&gt;unweighted average&lt;/strong&gt; (usually after &lt;em&gt;bias correction&lt;/em&gt; involving removing the model mean for part or all of the sample period). This is basically what the IPCC did for the 4th assessment report (4AR), and previous reports.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is more or less an example of the &quot;&lt;strong&gt;truth plus error&lt;/strong&gt;&quot; school of ensemble combination, where it is tacitly or explicitly assumed that observational series (eg. global temperature, local precipitation, etc) is true, and that if you take enough samples (eg. model runs), the noise in the model runs will cancel (see (1)).&lt;/p&gt;&#10;&#10;&lt;p&gt;More recently, methods for combining models based on &lt;strong&gt;performance weighting&lt;/strong&gt; have been used. Because climate models are so noisy, and have so many variables and parameters, the only ways of assessing the performance (that I know of) are by taking the covariance, or by taking the MSE between the model output and the observed time series. Models can then be combined by weighting the mean based on that measure. There's a good overview of this in (2).&lt;/p&gt;&#10;&#10;&lt;p&gt;One assumption behind this method of combining simulations is &lt;strong&gt;assumption that the models are all reasonably independent&lt;/strong&gt; - if some were highly dependent, they would bias the mean. This assumption was reasonably fair for the dataset used for 4AR (&lt;a href=&quot;http://www-pcmdi.llnl.gov/ipcc/about_ipcc.php&quot; rel=&quot;nofollow&quot;&gt;CMIP3&lt;/a&gt;, as this data set was made up of few model runs from many modelling groups (on the other hand, code is shared in the modelling community, so there may still be some interdependence. For an interesting look at this, see (3)). The dataset for the next assessment report, &lt;a href=&quot;http://cmip-pcmdi.llnl.gov/cmip5/&quot; rel=&quot;nofollow&quot;&gt;CMIP5&lt;/a&gt;, does not have this somewhat fortuitous attribute - some modelling teams will be submitting a few runs, while some will be submitting hundreds. Ensembles coming from different teams may be produced by initial condition peturbation, or by changes to the model physics and parametrisation. Also, this super ensemble isn't sampled in any systematic way - it's just who ever brings data is accepted (within reason). This is known in the field as an &quot;&lt;strong&gt;ensemble of opportunity&lt;/strong&gt;&quot;. There's a fair chance that using an unweighted mean on such an ensemble is going to git you some major bias toward the models with more runs (since even though there are hundreds of runs, there are likely a much smaller number of truly independent runs). &lt;/p&gt;&#10;&#10;&lt;p&gt;My supervisor has a paper in review at the moment describing a process of model combination involving performance AND &lt;strong&gt;independence weighting&lt;/strong&gt;. There is a conference paper abstract available (4), I'll post the link to the paper when it's published (slow process, don't hold your breath). Basically, this paper describes a process that involves taking the covariance of model errors (model-obs), and weighting down models that have high covariance with all other models, (ie. models with highly dependent errors). The model error variance is computed as well, and used as the performance-weighting component.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's also worth noting that climate modelling is obviously hugely impacted by the vagaries of numerical modelling in general. There's a thing called a &lt;strong&gt;&quot;laugh test&quot;&lt;/strong&gt; - if you end up with a model run that implies that global mean temperatures will be +20°C by 2050, you just throw it out, because it's clearly not physically relevant. Obviously this kind of test is fairly subjective. I haven't required it yet, but I expect to in the near future.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's my understanding of the state model combination in my field at the moment. Obviously I'm still learning, so if I hit on anything special, I'll come back and update this answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Tebaldi, C. &amp;amp; Knutti, R., 2007. The use of the multi-model ensemble in probabilistic climate projections. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 365(1857), pp.2053–2075.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Knutti, R. et al., 2010. IPCC Expert Meeting on Assessing and Combining Multi Model Climate Projections.&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) Masson, D. &amp;amp; Knutti, R., 2011. Climate model genealogy. Geophys. Res. Lett, 38(8), p.L08703.&lt;/p&gt;&#10;&#10;&lt;p&gt;(4) Abramowitz, G. &amp;amp; Bishop, C., 2010. Defining and weighting for model dependence in ensemble prediction. In AGU Fall Meeting Abstracts. p. 07.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-23T07:33:12.777" Id="25127" LastActivityDate="2012-03-24T02:28:14.880" LastEditDate="2012-03-24T02:28:14.880" LastEditorUserId="9007" OwnerUserId="9007" ParentId="562" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This is the solution that worked for me, and I'm posting it if anyone else get the same problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;I added all the restrictions as a vector &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;restrict &amp;lt;- c(&quot;eq1_p1+eq1_p2+eq1_p3=0&quot;, &quot;eq2_p1+eq2_p2+eq2_p3=0&quot;) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;system_eqn_restrict &amp;lt;- systemfit(eqlist, method=&quot;SUR&quot;, restrict.matrix = restrict)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-03-23T13:20:24.373" Id="25140" LastActivityDate="2012-03-23T13:20:24.373" OwnerUserId="7884" ParentId="24929" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="25152" AnswerCount="2" Body="&lt;p&gt;I'm having following problem while doing some analysis with R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataframe like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Name | Group | Count&#10;Person 1 | A | 3&#10;Person 2 | A | 1&#10;Person 3 | A | 0&#10;Person 1 | B | 5 &#10;Person 2 | B | 0&#10;Person 3 | B | 1&#10;Person 1 | C | 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and I'd need to &quot;expand&quot; it (not sure if the right term) to be like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Person 1 | A&#10;Person 1 | A&#10;Person 1 | A&#10;Person 2 | A&#10;Person 1 | B&#10;Person 1 | B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;So it takes value of pair Person 1 and A (in this example, 3) and makes three rows with Person 1 and A and does so for every Person - Group -combination. Can't figure out any good words for searching online.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-23T15:36:16.450" FavoriteCount="1" Id="25148" LastActivityDate="2012-03-24T15:10:09.817" LastEditDate="2012-03-23T16:35:02.580" LastEditorUserId="6029" OwnerUserId="10077" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;dataframe&gt;" Title="How to expand data frame in R" ViewCount="2888" />
  <row AcceptedAnswerId="25158" AnswerCount="2" Body="&lt;p&gt;It is known that the bootstrap can fail.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read in Section 6 of &lt;a href=&quot;http://www.jstor.org/stable/2240410&quot;&gt;Bickel and Freedman (1981)&lt;/a&gt; that the bootstrap fails when you wan to use it to evaluate the MLE for estimating the parameter of a continuous uniform distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read Secion 7.4 of &lt;a href=&quot;http://books.google.com.hk/books/about/An_introduction_to_the_bootstrap.html?id=gLlpIUxRntoC&quot;&gt;the book by Efron and Tibshirani&lt;/a&gt; but I'm not able to find the reference they pointed to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone point me to some more easily accessible things that I could refer to? Thanks!&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-03-23T16:07:26.170" FavoriteCount="1" Id="25151" LastActivityDate="2012-07-24T19:14:58.817" LastEditDate="2012-03-24T11:28:34.110" LastEditorUserId="1739" OwnerUserId="8937" PostTypeId="1" Score="7" Tags="&lt;bootstrap&gt;&lt;references&gt;" Title="Recommended reading for understanding when the bootstrap will fail?" ViewCount="257" />
  
  <row Body="&lt;p&gt;If you really want to use stacked barcharts with such a large number of items, here are two possible solutions.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Using &lt;code&gt;irutils&lt;/code&gt;&lt;/h2&gt;&#10;&#10;&lt;p&gt;I came across this package some months ago.&lt;/p&gt;&#10;&#10;&lt;p&gt;As of commit 0573195c07 on &lt;a href=&quot;https://github.com/jbryer/irutils&quot;&gt;Github&lt;/a&gt;, the code won't work with a &lt;code&gt;grouping=&lt;/code&gt; argument. Let's go for Friday's debugging session.&lt;/p&gt;&#10;&#10;&lt;p&gt;Start by downloading a zipped version from Github.&#10;You'll need to hack the &lt;code&gt;R/likert.R&lt;/code&gt; file, specifically the &lt;code&gt;likert&lt;/code&gt; and &lt;code&gt;plot.likert&lt;/code&gt; functions. First, in &lt;code&gt;likert&lt;/code&gt;, &lt;code&gt;cast()&lt;/code&gt; is used but the &lt;code&gt;reshape&lt;/code&gt; package is never loaded (although there's an &lt;code&gt;import(reshape)&lt;/code&gt; instruction in the &lt;code&gt;NAMESPACE&lt;/code&gt; file). You can load this yourself beforehand. Second, there's an incorrect instruction to fetch items labels, where a &lt;code&gt;i&lt;/code&gt; is dangling around line 175. This has to be fixed as well, e.g. by replacing all occurrences of &lt;code&gt;likert$items[,i]&lt;/code&gt; with &lt;code&gt;likert$items[,1]&lt;/code&gt;. Then you can install the package the way you are used to do on your machine. On my Mac, I did&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% tar -czf irutils.tar.gz jbryer-irutils-0573195&#10;% R CMD INSTALL irutils.tar.gz&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then, with R, try the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(irutils)&#10;library(reshape)&#10;&#10;# Simulate some data (82 respondents x 66 items)&#10;resp &amp;lt;- data.frame(replicate(66, sample(1:5, 82, replace=TRUE)))&#10;resp &amp;lt;- data.frame(lapply(resp, factor, ordered=TRUE, &#10;                          levels=1:5, &#10;                          labels=c(&quot;Strongly disagree&quot;,&quot;Disagree&quot;,&#10;                                   &quot;Neutral&quot;,&quot;Agree&quot;,&quot;Strongly Agree&quot;)))&#10;grp &amp;lt;- gl(2, 82/2, labels=LETTERS[1:2]) # say equal group size for simplicity&#10;&#10;# Summarize responses by group&#10;resp.likert &amp;lt;- likert(resp, grouping=grp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That should just work, but the visual rendering will be awful because of the high number of items. It works without grouping (e.g., &lt;code&gt;plot(likert(resp))&lt;/code&gt;), though. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cXzrw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would thus suggest to reduce your dataset to smaller subsets of items. E.g., using 12 items, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(likert(resp[,1:12], grouping=grp))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get a 'readable' stacked barchart. You can probably process them afterwards. (Those are &lt;code&gt;ggplot2&lt;/code&gt; objects, but you won't be able to arrange them on a single page with &lt;code&gt;gridExtra::grid.arrange()&lt;/code&gt; because of readability issue!)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/TIFaO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h2&gt;Alternative solution&lt;/h2&gt;&#10;&#10;&lt;p&gt;I would like to draw your attention on another package, &lt;a href=&quot;http://cran.r-project.org/web/packages/HH/index.html&quot;&gt;HH&lt;/a&gt;, that allows to plot Likert scales as diverging stacked barcharts. We could reuse the above code as shown below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;resp.likert &amp;lt;- likert(resp)&#10;detach(package:irutils)&#10;library(HH)&#10;plot.likert(resp.likert$results[,-6]*82/100, main=&quot;&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but that will complicate things a bit because we need to convert frequencies to counts, subset the &lt;code&gt;likert&lt;/code&gt; object produced by &lt;code&gt;irutils&lt;/code&gt;, detach package, etc. So let's start again with fresh (counts) statistics:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot.likert(t(apply(resp, 2, table)), main=&quot;&quot;, as.percent=TRUE,&#10;            rightAxisLabels=NULL, rightAxis=NULL, ylab.right=&quot;&quot;, &#10;            positive.order=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zDV3w.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To use a grouping variable, you'll need to work with an &lt;code&gt;array&lt;/code&gt; of numerical values.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# compute responses frequencies separately by grp&#10;resp.array &amp;lt;- array(NA, dim=c(66, 5, 2))&#10;resp.array[,,1] &amp;lt;- t(apply(subset(resp, grp==&quot;A&quot;), 2, table))&#10;resp.array[,,2] &amp;lt;- t(apply(subset(resp, grp==&quot;B&quot;), 2, table))&#10;dimnames(resp.array) &amp;lt;- list(NULL, NULL, group=levels(grp))&#10;plot.likert(resp.array, layout=c(2,1), main=&quot;&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This will produce two separate panels, but it fits on a single page.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xCKAG.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-23T16:48:15.120" Id="25156" LastActivityDate="2012-03-24T09:11:34.547" LastEditDate="2012-03-24T09:11:34.547" LastEditorUserId="930" OwnerUserId="930" ParentId="25109" PostTypeId="2" Score="18" />
  
  
  <row Body="&lt;p&gt;You are missing the data= argument.  You have to tell lmer which data frame to look at.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fm2test&amp;lt;-lmer(Feeding~MF.vs.OF+Age.class+tide.h.l+Site+HDp+(1|Brood), data=ABMtest.df)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="9" CreationDate="2012-03-23T21:01:36.613" Id="25172" LastActivityDate="2012-03-23T21:01:36.613" OwnerUserId="7972" ParentId="25169" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;One way to go about it would be:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Hypothesise a model distribution for your population.  Your population can itself be seen as a sample from a hypothetical infinite population.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Using your actual sample, estimate the parameters of your model distribution, and perform diagnostic checks to see if the model remains plausible.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Simulate data of the size of your actual population, using the fitted model to simulate the data from the population that you haven't observed, and repeating your actual sample for the part of the population that you &lt;em&gt;do&lt;/em&gt; have.  Do this say 500-1000 times to create confidence intervals for the maximum and minimum of the population.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Depending on your model, you could replace step 3 with a theory-based calculation of the confidence interval.  How to do that would depend on your model (ie what distribution you are hypothesising) however.  Which would depend on what sort of data you are looking at.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-23T21:14:41.197" Id="25173" LastActivityDate="2012-03-23T21:57:43.617" LastEditDate="2012-03-23T21:57:43.617" LastEditorUserId="7972" OwnerUserId="7972" ParentId="25129" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="25180" AnswerCount="1" Body="&lt;p&gt;I run a backward variable selection logistic regression and find out the SAS program selected 12 variables and give me the output like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lkm16.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/hWf7n.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/RvVB0.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/a7uQy.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/IRIhv.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It is funny that my configuration of the SAS backward selection is to make the SC(SBC) lower. Overall, the result looks like the SC result suppose that intercept with covariates is worse. But other two guys AIC and -2logL supports that Intercept with corvar is better model.&lt;/p&gt;&#10;&#10;&lt;p&gt;May you guys make some judge ment on this result and test output?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-03-24T00:18:25.777" FavoriteCount="1" Id="25179" LastActivityDate="2012-03-24T02:25:20.720" LastEditDate="2012-03-24T02:25:20.720" LastEditorUserId="9900" OwnerUserId="9900" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;hypothesis-testing&gt;&lt;aic&gt;&lt;likelihood-ratio&gt;" Title="How to interpret this criterion of logistic regression result" ViewCount="659" />
  
&#10;\ln(\lambda(x)_i) = \alpha + \beta x_i + \varepsilon_i \\
  <row AnswerCount="1" Body="&lt;p&gt;I have two features which are both continuous. How to perform a classification task based on them? I've read the Wikipedia entry on &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot; rel=&quot;nofollow&quot;&gt;Naive Bayes&lt;/a&gt;, but this is only for discrete outcome and one feature.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-24T07:45:25.847" FavoriteCount="2" Id="25190" LastActivityDate="2012-03-25T09:41:23.203" LastEditDate="2012-03-24T09:36:01.410" LastEditorUserId="930" OwnerUserId="10086" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;naive-bayes&gt;" Title="Naive Bayes for two continuous features" ViewCount="544" />
  <row AnswerCount="0" Body="&lt;p&gt;I've been assigned this question as &quot;homework&quot;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Run a simulation study to examine the performance of the bootstrap for obtaining 95% confidence intervals on the mean of a univariate sample of data.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;You should look at confidence interval coverage (what proportion of times&#10;  does the confidence interval contain the true mean?) and Monte Carlo variation (by how much do the upper and lower confidence limits vary between simulations?).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;You may choose which bootstrap methods to evaluate.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The confusing thing is that we've been given no data to use in our simulations.  How would I interpret the assignment given this fact?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-03-24T13:39:51.380" Id="25200" LastActivityDate="2012-03-24T13:39:51.380" OwnerUserId="10089" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;bootstrap&gt;" Title="Interpretation of bootstrap question" ViewCount="124" />
  
  <row AcceptedAnswerId="25205" AnswerCount="1" Body="&lt;p&gt;I have a set $S$ of $N$ real numbers. I wanna calculate the mean $m$ variance $v$ of $S$. But since $N$ is too large, I do not want to use all of the numbers. Instead, I would like to sample $n$ numbers uniformly randomly from the $N$-number set $S$, such that mean and variance of the sampled set does not deviate too much from the original set. Say, the error I can tolerate is $e_m$, and $e_v$ respectively (which means that the mean of the sample set should fall in $(1 \pm e_m)m$, and the sample variance should fall in $(1\pm e_v)v$). How large should $n$ be?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-24T17:30:41.643" Id="25204" LastActivityDate="2012-03-24T18:20:44.120" LastEditDate="2012-03-24T18:19:16.763" LastEditorUserId="9975" OwnerUserId="9622" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;variance&gt;&lt;mean&gt;&lt;sample-size&gt;" Title="How many samples should be taken to capture the mean and variance of the original set with given error?" ViewCount="221" />
  
  
  <row Body="&lt;p&gt;I suspect the best approach would be to get difference scores (i.e., post - pre) for each experimental unit, and then run a simple one-way ANOVA.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-25T15:00:46.440" Id="25233" LastActivityDate="2012-03-25T15:00:46.440" OwnerUserId="7290" ParentId="25232" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There is no best forecasting package in R. For one thing, R is open source so there are often multiple packages that do similar things and you can choose which one seems to be the most up-to-date and works to your taste.&lt;/p&gt;&#10;&#10;&lt;p&gt;More importantly, though, is: how you forecast depends on how you model. Do you have a univariate time series and nothing but that data? Or do you have a univariate time series with several predictor variables? Do you have a multivariate time series? Are you even using a time series at all? (&quot;Forecasting&quot; implies time series, but that's not necessarily the case.)&lt;/p&gt;&#10;&#10;&lt;p&gt;So the first question would probably be: what data do you have, and what kind of model are you trying to make? Modeling options and forecasting options will then be easier.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-25T17:24:29.450" Id="25240" LastActivityDate="2012-03-25T17:24:29.450" OwnerUserId="1764" ParentId="25230" PostTypeId="2" Score="1" />
  
  
&#10;ICC = \dfrac{\psi}{\theta+\psi}
  <row AnswerCount="1" Body="&lt;p&gt;I have a rather complicated decision analysis problem involving reliability testing and the logical approach (to me) seems to involve using MCMC to support a Bayesian analysis.  However, it has been suggested that it would be more appropriate to use a bootstrapping approach. Could someone suggest a reference (or three) that might support the use of either technique over the other (even for particular situations)? FWIW, I have data from multiple, disparate sources and few/zero failure observations. I also have data at the subsystem and system levels. &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems a comparison like this should be available, but I've had not luck searching the usual suspects.  Thanks in advance for any pointers.  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-26T14:57:30.927" FavoriteCount="3" Id="25286" LastActivityDate="2012-03-26T17:33:00.653" LastEditDate="2012-03-26T17:33:00.653" LastEditorUserId="930" OwnerUserId="3591" PostTypeId="1" Score="4" Tags="&lt;bayesian&gt;&lt;bootstrap&gt;" Title="When to use bootstrap vs. bayesian technique?" ViewCount="1414" />
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-and-mixed-effect-mode&quot;&gt;What is the difference between fixed effect, random effect and mixed effect models?&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Is there a criteria to establish such a difference?&lt;/p&gt;&#10;" ClosedDate="2012-03-26T17:25:30.443" CommentCount="0" CreationDate="2012-03-26T15:20:53.520" Id="25289" LastActivityDate="2012-03-26T15:27:11.997" LastEditDate="2012-03-26T15:27:11.997" LastEditorUserId="449" OwnerUserId="10128" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;" Title="How do you know something is a random effect or a fixed effect?" ViewCount="58" />
  <row AcceptedAnswerId="25297" AnswerCount="1" Body="&lt;p&gt;Given two independent variables with Beta distribution, $X \sim \text{Be}(a_1, b_1)$ and $Y \sim \text{Be}(a_2, b_2)$, how do you find the probability that the value of X is greater than the value of Y for a given observation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this probability have a name that I'm just blanking on?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-26T15:26:07.770" Id="25290" LastActivityDate="2012-03-26T19:53:36.437" LastEditDate="2012-03-26T19:53:36.437" LastEditorUserId="9738" OwnerUserId="1951" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;beta&gt;" Title="What is the probability P(X &gt; Y) given X ~ Be(a1, b1), and Y ~ Be(a2, b2), and X and Y are independent?" ViewCount="641" />
  <row Body="&lt;p&gt;Correlation is scaled to be between -1 and +1 depending on whether there is positive or negative correlation, and is dimensionless. The covariance however, ranges from zero, in the case of two independent variables, to Var(X), in the case where the two sets of data are equal. The units of COV(X,Y) are the units of X times the units of Y.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-26T16:42:06.367" Id="25302" LastActivityDate="2012-03-26T16:42:06.367" OwnerUserId="10131" ParentId="18082" PostTypeId="2" Score="-2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Case I&#10;&#10;         A&#10;|-----------------|&#10;         |------------------|B&#10;&#10;Case II&#10;                     A&#10;            |-----------------|&#10;|---------------------|&#10;          B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And the following numbers:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     Total generated by A            Total generated by B as seen by A&#10;A            1000                                    139&#10;&#10;     Total generated by B            Total generated by A as seen by B&#10;B            300                                      60&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am trying to get the probability that the events generated by A and B will overlap (or rather I am trying to look at the probability that the pair A-B will generate events that overlap). I am doing it the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Total generated by B as seen by A + Total generated by A as seen by B&#10;-------------------------------------------------------------------------------&#10;                  Total generated by A + Total generated by B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is this correct or am I missing something? Or is there a better way to characterize the relation between A and B?&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, if I have thousands of such pairs, what can I derive about the system?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-27T03:42:45.113" FavoriteCount="0" Id="25334" LastActivityDate="2012-04-27T02:11:28.407" OwnerUserId="2164" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;stata&gt;&lt;mathematical-statistics&gt;" Title="How do I get overlapping probability?" ViewCount="123" />
  <row AcceptedAnswerId="25337" AnswerCount="1" Body="&lt;p&gt;Suppose $x_i$ are drawn &lt;em&gt;i.i.d.&lt;/em&gt; from a $p$-variate Gaussian, $\mathcal{N}\left(\mu,\Sigma\right)$. Suppose one observes $x_1,x_2,\ldots,x_n$. One also observes $s_{n+1},s_{n+2},\ldots,s_{n+m},$ where $s_i = \mbox{sign}\left(x_i\right)$ is a $p$-vector consisting of -1 and +1's. (Well, in principle, it could contain some zeroes...)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to estimate $\mu$ from this information. One should be able to do at least as well as the estimate that ignores the $s_i$ (namely $\hat{\mu} = (1/n) \sum_{1\le i \le n} x_i$). But is there a method that does better? How good is the method? Is this a well-known problem?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-27T04:57:40.437" Id="25335" LastActivityDate="2012-03-27T08:53:19.460" LastEditDate="2012-03-27T08:53:19.460" LastEditorUserId="88" OwnerUserId="795" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;&lt;mean&gt;&lt;point-estimation&gt;" Title="Improving an estimate of mean with observations of sign" ViewCount="36" />
  <row AcceptedAnswerId="25363" AnswerCount="1" Body="&lt;p&gt;I have built a good model of time-to-stroke under cox ph assumptions using a predictor of stroke risk (Framingham risk score). It incorporates a score according to Age, Gender, controlled / uncontrolled bp, cardiovascular risk factors  and can be seen here :&#10;&lt;a href=&quot;http://www.framinghamheartstudy.org/risk/index.html&quot; rel=&quot;nofollow&quot;&gt;http://www.framinghamheartstudy.org/risk/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have hypothesised that it will be a predictor of time to event in my population.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it incorporates age, in fact it is correlated 85% +/-3% with age. So my question is: How do I effectively assess whether it is a &lt;em&gt;better&lt;/em&gt; predictor than age alone. (It's pretty useless clinically if it's not) My intuition has been to include age alone then the framingham score alone in the model with the controlling predictors and then compare the AIC between the two fits, choosing the one which changes the AIC most - compared to the model with only the controlling predictors in it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a good solution to this in general? Or is it just plain wrong.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-27T10:39:11.573" FavoriteCount="1" Id="25353" LastActivityDate="2012-03-27T14:24:40.523" LastEditDate="2012-03-27T10:47:23.997" LastEditorUserId="6666" OwnerUserId="6666" PostTypeId="1" Score="2" Tags="&lt;aic&gt;&lt;multicollinearity&gt;&lt;cox-model&gt;" Title="How to choose the best of two highly correlated predictors in cox proportional hazards regression" ViewCount="299" />
  
  <row Body="&lt;p&gt;A histogram would do exactly that. For descriptive stats, the mean, standard deviation and skewness, as well as the IQR are the most commonly used. Depending on the type of distribution, some of those become more or less relevant -- a mean isn't really representative in a highly skewed distribution, for instance. You would probably prefer the median, which is less sensitive to extremes.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-27T12:40:04.070" Id="25356" LastActivityDate="2012-03-27T12:40:04.070" OwnerUserId="4754" ParentId="25354" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In my opinion, this is not a matter of which model fits your data the best (AIC), rather it's a matter of predictive accuracy of your competing models. I would take a look at the extension of &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2005.030814.x/abstract&quot; rel=&quot;nofollow&quot;&gt;ROC curves for survival regression models&lt;/a&gt;, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;Reference: Heagerty &amp;amp; Zheng - Survival model predictive accuracy and ROC curves - Biometrics. 2005 Mar;61(1):92-105.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-27T14:24:40.523" Id="25363" LastActivityDate="2012-03-27T14:24:40.523" OwnerUserId="9253" ParentId="25353" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;A lot of the expenditure data I work with has a similar distribution to this.  Depending on what you need to do we often find it useful to plot a histogram of the logarithm of the data.  This can be done by transforming the variable and plotting that, or plotting the original data on a logarithmic axis.&lt;/p&gt;&#10;&#10;&lt;p&gt;This method can be useful for having another think about your outliers - are they really exceptional cases, or are they just the natural high values you'd expect with what an underlying distribution that is quite regular eg approximately log normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;For most audiences, it would be a good idea to present the original as well, as otherwise they can easily otherwise walk away with the incorrect idea that your data is symmetrically distributed when it is far from it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, a &lt;a href=&quot;http://en.wikipedia.org/wiki/Q-Q_plot&quot; rel=&quot;nofollow&quot;&gt;quantile-quantile plot&lt;/a&gt; is the best way of comparing the distribution of your variable to some reference distribution.  A qq-plot comparing the log of our &quot;spend&quot; variable compared to a normal distribution is included below.  This shows that our distribution isn't actually log-normal, but for a reasonable amount of its range, including the higher values, it is a reasonable approximation.  Interpreting qq-plots needs a bit of practice but there are plenty of explanations out there.&lt;/p&gt;&#10;&#10;&lt;p&gt;eg&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2)&#10;qplot(TotalSpend) # this is just a histogram&#10;qplot(TotalSpend, log=&quot;x&quot;)&#10;qplot(sample=log(TotalSpend), stat=&quot;qq&quot;) # or qqnorm() if you prefer&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ySEPE.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-28T01:09:15.857" Id="25395" LastActivityDate="2012-03-28T01:09:15.857" OwnerUserId="7972" ParentId="25354" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;Quite an old question, but as it came up again today: &lt;/p&gt;&#10;&#10;&lt;p&gt;The general keyword is &quot;validation in analytical chemistry&quot; and as such it is a bit off-topic here (but as there is no Chemistry site here (yet: &lt;a href=&quot;http://area51.stackexchange.com/proposals/4964/chemistry&quot;&gt;http://area51.stackexchange.com/proposals/4964/chemistry&lt;/a&gt;, I guess we can leave it here for the moment) &lt;/p&gt;&#10;&#10;&lt;p&gt;There are some standard procedures in analytical chemistry for this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Books: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Funk et. al: Quality Assurance in Analytical Chemistry, Wiley-VCH.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Kromidas (Hrsg.): Handbuch Validierung in der Analytik, Wiley-VCH &lt;br/&gt;&#10;(I don't know whether there is an English version and I don't have it (yet). But the table of contents lists validation of multivariate calibration.) &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The IUPAC has something to say about that, too:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Danzer, K. and Currie, L. A.: Guidelines for calibration in analytical chemistry. Part I. Fundamentals and single component calibration, Pure and Applied Chemistry, IUPAC, 1998, 4, 993-1014&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Danzer, K. and Otto, M. and Currie, L. A.: Guidelines for calibration in analytical chemistry. Part 2: Multicomponent calibration Pure and Applied Chemistry, 2004, 76, 1215-1225&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-03-28T13:40:29.527" Id="25426" LastActivityDate="2012-03-28T13:40:29.527" OwnerUserId="4598" ParentId="527" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You may want to consider other strategies based on propensity scores, like including them as model covariates, or very similar concepts, like Inverse-Probability-of-Treatment weights. These might work in situations where you can't, or don't want, to deal with matching.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1407370/&quot; rel=&quot;nofollow&quot;&gt;This&lt;/a&gt; seems like a decent overview.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-28T16:24:28.550" Id="25434" LastActivityDate="2012-03-28T16:24:28.550" OwnerUserId="5836" ParentId="25392" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="25443" AnswerCount="1" Body="&lt;p&gt;Ok, I have searched and searched and just have no clue where to start.  First, what I would like to do is produce a QQ-plot (or even a readable residual plot) to look at the fit of my model.  I guess I just don't understand how the parameters that go into &lt;code&gt;qnbinom()&lt;/code&gt; are obtained from the output of &lt;code&gt;MASS::glm.nb()&lt;/code&gt;.  I am attempting to use &lt;code&gt;probplot()&lt;/code&gt; from package &lt;a href=&quot;http://cran.r-project.org/web/packages/e1071/index.html&quot; rel=&quot;nofollow&quot;&gt;e1071&lt;/a&gt;, but am unsure of the inputs needed.  It would be great if someone having experience with fitting negative binomials could lend a hand.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, I came across a residual plot here: &lt;a href=&quot;http://www.stat.cmu.edu/~hseltman/Rclass/R8.R&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.cmu.edu/~hseltman/Rclass/R8.R&lt;/a&gt;&#10;I can make it to work, but I don't know how to interpret it or if I am using it correctly.  Has anyone else used this?&lt;/p&gt;&#10;&#10;&lt;p&gt;At the moment I am relying on the AIC and plots of fitted vs. actual values to assess the fit of my model and I would like something a little better!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&#10;Hopefully this will clarify what I am asking.  With &lt;code&gt;qnbinom(p, size, prob, mu, lower.tail = TRUE, log.p = FALSE)&lt;/code&gt;, how do I (or is it even possible to) get the &lt;code&gt;p&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;prob&lt;/code&gt;, &lt;code&gt;mu&lt;/code&gt; from the output of a &lt;code&gt;glm.nb&lt;/code&gt; fit model?  From my research, I have found that size is the dispersion parameter, but other than that I'm not sure where to go.  I know theta goes in there somehow, just not sure how to get it in the form needed. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; Ok, once I have a &lt;code&gt;distplot()&lt;/code&gt;, is there a guide to interpreting it?  I am fairly positive I have a bad fit because I have a curved plot with a red line going through it (with many points on the tails far from the red line).  The prob: ML = 0.011, is this rejecting that the distribution is from the negative binomial specified?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-03-28T17:00:33.620" FavoriteCount="1" Id="25440" LastActivityDate="2012-03-29T15:05:50.040" LastEditDate="2012-03-29T15:05:50.040" LastEditorUserId="10191" OwnerUserId="10191" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;negative-binomial&gt;&lt;validation&gt;" Title="Model validation after fitting a negative binomial GLM in R" ViewCount="2699" />
  <row Body="&lt;p&gt;I would suggest calculating the &lt;a href=&quot;http://en.wikipedia.org/wiki/Edit_distance&quot; rel=&quot;nofollow&quot;&gt;edit distance&lt;/a&gt; between the entered string and a master list of known countries and country abbreviations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-28T17:17:25.910" Id="25442" LastActivityDate="2012-03-28T17:17:25.910" OwnerUserId="2817" ParentId="25435" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You might find &lt;code&gt;distplot()&lt;/code&gt; from the &lt;a href=&quot;http://cran.r-project.org/web/packages/vcd/index.html&quot; rel=&quot;nofollow&quot;&gt;vcd&lt;/a&gt; package useful either for the original data (edit: you can't use it on residuals). This plots Friendly's &quot;negativebinomialness plots&quot; and provides how well the negative binomial model fits&lt;br&gt;&#10;&lt;code&gt;distplot(response, type = &quot;nbinomial&quot;, ...)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To obtain the parameters: &lt;code&gt;glm.nb&lt;/code&gt; uses the &quot;Gamma mixture of Poisson&quot; representation. It is actually a log-linear model that is fitted, so you should get the mean as $\exp(X\beta)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, let's say your data come from a negbin with mean 5 and theta of 1 (in the alternative representation as described above). Then you can get the mean estimate simply by  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(10)  &#10;df &amp;lt;- data.frame(y=rnbinom(100,size=1,mu=5))  &#10;m0 &amp;lt;- glm.nb(y~1,data=df)  &#10;m0  &#10;exp(coef(m0))  &#10;m0$theta  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which are in this case 5.1 for the mean (pretty close) and 1.6 for the dispersion parameter (pretty far off). &lt;/p&gt;&#10;&#10;&lt;p&gt;If you fit a model for the conditional mode, you interpret it accordingly as in every other log linear model, see &lt;a href=&quot;http://stats.stackexchange.com/questions/6728/understanding-the-parameters-inside-the-negative-binomial-distribution&quot;&gt;this discussion on stack exchange&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: If you want to know how to get the mean in a negbin regression model you need to sum up the linear predictor $X\beta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: I take the &lt;code&gt;quine&lt;/code&gt; data and fit  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- glm.nb(Days~Sex,data=quine)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;now males are 1 females are 0. To get the mean for males you write &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; exp(coef(m1)[1]+coef(m1)[2]*1)  &#10;[1] 17.95455    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and for females&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; exp(coef(m1)[1]+coef(m1)[2]*0)     &#10;[1] 15.225  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now to get the mean you must weight this with the occurence of all females and males which is &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; table(quine$Sex)  &#10; F  M   &#10;80 66  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and hence the mean is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; (80/(66+80))*15.225+(66/(80+66))*17.95455  &#10;[1] 16.45685  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is confirmed by&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; nb0 &amp;lt;- glm.nb(Days ~ 1, data = quine)    &#10;&amp;gt; exp(coef(nb0))  &#10;(Intercept)  &#10;[1] 16.4589&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(apart from rounding errors). &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-28T17:26:42.497" Id="25443" LastActivityDate="2012-03-29T10:01:20.090" LastEditDate="2012-03-29T10:01:20.090" LastEditorUserId="8413" OwnerUserId="8413" ParentId="25440" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;I am assessing the acceptability of a sentence in a within-subjects design with two crossed factors (type of sentence, missing word). After reading each sentence, the participants answer a simple yes/no question which is my dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because a dichotomous variable is not normally distributed I am guessing I can't use an ANOVA, what is my alternative? &#10;I want to test which group of sentences is less/more acceptable than the others.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-28T17:59:59.990" Id="25445" LastActivityDate="2012-04-27T22:12:06.327" OwnerUserId="10193" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;experiment-design&gt;&lt;within-subjects&gt;" Title="How to deal with a 2x2 experimental design if the dependent variable is dichotomous?" ViewCount="335" />
  <row Body="&lt;p&gt;Another approach would be to &quot;manually&quot; shrink the response rate to the sample mean using a quadratic penalty for deviating from the mean. If one uses quadratic loss function (with weighting each list by numberPromoted to take into account the amount of data in each list) then this becomes a modified version of weighted ridge regression with shrinking to the mean as opposed to shrinking to zero (aka &quot;poor man's bayesian&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;The penalty coefficient can be tuned using cross-validation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-03-28T19:02:34.013" Id="25446" LastActivityDate="2012-03-28T19:56:46.983" LastEditDate="2012-03-28T19:56:46.983" LastEditorUserId="6129" OwnerUserId="6129" ParentId="25324" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="26374" AnswerCount="1" Body="&lt;p&gt;I have four datasets: D1, D2, D3 and D4. Each dataset contains elements (&lt;strong&gt;&lt;em&gt;different sample size in each dataset&lt;/em&gt;&lt;/strong&gt;) that can be described by 100 categories (1 represents that the element belongs to that category and 0 represents otherwise). So for example,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;D1&#10;Element     Category 1       Category 2       Category 3     ....&#10;1               1                0                1&#10;2               0                1                1&#10;3               1                1                1&#10;..&#10;..&#10;&#10;D2&#10;Element     Category 1       Category 2       Category 3     ....&#10;1               1                0                0&#10;2               0                0                1&#10;3               1                0                1&#10;..&#10;..&#10;&#10;D3&#10;Element     Category 1       Category 2       Category 3     ....&#10;1               1                0                1&#10;2               0                1                0&#10;3               0                1                1&#10;..&#10;..&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to test for any statistical differences in the frequencies of the categories in each dataset. Can someone advise on how to approach this? &lt;/p&gt;&#10;&#10;&lt;p&gt;My initial thought is to get the frequencies of each category and get a single row that represents a dataset like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Dataset          f(Category 1)       f(Category 2)       f(Category 3)     ....&#10;D1                  20                    30                   10&#10;D2                  10                    10                   40&#10;D3                  20                    40                   15&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And then check to see for any statistical significance but am still not clear on what will be useful in understanding. My question in plain English is to understand the &quot;differences&quot; between the datasets. Any suggestions?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-03-28T20:17:51.070" FavoriteCount="2" Id="25451" LastActivityDate="2012-04-12T18:57:51.520" LastEditDate="2012-03-28T20:27:50.900" LastEditorUserId="2164" OwnerUserId="2164" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;statistical-significance&gt;&lt;dataset&gt;&lt;categorical-data&gt;&lt;binomial&gt;" Title="Characterizing datasets and estimating statistically significant differences?" ViewCount="431" />
  
  <row Body="&lt;p&gt;You'll want to use a mixed effects model treating &lt;code&gt;subject&lt;/code&gt; as a random effect, &lt;code&gt;type_of_sentence&lt;/code&gt; and &lt;code&gt;missing_word&lt;/code&gt; as fixed effects, and specifying the binomial error family. Search this site for mixed effect models for more detail.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-28T21:30:20.877" Id="25454" LastActivityDate="2012-03-28T21:30:20.877" OwnerUserId="364" ParentId="25445" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Using R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(rms)   &#10;x &amp;lt;- cph(Surv(time, event) ~ pred 1 + pred2, x=TRUE, y=TRUE, &#10;         surv=TRUE, time.inc =1, dxy = TRUE, data = dataname)    &#10;c1 &amp;lt;- calibrate(x, u=1)    &#10;plot(c1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-03-28T21:31:09.410" Id="25455" LastActivityDate="2012-03-28T21:45:51.507" LastEditDate="2012-03-28T21:45:51.507" LastEditorUserId="930" OwnerUserId="10196" ParentId="22428" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Does R fall into your idea of opaque software?  If you are interested in this sort of calculation I would strongly recommend you use a stats package of some sort - and probably R in particular.  In R, package pwr provides the function pwr.chisq.test which answers your question for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;It isn't quite as simple as saying &quot;for each sample size, what is the power of my test?&quot; because as well as sample size, there is the question of the size of the effect in the underlying population you are inferring to.  eg if there is a massive effect, then even a very small sample has a high power.  As the effect gets smaller, you need a bigger sample size for the same power.&lt;/p&gt;&#10;&#10;&lt;p&gt;The documentation for pwr.chisq.test refers to &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0805802835&quot; rel=&quot;nofollow&quot;&gt;Cohen, J. (1988). &lt;em&gt;Statistical power analysis for the behavioral sciences&lt;/em&gt; (2nd ed.)&lt;/a&gt;. Hillsdale,NJ: Lawrence Erlbaum.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, a quick google search comes up with &lt;a href=&quot;http://sites.stat.psu.edu/~dhunter/asymp/fall2002/lectures/ln11.pdf&quot; rel=&quot;nofollow&quot;&gt;this reference (lecture 25)&lt;/a&gt;, which shows that under the alternative hypothesis the test statistic has (asymptotically) a non-central Chi square distribution and provides a way to estimate the non-centrality parameter for a given alternative hypothesis.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-29T04:01:45.830" Id="25461" LastActivityDate="2012-03-29T18:41:34.393" LastEditDate="2012-03-29T18:41:34.393" LastEditorUserId="7972" OwnerUserId="7972" ParentId="25386" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I want to know how to to input a self-defined distance in R, in hierarchical clustering analysis. R implements only some default distance metrics, for example &quot;Euclidean&quot;, &quot;Manhattan&quot; etc. Suppose I want to input a self-defined distance '1-cos(x-y)'. Then what should I do?&lt;/p&gt;&#10;&#10;&lt;p&gt;Writing a function is obviously a solution. But, it will be quite complicated, and also difficult to write. Please help me. I am unable to write the code.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-29T07:20:23.287" FavoriteCount="2" Id="25468" LastActivityDate="2012-03-29T18:06:23.243" LastEditDate="2012-03-29T09:55:13.563" LastEditorUserId="930" OwnerUserId="10202" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;clustering&gt;&lt;distance-functions&gt;" Title="How to input self-defined distance function in R?" ViewCount="1901" />
  
  <row Body="&lt;p&gt;A process $Y_t$ is stationary when for any vector of times $(t_1,...,t_n)$ and for every time interval $\tau$ the joint distribution of the vector&#10;$$
&#10;(Y_{t_1},...,Y_{t_n})
  <row Body="&lt;p&gt;I have a deep lack of interest in big lotteries, so I'm going to answer this in terms of possible strategies people might use to pick a sequence of numbers that are randomly generated.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first strategy is the familiar one of picking a distinctly systematic sequence such as birthdays, etc. on the mystical belief that since the number is personal, if it is chosen, chance has effectively chosen the chooser and validated them with a prize.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But the second more interesting strategy is that people try to choose a 'random number'.  If there is regularity in what they might choose, i.e. if they aren't very good at this, then your 'strategy' would be to choose one of the ones outside these regularities.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is, it turns out, an interesting line of work assuming that subjective randomness judgements are actually judgements about the representativeness of data from specific generation models, e.g. 'alternation' models of coin flips.  Consequently people's judgements of whether a sequence is random are both incorrect and predictable.  Some old work that runs with this idea is &lt;a href=&quot;http://web.mit.edu/cocosci/Papers/random.pdf&quot;&gt;Griffiths and Tenenbaum (2001)&lt;/a&gt; and &lt;a href=&quot;http://web.mit.edu/cocosci/Papers/complex.pdf&quot;&gt;Griffiths and Tenenbaum (2003)&lt;/a&gt;.  No doubt there is more recent stuff, including lottery-specific things like @chl's JRSS A reference.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-29T09:20:56.057" Id="25473" LastActivityDate="2012-03-29T09:20:56.057" OwnerUserId="1739" ParentId="25464" PostTypeId="2" Score="10" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'd like to estimate the unique effects for each variable in a linear model, &#10;however I am unsure if I calculating these correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using a model estimated using each of the variables in my data plus a &#10;binary interaction with each variable, e.g. &lt;code&gt;y ~ x + x:z&lt;/code&gt; where x is a vector and &#10;z is two factors which is present in x. Doing this in R I believe a two levels factor is simply c(0,1).&lt;/p&gt;&#10;&#10;&lt;p&gt;From what understand I can calculate the individual effects of the variables &#10;for each case of the binary interaction term is as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;When the interaction term is 0, the variable effect of x on y is the &#10;estimated model coefficient for x&lt;/li&gt;&#10;&lt;li&gt;When the interaction term is 1, the variable effect of x on y is the &#10;estimated model coefficient for x plus the coefficient for the interaction &#10;term of x:z&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Assuming this is correct I would also like to calculate the standard errors &#10;associated with the each effect. I looked at &lt;a href=&quot;http://stats.stackexchange.com/questions/3653/adding-coefficients-to-obtain-interaction-effects-what-to-do-with-ses&quot;&gt;this question&lt;/a&gt; which states &#10;that the associated standard errors can be calculated as follows &lt;code&gt;sqrt(x + x:z + &#10;2*cov(x,x:z))&lt;/code&gt;. This however produces much smaller standard errors compared to &#10;the non-interaction term effects. This makes me assume I am doing something &#10;wrong. Could you tell me the correct way to estimate these effects?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-29T17:19:39.540" Id="25490" LastActivityDate="2012-04-28T23:12:28.580" LastEditDate="2012-03-29T19:33:30.163" LastEditorUserId="5659" OwnerUserId="5659" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;modeling&gt;&lt;standard-error&gt;" Title="How to correctly calculate linear model variable effects?" ViewCount="289" />
&#10;        $$  Now, using the $QR$ decomposition, we have \begin{align*}&#10;            (R^T Q^T) (QR) \hat \beta &amp;amp;= R^T Q^T y \\&#10;            R \hat \beta &amp;amp;= Q^T y&#10;        \end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;$R$ is upper triangular, we can write \begin{align*}&#10;        R_{pp} \hat \beta_p &amp;amp;= \langle q_p, y \rangle \\&#10;        \| z_p \| \hat \beta_p &amp;amp;= \| z_p \|^{-1} \langle z_p, y \rangle \\&#10;        \hat \beta_p &amp;amp;= \frac{\langle z_p, y \rangle}{\| z_p \|^2}&#10;    \end{align*} in accordance with our previous results.  Now, by back substitution, we can obtain the sequence of regression coefficients $\hat \beta_j$.  As an example, to calculate $\hat \beta_{p-1}$, we have \begin{align*}&#10;        R_{p-1, p-1} \hat \beta_{p-1} + R_{p-1,p} \hat \beta_p &amp;amp;= \langle q_{p-1}, y \rangle \\&#10;        \| z_{p-1} \| \hat \beta_{p-1} + \| z_{p-1} \| \gamma_{p-1,p} \hat \beta_p &amp;amp;= \| z_{p-1} \|^{-1} \langle z_{p-1}, y \rangle &#10;    \end{align*} and then solving for $\hat \beta_{p-1}$. This process can be repeated for all $\beta_j$, thus obtaining the regression coefficients in one pass of the Gram-Schmidt procedure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-29T19:57:38.590" Id="25500" LastActivityDate="2012-03-29T19:57:38.590" OwnerUserId="10199" ParentId="21022" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Kevin, please be carefull since I'll have to change your notation a little bit: your $z_i$'s are not my $z_i$'s. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the following Bayesian solution is worth a try. Cook a random parameter $\Lambda&amp;gt;0$ and let $Z_1,\dots,Z_n$ be conditionally i.i.d., given $\Lambda=\lambda$, with $Z_i\mid\Lambda = \lambda \sim \textrm{Poisson}(\lambda)$. Use the notation $Z=(Z_1,\dots,Z_n)$. You already have a sample $z=(z_1,\dots,z_n)$ of the $Z_i$'s, with $n=2^{28}$. Define the random variables &#10;$$\Theta_i = P\{Z_i=k\mid \Lambda\} = \frac{e^{-\Lambda}\Lambda^k }{k!} \, ,&#10;$$&#10;for $i\geq 0$ (if this is not clear, take a &lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_expectation#Definition_of_conditional_probability&quot; rel=&quot;nofollow&quot;&gt;look&lt;/a&gt;). Now, in this formulation your quadratic forms $Q_i=Q_i(\Theta_0,\dots,\Theta_i) = Q_i(\Lambda)$ are functions of $\Lambda$. So, the $Q_i$'s are random and you want to determine the posterior probability&#10;$$&#10;  P\{Q_7&amp;lt;Q_6 \,\,\,\textrm{and}\,\,\, Q_7&amp;lt;Q_8\mid Z=z\} \, . \qquad (*)&#10;$$&#10;With a prior $\Lambda\sim\textrm{Gamma}(a,b)$, using Bayes Theorem we have&#10;$$&#10;  \Lambda\mid Z=z \sim \, \textrm{Gamma}\left( a + \sum_{i=1}^n z_i, b + n\right) \, .&#10;$$&#10;You calculate $(*)$ generating i.i.d. $\lambda_i$'s from the former distribution (use &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt;!) and computing&#10;$$&#10;  \frac{1}{N} \sum_{i=1}^N I_{(-\infty,Q_6(\lambda_i))\cap(Q_8(\lambda_i),\infty)}(Q_7(\lambda_i)) \, ,&#10;$$&#10;which converges, by the strong law of large numbers, to $(*)$ almost surely. To get a &quot;yes&quot; to your original question, this posterior probability must be &quot;big enough&quot;.  With such a huge sample ($n=2^{28}$), I think it is possible to play with the values of $a$ and $b$ to make your prior choice not much &quot;informative&quot;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-29T20:37:39.003" Id="25502" LastActivityDate="2012-09-01T15:11:20.273" LastEditDate="2012-09-01T15:11:20.273" LastEditorUserId="9394" OwnerUserId="9394" ParentId="23761" PostTypeId="2" Score="0" />
  <row AnswerCount="4" Body="&lt;p&gt;There is a question with &lt;a href=&quot;http://programmers.stackexchange.com/questions/25139/efficiency-of-self-education&quot;&gt;similar intent on programmers.SE&lt;/a&gt;. That question has some quite good answers, but the general theme seems to be that without self study, you get no-where. &lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously there are some major difference between programming and statistics - with programming, you're really just learning some basic logic, and then applying it repeatedly. New languages all use the same basic concepts. Self study allows you learn more advanced concepts, and become more efficient. This kind of stuff is quite difficult to teach. &lt;/p&gt;&#10;&#10;&lt;p&gt;Statistics is quite different. It's easy to apply the logic involved - because somone else has usually laid out the methodology. Indeed, the methodology is usually most of what is taught in universities. But statistics is really far deeper than that, and involves some really high-level concepts. It's hard to even look for those concepts, if all you've be taught is applied statistics, let alone to understand them (although I wonder how much this may be due to jargon in the field). Also, I find that self study in programming involves reading a lot of short articles/blogs to introduce yourself to new concepts, whereas accessible articles about stats are nearly always aimed at the total beginner, and are therefore somewhat useless to an advancing novice, like myself.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the question is: Is self study more or less appropriate than a university education, for statistics? What methodologies for self study are there that work? Any examples of what has worked for people before would be welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;(this probably should be a community wiki, but I see no checkbox)&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2012-03-29T22:41:43.983" CreationDate="2012-03-29T21:21:32.330" FavoriteCount="12" Id="25506" LastActivityDate="2013-04-24T21:54:13.357" OwnerUserId="9007" PostTypeId="1" Score="22" Tags="&lt;self-study&gt;&lt;teaching&gt;&lt;references&gt;&lt;education&gt;" Title="Self study vs a taught education?" ViewCount="1933" />
  <row Body="&lt;p&gt;I think I'm in a fairly similar place, but I'll take a stab.  I started out as a sociology graduate student and, once I had completed all of the stats courses available through my department, wandered into some grad-level courses from the stats department at my university.  It was a revelation; the way that the stats professors approached problems was &lt;em&gt;radically&lt;/em&gt; different from my soc professors - much more intuitive and inspiring than what I had learned before, much less formulaic, and dependent on a lot of things that either I hadn't been taught or hadn't managed to learn in my more foundational courses.  I had to teach myself a lot of things over again just to keep up, and I still worry that I haven't truly nailed those foundational concepts down.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the intervening four or five years, I've spent a great deal of time reading widely - blogs, this site, and some standout textbooks have been really helpful.  But that self-learning has limits, the greatest of which isn't that I haven't sat through some lectures in school but rather that it's been four or five years since I've worked closely with somebody who actually knew any more than I did.  This site is my primary source of getting my incorrect notions shot down.  That scares me, to the point that I'm planning on applying to MS programs in biostats this fall - to take some interesting courses, definitely, but also because I just want somebody to run roughshod over my ideas and find out what I've really learned.&lt;/p&gt;&#10;&#10;&lt;p&gt;In contrast, I've been teaching myself R over roughly the same period and under the same conditions.  Until I helped found an R user group about a year and a half ago, I also didn't really have anyone to point out blatantly stupid constructs in my code.  But I don't feel nearly the same anxiety about my code, in large part because programming ultimately comes down to a question of whether something works.  I don't mean to diminish the challenges there - I've been on StackOverflow long enough to know that, for real software developers, there's a huge amount of expertise that goes into making something that's elegant, performant, maintainable, adaptable, and easy-to-use.  But software is ultimately judged on how well it performs its function.  As you say, statistics has almost the reverse problem - modern stats software makes it relatively easy to crank out complex models, but in many cases we don't have good systems in place for ensuring that those models are worth a damn.  It's difficult to recreate many published analyses, and reproducing previously-published studies from scratch isn't as glamorous as making new discoveries (apply scare quotes as you see fit).  I almost always know when my programs are junk, but I'm never entirely certain that my models are good.&lt;/p&gt;&#10;&#10;&lt;p&gt;So... as in programming, I think self-learning is essential.  But I also think it's critically important to have a mentor or peer around who will kick around ideas with you, expose you to new thinking, and kick your ass when necessary.  Formal education is one way to meet people like that.  Whether it's an efficient one depends more on your circumstances...&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2012-03-29T23:16:33.697" CreationDate="2012-03-29T23:16:33.697" Id="25509" LastActivityDate="2012-03-29T23:16:33.697" OwnerUserId="71" ParentId="25506" PostTypeId="2" Score="12" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a binary classification problem. &#10;My inputs consist of a time-series of values plus some binary values.&#10;For real-valued inputs I would usually use a neural network, while for binary-values inputs I would use either a neural network with a step function or some other method fit for binary inputs (like a maximum entropy classifier)&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case where input types are mixed what architecture should be used?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-30T00:05:44.637" Id="25513" LastActivityDate="2012-04-29T01:12:26.730" OwnerUserId="6317" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;neural-networks&gt;&lt;binary&gt;" Title="Binary classification problem" ViewCount="235" />
  
  
  <row Body="&lt;p&gt;Unfortunately, I don't know SPSS. That said, if you want to carry out a Wald test where the null is $H_0: \beta_{groupA} - \beta_{groupB} = 0$ you could  ask SPSS the variance/covariance matrix of your parameter estimates and construct the Wald test by hand.&lt;/p&gt;&#10;&#10;&lt;p&gt;Under $H_0$ your test statistics $\chi^2_{obs}$ is distributed as a $\chi^2$ r.v. with 1 degree of freedom&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  <row Body="&lt;p&gt;To explore the data you could use descriptives and it's very important to plot the data to get a visual representation and a feel for the data. &#10;It sounds like a regression could work for you, using dummies for the events and the different countries. Also, you could decide to give the countries a different slope and intercept. Finally, with time-series data, be careful of violations of the assumptions of regression, most specifically autocorrelation. Good luck!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-03-30T08:55:15.410" Id="25535" LastActivityDate="2012-03-30T08:55:15.410" OwnerUserId="10210" ParentId="25532" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;This is to correct &lt;a href=&quot;http://stats.stackexchange.com/a/22078&quot;&gt;another answer&lt;/a&gt; that any observational studies, whether cross-sectional or time-series or panel, do not give you causal relationships! If you run a cross-sectional regression of a group of people you may 'found' that there is a positive association between salary and whether a person wears trousers (as opposed to skirts). Of course what to wear does not &lt;strong&gt;&lt;em&gt;cause&lt;/em&gt;&lt;/strong&gt; higher/lower salary. The true omitted factor here is gender!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-30T09:16:24.530" Id="25536" LastActivityDate="2012-03-30T14:06:04.277" LastEditDate="2012-03-30T14:06:04.277" LastEditorUserId="919" OwnerUserId="10235" ParentId="22069" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;An example&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're happy with R you might want to look at the &lt;code&gt;Seatbelts&lt;/code&gt; dataset that's built into the base R distribution.  It sounds very similar.  Like your problem, this consists of several time series of counts (front seat casualties) and an intervention/treatment (imposition of a seatbelt law) that affects only one of them (the front seat passenger sequence) and a bunch of covariates (seasons, petrol prices etc).  On the help page you can see what a tiny analysis would look like.  You'll notice several things there:&lt;/p&gt;&#10;&#10;&lt;p&gt;To use count data it's often helpful to log the outcome.  That enables you to work with proportional increases rather than absolute ones which is usually what you want.  It also allowing you to fake a log linear link without the hassle of fitting a non-linear models.&lt;/p&gt;&#10;&#10;&lt;p&gt;The basic visual analysis consists of building some model, an ARIMA model in the example, of the series before the intervention and then projecting it forward, and comparing it to what actually happened in the treatment in a plot.  The code to do so is the line with the &lt;code&gt;ts.plot&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Time series modeling&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple linear modelling approach, such as you see on the last line of the example, adds an intervention variable to indicate the period, a seasonal component, and an autoregressive term to deal with the autocorrelation.  When you are content that the non-intervention variables are well set up, particularly that you have captured any seasonal components, christmas or weekend effects, or whatever it is in your movie sales domain that systematically drive variation in sales without being related to the intervention you are interested in, then you may want to interpret your intervention variable as a causal effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;In that simple analysis the intervention causes a shift in average level.  However, it might have different effects that you'd want to model differently.&lt;/p&gt;&#10;&#10;&lt;p&gt;What else you'd want to do depends on the model class.  For ARIMA modelling the basic issues are 'stationarity' and AR, MA, and seasonal 'order'.  It is also possible to take a state space approach.  Any good Time Series text will discuss these possibilities.  I quite like &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387293175&quot;&gt;Shumway and Stoffer (2006)&lt;/a&gt; and &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0199228876&quot;&gt;Commandeur (2007)&lt;/a&gt; but there are many good choices, and lots of web material.&lt;/p&gt;&#10;&#10;&lt;p&gt;Time series analysis can get quite complex quite fast, so taking a graphical exploratory approach first is very sensible, so you know how much time it's worth investing in figuring out these more complex parametric models.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Regression approaches&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative, non-time series approach is to treat the problem as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Regression_discontinuity_design&quot;&gt;regression discontinuity design&lt;/a&gt;.  There you compare the the period either side of the intervention to see the causal effect of the intervention.  &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521671930&quot;&gt;Morgan and Winship (2007)&lt;/a&gt; has a discussion of the pros and cons of this approach.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-03-30T10:05:51.513" Id="25537" LastActivityDate="2012-03-30T10:05:51.513" OwnerUserId="1739" ParentId="25532" PostTypeId="2" Score="6" />
&#10;{\sum_{DERF}P(C=c)\times P(R)\times P(E|C=c,R) \times P(F|E) \times P(D|F)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$=\frac{\sum_{ERF}P(C=c)\times P(R)\times P(E|C=c,R) \times P(F|E) \times P(D=d|F)}
&#10;{\sum_{DERF}P(C=c)\times P(R)\times P(E|C=c,R) \times \sum_F P(F|E) \times \sum_D P(D|F)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$=\frac{\sum_{ERF}P(C=c)\times P(R)\times P(E|C=c,R) \times P(F|E) \times P(D=d|F)}
  <row AcceptedAnswerId="25564" AnswerCount="1" Body="&lt;p&gt;I need to use ANCOVA to analyse effect of executive functions (EF-covariate) on scores collected during a test. I want to compare this effect between a control and schizophrenics group (variable diagnoza).&lt;/p&gt;&#10;&#10;&lt;p&gt;I conducted an analysis of covariance and I have some questions.&#10; Here is SPSS output for my ANCOVA model:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/xA1jg.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it a problem that the effect of EF is not significant? Can I interpret this result as follows: There is a significant difference in MIE between groups when the effect of EF is controlled?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-03-30T11:10:40.677" FavoriteCount="1" Id="25542" LastActivityDate="2012-03-30T18:57:34.920" LastEditDate="2012-03-30T11:55:21.020" LastEditorUserId="930" OwnerUserId="10238" PostTypeId="1" Score="2" Tags="&lt;interpretation&gt;&lt;ancova&gt;" Title="How to interpret non-significant effect of a covariate in ANCOVA?" ViewCount="5406" />
&#10;\beta_1 =\mu_1/\sigma_1^2\,,\quad \alpha_{11}-\mu_1\beta_1 =0
&#10;\alpha_{22} = \mu_2\beta_2 - \alpha_{21}\mu_1\,,\quad
  <row Body="&lt;p&gt;@andrea is giving you good advice.  It makes little sense to fit a model with the diagnosis*EF interaction, but without EF.  Furthermore, if both EF and the interaction are 'non-significant' that would generally be taken to imply that they are unrelated to the response variable.  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would urge some caution here.  First, it is not clear how much power you have to detect an effect, should one actually exist.  I notice that you have an $N$ of 61.  That simply may not be enough data to resolve what is nonetheless a real difference.  Consider this, without meaning to be dismissive, an ANCOVA with 2 groups is essentially a glorified t-test.  To use two arbitrary values that are often taken as standard, a 'medium' effect size &amp;amp; 80% power, a simple independent groups t-test requires an $N$ of 128, if I remember correctly.  In this case, there are additional complexities due to this being an ANCOVA (i.e., you are burning extra degrees of freedom, but also have the opportunity to account for some of the residual variability).  There are additional issues: power is maximal to detect a &lt;em&gt;linear&lt;/em&gt; effect, but it's thinkable that a non-linear effect exists, you could have range restrictions that limit power, and my first guess would be that EF is not orthogonal to diagnosis (i.e., the distribution differs between the two groups, although I don't know).  I would urge you to look at your data.  For example, you could make a scatterplot of EF vs. MIE, with two different plotting symbols for the different diagnoses, and overlaid with a loess fit for each group.  Check to see if the distributions and fits look sufficiently similar for your satisfaction before deciding what to conclude and what to do next.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-03-30T18:57:34.920" Id="25564" LastActivityDate="2012-03-30T18:57:34.920" OwnerUserId="7290" ParentId="25542" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;I'd like to find out if there is a statistical test that I could use to determine if a mean difference is significantly greater than another mean difference.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To determine if one mean is different from another mean, I would use a t-test, but that would not apply here because there are not actual distributions; instead, there are differences.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Below is a replicable example in R to help explain. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x1 &amp;lt;- c(1,1,1,1,0,0,0,0,1,1,1,1,1)&#10;x2 &amp;lt;- c(1,1,1,1,0,0,0,0,0,0,0,0,0)&#10;&#10;x1_mean_diff &amp;lt;- mean(x1) - mean(x2)&#10;x1_mean_diff #sig different with a t test&#10;&#10;y1 &amp;lt;- c(1,1,1,1,0,0,0,0,1,1,1,1,1)&#10;y2 &amp;lt;- c(1,1,1,1,0,0,0,0,0,1,1,0,0)&#10;&#10;y1_mean_diff &amp;lt;- mean(y1) - mean(y2)&#10;y1_mean_diff #NOT sig different with a t test&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Basically, I'd like to see if x1_mean_diff is significantly greater than y1_mean_diff. Is there a way to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please let me know if I'm being unclear.  Thanks!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-03-30T22:11:10.803" Id="25575" LastActivityDate="2012-09-02T07:09:53.540" OwnerUserId="7188" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;t-test&gt;&lt;mean&gt;&lt;group-differences&gt;" Title="Significance test for two mean differences? Like a t-test but for comparing differences" ViewCount="980" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have 500 rows and three columns. Two of the variables (that i want to analyze) are in ordinal form and the other in scale. However, when i run the correspondence analysis wizard in SPSS i can only select the scale variable and not the ordinals. How is it possible to perform correspondence analysis on the ordinal variables? &lt;/p&gt;&#10;&#10;&lt;p&gt;I have something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Variable1 | Variable2&lt;/p&gt;&#10;&#10;&lt;p&gt;..Home .  .| Strong &lt;/p&gt;&#10;&#10;&lt;p&gt;..Work  . . | Strong        &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-03-30T22:55:30.483" Id="25577" LastActivityDate="2013-01-22T12:16:27.603" LastEditDate="2012-03-31T10:48:19.550" LastEditorUserId="9424" OwnerUserId="9424" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;spss&gt;&lt;categorical-data&gt;&lt;ordinal&gt;&lt;correspondence-analysis&gt;" Title="How to perform correspondence analysis on ordinal data in SPSS?" ViewCount="1067" />
  
&#10;\tau = 4 \Pr(X &amp;lt; X&amp;#39;, Y &amp;lt; Y&amp;#39; ) - 1 = 4 \Pr( F(X) &amp;lt; F(X&amp;#39;), G(Y) &amp;lt; G(Y&amp;#39;) )  - 1 \&amp;gt;,
  <row AcceptedAnswerId="28134" AnswerCount="3" Body="&lt;p&gt;I have problem with homogeneity of regression slopes in ANCOVA. I have two species of snakes and I want to compare their tail lengths (dependent variable), that are dependent on body length (in ANCOVA used as covariate). Other ANCOVA assumptions are met, but there exist significant interaction between species*bodylength (intercepts are comparable, slopes are different). &lt;/p&gt;&#10;&#10;&lt;p&gt;Andy Field´s SPSS guide tells that broken assumption of homogeneity of regression slopes could be cast aside using multilevel statistic model. I´m trying to correctly run this analysis whole day, but I am unable to do it. Problem is that I want to compare tail lengths between species, which is my top of hierarchy (at the bottom are individual specimen). Do I have to set variable species as subject (assuming some hierarchy)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Undoubtely I have to set body length as covariate and tail length as dependent variable. How can I compare my groups (species) independently of slopes (slopes are random)? What should be set as fixed factors and what random factors in SPSS? Field´s guide also tells that for mixed model analysis there is bug (version 17.0) related to factors dialog box such that categorical variables (in this case my species variable) should be put into covariate box (at least if it is bivariate). Is he right? I know what I want to do but dont know how to do it. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-03-31T18:11:30.513" Id="25615" LastActivityDate="2012-09-06T02:01:14.280" LastEditDate="2012-04-01T07:10:02.603" LastEditorUserId="8705" OwnerUserId="8705" PostTypeId="1" Score="2" Tags="&lt;spss&gt;&lt;multilevel-analysis&gt;&lt;ancova&gt;" Title="How to get rid of heterogeneity of regression slopes using multilevel modeling?" ViewCount="1134" />
  <row Body="&lt;p&gt;I perused the &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387096159&quot; rel=&quot;nofollow&quot;&gt;Use R!&lt;/a&gt; book once.  It didn't look too bad, but I didn't work through it, and I don't use nonlinear regression, either, I was just curious--so take that for what it's worth.  I think if you know a little stats and are comfortable with R already, and want to play around with nonlinear regression, it would be a low-commitment place to start.  &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-03-31T20:21:21.387" CreationDate="2012-03-31T18:37:54.070" Id="25617" LastActivityDate="2012-03-31T18:37:54.070" OwnerUserId="7290" ParentId="25608" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;First, I'm not sure what you mean by &quot;fixed effects&quot; regression as compared to OLS. In econometrics, at least, the standard fixed effects model is estimated via OLS. I'm assuming that you run a regression using group means instead of individual data, but I'm not sure.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your model without $x$, it is fully flexible: all combinations of year and treatment are given different expected values. You are making no linearity assumptions here. Once you add in $x$, you are assuming that the response of $y$ to $x$ is linear and that &lt;em&gt;this response does not depend upon the values of the fixed effects&lt;/em&gt;; that is, there is no heterogeneity in the impact of $x$. If there is heterogeneity, you can get different results using different estimation procedures. &lt;/p&gt;&#10;&#10;&lt;p&gt;My coauthors and I discuss these issues in our paper, &lt;a href=&quot;http://cgibbons.berkeley.edu/Research/Papers/Gibbons--FixedEffects.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Broken or Fixed Effects?&quot;&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-01T04:04:50.737" Id="25631" LastActivityDate="2012-04-01T04:04:50.737" OwnerUserId="401" ParentId="25069" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm in the process of calculating inventory levels for a bunch of SKUs and I'm wondering if you all-mighty statisticians had any card up your sleeves that I could borrow.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the deal: service levels are inevitably going to be affected by the amount of material I decide to put in stock. The more I stockpile, the higher the fill-rate will be (but of course inventory doesn't come cheap). The less I keep on-hand, the higher the chance of having a stock-out. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have to strike a balance between holding costs, and service level.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on past sales, I was able to plot the expected fill-rate against the volume of on-hand materials. Check it out:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wVMjE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can all see, the service level increases very rapidly at first, but then the growth rate starts decreasing quite abruptly. Going from 90% to 100% requires 9 times the amount of material it took to go from 0% to 90%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, getting to the point, is there any scientific way to tell where I should stop? How do I know when the marginal increase in fill-rate is too little for that much of an increase in stocked volume?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know choosing the right service level is mostly a &quot;political&quot; decision, but I really hoped that you guys could come up with some kind of suggestion to help me in the process. I'm kinda out of my element here. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you guys for your time!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-01T08:34:53.000" FavoriteCount="2" Id="25641" LastActivityDate="2012-09-28T17:07:33.867" OwnerUserId="9251" PostTypeId="1" Score="2" Tags="&lt;optimization&gt;" Title="Can statistics help in inventory management decisions?" ViewCount="753" />
  
  <row Body="&lt;p&gt;Linear regression can be illustrated geometrically in terms of an orthogonal projection of the predicted variable vector $\boldsymbol{y}$ onto the space defined by the predictor vectors $\boldsymbol{x}_{i}$. This approach is nicely explained in Wicken's book &quot;The Geometry of Multivariate Statistics&quot; (1994). Without loss of generality, assume centered variables. In the following diagrams, the length of a vector equals its standard deviation, and the cosine of the angle between two vectors equals their correlation (see &lt;a href=&quot;http://www.johndcook.com/blog/2010/06/17/covariance-and-law-of-cosines/&quot;&gt;here&lt;/a&gt;). The simple linear regression from $\boldsymbol{y}$ onto $\boldsymbol{x}$ then looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pl78D.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\boldsymbol{y}} = b \cdot \boldsymbol{x}$ is the prediction that results from the orthogonal projection of $\boldsymbol{y}$ onto the subspace defined by $\boldsymbol{x}$. $b$ is the projection of $\boldsymbol{y}$ in subspace coordinates (basis vector $\boldsymbol{x}$). This prediction minimizes the error $\boldsymbol{e} = \boldsymbol{y} - \hat{\boldsymbol{y}}$, i.e., it finds the closest point to $\boldsymbol{y}$ in the subspace defined by $\boldsymbol{x}$ (recall that minimizing the error sum of squares means minimizing the variance of the error, i.e., its squared length). With two correlated predictors $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$, the situation looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Dn29l.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\boldsymbol{y}$ is projected orthogonally onto $U$, the subspace (plane) spanned by $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$. The prediction $\hat{\boldsymbol{y}} = b_{1} \cdot \boldsymbol{x}_{1} + b_{2} \cdot \boldsymbol{x}_{2}$ is this projection. $b_{1}$ and $b_{2}$ are thus the ends of the dotted lines, i.e. the coordinates of $\hat{\boldsymbol{y}}$ in subspace coordinates (basis vectors $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The next thing to realize is that the orthogonal projections of $\hat{\boldsymbol{y}}$ onto $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$ are the same as the orthogonal projections of $\boldsymbol{y}$ itself onto $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bN9AC.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This allows us to directly compare the regression weights from each simple regression with the regression weights from the multiple regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZdNyW.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\boldsymbol{y}}_{1}$ and $\hat{\boldsymbol{y}}_{2}$ are the predictions from the simple regressions $\boldsymbol{y}$ onto $\boldsymbol{x}_{1}$, and $\boldsymbol{y}$ onto $\boldsymbol{x}_{2}$. Their endpoints give the individual regression weights $b^{1} = \rho_{x_{1} y} \cdot \sigma_{y}$ and $b^{2} = \rho_{x_{2} y} \cdot \sigma_{y}$, where $\rho_{x_{1} y}$ is the correlation between $\boldsymbol{x}_{1}$ and $\boldsymbol{y}$, and $\sigma_{y}$ is the standard deviation of $\boldsymbol{y}$. In contrast, the endpoints of the dotted lines give the regression weights from the multiple regression of $\boldsymbol{y}$ onto $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$: $b_{1} = \beta_{1} \sigma_{y}$, where $\beta_{1}$ is the standardized regression coefficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now it is easy to see that $b^{1}$ and $b^{2}$ will coincide exactly with $b_{1}$ and $b_{2}$ only if $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$ are orthogonal (or if $\boldsymbol{y}$ is orthogonal to the plane spanned by $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$). It is also easy to geometrically construct cases that sometimes seem puzzling, e.g., when the regression weight has the opposite sign as the bivariate correlation between a predictor and the predicted variable:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dHien.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, $\boldsymbol{x}_{1}$ and $\boldsymbol{x}_{2}$ are highly correlated. Now the sign of the correlation between $\boldsymbol{y}$ and $\boldsymbol{x}_{1}$ is positive (red line: orthogonal projection of $\boldsymbol{y}$ onto $\boldsymbol{x}_{1}$), but the regression weight from the multiple regression is negative (end of green line onto subspace defined by $\boldsymbol{x}_{1}$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-01T12:17:47.533" Id="25650" LastActivityDate="2012-04-01T13:33:36.837" LastEditDate="2012-04-01T13:33:36.837" LastEditorUserId="1909" OwnerUserId="1909" ParentId="25605" PostTypeId="2" Score="11" />
  
  
&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Integration is defined in terms of characteristic functions: the integral over an event $E$ with respect to a measure $d\mu$, written, $\int \cdots \int_E d\mu$, equals $\int \cdots \int I_E(x) d\mu(x)$ where the multiple integral is taken over all possible values of $x$ and $I_E(x) = 1$ when $x\in E$ and $I_E(x)=0$ otherwise.  The figure &lt;em&gt;immediately&lt;/em&gt; shows that the solution is&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\Pr}_f[(a,b)\in R] = \int \int_R f(a,b)\ da \ db$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the original double integral expression follows &lt;em&gt;immediately&lt;/em&gt; from this expression by the definition of integration.  For more about this, consult any textbook on measure theory and integration or--for a less formal approach--consult any advanced calculus text that covers multiple integration.  &lt;strong&gt;End&lt;/strong&gt; of edit.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;This is essentially problem #50 from Fred Mosteller's &lt;a href=&quot;http://books.google.com/books?id=QiuqPejnweEC&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Fifty Challenging Problems in Probability&lt;/em&gt;&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What is the probability that the quadratic equation $x^2 + 2bx + c = 0$ has real roots?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Solving it requires proposing some &quot;reasonable&quot; probability distribution for $(b,c)$.  Mosteller chooses a set of uniform distributions over a sequence of rectangles that grows without bound and takes the limit.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-04-01T19:28:21.187" Id="25662" LastActivityDate="2012-04-02T15:25:21.830" LastEditDate="2012-04-02T15:25:21.830" LastEditorUserId="919" OwnerUserId="919" ParentId="25661" PostTypeId="2" Score="5" />
  
&#10;   0 &amp;amp; \theta &amp;lt;{{x}_{(1)}}  \\
  
  
  
  
  <row Body="&lt;p&gt;I can sign under what Dirk and EpiGrad said; yet there is one more thing that makes R an unique lang in its niche -- data-oriented type system. &lt;/p&gt;&#10;&#10;&lt;p&gt;R's was especially designed for handling data, that's why it is vector-centered and has stuff like data.frames, factors, NAs and attributes.&lt;br&gt;&#10;Julia's types are on the other hand numerical-performance-oriented, thus we have scalars, well defined storage modes, unions and structs.&lt;/p&gt;&#10;&#10;&lt;p&gt;This may look benign, but everyone that has ever try to do stats with MATLAB knows that it really hurts.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, at least for me, Julia can't offer anything which I cannot fix with a few-line C chunk and kills a lot of really useful expressiveness. &lt;/p&gt;&#10;" CommentCount="8" CommunityOwnedDate="2012-04-02T13:14:12.740" CreationDate="2012-04-02T13:14:12.740" Id="25692" LastActivityDate="2012-04-02T13:14:12.740" OwnerUserId="88" ParentId="25672" PostTypeId="2" Score="25" />
  <row AcceptedAnswerId="25887" AnswerCount="2" Body="&lt;p&gt;We draw $N$ samples, each of size $n$, independently from a Normal $(\mu,\sigma^2)$ distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the $N$ samples we then choose the 2 samples which have the highest (absolute) Pearson correlation with each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the expected value of this correlation ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&#10;[P.S. This is not homework]&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-02T15:41:06.257" FavoriteCount="1" Id="25704" LastActivityDate="2012-04-06T16:12:04.577" LastEditDate="2012-04-02T16:27:29.707" LastEditorUserId="6884" OwnerUserId="6884" PostTypeId="1" Score="11" Tags="&lt;correlation&gt;" Title="Expected value of spurious correlation" ViewCount="310" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Clearly, L(θ|x)  is an increasing function of θ  on −∞&amp;lt;θ≤x1  .&#10;  (Does this even matter?).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes, this matters. Since L() is increasing wrt θ, we want to choose θ as large as we can in order maximize L(), but we have the constraint that θ≤x1, so the largest we can choose θ is x1.&lt;/p&gt;&#10;&#10;&lt;p&gt;So x1 is our &lt;strong&gt;unrestricted&lt;/strong&gt; maximum likelihood estimator for θ.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the numerator, however, we have the additional constraint θ≤θ0, so we have to take that into consideration, too.&lt;/p&gt;&#10;&#10;&lt;p&gt;Two things can happen. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) We have x1&amp;lt;θ0 and the constraint doesn't matter, so our unrestricted MLE is the same as our restricted MLE, so the likelihoods are the same, and thus we have a likelihood ratio of 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) We have x1&gt;θ0, so we can't choose x1 without violating θ≤θ0. We've already seen that L() is an increasing function wrt θ, so choosing θ as large as we can will maximize L, and in this case, this corresponds to θ=θ0. When we take the ratio of the two likelihoods, we end up with the second term.&lt;/p&gt;&#10;&#10;&lt;p&gt;The rejection region will be found by inverting the test statistic. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;&#10;&lt;p&gt;To give a little more concrete and simple example, consider:&lt;/p&gt;&#10;&#10;&lt;p&gt;I flip a coin 10 times and observe 8 heads. I would like to know if the coin is fair.&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be characterized as a binomial(10,p) distribution, and we are testing H0: p=0.5 vs p not equal 0.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the numerator, I'll plug in p=0.5 to the binomial distribution, since that's p0.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the denominator, I'll plug in p=0.8 to the same binomial distribution, since that's the MLE.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll reject the null hypothesis when this ratio is very small, because the null hypothesis is very unlikely compared to the alternative hypothesis.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-04-02T16:47:46.523" Id="25711" LastActivityDate="2012-04-02T16:58:03.017" LastEditDate="2012-04-02T16:58:03.017" LastEditorUserId="9444" OwnerUserId="9444" ParentId="25667" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The n vs. n-1 denominator in the sample variance is a common source of confusion for students. To see where the difference comes from, try calculating the method of moments estimator of the variance, and then the maximum likelihood estimator of the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;One explanation for the use of n-1 is that we start of with n data values, and therefore n degrees of freedom, but before we get the sample variance, we need to estimate the mean, (we plug x-bar into the sample variance formula) so we have used 1 degree of freedom, and thus, are left with n-1.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-02T20:23:36.100" Id="25729" LastActivityDate="2012-04-02T20:23:36.100" OwnerUserId="9444" ParentId="25221" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="25754" AnswerCount="1" Body="&lt;p&gt;So I'm currently training a stack of RBMs (Restricted Boltzmann Machines), eventually to be build into a DBN (Deep Belief Net), on a set of gray-scale images featuring objects placed in different locations on a complex background. &lt;/p&gt;&#10;&#10;&lt;p&gt;After training for a good long while, I tried generating from the RBMs and found that for a large percentage of the generated images were identical; they all looked like the background.&lt;/p&gt;&#10;&#10;&lt;p&gt;This makes sense: the background is static across the training set, so learning a high fidelity representation of that is a better idea (from the network's perspective) than learning a low fidelity representation of the objects, since they appear in different positions.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is this: &lt;strong&gt;I want to learn about the whole image, not just the background; what should I change in order get this to happen?&lt;/strong&gt; I could always lower the learning rate/momentum and keep training, but I've love some opinions as to whether or not that is the best solution. &lt;strong&gt;Is there anyway to utilize this learned representation of the image background to learn a better representation of the foreground?&lt;/strong&gt; I've thought of just subtracting it out, but that seems rather ad hoc.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. I've had this problem in the past with traditional autoencoders (i.e. three-layer backpropagation net predicting the input pattern), so I know it isn't unique to deep learning approaches or the particular dataset, just any dataset that contains a static signal amongst more varied information.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-02T21:40:14.777" Id="25735" LastActivityDate="2012-04-03T07:10:00.557" OwnerUserId="10320" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;" Title="What to do when a NN learns a static representation for a large chunk of your dataset?" ViewCount="81" />
  <row AcceptedAnswerId="25752" AnswerCount="1" Body="&lt;p&gt;In Statistica one can do a &quot;two series&quot;/bivariate/cross spectrum Fourier analysis to examine the coherency, gain, and phase spectrum across a pair of signals.  It is probably a failing on my part, but my attempts to use those search terms to come up with similar values from R have come up blank.  What functions/packages should I review in more detail?  Can you provide some simple examples?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-03T03:06:51.993" Id="25745" LastActivityDate="2012-04-03T06:55:10.360" OwnerUserId="196" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;fourier-transform&gt;" Title="How can one do &quot;two series&quot; Fourier analyses in R?" ViewCount="366" />
  <row Body="&lt;p&gt;The same familiar spectrum instruction yields cross-spectra. coherency and phase when used on a bivariate time series. Look at this sample code. &lt;code&gt;dIPC&lt;/code&gt; and &lt;code&gt;Crudo&lt;/code&gt; are respectively (differenced) consumer price index and oil price.&#10;We first build a bivariate time series (out of ordinary ts time series) with ts.union, then invoke spectrum.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;datos &amp;lt;- ts.union(dIPC,&#10;           Crudo)&#10;datos &amp;lt;- window(datos,&#10;           start=c(1979,1),&#10;           end=c(2002,1))&#10;sp &amp;lt;- spectrum(datos,&#10;           main=&quot;Petróleo e IPC&quot;,&#10;           spans=rep(3,5))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We only need now to plot coherency and phase. I find it useful to align them one below the other with &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(2,1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then do the plots:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(sp,plot.type=&quot;coh&quot;)&#10;plot(sp,plot.type=&quot;phase&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2012-04-03T06:55:10.360" Id="25752" LastActivityDate="2012-04-03T06:55:10.360" OwnerUserId="892" ParentId="25745" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I don't have a good intuition as to whether or not you'll be able to &quot;force&quot; them to use specific features by tweaking the learning rate. It might work, but it might also end up with a slow convergence on the same local minima. &lt;/p&gt;&#10;&#10;&lt;p&gt;Subtracting the background out isn't a terrible idea. Can you preprocess the input? If you take the 2D Fourier transform of the images and throw away the phase information, you might end up with a more position-invariant representation (depending on lighting, etc)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-03T07:10:00.557" Id="25754" LastActivityDate="2012-04-03T07:10:00.557" OwnerUserId="7250" ParentId="25735" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Every time I see a new language, I ask myself why an existing language can't be improved instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;Python's big advantages are&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a rich set of modules (not just statistics, but plotting libraries, output to pdf, etc.)&lt;/li&gt;&#10;&lt;li&gt;language constructs that you end up needing in the long run (objected-oriented constructs you need in a big project; decorators, closures, etc. that simplify development)&lt;/li&gt;&#10;&lt;li&gt;many tutorials and a large support community&lt;/li&gt;&#10;&lt;li&gt;access to mapreduce, if you have a lot of data to process and don't mind paying a few pennies to run it on a cluster.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In order to overtake R, Julia, etc., Python could use&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;development of just-in-time compilation for restricted Python to give you more speed on a single machine (but mapreduce is still better if you can stand the latency)&lt;/li&gt;&#10;&lt;li&gt;a richer statistical library&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="10" CommunityOwnedDate="2012-04-03T14:29:09.400" CreationDate="2012-04-03T14:29:09.400" Id="25771" LastActivityDate="2012-04-04T18:32:54.390" LastEditDate="2012-04-04T18:32:54.390" LastEditorUserId="858" OwnerUserId="858" ParentId="25672" PostTypeId="2" Score="12" />
  <row AnswerCount="1" Body="&lt;p&gt;I am really new with R and time series. But, I have understood most concept. Part where I am (very) confused is the &lt;code&gt;xreg=&lt;/code&gt; argument in &lt;code&gt;arima()&lt;/code&gt; from &lt;a href=&quot;http://cran.r-project.org/web/packages/tseries/index.html&quot; rel=&quot;nofollow&quot;&gt;tseries&lt;/a&gt; package and &lt;a href=&quot;http://cran.r-project.org/web/packages/forecast/index.html&quot; rel=&quot;nofollow&quot;&gt;forecast&lt;/a&gt; package. As I have read most of the related threads on this site, I understand that &lt;code&gt;xreg&lt;/code&gt; is used for exogenous data. See for example, &lt;a href=&quot;http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r&quot;&gt;How to fit an ARIMAX-model with R?&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, I conclude that I can fit SARIMAX or ARMAX method using &lt;code&gt;arima()&lt;/code&gt;. However, I am currently reading Shumwhay's book and his web &lt;a href=&quot;http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm&quot; rel=&quot;nofollow&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you check the website (close to the bottom of the page), you can found that he said &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;code&gt;xreg&lt;/code&gt; in &lt;code&gt;arima()&lt;/code&gt; does not fix ARMAX model.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It is also explained in the &lt;a href=&quot;http://www.stat.pitt.edu/stoffer/tsa2/Rissues.htm&quot; rel=&quot;nofollow&quot;&gt;R issues&lt;/a&gt; on his website. In addition, he proposed fitting ARMAX model in state space model. So, the answers found on this site and the explanation in the website are completely different.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain me why there is such discrepancy? Which one is correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;If Shumwhay is correct, is there any function/package that can fit ARMAX or SARIMAX model in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry if it turns out I only missed some points, but I hope you can point me to the right direction.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-03T15:32:32.290" FavoriteCount="2" Id="25780" LastActivityDate="2012-04-03T23:25:48.517" LastEditDate="2012-04-03T22:00:54.630" LastEditorUserId="930" OwnerUserId="10336" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;time-series&gt;&lt;multivariate-analysis&gt;&lt;arima&gt;" Title="What is the purpose of and how to use the xreg argument when fitting ARIMA model in R?" ViewCount="3172" />
  
  <row Body="&lt;p&gt;It depends on requirements of software and specific procedure you use for the analysis. In general, I personally like to register binary variables (1=present vs 0=absent) as ordinal level, not nominal level, such as sex (1=male, 2=female). But is matter of taste and custom...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-03T15:54:47.250" Id="25782" LastActivityDate="2012-04-03T15:54:47.250" OwnerUserId="3277" ParentId="25776" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="25790" AnswerCount="2" Body="&lt;p&gt;I am developing a neuroscience-inspired training algorithm for  feed-forward neural networks. The natural benchmark for comparison is backpropagation. So I need to know to know what sort of classification problems backpropagation (and thus feed-forward NNs) is most suited for. Also, since I ultimately want to move beyond backprop, what non NN algorithms are the best alternatives to backprop, when it comes to the sort of problems that backprop is good at. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or, in other words, what problems should I benchmark a feed-forward network on, and what are the algorithms to beat for those problems?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-03T17:50:54.817" FavoriteCount="1" Id="25786" LastActivityDate="2012-04-08T13:40:05.300" OwnerUserId="3443" PostTypeId="1" Score="3" Tags="&lt;classification&gt;&lt;neural-networks&gt;&lt;pattern-recognition&gt;" Title="What sort of problems is backpropagation best suited to solving, and what are the best alternatives to backprop for solving those problems?" ViewCount="684" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://etd.nd.edu/ETD-db/theses/available/etd-05182010-071040/&quot;&gt;See Tueller (2010)&lt;/a&gt;, &lt;a href=&quot;http://www.tandfonline.com/doi/pdf/10.1080/10705511.2010.510050&quot;&gt;Tueller and Lubke (2010)&lt;/a&gt;, and [Ruscio et al.'l book][3] for complete detail on what is summarized below. Taxometric &lt;strong&gt;procedures&lt;/strong&gt; generally work by computing simple statistics on subset of sorted data. MAMBAC uses the mean, MAXCOV uses the covariance, and MAXEIG using the eigen value. Latent class analysis is a special case of the general latent variable mixture model (LVMM). The LVMM specifies a &lt;strong&gt;model&lt;/strong&gt; for the data which may include latent classes, latent factors, or both. Parameters of the model are obtained using maximum likelihood or Bayesian estimates. Refer to the literature above for complete detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is more important that the mathematical underpinnings (which are beyond the scope of this forum) are the hypotheses that can be tested under each approach. Taxometric procedures test the hypothesis&lt;/p&gt;&#10;&#10;&lt;p&gt;H1: Two classes explain all (or most) of the observed correlation among a set of indicators&#10;H0: One (or more) continuous underlying dimension(s) explain all of the observed correlation among a set of indicators&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually the CCFI is used to ascertain which hypothesis to reject/retain. See [John Ruscio's book on the topic][4]. Taxometric procedures can test &lt;strong&gt;only&lt;/strong&gt; these two hypothesis and no others. &lt;/p&gt;&#10;&#10;&lt;p&gt;Used alone, latent class analysis cannot test the taxometric alternative hypothesis, H0 above. &#10;However, latent class analysis can test the following alternative hypotheses:&lt;/p&gt;&#10;&#10;&lt;p&gt;H1a: Two classes explain all of the observed correlation among a set of indicators&#10;H1b: Three classes explain all of the observed correlation among a set of indicators&#10;...&#10;H1k: k classes explain all of the observed correlation among a set of indicators&lt;/p&gt;&#10;&#10;&lt;p&gt;To test H0 from above in a latent variable framework, fit a single factor confirmatory factor analysis (CFA) model to the data (call this H0cfa which is different from H0 - H0 only tests a hypothesis of fit under the taxometric framework, but doesn't produce parameter estimates as you would get by fitting a CFA model). To compare H0cfa to H1a, H1b, ..., H1k, use the Bayesian Information Criterion (BIC) ala [Nylund et al. (2007)][5]. &lt;/p&gt;&#10;&#10;&lt;p&gt;To summarize thus far, taxometric procedures can look at two vs. one class solutions, while latent class + CFA can test one vs. two or more class solutions. We see that taxometric procedures test a &lt;strong&gt;subset&lt;/strong&gt; of the hypotheses tested by latent class + CFA model comparisons. &lt;/p&gt;&#10;&#10;&lt;p&gt;All of the hypotheses present thus far are extremes at two ends of a spectrum. The more general hypothesis is that some number of latent classes &lt;strong&gt;and&lt;/strong&gt; some number of latent dimensions (or latent factors) best explain the data. The approaches described above reject this outright, which is a very strong assumption. Put differently, a latent class model and a taxometric procedure that leads to a conclusion of taxonic structure (rather than dimensional) assume within class individual differences besides random error. In your context, this is equivalent to say that within the chronic pain class, there is no systematic variation in the tendency to develop chronic pain, only random chance. &lt;/p&gt;&#10;&#10;&lt;p&gt;The weakness of this assumption is better illustrated with an example from psychopathology. &#10;Say you have a set of indicators for depression, and your taxometric and/or latent class models lead you to conclude there is a depressed class and a non-depressed class. These models implicitly assume no variance in severity of depression within class (beyond random error or noise). In other words, you are depressed, or you are not, and among the depressed everyone is equally depressed (beyond variation in error prone observed variables). So we only need one treatment for depression at one dose level! It is easily seen that this assumption is absurd for depression, and is often just as limited for most other research contexts. &lt;/p&gt;&#10;&#10;&lt;p&gt;To avoid making this assumption, use a factor mixture modeling approach following the papers of [Lubke and Muthen and Lubke and Neale][6].&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-03T22:18:49.223" Id="25803" LastActivityDate="2012-04-03T22:18:49.223" OwnerUserId="10347" ParentId="25734" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="25813" AnswerCount="6" Body="&lt;p&gt;I am a graduate student in economics who recently converted to R from other very well-known statistical packages (I was using SPSS mainly). My little problem at the moment is that I am the only R user in my class. My classmates use Stata and Gauss and one of my professors even said that R is perfect for engineering, but not for economics. He said that many packages are built by people who know a lot about programming, but not much about economics and therefore are not reliable. He also mentioned the fact that since no money is actually involved in building an R package, there is therefore no incentive to do it correctly (unlike in Stata for example) and that he used R for a time and got some &quot;ridiculous&quot; results in his attempts to estimate some stuff. Moreover, he complained about he random number generator in R which he said was &quot;messy&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been using R for just a little more than a month and I must say I have fallen in love with it. All this stuff I am hearing from my professor is just discouraging me.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is: &quot;Is R reliable for the field of economics?&quot;.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-04-03T23:40:36.300" FavoriteCount="14" Id="25811" LastActivityDate="2012-10-10T16:18:10.343" LastEditDate="2012-04-04T10:36:48.993" LastEditorUserId="930" OwnerUserId="43908" PostTypeId="1" Score="49" Tags="&lt;r&gt;&lt;software&gt;&lt;econometrics&gt;" Title="Is the R language reliable for the field of economics?" ViewCount="5498" />
  <row Body="&lt;p&gt;I'd be very careful of anyone who claims a fact but never backs it up with anything substantial. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can easily turn his arguments around.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, people getting paid to write code could have LESS incentive to get it right because there is an expectation that their code will be correct, whereas the typical basement dweller wants to make a commit that will impress the project leaders. Maybe he couldn't care less about how much extra time he spends doing it for free if it means quality work gets done.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the random number generator is 'messy' (which is a vague term; easily replacing a real fact to back up his argument), then he should be able to prove it or show you someone who can.&lt;/p&gt;&#10;&#10;&lt;p&gt;If he gets incoherent results from a package, he should be able to point out the steps he took to get that result. If it's really a bug and you have good programming skills, you can even try and fix it for him!&lt;/p&gt;&#10;&#10;&lt;p&gt;I realize my answer doesn't answer your question directly (sorry). Simply from the way he words his points, you can see there is no meat behind it. If there is, feel free to edit it in your question for people here to discuss it further!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-04T03:20:09.253" Id="25816" LastActivityDate="2012-04-04T03:20:09.253" OwnerUserId="10350" ParentId="25811" PostTypeId="2" Score="11" />
  
  <row Body="&lt;p&gt;If you have not only the frequencies but the actual counts, you can use a $\chi^2$ &lt;a href=&quot;http://en.wikipedia.org/wiki/Goodness_of_fit#Categorical_data&quot;&gt;goodness-of-fit test&lt;/a&gt; for each data series. In particular, you wish to use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Discrete_uniform_distribution&quot;&gt;test for a discrete uniform distribution&lt;/a&gt;. This gives you a good &lt;em&gt;test&lt;/em&gt;, which allows you to find out which data series are likely not to have been generated by a uniform distribution, but does not provide a measure of uniformity.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are other possible approaches, such as computing the &lt;a href=&quot;http://en.wikipedia.org/wiki/Entropy_%28information_theory%29&quot;&gt;entropy&lt;/a&gt; of each series - the uniform distribution maximizes the entropy, so if the entropy is suspiciously low you would conclude that you probably don't have a uniform distribution. That works as a measure of uniformity in some sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another suggestion would be to use a measure like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Kullback-Leibler divergence&lt;/a&gt;, which measures the similarity of two distributions.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-04T09:18:59.573" Id="25828" LastActivityDate="2012-04-04T09:18:59.573" OwnerUserId="8507" ParentId="25827" PostTypeId="2" Score="12" />
  
&#10;$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;which under the null is distributed as a Snedecor's $F$ r.v. with $1$ and $n-k_{unrestricted}-1$ degrees of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;($n =$ # observations,&#10;$k =$ # predictors)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-04T09:24:39.283" Id="25829" LastActivityDate="2012-04-04T10:03:24.540" LastEditDate="2012-04-04T10:03:24.540" LastEditorUserId="9253" OwnerUserId="9253" ParentId="25825" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="35596" AnswerCount="1" Body="&lt;p&gt;The alpha-expansion algorithm described &lt;a href=&quot;http://books.google.co.in/books?id=7dzpHCHzNQ4C&amp;amp;pg=PA592&amp;amp;lpg=PA592&amp;amp;dq=alpha+expansion+algorithm&amp;amp;source=bl&amp;amp;ots=pu1CCm--AR&amp;amp;sig=PIN3b33Wm02_FWi3f1dJO_np0W8&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=Z2d8T4veCsrnrAetw6THAg&amp;amp;redir_esc=y#v=onepage&amp;amp;q=alpha%20expansion%20algorithm&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (Probabilistic Graphical Models: Principles and Techniques By Daphne Koller, Nir Friedman, Page 593: algorithm 13.5), is designed for graphs where each vertex has the same set of labels. I am interested in a more general problem where each vertex can have different set of labels having different cardinalities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can the alpha-expansion algorithm be used in such cases?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, let the graph have 3 vertices. The first vertex can assume labels (a1, a2, a3). The second vertex can assume labels (b1, b2) and the third one can have (c1, c2, c3, c4, c5). The node and edge potentials are defined accordingly. Can alpha-expansion still help me in finding an optimal (approximate) label combination?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-04T16:01:17.217" Id="25851" LastActivityDate="2012-09-03T10:06:02.257" OwnerUserId="10378" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;graphical-model&gt;" Title="Alpha Expansion algorithm over graph with vertices having different sets of labels" ViewCount="726" />
  <row AnswerCount="1" Body="&lt;p&gt;experts,&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to ask for advice regarding the analysis of a dataset I am currently working on. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the experiment subjects were tested in a reaction time task (RT as dependent variable). In random order every subject was tested in two treatment conditions (factor 'A': placebo vs. drug). Also for subject in a third session the level of a blood value was measured (a numeric covariate 'B'). This covariate 'B' is stable within a subject and believed to be a predictor for the effect of factor 'A'. Moreover subjects were grouped according to their genotype (factor 'C'). Also I coded the first or second testing in factor 'D' in order to model potential learning effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please find below some sample code. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# some example data&#10;subject &amp;lt;- c(1,1,2,2,3,3,4,4)&#10;A &amp;lt;- rep(c('placebo', 'drug'), 4) # factor A&#10;B &amp;lt;- c(1,1,4,4,8,8,12,12) # covariate B&#10;C &amp;lt;- c('x','x','y','y','x','x','y','y')&#10;D &amp;lt;- c('1st','2nd','1st','2nd','2nd','1st','2nd','1st')&#10;RT &amp;lt;- c(2,12,1,16,2,26,3,39)&#10;&#10;data &amp;lt;- as.data.frame(cbind(subject, A, B, C, D, RT))&#10;data$B &amp;lt;- as.numeric(as.character(data$B))&#10;data$RT &amp;lt;- as.numeric(as.character(data$RT))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would know how to estimate the overall effect of the covariate 'B' using lm() and how to estimate the overall effect of factor 'A' using aov(). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Model effect of covariate B using lm()&#10;data.lm &amp;lt;- lm(RT~B, data=data)&#10;summary(data.lm)&#10;&#10;# Model effect of factor A using aov()&#10;data.aov &amp;lt;- aov(RT~A+Error(subject/A),data=data)&#10;summary(data.aov)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;From the comments I understood that the correct ANCOVA model to test effects of A, B and their interaction would be as followed:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# ANCOVA for effects of A, B and their interaction&#10;data.aov2 &amp;lt;- aov(RT~ B * A + Error(subject/A), data = data)&#10;summary(data.aov2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However if I would like to look at B, C and their interaction the output from summary() does not provide significances anymore which makes me assume the model is not correct:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# ANOVA for effects B, C and their interaction&#10;data.aov3 &amp;lt;- aov(RT ~ B * C + Error(subject), data=data)&#10;summary(data.aov3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also of course I would like to know whether it is possible to estimate the most comprehensive model including all factors (A, B, C, D) and their interaction (?). Probably the dataset is too small to estimate this. But how can I know whether my data is sufficient to allow for the estimation of such a complicated model?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# ANOVA for effects B, C and their interaction&#10;data.aov4 &amp;lt;- aov(RT ~ A * B * C * D + Error(subject/A), data=data)&#10;summary(data.aov4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also I would like to know whether the same models can be implemented in the same way in case there are repeated-measures of the dependent variable 'RT'? E.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data2 &amp;lt;- rbind(data, data, data, data, data)&#10;data2$RT &amp;lt;- rnorm(nrow(data2))&#10;&#10;# ANOVA for effects B, C and their interaction&#10;data.aov5 &amp;lt;- aov(RT ~ A * B * C * D + Error(subject/A), data=data2)&#10;summary(data.aov5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any help regarding this (including hints for an implementation in R as well as literature) is highly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks!&#10;Jokel&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-04T20:59:31.290" FavoriteCount="1" Id="25875" LastActivityDate="2012-04-18T09:17:31.867" LastEditDate="2012-04-05T06:14:05.440" LastEditorUserId="10389" OwnerUserId="10389" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;&lt;ancova&gt;" Title="Repeated-measures ANCOVA in R?" ViewCount="2190" />
  
  <row Body="&lt;p&gt;One of the things I find myself doing a lot is tracking the commands used to generate data and objects, and have found the comment to be a useful tool for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;The 'matched.call.data' and 'generate.command.string' do the trick.  Not perfect, but helpful and a use for 'comment()'. :)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Comments only accept strings...&#10;# Substituting the escaped quotes ('\&quot;') makes it prettier.&#10;generate.command.string &amp;lt;- function( matched.call.data )&#10;{&#10;  command.string &amp;lt;- as.character( bquote( .( list( matched.call.data ) ) ) )&#10;  sapply( bquote( .(command.string) ),&#10;                  USE.NAMES=FALSE,&#10;                  function( x )&#10;                    gsub( &quot;\\\&quot;&quot;, &quot;\'&quot;, as.list( match.call() )$x )[[2]] )&#10;}&#10;&#10;# Some generating function...&#10;generate.matrix &amp;lt;- function( nrows, ncols, data=NA ) {&#10;  # Some generated object&#10;  mat &amp;lt;- matrix( data= data, nrow= nrows, ncol= ncols )&#10;&#10;  matched.call.data &amp;lt;- do.call( &quot;call&quot;,&#10;                                c( list( as.character( match.call()[[1]] ) ),&#10;                                lapply( as.list( match.call() )[-1], eval ) ) )&#10;  comment( mat ) &amp;lt;- c( Generated= date(),&#10;                       Command = generate.command.string( matched.call.data ) )&#10;&#10;  mat&#10;}&#10;&#10;# Generate an object with a missing argument.&#10;emptyMat &amp;lt;- generate.matrix( nrows=2, ncols=2 )&#10;comment( emptyMat )&#10;&#10;# Generate without formally stating arguments.&#10;dataMat &amp;lt;- generate.matrix( 2, 2, sample(1:4, 4, replace= TRUE ) )&#10;comment( dataMat )&#10;&#10;# And with a longer command.&#10;charMat &amp;lt;- generate.matrix( 3, 3,&#10;                  c( 'This', 'is', 'a', 'much', 'longer',&#10;                     'argument', 'section', 'that', 'wraps') )&#10;comment( charMat )&#10;&#10;# And with a variable.&#10;myData &amp;lt;- c( 'An', 'expanded', 'command', 'argument')&#10;charMat2 &amp;lt;- generate.matrix( 2, 2, myData )&#10;comment( charMat2 )&#10;&#10;# Create a new object from an original command.&#10;Sys.sleep(1)&#10;emptyMat2 &amp;lt;- eval( parse( text= comment( emptyMat )[['Command']] ) )&#10;dataMat2 &amp;lt;- eval( parse( text= comment( emptyMat )[['Command']] ) )&#10;&#10;# Check equality of the static matrices.&#10;identical( emptyMat, emptyMat2 )&#10;&#10;# The generation dates are different.&#10;all.equal( emptyMat, emptyMat2, check.attributes= FALSE )&#10;comment( emptyMat )['Generated'] &amp;lt;- NA&#10;comment( emptyMat2 )['Generated'] &amp;lt;- NA&#10;identical( emptyMat, emptyMat2 )&#10;&#10;# Command argument structure still works too.&#10;str( as.list( match.call(&#10;  generate.matrix, parse( text=comment( charMat2 )[[ 'Command' ]] ) ) )[-1] )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-04-04T22:36:58.707" Id="25881" LastActivityDate="2012-04-04T22:36:58.707" OwnerUserId="10388" ParentId="4335" PostTypeId="2" Score="4" />
&#10;$$&#10;where $a = \lim_{n\to\infty} n/N$ is assumed to exist in the paper and $N$ is a function of $n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Apparently this result holds for &lt;strike&gt;any distribution&lt;/strike&gt; distributions with a sufficient number of finite moments (&lt;strong&gt;Edit:&lt;/strong&gt; See @cardinal's comment below). Jiang points out that this is a Type I extreme value distribution. The location and scale are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;\lim_{n\to\infty} \mathbb E\left[ nL_n^2 - 4\log n + \log(\log(n)) \right] = -2\log\left(a^2\sqrt{8\pi} \right) + 2\gamma \,.
  <row Body="&lt;p&gt;Any variable (univariate distribution) &lt;em&gt;v&lt;/em&gt; with observed &lt;em&gt;min&lt;/em&gt; and &lt;em&gt;max&lt;/em&gt; values (or these could be preset potential bounds for values) can be rescaled to range &lt;em&gt;min'&lt;/em&gt; to &lt;em&gt;max'&lt;/em&gt; by formula&#10;&lt;em&gt;(max'-min')/(max-min)(v-max)+max'&lt;/em&gt;&#10;or&#10;&lt;em&gt;(max'-min')/(max-min)(v-min)+min'&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-05T05:25:21.293" Id="25897" LastActivityDate="2012-04-05T05:25:21.293" OwnerUserId="3277" ParentId="25894" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;There are several algorithms to do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;The simplest is k-means, one implementation of which is:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;you pick the number of clusters you want, K&lt;/li&gt;&#10;&lt;li&gt;you choose the centre of the clusters randomly to start with.  There are K clusters, so K cluster centers&lt;/li&gt;&#10;&lt;li&gt;now you iterate:&#10;&lt;ul&gt;&#10;&lt;li&gt;first you look at each point, and assign it to the nearest cluster centre&lt;/li&gt;&#10;&lt;li&gt;then, once you've assigned all the points to the clusters, you take the average of all the points in each cluster, and that becomes the new cluster centre, of that cluster&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A good resource for learning clustering is 'Pattern Recognition and Machine Learning', by Christopher M. Bishop, chapter 9 'Mixture models and EM'.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-05T08:54:57.990" Id="25906" LastActivityDate="2012-04-05T08:54:57.990" OwnerUserId="10278" ParentId="24121" PostTypeId="2" Score="0" />
&#10;2p(x)p(y), &amp;amp; \text{if}~ x \geq 0, y \geq 0, \text{or}~ x &amp;lt; 0, y &amp;lt; 0,\\
  
  <row Body="&lt;p&gt;First. It is odd to use $1-r$ distance with K-means clustering, which internally operates with euclidean distance. You could easily turn &lt;em&gt;r&lt;/em&gt; into true euclidean &lt;em&gt;d&lt;/em&gt; by the formula derived from &lt;em&gt;cosine theorem&lt;/em&gt;: $\sqrt{2(1-r)}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second. I wonder how you manage to input distance matrix into K-means clustering procedure. Does R allow it? (I don't use R, and the K-means programs I know require casewise data as input.) Note: it is possible to create raw casewise data out of euclidean distance matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;Third. There is a great number of internal &quot;clustering criterions&quot; (over 100 I believe) helpful to decide what cluster solution is &quot;better&quot;. They differ in assumptions. Some (like cophenetic correlation or Silhouette Statistic) are very general and can be used with any distance or similarity measure. Some (like Calinski-Harabasz or Davies-Bouldin) imply euclidean distance (or at least metric one) in order to have geometrically sensible meaning. I haven't heard of Dunn's index you mention.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. Reading Wikipedia page on Dunn index suggests that this index is of general type.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-05T13:45:41.150" Id="25921" LastActivityDate="2012-04-05T17:23:27.010" LastEditDate="2012-04-05T17:23:27.010" LastEditorUserId="3277" OwnerUserId="3277" ParentId="25908" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="26376" AnswerCount="1" Body="&lt;p&gt;I'm trying to write my own Python code to compute t-statistics and p-values for one and two tailed independent t tests. I can use the normal approximation, but for the moment I am trying to just use the t-distribution. I've been unsuccessful in matching the results of SciPy's stats library on my test data. I could use a fresh pair of eyes to see if I'm just making a dumb mistake somewhere.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, this isn't so much of a coding question as it is a &quot;why isn't this computation yielding the right t-stat?&quot; I give the code for completeness, but don't expect any software advice. Just help understanding why this isn't right.&lt;/p&gt;&#10;&#10;&lt;p&gt;My code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;import scipy.stats as st&#10;&#10;def compute_t_stat(pop1,pop2):&#10;&#10;    num1 = pop1.shape[0]; num2 = pop2.shape[0];&#10;&#10;    # The formula for t-stat when population variances differ.&#10;    t_stat = (np.mean(pop1) - np.mean(pop2))/np.sqrt( np.var(pop1)/num1 + np.var(pop2)/num2 )&#10;&#10;    # ADDED: The Welch-Satterthwaite degrees of freedom.&#10;    df = ((np.var(pop1)/num1 + np.var(pop2)/num2)**(2.0))/(   (np.var(pop1)/num1)**(2.0)/(num1-1) +  (np.var(pop2)/num2)**(2.0)/(num2-1) ) &#10;&#10;    # Am I computing this wrong?&#10;    # It should just come from the CDF like this, right?&#10;    # The extra parameter is the degrees of freedom.&#10;&#10;    one_tailed_p_value = 1.0 - st.t.cdf(t_stat,df)&#10;    two_tailed_p_value = 1.0 - ( st.t.cdf(np.abs(t_stat),df) - st.t.cdf(-np.abs(t_stat),df) )    &#10;&#10;&#10;    # Computing with SciPy's built-ins&#10;    # My results don't match theirs.&#10;    t_ind, p_ind = st.ttest_ind(pop1, pop2)&#10;&#10;    return t_stat, one_tailed_p_value, two_tailed_p_value, t_ind, p_ind&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After reading a bit more on the Welch's t-test, I saw that I should be using the Welch-Satterthwaite formula to calculate degrees of freedom. I updated the code above to reflect this.&lt;/p&gt;&#10;&#10;&lt;p&gt;With the new degrees of freedom, I get a closer result. My two-sided p-value is off by about 0.008 from the SciPy version's... but this is still much too big an error so I must still be doing something incorrect (or SciPy distribution functions are very bad, but it's hard to believe they are only accurate to 2 decimal places).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second update:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;While continuing to try things, I thought maybe SciPy's version automatically computes the Normal approximation to the t-distribution when the degrees of freedom are high enough (roughly &gt; 30). So I re-ran my code using the Normal distribution instead, and the computed results are actually further away from SciPy's than when I use the t-distribution.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-04-05T21:48:13.807" Id="25940" LastActivityDate="2013-08-23T02:49:02.567" LastEditDate="2013-08-23T02:49:02.567" LastEditorUserId="805" OwnerUserId="8927" PostTypeId="1" Score="6" Tags="&lt;statistical-significance&gt;&lt;t-test&gt;&lt;python&gt;" Title="Tracking down the assumptions made by SciPy's ttest_ind() function" ViewCount="743" />
  
  <row Body="&lt;p&gt;maybe mixed modelling with patient as random effect could be a way. With mixed modelling the correlation structure in the paired case and the partial missings in the unpaired case could be accounted for.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-05T22:43:35.403" Id="25943" LastActivityDate="2012-04-05T22:43:35.403" OwnerUserId="1573" ParentId="25941" PostTypeId="2" Score="4" />
&#10;$$&#10;$$
&#10;  \sigma_{ij} = \log\left(\frac{c_{ij}}{m_i m_j} + 1 \right) \, , \quad i,j=1,\dots,k \, ,
  
  <row Body="&lt;p&gt;User10405, I didn't mean to offend you by the homework comment, but proving that the sum of independent Bernoullis is a Binomial is exactly a homework problem I assigned the statistics class (Mathematical Statistics 1) I teach recently. Apologies for the &quot;useless prejudice.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm confused, Macro's comment seemed appropriate, yet:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Macro: This is not a bunch of Bernoulli trials and a binomial distribution does not apply. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;But in the original statement:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $X_1,X_2,...,X_n$ be a fixed number of Bernoulli random variables&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://en.wikipedia.org/wiki/Bernoulli_random_variable&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Bernoulli_random_variable&lt;/a&gt; for verification that the sum of Bernoullis is indeed a Binomial, and since $avg$  is just the sum divided by $n$, it will be a scaled Binomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as $f=max$  goes, since each of the X 's is a Bernoulli, i.e. 0 or 1, the max also has to be 0 or 1. It will be 0 only if all of the X 's are 0. So treat it as a Bernoulli($p∗$) where $p∗=1−P(X_1=X_2=...=X_n=0)$ . &lt;/p&gt;&#10;&#10;&lt;p&gt;An assumption of independence among the $X$'s makes $p∗$ much easier to calculate.&lt;/p&gt;&#10;&#10;&lt;p&gt;I get the feeling that this answer still won't be what you're looking for, but without more clarification, I don't see how we can help further.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-06T01:31:54.847" Id="25948" LastActivityDate="2012-04-06T01:38:24.540" LastEditDate="2012-04-06T01:38:24.540" LastEditorUserId="9444" OwnerUserId="9444" ParentId="25914" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;Type : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(y ~ x1 + x2 + I(x1*x2) + I(x1^2) + ...)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-04-06T13:17:04.047" Id="25976" LastActivityDate="2012-04-06T13:17:04.047" OwnerUserId="8402" ParentId="25975" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;My first post in this useful community. Ive been trying to use GLS methods in non-linear mixed effects models and have found this post, &lt;a href=&quot;http://stats.stackexchange.com/questions/14426/prediction-with-gls&quot;&gt;Prediction with GLS&lt;/a&gt; extremely informative and helpful. &#10;Some of my questions are probably more of a follow up to the above post which discusses the predictive capability of GLS. Below I will state my question and give a brief description of why I would like to know more.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How does one evaluate a GLS fit for a &lt;strong&gt;real dataset&lt;/strong&gt; as compared to a regular fit?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Using simulations I could show that GLS is robust to error model mis-specification, especially when the data is sparse or there is deviation from normality (heavy-tailed distribution or time-varying distribution of residuals). For a simulated model, there is decreased bias and improved precision of the model parameters in a gls fit  compared to the regular nlme fit. However, I don't understand how would I evaluate my gls fit for a real dataset as compared to a regular fit. Standard errors/Goodness of Fit/etc..?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;At which stage of model building does one test a GLS fit?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Carrol and Ruppert state that when there is enough evidence of heteroschedasticity of your variance, then GLS estimators are very efficient. I obviously could see that in simulations studies, but how do I justify a better fit in a &lt;strong&gt;real dataset&lt;/strong&gt; when the parameter estimates of the model from a gls fit are not very different from those obtained after a regular fit (even though the gls estimator is more efficient). This obviously comes back to the question, why (or actually why not) use GLS estimation at all.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The estimation of residual variance parameters is independent of response prediction. T/F?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If the variance changes with your predictions (e.g. a constant CV model), is my understanding of the above statement correct that the estimation of the residual variance parameters is independent of model prediction of responses (or vice-versa). I believe this is the basic principle behind GLS.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Can anyone suggest good references whether gls is design dependent ( both structure and density of observations)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Any comments on gls performance/robustness for variable selection. Given bullet 3 above, can we expect gls to be  more sensitive in picking up predictors for a response variable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks in advance,&lt;/p&gt;&#10;&#10;&lt;p&gt;VJ&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-06T15:41:57.047" Id="25986" LastActivityDate="2012-04-06T15:41:57.047" OwnerUserId="7131" PostTypeId="1" Score="2" Tags="&lt;gls&gt;" Title="Evaluation of a GLS fit" ViewCount="389" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Suppose you take 3 samples and report a 99% confidence interval for each dataset. How would you go about calculating the probability that all the datasets contain the true population mean? No other statistics are provided.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-06T17:40:03.423" Id="25992" LastActivityDate="2012-05-04T02:16:45.367" LastEditDate="2012-04-06T17:45:08.267" LastEditorUserId="2970" OwnerUserId="10431" PostTypeId="1" Score="4" Tags="&lt;confidence-interval&gt;" Title="Probability that multiple confidence intervals contain the true population mean" ViewCount="1254" />
  <row AcceptedAnswerId="26004" AnswerCount="2" Body="&lt;p&gt;I am working on implementing a Fisher Exact Test for some unemployment and wage data. The idea is to describe two populations (one receiving an assistance program (the &quot;treatment&quot;) and one not receiving it) via a summary statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first statistic that I am using is the difference in average wage-level post-treatment between the two populations.&lt;/p&gt;&#10;&#10;&lt;p&gt;To calculate a p-value for such a set-up, the idea is to make a large number $N$ of random permutations of the assignment vector (the list of 0's and 1's that indicates whether a given observation belongs to the control population or not). I calculate the summary statistic for each of these random assignments, to get a distribution of summary statistics in counterfactual experiments where the control group had been different.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I also have the summary statistic for the observed assignment vector (the actual vector of 0's and 1's for the experiment that was truly observed), I can then count the number of simulated summary statistics which are more extreme than the observed summary statistic. &lt;/p&gt;&#10;&#10;&lt;p&gt;This proportion out of all of my simulated trials serves as the estimated p-value for my summary statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is as follows: is there a standard way to get a standard error for such an estimated p-value? Obviously, I can calculate the Monte Carlo standard error for the summary statistics in my simulations, but since all of the simulations are used for just one single p-value calculation, it''s not clear how to get the standard error for that.&lt;/p&gt;&#10;&#10;&lt;p&gt;I had the following thought about what the standard error might be in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;In usual Monte Carlo, we have some function $f$ that we compute at each of the simulated draws $x_{i}$ (where here, the $x_{i}$ are understood to be the assignment vectors). If I define:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ f(x_{i}) = \mathbb{1}_{ |stat(x_{i})| &amp;gt; |stat(x_{obs})| } $$&lt;/p&gt;&#10;&#10;&lt;p&gt;then it seems that the p-value I calculate is given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat{p} = \frac{1}{N}\sum_{i=1}^{N}f(x_{i}) = \frac{\textrm{# more extreme}}{\textrm{total samples}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And following the usual Monte Carlo formulas, would it then make sense to write the variance of the estimate as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Var(\hat{p}) = \frac{1}{N}\sum_{i=1}^{N}[ f(x_{i}) - \hat{p} ]^{2} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and then take the square root to obtain the standard error?&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason this confuses me is that for each $i$, $f(x_{i})$ will be binary, either the computed statistic was more extreme in that iteration or it wasn't. It seems like it would be error prone to sum up a bunch of binary things like that to estimate the variance in a p-value, but that could just be my unfamiliarity with this method.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone confirm that this is right? Also, if I exposed any other ignorance about what I am doing here, corrective comments are appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-06T22:42:25.310" FavoriteCount="1" Id="26001" LastActivityDate="2012-04-07T03:26:15.007" LastEditDate="2012-04-06T23:13:37.637" LastEditorUserId="919" OwnerUserId="8927" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;&lt;p-value&gt;&lt;standard-error&gt;&lt;monte-carlo&gt;" Title="Not sure if standard error of p-values makes sense in Fisher Exact Test" ViewCount="463" />
  
  <row AcceptedAnswerId="26043" AnswerCount="1" Body="&lt;p&gt;I've fit a Linear Mixed Effects model to some &quot;accuracy&quot; scores for a study with rats. The fixed effects are &lt;code&gt;TrialNumber&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt;, and the random effect is the individual rat (&lt;code&gt;Rat&lt;/code&gt;). &lt;code&gt;Age&lt;/code&gt; is included because the trials span a number of months (and &lt;code&gt;Age&lt;/code&gt; is measured in days; not all rats were the same age at the start of the study, for a number of reasons).&lt;/p&gt;&#10;&#10;&lt;p&gt;The resulting model in R was:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme)&#10;library(car) #for the logit() function, since scores are percentages&#10;lmer(logit(Score) ~ Age*TrialNumber + (1|Rat))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get negative values for &lt;code&gt;Trial&lt;/code&gt; and &lt;code&gt;Age&lt;/code&gt; (the study is designed such that we predicted scores to actually go down with more experience, which indeed turned out to be the case). However, the interaction term is also significant, but with a positive estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fixed effects:&#10;                 Estimate   Std. Error  t value&#10;(Intercept)      1.182e+00  1.689e-02   70.00&#10;Age             -2.788e-03  2.849e-04   -9.78&#10;TrialNumber     -1.872e-06  9.344e-08  -20.04&#10;Age:TrialNumber  2.123e-08  1.741e-09   12.20 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do I interpret this interaction? Does it mean that for repeated trials on the same day, &lt;code&gt;Score&lt;/code&gt; went up, or does it mean that older rats did better on the same TrialNumbers?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-04-06T23:30:59.597" FavoriteCount="1" Id="26005" LastActivityDate="2012-04-07T20:43:40.350" OwnerUserId="10266" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;mixed-model&gt;&lt;multiple-regression&gt;&lt;interaction&gt;&lt;interpretation&gt;" Title="How to interpret two-way interactions in Linear Mixed Effects modeling?" ViewCount="4893" />
  
  
  <row AcceptedAnswerId="26030" AnswerCount="1" Body="&lt;p&gt;Suppose we have $\newcommand{\E}{\mathrm{Exp}} X \sim \E(\lambda)$, $Y \sim \E(\mu)$, and $W = \min(X,Y)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that $W \sim \E(\lambda+\mu)$. I know how to derive it. But, I tried this alternate derivation that gave me a different distribution for $W$, and I still can't figure out what's wrong with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I started with &lt;/p&gt;&#10;&#10;&lt;p&gt;$\newcommand{\rd}{\,\mathrm d}\renewcommand{\Pr}{\mathbb P}f_W(t) = f_X(t) \Pr(X&amp;lt;Y) + f_Y(t) \Pr(Y&amp;lt;X)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I need $\Pr(X &amp;lt; Y)$. Seems straightforward: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{align}
&#10;&amp;amp;= \frac{\lambda\mu}{\lambda + \mu}\left(e^{-\lambda t} + e^{-\mu t}\right) \&amp;gt;. 
&#10;\E \Zj = \sum_{k=1}^j \frac{{w \choose k}}{{n \choose k}} = \frac{w}{n-w+1}\left(1 - \frac{{{w-1} \choose j}}{{n \choose j}} \right) \&amp;gt;.
&#10;$$&#10;Now, note that $k p_k = w S_k - n S_{k+1}$. All that is left to do is to sum over $k$, rearrange, and solve for $\mu$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-07T16:33:54.700" Id="26033" LastActivityDate="2012-04-07T19:40:35.687" LastEditDate="2012-04-07T19:40:35.687" LastEditorUserId="2970" OwnerUserId="2970" ParentId="26020" PostTypeId="2" Score="6" />
  
  <row AnswerCount="5" Body="&lt;p&gt;I am a software developer (mostly .NET and Python about 5 years experience). What can I do to help me get a job in the machine learning field or really anything that will get me started in that field? Is post-graduate degree a hard requirement?&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-04-10T11:54:34.737" CreationDate="2012-04-07T20:52:58.247" FavoriteCount="11" Id="26044" LastActivityDate="2012-12-18T01:39:58.650" LastEditDate="2012-04-10T11:54:45.267" LastEditorUserId="88" OwnerDisplayName="user10459" PostTypeId="1" Score="15" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;careers&gt;" Title="Programmer looking to break into machine learning field" ViewCount="4670" />
  
  
  
  <row AcceptedAnswerId="112131" AnswerCount="1" Body="&lt;p&gt;The following is question 8 of chapter 8 in Wasserman's &lt;em&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387402721&quot; rel=&quot;nofollow&quot;&gt;All of Statistics&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $T_n = \overline{X}_n^2$, $\mu = \mathbb{E}(X_1)$,&#10;  $\alpha_k = \int|x - \mu|^kdF(x)$, and $\hat{\alpha}_k = n^{-1}\sum_{i=1}^n|X_i - \overline{X}_n|^k$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Show that $$v_{\mathrm{boot}} = \frac{4\overline{X}_n^2\hat{\alpha}_2}{n} + \frac{4\overline{X}_n\hat{\alpha}_3}{n^2} + \frac{\hat{\alpha}_4}{n^3} \&amp;gt;.$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;He previously defines&#10;$v_{\mathrm{boot}} = \frac{1}{B}\sum_{b=1}^B(T_{n,b}^* - \frac{1}{B}\sum_{r=1}^BT_{n,r}^*)^2$, where $T_{n,i}^*$ is the desired statistic from the $i$th bootstrap replication of the sample $X_1,...,X_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that the question as stated does not make sense: how can there be a formula for the bootstrap variance if the quantity requires simulation? Perhaps he meant to ask for the variance of the sampling distribution, but I get $\frac{\sigma^2}{n}$ for that. Any hints on how to intepret or solve this?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-04-08T18:02:49.290" FavoriteCount="1" Id="26082" LastActivityDate="2014-08-16T17:44:19.010" LastEditDate="2012-04-08T19:26:29.783" LastEditorUserId="2970" OwnerUserId="10464" PostTypeId="1" Score="3" Tags="&lt;variance&gt;&lt;nonparametric&gt;&lt;mathematical-statistics&gt;&lt;bootstrap&gt;" Title="Bootstrap variance of squared sample mean" ViewCount="381" />
  <row Body="&lt;p&gt;Interpreting a $100\alpha \%$ Confidence Interval $I$ as the probability of finding the &quot;real&quot; parameter inside the interval. &lt;/p&gt;&#10;&#10;&lt;p&gt;The most common case is when someone calculates this C.I. ($I$) and interprets the number $\alpha$ as the probability of finding the &quot;true mean&quot; say, $\mu$, inside the interval, i.e., interpreting the C.I. as $P(\mu \in I)=\alpha$.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2012-04-08T21:58:52.200" CreationDate="2012-04-08T21:58:52.200" Id="26090" LastActivityDate="2012-04-09T04:59:12.780" LastEditDate="2012-04-09T04:59:12.780" LastEditorUserId="9174" OwnerUserId="9174" ParentId="4551" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;So HS is a grouping variable.  I would recode it as 0/1, with 0 for whichever side you consider the default (or just flip a coin), for better interpretability of your model, but it doesn't actually influence anything.  Since you have an interaction, it is best not to interpret the main effects, for example, the coefficient for HS does &lt;em&gt;not&lt;/em&gt; give the separation between the two lines (i.e., the relationship between RC, whatever that is, and the response variable for right vs. left), except exactly where RC=0.  I do not know whether that fact is meaningful, but it typically isn't.  More important is to look at the simple effects, that is, the two slopes.  If you coded 0/1, then the slope for the first side is $\beta_{RC}$, and the slope for the other side is $\beta_{RC}+\beta_{RC:HS}$.  As you can see, the difference between the slopes is simply $\beta_{RC:HS}$, and the 95% confidence interval for $\beta_{RC:HS}$ is the 95% CI for the difference.  With other codings, this could be more complicated.  As for a measure of effect size, if the numbers for RC are intrinsically meaningful, I would just go with them.  You could also use partial $\eta^2$, which is $SS_{RC:HS}/(SS_{RC:HS}+SS_{err})$.  If you did want to talk about the intercepts for some reason, then for the first side, it would be $\beta_{int}$, and for the other: $\beta_{int}+\beta_{HS}$, but even here, I would just write out the entire regression equation for each side--I really don't see what only the intercepts in isolation is going to do for you.  Lastly, remember that all of this is predicated on having recoded your HS as 0/1 for simplicity and refit the model.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-09T04:59:05.510" Id="26106" LastActivityDate="2012-04-09T04:59:05.510" OwnerUserId="7290" ParentId="26065" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Yes, there are many methods for dealing with correlated data errors. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is usually done by modelling the likelihood function, $p(D|\vec{\theta})$, i.e., modelling how the data was sampled as a function of the parameters $\vec{\theta}$ which include not only the parameters of your model ($F(x_i,\theta)$), but the parameters that generate the correlations. For example, a model for the likelihood could be the multivariate gaussian distribution, i.e.,&#10;\begin{equation*}&#10;p(D|\vec{\theta})=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\text{exp}\left(-\frac{1}{2}[\vec{y}-\vec{F}(\vec{x},\theta)]^T\Sigma^{-1}[\vec{y}-\vec{F}(\vec{x},\theta)]\right)\text{,}&#10;\end{equation*}&#10;which can be thought of as the most conservative distribution to use (in a &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_maximum_entropy&quot; rel=&quot;nofollow&quot;&gt;maximum entropy sense&lt;/a&gt;) where $\theta\in\vec{\theta}$ are the parameters of your deterministic model $F$, $\Sigma\in\vec{\theta}$ is the covariance matrix (which will model the correlation between your data points), $\vec{y}$ is the vector containing the observations (i.e. the $i$th element is $y_i$) and $\vec{F}(\vec{x},\theta)$ the corresponding values of the parametric model you are trying to fit (i.e. the $i$th value is $F(x_i,\theta)$). The special case you mention (for uncorrelated errors) can be derived by turning $\Sigma$ into a diagonal matrix (with elements $\Sigma_{i,j}=\delta_{i,j}\sigma_i^2$) and maximizing $p(D|\vec{\theta})$ (which in turn will minimize the equation that you cite). &lt;/p&gt;&#10;&#10;&lt;p&gt;How to model the covariance matrix $\Sigma$ in this case is the real problem, and will vary between data sets or applications. For example, in time series analysis one can model correlation between residuals as order $p$ autoregressive procceses (which are, in a sense, &quot;maximum entropy&quot; procceses), which for order $p=1$ model the elements of the covariance matrix with only two parameters $\alpha$ and $\sigma$,&#10;\begin{equation*}&#10;\Sigma_{i,j}=\frac{\sigma^2}{1-\alpha^2}\alpha^{|j-i|}\text{.}&#10;\end{equation*} &#10;There are other models (both for the likelihood function and for the modelling of the covariance matrix), however, and you'll have to search for the one to fit your needs. In general, an analysis to the autocorrelation function of your residuals with the model you cited will give you advice on what kind of model you need. If you need help in those subjects, please consult any time series analysis textbook such as the one by &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387953515&quot; rel=&quot;nofollow&quot;&gt;Brockwell &amp;amp; Davis&lt;/a&gt;. Another good reference when modelling flicker noise (which arises frequently in physics) is the wavelet method of &lt;a href=&quot;http://iopscience.iop.org/0004-637X/704/1/51/pdf/0004-637X_704_1_51.pdf&quot; rel=&quot;nofollow&quot;&gt;Carter &amp;amp; Winn (2009)&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-09T05:34:50.257" Id="26108" LastActivityDate="2012-04-09T05:34:50.257" OwnerUserId="9174" ParentId="26094" PostTypeId="2" Score="1" />
  
&#10;$$&#10;where the notation $\mathbf{Y}_{u:v} := [Y_u,\,Y_{u+1}, \, \dots,\,Y_v]$ is for&#10;the vector of the observations from time $u$ to time $v$. Above,&#10;the gap is assumed to be the interval ranging from time $r+1$ to $s-1$, and $n$ &#10;is the  whole series' length. The time&#10;$t$ is in the gap and the expectation could be written $\widehat{Y}_{t|1:r, s:n}$&#10;to recall its conditional nature. &lt;/p&gt;&#10;&#10;&lt;p&gt;The smoothed value does not have the simple form you guess. For a &#10;gaussian stationnary time series with known covariance structure, the estimated&#10;$ \widehat{Y}_{t}$ for $t$ in the gap can be found by solving a linear system.  &lt;/p&gt;&#10;&#10;&lt;p&gt;When the time series model can be put in State Space (SS) form, the&#10;FI smoothing is a standard operation based on Kalman filtering&#10;and it can be done e.g. using available R functions.  You simply need&#10;to specify that the values in the gap are missing. The smoothing&#10;algorithm estimates the hidden state $\boldsymbol{\alpha}_t$ which&#10;contains all the relevant information about $Y_t$ for $t$ in the gap.&#10;ARIMA models can be put in SS form.&lt;/p&gt;&#10;&#10;&lt;p&gt;Interestingly, the FI smoothing can be written as a combination&#10;of two filters: one forward and one backward, leading to a formula &#10;of the kind you expected, but for the hidden state estimation $\boldsymbol{\alpha}_t$&#10;(forecast and backcast), but not for the observation $Y_t$. This is &#10;known as &lt;em&gt;Rauch-Tung-Striebel filtering&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;At least in the multiplicative versions, 'ad hoc' forecasting&#10;procedures like Holt-Winters rely on stochastic models with no&#10;simple FI &#10; algorithms  since they can not be put in&#10;SS form. The smoothing formula can probably be approximated by using&#10;SS model, but it is much simpler then to use &lt;em&gt;Structural Time&#10;  Series models&lt;/em&gt;&#10; with log transformations.  The 'KalmanSmooth', 'tsSmooth'&#10;and 'StructTS' functions of the R &lt;strong&gt;stats&lt;/strong&gt; package can do the&#10;job. You should have a look at the books by Harvey or by Durbin and&#10;Koopman cited in the R help pages.  The smoothing algorithm can provide a&#10;conditional variance for the estimated $Y_t$ and can be used to build&#10;smoothing intervals, which usually tend to be larger in the middle of&#10;the gap. Note however that the estimation of Stuctural Models can&#10;be difficult.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;AP &amp;lt;- log10(AirPassengers) &#10;## Fit a Basic Structural Model&#10;fit &amp;lt;- StructTS(AP, type = &quot;BSM&quot;)&#10;&#10;## Fit with a gap&#10;AP.gap &amp;lt;- AP&#10;AP.gap[73:96] &amp;lt;- NA&#10;fit.gap &amp;lt;- StructTS(AP.gap, type = &quot;BSM&quot;, optim.control = list(trace = TRUE))&#10;&#10;# plot in orginal (non-logged) scale&#10;plot(AirPassengers, col = &quot;black&quot;, ylab = &quot;AirPass&quot;)&#10;AP.missing &amp;lt;- ts(AirPassengers[73:96], start=1955, , freq=12)&#10;lines(AP.missing, col = &quot;grey&quot;, lwd = 1)&#10;&#10;## smooth and sum 'level' and 'sea' to retrieve series&#10;sm &amp;lt;- tsSmooth(fit.gap)&#10;fill &amp;lt;- apply(as.matrix(sm[ , c(1,3)]), 1, sum)&#10;AP.fill &amp;lt;- ts(fill[73:96], start=1955, , freq=12)&#10;lines(10^AP.fill, col = &quot;red&quot;, lwd = 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/EhscF.png&quot; alt=&quot;Smoothed fill&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-09T17:32:47.707" Id="26127" LastActivityDate="2012-04-10T05:44:58.977" LastEditDate="2012-04-10T05:44:58.977" LastEditorUserId="9007" OwnerUserId="10479" ParentId="13984" PostTypeId="2" Score="2" />
  
&#10;$$&#10;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible that $x_t$ is autocorrelated, while $\epsilon_t$ is i.i.d? If so, what does that mean for all that methods that adjust standard errors for autocorrelation? Do you still have to do that or do they only apply to autocorrelated errors? Or would you always model the autocorrelation in such a setting in the error term, so it basically doesn't make a difference if $x_t$ is autocorrelated or $e_t$?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is my first question here. I hope it's not too confusing and I hope I didn't miss anything obvious...I also tried to google it and found some interesting links (for instance, here on &lt;a href=&quot;http://stats.stackexchange.com/questions/23704/time-series-regression-serially-correlated-errors-vs-autocorrelation-of-residua&quot;&gt;SA&lt;/a&gt;), but nothing really helped me.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-09T19:03:33.943" FavoriteCount="1" Id="26133" LastActivityDate="2012-04-09T23:06:22.780" OwnerUserId="7964" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;autocorrelation&gt;" Title="Is there a difference between an autocorrelated time-series and serially autocorrelated errors?" ViewCount="3523" />
  <row AnswerCount="1" Body="&lt;p&gt;In most of the examples I've seen so far of neural networks, the network is used for classification and the nodes are transformed with a sigmoid function .  However, I would like to use a neural network to output a continuous real value (realistically the output would usually be in the range of -5 to +5).  &lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1. Should I still scale the input features using feature scaling? What range?&#10;2. What transformation function should I use in place of the sigmoid?&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm looking to initially implement it PyBrain which describes these &lt;a href=&quot;http://pybrain.org/docs/api/structure/modules.html&quot; rel=&quot;nofollow&quot;&gt;layer types&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'm thinking that I should have 3 layers to start (an input, hidden, and output layer) that are all linear layers?  Is that a reasonable way?  Or alternatively could I &quot;stretch&quot; the sigmoid function over the range -5 to 5?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-04-09T23:34:37.530" FavoriteCount="3" Id="26144" LastActivityDate="2012-04-15T10:57:00.397" OwnerUserId="5618" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;neural-networks&gt;" Title="How to get real-valued continous output from Neural Network?" ViewCount="3425" />
  <row Body="&lt;p&gt;Have a look at &lt;a href=&quot;http://vision.ucla.edu/~raptis/tracklets.html&quot; rel=&quot;nofollow&quot;&gt;this work on the Tracklet Descriptor&lt;/a&gt;. In this case, they need to compute distances between time series of different lengths, and they use a nice dynamic time warping approach to inflate/deflate the two time series in question to find a minimum distance between them based on an idea of optimally distorting them. Outside of a reasonable set of distortions, they define the distance to be infinite.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could probably adapt this approach to define a distance between descriptors of different lengths. You need to be very careful and do some numerical testing though, because if you have undersampled a feature in a given part of the image because the set of features is sparser there, it can lead to very inaccurate numerical comparisons.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another alternative is to use something like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Isomap&quot; rel=&quot;nofollow&quot;&gt;Isomap algorithm&lt;/a&gt; which is a non-linear, manifold-based analogue of PCA. The idea here is project a set of points down onto a lower dimensional subset that captures the relevant topology best. It's normally used for dimensionality reduction, hence data sets are normally consisting of samples all of the same length, but it would be interesting to consider a modification that uses something like the dynamic time warping distance mentioned above to map mixed-size descriptors onto an optimal fixed lower dimensional space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, it's not clear what your edge-based directionality features are. Are you using a form of Histogram of Oriented Gradient? If so, you really should be ensuring that the histograms are always of the same length. If not, you might consider using Histogram of Oriented Gradient instead of your current descriptor, because mixed-dimensionality descriptors often wind up having unforeseen modeling consequences and tend not to work well in practice, in addition to requiring more difficult distance calculations. If interested, I have a project page &lt;a href=&quot;http://people.seas.harvard.edu/~ely/faceparts/index.html&quot; rel=&quot;nofollow&quot;&gt;linked here&lt;/a&gt; that includes a lot of Python code for using Histogram of Oriented Gradient (including GPU code in PyCuda). The useful library &lt;a href=&quot;http://scikits-image.org/&quot; rel=&quot;nofollow&quot;&gt;scikits.image&lt;/a&gt; also has a built-in HoG function.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-10T00:59:19.800" Id="26150" LastActivityDate="2012-04-10T00:59:19.800" OwnerUserId="8927" ParentId="26100" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;&lt;em&gt;@Richiemorrisroe has provided a concise answer, but here's a little elaboration&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In CFA you choose to permit factors to correlate (e.g., in path diagrams by adding double headed arrows between factors). Part of interpreting the model involves examining the relative size of correlation between factors. &lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds like you were expecting all the factors to correlate. Just as it is possible  in a set of &lt;em&gt;observed&lt;/em&gt; variables for some to intercorrelate and others not to, it is possible in a set of &lt;em&gt;latent&lt;/em&gt; variables for some to intercorrelate and others not to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming the CFA model is appropriate for the data, your aim should be to understand, among other things, what the pattern of factor intercorrelations mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;A few other things you could explore:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Compare fits when constraining particular covariances to zero&lt;/li&gt;&#10;&lt;li&gt;Compare fit to a model where all covariances are constrained to be equal&lt;/li&gt;&#10;&lt;li&gt;Explore a hierarchical factor model (the pattern of factor correlations may suggest that a higher level factor underlies some but not all the latent factors)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-04-10T06:30:13.113" Id="26160" LastActivityDate="2012-04-10T06:30:13.113" OwnerUserId="183" ParentId="26027" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The previous two answer's are I think coming at the problem &quot;backwards&quot; - though they are both correct.  They do not start with the postulate and end with the conclusion.  If we start from the postulate, then we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Pr(\text{No event in} [t,t+dt])=1-Pr(\text{1 event in} [t,t+dt])=1-\lambda dt$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we define the function $h(t)$ as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Pr(\text{No event in} [0,t])=h(t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Pr(\text{No event in} [0,t+dt])=h(t+dt)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, we can use the independence of the increments - another postulate of the poisson process and we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$h(t+dt)=h(t)[1-\lambda dt]\implies\frac{h(t+dt)-h(t)}{dt}=-\lambda h(t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking the limit as $dt\to 0$ we have $h&amp;#39;(t)=-\lambda h(t)$ which implies $h(t)=K\exp(-\lambda t)$.  We can resolve the proportionality constant by noting that $h(0)=1$ - i.e. it is certain to see no events in $[0,0]$.  This gives $K=1$.  This derivation can be found &lt;a href=&quot;http://bayes.wustl.edu/etj/articles/prob.as.logic.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (page 4) along with how to extend it to the probability for any number of events (basically by multiplying the zero count probability by $\lambda^n$ where $n$ is the number of events).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-10T07:10:29.973" Id="26163" LastActivityDate="2012-04-10T07:10:29.973" OwnerUserId="2392" ParentId="11956" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm currently working with a data set that consists of a monthly case count for several sites, along with a number of site-specific covariates. We're trying to estimate the effect of one of them on the case-load (either in terms of numbers or a percent). At the moment, I've been using Peng and Dominici's &lt;em&gt;Statistical Methods for Environmental Epidemiology with R&lt;/em&gt; as a reference for the approach I'm considering using.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a single site, the idea is to fit a Poisson model with roughly the following form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log ({\mu _t}) = \alpha  + \beta {x_t} + \eta {z_t} + f(t) + {\varepsilon _t}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where &lt;code&gt;mu&lt;/code&gt; is the monthly case count, &lt;code&gt;beta&lt;/code&gt; is the log-relative increase in cases due to a one unit increase in the variable of interest, &lt;code&gt;eta&lt;/code&gt; is the vector of other measured covariates and f(t) is some sort of smoothed spline over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, I'm comfortable enough with this. My problem is implementing a hierarchical model that gives an estimate of the effect pulling from information from each site. Is there a particularly good way to approach this in R? A well-documented package for example? Is it sufficient to essentially treat this as a meta-analysis with each site as a &quot;study&quot; and use something like &lt;code&gt;metafor&lt;/code&gt;? And is there a way to do it while modeling a site-specific covariate? The authors seem to be pushing for the use of a Bayesian hierarchical model, but the implementation of something like that is admittedly a bit of unexplored territory for me.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-10T07:18:13.357" Id="26164" LastActivityDate="2012-04-10T07:18:13.357" OwnerUserId="5836" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;time-series&gt;&lt;multilevel-analysis&gt;&lt;epidemiology&gt;" Title="How to implement a two-stage hierarchical model of time series data in R?" ViewCount="224" />
  <row AcceptedAnswerId="26210" AnswerCount="1" Body="&lt;p&gt;So, I am new here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to perform a sample size calculation for a clinical trial. The study sample will be select according to criteria of person's height. Persons within the particular height range (female, 1.6m to 1.7m) will be invited to participate in trial. We know the expected sample standard deviation from previous trial. But my concern is that sample is not from a normal distribution. Usual power/sample size calculation need the assumption of normal distribution of test statistic under $H_0$ and $H_1$, but here I believe we have truncated normal distribution. So how may I modify &lt;code&gt;power.t.test&lt;/code&gt;, or make some other calculation in R, to accommodate this? My colleague says to just rely on central limit theory and assume normal with 1.65 mean and known standard deviation, but I believe this is wrong due to the truncation. Any advice would be appreciated.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-04-10T09:24:31.703" Id="26170" LastActivityDate="2012-04-10T22:08:55.140" LastEditDate="2012-04-10T19:09:18.533" LastEditorUserId="2970" OwnerUserId="10497" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;sample-size&gt;&lt;epidemiology&gt;" Title="Sample size calculation for truncated normal distribution" ViewCount="615" />
  <row Body="&lt;p&gt;Generalized linear mixed effects model with a binomial response.  This would allow you directly to compare a model with &quot;App&quot; as a group in the randomness to your null hypothesis in q1.  Then q2 might also flow from the way the model has been fit.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-10T10:55:48.750" Id="26173" LastActivityDate="2012-04-10T10:55:48.750" OwnerUserId="7972" ParentId="26054" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="26231" AnswerCount="3" Body="&lt;p&gt;Is it common practice (and adequate) to regroup two binary dependant variables into a single 4-level dependent variable to take advantage of the multinomial regression? For instance, say we have information on two related conditions (outcomes) A and B. A new 4-category variable would be defined such that:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;category 1 = Neither conditions A nor B&#10;category 2 = Condition A (only)&#10;category 3 = Condition B (only)&#10;category 4 = Both conditions A and B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This allows running a single multinomial regression instead of using two binary logistic models that include the same predictors.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-10T14:48:36.523" Id="26197" LastActivityDate="2014-07-23T17:27:46.733" LastEditDate="2012-04-10T22:50:43.903" LastEditorUserId="4754" OwnerUserId="4754" PostTypeId="1" Score="5" Tags="&lt;logistic&gt;&lt;multinomial&gt;&lt;multivariate-regression&gt;" Title="Using multinomial logistic regression for multiple related outcomes" ViewCount="726" />
  <row Body="&lt;p&gt;I've always found the implicit updates framework (that includes the passive-aggressive algorithms mentioned in another answer here) to be unnecessarily more complex than the explicit updates framework (not to mention that implicit updates can be much slower than the explicit ones unless a closed-form solution for implicit update is available). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1011.1576v4.pdf&quot;&gt;Online Importance Weight Aware Updates&lt;/a&gt; is an example of a state-of-the-art explicit update algorithm which is simpler, faster, and more flexible (supporting multiple loss functions, multiple penalties, cost-sensitive learning etc.) than its implicit counterparts. The paper deals with linear models only though (linear svm corresponds to the case of hinge loss function with quadratic penalty)&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you need multi-class classification, one approach is to use the &quot;reductions&quot; functionality of &lt;a href=&quot;https://github.com/JohnLangford/vowpal_wabbit/wiki&quot;&gt;vowpal wabbit&lt;/a&gt; (built on the top of the approach from the paper) which is not documented well unfortunately.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-10T18:08:16.143" Id="26211" LastActivityDate="2012-04-10T18:34:51.680" LastEditDate="2012-04-10T18:34:51.680" LastEditorUserId="6129" OwnerUserId="6129" ParentId="26041" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;What were you plotting for predicted values and actual values?  The model predicts either log-odds, or some other value depending on what you ask predict to return.  It could be probability.  Actual values are just 0,1.  One way around this is to bin your actual values over subranges of the predictor and get the means (probability of 1) or make log-odds values.&lt;/p&gt;&#10;&#10;&lt;p&gt;You need to specify in your question what you're asking predict to return and what the &quot;actual&quot; values you're comparing it to are.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-10T18:20:06.990" Id="26212" LastActivityDate="2012-04-10T18:20:06.990" OwnerUserId="601" ParentId="26198" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I don't think so. The multinomial distribution is derived from n independent variables, but your situation has two dependent variables. A multinomial regression is not applicable here. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-10T21:18:33.927" Id="26226" LastActivityDate="2012-04-10T21:26:46.257" LastEditDate="2012-04-10T21:26:46.257" LastEditorUserId="10393" OwnerUserId="10393" ParentId="26197" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;A multinomial is perfectly fine in this situation, but it comes at two costs:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;An explosion in the number of parameters. (If you were to combine $n$ binary variables like this, you would have $2^n$ parameters instead of the original $n$.)&lt;/li&gt;&#10;&lt;li&gt;The solution is harder to interpret if the original variables are actually independent.  (If you would have had a simple relationship such that input variable $x$ implies dependent variable $y=1$, you would now have $x$ implies that the combined dependent variable takes on one of the many outcomes corresponding to $y=1$.)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The major advantage is that your model can use the additional parameters to encode distributions not possible in the original model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-10T21:53:30.730" Id="26232" LastActivityDate="2012-04-10T22:22:32.847" LastEditDate="2012-04-10T22:22:32.847" LastEditorUserId="858" OwnerUserId="858" ParentId="26197" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Take the expectation of $X(t)$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E \big( X(t) \big) =E (A) \cdot \cos (t) + E(B) \cdot \sin(t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and think what you have on the left hand side (constant), and on the right hand side (real function). Now when the expression on the left hand side is constant &lt;strong&gt;for all&lt;/strong&gt; $t$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also note that your last statement is incorrect. Weak stationarity requires more than just a constant mean. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-11T06:08:13.643" Id="26243" LastActivityDate="2012-06-10T06:40:48.573" LastEditDate="2012-06-10T06:40:48.573" LastEditorUserId="4856" OwnerUserId="2116" ParentId="26240" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;This question is similar to the one posed here:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/12164/testing-significance-of-peaks-in-spectral-density&quot;&gt;Testing significance of peaks in spectral density&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In that post, Pantera asks how to test whether a peak in a periodogram has a spike that is significantly different from the spikes generated by noise.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is this, when using the spec.pgram() function in R, a confidence interval is calculated and plotted as a blue line -- what is the correct interpretation of this interval, and how can we extract what its numerical values are?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've seen it suggested that to find an answer to this question, we should inspect the code for the plot.spec() function and then observe the spec.ci() function code, and figure things out from there.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;spec.ci &amp;lt;- function(spec.obj, coverage = 0.95) {&#10;      if (coverage &amp;lt; 0 || coverage &amp;gt;= 1) &#10;          stop(&quot;coverage probability out of range [0,1)&quot;)&#10;      tail &amp;lt;- (1 - coverage)&#10;      df &amp;lt;- spec.obj$df&#10;      upper.quantile &amp;lt;- 1 - tail * pchisq(df, df, lower.tail = FALSE)&#10;      lower.quantile &amp;lt;- tail * pchisq(df, df)&#10;      1/(qchisq(c(upper.quantile, lower.quantile), df)/df)&#10;  }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mN8x5.png&quot; alt=&quot;Plot generated by specpgram&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect that the height of the CI is some indication of the amplitude for which 95% of the data are less than.  But what is the &quot;baseline&quot; for that? Would it be possible to calculate and add horizontal lines to the plot indicating which peaks are significant?  Any help would be greatly appreciated.  For context, I am a biologist and these data are from imaging of rhythmic behavior in insects.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-11T06:14:56.730" Id="26244" LastActivityDate="2012-04-11T06:14:56.730" OwnerUserId="7086" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;confidence-interval&gt;" Title="What is the confidence interval calculated in a spectral density periodogram in R?" ViewCount="688" />
  
  <row Body="&lt;p&gt;It looks like this may fall into the category of &lt;a href=&quot;http://www.john-uebersax.com/stat/agree.htm&quot; rel=&quot;nofollow&quot;&gt;statistical methods for rater agreement&lt;/a&gt;, for dichotomous (categorical) ratings and multiple raters.  Apparently there are many methods for assessing rater agreement.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-11T09:50:50.690" Id="26254" LastActivityDate="2012-04-11T09:50:50.690" OwnerUserId="2921" ParentId="26054" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="26268" AnswerCount="1" Body="&lt;p&gt;CLT states in short, that sum/mean of random iid variables from almost any distribution approaches normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I failed to find information about asymptotic behavior of sample variance when sample is drawn from unknown distribution. Do we have any reason to believe, that variance of random iid variables asymptotically approach any particular distribution (like chi-squared for normal case)?&lt;/p&gt;&#10;&#10;&lt;p&gt;What about covariance of multivariate iid distribution? Can we have any reason to believe, that covariance calculated on sample drawn from it can asymptotically approach Wishart distribution? (or any other?)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-11T11:26:52.693" Id="26259" LastActivityDate="2012-04-11T13:21:26.383" LastEditDate="2012-04-11T12:01:38.903" LastEditorUserId="4856" OwnerUserId="10069" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;sampling&gt;&lt;variance&gt;&lt;covariance&gt;" Title="Something like Central Limit Theorem for variance and maybe even for covariance?" ViewCount="669" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm training some neural networks using NEAT C++, and would like to use the &lt;em&gt;cross-entropy&lt;/em&gt; error function:&#10;$$E = -t\ln(y) -(1-t)\ln(1-y)$$&#10;to train a network for a two-class classification problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://nn.cs.utexas.edu/?neat-c&quot; rel=&quot;nofollow&quot;&gt;NEAT C++&lt;/a&gt; is a C++ library that allows one to use a special kind of neuro evolution (namely that of &lt;em&gt;augmenting topologies&lt;/em&gt;) to train the networks. Yet to be able to use it, one needs to define a fitness function (and not an error function, such as the one above). &lt;/p&gt;&#10;&#10;&lt;h3&gt;Question:&lt;/h3&gt;&#10;&#10;&lt;p&gt;How can I construct a fitness function from the cross-entropy error function?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Everything below this line is wrong I guess because $E\in [0,\infty]$, and thus it's impossible to define a maximum. How should I do it then?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I was thinking about this: calculate the maximum error $E_{\text{max}}$ somehow and define the fitness, given a certain network output $y$, as:&#10;$$f(y)=E_{\text{max}} - E(y)$$&#10;This way the fitness will be $0$ when $E(y)=E_{\text{max}}$ and it should reach a maximum if $E(y)=0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this the right method? If so how do I calculate the maximum error $E_{\text{max}}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;note: NEAT does not allow the use of negative fitness functions, so using $f(y)=-E(y)$ would not work. Also I would like to refrain from using arbitrary stuff such as $f(y)=10000000 - E(y)$&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-11T14:55:56.873" FavoriteCount="0" Id="26276" LastActivityDate="2013-03-11T16:36:21.787" LastEditDate="2012-04-11T15:03:58.923" LastEditorUserId="7122" OwnerUserId="7122" PostTypeId="1" Score="1" Tags="&lt;neural-networks&gt;&lt;maximum-likelihood&gt;&lt;error&gt;&lt;entropy&gt;" Title="Cross entropy and the fitness function" ViewCount="837" />
  <row AcceptedAnswerId="26292" AnswerCount="2" Body="&lt;p&gt;I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=&quot;logit&quot;))&#10;&amp;gt; summary(mod1)&#10;&#10;Call:&#10;glm(formula = factor(won) ~ bid, family = binomial(link = &quot;logit&quot;), &#10;    data = mydat)&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-1.5464  -0.6990  -0.6392  -0.5321   2.0124  &#10;&#10;Coefficients:&#10;              Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -2.133e+00  1.947e-02 -109.53   &amp;lt;2e-16 ***&#10;bid          2.494e-03  5.058e-05   49.32   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 83081  on 80337  degrees of freedom&#10;Residual deviance: 80645  on 80336  degrees of freedom&#10;AIC: 80649&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So my equation would look like:&#10;$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;From here I calculated probabilities from all bid levels. &#10;&lt;img src=&quot;http://i.stack.imgur.com/5mLa9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-11T17:00:58.220" FavoriteCount="1" Id="26288" LastActivityDate="2013-06-03T13:40:27.460" LastEditDate="2013-06-03T13:38:42.373" LastEditorUserId="805" OwnerUserId="3310" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;logistic&gt;" Title="Understanding Odds Ratios in Logistic Regression" ViewCount="605" />
  
  <row Body="&lt;p&gt;@whuber is right that we need a little bit more context to decipher what you mean by &quot;samples.&quot;  If you mean &quot;samples&quot; in the sense of &quot;the result of doing sampling,&quot; and thus you're using the term as a synonym of &quot;realizations&quot;, then the following applies:&lt;/p&gt;&#10;&#10;&lt;p&gt;Samples are dependent conditional on some (or possibly no) prior knowledge if and only if knowing something about one sample could tell you something new about the other sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;The most common case is where samples $x_1, x_2$ are &lt;strong&gt;assumed&lt;/strong&gt; to be &quot;independently and identically distributed&quot; according to a distribution $D$.  In that case, given that you know $D$ is say a normal distribution with mean zero and variance one, knowing that the value of $x_1$ is $1.2$ your belief about $x_2$ is still that it follows $D$.  Without knowing $D$, however, but knowing that the samples were iid from some normal distribution, leaves the samples clearly dependent: knowing something about one tells you something about $D$, which tells you something about the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without &lt;strong&gt;assuming&lt;/strong&gt; dependence or independence between samples, it's impossible to know whether they are dependent or independent, but one can often make good guesses by trying to find patterns.  Correlation, the example above, is just one such pattern.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-04-11T19:06:47.697" Id="26297" LastActivityDate="2012-04-12T20:56:42.230" LastEditDate="2012-04-12T20:56:42.230" LastEditorUserId="858" OwnerUserId="858" ParentId="26237" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="26370" AnswerCount="6" Body="&lt;p&gt;Correlation does not imply causation, as there could be many explanations for the correlation. But does causation imply correlation? Intuitively, I would think that the presence of causation means there is necessarily some correlation. But my intuition has not always served me well in statistics. Does causation imply correlation?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-11T20:00:40.300" FavoriteCount="21" Id="26300" LastActivityDate="2014-02-28T15:55:17.970" OwnerUserId="10543" PostTypeId="1" Score="69" Tags="&lt;correlation&gt;&lt;causal-inference&gt;" Title="Does causation imply correlation?" ViewCount="15536" />
  
  <row Body="&lt;p&gt;I'm hesitant to answer this.  These Frequentist vs. Bayesian spats are generally unproductive, and can be nasty and juvenile.  For what it's worth, Wagenmakers is kind of a big deal, whereas largely forgotten 3k+ year old Chinese philosophers on the other hand...  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would argue that the standard Frequentist interpretation of a 50% confidence interval is not that you should be 50% confident the true value lies within the interval, or that there is a 50% probability that it does.  Rather, the idea is simply that, if the same process were repeated indefinitely, the percentage of CI's that included the true value would converge to 50%.  For any given single interval, however, the probability that it includes the true value is either 0 or 1, &lt;em&gt;but you don't know which&lt;/em&gt;.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-11T22:02:41.437" Id="26314" LastActivityDate="2012-04-14T09:54:54.660" LastEditDate="2012-04-14T09:54:54.660" LastEditorUserId="364" OwnerUserId="7290" ParentId="26313" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;There's some intricate cheating involved. The confidence interval $(s,l)$ does not use the information that the range of the uniform is 1, and is thus non-parametric, while the claim made about the sample with $l-s=0.9$ does, and is highly model-dependent. I am pretty sure one can improve either the coverage or the (expected) length of the confidence interval if this information is taken into account. For one thing, the end points of the distribution are at most $1-(l-s)$ away from either $s$ or $l$. Hence, a 100% confidence interval for $\mu$ is $(l-1/2, s+1/2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This particular problem falls into the domain of inference for &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387004548&quot;&gt;partially identified distributions&lt;/a&gt; studied in the last 10-15 years extensively in theoretical econometrics. Likelihood, and hence Bayesian, inference for the uniform distribution is ugly, since it constitutes an &lt;a href=&quot;http://www.stat.unc.edu/postscript/rs/ISI89.pdf&quot;&gt;non-regular problem&lt;/a&gt; (the support of the distribution depends on the unknown parameter).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-11T23:47:21.227" Id="26317" LastActivityDate="2012-04-11T23:47:21.227" OwnerUserId="5739" ParentId="26313" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="26331" AnswerCount="1" Body="&lt;p&gt;I've been reading Wagenmakers (2007) &lt;a href=&quot;http://www.brainlife.org/reprint/2007/Wagenmakers_EJ071000.pdf&quot; rel=&quot;nofollow&quot;&gt;A practical solution to the pervasive problem of p values&lt;/a&gt;. I'm intrigued by the conversion of BIC values into Bayes factors and probabilities. However, so far I don't have a good grasp of what exactly a &lt;strong&gt;unit information prior&lt;/strong&gt; is. I would be grateful for an explanation with pictures, or the R code to generate pictures, of this particular prior.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-12T04:28:55.427" FavoriteCount="1" Id="26329" LastActivityDate="2012-04-12T05:03:00.657" OwnerUserId="966" PostTypeId="1" Score="3" Tags="&lt;prior&gt;&lt;bic&gt;&lt;bayes&gt;&lt;unit-information-prior&gt;" Title="What is a &quot;Unit Information Prior&quot;?" ViewCount="695" />
  
  
  
  <row Body="&lt;p&gt;I recommend &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471056693&quot; rel=&quot;nofollow&quot;&gt;Pattern Classification&lt;/a&gt; by Duda, Hart, and Stork. It's good because it is front-loaded with a lot of classical results on multivariate Gaussians, and discusses things like Fisher linear discriminant analysis, maximum likelihood, etc. The other sections of the book allow you to develop an understanding of how this connects with machine learning, which is important. &lt;/p&gt;&#10;&#10;&lt;p&gt;A lot of statistical texts do not make clear the overlap between classical statistics (e.g. likelihood models, logistic fitting, regression) and more modern machine learning (support vector machines, decision trees). While Duda et. al. is not a great encyclopedic source for all things statistics, I think it's a good first book. It's readable and bridges gaps, which is usually the early priority before needing to chase down lots of specifics to become an expert.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another book that might be helpful in a similar way is: &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/052168689X&quot; rel=&quot;nofollow&quot;&gt;Data Analysis Using Regression and Multilevel/Hierarchical Models&lt;/a&gt; by Gelman and Hill.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-04-13T14:21:53.720" CreationDate="2012-04-12T19:13:05.967" Id="26375" LastActivityDate="2012-04-12T19:24:34.800" LastEditDate="2012-04-12T19:24:34.800" LastEditorUserId="8927" OwnerUserId="8927" ParentId="26372" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Basically, both of your approaches are OK for the problem at hand.&lt;/p&gt;&#10;&#10;&lt;p&gt;On (1), you should look at the appropriate matrix algebra expressions and convince yourself that the coefficient of the dummy variable is indeed the difference between the two group means.&lt;/p&gt;&#10;&#10;&lt;p&gt;On (2), it has as much validity as taking the mean, in the first place. If the mean difference is in any way interpretable, so will be the permutation distribution. Typically, you get the $p$-value directly from the permutation distribution, rather than the standard error: the former is a consistent non-parametric procedure, while the latter would inquire some sort of asymptotic normality argument.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.mostlyharmlesseconometrics.com/&quot; rel=&quot;nofollow&quot;&gt;Angrist and Pischke&lt;/a&gt; are your two best friends here. They treat (1) at great lengths, although I don't think they appreciate permutation that much, so you would also want to make friends with &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/038720279X&quot; rel=&quot;nofollow&quot;&gt;Good&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You would also want to consider (3) $t$-test with Satterthwaite approximation for the degrees of freedom (default in Stata, BTW).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-12T20:19:27.950" Id="26379" LastActivityDate="2012-04-12T20:19:27.950" OwnerUserId="5739" ParentId="26371" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking for clarification: Do we have to run correlation analysis before carrying out principal component analysis (PCA), or is this implicitly subsumed under PCA framework? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am on my way to write a research plan:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Isolate predictors with high collinear relationship to the process.&lt;/li&gt;&#10;&lt;li&gt;Run PCA to convert correlated predictors to uncorrelated principal component variables.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I am wondering if correlation analysis has to be done separately on the dataset in order to choose highly correlated predictors before applying PCA.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-12T20:33:08.620" Id="26380" LastActivityDate="2012-05-13T07:17:00.320" LastEditDate="2012-04-13T06:09:36.103" LastEditorUserId="183" OwnerUserId="10575" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;pca&gt;" Title="Should PCA or correlations be examined first in the context of correlated predictors in regression?" ViewCount="385" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to model some time series data, and I've been reading about tapped delay line and sliding window to transform the input data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my understanding a sliding window with windows size 1 shifts the inputs by one at each time step and feeds them back to a feed forward neural network.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;target| input&#10;     2| 1 0&#10;     3| 2 1&#10;     4| 3 2&#10;     5| 4 3&#10;     6| 5 4&#10;     7| 6 5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In my understanding, a tapped delay line with delay 2 takes the two most recent inputs and outputs and feeds them back to a recurrent neural network&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y(t) = f(x(t-1),y(t-1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there a difference? Why are there different names? Please correct me if I'm wrong.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-12T21:36:59.840" Id="26381" LastActivityDate="2012-04-12T21:36:59.840" OwnerUserId="7255" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;machine-learning&gt;" Title="What is the difference between tapped delay line and sliding window in a neural network?" ViewCount="211" />
  
  <row Body="&lt;p&gt;You could try having a look at the datasets from Kaggle competitions at kaggle.com. Some require a fair degree of pre-processing, but there are some relatively 'clean' datasets there. You can see how your algorithm performs by submitting predictions to either current or past competitions and see how well it performs relatively to other participants.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-13T03:53:42.640" Id="26394" LastActivityDate="2012-04-13T03:53:42.640" OwnerUserId="10354" ParentId="24261" PostTypeId="2" Score="1" />
  
  
  
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and this gives you the confidence interval they presented.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-13T14:43:09.810" Id="26422" LastActivityDate="2012-04-13T14:43:09.810" OwnerUserId="8013" ParentId="26395" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row Body="&lt;p&gt;With the keyword &quot;cluster&quot; and &quot;0/1 data&quot;, my knee-jerk reaction would be to put everything into a cluster analysis machine using a measure of &quot;distance&quot; between observations that only have binary variables. See e.g. &lt;a href=&quot;http://stata.com/help.cgi?measure_option#binary_measure&quot; rel=&quot;nofollow&quot;&gt;Stata help file&lt;/a&gt; describing about a dozen such measures. I would run all possible analyses (hierarchical linkage/dendrogram) to see if there really are any interesting clusters. I would also get rid of the redundant answers that are perfectly collinear with one another (Cat == !Dog in your example); while they probably don't hurt in the simple analyses, they may bite in more complicated analysis and screw identification (Peter Ellis mentioned this in the comments).&lt;/p&gt;&#10;&#10;&lt;p&gt;More advanced, model-based, analysis can be performed along the lines of &lt;a href=&quot;http://www.springerlink.com/content/a351385233p775pu/&quot; rel=&quot;nofollow&quot;&gt;multidimensional item-response theory&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-14T16:10:57.480" Id="26455" LastActivityDate="2012-04-14T16:10:57.480" OwnerUserId="5739" ParentId="26427" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This data set seems a tipical marketing data set. Moreover the question is very vague and obviously, the possible answers are many. I suppose you wish to investigate what are the key factors that influence variables 8 and 9 (intensity of internet purchases). I would do some descriptives about the demographic variables, then I would perform a factor analysis on items  9 to 45 and I would use extracted factors to predict 8 and 9. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-14T19:57:03.973" Id="26460" LastActivityDate="2012-04-14T19:57:03.973" OwnerUserId="6547" ParentId="26451" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="27326" AnswerCount="1" Body="&lt;p&gt;In their textbook, &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CDYQFjAA&amp;amp;url=http://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf&amp;amp;ei=FtiJT_SWDomCgAfenZTPBA&amp;amp;usg=AFQjCNFpT-knxrqr1s23vQe5s2ihvGT9kw&amp;amp;sig2=nj4Qw64tsPNcW6aoewFXIw&quot; rel=&quot;nofollow&quot;&gt;Graphical Models, Exponential Families and Variational Inference&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Michael_I._Jordan&quot; rel=&quot;nofollow&quot;&gt;M. Jordan&lt;/a&gt; and &lt;a href=&quot;http://www.cs.berkeley.edu/~wainwrig/&quot; rel=&quot;nofollow&quot;&gt;M. Wainwright&lt;/a&gt; discuss the connection between &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_families&quot; rel=&quot;nofollow&quot;&gt;Exponential families&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_random_fields&quot; rel=&quot;nofollow&quot;&gt;Markov Random Fields&lt;/a&gt; (undirected graphical models).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to understand better the relationship between them with the following questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Are all MRFs members of the Exponential families? &lt;/li&gt;&#10;&lt;li&gt;Can all members from the Exponential families be represented as an MRF?&lt;/li&gt;&#10;&lt;li&gt;If MRFs $\neq$ Exponential families, &lt;strong&gt;what are some good examples of distributions of one type not ncluded in the other&lt;/strong&gt; ? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;From what I understand in their textbook (Chapter 3), Jordan and Wainwright present the next argument: &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Say we have a a scalar random variable X that follows some distribution $p$, and draw $n$ i.i.d. observations $X^1, \ldots X^n$, and we want to identify $p$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We compute the empirical expectations of certain functions $\phi_\alpha%$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\mu}_\alpha= \frac{1}{n}\sum^n_{i=1}\phi_\alpha(X^i), $ for all $\alpha \in &#10;\mathcal{I}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where each $\alpha$ in some set $\mathcal{I}$ indexes a function $\phi_\alpha: \mathcal{X} \rightarrow R$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Then if we force the following two sets of quantities to be consistent, i.e. to match (to identify $p$):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The expectations $E_p[(\phi_\alpha(X)]=\int_\mathcal{X}\phi_\alpha(x)p(x)\nu(dx)$ of the sufficient statistics $\phi$ of the distribution $p$ &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The expectations  under the empirical distribution&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;we get an &lt;strong&gt;underdetermined problem&lt;/strong&gt;, in the sense that there are many distributions $p$  that are consistent with the observations. So we need a principle for choosing among them (to identify $p$). &lt;/p&gt;&#10;&#10;&lt;p&gt;If we use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_Maximum_Entropy&quot; rel=&quot;nofollow&quot;&gt;principle of maximum entropy&lt;/a&gt; to remove this undeterminancy, we can get a single $p$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\DeclareMathOperator*{\argmax}{arg\,max}&#10;p^* = \argmax_{p\in{\mathcal{P}}} \,H(p)$ subject to $E_p[(\phi_\alpha(X)] = \hat{\mu}_\alpha$ for all $\alpha \in \mathcal{I}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where this $p^*$ takes the form $p_\theta(x)  \propto  $ exp${\sum_{\alpha \in \mathcal{I}}\theta_\alpha \phi_\alpha(x)},$ where $\theta \in R^d$ represents a parameterization of the distribution in exponential family form.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, if we&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Make the expectations of the distributions be consistent with the expectations under the empirical distribution&lt;/li&gt;&#10;&lt;li&gt;Use the principle of maximum entropy to get rid of undetermination&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$\rightarrow$ We end up with a a distribution of the exponential family.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;However, this looks more like an argument to introduce exponential families, and (as far as I can understand) it does not describe the relationship between MRFs and exp. families. Am I missing anything?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-04-14T20:34:55.017" FavoriteCount="4" Id="26467" LastActivityDate="2013-05-01T14:34:20.707" LastEditDate="2013-05-01T14:34:20.707" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="15" Tags="&lt;mathematical-statistics&gt;&lt;graphical-model&gt;" Title="When do Markov random fields $\neq$ exponential families?" ViewCount="492" />
  
  <row Body="&lt;p&gt;Firstly you should be more rigorous when defining the distribution of $X_1$ and $X_2$. I think you mean &#10;$$ (X_1 \mid \mu_1) \sim N(\mu_1, \sigma^2_1)$$&#10;$$ (X_2 \mid \mu_2) \sim N(\mu_2, \sigma^2_2)$$&#10;and even more precisely you should say that $X_1$ and $X_2$ are &lt;em&gt;conditionally independent&lt;/em&gt; given $\mu_1, \mu_2$. Right ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Then use the well-known formula&#10;$$\boxed{\mathrm{cov}(X_1, X_2) = \mathbb{E}\left[\mathrm{cov}(X_1, X_2 \mid \mu_1, \mu_2)\right] + \mathrm{cov}\bigl(\mathbb{E}[X_1 \mid \mu_1, \mu_2], \mathbb{E}[X_2 \mid \mu_1, \mu_2]\bigr)}.$$&#10;Now $\mathrm{cov}(X_1, X_2 \mid \mu_1, \mu_2)=0$ (conditional independance) and $\mathbb{E}[X_1 \mid \mu_1, \mu_2]=\mu_1$, $\mathbb{E}[X_2 \mid \mu_1, \mu_2]=\mu_2$, then it is easy to conclude.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-15T06:43:37.560" Id="26487" LastActivityDate="2012-04-15T06:43:37.560" OwnerUserId="8402" ParentId="26473" PostTypeId="2" Score="4" />
  <row AnswerCount="3" Body="&lt;p&gt;I must make a critical appraisal of a cancer survival paper. I hope someone here can give some hints. The paper investigates person-level markers of socio-economic status as exposure and looks for association of these with survival and also comorbidities, disease/prognostic factors and treatment using Cox proportional hazards for survival and logistic regression for mortality.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the paper is a good one and I can't find the criticisms. I am not sure if Cox proportional hazards is the best model for use; maybe a full paramteric model of relative survival is better, but I don't known. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for a hint if you can. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22315055&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/pubmed/22315055&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-15T09:02:56.653" Id="26493" LastActivityDate="2012-05-18T15:02:05.033" LastEditDate="2012-04-15T11:39:51.853" LastEditorUserId="2970" OwnerUserId="10497" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;survival&gt;&lt;cox-model&gt;" Title="Critical appraisal of survival paper" ViewCount="287" />
  
  <row Body="&lt;p&gt;It depends on several things.&lt;/p&gt;&#10;&#10;&lt;p&gt;One is what your scale was.  Unless it can plausibly be treated as a continuous scale, a t test could not be appropriate.  Some purists argue that you should never treat an ordinal scale as continuous for such purposes.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other, probably more important issue, is what you are interested in.  Do you want to see if there is a perceived net improvement/decline in life; or are you interested in the correlation between the two.  For example (assuming you can treat the variable as continuous) a paired t test would show you if on average people think life is better; but a correlation coefficient might be useful if what you are interested in is whether &quot;Obama&quot; is a decisive factor one way or another.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a highly politically partisan sample balanced between Republicans and Democrats, where everyone who agrees with the first statement disagrees with the second and vice versa.  A paired t statistic will be exactly zero, showing no impact of Obama - but the reality is there was a big impact, in switching around who thinks life is good.  A correlation coefficient of some sort would show this up.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to do a test with a correlation coefficient you should use a bootstrap method.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want an appropriate correlation coefficient for ordinal data you should consider a polychoric correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;If in fact you are interested in the net increase (up or down), then a paired t test would be ok if the data is sufficiently &quot;close&quot; to being continuous that you can pretend it is; but a non-parametric paired test as @psj suggests might be better.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-15T19:45:17.863" Id="26514" LastActivityDate="2012-04-15T19:45:17.863" OwnerUserId="7972" ParentId="26486" PostTypeId="2" Score="1" />
&#10;\E e^{k X} = m_X(k) = (1 - p + p e^k)^n \&amp;gt;,
&#10;\int_0^\infty e^{rx}(\overline{F}(x)) = \int_0^\infty e^{rx}e^{-ax^b}.
  <row Body="&lt;p&gt;I'm dropping the $|x$ for more convenient notation. So read $E(e)$ as $E(e|x)$ if you prefer that...&lt;/p&gt;&#10;&#10;&lt;p&gt;By definition $Var(e)=E\Big(e-E(e)\Big)^2$. Since $E(e)=0$, we have $Var(e)=E\Big(e-E(e)\Big)^2=E(e-0\Big)^2=E(e^2)$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-16T07:59:48.910" Id="26519" LastActivityDate="2012-04-16T07:59:48.910" OwnerUserId="8507" ParentId="26518" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Remember that the &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf&quot; rel=&quot;nofollow&quot;&gt;difference between significant and non-significant is not (always) statistically significant&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, more to the point of your question, model 1 is called pooled regression, and model 2 unpooled regression. As you noted, in pooled regression, you assume that the groups aren't relevant, which means that the variance between groups is set to zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the unpooled regression, with an intercept per group, you set the variance to infinity.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, I'd favor an intermediate solution, which is a hierarchical model or partial pooled regression (or shrinkage estimator). You can fit this model in R with the lmer4 package.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, take a look at &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/multiple2.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper by Gelman&lt;/a&gt;, in which he argues why hierarchical models helps with the multiple comparisons problems (in your case, are the coefficients per group different? How do we correct a p-value for multiple comparisons).&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, in your case,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;summary(lmer( leg ~ head + (1 | site)) # varying intercept model&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you want to fit a varying-intercept, varying slope (the third model), just run&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(lmer( leg ~ head + (1 | site) + (0+head|site) )) # varying intercept, varying-slope model&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then you can take a look at the group variance and see if it's different from zero (the pooled regression isn't the better model) and far from infinity (unpooled regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;update:&#10;After the comments (see below),  I decided to expand my answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The purpose of a hierarchical model, specially in cases like this, is to model the variation by groups (in this case, Sites). So, instead of running an ANOVA to test if a model is different from another, I'd take a look at the predictions of my model and see if the predictions by group is better in the hierarchical models vs the pooled regression (classical regression). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I ran my sugestions above and foudn that &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ranef(lmer( leg ~ head + (1 | site) + (0+head|site) )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Would return zero as estimates of varying slope (varying effect of head by site).&#10;then I ran&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ranef(lmer( leg ~ head + (head| site))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And I got a non-zero estimates for the varying effect of head. I don't know yet why this happened, since it's the first time I found this. I'm really sorry for this problem, but, in my defense, I just followed the specification outlined in the help of the lmer function.&#10;(See the example with the data sleepstudy). I'll try to understand what's happening and I'll report here when (if) I understand what's happening.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-16T16:27:09.097" Id="26546" LastActivityDate="2012-04-19T15:17:22.590" LastEditDate="2012-04-19T15:17:22.590" LastEditorUserId="3058" OwnerUserId="3058" ParentId="26461" PostTypeId="2" Score="6" />
&#10;     $ position: Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 1 1 1 2 2 2 2 2 ...&#10; $ y       : num  2.76 5.5 2.54 1.56 6.46 ...&#10;&amp;gt; head(dat)&#10;  tube position        y&#10;1    1        A 2.759443&#10;2    1        A 5.496689&#10;3    1        A 2.540150&#10;4    1        A 1.558261&#10;5    1        A 6.462050&#10;6    1        B 4.239749&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The corresponding nlme model is the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; # firstly set position C as the &quot;intercept&quot; for concordance with SAS&#10;&amp;gt; dat$position &amp;lt;- relevel(dat$position, &quot;C&quot;)&#10;&amp;gt; # load nlme&#10;&amp;gt; library(nlme)&#10;&amp;gt; # fit the model&#10;&amp;gt; ( fit1 &amp;lt;- lme(y ~ position, data=dat, random= list(tube = pdCompSymm(~ 0+position ))) )&#10;Linear mixed-effects model fit by REML&#10;  Data: dat &#10;  Log-restricted-likelihood: -199.0196&#10;  Fixed: y ~ position &#10;(Intercept)   positionA   positionB &#10;   8.526544   -4.800535   -3.322507 &#10;&#10;Random effects:&#10; Formula: ~0 + position | tube&#10; Structure: Compound Symmetry&#10;          StdDev   Corr       &#10;positionC 1.892433            &#10;positionA 1.892433 0.082      &#10;positionB 1.892433 0.082 0.082&#10;Residual  1.932750            &#10;&#10;Number of Observations: 90&#10;Number of Groups: 6 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This model is equivalent to a 2-way ANOVA with mixed effets (in the sense that the marginal models are the same), which is more easy to fit with lme4:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(lme4)&#10;&amp;gt; lmer(y ~ position + (1|tube) + (1|tube:position), data=dat)&#10;Linear mixed model fit by REML &#10;Formula: y ~ position + (1 | tube) + (1 | tube:position) &#10;   Data: dat &#10; AIC BIC logLik deviance REMLdev&#10; 410 425   -199    402.5     398&#10;Random effects:&#10; Groups        Name        Variance Std.Dev.&#10; tube:position (Intercept) 3.28587  1.81270 &#10; tube          (Intercept) 0.29543  0.54354 &#10; Residual                  3.73552  1.93275 &#10;Number of obs: 90, groups: tube:position, 18; tube, 6&#10;&#10;Fixed effects:&#10;            Estimate Std. Error t value&#10;(Intercept)   8.5265     0.8493  10.039&#10;positionA    -4.8005     1.1595  -4.140&#10;positionB    -3.3225     1.1595  -2.866&#10;&#10;Correlation of Fixed Effects:&#10;          (Intr) postnA&#10;positionA -0.683       &#10;positionB -0.683  0.500&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Below I check that the results are indeed the same:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; # same standard erros&#10;&amp;gt; sqrt(diag(fit1$varFix))&#10;(Intercept)   positionA   positionB &#10;  0.8493533   1.1594505   1.1594505 &#10;&amp;gt; # the total variance in the second model is given in the first model:&#10;&amp;gt; sqrt(1.81270^2+ 0.54354^2)&#10;[1] 1.892437&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Well. Now here are the two equivalent SAS models:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/* First model */&#10;PROC MIXED DATA=dat ;&#10;CLASS POSITION TUBE ;&#10;MODEL y = POSITION ;&#10;RANDOM POSITION / subject=TUBE type=CS ;&#10;RUN; QUIT;&#10;&#10;/* Second model */&#10;PROC MIXED DATA=dat ;&#10;CLASS POSITION TUBE ;&#10;MODEL y = POSITION ;&#10;RANDOM TUBE TUBE*POSITION ;&#10;RUN; QUIT;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Results are identical to the R results. But SAS assigns different degrees of freedom for the fixed effects and consequently gives different confidence intervals, as shown below.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first model gives degrees of freedom 5, 10, 10:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Effect  position    Estimate    StandardError   DF  Alpha   Lower   Upper&#10;Intercept       8.5265  0.8494  5   0.05    6.3432  10.7099&#10;position    A   -4.8005 1.1595  10  0.05    -7.384  -2.2171&#10;position    B   -3.3225 1.1595  10  0.05    -5.9059 -0.7391&#10;position    C   0   .   .   .   .   .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;whereas the second model gives degrees of freedom 15, 15, 15:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Effect  position    Estimate    StandardError   DF  Alpha   Lower   Upper&#10;Intercept       8.5265  0.8494  15  0.05    6.7162  10.3369&#10;position    A   -4.8005 1.1595  15  0.05    -7.2718 -2.3292&#10;position    B   -3.3225 1.1595  15  0.05    -5.7938 -0.8512&#10;position    C   0   .   .   .   .   .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="8" CreationDate="2012-04-16T19:37:09.020" FavoriteCount="1" Id="26556" LastActivityDate="2012-05-31T14:11:06.607" OwnerUserId="8402" PostTypeId="1" Score="5" Tags="&lt;mixed-model&gt;&lt;sas&gt;" Title="Equivalent mixed models yielding different results in SAS" ViewCount="639" />
  <row Body="&lt;p&gt;The function &lt;code&gt;cv.glmnet&lt;/code&gt; from the R package &lt;a href=&quot;http://cran.r-project.org/web/packages/glmnet/&quot;&gt;glmnet&lt;/a&gt; does automatic cross-validation on a grid of $\lambda$ values used for $\ell_1$-penalized regression problems. In particular, for the lasso. The glmnet package also supports the more general &lt;em&gt;elastic net&lt;/em&gt; penalty, which is a combination of $\ell_1$ and $\ell_2$ penalization. As of version 1.7.3. of the package taking the $\alpha$ parameter equal to 0 gives ridge regression (at least, this functionality was not documented until recently).&lt;/p&gt;&#10;&#10;&lt;p&gt;Cross-validation is an estimate of the expected generalization error for each $\lambda$ and $\lambda$ can sensibly be chosen as the minimizer of this estimate. The &lt;code&gt;cv.glmnet&lt;/code&gt; function returns two values of $\lambda$. The minimizer, &lt;code&gt;lambda.min&lt;/code&gt;, and the always larger &lt;code&gt;lambda.1se&lt;/code&gt;, which is a heuristic choice of $\lambda$ producing a less complex model, for which the performance in terms of estimated expected generalization error is within one standard error of the minimum. Different choices of loss functions for measuring the generalization error are possible in the glmnet package. The argument &lt;code&gt;type.measure&lt;/code&gt; specifies the loss function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, the R package &lt;a href=&quot;http://cran.r-project.org/web/packages/mgcv/index.html&quot;&gt;mgcv&lt;/a&gt; contains extensive possibilities for estimation with quadratic penalization including automatic selection of the penalty parameters. Methods implemented include generalized cross-validation and REML, as mentioned in a comment. More details can be found in the package authors book: &lt;em&gt;Wood, S.N. (2006) Generalized Additive Models: an introduction with R, CRC.&lt;/em&gt; &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-16T19:57:52.173" Id="26560" LastActivityDate="2012-04-17T11:59:37.753" LastEditDate="2012-04-17T11:59:37.753" LastEditorUserId="4376" OwnerUserId="4376" ParentId="26528" PostTypeId="2" Score="11" />
  
  <row AnswerCount="0" Body="&lt;p&gt;This may be a lame question, but I got stuck and can't get my head around it. I am running a gene expression analysis, comparing $\sim 10,000$ genes between two groups, $n=6$ samples per group. My pipeline goes like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I check whether the two groups have equal variances using Levene's test for each gene.&lt;/li&gt;&#10;&lt;li&gt;For those genes that do have equal variances, I run a moderated t-test (as in limma), as it assumes equal variances.&lt;/li&gt;&#10;&lt;li&gt;For the rest of the genes I run Mann-Whitney U.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;(Before you tell me about multiple test correction, I do calculate FDR later on by permuting the population labels on samples and re-running the above tests).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I want to calculate effect size for all the genes. I am using Cohen's d with pooled variance (&lt;a href=&quot;http://en.wikipedia.org/wiki/Cohen%27s_d#Cohen.27s_d&quot; rel=&quot;nofollow&quot;&gt;see Wikipedia&lt;/a&gt;), but I wonder whether I can do it for both groups of genes (ie ones with and without equal variance between groups). I think I can't, but it's a guess really (I think I can only run it for the equal variance genes). If I can't, is there a more appropriate way to do this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-16T21:57:06.143" Id="26567" LastActivityDate="2014-04-26T23:51:18.740" LastEditDate="2014-04-26T23:51:18.740" LastEditorUserId="26338" OwnerUserId="10662" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;effect-size&gt;&lt;cohens-d&gt;" Title="Does pooled variance correct for/protect from unequal variance when calculating effect size?" ViewCount="332" />
  
  
  <row AcceptedAnswerId="26592" AnswerCount="2" Body="&lt;p&gt;does anyone know if there exists some binary dataset that i can freely download?&lt;/p&gt;&#10;&#10;&lt;p&gt;If possible, i prefer dataset in textual format and inspired by some real phenomenon.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance,&lt;/p&gt;&#10;" ClosedDate="2012-04-17T17:31:52.973" CommentCount="0" CreationDate="2012-04-17T10:48:48.923" Id="26591" LastActivityDate="2012-04-17T12:02:32.110" OwnerUserId="10669" PostTypeId="1" Score="1" Tags="&lt;dataset&gt;" Title="Public available binary dataset" ViewCount="1272" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;First of all, I apologize for my half-forgotten understanding of statistics.  I hope I can accurately describe what kind of test I'm after in a way that doesn't sound confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have one sample and one population.  I would like to know the probability that my sample is part of that population.  But I also want to know what are the maximum number of &quot;guesses&quot; from my sample it will take in order to conclude that it is, or is not a part of that population.  Let's say I'm looking for a 5% confidence that the sample is or is not part of the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;So basically, I take a number randomly from my sample, and check to see if it exists in the population.  How many times should I guess until I can feel confident that the sample is or is not part of the population?  I have a funny feeling this is a pretty elementary question and I'm just confused by the word problem aspect of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, I know the size of the sample and the size of the population, if that helps.  A mean of the sample &amp;amp; population is meaningless because these numbers are values, they're labels.  I'm certain there's a more correct way to describe this.  But essentially the numbers are like house addresses, so the difference between 1 and 5 compared to 25 and 300 is meaningless.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help.  I suspect this problem is related to statistical process control.  Thanks again.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-17T18:43:32.430" Id="26622" LastActivityDate="2012-04-17T18:43:32.430" OwnerUserId="10680" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;confidence-interval&gt;" Title="How many times can I guess a number from Sample 1 belongs to Sample 2 before concluding that they are not the same?" ViewCount="51" />
  <row AcceptedAnswerId="26634" AnswerCount="1" Body="&lt;p&gt;I need to calculate the pooled odds ratio and associated 95% confidence interval for meta-analysis of 2 studies about the risk of bleeding. The only information I have is the odds ratios and 95% confidence intervals. They are 2.7 (1.8 – 4.0) in the first, and 1.3 (0.5 – 3.4) in the second study.&lt;/p&gt;&#10;&#10;&lt;p&gt;I computed the standard errors, weights, and pooled SE, OR and CI from the available ORs and CIs. According to my calculations, the standard errors are 0.204 in the first, and 0.489 in the second study, which gives the pooled OR and CI of 2.49 (1.72 – 3.60).&lt;/p&gt;&#10;&#10;&lt;p&gt;I’m not sure about this and I would appreciate if someone checks this up.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-17T18:57:25.517" FavoriteCount="1" Id="26623" LastActivityDate="2012-04-17T22:00:32.413" OwnerUserId="10681" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;meta-analysis&gt;&lt;odds-ratio&gt;" Title="Odds ratio and confidence interval in meta-analysis" ViewCount="920" />
  
  <row Body="&lt;p&gt;To generate multivariate normal data with a specified correlation structure, you need to construct the variance covariance matrix and calculate its Cholesky decomposition using the &lt;code&gt;chol&lt;/code&gt; function. The product of the Cholesky decomposition of the desired vcov matrix and independent random normal vectors of observations will yield random normal data with that variance covariance matrix.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v &amp;lt;- matrix(c(2,.3,.3,2), 2)&#10;cv &amp;lt;- chol(v)&#10;&#10;o &amp;lt;- replicate(1000, {&#10;  y &amp;lt;- cv %*% matrix(rnorm(100),2)&#10;&#10;  v1 &amp;lt;- var(y[1,])&#10;  v2 &amp;lt;- var(y[2,])&#10;  v3 &amp;lt;- cov(y[1,], y[2,])&#10;&#10;  return(c(v1,v2,v3))&#10;})&#10;&#10;## MCMC means should estimate components of v&#10;rowMeans(o)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-04-17T20:34:39.540" Id="26628" LastActivityDate="2012-04-17T20:34:39.540" OwnerUserId="8013" ParentId="26606" PostTypeId="2" Score="1" />
&#10;\frac{\bar{D}_{1} \hat{\beta}_{1}}{ln(\bar{SF}_{1}) - \hat\alpha_{1}} + \frac{\bar{SF}_{1}  - \hat{\alpha}_{2}}{{0.5} - \hat\alpha_{2}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Notes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$\bar{D}_{1}$ is independent of $(\hat\alpha_{1}, \hat\beta_{1})$.&lt;/li&gt;&#10;&lt;li&gt;$(\hat\alpha_{1}, \hat\beta_{1})$ are independent of $(\hat\alpha_{2})$.&lt;/li&gt;&#10;&lt;li&gt;$log(SF_{1}) = \alpha_{1} + \beta_{1} x + \epsilon$&lt;/li&gt;&#10;&lt;li&gt;$E(\bar{D}_{1}) = \mu_{D}$, and $Var(\bar{D}_{1}) = \sigma^{2}_{D}$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I'm trying to find $E({log(CI))}$ and $Var(log(CI))$. I know this can be done using delta method, but I'm not sure how... your help is appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-17T20:49:26.450" Id="26629" LastActivityDate="2012-04-18T14:36:43.077" LastEditDate="2012-04-18T14:36:43.077" LastEditorUserId="3748" OwnerUserId="9136" PostTypeId="1" Score="2" Tags="&lt;delta-method&gt;" Title="The mean and Variance of $log(Combination Index)$" ViewCount="145" />
  <row Body="&lt;p&gt;I did the following in Stata, the first is fixed effect and the second is random effect. I got different answers than you did.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           Study     |     ES    [95% Conf. Interval]     % Weight&#10;---------------------+---------------------------------------------------&#10;1                    |  2.700       1.800     4.000         63.47&#10;2                    |  1.300       0.500     3.400         36.53&#10;---------------------+---------------------------------------------------&#10;I-V pooled ES        |  2.189       1.312     3.065        100.00&#10;---------------------+---------------------------------------------------&#10; Heterogeneity calculated by formula&#10;  Q = SIGMA_i{ (1/variance_i)*(effect_i - effect_pooled)^2 } &#10;where variance_i = ((upper limit - lower limit)/(2*z))^2 &#10;&#10;&#10;&#10; Heterogeneity chi-squared =   2.27 (d.f. = 1) p = 0.132&#10;  I-squared (variation in ES attributable to heterogeneity) =  56.0%&#10;&#10;  Test of ES=0 : z=   4.89 p = 0.000&#10;&#10;. metan or ll ul, effect(Odds Ratio) null(1) lcols(trialname) texts(200) random&#10;&#10;           Study     |     ES    [95% Conf. Interval]     % Weight&#10;---------------------+---------------------------------------------------&#10;1                    |  2.700       1.800     4.000         55.93&#10;2                    |  1.300       0.500     3.400         44.07&#10;---------------------+---------------------------------------------------&#10;D+L pooled ES        |  2.083       0.721     3.445        100.00&#10;---------------------+---------------------------------------------------&#10; Heterogeneity calculated by formula&#10;  Q = SIGMA_i{ (1/variance_i)*(effect_i - effect_pooled)^2 } &#10;where variance_i = ((upper limit - lower limit)/(2*z))^2 &#10;&#10;  Heterogeneity chi-squared =   2.27 (d.f. = 1) p = 0.132&#10;  I-squared (variation in ES attributable to heterogeneity) =  56.0%&#10;  Estimate of between-study variance Tau-squared =  0.5488&#10;&#10;  Test of ES=0 : z=   3.00 p = 0.003&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-04-17T22:00:32.413" Id="26634" LastActivityDate="2012-04-17T22:00:32.413" OwnerUserId="561" ParentId="26623" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two matrices A and B of the same dimensions.  Each cell a(i,j) represents some parameter I'm interested in, and b(i,j) represents the corresponding value in the other matrix.  The cells are not interrelated.  What's the best way to represent how close these two matrices are?  &lt;/p&gt;&#10;&#10;&lt;p&gt;The entries in these matrices are the number of occurrences of some event out of a large sample of events.  If this is the case, does the problem make more sense?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-17T22:13:48.593" Id="26637" LastActivityDate="2012-04-18T01:09:08.373" OwnerUserId="10686" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;matrix&gt;" Title="Measuring correlation between two matrices" ViewCount="869" />
  <row Body="&lt;p&gt;If there really is an underlying ability then it should &lt;em&gt;certainly&lt;/em&gt; be the case that the items you use to measure it correlate with one another (which is what I assume 'condition on' means here).  Indeed, most explicit measurement models, e.g. factor analysis, assume that you want the model such that defines the underlying ability is the quantity that, if it were to be known, there would be no remaining correlation to explain.  So if you are thinking of weighting these quantities, you are implicitly thinking of applying such a measurement model because factor analysis 'scores' are (often) essentially weighted averages of the model's items.&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case, whether or not you actually want to fit such a model it will be helpful to think what sort of transformation of your items would make it best behaved if you did.  You have two difference measures and, conditional on the unobserved ability of your manager, these look like they will have rather different variances.  For example, earnings projections in 10s and in 1000s should have errors of similar orders of magnitude and long and short term deals should have differences on a long and short time scale, both regardless of actual ability.  So you might be better transforming them to make different managers comparable.  (This is the equivalent in a model of asking for reasonably constant item variance conditional on ability).  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, you might consider working with a chi-square type measure that rescales each item by dividing the difference between managers' predictions and their outcomes by their predictions.  At that point you can decide whether you want to simply take an average or run a factor analysis on it to get weights.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-17T22:32:08.240" Id="26638" LastActivityDate="2012-04-17T22:32:08.240" OwnerUserId="1739" ParentId="26619" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;em&gt;Note: the original question title was &quot;Is there a way to predict future change based on data?&quot; This answer is phrased as a response to that&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The short answer to your question is &quot;yes, with an appropriate model&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The long answer is &quot;no, probably not&quot;, because what constitutes &quot;an appropriate model&quot; is highly dependent on your data, and on any assumptions you make about that data.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, do you envisage linear growth (i.e. the number of visitors increases by approximately the same number each month), or exponential growth (visitor numbers increase by a similar percentage each month)? Both of these might be optimistic, it might be that you are experiencing annual periodic visitations, and are just seeing the upswing in your three months of data, in which case you might expect a downswing of equivalent magnitude three months later. You might equally see spikes in your data (very common on websites), due to external factors that are outside your control, and largely unpredictable (e.g. you got re-tweeted). &lt;/p&gt;&#10;&#10;&lt;p&gt;Lots of the above can be modelled by various statistical methods, such as an ARIMA model, which will separated out the trend and cyclical components, so that you can see more clearly what's going on. Whether models like this are appropriate for forecasting is highly dependent on the quality, length, and various other attributes (like variability) of your data. There's a good introduction to this kind of stuff, with examples in R, at &lt;a href=&quot;http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html&quot; rel=&quot;nofollow&quot;&gt;http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Probably a better way to model your site's visitations in many cases is to base your model on some real world factors. For example, if your website is for a summer clothing company, or a sports manufacturer (say, soccer balls), you really ARE going to see annual periodic patterns in your data, as those products are only in high demand during half the year. If your website is for a restaurant, you'll probably find that you have a weekly periodic pattern. An ARIMA model will pick up these cycles if your data set is long enough. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your website is promoting a Thneed (or some other new product that everyone needs), you might be able to make the assumption that website growth will be exponential, just because the product is so brilliant, or whatever. You could then fit an exponential trend to the data, and see where it takes you. This assumption would have to be defended (and in most cases that would be pretty difficult). This emphasises the point that, especially when used for prediction, &lt;em&gt;statistics isn't just about putting numbers on things, it's about making good arguments&lt;/em&gt;, any using numbers to back yourself up.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your specific case, it's a bit hard to give any useful advice without seeing the dataset, but if the set is sparse, the answer would have to be &quot;don't bother&quot; (in particular, I think &lt;em&gt;three months&lt;/em&gt; of data is &lt;em&gt;highly unlikely&lt;/em&gt; to be adequate for predicting &lt;em&gt;six months&lt;/em&gt; ahead - usually you're gonna want a much larger sample than your prediction interval).&lt;/p&gt;&#10;&#10;&lt;p&gt;Ultimately, you're probably better off putting effort into site design, promotion, and more importantly, &lt;em&gt;quality content&lt;/em&gt;, because this is what's going to decide how the future pans out for your website. You never know, if you spend a week learning the stats required for this prediction, you might miss adding the one piece of timely content that gets slash-dotted, and shoots you into the big league. On the other hand, stats is worth learning for other reasons, like being able to cut through other people's bullshit, and a week out of your life learning some basic stats is probably time well spent.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-18T00:55:11.557" Id="26642" LastActivityDate="2012-04-18T08:49:32.727" LastEditDate="2012-04-18T08:49:32.727" LastEditorUserId="9007" OwnerUserId="9007" ParentId="25181" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two observational variables, $\text{C}_{\text{obs}}$ and $R$, both subject to measurement error. I believe that a model of the form $$\text{C}_{\text{mod}} = a_0 + a_1R + a_2R^2 + a_3R^3$$ would be a reasonable representation of the relationship between $\text{C}_{\text{obs}}$ and $R$.  I want to determine the coefficient vector $[a_0, a_1, a_2, a_3]$ in this model.  An often-cited reference in my field describes finding these parameters using a procedure that forces the slope and intercept of the linear relationship between $\text{C}_{\text{mod}}$ and $\text{C}_{\text{obs}}$ to be 1 and 0, but does not provide any details on the method.  I have attempted to contact the author directly without success, and I have been unable to find references to such a procedure (but I may not be using the correct search terms).  I am an IDL user and am familiar with R; I would appreciate it if someone could explain how to solve this problem or point me to IDL or R routines that might be applied.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-18T02:50:02.610" Id="26648" LastActivityDate="2014-05-11T18:21:13.400" LastEditDate="2014-05-11T18:21:13.400" LastEditorUserId="26338" OwnerUserId="10695" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;polynomial&gt;" Title="Determining polynomial model coefficients forcing slope = 1 and intercept =0" ViewCount="171" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am running a multi-level model on data which is set out with one row per individual survey return, each with Individual ID and Organisational ID as columns. There are variables which can be aggregated in to group, but I first have to justify this aggregation, and believe I have to use ICC(1), ICC(2) and Rwg. Although I can find a lot explaining what these are, I cannot seem to figure out how to do this with my output in SPSS. &lt;/p&gt;&#10;&#10;&lt;p&gt;I at first believed it came from the output of a one-way ANOVA, but have since been told it should be two-way for ICC(2) as this is a two-level multi-level model.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help me on this one? I can't see how I can do a two-way ANOVA as I surely only need to use the Organisational ID and aggregating variable in question and two-way requires three variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be very gratefully received.&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-18T09:06:14.360" Id="26661" LastActivityDate="2012-09-26T04:02:23.833" LastEditDate="2012-07-27T06:58:57.667" LastEditorUserId="930" OwnerUserId="10707" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;spss&gt;&lt;multilevel-analysis&gt;&lt;aggregation&gt;&lt;intraclass-correlation&gt;" Title="Justification of aggregation in multi-level model - ICC in SPSS" ViewCount="3454" />
&#10;p=n(\frac{x-1}{x})^{(n-1)}
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Good variable names are:&lt;/p&gt;&#10;&#10;&lt;p&gt;a) short / easy to type, &lt;/p&gt;&#10;&#10;&lt;p&gt;b) easy to remember, &lt;/p&gt;&#10;&#10;&lt;p&gt;c) understandable / communicative.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Am I forgetting anything?  Consistency is something to look for.  The way I would put it is that consistent naming conventions contribute to the qualities above.  Consistency contributes to (b) ease of recall and (c) understandability, though other factors are often more important.  There is a clear tradeoff between (a) name length / ease of typing (e.g. all lowercase) and (c) understandability.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm investing a fair bit of thought in these issues because thousands of people are using the &lt;a href=&quot;http://www.cpc.unc.edu/projects/addhealth&quot; rel=&quot;nofollow&quot;&gt;data&lt;/a&gt; and I hope many will use &lt;a href=&quot;https://github.com/MichaelMBishop&quot; rel=&quot;nofollow&quot;&gt;my code&lt;/a&gt; to prepare the data and facilitate some  types of analysis.  The data, from the Longitudinal Study of Adolescent Health, is broken down into multiple datasets.  My first step was to take the 227 variables in the most commonly used dataset, recode them, give them more meaningful names.  Original variable names are things like &quot;aid&quot;, &quot;s1&quot;, &quot;s2&quot;, which I renamed &quot;aid2&quot;, &quot;age&quot;, and &quot;male.is&quot;.  There are thousands of other variables in the other datasets which may be merged depending on what the researcher's goals are.&lt;/p&gt;&#10;&#10;&lt;p&gt;As long as I'm renaming variables, I want to make them as useful as possible.  Here are some of the issues I've considered.&#10;So far, I have only used lower-case and avoided using any dashes or underscores, and I've only used periods for one very specific purpose.  This has the virtue of simplicity and consistency, and causes no problems for most variables.  But as things get more complex I'm tempted to break my consistency.  Take, for example, my variable &quot;talkprobmsum&quot;, it would be easier to read as &quot;talkProbMSum&quot; or better still &quot;talk.prob.m.sum&quot;, but if I'm going to use capital letters or periods to separate words then shouldn't I do it for all variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;Some variables are recorded at more than one time, e.g. the race variables so I appended .is or .ih to indicate whether they come from the in-school or in-home questionnaire.  But there are surely some repeats I'm not aware of yet, would it be better to append a reference to the dataset to the name of every variable?&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to group-center and standardize a lot of variables, the way I've done that is by appending .zms meaning z-score by male and by school.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any general or specific thoughts or resources are greatly appreciated.  See &lt;a href=&quot;https://github.com/MichaelMBishop/inschoolAddHealth&quot; rel=&quot;nofollow&quot;&gt;this repository&lt;/a&gt; for some of my code, and descriptive statistics with a list of variable names.  I briefly described the reason for sharing this code &lt;a href=&quot;http://permut.wordpress.com/2012/03/26/share-your-code-here-is-some-for-add-health/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, and it was publicized a bit &lt;a href=&quot;http://orgtheory.wordpress.com/2012/04/02/code-for-the-add-health-data-set/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, but these last two links aren't really relevant to the issue of variable naming conventions.  &lt;strong&gt;Added:&lt;/strong&gt; I edited this lightly, mostly just moving a paragraph, to try to avoid some of the confusion evident in the comments.  Thanks for thoughts!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-18T17:20:13.310" Id="26699" LastActivityDate="2012-04-18T20:20:01.323" LastEditDate="2012-04-18T20:20:01.323" LastEditorUserId="3748" OwnerUserId="3748" PostTypeId="1" Score="4" Tags="&lt;best-practices&gt;&lt;open-source&gt;" Title="improving my variable names" ViewCount="181" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm working on a Bayesian inference package and I have a function that finds the maximum a posteriori point. Right now I'm using scipy's &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html&quot; rel=&quot;nofollow&quot;&gt;implementation&lt;/a&gt; of BFGS to find that minimum of the log posterior, but I've had some trouble with handling large negatives and -infinities. I'd like to see if there's a better algorithm, particularly something that works well on problems that &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Will be stiff in some dimensions (like scale parameters)&lt;/li&gt;&#10;&lt;li&gt;Have boundaries (like uniform distributions) &lt;/li&gt;&#10;&lt;li&gt;Have a largish number of dimensions (say at least 100)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I'm looking for suggestions on what optimization algorithms to look at.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm okay with implementing something myself.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-18T17:29:03.563" Id="26701" LastActivityDate="2012-10-14T21:34:44.617" LastEditDate="2012-04-18T17:35:00.530" LastEditorUserId="1146" OwnerUserId="1146" PostTypeId="1" Score="3" Tags="&lt;maximum-likelihood&gt;&lt;optimization&gt;" Title="What is a good optimization algorithm for finding the MAP?" ViewCount="274" />
  <row Body="&lt;p&gt;I don't see how you can prove Algo2 is significantly faster than Algo 1 using one-sample t-test. If you know or can assume that Algo1 and Algo2 are independent of each other and have equal variance, you can definitely prove the difference between them using two-sample t-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Two sample t-test:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Independent two-sample t-test (equal sample sizes and equal variance) is only used when both (1) two sample sizes (the number of Algo1 and Algo2) are equal AND (2) can be assumed that the two distributions have the same variance.&lt;/li&gt;&#10;&lt;li&gt;Independent two-sample t-test (unequal sample sizes and equal variance) is only used when it can be assumed that the two distributions have the same variance.&lt;/li&gt;&#10;&lt;li&gt;Independent two-sample t-test (unequal sample sizes and unequal variance)&lt;/li&gt;&#10;&lt;li&gt;Dependent t-test is used when the samples are dependent (e.g. repeated measure).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Student_t_test#Independent_two-sample_t-test&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; has a nice explanation of them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-18T19:20:53.373" Id="26710" LastActivityDate="2012-04-18T19:20:53.373" OwnerUserId="9651" ParentId="26707" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;How can I express multiple response variables for linear model?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I can do this with single response variable&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; lm(Y1 ~ X1+X2+X3, data=somedata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But if I want to use Y1-Y100 response variable in the model, how can I do that?&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: I am hasty. I just found an answer from &lt;a href=&quot;http://www.mail-archive.com/r-help@r-project.org/msg73770.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mail-archive.com/r-help@r-project.org/msg73770.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with(data, lm.fit(x=cbind(Intercept=1,x1,x2,x3), y=cbind(y1,y2,y3)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and this one works too&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; lm(cbind(Y1,Y2,Y3) ~ X1+X2+X3, data=somedata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2013-10-02T14:23:17.077" CommentCount="2" CreationDate="2012-04-18T20:42:36.910" Id="26712" LastActivityDate="2012-04-18T21:15:25.047" LastEditDate="2012-04-18T21:15:25.047" LastEditorUserId="5597" OwnerUserId="5597" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="regression with multiple response variable in R" ViewCount="1008" />
  
  <row Body="&lt;p&gt;This is the only reference I know of&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Frölich, M. (2006), Non-parametric regression for binary dependent&#10;  variables. The Econometrics Journal, 9: 511–540. doi:&#10;  10.1111/j.1368-423X.2006.00196.x&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2012-04-19T03:30:36.170" Id="26731" LastActivityDate="2012-04-19T11:46:06.367" LastEditDate="2012-04-19T11:46:06.367" LastEditorUserId="930" OwnerUserId="8926" ParentId="26728" PostTypeId="2" Score="1" />
  
  
  
  
  <row AcceptedAnswerId="26752" AnswerCount="1" Body="&lt;p&gt;Disclosure: I'm preparing my oral exam for statistics (so I'm tagging as homework, hope it's appropriate).&lt;/p&gt;&#10;&#10;&lt;p&gt;I think that I'm failing to understand the concept of power of a test (under a graphical point of view). I'll try to explain what I have in mind:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a test for which I have&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;H0: Mu = 52&#10;H1: Mu &amp;gt; 52&#10;Confidence: alpha&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And I'm asked to find the power of the test when the true mean is 50.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I know is that power = 1 - beta = P(rej H0 | H0 is false); and that beta = P(not rej H0 | H0 is false) aka type II error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this figure a correct representation of my reasoning?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/v3den.png&quot; alt=&quot;power of a test&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How would the picture look like if my test was something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;H0: Mu = 50&#10;H1: Mu &amp;lt; 50&#10;Confidence: alpha&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And I was given the true mean = 52 for checking the power of the test? Would it be the same figure but with alpha and beta swapped?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-19T10:14:00.693" FavoriteCount="1" Id="26748" LastActivityDate="2012-04-19T17:10:01.630" LastEditDate="2012-04-19T11:52:58.980" LastEditorUserId="10744" OwnerUserId="10744" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;power&gt;" Title="How to represent statistical power graphically for a given hypothesis test?" ViewCount="553" />
  
  <row Body="&lt;p&gt;I have had good success using effective AIC, that is using AIC with the effective degrees of freedom - see Gray JASA 87:942 1992 for effective d.f.  This is implemented for $L_{2}$ penalty in the R &lt;code&gt;rms&lt;/code&gt; package for linear and logistic models, and the &lt;code&gt;rms&lt;/code&gt; &lt;code&gt;pentrace&lt;/code&gt; function can be used to solve for the shrinkage coefficient that optimizes effective AIC.  A case study that shows how to do differential shrinkage (e.g. more shrinkage for interactions) is Harrell et al Stat in Med 17:909, 1998.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-19T12:08:25.107" Id="26751" LastActivityDate="2012-04-19T12:08:25.107" OwnerUserId="4253" ParentId="26528" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="26796" AnswerCount="2" Body="&lt;p&gt;I'm analysing reaction time data from a grammaticality judgement task (collected in a masked-priming experiment).  The stimulus were noun-noun compounds, including 3 types of compounds (depending on semantic relation).  Each compound was tested 4 times, in a 2x2 design (prime = N1 or N2; order = grammatical or ungrammatical).  Participants included native and non-native speakers.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have removed items with physically impossibly short RTs and latencies excluding 5 seconds, and am now concerned that more subtle outliers should be removed prior to analysis.  Following  &lt;a href=&quot;http://dialnet.unirioja.es/servlet/articulo?codigo=3405162&quot; rel=&quot;nofollow&quot;&gt;Baayen &amp;amp; Milin (2010)&lt;/a&gt; &lt;a href=&quot;http://mvint.usbmed.edu.co:8002/ojs/index.php/web/article/view/483/483&quot; rel=&quot;nofollow&quot;&gt;[pdf]&lt;/a&gt;, I have transformed reaction times as 1/RT.  They suggest that &quot;if the precondition of normality is well met, [...] outlier removal before model fitting is not necessary&quot;.  My data is not normally distributed when considered by subject (the Shapiro test indicates only 1 in 21 subjects yields p &gt; 0.05), but I guess this is to be expected given the design?&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I screen the data for outliers prior to model fitting? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;@article{&#10;   Author = {Baayen, R. Harald and Milin, Petar},&#10;   Title = {Analysing Reaction Times},&#10;   Journal = {International Journal of Psychological Research},&#10;   Volume = {3},&#10;   Number = {2},&#10;   Pages = {12--28},&#10;      Year = {2010} }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-04-19T14:17:21.277" FavoriteCount="1" Id="26757" LastActivityDate="2012-04-20T15:02:21.687" LastEditDate="2012-04-20T15:02:21.687" LastEditorUserId="442" OwnerUserId="10746" PostTypeId="1" Score="6" Tags="&lt;normal-distribution&gt;&lt;multilevel-analysis&gt;&lt;outliers&gt;" Title="Outlier removal prior to mixed-effect modelling" ViewCount="962" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have this question: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It is reported that 45% of the population prefer apples and 55% prefer oranges. Describe the methodology for conducting the survey. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I am not sure if my reasoning is correct:&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming the population prefers either apples or oranges (not both nor neither)&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Do a sample of the population, assign a value of 1 to apple, 0 to oranges (for example)&lt;/li&gt;&#10;&lt;li&gt;Take the sample mean and sample variance&lt;/li&gt;&#10;&lt;li&gt;Plot against the t-distribution&lt;/li&gt;&#10;&lt;li&gt;Set a confidence interval&lt;/li&gt;&#10;&lt;li&gt;Compare if the sample mean lies within the confidence interval of the t-value corresponding to 45% (apple).&lt;/li&gt;&#10;&lt;li&gt;If so, conclude that the statement is true, for the given confidence interval.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Sorry a bit lengthy, but thanks for the help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-19T16:02:12.573" Id="26771" LastActivityDate="2012-04-20T12:43:35.993" LastEditDate="2012-04-20T12:43:35.993" LastEditorUserId="88" OwnerUserId="10101" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;confidence-interval&gt;" Title="Applied statistics methodology" ViewCount="136" />
  
  
  <row AcceptedAnswerId="26821" AnswerCount="2" Body="&lt;p&gt;In “A Comparative Study of Nursing Home Residents …”, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/14706124&quot; rel=&quot;nofollow&quot;&gt;Aigner et al.&lt;/a&gt; write (p. 20)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Acute visits were significantly higher for the nurse practiticoner/physician team at 3.0 visits per year (± 2.4) versus 1.2 visits per year (± 1.5) for the physician-only group (P &amp;lt;0.0001)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Furthermore, (p. 19)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The statistical analysis was performed using the chi-squared or Fisher exact test for comparisons of percent and Student t test for comparison of means&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I always thought that if confidence intervals overlap by that much, differences can’t be significant. Am I wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Aigner, M. J., Drew, S., &amp;amp; Phipps, J. (2004). A comparative study of nursing home resident outcomes between care provided by nurse practitioners/physicians versus physicians only.  Journal of the American Medical Directors Association, 5(1), 16-23.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-20T10:23:42.040" Id="26817" LastActivityDate="2012-04-20T11:37:09.827" LastEditDate="2012-04-20T10:53:31.397" LastEditorUserId="8" OwnerUserId="5191" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;students-t&gt;" Title="Significance of difference in means" ViewCount="118" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have been comparing tails of various distributions for an application where we will need the conditional expectation of the tail or a truncated tail:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(X | a&amp;lt;X \leq b )= \frac{\int_a^b x f(x) dx}{F(b) - F(a)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;There is some limited information out there on the web [such as   &lt;a href=&quot;http://en.wikipedia.org/wiki/Truncated_normal_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Truncated_normal_distribution&lt;/a&gt; ], and I am capable of carrying out the integrals with difficulty, but ideally I would like to see worked-out formulas and approximations for the major continuous distributions. Is there a good reference work I am missing?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-20T16:59:42.680" Id="26837" LastActivityDate="2012-04-20T17:08:49.157" LastEditDate="2012-04-20T17:08:49.157" LastEditorUserId="7290" OwnerUserId="10775" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;truncation&gt;" Title="Where can I get reference formulas for truncated distributions?" ViewCount="66" />
&#10;\Pr(\Wo = w) = \Pr(\Wo &amp;gt; w - 1) - \Pr(\Wo &amp;gt; w) = (1-F(w-1))^n - (1-F(w))^n \&amp;gt;.
  
  
  
  <row Body="&lt;p&gt;In my field (analytical chemistry), &lt;code&gt;absolute error / absolute value&lt;/code&gt; = &lt;code&gt;relative error&lt;/code&gt;, so relative RMSE [at mean x] would be understood easily.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd clarify that the value I divide by is the average, as often the relative error at the extreme values is used:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;error specification of measuring instruments often is relative error at maximum value&lt;/li&gt;&#10;&lt;li&gt;in (chemical-analytical) calibration the relative error at the limit of quantitation or the lower limit of the actual calibration is important.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-04-21T10:39:44.120" Id="26877" LastActivityDate="2012-04-21T10:39:44.120" OwnerUserId="4598" ParentId="26863" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As far as I understand the linked info, GWR seems to be a special case and extension of weighted least squares (WLS) regression. With WLS the goal is to minimize variance, most often in order to create accurate predictions, i.e. bs. For ordinary WLS, a pseudoR2 can be calculated (Willett &amp;amp; Singer, 1988). Depending on the variability of the physiographic parameters, you might wish to consider segmented regression if there are clear breakpoints in the data. Given that GWR is somewhat like WLS, I assume that GWR leads to more accurate predictions than MVR. I guess it's easier to implement the MVR, so the answer depends on your resources. If there is only one parameter you would need to take into account (e.g. the one with the most variance), then try WLS. Just be careful on interpreting the meaning of R2.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-21T13:02:42.827" Id="26882" LastActivityDate="2012-04-21T13:02:42.827" OwnerUserId="10790" ParentId="26732" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="26887" AnswerCount="2" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nXT1s.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have one question about this. I know that if we have $\mathrm{X}_1,\mathrm{X}_2,\ldots,\mathrm{X}_n$ independent and normally distributed random variables, then the sum $\mathrm{X}_1+\mathrm{X}_2+\ldots+\mathrm{X}_n$ has the normal distribution with mean $M_1+M_2+..+M_n$ and variance $\sigma^2_1 + \ldots + \sigma^2_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is in this problem the difference $W-M$ the mean obtained by subtraction and variance obtained by addition? Thank you. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-21T14:29:48.650" FavoriteCount="1" Id="26886" LastActivityDate="2012-06-29T01:51:04.747" LastEditDate="2012-06-29T00:31:34.420" LastEditorUserId="4856" OwnerUserId="8288" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;variance&gt;" Title="Why is the variance of $X-Y$ equal to the sum of the variances when $X,Y$ are independent?" ViewCount="3451" />
  
  
  
  <row Body="&lt;p&gt;If $m_i=0$ then the likelihood $L_i$ of observing $0$ is $1$ and of observing anything else is $0$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This is an example of the wider convention where $0^0$ is taken to be $1$ so $\frac{0^0}{0!}\exp(-0)=1$  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-22T12:08:52.260" Id="26908" LastActivityDate="2012-04-22T12:08:52.260" OwnerUserId="2958" ParentId="26901" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If you do an initial analysis ignoring the minor variations you may get a estimate of the impact of RPM on the response. If this is reasonably smooth and plausible, you could then adjust your models for the implied impact of the minor variations, and if necessary rinse and repeat. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-22T14:59:33.543" Id="26912" LastActivityDate="2012-04-22T14:59:33.543" OwnerUserId="2958" ParentId="26906" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to find the variance of $L$, $Var(L)$, using the delta method (I want to find a closed form). $L$ is defined as:&#10;$$L = \frac{A}{B}  + \frac{C}{D}$$&#10;All $A$, $B$, $C$, and $D$ are dependent.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I'm not sure how to proceed. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-22T16:25:02.633" Id="26916" LastActivityDate="2012-04-22T21:29:23.833" LastEditDate="2012-04-22T21:06:22.783" LastEditorUserId="919" OwnerUserId="9136" PostTypeId="1" Score="1" Tags="&lt;delta-method&gt;" Title="Calculating the Variance using Delta Method" ViewCount="1298" />
  
  
&#10;= \sum_{1 \leq i,j \leq 4} \textrm{Cov}(X_i,X_j) \partial_i f(\mu) \partial_j f(\mu).$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-22T21:29:23.833" Id="26928" LastActivityDate="2012-04-22T21:29:23.833" OwnerUserId="368" ParentId="26916" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The MASS package has a function called mvrnorm() that can generate a group or random numbers to a specified level of correlation. An example of the setup can be found in the beginning of the example here: &lt;a href=&quot;http://menugget.blogspot.de/2011/11/propagation-of-error.html&quot; rel=&quot;nofollow&quot;&gt;http://menugget.blogspot.de/2011/11/propagation-of-error.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-23T12:45:25.507" Id="26960" LastActivityDate="2012-04-23T12:45:25.507" OwnerUserId="10675" ParentId="26958" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I have data that were collected at a number of sites, and each site was located within one of three zones (Lake Ontario, Erie and the St. Lawrence), so I was hoping to do nested ANOVAs to compare between sites and zones. Unfortunately I have an unequal number of sites in each zone (3, 4 and 5 respectively) due to not enough data being collected at a couple sites. Also, my variances and sample sizes are also not equal between sites (sample size running from 3 to 43, I know, terrible!). The total number of observations for all sites was 182. Most sites had around 15 observations&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, is it possible for me to do nested analysis of some sort? I can't find much information on nested analysis with unequal variances.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried transforming, the closest I can get to homoscedasticity is with $x_{new}=\frac{1}{(x+2)}$, and that gives me a $p \approx 0.02$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-23T17:10:46.437" FavoriteCount="1" Id="26974" LastActivityDate="2012-04-24T17:30:18.927" LastEditDate="2012-04-24T17:30:18.927" LastEditorUserId="10821" OwnerUserId="10821" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;multilevel-analysis&gt;&lt;heteroscedasticity&gt;" Title="I want to do an nested ANOVA but my variances are very unequal" ViewCount="1167" />
  
  <row Body="&lt;p&gt;This depends on the nature of the data. Your problem formulation suggests you should reconsider what you're trying to estimate. I'm confused. I don't know why you'd want to estimate email volume in a subset of persons that vary with time. You could use a Poisson regression model using offsets to estimate the individual rate of email return, assuming you've formulated this problem correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Turning to the problem of estimating a state-space model in the presence of missing data:&lt;/p&gt;&#10;&#10;&lt;p&gt;Using a regression modeling approach, you can estimate a lagged effects model by using any number of previous states as regressors in the model. The number of such states required depends on the stationary nature of the process. This can be estimated using cross validation in the modeling process, though. If X is a polytomous model, consider a multinomial regression model, if X outcomes are continuous, consider a linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having a regression modeling approach allows us to use methods for handling missing data. First off, you can ignore missing data (complete case analysis) and estimate a consistent, albeit inefficient, model. Multiple imputation and EM algorithms are alternate approaches which give modest improvements in efficiency. Note: in multiple imputation, one can use &lt;em&gt;future&lt;/em&gt; states to impute &lt;em&gt;past&lt;/em&gt; states. For an overview of missing data methods in regression modeling, consult &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471183865&quot; rel=&quot;nofollow&quot;&gt;Rubin, &quot;Statistical Analysis with Missing Data&quot;&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-23T20:12:07.673" Id="26986" LastActivityDate="2012-04-23T20:12:07.673" OwnerUserId="8013" ParentId="26972" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Does anyone understand how the argument &lt;strong&gt;&quot;span&quot;&lt;/strong&gt; is used in  &lt;strong&gt;spec.pgram&lt;/strong&gt; in &lt;strong&gt;R&lt;/strong&gt;? From the help menu, it provides the following examples:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;spectrum(ldeaths)&#10;spectrum(ldeaths, spans = c(3,5)) gives bandwidth= 0.241&#10;spectrum(ldeaths, spans = c(5,7)) gives bandwidth= 0.363&#10;spectrum(ldeaths, spans = 3)      gives bandwidth= 0.127&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What do &lt;strong&gt;c(3,5)&lt;/strong&gt; and &lt;strong&gt;c(3,7)&lt;/strong&gt; or &lt;strong&gt;spans=3&lt;/strong&gt; represent? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How the bandwidths are calculated?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-24T03:18:24.210" Id="27000" LastActivityDate="2012-04-24T06:55:21.463" OwnerUserId="10302" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;" Title="Clarification on using spec.pgram in R" ViewCount="194" />
  <row AnswerCount="0" Body="&lt;p&gt;The coefficient of variation is defined as the ratio of S.D to mean, a dimensionless number which on multiplication by 100 gives the percentage of dispersion with mean. But when we square this coefficient of variation, what does it signify?&lt;/p&gt;&#10;&#10;&lt;p&gt;(I have to understand a graph having squared coefficient of variation on y axis and mean on x axis for Poisson distributed data).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-04-24T06:58:36.173" FavoriteCount="1" Id="27007" LastActivityDate="2015-03-06T01:13:47.680" LastEditDate="2012-04-24T13:10:04.693" LastEditorUserId="919" OwnerUserId="10580" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;mean&gt;&lt;standard-deviation&gt;&lt;coefficient-of-variation&gt;" Title="What does a squared coefficient of variation signify?" ViewCount="2408" />
  <row Body="&lt;p&gt;Posterior predictive checks, outlined in Gelman et al (1996), are an obvious starting point.  Given how simple the model is, it probably makes sense to use graphical checks.  Plot the histogram of $(y_1,...,y_n)$ against histograms of several posterior predictive replications $(y_1^{rep},...,y_n^{rep})$.  If you spot a feature that doesn't fit, you can formalize things by defining an appropriate discrepancy statistic and computing the posterior predictive p-value of your model against that statistic. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-24T07:40:52.310" Id="27012" LastActivityDate="2012-04-24T07:40:52.310" OwnerUserId="493" ParentId="26970" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The reason that there is &quot;no follow up&quot; is that very few people understand the work of Rodriguez on this going back many years.  It's important stuff and we will see more of it in the future I am sure.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, some would argue that the Fisher metric is only a 2nd order approximation to the true metric (e.g. &lt;a href=&quot;http://www.mendeley.com/research/bayesian-inference-featuring-entropic-priors/&quot; rel=&quot;nofollow&quot;&gt;Neumann's paper on establishing entropic priors&lt;/a&gt; ) which is actually defined by the Kullback-Liebler distance (or generalisations thereof) and which leads to Zellner's formulation of MDI priors.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-24T10:02:06.190" Id="27019" LastActivityDate="2012-04-24T10:02:06.190" OwnerUserId="10838" ParentId="1621" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have to compute a similarity measure between different sets (Actually they are more like maps than sets). A weight is associated to each element of the set.&lt;/p&gt;&#10;&#10;&lt;p&gt;The sets I want to compare represent different journal or conferences. Each element of the set is an author and the associated weight is the number of papers published in that venue.&#10;The goal of my analysis is to discover whether there are similarities in the most prolific authors of different venues.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking to represent the content of the sets in a vector space (Using the elements as features and the weight as their value) and compute a cosine distance.&#10;Is it a good approach or there is something better I should use?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-24T12:30:07.193" Id="27025" LastActivityDate="2012-04-24T13:23:29.687" LastEditDate="2012-04-24T13:23:29.687" LastEditorUserId="10841" OwnerUserId="10841" PostTypeId="1" Score="2" Tags="&lt;similarities&gt;" Title="Similarity measure for weighted sets" ViewCount="137" />
  <row AcceptedAnswerId="27043" AnswerCount="1" Body="&lt;p&gt;I have a set of scores on a specific dataset given by human evaluators, let's call it &lt;code&gt;HJ&lt;/code&gt;. I also have the scores of two computer models on the &lt;strong&gt;same&lt;/strong&gt; dataset, let's call these sets &lt;code&gt;M1&lt;/code&gt; and &lt;code&gt;M2&lt;/code&gt;. I have calculated Spearman's coefficients for &lt;code&gt;(HJ,M1)&lt;/code&gt; and &lt;code&gt;(HJ,M2)&lt;/code&gt;, which show that &lt;code&gt;M2&lt;/code&gt; is closer to human judgements than &lt;code&gt;M1&lt;/code&gt;. Now I've been asked to find out if the &lt;strong&gt;difference&lt;/strong&gt; between these two rho coefficients is statistically significant. My statistical knowledge is limited, so any help on how I could do that would be highly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;In the book &quot;Applied Multiple Regression/Correlation Analysis for the Behavioural Sciences&quot; (Cohen and Cohen, 1975), I found the following $t$-statistic formula for cases identical to the one I describe above, but for &lt;em&gt;Pearson&lt;/em&gt; coefficients:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;t=\frac{(r_{XY}-r_{VY})\sqrt{(n-3)(1+r_{XV})}}{\sqrt{2(1-r^2_{XY}-r^2_{VY}-r^2_{XV}+2r_{XY} r_{XV} r_{VY})}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $r_{XY}$ is the correlation between $X$ and $Y$ and $n$ is the sample size. Is this also applicable to rank correlations? For a 2-tailed $t$-test it gives different results than the permutation test suggested by Greg below (in case I ask something trivial please excuse my ignorance).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-24T12:41:09.407" FavoriteCount="1" Id="27027" LastActivityDate="2013-04-09T21:28:42.297" LastEditDate="2013-04-09T21:28:42.297" LastEditorUserId="10840" OwnerUserId="10840" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;statistical-significance&gt;&lt;rank-correlation&gt;&lt;spearman-rho&gt;" Title="Calculating statistical significance between dependent Spearman coefficients" ViewCount="665" />
  <row AcceptedAnswerId="27077" AnswerCount="2" Body="&lt;p&gt;I am using constrOptim to minimize a log likelihood function for maximum likelihood estimation of parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to set the bounds on my parameters, but to not understand the constrOptim definition of the feasibility region.&lt;/p&gt;&#10;&#10;&lt;p&gt;The feasible region is defined by ui %*% theta - ci &gt;= 0&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a set of parameters with bounds [lower, upper]&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a[0,5]  ie 0&amp;lt;a&amp;lt;5&#10;b[0,Inf]&#10;c[0,Inf]&#10;e[0,1]&#10;&#10;theta (starting values) = c(1, 1, 0.01,0.1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What are the &lt;em&gt;ui&lt;/em&gt; (constraint matrix (k x p)) and &lt;em&gt;ci&lt;/em&gt; (constraint vector of length k) for these parameter bounds?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a straightforward way to get from a list of upper and lower bounds to a &lt;em&gt;ui&lt;/em&gt; and &lt;em&gt;ci&lt;/em&gt; value?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-24T13:52:10.310" Id="27030" LastActivityDate="2012-04-25T22:54:49.170" OwnerUserId="4843" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;maximum-likelihood&gt;&lt;optimization&gt;&lt;bounds&gt;" Title="How to set limits using constrOptim in R?" ViewCount="3351" />
  <row Body="&lt;p&gt;If you're asking whether you can construct a confidence interval for the &lt;em&gt;mean&lt;/em&gt; (or some other statistic) using a small sample from a large population then the answer is yes. In fact, the basic theory says that such a population is infinite in size. That is: you could repeatedly redraw mutually independent samples of size 8 from this population again and again. Estimating the standard deviation from the sample is common practice. Using the T-distribution's percentiles is the correct way to construct the confidence interval for your mean.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-24T15:05:32.793" Id="27036" LastActivityDate="2012-04-24T15:05:32.793" OwnerUserId="8013" ParentId="27034" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;You don't find the formula because it's an input.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;utility&quot; is whatever you are trying to maximize, so you can say that the utility function is part of the definition of the problem you are trying to solve. The utility could represent profit, customer share, fun, number of followers, whatever it is you're trying to maximize. The influence diagram is a tool to help you maximize it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-25T12:39:24.880" Id="27092" LastActivityDate="2012-04-25T12:39:24.880" OwnerUserId="666" ParentId="27086" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am struggling to find a solution on finding the distribution of following random variable:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y = Z \cdot  |X|$$&lt;/p&gt;&#10;&#10;&lt;p&gt;here, $Z$ is a random variable takes 1 or -1 with equal probability, and $X$ is a standard normal variate, and $|\cdot|$ denotes absolute value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody help me with some pointer from where I should start?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-04-25T14:16:37.210" FavoriteCount="1" Id="27097" LastActivityDate="2012-04-27T04:38:58.447" LastEditDate="2012-04-27T04:38:58.447" LastEditorUserId="4479" OwnerUserId="9914" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;" Title="What is the distribution of the product of a Bernoulli &amp; a normal random variable?" ViewCount="1602" />
  
  <row Body="&lt;p&gt;There is a section entitled 'Bivariate Half-normal distribution in:&#10;&lt;em&gt;Continuous Multivariate Distributions: Models and applications&lt;/em&gt;&#10;By Samuel Kotz, Norman Lloyd Johnson, N. Balakrishnan.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be curious to see how this can be generalized to a random vector of any dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, the bivariate case appears to be thoroughly treated in this paper:&#10;&lt;a href=&quot;http://www.stat-athens.aueb.gr/~jpan/papers/Panaretos-ApplStatScience2001(119-136)ft.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat-athens.aueb.gr/~jpan/papers/Panaretos-ApplStatScience2001(119-136)ft.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-25T16:55:27.230" Id="27108" LastActivityDate="2012-04-25T17:44:33.597" LastEditDate="2012-04-25T17:44:33.597" LastEditorUserId="919" OwnerUserId="10874" ParentId="21850" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;What is the exact definition of &lt;em&gt;precision&lt;/em&gt; of a statistical estimator? Any references to textbooks are appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-25T18:59:15.343" Id="27116" LastActivityDate="2012-04-26T15:01:49.330" OwnerUserId="10876" PostTypeId="1" Score="3" Tags="&lt;estimation&gt;" Title="Definition of &quot;precision&quot; of an estimator" ViewCount="807" />
  <row AnswerCount="2" Body="&lt;p&gt;This is a soft-question:&#10;I have been evaluation various regression techniques over a regression dataset that I have. I am surprised by the fact that cross-validated RMSE of Lasso is better than SVM and Random Forest in my case. Can this happen?- I mean, I believed that a non-linear modeling technique like random forest or SVM would do better than a linear model like Lasso. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is that really possible!?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-25T19:28:20.873" FavoriteCount="1" Id="27120" LastActivityDate="2013-01-13T14:57:44.020" LastEditDate="2012-04-26T14:03:30.737" LastEditorUserId="919" OwnerUserId="10877" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;predictive-models&gt;&lt;cross-validation&gt;&lt;lasso&gt;" Title="Why does Lasso do better than SVM?" ViewCount="1303" />
  <row Body="&lt;p&gt;In each iteration of your backpropagation algorithm, you will update the weights by multiplying the existing weight by a delta determined by backpropagation. If the initial weight value is 0, multiplying it by any value for delta won't change the weight which means each iteration has no effect on the weights you're trying to optimize.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-25T20:00:40.977" Id="27125" LastActivityDate="2012-04-27T16:17:34.257" LastEditDate="2012-04-27T16:17:34.257" LastEditorUserId="7282" OwnerUserId="7282" ParentId="27112" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The Pearson Correlation assumes several assumptions for it to be accurate: 1) Each variable is normally distributed; 2) Homoscedasticity, the variance of each variable remains constant; and 3) Linearity, meaning that a scatter plot depicting the relationship shows data points clustering symmetrically around the regression line. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Spearman Correlation is a nonparametric alternative to the Pearson one based on rank of the observations.  The Spearman Correlation allows you to relax all three assumptions about your data set and derive correlations that are still reasonably accurate.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What your data implies is that it probably breaks materially one or more of the mentioned assumptions materially so that the two correlations differ significantly.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Given that you have a large gap between the two correlation you should investigate whether the variables of your data set are normally distributed, homoscedastic, and linear within a scatter plot.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The above investigation will facilitate your decision on whether the Spearman or the Pearson correlation coefficient is the more representative one.    &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-25T20:30:45.833" Id="27129" LastActivityDate="2012-04-25T20:30:45.833" OwnerUserId="1329" ParentId="27127" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Unless it is common practice, I would avoid working with ratios as a response.  You already have a response rate, visits/per unit of time.    This is a typical count regression response-usually but not always fit with Poisson regression.  I would build a model with brood size and the other explanatory variables.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-25T20:48:34.587" Id="27131" LastActivityDate="2012-04-25T20:48:34.587" OwnerUserId="9322" ParentId="27016" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am interested in determining if the decisionmaking of a particular government body was responsive to policy and statutory changes that occurred at known points in time.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can classify the decisions of this body as 1) rejected proposal; 2) accepted modified proposal; 3) accepted proposal (without change).&lt;/p&gt;&#10;&#10;&lt;p&gt;I could classify the policy or statutory changes dichotomously -- &quot;with deregulation&quot; and &quot;without deregulation,&quot; for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;The intervention model discussed in this previous question - &lt;a href=&quot;http://stats.stackexchange.com/questions/19567/quantifying-effect-of-a-categorical-variable-in-time-series-analysis&quot;&gt;&quot;Quantifying effect of a categorical variable in time series analysis&quot;&lt;/a&gt; - seems highly relevant, but I think there are important differences between my dataset and the dataset described there:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;In the linked question, the OP had a continuous response variable.  In my case, I could take the categorical &quot;decision&quot; variable and convert it to a continuous &quot;rate accepted&quot; or &quot;rate rejected&quot; variable, BUT&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The timing of the response variable may not be as easily coded.  There is a lag between the time at which a decision was requested from the government body and the time at which the body made this decision, and this lag varies considerably from observation to observation.  Further, decisions generally occur in bunches (a phenomenon I can't readily explain) -- that is, a scheme of intervals used to measure &quot;rate accepted&quot; would have considerable variation in N from interval to interval.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In general, I'm interested in whether there's a modification to the intervention model described in the link above that I should investigate, or any other model that might be relevant?   A colleague suggested ARIMAX, of which I know nothing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any help you may be able to provide.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-25T22:29:05.080" Id="27138" LastActivityDate="2012-04-25T22:29:05.080" OwnerUserId="10507" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;intervention-analysis&gt;&lt;social-science&gt;" Title="Model to use in assessing the responsiveness of a categorical variable to policy changes over time?" ViewCount="115" />
  <row Body="&lt;p&gt;The impact of level shifts , seasonal pulses and local time trends ... in addition to one-time pulses. Changes in parameters over time are important to investigate/model. Possible changes in variance of the errors over time have to be investigated. How to determine how Y is impacted by contemporaneous and lagged values of X . How to identify if future values of X can impact current values of Y. How to find out of particular days of the month have an impact. How to model mixed frequency problems where hourly data is impacted by daily values ?&lt;/p&gt;&#10;&#10;&lt;p&gt;naught asked me to provide more specific information/examples on level shifts and pulses. To that end I now include some more discussion. A series that exhibits an ACF suggesting non-stationarity is in effect delivering a &quot;symptom&quot;. One suggested remedy is to &quot;difference&quot; the data. An overlooked remedy is to &quot;de-mean&quot; the data. If a series has a &quot;major&quot; level shift in the mean (i.e.intercept) the acf of this entire series can be easily misinterpreted to suggest differencing. I will show an example of a series that exhibits a level shift.If I had accentuated (enlarged) the difference between the two means the acf of the total series would suggest (incorrectly ! ) the need to difference. Untreated Pulses/Level Shifts/Seasonal Pulses/Local Time Trends inflate the variance of the errors obfuscating the importance of model structure and are the cause of flawed parameter estimates and poor forecasts. Now on to an example . Th&lt;img src=&quot;http://i.stack.imgur.com/7VzB4.jpg&quot; alt=&quot;enter image description here&quot;&gt;is is a list of the 27 monthly values. This is the graph &lt;img src=&quot;http://i.stack.imgur.com/TY2Pw.jpg&quot; alt=&quot;enter image description here&quot;&gt;. There are four pulses and 1 level shift AND NO TREND !&lt;img src=&quot;http://i.stack.imgur.com/qP2mz.jpg&quot; alt=&quot;enter image description here&quot;&gt; and &lt;img src=&quot;http://i.stack.imgur.com/CXDfs.jpg&quot; alt=&quot;enter image description here&quot;&gt; . The residuals from this model suggest a white noise process &lt;img src=&quot;http://i.stack.imgur.com/MkPCa.jpg&quot; alt=&quot;enter image description here&quot;&gt; . Some (most !) commercial and even free forecasting packages deliver the following silliness as a result of assuming a trend model with additive seasonal factors&lt;img src=&quot;http://i.stack.imgur.com/VfTtN.jpg&quot; alt=&quot;enter image description here&quot;&gt;. To conclude and to paraphrase Mark Twain. &quot;There is nonsense and there is nonsense but the most non sensical nonsence of them all is statistical nonsense !&quot; as compared to a more reasonable &lt;img src=&quot;http://i.stack.imgur.com/Qgg4X.jpg&quot; alt=&quot;enter image description here&quot;&gt; . Hope this helps !&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2012-04-26T07:10:09.067" CreationDate="2012-04-26T00:58:19.170" Id="27147" LastActivityDate="2012-04-28T18:42:26.017" LastEditDate="2012-04-28T18:42:26.017" LastEditorUserId="3382" OwnerUserId="3382" ParentId="27146" PostTypeId="2" Score="1" />
  <row Body="" CommentCount="0" CreationDate="2012-04-26T01:08:49.377" Id="27148" LastActivityDate="2012-04-26T01:08:49.377" LastEditDate="2012-04-26T01:08:49.377" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  
  <row Body="&lt;p&gt;With only 40 subjects, the error rate will be highly discretised and probably not a very good indication of performance.  The area under the ROC curve would be a better performance measure as it will be less discretised as it gives an indication of the accuracy of the ranking of the subjects (although of course it doesn't indicate how well the threshold is set).  There is a really good tutorial on ROC analysis by &lt;a href=&quot;http://home.comcast.net/~tom.fawcett/public_html/ROCCH/index.html&quot; rel=&quot;nofollow&quot;&gt;Fawcett&lt;/a&gt; that should be helpful.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-26T09:11:20.013" Id="27166" LastActivityDate="2012-04-26T09:11:20.013" OwnerUserId="887" ParentId="27165" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;In the attached images (bottom of this post), I wish to compare the plotted distributions as follows. Each plot shows two distributions labelled 'Forwards' and 'Backwards'. For each attached example, I wish to compare each of 'Forwards' and 'Backwards' of sub-figure a with 'Forwards' and 'Backwards' of sub-figure c (and same for sub-figures b and d)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now considering a and c from the distros A example, the distributions are approximately bimodal. I was thinking of taking the absolute values of these distributions so that they are no longer bimodal and then comparing them. (Probably I'm barking up the wrong tree with this?)&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am uncertain as to what test I should use to test for differences. I was initially inclining towards a ranksum test since the distributions are clearly non-uniform. But, I was also maybe considering a Kolmogorov-Smirnov Test. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Note about distributions: Each 'Forwards' and 'Backwards' data distribution consists of ~7000 and ~5000 data points respectively. Each data point within a given data set indicates the difference in value of a property specific to an artificial neural network. Hence all data points represent such measurements over collections (~7000 and ~5000) of neural networks.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yiesC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rToL2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2012-04-26T10:03:30.213" Id="27167" LastActivityDate="2012-08-31T08:50:43.060" LastEditDate="2012-08-31T08:50:43.060" LastEditorUserId="3826" OwnerUserId="10893" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="What statistical tests should be applied to the attached distributions?" ViewCount="169" />
  
  <row AcceptedAnswerId="27181" AnswerCount="1" Body="&lt;p&gt;I posted this on stackoverflow.com yesterday and its had very few views.  I came across this stackexchange site and thought its got to be worth an ask:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have an interesting problem and I'm sure there is an elegant algorithm with which to solve the solution but I'm having trouble describing is succinctly which would help finding such an algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a symmetric matrix of comparison values e.g:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;-104.2732   -180.3972   -130.6969   -160.8333   -141.5499   -139.2758   -144.7697   -114.0545   -117.6409   -140.1391&#10;-180.3972   -93.05421   -171.618    -162.0157   -156.8562   -156.3221   -159.9527   -163.2649   -170.127    -153.2709&#10;-130.6969   -171.618    -101.1591   -154.4978   -143.6272   -116.3477   -137.2391   -125.5645   -128.9505   -131.6046&#10;-160.8333   -162.0157   -154.4978   -96.96312   -122.7894   -141.5103   -127.7861   -149.6883   -153.0445   -130.2555&#10;-141.5499   -156.8562   -143.6272   -122.7894   -101.7487   -141.451    -123.9087   -138.7041   -139.2517   -125.3494&#10;-139.2758   -156.3221   -116.3477   -141.5103   -141.451    -99.99486   -134.6553   -132.7735   -138.7249   -134.1319&#10;-144.7697   -159.9527   -137.2391   -127.7861   -123.9087   -134.6553   -100.0683   -141.3492   -138.0292   -120.5331&#10;-114.0545   -163.2649   -125.5645   -149.6883   -138.7041   -132.7735   -141.3492   -106.8555   -115.58 -139.3355&#10;-117.6409   -170.127    -128.9505   -153.0445   -139.2517   -138.7249   -138.0292   -115.58 -104.9484   -140.4741&#10;-140.1391   -153.2709   -131.6046   -130.2555   -125.3494   -134.1319   -120.5331   -139.3355   -140.4741   -101.3919&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The diagonal will always show the maximum score (as it is a self-to-self comparison).  However I know that of these values some of them represent the same item.  Taking a quick look at the matrix I can see (and have confirmed manually) that items 0, 7 &amp;amp; 8 as well as 2 &amp;amp; 5 and 3, 4, 6 &amp;amp; 9 all identify the same item.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now what I'd like to do is find an elegant solution as to how I would cluster these together to produce me 4 clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know of such an algorithm?  Any help would be much appreciated as I seem so close to a solution to my problem but am tripping at this one last stumbling block :(&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-04-26T14:05:38.613" FavoriteCount="2" Id="27175" LastActivityDate="2012-04-26T16:39:00.637" OwnerUserId="10897" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;matrix&gt;" Title="Clustering similar values in a matrix" ViewCount="229" />
  <row Body="&lt;p&gt;Defining Trend as a &lt;strong&gt;Linear&lt;/strong&gt; growth over time . &lt;/p&gt;&#10;&#10;&lt;p&gt;Although some trends are somehow linear (see Apple stock price), and although time series chart looks like a line chart where you can find linear regression, &lt;strong&gt;most&lt;/strong&gt; trends are not linear. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are &lt;strong&gt;Step&lt;/strong&gt; changes like changes when something happened in a specific point in time that changed the measure behavior (&lt;em&gt;&quot;The bridge collapsed and no cars is going over it since&lt;/em&gt;&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Another popular trend is &lt;strong&gt;&quot;Buzz&quot;&lt;/strong&gt; - exponential growth and a similar sharp decline afterward (&lt;em&gt;&quot;Our marketing campaign was a huge success, but the effect faded after couple of weeks&quot;&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Knowing the right model (Logistic Regression, etc.) of the trend in the time series is crucial in the ability to detect it in the time series data.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-04-26T18:59:18.743" CreationDate="2012-04-26T18:59:18.743" Id="27187" LastActivityDate="2012-04-26T18:59:18.743" OwnerUserId="9004" ParentId="27146" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="27207" AnswerCount="1" Body="&lt;p&gt;I would like to test several (multivariate --between 5 and 20 dimensions) depth measures empirically. I'm looking for a couple of test-benches (read distributions). These should be:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;non-elliptical, &lt;/li&gt;&#10;&lt;li&gt;absolutely continuous,&lt;/li&gt;&#10;&lt;li&gt;uni-modal &lt;/li&gt;&#10;&lt;li&gt;have convex contours (if you prefer, level sets)&lt;/li&gt;&#10;&lt;li&gt;have non independent components. &lt;/li&gt;&#10;&lt;li&gt;Easy to implement (i.e. implemented in R or source code available on-line). &lt;/li&gt;&#10;&lt;li&gt;defined in arbitrary dimensions &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-26T23:02:40.970" Id="27202" LastActivityDate="2012-04-28T08:19:45.557" LastEditDate="2012-04-28T08:19:45.557" LastEditorUserId="603" OwnerUserId="603" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;&lt;simulation&gt;" Title="Looking for example of non-elliptical, multivariate, absolutely continuous distributions with non independent components" ViewCount="167" />
  <row AnswerCount="1" Body="&lt;p&gt;In the middle of an argument that I will present to my students I'll have to prove that $E[X^2]\geq E^2[X]$, but I don't want to use Jensen's inequality to do so. Is there any elementary way to go?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-27T03:17:53.177" Id="27208" LastActivityDate="2012-04-27T03:21:54.643" LastEditDate="2012-04-27T03:21:54.643" LastEditorUserId="7290" OwnerUserId="10904" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;proof&gt;" Title="Proof without Jensen" ViewCount="130" />
  <row AnswerCount="3" Body="&lt;p&gt;One standard model used with panel data is fixed effects: $y_{it} = \mu_i + \theta_t + \epsilon_{it}$, where $i$ is the individual and $t$ is time subscripts. This can be estimated easily with OLS and dummy variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;The model assumes that there is a single underlying time series, $\theta_t$ for $t = 1,2,...,T$. All individuals in the data are assumed to follow this time series plus some individual effect, which is constant relative to time. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose, however, that there are two or more groups of individuals, that each group has its own time series, and that, before looking at the data, we don't know who is in which group. I would like to &lt;strong&gt;estimate the fixed effects model in this case and to figure out which individual is in which group / cluster&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The approach that I thought of is that the model should become: $y_{it} = \mu_i + \sum_j \pi_{ij} \theta_{jt} + \epsilon_{it}$. $j$ indicates the group $1,2,...,J$. $\pi_{ij}$ is the probability that individual $i$ is in group $j$. $\theta_{jt}$ is the time effect for group $j$ at time $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my particular application, $\{\pi_{ij}\}$ needs to be a set of additional parameters. However, I can see that in other applications, it could be modeled as a function of some covariates. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this model a good approach? Has anyone tried it before? How do I estimate this model? The model has a lot of parameters. I've tried basic optimization, and it has not worked. &lt;strong&gt;Ideally, I am looking for software to do the estimation, such as a package in R; or, a solid reference that I could use to program this.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;If the model that I've proposed is not a good approach, how else could I solve this problem? One possibility that I could see is first figuring out which individual belongs to which cluster, and then estimating a regular fixed effects model on each cluster. The issue then is how to perform the data classification. Regardless of the approach, I am still looking either for software or a good reference.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-04-27T03:46:13.513" FavoriteCount="1" Id="27210" LastActivityDate="2012-05-06T23:13:58.297" LastEditDate="2012-04-30T17:58:15.470" LastEditorUserId="7169" OwnerUserId="7169" PostTypeId="1" Score="4" Tags="&lt;clustering&gt;&lt;mixed-model&gt;&lt;panel-data&gt;&lt;longitudinal&gt;&lt;fixed-effects-model&gt;" Title="Mixture model fixed effects" ViewCount="393" />
  
  
  <row Body="&lt;p&gt;I think I have figured it out for the one sample case after updating my knowledge with &lt;a href=&quot;http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/&quot; rel=&quot;nofollow&quot;&gt;Doing Bayesian Data Analysis&lt;/a&gt; and the rjags code contained within. The model specification I've used below gives me a slope of ~ 1 and intercept of ~ 0 when the BIC Bayes factor approximation is regressed against the Bayes factor from the rjags model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model {&#10;# Likelihood:&#10;  for( i in 1 : N ) {&#10;    y[i] ~ dnorm( mu , tau ) # tau is precision, not SD&#10;  }&#10;# Prior:&#10;  tau ~ dnorm( yprec , (yprec^2)/2 )&#10;  mu ~ dnorm( amu , atau )&#10;#Hyperprior&#10;  atau &amp;lt;- tauModel[ modelIndex ]&#10;  amu  &amp;lt;- muModel[ modelIndex ]&#10;  tauModel[1] &amp;lt;- 10000&#10;  tauModel[2] &amp;lt;- yprec&#10;  muModel[1] &amp;lt;- 0&#10;  muModel[2] &amp;lt;- ymean   &#10;&#10;#Hyperhyperprior&#10;  modelIndex ~ dcat( modelProb[] )&#10;  modelProb[1] &amp;lt;- 0.5&#10;  modelProb[2] &amp;lt;- 0.5&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And the input data:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = rnorm( n=200 , mean=1 , sd=1 )&#10;yprec = 1/var(y)&#10;ymean = mean(y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And for linear regression:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model {&#10;  for( i in 1 : Ndata ) {&#10;    y[i] ~ dnorm( mu[i] , tau )&#10;    mu[i] &amp;lt;- beta0 + beta1 * x[i]&#10;  }&#10;  beta0 ~ dnorm( b0mu , b0tau )&#10;  beta1 ~ dnorm( b1mu , b1tau )&#10;  tau ~ dgamma( yrprec , (yrprec^2)/2 )&#10;&#10;#Hyperprior&#10;  b0tau &amp;lt;- b0tauModel[ modelIndex ]&#10;  b0mu  &amp;lt;- b0muModel[ modelIndex ]&#10;  b1tau &amp;lt;- b1tauModel[ modelIndex ]&#10;  b1mu  &amp;lt;- b1muModel[ modelIndex ]&#10;&#10;  b0muModel[1] &amp;lt;- zymean&#10;  b0tauModel[1] &amp;lt;- yrprec&#10;  b1muModel[1] &amp;lt;- 0&#10;  b1tauModel[1] &amp;lt;- 10000 &#10;&#10;  b0muModel[2] &amp;lt;- xbeta0&#10;  b0tauModel[2] &amp;lt;- yrprec&#10;  b1muModel[2] &amp;lt;- xbeta1&#10;  b1tauModel[2] &amp;lt;- yrprec&#10;&#10;#Hyperhyperprior&#10;  modelIndex ~ dcat( modelProb[] )&#10;  modelProb[1] &amp;lt;- 0.5&#10;  modelProb[2] &amp;lt;- 0.5&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Regression input data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rnorm(50, sd=30) + 1:50  ;  y &amp;lt;- rnorm(50, sd=30) + 1:50&#10;nSubj &amp;lt;- length(x)&#10;xM = mean( x ) ; xSD = sd( x ); yM = mean( y ) ; ySD = sd( y )&#10;zx = ( x - xM ) / xSD         ; zy = ( y - yM ) / ySD&#10;#&#10;lm1 &amp;lt;- lm(zy ~ zx); slm1 &amp;lt;- summary(lm1)&#10;yrprec &amp;lt;- 1/var(resid(lm1))&#10;zymean &amp;lt;- mean(zy)&#10;xbeta0 &amp;lt;- coef(slm1)[1,1]      ; xbeta1 &amp;lt;- coef(slm1)[2,1]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Framework for these models is from &lt;a href=&quot;http://doingbayesiandataanalysis.blogspot.com.au/&quot; rel=&quot;nofollow&quot;&gt;Kruschke&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-27T08:34:19.957" Id="27226" LastActivityDate="2012-05-10T05:50:58.743" LastEditDate="2012-05-10T05:50:58.743" LastEditorUserId="966" OwnerUserId="966" ParentId="26614" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working on time series prediction. I have two data sets $D1=\{x_1, x_2,....x_n\}$ and $D2=\{x_n+1, x_n+2, x_n+3,...., x_n+k\}$. I have three prediction models: $M1, M2, M3$. All of those model are trained using samples in data set $D1$, and their performance is measured using the samples in data set $D2$. Let say the performance metrics is MSE (or anything else). The MSE of those models when measured for data set $D2$ are $MSE_1, MSE_2,  $ and $MSE_3$. How can I test that improvement of one model over another is statistically significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, let say $MSE_1=200$, $MSE_2=205$, $MSE_3=210$, and total number of sample in data set $D2$ based upon which those MSE are calculated is 2000. How can I test that $MSE_1$, $MSE_2$, and $MSE_3$ are significantly different. I would greatly appreciate if anyone can help me in this problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-27T09:37:26.820" Id="27228" LastActivityDate="2012-04-27T15:48:46.117" LastEditDate="2012-04-27T15:48:46.117" LastEditorUserId="9651" OwnerUserId="10037" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;statistical-significance&gt;&lt;classification&gt;&lt;performance&gt;" Title="How to compare the accuracy of two different models using statistical significance" ViewCount="203" />
  
  <row Body="&lt;p&gt;For the US:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://research.stlouisfed.org/fred2/&quot;&gt;FRED: Federal Reserve Economic Data&lt;/a&gt; (the best) &lt;br&gt;&#10;&lt;a href=&quot;http://www.bls.gov/&quot;&gt;Bureau of Labor Statistics&lt;/a&gt; &lt;br&gt;&#10;&lt;a href=&quot;http://bea.gov/&quot;&gt;Bureau of Economic Analysis&lt;/a&gt; &lt;br&gt;&#10;&lt;a href=&quot;http://www.census.gov/&quot;&gt;U.S. Census&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-07-30T20:09:14.610" CreationDate="2011-10-15T17:27:28.020" Id="27241" LastActivityDate="2011-10-15T17:27:28.020" OwnerDisplayName="user975904" OwnerUserId="6793" ParentId="27237" PostTypeId="2" Score="11" />
  
  
  <row Body="&lt;p&gt;There is no particular reason why this should &lt;em&gt;not&lt;/em&gt; happen. Multiple regression asks a different question from simple regression. In particular, multiple regression (in this case, multiple logistic regression) asks about the relationship between the dependent variables and the independent variables, controlling for the other independent variables. Simple regression asks about the relationship between a dependent variable and a (single) independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you add the context of your study (e.g. what are these variables?) it may be possible to give more specific responses. Also, given that all three variables in your case are dichotomies, you could present us with the data pretty easily....there are only 8 lines needed to summarize this:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{table}&#10;DV   IV1    IV2    Count&#10;A     A      A      10&#10;A     A      B      20&#10;\end{table}&lt;/p&gt;&#10;&#10;&lt;p&gt;etc.      &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-27T16:51:45.620" Id="27260" LastActivityDate="2012-04-27T16:51:45.620" OwnerUserId="686" ParentId="27257" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Homework?&lt;/p&gt;&#10;&#10;&lt;p&gt;Hint:&lt;/p&gt;&#10;&#10;&lt;p&gt;Remember the binomial theorem:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;
  <row Body="&lt;p&gt;You could do a simple t test of the means of the two periods. Be sure that there is not some kind of upward or downward trend in the data over the two years, because this can lead to the wrong conclusions of the t test value.&lt;/p&gt;&#10;&#10;&lt;p&gt;you may need to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Welch%27s_t_test&quot; rel=&quot;nofollow&quot;&gt;Welch's t test&lt;/a&gt;(R uses this as standard) which is a modification of the standard &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-test&quot; rel=&quot;nofollow&quot;&gt;Student's t test&lt;/a&gt; to take into account different variance and sample sizes. It should be a one sided alternative hypothesis since you are testing of whether with period_after_change is less than period_before_change.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the simplest test of whether the change has had a negative or no effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can calculate the t test score in excel using Welch's t test (&lt;a href=&quot;http://answers.yahoo.com/question/index?qid=20091026092453AABP31E&quot; rel=&quot;nofollow&quot;&gt;googles first hit&lt;/a&gt;)&#10;if the test is significant then you have done something wrong and the drop can be explained by the relaunch(if we only look at that as an explanatory variable). if it is not significant then you have done nothing wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;this is the statistically method, another side of this is to explain it to the client... I won't dive in to this here ... ;)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-27T18:53:08.947" Id="27270" LastActivityDate="2012-04-27T18:53:08.947" OwnerUserId="8453" ParentId="27254" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="27398" AnswerCount="2" Body="&lt;p&gt;I'm looking for an efficient algorithm to detect Tomek links. I'm wondering if anyone knows where to find it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This is the definition of Tomek links:&#10;Suppose $\{ E_1,\ldots,E_n\} \subset R^k$ is a dataset, with each $E_i$ having exactly one of two labels  $+$ or $-$. A pair $(E_i,E_j)$ is called a Tomek link if $E_i$ and $E_j$ have different labels, and there is not an $E_l$ such that $d(E_i,E_l) &amp;lt; d(E_i,E_j)$ or $d(E_j,E_l) &amp;lt; d(E_i,E_j)$, where $d(x,y)$ is the distance between $x$ and $y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-27T21:25:46.420" FavoriteCount="1" Id="27276" LastActivityDate="2013-09-19T09:14:07.260" LastEditDate="2012-04-30T04:41:02.400" LastEditorUserId="5264" OwnerUserId="5264" PostTypeId="1" Score="3" Tags="&lt;data-mining&gt;&lt;data-cleaning&gt;" Title="Looking for an efficient algorithm to detect Tomek links" ViewCount="548" />
  
  
  <row Body="&lt;p&gt;Interesting question. A possibility that comes to my mind is including an additional parameter $p\in[0,1]$ in order to control the upper bound of the 'link' function. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\{{\bf x}_j,y_j,n_j\}$, $j=1,...,n$ be independent observations, where $y_j\sim \text{Binomial}\{n_i,pF({\bf x}_j^T\beta)\}$, $p\in[0,1]$, ${\bf x}_j=(1,x_{j1},...,x_{jk})^T$ is a vector of explanatory variables, $\beta=(\beta_0,...,\beta_k)$ is a vector of regression coefficients and $F^{-1}$ is the link function. Then the likelihood function is given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\mathcal L}(\beta,p) \propto \prod_{j=1}^n p^{y_j}F({\bf x}_j^T\beta)^{y_j}[1-pF({\bf x}_j^T\beta)]^{n_j-y_j}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The next step is to choose a link, say the logistic distribution and find the corresponding MLE of $(\beta,p)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following simulated toy example using a dose-response model with $(\beta_0,\beta_1,p)=(0.5,0.5,0.25)$ and $n=31$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dose = seq(-15,15,1)&#10;a = 0.5&#10;b = 0.5&#10;n=length(dose)&#10;sim = rep(0,n)&#10;for(i in 1:n) sim[i] = rbinom(1,100,0.25*plogis(a+b*dose[i]))&#10;&#10;plot(dose,sim/100)&#10;&#10;lp = function(par){&#10;if(par[3]&amp;gt;0&amp;amp; par[3]&amp;lt;1) return(-(n*mean(sim)*log(par[3]) +  sum(sim*log(plogis(par[1]+par[2]*dose)))  + sum((100-sim)*log(1-par[3]*plogis(par[1]+par[2]*dose))) ))&#10;else return(-Inf)&#10;}&#10;&#10;optim(c(0.5,0.5,0.25),lp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;One of the outcomes I got is $(\hat\beta_0,\hat\beta_1,\hat p)=( 0.4526650, 0.4589112, 0.2395564)$. Therefore it seems to be accurate. Of course, a more detailed exploration of this model would be necessary because including parameters in a binary regression model can be tricky and problems of identifiability or existence of the MLE may jump on the stage &lt;a href=&quot;http://biomet.oxfordjournals.org/content/71/1/1.abstract&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;http://www.jstor.org/stable/10.2307/2290139&quot; rel=&quot;nofollow&quot;&gt;2&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the edit (which changes the problem significantly), the method I proposed previously can be modified for fitting the data you have provided. Consider the model &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mbox{accuracy} = pF(x;\mu,\sigma),$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $F$ is the logistic CDF, $\mu$ is a location parameter, $\sigma$ is a scale parameter, and the parameter $p$ controls the height of the curve similarly as in the former model. This model can be fitted using &lt;a href=&quot;http://en.wikipedia.org/wiki/Non-linear_least_squares&quot; rel=&quot;nofollow&quot;&gt;Nonlinear Least Squares&lt;/a&gt;. The following R code shows how to do this for your data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rm(list=ls())&#10;y = c(0,0,0,0,0,1,3,5,9,13,14,15,14,15,16,15,14,14,15)/100&#10;x = 1:length(y)&#10;N = length(y)&#10;&#10;plot(y ~ x)&#10;&#10;Data = data.frame(x,y)&#10;&#10;nls_fit = nls(y ~ p*plogis(x,m,s), Data, start = list(m = 10, s = 1,  p = 0.2) )&#10;&#10;lines(Data$x, predict(nls_fit), col = &quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-04-28T12:18:20.290" Id="27291" LastActivityDate="2012-10-08T18:45:14.543" LastEditDate="2012-10-08T18:45:14.543" LastEditorDisplayName="user10525" OwnerDisplayName="user10525" ParentId="27283" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;In the text mining, besides N-gram model, what are the state-of-art models for building feature space while capturing the dependence among the different words, or capturing the semantic meaning in the article?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-28T12:46:26.930" Id="27292" LastActivityDate="2015-01-16T07:37:54.980" LastEditDate="2012-04-29T10:40:35.737" LastEditorUserId="88" OwnerUserId="3125" PostTypeId="1" Score="2" Tags="&lt;text-mining&gt;&lt;feature-selection&gt;&lt;natural-language&gt;" Title="Feature construction for text mining" ViewCount="138" />
  <row AcceptedAnswerId="27317" AnswerCount="2" Body="&lt;p&gt;Is there such a thing as a fair die? On dice where the number is represented by a scooped out dot, surely that makes a difference? Has anyone done any research?&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact thinking about it, why would a coin flip be fair? the physics on each side is completely different.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-04-28T13:37:34.733" FavoriteCount="2" Id="27294" LastActivityDate="2012-04-29T10:39:58.483" LastEditDate="2012-04-29T10:39:58.483" LastEditorUserId="88" OwnerUserId="10939" PostTypeId="1" Score="11" Tags="&lt;dice&gt;" Title="Is there such a thing as a fair die?" ViewCount="312" />
  
  
&#10;   0 &amp;amp; \text{if } \eta(x)\leq\dfrac{1}{2} \\
  <row AcceptedAnswerId="27727" AnswerCount="2" Body="&lt;p&gt;I am learning Bayesian statistics for a data mining module. We were given a set of practice questions but no answers and I want to make sure that I am going about Bayes in the right way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose Manchester United were about to play Arsenal in the Premiership and you assess the probability&#10;of Manchester United winning to be 0.7. You also feel that if they did win, there is a probability of&#10;0.9 that your local pub will be packed with fans celebrating their club’s victory. Alternatively, if they&#10;loose, you believe that there is still a probability of 0.6 that the pub will be packed with fans albeit for&#10;drowning their sorrows. As someone not interested in football, you enjoy a quiet day at work and on&#10;the way home, notice a big crowd in the pub. Can you assume that Manchster United won the game?&#10;If so, what is the probability of them having won the game?&lt;/p&gt;&#10;&#10;&lt;p&gt;Attempted solution:&lt;/p&gt;&#10;&#10;&lt;p&gt;let $p(w)$ denote prob of manchester united winning&#10;let $p(f)$ denote prob of pub being full&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(w|f)=\frac{p(f|w)*p(w)}{p(f)}$ ,&#10;where $p(f)=p(f|w)*p(w)+p(f|&amp;#172;w)*p(&amp;#172;w)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(f)= 0.9*0.7+0.6*0.3=0.81$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\rightarrow$$p(w|f)=\frac{0.9*0.7}{0.81}=0.78$&lt;/p&gt;&#10;&#10;&lt;p&gt;So prob of Manchester untied having won is 0.78.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think we can assume they won just because there were people in the pub  as there was a 0.6 prob of them drowning their sorrows.&lt;/p&gt;&#10;&#10;&lt;p&gt;Have I got this right or have I misunderstood Bayesian statistics?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks very much&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-29T20:17:56.817" Id="27356" LastActivityDate="2012-05-04T02:50:46.573" LastEditDate="2012-04-30T17:23:48.213" LastEditorUserId="9651" OwnerUserId="10960" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;self-study&gt;" Title="Bayesian statistics example" ViewCount="531" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm not sure how this would look.  It seems you've have to specify regime breakpoints within the data, right?  Or is there some rolling method that could compare window sizes against each other?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also key, is it possible to do this without specifying a model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-29T23:15:38.077" Id="27363" LastActivityDate="2012-04-30T03:13:48.907" OwnerUserId="7420" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;hypothesis-testing&gt;&lt;modeling&gt;&lt;nonparametric&gt;" Title="Is there a non-parametric test for whether a series' unconditional variance has changed over time?" ViewCount="97" />
  <row Body="&lt;p&gt;If you're fitting anything like a regression, ARMA, etc., then you might consider the LM test described in &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/030440769290104Y&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; (&lt;a href=&quot;http://www.ccee.edu.uy/ensenian/catmetec/material/KPSS.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf link&lt;/a&gt;) [1]. It tests for stationarity, which is stronger than just testing for changes in variance, but perhaps you can modify for your purposes, or use that to go looking for more applicable tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;Along the same lines, you could set up a prior distribution over the value of $\sigma^{2}_{u}$ (in the paper's notation for the random walk variance) and use your regression/ARMA/whatever to obtain a likelihood model for $P(y_{t}|\sigma^{2}_{u})$, and then use simulations to draw lots of samples from $P(\sigma^{2}_{u}|y_{t})$. Then you can use posterior predictive checking and Bayesian p-values to test whether $\sigma^{2}_{u}$ is meaningfully different than 0. If not, you have reason to suspect the variance is not changing.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know this isn't spelled out too carefully, and there may be more &quot;ready-to-use&quot; statistical tests, but hopefully it gives you some ideas.&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Kwiatkowski et al, &lt;em&gt;Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?&lt;/em&gt;, Journal of Econometrics, Vol. 54 Issues 1-3. 1992. DOI:  10.1016/0304-4076(92)90104-Y&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-30T01:21:38.813" Id="27368" LastActivityDate="2012-04-30T03:13:48.907" LastEditDate="2012-04-30T03:13:48.907" LastEditorUserId="8927" OwnerUserId="8927" ParentId="27363" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;Please give me mathematical explanation if possible. And also in the book Kothari 2004, it says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There is also not as much information in ‘n’ observations within a cluster as there happens to be in ‘n’ randomly drawn observations.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Can you also give me mathematical explanation for this?&lt;/p&gt;&#10;&#10;&lt;p&gt;My second question is why is simple random sampling preferable to cluster sampling? I'm interested in the mathematical proof of betterness of the former in terms of randomness.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-04-30T08:05:00.927" Id="27378" LastActivityDate="2013-03-07T16:14:54.337" LastEditDate="2013-03-07T16:14:54.337" LastEditorUserId="5739" OwnerUserId="10971" PostTypeId="1" Score="5" Tags="&lt;sampling&gt;&lt;cluster-sample&gt;" Title="Why are samples within a cluster less informative than randomly chosen ones from entire population?" ViewCount="379" />
&#10;&amp;amp;\leq \mathbb{E}\left\{\mathbf{I}_{\{\eta(X)\ne1/2\}}|\eta(X)-\tilde\eta(X)|\mathbf{I}_{\{g(X)\ne g^*(X)\}}\right\}\\
&#10;  \end{cases}$$&#10;and finally, $g(x)$ is defined like $g^*(x)$ with $\tilde\eta(x)$ replacing $\eta(x)$. $\epsilon&amp;gt;0$ is fixed. $\mathbf{I}_A$ is the indicator function of the set $A$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-30T13:16:13.907" Id="27389" LastActivityDate="2012-04-30T17:26:40.603" LastEditDate="2012-04-30T17:26:40.603" LastEditorUserId="7603" OwnerUserId="7603" PostTypeId="1" Score="2" Tags="&lt;probability-inequalities&gt;" Title="An instance of the Cauchy–Schwarz inequality" ViewCount="298" />
  
  
  <row Body="&lt;p&gt;This type of model has been entertained by education researchers for years under the name of &lt;a href=&quot;http://www.google.com/search?q=latent+growth+mixture+model&quot;&gt;growth mixture model&lt;/a&gt; (do different students exhibit different rate of learning?), although they work with it as a random effects model. I don't think you'd be able to come up with a proper fixed effect estimation for this model, as it may lack the sufficient statistic that you could condition on. But a full blown ML model should not be extremely difficult to fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;In Stata, you could try &lt;code&gt;fmm&lt;/code&gt; with a full set of panel id dummies and/or interactions with time, although this is a very wasteful approach in terms of degrees of freedom.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-04-30T14:44:28.563" Id="27397" LastActivityDate="2012-04-30T14:44:28.563" OwnerUserId="5739" ParentId="27210" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;The answer is no. It is said that there must be some particular linear combination of $I(1)$ variables, but there may not exist one. In that case we have so called spurious regression, you may find an example &lt;a href=&quot;http://faculty.washington.edu/ezivot/econ584/notes/cointegration.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. We &lt;a href=&quot;http://en.wikipedia.org/wiki/Cointegration&quot; rel=&quot;nofollow&quot;&gt;say&lt;/a&gt; that two or more time series are cointegrated if they share a common stochastic drift, so it is possible for variables to be $I(1)$ and to have different stochastic drifts.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-30T22:34:12.993" Id="27422" LastActivityDate="2012-04-30T22:34:12.993" OwnerUserId="10172" ParentId="27414" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="27427" AnswerCount="1" Body="&lt;p&gt;Have, let's say, the following data:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;[1] 8232302  684531  116857   89724   82267   75988   63871   23718    1696     436     439     &gt;[12] 248 235&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Want a simple way to fit this (and several other datasets) to a Pareto distribution. Ideally it would output the matching theoretical values, less ideally the parameters. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-04-30T23:37:20.050" FavoriteCount="1" Id="27426" LastActivityDate="2012-05-26T19:27:18.763" OwnerUserId="9680" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;pareto-distribution&gt;" Title="How do I fit a set of data to a Pareto distribution in R?" ViewCount="6132" />
  
&#10;&amp;amp;= -\frac{1}{2} \left(
&#10;}{\partial {\boldsymbol \mu}} \right) \nonumber \\ 
  <row Body="&lt;p&gt;The interesting and useful property of the Normal is that it's what we get, very generally, when we average lots of measurements. Thus, the height of a randomly-selected individual isn't Normal, as you note - and their blood pressure, shoe size and number of years of education are also non-Normal - but when we take an average of a sample of these measurements, on different people, their average behaves very like a Normal - and more Normal when we average more measurements. In this sense, yes, the Normal is very like reality, in all sorts of applications.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, it's not just the average that looks Normal, all sorts of manipulations of averages look Normal too - and this is fundamentally what makes regression work. In fact for many statistical purposes, Normality of data is basically irrelevant; what matters is that it's what you get when averaging lots of similarly-behaved variables. It is a tremendously useful property.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-01T04:33:02.583" Id="27439" LastActivityDate="2012-05-01T04:33:02.583" OwnerUserId="7497" ParentId="27430" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="27445" AnswerCount="1" Body="&lt;p&gt;I was wondering how you would generate data from a Poisson regression equation in R? I'm kind of confused how to approach the problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;So if I assume we have two predictors $X_1$ and $X_2$ which are distributed $N(0,1)$. And the intercept is 0 and both of the coefficients equal 1. Then my estimate is simply:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log(Y) = 0+ 1\cdot X_1 + 1\cdot X_2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But once I have calculated log(Y) - how do I generate poisson counts based on that? What is the rate parameter for the Poisson distribution? &lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone could write a brief R script that generates Poisson regression samples that would be awesome!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-01T05:52:28.880" FavoriteCount="3" Id="27443" LastActivityDate="2012-05-01T09:21:31.413" LastEditDate="2012-05-01T09:21:31.413" LastEditorUserId="88" OwnerUserId="5464" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;regression&gt;&lt;poisson&gt;" Title="Generate data samples from Poisson regression" ViewCount="1941" />
  
  <row Body="&lt;p&gt;It is best to think of cross-validation as a way of estimating the generalisation performance of models generated by a particular procedure, rather than of the model itself.  Leave-one-out cross-validation is essentially an estimate of the generalisation performance of a model trained on $n-1$ samples of data, which is generally a slightly pessimistic estimate of the performance of a model trained on $n$ samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rather than choosing one model, the thing to do is to fit the model to all of the data, and use LOO-CV to provide a slightly conservative estimate of the performance of that model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note however that LOOCV has a high variance (the value you will get varies a lot if you use a different random sample of data) which often makes it a bad choice of estimator for performance evaluation, even though it is approximately unbiased.  I use it all the time for model selection, but really only because it is cheap (almost free for the kernel models I am working on).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-01T11:10:42.027" Id="27456" LastActivityDate="2012-05-01T11:10:42.027" OwnerUserId="887" ParentId="27454" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Adding to C. Pieters' answer: Along with or instead of Cronbach's alpha, you can use factor analysis to validate a group of questions. If a single factor explains almost all of the variation in the group of questions, you can be more confident that the group of questions measures the same thing. I would recommend factor analysis because it is more easily interpreted.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-01T11:59:14.893" Id="27460" LastActivityDate="2012-05-01T11:59:14.893" OwnerUserId="10500" ParentId="27452" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I recently studied two asymmetric &lt;em&gt;t&lt;/em&gt; distribution both with a name of &lt;em&gt;skewed-$t$&lt;/em&gt;. I am confused with their differences or are they actually the same?&lt;/p&gt;&#10;&#10;&lt;p&gt;The first one is introduced by Hansen (1994) with pdf:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(x;\nu,\zeta)=\begin{cases}
  
  
  
  
  <row AcceptedAnswerId="27604" AnswerCount="2" Body="&lt;p&gt;So, [Wikipedia says] that the standard definition of independence is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f_{X,Y}(x,y) = f_X(x) f_Y(y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;How is this applied to timeseries? How do we calculate each side of the equation? &lt;/p&gt;&#10;&#10;&lt;p&gt;If we're assuming that the probability distributions of the two series are the same, then $X=Y$ and we can just use $f(x,y) = f(x) f(y)$, right? So then I geuss I am basically asking what is f(x,y), and how do we calculate it?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-02T05:56:25.873" Id="27508" LastActivityDate="2012-05-07T18:14:43.543" OwnerUserId="9007" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;independence&gt;&lt;definition&gt;" Title="How is the standard definition of independence applied to time-series?" ViewCount="190" />
  
  
  <row Body="&lt;p&gt;That's what Gerd Gigerenzer has been working on in the past: &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0140297863&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Reckoning-With-Risk-Gerd-Gigerenzer/dp/0140297863/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1335941282&amp;amp;sr=1-1&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit to summarize what I think might be what Gigerenzer means:&lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand it, Gigerenzer proposes to communicate risk differently. In the traditional way, a treatment (he's into medical statistics) is reported as having an effect of reducing an illness by a certain percentage. E.g. &quot;eating 100 bananas a day reduces your risk of getting toe nail cancer by 50%&quot;. This is a huge benefit of eating bananas, it seems. The problem is that the prevalence of toe nail cancer isn't exactly high. Let's assume, there is a disease called &quot;toe nail cancer&quot; and its prevalence is 1 in 100000 people. Gigerenzer proposes to report the absolute probability of getting toe nail cancer before and after - e.g. &quot;reduces the risk of getting toe nail cancer from 0,001% to 0,0005%&quot; - which is lot less impressive in the case of rare diseases.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-02T06:48:43.397" Id="27514" LastActivityDate="2012-05-02T15:41:16.750" LastEditDate="2012-05-02T15:41:16.750" LastEditorUserId="1048" OwnerUserId="1048" ParentId="27512" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a response variable(y) and 20 independent variables (Xs). I want to select several Xs in the linear regression, but I'm not sure how many variables should be selected. To select the best number of variables, I use the sum of the squared residuals (Res) in the 10-fold cross-validation given N selected variables (N=2~20). The process is repeated 1000 times given each N. My idea is that Res should firstly decrease as more variable could explain y better and then it should increase as too many variables should lead to over-fitting. To my surprise, Res decrease continually as N increase(see the Figure). I don't know how to explain it.  Is it mean that all 20 variables contribute to y, or over-fitting happened?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P.S.:&lt;/strong&gt; there are about 600 data points. The Res is calculated as the sum of the square of the difference between observed y and predicted y in each 10-fold cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mT7v7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-02T09:50:47.803" FavoriteCount="1" Id="27540" LastActivityDate="2012-05-03T02:59:09.100" LastEditDate="2012-05-03T02:59:09.100" LastEditorUserId="10697" OwnerUserId="10697" PostTypeId="1" Score="4" Tags="&lt;cross-validation&gt;&lt;fitting&gt;" Title="Over-fitting in the cross-validation" ViewCount="277" />
  
  
  
  <row AcceptedAnswerId="27583" AnswerCount="1" Body="&lt;p&gt;I thought it had to do with varying numbers of factors but based on the literature I have read it seems to do more with fixing/freeing of parameters. Any info would be greatly appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-02T15:29:33.717" Id="27560" LastActivityDate="2012-05-02T19:54:08.923" OwnerUserId="11031" PostTypeId="1" Score="2" Tags="&lt;sem&gt;&lt;confirmatory-factor&gt;" Title="What is the difference between a nested and non-nested model in CFA?" ViewCount="1184" />
  <row Body="&lt;p&gt;I will answer the questions that I think I know the answers to.  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;This answer is no because you are picking an $x$ that wasn't part of the fit set $D$ and so $h$ depends on $x$.&lt;/li&gt;&#10;&lt;li&gt;$h$ is only evaluated at the values $x$ in the test set to obtain the expected error rate so it is not evaluated over the entire set $H$ but only at the discrete set of $x$'s in the test set.&lt;/li&gt;&#10;&lt;li&gt;$\mathcal{E}_i(E|F, D)$ is the expected off training set error rate given the function $F$ and the training set $D$.  But $\mathcal{E}_i(E|F, n)$ I think is different because you are only conditioning on the number of training points $n$ and not the actual $x$ values. But this is puzzling given the subsequent statements.&lt;/li&gt;&#10;&lt;li&gt;$D$ is the set of training vectors.  There are $n$ training vectors in $D$.  So you are summing over the fixed $n$ training vectors in $D$.  There is only one set $D$.&lt;/li&gt;&#10;&lt;li&gt;I think the answer to 5 is no.  The notation seems to be a bit confusing.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Can't comment on 6 and 7.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-02T16:18:43.593" Id="27566" LastActivityDate="2012-05-02T16:48:17.813" LastEditDate="2012-05-02T16:48:17.813" LastEditorUserId="7290" OwnerUserId="11032" ParentId="22648" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;This could be a problem of the structure of the chain.  Where you end up depends on where you start.  To use MCMC you want the chain to be recurrent which means that no matter where you start you can get to every other state in the state space. If the chain is not recurrent you can be trapped in a subset of the state space.  The idea of MCMC is to have an existing stationary distribution that the chain will eventually wind up in.  This stationary distribution usually has a positive probability for being in any of the states in the chain and not wind up trapped at a single point as you described.  I can't check your algorithm but maybe you have a mistake in it.  It is also possible that you have defined a problem where your Markov chain is not recurrent.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to get knowledgeable in MCMC I recommend that you take a look at the Handbook of Markov Chain Monte Carlo which has articles that describe every aspect of MCMC. It was published by CRC Press in 2011.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-02T17:03:20.930" Id="27571" LastActivityDate="2012-06-01T17:59:20.193" LastEditDate="2012-06-01T17:59:20.193" LastEditorUserId="11032" OwnerUserId="11032" ParentId="11878" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;If we assume the distribution of hits is binormal&lt;/strong&gt;, then the distribution of the distance to the center of the target is a scaled &lt;a href=&quot;http://en.wikipedia.org/wiki/Chi_distribution&quot; rel=&quot;nofollow&quot;&gt;Chi distribution&lt;/a&gt; with two degrees of freedom.  From this (applying its inverse CDF), we find that its 95th percentile coincides with the radius of 16/2 = 8 centimeters when the scaling factor equals 1.33523.  This factor is also the standard deviation of the components of the binormal distribution.  Integrating the PDF of that binormal distribution over a 13 by 10 cm rectangle centered at the point of aim gives 0.83311, the desired value of $B$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a picture showing a shaded contour plot of the PDF restricted to that rectangle, with the circular target behind it for reference:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5RM3O.png&quot; alt=&quot;Target&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The value of 0.83311 was found with &lt;em&gt;Mathematica&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;With[{s = 8 / InverseCDF[ChiDistribution[2], 0.95]}, &#10; NIntegrate[PDF[BinormalDistribution[{0,0},{s,s},0],{x,y}], {x,-13/2,13/2}, {y,-10/2,10/2}]&#10;]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It was checked by simulating 100,000 independent shots and reporting the proportions that (a) fell within the 16 cm circular target and (b) fell within the rectangular target:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;With[{s = 8 / InverseCDF[g, 0.95], n = 100000}, &#10; data = RandomReal[BinormalDistribution[{0, 0}, {s, s}, 0], n];&#10; old = Length[Select[data, Norm[#] &amp;lt;= 8 &amp;amp;]] / n;&#10; new = Length[Select[data, Abs[#[[1]]] &amp;lt;= 13/2 &amp;amp;&amp;amp; Abs[#[[2]]] &amp;lt;= 10/2 &amp;amp;]] / n;&#10; {old, new} // N&#10;]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output of &lt;code&gt;{0.94919, 0.83331}&lt;/code&gt; is close enough to the intended values of &lt;code&gt;{0.95, 0.83311}&lt;/code&gt; to confirm the correctness of the calculations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-02T20:53:07.167" Id="27584" LastActivityDate="2012-05-02T21:58:10.860" LastEditDate="2012-05-02T21:58:10.860" LastEditorUserId="919" OwnerUserId="919" ParentId="27475" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;In your example, you are indeed confusing the variance of the mean with the variance of the population. To understand where the $N-1$ factor is coming from, you would have to follow the geometry of a multivariate normal distribution and &lt;a href=&quot;http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_cochran.pdf&quot; rel=&quot;nofollow&quot;&gt;Cochran's theorem&lt;/a&gt; that describes the decomposition of the sums of squares.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-02T21:43:42.940" Id="27585" LastActivityDate="2012-05-02T21:43:42.940" OwnerUserId="5739" ParentId="25221" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Two reasons one may go with a Bayesian approach even if you're using highly non-informative priors:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Convergence problems. There are some distributions (binomial, negative binomial and generalized gamma are the ones I'm most familiar with) that have convergence issues a non-trivial amount of the time. You can use a &quot;Bayesian&quot; framework - and particular Markov chain Monte Carlo (MCMC) methods, to essentially plow through these convergence issues with computational power and get decent estimates from them.&lt;/li&gt;&#10;&lt;li&gt;Interpretation. A Bayesian estimate + 95% credible interval has a more intuitive interpretation than a frequentist estimate + 95% confidence interval, so some may prefer to simply report those.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-05-02T23:12:39.290" Id="27593" LastActivityDate="2012-05-08T20:26:22.647" LastEditDate="2012-05-08T20:26:22.647" LastEditorUserId="10875" OwnerUserId="5836" ParentId="27589" PostTypeId="2" Score="15" />
  <row Body="&lt;p&gt;Sir &lt;a href=&quot;http://en.wikipedia.org/wiki/Harold_Jeffreys&quot; rel=&quot;nofollow&quot;&gt;Harold Jeffreys&lt;/a&gt; was a strong proponent of the Bayesian approach. He showed that if you use diffuse improper priors the resulting Bayesian inference would be the same as the frequentist inferential approach (that is, Bayesian credible regions are the same as frequentist confidence intervals). Most Bayesians advocate proper informative priors. There are problems with improper priors and some can argue that no prior is truly non-informative. I think that the Bayesians that use these Jeffreys' prior do it as followers of Jeffreys. &lt;a href=&quot;http://en.wikipedia.org/wiki/Dennis_Lindley&quot; rel=&quot;nofollow&quot;&gt;Dennis Lindley&lt;/a&gt;, one of the strongest advocates of the Bayesian approach, had a great deal of respect for Jeffreys but advocated informative priors.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-05-03T00:18:17.117" Id="27600" LastActivityDate="2012-05-04T09:42:27.883" LastEditDate="2012-05-04T09:42:27.883" LastEditorUserId="509" OwnerUserId="11032" ParentId="27589" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="27657" AnswerCount="1" Body="&lt;p&gt;What does mean by the &quot;the cumulative frequency in the form of probits&quot; (see diagram)? How to use this methodology  when one has only the Ratio and group information. I wonder how to compute these probits for the cumulative frequencies (preferably in R). Thanks in advance for your help and time.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KJkYd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-03T04:49:52.697" FavoriteCount="2" Id="27607" LastActivityDate="2012-05-03T21:44:12.503" LastEditDate="2012-05-03T21:44:12.503" LastEditorUserId="930" OwnerUserId="3903" PostTypeId="1" Score="4" Tags="&lt;data-visualization&gt;&lt;modeling&gt;&lt;biostatistics&gt;" Title="What does mean by &quot;the cumulative frequency in the form of probits&quot;?" ViewCount="529" />
  <row Body="&lt;p&gt;The eigenvalues are proportional to the explained variance. You have given us their cumulative sum, therefore we conclude the first two eigenvectors alone account for over 98% of the variance, per your requirement.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-03T05:42:16.667" Id="27608" LastActivityDate="2012-05-03T05:42:16.667" OwnerUserId="4479" ParentId="27572" PostTypeId="2" Score="5" />
  
  
&#10;\end{pmatrix}
&#10;\mu_2 \\
&#10; \end{pmatrix}
&#10;$$&#10;with covariance matrix having the following structure:&#10;$$
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have data from a study with three conditions. The independent variable was within-subjects and within-items.&lt;/p&gt;&#10;&#10;&lt;p&gt;I did an F1 and F2 analysis (= twice a repeated measures ANOVA) on the averages, by subject (the participants) and by item (the words). I checked/corrected for sphericity and the means do differ significantly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to know how the three conditions differ from one another. I have a &quot;Pairwise Comparisons&quot; box and p-values (I checked &quot;Bonferroni&quot; when doing the analyses). But I do not have t-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I know my corresponding t-values? Or should I do some paired-samples t-tests? If so, what does this &quot;Pairwise Comparisons&quot; box tell me?&lt;/p&gt;&#10;&#10;&lt;p&gt;Screenshot of the &quot;PC&quot; box:&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/cM1Zn.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-03T12:56:10.857" Id="27624" LastActivityDate="2014-08-25T22:33:24.627" LastEditDate="2014-06-29T05:46:32.300" LastEditorUserId="805" OwnerUserId="3140" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;spss&gt;" Title="Where are the t-values in my pairwise comparisons?" ViewCount="971" />
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/27554/logistic-regression-sub-group-size-parameters&quot;&gt;Logistic regression sub-group size parameters&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Someone in my lab has a sample of 500 older kids and he wants to investigate what factors are related to the probability that they will bully. Groups: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;kids were bullied and go on to bully is made up of 22; &lt;/li&gt;&#10;&lt;li&gt;were not bullied but go on to bully is made up of 28 kids; &lt;/li&gt;&#10;&lt;li&gt;never bullied and never bully is made up of 250; &lt;/li&gt;&#10;&lt;li&gt;were bullied and never bully is made up of 200. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;They created an interaction variable using age*not bullied/bullied and added 14 other &quot;predictors&quot; to the mode.  Interaction of age and bullying is sig but the prototypical plot looks weird, the group who were bullied have a 0 slope by age and the line starts and ends at 0. My guess is the the bullied/bullying group is way too small for all the info in the model. IF so, what are the group size parameters or ratio to total group parameters for logistic regression? Thank you very much!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zw6Gd.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" ClosedDate="2012-05-03T14:56:56.723" CommentCount="2" CreationDate="2012-05-03T13:31:08.437" FavoriteCount="0" Id="27626" LastActivityDate="2012-05-03T14:53:23.577" LastEditDate="2012-05-03T14:53:23.577" LastEditorUserId="11031" OwnerUserId="11031" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;" Title="Does the size of the reference sample matter in logistic regression?" ViewCount="35" />
  <row Body="&lt;p&gt;I don't see why this is so complicated.  Why can't you simply do the two hypothesis tests and apply a mutliplicity adjustment and conclude that $p_a$ is the largest if the adjusted $p$-value is less than say $0.05$?  I don't think the fact that the tests are dependent matters.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-03T15:12:33.070" Id="27641" LastActivityDate="2012-05-04T02:18:30.153" LastEditDate="2012-05-04T02:18:30.153" LastEditorUserId="4856" OwnerUserId="11032" ParentId="27447" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I wouldn't call the response a random component.  It is a combination of a deterministic and a random component.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I would describe generalized linear models this way.  We have a response variable and a set of related variables that can aid in predicting the response.  However the response and the predictors are not linearly related.  The link function provides a transformation of the response so that the transformed response is linearly related to the predictors.  For example in logistic regression the predictor could be continuous variables that can take on values over the entire real line.  But the response is a probability (the probability of a successful outcome in a clinical trial for example).  So the response is constrained to fall between 0 and 1.  The link function in logistic regression is called the logit function. It equals $\log(p/(1-p))$.  You can see that the logit function transforms a variable constrained to $[0,1]$ to a variable that can take values over the entire real line.  In this case the link function makes the response compatible with the predictor variables and hence it is possible to make it a linear function of the predictors plus a random component.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-03T17:23:38.273" Id="27658" LastActivityDate="2012-05-04T02:16:12.590" LastEditDate="2012-05-04T02:16:12.590" LastEditorUserId="4856" OwnerUserId="11032" ParentId="27651" PostTypeId="2" Score="8" />
  
  <row AcceptedAnswerId="27671" AnswerCount="2" Body="&lt;p&gt;When people speak of &quot;simulation variance&quot;, what does this mean? Does simulation variance disappear as $N \rightarrow \infty $? &lt;/p&gt;&#10;&#10;&lt;p&gt;Are models that have less simulation variance for a fixed N considered more &quot;robust&quot;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-23T00:58:06.777" Id="27670" LastActivityDate="2012-05-03T19:30:25.150" LastEditDate="2012-05-03T19:30:25.150" LastEditorUserId="4856" OwnerDisplayName="dchandler" OwnerUserId="6877" PostTypeId="1" Score="2" Tags="&lt;econometrics&gt;&lt;optimization&gt;" Title="In structural econometrics, what is meant by &quot;simulation variance&quot;" ViewCount="53" />
  <row Body="&lt;p&gt;There is no very strong reason for preferring natural logarithms.  Suppose we are estimating the model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ln Y = a + b ln X&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The relation between natural (ln) and base 10 (log) logarithms is ln X = 2.303 log X &lt;a href=&quot;http://faculty.washington.edu/djaffe/natlogs.html&quot;&gt;(source)&lt;/a&gt;.  Hence the model is equivalent to:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2.303 log Y = a + 2.303b log X&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or, putting a / 2.303 = a*:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;log Y = a* + b log X&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Either form of the model could be estimated, with equivalent results.&lt;/p&gt;&#10;&#10;&lt;p&gt;A slight advantage of natural logarithms is that their first differential is simpler: d(ln X)/dX = 1/X,  while d(log X)/dX = 1 / ((ln 10)X) &lt;a href=&quot;http://au.answers.yahoo.com/question/index?qid=20110209230807AAGk6by&quot;&gt;(source)&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For a source in an econometrics textbook saying that either form of logarithms could be used, see Gujarati, &lt;em&gt;Essentials of Econometrics&lt;/em&gt; 3rd edition 2006 p 288.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-03-27T14:29:48.753" Id="27683" LastActivityDate="2012-03-27T14:29:48.753" OwnerDisplayName="Adam Bailey" OwnerUserId="11060" ParentId="27682" PostTypeId="2" Score="12" />
  
  <row AcceptedAnswerId="27709" AnswerCount="2" Body="&lt;p&gt;I have two time-series:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A proxy for the market risk premium (ERP; red line)&lt;/li&gt;&#10;&lt;li&gt;The risk-free rate, proxied by a government bond (blue line)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/evTDC.png&quot; alt=&quot;Risk premium proxy and risk-free rate over time&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test if the risk-free rate can explain the ERP. Hereby, I basically followed the advice of Tsay (2010, 3rd edition, p. 96): Financial Time Series:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fit the linear regression model and check serial correlations of the residuals.&lt;/li&gt;&#10;&lt;li&gt;If the residual series is unit-root nonstationarity, take the first difference of both the dependent and explanatory variables.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Doing the first step, I get the following results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;               Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)     6.77019    0.25103   26.97   &amp;lt;2e-16 ***&#10;Risk_Free_Rate -0.65320    0.04123  -15.84   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As expected from the figure, the relation is negative and significant. However, the residuals are serially correlated:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GfdWn.png&quot; alt=&quot;ACF function of the residuals of the regression of risk-free rate on ERP&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I first difference both the dependent and explanatory variable. Here is what I get:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;                Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)    -0.002077   0.016497  -0.126      0.9    &#10;Risk_Free_Rate -0.958267   0.053731 -17.834   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And the ACF of the residuals looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/J25Gn.png&quot; alt=&quot;ACF function of the residuals of the regression of risk-free rate on ERP (differenced)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This result looks great: First, the residuals are now uncorrelated. Second, the relation seems to be more negative now. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are my questions (you probably wondered by now ;-) The first regression, I would have interpreted as (econometric problems aside) &quot;if the riskfree rate rises by one percentage point, the ERP falls by 0.65 percentage points.&quot; Actually, after pondering about this for a while, I would interpret the second regression just the same (now resulting in a 0.96 percentage points fall though). Is this interpretation correct? It just feels weird that I transform my variables, but don't have to change my interpretation. If this, however, is correct, why do the results change? Is this just the result of econometric problems? If so, does anyone have an idea why my second regression seems to be even &quot;better&quot;? Normally, I always read that you can have spurious correlations that vanish after you do it correctly. Here, it seems the other way round.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-03T18:58:06.457" FavoriteCount="7" Id="27691" LastActivityDate="2013-07-12T12:18:59.907" OwnerUserId="7964" PostTypeId="1" Score="10" Tags="&lt;regression&gt;&lt;time-series&gt;" Title="How do I interpret my regression with first differenced variables?" ViewCount="12505" />
  
  <row Body="&lt;p&gt;Whether a job requires a PhD or not depends on level of responsibility and the perception of the employer and/or his clients.  I do not think there is a discipline that requires a PhD.  Certainly data mining can be learned and an employee can do productive work without a PhD.  This depends more on the person, his or her ability to learn quickly and adapt as well as being able to understand the literature, than on previous education.  This is especially true for data mining which is an evolving field.  So even the data miners with PhDs will have more to learn as time goes on.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2013-08-08T22:45:51.357" CreationDate="2012-05-03T19:53:26.277" Id="27699" LastActivityDate="2012-05-03T19:53:26.277" OwnerUserId="11032" ParentId="27495" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="27716" AnswerCount="1" Body="&lt;p&gt;I have the following multiple linear regression model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = Y ~ X1 + X2 + X2 + X3 + X4 + X5 + X6 + X7, &#10;    data = my.model, na.action = na.omit)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-43.836  -1.507   0.010   1.485  46.231 &#10;&#10;Coefficients:&#10;               Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  -0.0244927  0.0245157  -0.999    0.318    &#10;X1           -0.3484619  0.0134383 -25.931   &amp;lt;2e-16 ***&#10;X2            0.1195273  0.0106940  11.177   &amp;lt;2e-16 ***&#10;X3            0.1224587  0.0108849  11.250   &amp;lt;2e-16 ***&#10;X4           -0.0010173  0.0028247  -0.360    0.719    &#10;X5            0.5496942  0.0156319  35.165   &amp;lt;2e-16 ***&#10;X6           -0.2287941  0.0145018 -15.777   &amp;lt;2e-16 ***&#10;X7           -0.2315801  0.0146361 -15.823   &amp;lt;2e-16 ***&#10;X8            0.0005465  0.0003595   1.520    0.128    &#10;---&#10;Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 &#10;&#10;Residual standard error: 2.936 on 35849 degrees of freedom&#10;  (12534 observations deleted due to missingness)&#10;Multiple R-squared: 0.05968,    Adjusted R-squared: 0.05947 &#10;F-statistic: 284.4 on 8 and 35849 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The model is affected by multicollinearity but my question is about the forecast, so this shouldn't be an issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;I checked the absolute values of my model forecast and compared against the actual Y absolute values. The average of the absolute predicted values is significantly lower than the absolute observed values mean:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; lm1.predict = predict(lm1, mydata)&#10;&amp;gt; mean(abs(lm1.predict))&#10;[1] 0.3294776&#10;&amp;gt; mean(abs(mydata$Y))&#10;[1] 1.206954&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does this mean that the linear regression variables I am using tend to underestimate the outcomes? Can any other conclusion be derived from this simple comparison?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way to look at this is to calculate the absolute difference between each observation and the relative outcome:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mean(abs(mydata$Y - lm1.predict))&#10;[1] 1.208378&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These are the diagnostic from the regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y7zmf.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-03T19:59:30.593" Id="27700" LastActivityDate="2012-05-08T05:28:21.523" LastEditDate="2012-05-03T21:25:22.467" LastEditorUserId="7795" OwnerUserId="7795" PostTypeId="1" Score="0" Tags="&lt;forecasting&gt;&lt;multiple-regression&gt;" Title="Linear regression forecast underestimation" ViewCount="539" />
  <row Body="&lt;p&gt;Suppose that we have the model&#10;$$\begin{equation*} y_t = \beta_0 + \beta_1 x_t + \beta_2 t + \epsilon_t. \end{equation*}$$&#10;You say that these coefficients are easier to interpret. Let's subtract $y_{t-1}$ from the lefthand side and $\beta_0 + \beta_1 x_{t-1} + \beta_2 ({t-1}) + \epsilon_{t-1}$, which equals $y_{t-1}$, from the righthand side. We have&#10;$$\begin{equation*} \Delta y_t = \beta_1 \Delta x_t + \beta_2 + \Delta \epsilon_t. \end{equation*}$$&#10;The intercept in the difference equation is the time trend. And the coefficient on $\Delta x$ has the same interpretation as $\beta_1$ in the original model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the errors were non-stationary such that&#10;$$\begin{equation*} \epsilon_t = \sum_{s=0}^{t-1}{\nu_s}, \end{equation*}$$&#10;such that $\nu_s$ is white noise, the the differenced error is white noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the errors have a stationary AR(p) distribution, say, then the differenced error term would have a more complicated distribution and, notably, would retain serial correlation. Or if the original $\epsilon$ are already white noise (An AR(1) with a correlation coefficient of 0 if you like), then differencing induces serial correlation between the errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;For these reasons, it is important to only difference processes that are non-stationary due to unit roots and use detrending for so-called trend stationary ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;(A unit root causes the variance of a series to change and it actually explode over time; the expected value of this series is constant, however. A trend stationary process has the opposite properties.)&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-05-03T21:02:09.657" Id="27709" LastActivityDate="2012-05-04T00:30:29.437" LastEditDate="2012-05-04T00:30:29.437" LastEditorUserId="401" OwnerUserId="401" ParentId="27691" PostTypeId="2" Score="12" />
  
  
  <row AcceptedAnswerId="27726" AnswerCount="8" Body="&lt;p&gt;I am actually reviewing a manuscript where the authors compare 5-6 logit regression models with AIC. However, some of the models have interaction terms without including the individual covariate terms. Does it ever make sense to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example (not specific to logit models):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;M1: Y = X1 + X2 + X1*X2&#10;M2: Y = X1 + X2&#10;M3: Y = X1 + X1*X2 (missing X2)&#10;M4: Y = X2 + X1*X2 (missing X1)&#10;M5: Y = X1*X2 (missing X1 &amp;amp; X2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've always been under the impression that if you have the interaction term X1*X2 you also need X1 + X2. Therefore, models 1 and 2 would be fine but models 3-5 would be problematic (even if AIC is lower). Is this correct? Is it a rule or more of a guideline? Does anyone have a good reference that explains the reasoning behind this? I just want to make sure I don't miscommunicate anything important in the review.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any thoughts,&#10;Dan&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-04T02:10:29.487" FavoriteCount="18" Id="27724" LastActivityDate="2013-12-13T11:50:24.197" OwnerUserId="8289" PostTypeId="1" Score="32" Tags="&lt;regression&gt;&lt;modeling&gt;&lt;interaction&gt;&lt;aic&gt;" Title="Do all interactions terms need their individual terms in regression model?" ViewCount="11078" />
  <row Body="&lt;p&gt;You have correctly applied Bayes rule but this is a far cry from understanding Bayesian statistics.  Bayesian statistics is a way of doing statistical inference that is different the frequentist approach.  Suppose you want to get an interval estimate of the mean of a normal distribution.  You first put a prior distribution on the mean then you collect data. Bayes' theorem is used to relate the prior distribution for the mean to the posterior distribution by posterior is proportional to prior times likelihood.  The posterior distribution is then used for inference.  An interval estimate for the mean that is the Bayesian analog of a 95% confidence interval is a 95% credible interval.  You get this by picking points equal distant in each direction from the mean of the posterior distribution and far enough so that the integral from the lower endpoint to the upper endpoint is 0.95.  This is what Bayesian statistics is about. It is much more than just Bayes theorem.  Bayes theorem is an uncontroversial mathematical result in probability.  Bayesian statistics is a paradigm for statistical inference and is somewhat controversial because it assigns probability distributions to unknown parameters.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-04T02:50:46.573" Id="27727" LastActivityDate="2012-05-04T02:50:46.573" OwnerUserId="11032" ParentId="27356" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;GMM is practically the only estimation method which you can use, when you run into endogeneity problems. Since these are  more or less unique to econometrics, this explains GMM atraction. Note that this applies if you subsume IV methods into GMM, which is perfectly sensible thing to do.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-13T17:11:34.123" Id="27739" LastActivityDate="2011-12-13T17:11:34.123" OwnerDisplayName="mpiktas" OwnerUserId="2116" ParentId="27736" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;As Rob Hyndman wrote, you can use Kalman filter. Your &#10;interest seems on estimating the unobservable AR(1) component $\alpha_t$  in&#10;your model written as&#10;$$
  
  <row Body="&lt;p&gt;This is classical hypothesis testing.  The null hypothesis state that the data are randomly placed in the categories.  If the null hypothesis is true V will ASYMPTOTICALLY have the stated chi square distribution (this is an approximate (not an exact) test). v is your test statistic.  You reject randomness if the value of v is large relative the reference chi square distribution which in your case would mean the test for randomness fails.  Often the 5% level is chosen as the cutoff point for rejection which means you would compare the observed v to the 95th percentile of the chi square distribution (reject if it is larger).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-04T12:36:28.257" Id="27755" LastActivityDate="2012-05-04T12:36:28.257" OwnerUserId="11032" ParentId="27749" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;To add a slightly different and more general description of the problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do any kind of &lt;strong&gt;data-driven pre-processing&lt;/strong&gt;, e.g.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;parameter optimization guided by cross validation / out-of-bootstrap&lt;/li&gt;&#10;&lt;li&gt;dimensionality reduction with techniques like PCA or PLS to produce input for the model (e.g. PLS-LDA, PCA-LDA)&lt;/li&gt;&#10;&lt;li&gt;...&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;and want to use cross validation/out-of-bootstrap(/hold out) validation to estimate the &lt;em&gt;final&lt;/em&gt; model's performance, the data-driven pre-processing needs to be done on the surrogate training data, i.e. separately for each surrogate model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the data-driven pre-processing is of type 1., this leads to &quot;double&quot; or &quot;nested&quot; cross validation: the parameter estimation is done in a cross validation using only the training set of the &quot;outer&quot; cross validation.&#10; The ElemStatLearn have an illustration (p. 222 of &lt;a href=&quot;http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf&quot;&gt;http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;You may say that the pre-processing is really part of the building of the model. only pre-processing that is done&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;independently for each case or&lt;/li&gt;&#10;&lt;li&gt;independently of the actual data set&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;can be taken out of the validation loop to save computations. &lt;/p&gt;&#10;&#10;&lt;p&gt;So the other way round: if your model is completely built by knowledge &lt;em&gt;external&lt;/em&gt; to the particular data set (e.g. you decide beforehand by your expert knowledge that measurement channels 63 - 79 cannot possibly help to solve the problem, you can of course exclude these channels, build the model and cross-validate it. The same, if you do a PLS regression and decide by your experience that 3 latent variables are a reasonable choice (but do &lt;em&gt;not&lt;/em&gt; play around whether 2 or 5 lv give better results) then you can go ahead with a normal out-of-bootstrap/cross validation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-04T13:37:07.297" Id="27761" LastActivityDate="2012-05-04T13:37:07.297" OwnerUserId="4598" ParentId="27750" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;It is about sas programming. Thank you in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I build an ordinal logistic regression model on my data. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Level  Meaning&#10;  3     Alert&#10;  2    Unusual Event&#10;  1    Non Emergency&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, it is my ordinal response variable. 3 means large loss and Alert situation, etc. I build a very simple model, the graph below shows the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; proc logistic data = sasuser.Pacific_West_ outmodel=sasuser.Pacifici_West_model;;&#10;     model Numerical_Emergency(event = '1') = AGE;&#10; run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xBihb.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Everything looks fine. Now I would like to apply the BOOTSTRAP method to validate this model. What thing I would like to see is the test c value as ROC value. As you may see in the graph above, the c (ROC) value is 0.640. The basic idea of bootstrap is to generate new data set by random picking process from the data we used to train the model as we all know. I prefer to generate 1000 copies of data with same size of original data for building the model in graph above.&lt;/p&gt;&#10;&#10;&lt;p&gt;THE PROBLEM:&lt;/p&gt;&#10;&#10;&lt;p&gt;In the SAS Score statement, I just found outroc method which does not support the ordinal regression. Is there a simple procedure to help me to output the statistics about fitting of trained model on the validation dataset build by myself?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much for your time. Have a nice day!&lt;/p&gt;&#10;&#10;&lt;p&gt;Wenhao SHE&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-04T14:38:33.300" Id="27766" LastActivityDate="2012-11-23T15:07:50.187" LastEditDate="2012-11-23T15:07:50.187" LastEditorUserId="919" OwnerUserId="9900" PostTypeId="1" Score="0" Tags="&lt;sas&gt;&lt;roc&gt;&lt;validation&gt;&lt;scores&gt;" Title="Method to output statistical score when validating using bootstrap method on ordinal logistic regression model" ViewCount="626" />
  <row AcceptedAnswerId="27797" AnswerCount="1" Body="&lt;p&gt;I'm designing a web service that will predict and recommend new items a user might like based on their expressed preferences on previous items (simple thumbs up/down interface). &lt;/p&gt;&#10;&#10;&lt;p&gt;I was told to look into decision trees, since they're a simple way to group things based on their characteristics, and they can be easily used to classify new items. But after having done some reading and looked at a number of examples, I'm wondering if they're the best solution in this instance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some problems I see:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There are thousands of items in the database, which means there are&#10;potentially thousands of items on which to split the tree. Running&#10;all of these splits and then calculating which ones provide the most&#10;gain (loss of entropy) seems inefficient to me.&lt;/li&gt;&#10;&lt;li&gt;The tree would be changing constantly. Most of the examples I've&#10;looked at build a tree with an initial sample set of data, and then&#10;run new items through it to classify them. While I might be able to&#10;keep a stable tree in memory after awhile, it would need to be&#10;updated pretty frequently for the first few users.&lt;/li&gt;&#10;&lt;li&gt;The tree is stored in memory, not a database. This could lead to issues if a&#10;server dies or goes offline. Also, I'm planning to user multiple&#10;machines to host the project, so there'd need to be some sort of&#10;shared memory space they could all access.&lt;/li&gt;&#10;&lt;li&gt;I don't have any categorical or descriptive data on each of the items to help with the decisions. The entire tree would have to be &quot;did they like this other item or not?&quot; I'm not sure if that poses any problems, but I haven't seen any other examples that use that sort of data for building the tree.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This is my first attempt at machine learning, so it could be that these are all trivial problems compared to using other solutions, and I should just stick with decision trees. But if there's another solution that seems more appropriate, or if you can address some of these concerns, I'd love to hear it!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-04T16:00:45.110" Id="27770" LastActivityDate="2012-05-04T20:21:55.707" LastEditDate="2012-05-04T16:07:39.190" LastEditorUserId="7842" OwnerUserId="7842" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;cart&gt;" Title="Should I use decision trees to predict user preferences?" ViewCount="207" />
  <row AnswerCount="3" Body="&lt;p&gt;I conducted a survey using a 5 point Likert scale. Firstly, I asked the respondent to select one airline out of six airlines given and then there were 20 statements regarding the services offered by the airline. The respondents have to mark their extent of agreement or disagreement on a 5 point Likert scale. So I received about 120 responses (20 questionnaires each for one airline). &lt;/p&gt;&#10;&#10;&lt;p&gt;Which tool should I use to analyze the these data to compare the 6 airlines with respect to the responses obtained in the questionnaire? Also, can you help identify the dependent and independent variables?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-05-04T18:01:44.647" FavoriteCount="0" Id="27786" LastActivityDate="2013-07-21T12:11:44.500" LastEditDate="2013-07-21T12:11:44.500" LastEditorUserId="6029" OwnerUserId="11093" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;" Title="How to compare ratings of airlines on a Likert scale when different participants have rated different airlines?" ViewCount="795" />
  <row Body="&lt;p&gt;The method is going to be some form of regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The independent variable will be the airline.&lt;/p&gt;&#10;&#10;&lt;p&gt;What the dependent variable will be depends on the nature of the questions and whether it is sensible to add the responses; or perhaps you should do a factor analysis of the responses; it's also possible that you  want 20 different regressions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which form of regression depends on the answers to these questions. Maybe OLS, maybe ordinal logistic. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-04T19:08:21.453" Id="27791" LastActivityDate="2012-05-04T19:08:21.453" OwnerUserId="686" ParentId="27786" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I have spoken on this at a different forum the ASA Statistical Consulting eGroup.  My response was more specifically to data mining but the two go hand in hand.  We statisticians have snubbed our noses at data miners, computer scientists, and engineers. It is wrong. I think part of the reason it happens is because we see some people in those fields ignoring the stochastic nature of their problem.  Some statisticians call data mining  data snooping or data fishing.  Some people do abuse and misuse the methods but statisticians have fallen behind in data mining and machine learning because we paint them with a broad brush.  Some of the big statistical results have come from outside the field of statistics.  Boosting is one important example.  But statisticians like Brieman, Friedman, Hastie, Tibshirani, Efron, Gelman and others got it and their leadership has brought statisticians into the analysis of microarrays and other large scale inference problems. So while the cultures may never mesh there is now more cooperation and collabortion between the computer scientists, engineers and statisticians.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-05-04T21:08:26.210" CreationDate="2012-05-04T21:08:26.210" Id="27799" LastActivityDate="2012-05-04T21:08:26.210" OwnerUserId="11032" ParentId="6" PostTypeId="2" Score="12" />
  
  
  
  
  <row Body="&lt;p&gt;A possible alternative to correction, depending on you question, is to test for significance of the sum of p-values.  You can then even penalize yourself for test that are not done by adding high p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Extension's (which don't require independence) of Fisher's method (which require independence of test) could be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1016/S0167-7152%2802%2900310-3&quot; rel=&quot;nofollow&quot;&gt;Eg. Kost's method&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-05T13:16:46.160" Id="27837" LastActivityDate="2012-05-05T13:16:46.160" OwnerUserId="4843" ParentId="16779" PostTypeId="2" Score="1" />
  
  
&#10;&amp;amp;Bed&amp;amp;Table&amp;amp;Chair&amp;amp;Door&amp;amp;\text{Total}\\
&#10;\text{Average contacts}&amp;amp;3&amp;amp;2&amp;amp;1&amp;amp;1&amp;amp;\text{7}\\
&#10;\end{array}
  
&#10;\sum_i x_i(Y_i -  g^{-1}(x_i^T\beta)) = \mathbf{0},
  
  
  <row Body="&lt;p&gt;What you are doing or can do is generate a 95% exact confidence interval for the proportion in the population that will properly generate a nickname.  The interval you get when there are only successes will be [p, 1].  You can interpret this to mean that this procedure when taking a sample of size 400 repeatedly will include the true proportion approximately 95% of the time or more.  So you have strong confidence to think that the proportion is greater than p. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-06T02:37:52.877" Id="27871" LastActivityDate="2012-05-06T02:37:52.877" OwnerUserId="11032" ParentId="27817" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;If your using linear regression I would recommend using &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/Main/Rrms&quot; rel=&quot;nofollow&quot;&gt;the rms package&lt;/a&gt; in R. It is very easy to use and has lots of nice features. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Load package (remember to install.packages(&quot;rms&quot;) or this will fail the first time)&#10;library(rms)&#10;&#10;# Get a dataset to experiment with&#10;data(mtcars)&#10;mtcars$am &amp;lt;- factor(mtcars$am, levels=0:1, labels=c(&quot;Automatic&quot;, &quot;Manual&quot;))&#10;&#10;# The rms package needs this work properly&#10;dd &amp;lt;- datadist(mtcars)&#10;options(datadist=&quot;dd&quot;)&#10;&#10;# Do the regression&#10;f &amp;lt;- ols(mpg~wt, data=mtcars, x=T, y=T)&#10;&#10;# Plot regular mean confidence interval&#10;p &amp;lt;- Predict(f, wt=seq(2.5, 4, by=.001), conf.type=&quot;mean&quot;)&#10;plot(p, ylim=c(10, 30), col=&quot;lightblue&quot;)&#10;&#10;# Plot wide confidence interval&#10;p &amp;lt;- Predict(f, wt=seq(2.5, 4, by=.001), conf.type=&quot;individual&quot;)&#10;plot(p, ylim=c(10, 30), col=&quot;lightblue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives this output:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UAvh7.png&quot; alt=&quot;Plain line&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now usually you want to test the linearity assumption:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Try the model with a restricted cubic spline&#10;f &amp;lt;- ols(mpg~rcs(wt, 3), data=mtcars, x=T, y=T)&#10;anova(f)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives this output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; anova(f)&#10;                Analysis of Variance          Response: mpg &#10;&#10; Factor     d.f. Partial SS MS         F     P     &#10; wt          2   922.04230  461.021149 65.54 &amp;lt;.0001&#10;  Nonlinear  1    74.31705   74.317047 10.56 0.0029&#10; REGRESSION  2   922.04230  461.021149 65.54 &amp;lt;.0001&#10; ERROR      29   204.00489    7.034651             &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And if you plot the graphs with the same code as a bove you get this picture:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jkftF.png&quot; alt=&quot;Regression with a spline&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to make your formula more complicated just add that variable:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;f &amp;lt;- ols(mpg~rcs(wt, 3)+am, data=mtcars, x=T, y=T)&#10;p &amp;lt;- Predict(f, wt=seq(2.5, 4, by=.001), am=levels(mtcars$am), conf.type=&quot;individual&quot;)&#10;plot(p)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I don't know anything about JMP, it shouldn't be too difficult but I recommend learning R because it gives you an incredible freedom. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helped.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-06T10:01:28.687" Id="27886" LastActivityDate="2012-05-06T10:01:28.687" OwnerUserId="5429" ParentId="27777" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm reading the paper &lt;a href=&quot;http://www-stat.wharton.upenn.edu/~tcai/paper/Sparse-Covariance-Matrix.pdf&quot; rel=&quot;nofollow&quot;&gt;Optimal Rates of Convergence for Sparse Covariance Matrix Estimation&lt;/a&gt; by T. Cai and H. Zhou.&lt;/p&gt;&#10;&#10;&lt;p&gt;They refer to a specific version of Assouad's Lemma (see lemma 2, p. 6 of the linked paper), which differs from the one I've found in Yu [1] and Le Cam [2]. I've also ordered van der Vaart [3], but it has yet to arrive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can do you know of a resource that contains a proof of the version Cai states? Or can you give a proof?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm especially interested because I'm having trouble with their proof of lemma 3 (p. 7 of linked paper, proof p. 21) and the lemmas seem very related. Specifically, I don't understand how the use of&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\theta} = \mbox{argmin}_{\theta \in \Theta} d(T,\psi (\theta))$&lt;/p&gt;&#10;&#10;&lt;p&gt;in eq. (37) is meaningful as $T$ is a random variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] B. Yu (1997). Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam. D. Pollard, E. Torgersen, and G. Yang (eds), pp. 423-435, Springer-Verlag.&lt;/p&gt;&#10;&#10;&lt;p&gt;[2] Le Cam, L. (1986). Asymptotic Methods in Statistical Decision Theory. SpringerVerlag, New York.&lt;/p&gt;&#10;&#10;&lt;p&gt;[3] van der Vaart, A.W. (1998). Asymptotic Statistics. Cambridge University Press, Cambridge.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-06T16:29:36.730" Id="27895" LastActivityDate="2012-05-06T21:32:10.103" LastEditDate="2012-05-06T21:32:10.103" LastEditorUserId="11130" OwnerUserId="11130" PostTypeId="1" Score="2" Tags="&lt;asymptotics&gt;" Title="Variations of Assouad’s lemma" ViewCount="159" />
  
&#10;a_{ij} = \min(1, \frac{\pi_{j} h_{ji}}{\pi_{i} h_{ij}}).
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;When $h_i$ is a Guassian distribution centered at state $i$ and has the same variance $\sigma^2$ for all $i$, $h$ is symmetric. From &lt;a href=&quot;http://en.wikipedia.org/wiki/Multiple-try_Metropolis&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If $\sigma^2 \,$ is too large, almost all steps under the MH algorithm&#10;  will be rejected. On the other hand, if $\sigma^2 \,$ is too small,&#10;  almost all steps will be accepted.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I wonder why the accept probability changes in the reverse direction of the change of variance of the proposal density, as mentioned in the above quote?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-06T23:24:04.160" FavoriteCount="2" Id="27913" LastActivityDate="2012-05-07T03:10:15.260" LastEditDate="2012-05-07T03:10:15.260" LastEditorUserId="10950" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;mcmc&gt;&lt;metropolis-hastings&gt;" Title="Accept rate in Metropolis–Hastings algorithm" ViewCount="2107" />
  <row Body="&lt;p&gt;Following Henry and Gung's advice in comments (thanks!)&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the value of an estimator is a function of the observed data, and the observed data is assumed to follow some distribution, it follows that the value of the estimator also follows some distribution. As the sample size gets large, estimators in the class of &quot;M-estimators&quot; become distributed according to a Normal distribution, and one can use this fact, along with a little bit of hope (or bootstrapping) to develop approximate confidence intervals.   &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-06T23:36:30.010" Id="27914" LastActivityDate="2012-05-06T23:36:30.010" OwnerUserId="7555" ParentId="24960" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;When doing least squares estimation (assuming a normal random component) the regression parameter estimates are normally distributed with mean equal to the true regression parameter and covariance matrix $\Sigma = s^2\cdot(X^TX)^{-1}$ where $s^2$ is the residual variance and $X^TX$ is the design matrix. $X^T$ is the transpose of $X$ and $X$ is defined by the model equation $Y=X\beta+\epsilon$ with $\beta$ the regression parameters and $\epsilon$ is the error term.  The estimated standard deviation of a beta parameter is gotten by taking the corresponding term in $(X^TX)^{-1}$ multiplying it by the sample estimate of the residual variance and then taking the square root.  This is not a very simple calculation but any software package will compute it for you and provide it in the output. &lt;/p&gt;&#10;&#10;&lt;h3&gt;Example&lt;/h3&gt;&#10;&#10;&lt;p&gt;On page 134 of Draper and Smith (referenced in my comment), they provide the following data for fitting by least squares a model $Y = \beta_0 + \beta_1 X + \varepsilon$ where $\varepsilon \sim N(0, \mathbb{I}\sigma^2)$.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                      X                      Y                    XY&#10;                      0                     -2                     0&#10;                      2                      0                     0&#10;                      2                      2                     4&#10;                      5                      1                     5&#10;                      5                      3                    15&#10;                      9                      1                     9&#10;                      9                      0                     0&#10;                      9                      0                     0&#10;                      9                      1                     9&#10;                     10                     -1                   -10&#10;                    ---                     --                   ---&#10;Sum                  60                      5                    32&#10;Sum of  Squares     482                     21                   528&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Looks like an example where the slope should be close to 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X^t = \pmatrix{
  <row Body="&lt;p&gt;In order to get this, and to simplify the matters, I always think first in just one parameter with uniform (long-range) a-priori distribution, so that in this case, the MAP estimate of the parameter is the same as the MLE. However, assume that your likelihood function is complicated enough to have several local maxima.&lt;/p&gt;&#10;&#10;&lt;p&gt;What MCMC does in this example in 1-D is to explore the posterior curve until it finds values of maximum probability. If the variance is too short, you'll most surely get stuck on local maxima, because you'll be always sampling values near it: the MCMC algorithm will &quot;think&quot; it is stuck on the target distribution. However, if the variance is too large, once you get stuck on one local maximum, you'll more-or-less reject values until you find other regions of maximum probability. If you happen to propose the value at the MAP (or a similar region of local maximum probability which is larger than the others), with a large variance you'll end up rejecting almost every other value: the difference between this region and the others will be too large.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, all of the above will affect the convergence rate and not the convergence &quot;per-se&quot; of your chains. Recall that whatever the variance, as long as the probability of selecting the value of this global maximum region is positive, your chain will converge.&lt;/p&gt;&#10;&#10;&lt;p&gt;To by-pass this problem, however, what one can do is to propose different variances in a burn-in period for each parameter and aim at a certain acceptance rates which can satisfy your needs (say $0.44$, see &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/baystat5.pdf&quot;&gt;Gelman, Roberts &amp;amp; Gilks, 1995&lt;/a&gt; and &lt;a href=&quot;http://projecteuclid.org/DPubS?verb=Display&amp;amp;version=1.0&amp;amp;service=UI&amp;amp;handle=euclid.aoap/1034625254&amp;amp;page=record&quot;&gt;Gelman, Gilks &amp;amp; Roberts, 1997&lt;/a&gt; to learn more on the issue of selecting a &quot;good&quot; acceptance rate which, of course, will depende on the form of you posterior distribution). Of course, in this case the chain is non-markovian, so you DON'T have to use them for inference: you just use them to adjust the variance. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-07T01:47:11.347" Id="27921" LastActivityDate="2012-05-07T01:47:11.347" OwnerUserId="9174" ParentId="27913" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;The first thing you can do is, for example, interpret $\hat{\beta_2}$ as the estimated effect of $sex$ on the logit of the quantile you're looking at. &lt;/p&gt;&#10;&#10;&lt;p&gt;$\exp\{\hat{\beta_2}\}$, similarly to &quot;classic&quot; logistic regression, is the odds ratio of median (or any other quantile) outcome in males versus females. The difference with &quot;classic&quot; logistic regression is how the odds are calculated: using your (bounded) outcome instead of a probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides, you can always look at the predicted quantiles according to one covariate. Of course you have to fix (condition on) the values of the other covariates in your model (like you did in your example).&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, the transformation should be $\log(\frac{y-y_{min}}{y_{max}-y})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;(This is not really intended to be an answer, as it's just a (poor) rewording of what it's written in &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/sim.3781/abstract;jsessionid=5C98E7EAD10B98086592DF817A979D21.d03t03&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, that you cited yourself. However, it was too long to be a comment and someone who doesn't have access to on-line journals could be interested anyway).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-07T07:42:25.320" Id="27930" LastActivityDate="2012-05-07T07:42:25.320" OwnerUserId="9253" ParentId="27830" PostTypeId="2" Score="3" />
  
&#10;because of independence of the individual events we get
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The first two terms are zero (see above), so what remains is $2^{-5}\binom{2}{2}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;You get your probability by&lt;/p&gt;&#10;&#10;&lt;p&gt;P(exactly N sucesses) = P(exactly N successes + no chains of length X) + P(exactly N successes + chains of length X). The left hand is simply given by the binomial theorem.&lt;/p&gt;&#10;&#10;&lt;p&gt;....so the right-most probability:&#10;$$
  
  <row AnswerCount="3" Body="&lt;p&gt;I'm interested in &lt;a href=&quot;http://www.aiqus.com/upfiles/PFAlgo.png&quot; rel=&quot;nofollow&quot;&gt;the simple algorithm for particles filter given here&lt;/a&gt;. It seems very simple but I have no idea on how to do it practically. Any idea on how to implement it (just to better understand how it works) ?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&#10;&lt;a href=&quot;http://www.aiqus.com/questions/39942/very-simple-particle-filters-algorithm-sequential-monte-carlo-method-implementation?page=1#39950&quot; rel=&quot;nofollow&quot;&gt;This is a great simple example &lt;/a&gt; that explain how it works. I've tried to implement it in C++: &lt;a href=&quot;http://pastebin.com/M1q1HcN4&quot; rel=&quot;nofollow&quot;&gt;see my code here&lt;/a&gt;, but I'm note sure if I do it the right way. Can you please check if I understood it well, or there are some misunderstanding according to my code ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-07T12:17:30.130" FavoriteCount="1" Id="27946" LastActivityDate="2012-06-15T14:06:23.950" LastEditDate="2012-05-10T11:08:10.843" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="-2" Tags="&lt;monte-carlo&gt;&lt;sequential-analysis&gt;&lt;particle-filter&gt;" Title="Very simple particle filters algorithm (sequential monte carlo method) implementation" ViewCount="6392" />
  
  
  <row Body="&lt;p&gt;In the narrower context of facial analysis, your problem is called &lt;a href=&quot;http://en.wikipedia.org/wiki/Eigenface&quot; rel=&quot;nofollow&quot;&gt;eigenface analysis&lt;/a&gt;. Since PCA works with vectors, you have to vectorize each image matrix by concatenating all the rows or columns before proceeding. (&lt;a href=&quot;http://en.wikipedia.org/wiki/Tensor_decomposition&quot; rel=&quot;nofollow&quot;&gt;Tensor decomposition&lt;/a&gt; has been tried too, but don't worry about that since you're new to PCA.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The important thing to note is that the images must be standardized---much like a passport photo. If you're trying to compare wildly different images, you'll find that you need a large number of eigenvectors, indicating that dimension reduction is not feasible.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-05-07T18:33:07.423" Id="27968" LastActivityDate="2012-05-07T18:33:07.423" OwnerUserId="4479" ParentId="27961" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;$k$ is the length of $\beta$, the number of coefficients.  The penalty is a function of the coefficients, not of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, however, as a minor point, that you don't have to apply the penalty to all the coefficients in the model, although in your formulation you are.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-07T19:12:13.127" Id="27973" LastActivityDate="2012-05-07T19:12:13.127" OwnerUserId="7555" ParentId="27969" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;What that sentence means is you should generate $x_t^{(i)}$ from the distribution of $p(x_t|x^{(i)}_{t−1})$. &lt;/p&gt;&#10;&#10;&lt;p&gt;For an alternative introduction to &lt;a href=&quot;http://en.wikipedia.org/wiki/Particle_filter&quot; rel=&quot;nofollow&quot;&gt;particle filters&lt;/a&gt; I recommend &lt;a href=&quot;http://www.irisa.fr/aspi/legland/ref/cappe07a.pdf&quot; rel=&quot;nofollow&quot;&gt;An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;MATLAB has &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/?term=%22particle+filter%22&quot; rel=&quot;nofollow&quot;&gt;numerous toolboxes on particle filters&lt;/a&gt;. If you are working in C++, &lt;a href=&quot;http://www.jstatsoft.org/v30/i06/&quot; rel=&quot;nofollow&quot;&gt;here is an implementation&lt;/a&gt; you can use to compare your code with.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-07T22:11:45.300" Id="27979" LastActivityDate="2012-05-10T23:43:19.167" LastEditDate="2012-05-10T23:43:19.167" LastEditorUserId="4479" OwnerUserId="4479" ParentId="27946" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I haven't completely read all the answers but I do see they are pretty thorough and accurate. Running the risk that I am repeating something buried in the long answers I just want to say that v=the chi square test can be used for continuous data.  It may not be the best test and like many tests it relies on asymptotic theory and so may not be accurate in small samples with sparse cells (this depends also on how you do the binning).  Anderson-Darling is more powerful for testing normality than the K-S test but K-S may be better for other continuous distributions.  Lillefors has a test that is designed for exponential distributions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-07T22:26:15.773" Id="27983" LastActivityDate="2012-05-08T00:18:13.753" LastEditDate="2012-05-08T00:18:13.753" LastEditorUserId="11032" OwnerUserId="11032" ParentId="27958" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;@NRH's answer to this question gives a nice, simple proof of the biasedness of the sample standard deviation. Here I will explicitly calculate the expectation of the sample standard deviation (the original poster's second question) from a normally distributed sample, at which point the bias is clear. &lt;/p&gt;&#10;&#10;&lt;p&gt;The unbiased sample variance of a set of points $x_1, ..., x_n$ is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ s^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2 $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;If the $x_i$'s are normally distributed, it is a fact that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{(n-1)s^2}{\sigma^2} \sim \chi^{2}_{n-1} $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\sigma^2$ is the true variance. The $\chi^2_{k}$ distribution has probability density &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ p(x) = \frac{(1/2)^{k/2}}{\Gamma(k/2)} x^{k/2 - 1}e^{-x/2} $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;using this we can derive the expected value of $s$; &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{align} E(s) &amp;amp;= \sqrt{\frac{\sigma^2}{n-1}} E \left( \sqrt{\frac{s^2(n-1)}{\sigma^2}} \right) \\
&#10;&amp;amp;= \sqrt{\frac{\sigma^2}{n-1}}
&#10;\int_{0}^{\infty}
  <row Body="&lt;p&gt;I think you mean the number of draws until $X$ occurs again. The probability distribution is the geometric distribution.  The probability that after $X$ occurs it will occur again on the next draw is $p=1/N$.  So Let $k= \text{number of draws until the next } $X$ \text{ is drawn}$.  For any $k\geq 1$ the probability that the random variable $M=k$ is $(1-p)^{k-1} p$ for $k=1,2,3,...,\infty$, where $p=1/N$.  The reason is that each draw is independent and for $M$ to be equal to $k$, the first $k-1$ draws must be different from $X$ and $X$ must occur on the $k$th draw.  So the mean is $m = \sum k p^{k-1} p$ and the variance is $\sum(k-m)^2 p^{k-1} p$. It is known that $m=1/p=N$ and the variance is $(1-p)/p^2$ and the standard deviation = $\sqrt{1-p}/p=\sqrt{(N-1)N}$&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-05-08T05:20:15.023" Id="27997" LastActivityDate="2012-05-08T14:43:29.627" LastEditDate="2012-05-08T14:43:29.627" LastEditorUserId="11032" OwnerUserId="11032" ParentId="27994" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;For those who may be interested, I have found a paper that proposes a solution to my problem: &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0096300383900322&quot; rel=&quot;nofollow&quot;&gt;&quot;A min-max algorithm for non-linear regression models&quot;&lt;/a&gt;, by A. Tishler and I. Zang.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tested it myself, and I get the results I need.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-08T13:23:13.837" Id="28015" LastActivityDate="2012-08-09T01:07:56.427" LastEditDate="2012-08-09T01:07:56.427" LastEditorUserId="3826" OwnerUserId="10236" ParentId="25538" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;em&gt;Disclaimer:&lt;/em&gt; I am only remotely familiar with the work on model selection by David F. Hendry among others. I know, however, from respected colleagues that Hendry has done very interesting progress on model selection problems within econometrics. To judge whether the  statistical literature is not paying enough attention to his work on model selection would require a lot more work for my part. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is, however, interesting to try to understand why one method or idea generates much more activity than others. No doubt that there are aspects of fashion in science too. As I see it, lasso (and friends) has one major advantage of being the solution of a very easily expressed optimization problem. This is key to the detailed theoretical understanding of the solution and the efficient algorithms developed. The recent book, &lt;a href=&quot;http://books.google.dk/books/about/Statistics_for_High_Dimensional_Data.html?id=S6jYXmh988UC&amp;amp;redir_esc=y&quot; rel=&quot;nofollow&quot;&gt;Statistics for High-Dimensional Data&lt;/a&gt; by Bühlmann and Van De Geer, illustrates how much is already known about lasso. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can do endless simulation studies and you can, of course, apply the methods you find most relevant and suitable for a particular application, but for parts of the statistical literature substantial theoretical results must also be obtained. That lasso has generated a lot of activity reflects that there are theoretical questions that can actually be approached and they have interesting solutions.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Another point is that lasso or variations &lt;em&gt;do&lt;/em&gt; perform well in many cases. I am simply not convinced that it is correct that lasso is so easily outperformed by other methods as the OP suggests. Maybe in terms of (artificial) model selection but not in terms of predictive performance. None of the references mentioned seem to really compare Gets and lasso either.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-08T15:05:57.617" Id="28022" LastActivityDate="2012-05-08T15:05:57.617" OwnerUserId="4376" ParentId="22454" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose you have some sample of huge numbers and you want to fit some continuous distribution to these numbers. You will get some distribution that is loosely speaking highly &quot;smeared&quot;, e.g. it has a huge scale factor. For numerical purposes this might be very inconvenient.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now what happens if one rescales the data and then fits the distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;In case of scale invariant distributions there should be no difference. You just scale the distribution again.&lt;/p&gt;&#10;&#10;&lt;p&gt;How about distributions that are not scale invariant? Intuitively, I'd say it is hazardous to scale and then fit. One might introduce a serious error if you compute e.g. a quantile from the resulting distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;So how does one deal with such huge data - is scaling unfavorable?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-05-08T17:46:57.760" Id="28040" LastActivityDate="2012-05-08T17:46:57.760" OwnerUserId="8809" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;estimation&gt;&lt;scale-invariance&gt;" Title="Scaling of huge data and estimating the distribution" ViewCount="59" />
  <row Body="&lt;p&gt;Ties in hierarchical clustering are common, in particular when you have discrete data, duplicate records and such. If you run hierarchical clustering with Manhattan distance on the classic iris data set, you will have plenty of ties.&#10;Real data just is not a smooth as artificially generated data with 20+ digits of precision. Real data just has ties.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not understand why SAS considers this to be worth a warning. But SAS clustering capabilities are very outdated anyway.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-08T19:12:29.163" Id="28050" LastActivityDate="2012-05-08T19:12:29.163" OwnerUserId="7828" ParentId="28044" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;Suppose I have $k$ 'experts', from whom I would like to elicit a prior distribution on some variable $X$. I would like to motivate them with &lt;em&gt;real money&lt;/em&gt;. The idea is to elicit the priors, observe $n$ realizations of the random variable $X$, then divvy up some predetermined 'purse' among the experts based on how well their priors match the evidence. What are suggested methods for this last part, mapping the priors and evidence onto a payout vector?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-09T00:44:16.893" FavoriteCount="1" Id="28068" LastActivityDate="2012-05-15T10:16:34.663" OwnerUserId="795" PostTypeId="1" Score="7" Tags="&lt;bayesian&gt;&lt;prior&gt;" Title="Eliciting priors ... with money!" ViewCount="210" />
  
  <row AcceptedAnswerId="28313" AnswerCount="4" Body="&lt;p&gt;Here are four different sets of numbers:&lt;/p&gt;&#10;&#10;&lt;p&gt;A = {95.47, 87.90, 99.00}&lt;br&gt;&#10;B = {79.2, 75.3, 66.3}&lt;br&gt;&#10;C = {38.4, 40.4, 32.8}&lt;br&gt;&#10;D = {1.8, 1.2, 1.1}  &lt;/p&gt;&#10;&#10;&lt;p&gt;Using a two-sample t-test without assuming equal variances, I compare B, C, and D to A and get the following p-values:&lt;/p&gt;&#10;&#10;&lt;p&gt;0.015827  (A vs B)&lt;br&gt;&#10;0.000283  (A vs C)&lt;br&gt;&#10;0.001190  (A vs D)  &lt;/p&gt;&#10;&#10;&lt;p&gt;I find it strange that the p-value from the A-D test is worse than the A-C test: the difference between the means is clearly much bigger AND the variance of D is much lower than the variance of C. Intuitively (at least for my intuition), both these facts should drive the p-value lower.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone explain if this is a desired or expected behavior of the t-test or whether it has to do more with my particular data set (extreme low sample size perhaps?). Is the t-test inappropriate for this particular set of data?&lt;/p&gt;&#10;&#10;&lt;p&gt;From a purely computational point of view, the reason for a worse p-value seems to be the degrees of freedom, which in the A-D comparison is 2.018 while it is 3.566 in the A-C comparison. But surely, if you just saw those numbers, wouldn't you think that there is stronger evidence for rejecting the null hypothesis in the A-D case compared to A-C?&lt;/p&gt;&#10;&#10;&lt;p&gt;Some might suggest that this is not a problem here since all p-values are quite low anyway. My problem is that these 3 tests are part of a suite of tests that I am performing. After correcting for multiple testing, the A-D comparison doesn't make the cut, while the A-C comparison does. Imagine plotting those numbers (say bar-plots with error bars as biologists often do) and trying to justify why C is significantly different from A but D is not... well, I can't.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update: why this is really important&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me clarify why this observation could have a great impact on interpreting past studies. In bioinfomatics, I have seen the t-test be applied to small sample sizes on a large scale (think differential gene expression of hundreds or thousands of genes, or the effect of many different drugs on a cell line, using only 3-5 replicates). The usual procedure is to do many t-tests (one for each gene or drug) followed by multiple testing correction, usually FDR. Given the above observation of the behaviour of Welch's t-test, this means that some of the very best cases are being systematically filtered out. Although most people will look at the actual data for the comparisons at the top of their list (the ones with best p-values), I don't know of anyone who will look through the list of all comparisons where the null hypothesis wasn't rejected.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-09T04:35:10.870" Id="28077" LastActivityDate="2012-05-12T00:23:01.617" LastEditDate="2012-05-09T20:16:06.687" LastEditorUserId="11195" OwnerUserId="11195" PostTypeId="1" Score="6" Tags="&lt;t-test&gt;" Title="Welch's t-test gives worse p-value for more extreme difference" ViewCount="3957" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Several years ago I read about a contest to generate an algorithm to predict the outcome of a college football game. The contest provided a comprehensive data set of games from previous years. I didn't get the data set at the time and as a college football fan and amateur statistician I wish I had.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is anybody aware of a data set or source like this?&lt;/p&gt;&#10;" ClosedDate="2012-05-09T16:46:29.030" CommentCount="11" CreationDate="2012-05-09T15:02:41.823" Id="28109" LastActivityDate="2012-05-09T15:02:41.823" OwnerUserId="11203" PostTypeId="1" Score="1" Tags="&lt;dataset&gt;" Title="College Football Dataset" ViewCount="309" />
  <row Body="&lt;p&gt;See if something like this works:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;proc mcmc&#10; data = /* your data here */&#10; nmc = 20000, nbi = 1000; /* iterations and burn-in */&#10;&#10; /* Specify parameters -- separate statements means they're proposed separately */&#10; parms a;&#10; parms b;&#10; parms theta;&#10;&#10; /* Specify priors */&#10; hyper a ~ gamma(1.5, scale = .25);&#10; hyper b ~ gamma(1.5, scale = .25); &#10; prior theta ~ gamma(a, scale = b);&#10;&#10; /* Sampling model */&#10; lam = theta*time;&#10; model fail ~ poisson(lam);&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-05-09T19:23:56.853" Id="28124" LastActivityDate="2012-05-10T13:25:32.333" LastEditDate="2012-05-10T13:25:32.333" LastEditorUserId="24779" OwnerUserId="24779" ParentId="25400" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;I think the author meant $P(0&amp;lt;Z&amp;lt;z) = \alpha/2$ when $P(-z&amp;lt;Z&amp;lt;z) = \alpha$ by symmetry whereas @Néstor has taken the interval to be from $-\infty$ to $z$ which would be $(1+\alpha)/2$.&#10;The symmetry argument is that $P(-z&amp;lt;Z&amp;lt;z) = \alpha = P(-z&amp;lt;Z&amp;lt;0) + P(0&amp;lt;Z&amp;lt;z)$ and since by symmetry $P(-z&amp;lt;Z&amp;lt;0) = P(0&amp;lt;Z&amp;lt;z)$ so $\alpha = 2 P(0&amp;lt;Z&amp;lt;z)$ or $P(0&amp;lt;Z&amp;lt;z) = \alpha/2$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-10T03:47:02.077" Id="28148" LastActivityDate="2012-05-10T07:42:52.673" LastEditDate="2012-05-10T07:42:52.673" LastEditorDisplayName="user10525" OwnerUserId="11032" ParentId="28146" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You want to compare the distributions of the samples. If the border is distinct, using a single hue dimension with a Kolmogorov-Smirnov test might suffice. If hue alone does not provide enough separation you'll have to add another dimension. If you want to stick to &lt;a href=&quot;http://en.wikipedia.org/wiki/Chromaticity&quot; rel=&quot;nofollow&quot;&gt;chromaticity&lt;/a&gt; (color without the brightness dimension), I would suggest the a* and b* color-opponent dimensions in the &lt;a href=&quot;http://en.wikipedia.org/wiki/CIELAB&quot; rel=&quot;nofollow&quot;&gt;CIELAB&lt;/a&gt; color model. With two dimensions you can apply &lt;a href=&quot;http://v-scheiner.brunel.ac.uk/bitstream/2438/1166/1/acat2007.pdf&quot; rel=&quot;nofollow&quot;&gt;this KS test&lt;/a&gt;. If you want to go whole hog and use all three dimensions, look &lt;a href=&quot;http://www.subcortex.net/research/code/testing-for-differences-in-multidimensional-distributions&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-10T04:33:44.683" Id="28153" LastActivityDate="2012-05-10T04:33:44.683" OwnerUserId="4479" ParentId="28138" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;What are alternative measures of the prediction power of a model, apart from the coefficient of determination $R^2$? What are their strengths and weaknesses, especially in comparison to the $R^2$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-10T08:33:32.010" Id="28165" LastActivityDate="2012-10-08T11:34:06.613" OwnerUserId="10106" PostTypeId="1" Score="1" Tags="&lt;modeling&gt;&lt;power&gt;&lt;prediction&gt;" Title="Alternative measures of prediction power?" ViewCount="160" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to run a regression from a 11300x21500 observation matrix (where there are 11300 observations and 21500 independent variables). However, when I try to implement the usual  $(X^T X)^{-1}$ formula in C++, I can't even initialize the observation matrix $X$ due to memory limitations (The application closes itself when the memory usage is around 1 GB. I have 8 GB memory but I think there is an OS command to limit memory usage of each application). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any other technique which is more suitable for such a regression? Can there be any way to overcome this situation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-05-10T12:31:12.087" Id="28177" LastActivityDate="2012-05-10T12:41:59.323" LastEditDate="2012-05-10T12:41:59.323" LastEditorUserId="2970" OwnerUserId="11230" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;large-data&gt;&lt;matrix&gt;" Title="Calculating Regression Coefficients for Very Large Observation Matrices" ViewCount="123" />
  
  <row Body="&lt;p&gt;K-means obviously doesn't make any sense, as it computes means (which are nonsensical). Same goes for GMM.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to try distance-based clustering algorithms with appropriate distance functions, for example DBSCAN.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main challenge is to find a distance function!&lt;/p&gt;&#10;&#10;&lt;p&gt;While you could put a different distance function into k-means, it will still compute the mean which probably doesn't make much sense (and probably messes with a distance function for discrete values).&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, first focus on &lt;strong&gt;define what &quot;similar&quot; is&lt;/strong&gt;. Then cluster using this definition of similar!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-10T16:14:55.800" Id="28189" LastActivityDate="2012-05-10T16:14:55.800" OwnerUserId="7828" ParentId="28170" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I don't know the SVM lingo but it sounds like you want to do kernel discrimnation. D. J. Hand has a good book on it.  Basically you use the kernel method to estimate the class conditional densities.  Then you apply the Bayes rule as if the estimated densities were the actual ones.  What is best with respect to a kernel function is not usually important although there are assumptions that lead to a theoretical best kernel.  But what really matters is the bandwidth.  However it is sometimes hard to tell what is too small leading to a density that is too rough versus too large which leads to a density that is too smooth.  The book that I think provides the clearest explanation of probability density estimation is Bernie Silverman's book.  Check it out if you don't follow what I ahve told you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-10T21:06:14.367" Id="28207" LastActivityDate="2012-05-10T21:06:14.367" OwnerUserId="11032" ParentId="28183" PostTypeId="2" Score="1" />
  
  <row Body="A discrete distribution defined on the non-negative integers that has the property that the mean is equal to the variance. " CommentCount="0" CreationDate="2012-05-10T22:12:31.060" Id="28212" LastActivityDate="2012-05-10T22:18:39.803" LastEditDate="2012-05-10T22:18:39.803" LastEditorUserId="4856" OwnerUserId="4856" PostTypeId="4" Score="0" />
  <row Body="" CommentCount="0" CreationDate="2012-05-10T22:14:30.310" Id="28215" LastActivityDate="2012-05-10T22:14:30.310" LastEditDate="2012-05-10T22:14:30.310" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;The issue becomes less contentious if you state the facts[1]. After all, all multivariate robust estimation procedures have at their core an outlier detection algorithm and all will in some form or another output a list of suspect observations. Stated otherwise, given a robust fit, identifying outliers is in principle not an issue. &lt;/p&gt;&#10;&#10;&lt;p&gt;The main difference between robust estimation approaches and the testing approaches (Dixon, Grubbs) is that the latter can sustain at most a single outlier. In contrast, most state of the art robust estimation procedures have been designed to handle nearly 50% contamination (they can in principle be tuned to handle anywhere between 0 and nearly 50 percent outliers trading off robustness for computational costs).&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Rousseeuw P. J. and Van Zomeren B. C., &#10;Unmasking Multivariate Outliers and Leverage Points.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-10T22:31:19.150" Id="28221" LastActivityDate="2012-05-10T22:31:19.150" OwnerUserId="603" ParentId="28180" PostTypeId="2" Score="3" />
  <row Body="Refers to a general estimation technique that selects the parameter value to minimize the squared difference between two quantities, such as the observed value of a variable, and the expected value of that observation conditioned on the parameter value. Gaussian linear models are fit by least squares and least squares is the idea underlying the use of mean-squared-error (MSE) as a way of evaluating an estimator. " CommentCount="0" CreationDate="2012-05-10T23:13:36.023" Id="28226" LastActivityDate="2012-05-10T23:21:32.503" LastEditDate="2012-05-10T23:21:32.503" LastEditorUserId="4856" OwnerUserId="4856" PostTypeId="4" Score="0" />
  
  
  <row Body="&lt;p&gt;Please note that there is a difference between the kernel questioned here and the one in the answer above. See &lt;a href=&quot;http://stats.stackexchange.com/a/1433/7603&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The best &lt;em&gt;and first&lt;/em&gt; resource on the kernels in question is &quot;Theory of Reproducing Kernels&quot; by Nachman Aronszajn. You can also look at Grace Wahba's book: &quot;Spline Models for Observational Data&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A reproducing kernel uniquely identifies a &lt;a href=&quot;http://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space&quot; rel=&quot;nofollow&quot;&gt;reproducing kernel Hilbert space&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-11T03:55:30.883" Id="28243" LastActivityDate="2012-05-11T03:55:30.883" OwnerUserId="7603" ParentId="28183" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="28307" AnswerCount="1" Body="&lt;p&gt;What is the distribution of the following monomial?&#10;$$X^a \cdot Y^b$$&#10;where $X$ and $Y$ are normal random variables and $a$ and $b$ are natural numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, when $X \sim N(0,1)$, $a=2$, and $b=0$ it is a Chi-squared distribution, which has a variance of 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;What if we have $n$ independent variables $X_1, X_2, \dots , X_n$, with $X_i \sim N(0,\sigma^2)$ and some natural numbers $p_1, p_2, \dots,p_n$. What can we say about the variance of the following r.v.?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_1^{p_1} \cdot X_2^{p_2} \cdots X_n^{p_n}$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-11T05:45:05.050" Id="28245" LastActivityDate="2012-05-11T20:15:17.700" LastEditDate="2012-05-11T07:10:37.173" LastEditorUserId="2719" OwnerUserId="5725" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;&lt;variance&gt;&lt;polynomial&gt;" Title="Monomial distribution of $X^a \cdot Y^b$ " ViewCount="108" />
  
  
&#10;\end{cases}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So I reckon this is a case of left censoring?&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is how can I compute the marginal effect at $X_1=10$ in this setting ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I found a formula which said that the $\text{marg effect} = \beta_j*\Phi(\frac{X\beta}{ \sigma})$, but I have no clue how to find $\sigma$..?&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope you can help :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-11T11:39:57.010" Id="28253" LastActivityDate="2013-05-07T08:59:48.287" LastEditDate="2012-06-10T22:46:31.740" LastEditorUserId="4856" OwnerUserId="11245" PostTypeId="1" Score="1" Tags="&lt;marginal&gt;&lt;tobit-regression&gt;" Title="Computation of $\sigma$ from $\beta$ coefficients in data censoring regression" ViewCount="215" />
  <row Body="&lt;p&gt;Entropy is defined for probability distributions. When you do not have one, but only data, and plug in a naive estimator of the probability distribution, you get empirical entropy. This is easiest for discrete (multinomial) distributions, as shown in another answer, but can also be done for other distributions by binning, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;A problem with empirical entropy is that it is biased for small samples. The naive estimate of the probability distribution shows extra variation due to sampling noise. Of course one can use a better estimator, e.g., a suitable prior for the multinomial parameters, but getting it really unbiased is not easy. &lt;/p&gt;&#10;&#10;&lt;p&gt;The above applies to conditional distributions as well. In addition, everything is relative to binning (or kernelization), so you actually have a kind of differential entropy. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-11T12:26:47.080" Id="28256" LastActivityDate="2012-05-11T12:26:47.080" OwnerUserId="11052" ParentId="28178" PostTypeId="2" Score="3" />
  
  
  <row Body="" CommentCount="0" CreationDate="2012-05-11T13:17:46.703" Id="28267" LastActivityDate="2012-05-11T13:17:46.703" LastEditDate="2012-05-11T13:17:46.703" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;I suggest using RMSE (root mean square error) of your predictions on your test set when compared to the actual value. This is a standard method of reporting prediction error of a continuous variable.&#10;You can use R^2 to examine how well your model fits the training data.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-11T13:28:49.920" Id="28274" LastActivityDate="2012-05-11T13:28:49.920" OwnerUserId="11030" ParentId="28250" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The &lt;code&gt;str&lt;/code&gt; function will show you the structure of any object, including the results from &lt;code&gt;aov&lt;/code&gt;.  Some values of interest (such as p-values) are not in the aov object, but in the summary object from &lt;code&gt;summary(aov.object)&lt;/code&gt; (run &lt;code&gt;str&lt;/code&gt; on that as well).  Some statistics will be a column in a matrix (the p-value is a column in the coefficient matrix of the summary object).&lt;/p&gt;&#10;&#10;&lt;p&gt;The aov object has an additional complexity in that it can be a list of aov objects (due to possible nesting structures, each level of nesting producing an object in the list).  You might want to examine the code of other functions that use aov objects to see how they access the parts and what they do with them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-11T17:29:40.800" Id="28295" LastActivityDate="2012-05-11T17:29:40.800" OwnerUserId="4505" ParentId="28242" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I could well be wrong about this, but...I think I would take the square root, yielding eta, which is akin to multiple R.  Then I would use the standard method for finding a confidence interval for a correlation, which involves transforming to a Fisher's &lt;em&gt;Z&lt;/em&gt;, obtaining upper and lower confidence limits based on &lt;em&gt;Z&lt;/em&gt; and &lt;em&gt;N&lt;/em&gt;, and transforming back.  Then I'd square these limits to get my upper and lower limits for eta-squared. (One online calculator that could help is &lt;a href=&quot;http://vassarstats.net/tabs.html#fisher&quot;&gt;Vassar's&lt;/a&gt;.)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-11T19:40:39.207" Id="28306" LastActivityDate="2012-05-11T19:40:39.207" OwnerUserId="2669" ParentId="28247" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;To strictly enforce the axis limits, include this in your plot call: &lt;code&gt;xaxs=&quot;i&quot;, yaxs=&quot;i&quot;&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Check &lt;code&gt;?par&lt;/code&gt; for lots of other plotting options. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I find this page really helpful for base graphics customization: &lt;a href=&quot;http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter3.html&quot;&gt;http://www.stat.auckland.ac.nz/~paul/RGraphics/chapter3.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-12T02:17:13.460" Id="28320" LastActivityDate="2012-05-12T02:17:13.460" OwnerUserId="3546" ParentId="28318" PostTypeId="2" Score="7" />
  <row Body="Refers to general procedures that attempt to determine the generalizability of a statistical result. Cross-validation arises frequently in the context of assessing how a particular model fit predicts future observations. Methods for cross-validation usually involve withholding a random subset of the data during model fitting and quantifying how accurate the withheld data are predicted and repeating this process to get a measure of prediction accuracy." CommentCount="0" CreationDate="2012-05-12T05:15:06.010" Id="28325" LastActivityDate="2012-05-12T07:47:54.137" LastEditDate="2012-05-12T07:47:54.137" LastEditorUserId="4856" OwnerUserId="4856" PostTypeId="4" Score="0" />
  
  <row Body="" CommentCount="0" CreationDate="2012-05-12T05:40:19.270" Id="28335" LastActivityDate="2012-05-12T05:40:19.270" LastEditDate="2012-05-12T05:40:19.270" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;In a gradient descent algorithm, the algorithm proceeds by finding a direction along which you can find the optimal solution. The optimal direction turns out to be the gradient. However, since we are only interested in the direction and not necessarily how far we move along that direction, we are usually not interested in the magnitude of the gradient. Thereby, normalized gradient is good enough for our purposes and we let $\eta$ dictate how far we want to move in the computed direction. However, if you use unnormalized gradient descent, then at any point, the distance you move in the optimal direction is dictated by the magnitude of the gradient (in essence dictated by the surface of the objective function i.e a point on a steep surface will have high magnitude whereas a point on the fairly flat surface will have low magnitude).&lt;/p&gt;&#10;&#10;&lt;p&gt;From the above, you might have realized that normalization of gradient is an added controlling power that you get (whether it is useful or not is something upto your specific application). What I mean by the above is: &lt;br /&gt;&#10;1] If you want to ensure that your algorithm moves in fixed step sizes in every iteration, then you might want to use normalized gradient descent with fixed $\eta$. &lt;br /&gt;&#10;2] If you want to ensure that your algorithm moves in step sizes which is dictated precisely by you, then again you may want to use normalized gradient descent with your specific function for step size encoded into $\eta$. &lt;br /&gt;&#10;3] If you want to let the magnitude of the gradient dictate the step size, then you will use unnormalized gradient descent. &#10;There are several other variants like you can let the magnitude of the gradient decide the step size, but you put a cap on it and so on. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, step size clearly has influence on the speed of convergence and stability. Which of the above step sizes works best depends purely on your application (i.e objective function). In certain cases, the relationship between speed of convergence, stability and step size can be analyzed. This relationship then may give a hint as to whether you would want to go with normalized or unnormalized gradient descent.&lt;/p&gt;&#10;&#10;&lt;p&gt;To summarize, there is no difference between normalized and unnormalized gradient descent (as far as the theory behind the algorithm goes). However, it has practical impact on the speed of convergence and stability. The choice of one over the other is purely based on the application/objective at hand.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-12T12:20:53.313" Id="28345" LastActivityDate="2012-05-12T12:20:53.313" OwnerUserId="10823" ParentId="22568" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The above answer may be useful but is complicated and I am not sure how to apply it. Thinking in simple terms this is a special case of testing whether or not a person has psychic powers (really no different from a scientific evaluation of the mechanics and physics of the coin flip).  Obviously have psychic capability would have to be defined as doing something much better than by random chance.  I would first want to define what is enough.  The difficulty is to decide how much better than random guessing is and what random guessing will do.  If all the coins were fair random choice would be 0.5 and so maybe saying anything over 0.75 is  Then testing the hypothesis that the person has psychic powers is doing a one-sided hypothesis that a binomial parameter p is &amp;lt;= to 0.75 versus the alternative that it is greater.  As an estimator I have chosen the binomial parameter for successfullu calling heads or tails and the variance of my estimator is p(1-p)/n. The added difficulty is that the coins are not fair and the individual pis are unknown. I would still define chance as random guessing of heads or tails and 0.5 is what I would test against.  However with unfair coins there may be statistical strategies that would lead to a better than chance success rate but neither indicate skill with the individual coin flips or psychic powers.  To illustrate suppose the average for the pis is 0.80.  Then after seing heads come up much more frequently than tails we could switch to an all heads strategy and tend to be correct close to 80% of the time.  This assumes randomguessing until we are convinced that head occurs much more often than tails and at that point we switch to all heads.  So without knowing the pis or at least their averages I cannot tell what success rate would indicate skill.  Comparing against random guessing is not the standard to beat in this case. Note that my argument only makes sense if the stack of coins is very large.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-12T21:02:29.123" Id="28367" LastActivityDate="2012-05-12T21:02:29.123" OwnerUserId="11032" ParentId="28338" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Cosine similarity is not a clustering technique. It's a common distance measure for sparse vectors all over the place, in information retrieval and classification maybe even more than in clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not have the impression that you really have understood clustering. It is an unsupervised knowledge discovery technique. As it is unsupervised, you cannot &quot;direct&quot; it towards building a &quot;sports&quot; and a &quot;non-sports&quot; cluster. It might just as well find an &quot;Obama&quot; cluster and a &quot;non-Obama&quot; cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are interested in Sports as opposed to non-Sports, you are doing classification. And yes, you may use cosine distance in classification!&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt; is when you want to assign instances the appropriate class of your known types.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt; is when you have no clue of what types there are, and you want an algorithm to &lt;em&gt;discover&lt;/em&gt; what (if any!) types there might be. This may involve a lot of trial and error, as the algorithms may find clusters that are not interesting to you.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A clustering algorithm &lt;em&gt;may&lt;/em&gt; find clusters such as &quot;Sentences containing the word Banana&quot; (most likely it will not give you this explanation though!), and it hasn't failed. It's a &lt;em&gt;mathematically valid cluster&lt;/em&gt;, and how is the algorithm supposed to know that you don't like Bananas?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-13T07:56:54.493" Id="28387" LastActivityDate="2012-05-13T07:56:54.493" OwnerUserId="7828" ParentId="28385" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I've run a Tobit regression but I'm not sure of how I should interpret the coefficients. Do you have any suggestions? Thank you very much!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-13T12:15:01.690" Id="28395" LastActivityDate="2012-05-13T12:15:01.690" OwnerUserId="11289" PostTypeId="1" Score="0" Tags="&lt;tobit-regression&gt;" Title="How to interpret coefficients in a Tobit regression?" ViewCount="3841" />
  <row Body="&lt;p&gt;If I understand correctly the main problem is that you only have 10 observations for the time-dependent IV and you can only use the same or previous year observations in the model, relative to the DV.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you must use the time-dependent IV (I read you have other IVs?) and the goal is forecasting, one approach is to focus on the last year data and run &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471822906&quot; rel=&quot;nofollow&quot;&gt;simulations&lt;/a&gt; given the probability distribution function of the rare variable. Of course, you're making lots of assumptions about the behavior of the IV over time, but you can't possibly get something from nothing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have access to the methodology used to calculate this IV? Can you reconstruct it based on some other data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-13T14:07:14.523" Id="28403" LastActivityDate="2012-05-13T14:07:14.523" OwnerUserId="7795" ParentId="28298" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="28475" AnswerCount="1" Body="&lt;p&gt;I'd like to have your feedback to analyze my data from the following experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Experiment Information&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A total of 30 (15 from one city and other 15 from another city) slaughtered animals were sampled.&lt;/li&gt;&#10;&lt;li&gt;Three tissues (Muscle, Kidney and Liver) were collected from each sampled slaughtered animal.&lt;/li&gt;&#10;&lt;li&gt;Each sample was analyzed for the presence of three drugs and each drug amount was measured. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The objectives of the experiment are to compare the level of drugs between&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Types of drugs&lt;/li&gt;&#10;&lt;li&gt;Cities&lt;/li&gt;&#10;&lt;li&gt;Tissues. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your time and help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-13T17:26:25.960" Id="28412" LastActivityDate="2012-05-14T18:06:44.330" LastEditDate="2012-05-14T18:06:44.330" LastEditorUserId="7290" OwnerUserId="3903" PostTypeId="1" Score="0" Tags="&lt;data-transformation&gt;&lt;experiment-design&gt;&lt;biostatistics&gt;&lt;manova&gt;&lt;observational-study&gt;" Title="What statistical test should be used to accomplish the objectives of the experiment" ViewCount="176" />
  
  <row Body="&lt;p&gt;The first decision you have you have to make is what to control for? Is it the probability of making any false discovery (FWE control) or the proportion of false discoveries (FDR control). &#10;Bonferroni will control for both, but is very conservative. At 100 tests, you might lose much power. &#10;I would have a look at &lt;a href=&quot;http://en.wikipedia.org/wiki/False_discovery_rate#Independent_tests&quot; rel=&quot;nofollow&quot;&gt;Benjamini-Hochberg&lt;/a&gt; procedure for FDR control under mild dependence assumptions or the more general (and less powerful) &lt;a href=&quot;http://en.wikipedia.org/wiki/False_discovery_rate#Dependent_tests&quot; rel=&quot;nofollow&quot;&gt;Benjamini-Yekutieli&lt;/a&gt; procedure. I would look into &lt;a href=&quot;http://en.wikipedia.org/wiki/Bonferroni_correction#Holm-Bonferroni_method&quot; rel=&quot;nofollow&quot;&gt;Holm&lt;/a&gt; procedure for FWE control. &#10;I am not familiar with Statistica, but luckily, all the mentioned procedures, take p-values as  inputs and are easily computable &quot;manually&quot;. If you are familiar with R, have a look at the p.adjust function. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-13T19:44:45.773" Id="28417" LastActivityDate="2012-05-13T19:44:45.773" OwnerUserId="6961" ParentId="18849" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="28457" AnswerCount="2" Body="&lt;p&gt;Before it is pointed, I am aware that a &lt;a href=&quot;http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi&quot;&gt;very similar question was already asked&lt;/a&gt;. Still, I am in doubt regarding the concept.&lt;/p&gt;&#10;&#10;&lt;p&gt;More specifically, it is mentioned by the most voted answer that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In terms of a &lt;strong&gt;simple rule of thumb&lt;/strong&gt;, I'd suggest that you:&lt;/p&gt;&#10;  &#10;  &lt;ol&gt;&#10;  &lt;li&gt;&lt;p&gt;Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.&lt;/p&gt;&lt;/li&gt;&#10;  &lt;li&gt;&lt;p&gt;Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables. &lt;/p&gt;&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 1:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am having difficulties on understanding based on the results I obtained from R where exactly I am inputing my &lt;em&gt;theoretical model of latent factors&lt;/em&gt;. I am using the functions from &lt;a href=&quot;http://www.statmethods.net/advstats/factor.html&quot; rel=&quot;nofollow&quot;&gt;statsmethods&lt;/a&gt;. On both &lt;strong&gt;factanal()&lt;/strong&gt; and &lt;strong&gt;princomp()&lt;/strong&gt; the inputs were the same: A table where each row represented one data point and the columns consisted of different attributes I was interested on reducing. Thus, this add to my confusion on where is this pre assumed model play its role. I noticed that for factor analysis function I used parallel analysis also suggested by the site using the nScree() function to determine the number of factors and I specified if I wanted a varimax (orthogonal) or promax (oblique) rotation. Is that what is it mean by the model? Being able to choose the amount of factors and the type of rotation?&lt;/p&gt;&#10;&#10;&lt;p&gt;The results being provided as visual graphs for both PCA and EFA also doesn't seem to highlight this difference which adds to my confusion. Where does this distinction can be observed on them?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SGK56.jpg&quot; alt=&quot;PCA&quot;&gt;&#10;PCA &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Eg4MN.jpg&quot; alt=&quot;EFA&quot;&gt;&#10;EFA&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 2:&lt;/strong&gt; -- Answered&lt;/p&gt;&#10;&#10;&lt;p&gt;I bought a book to study about this from Richard L. Gorsuch. On this book there is something that the author caught attention on the difference between PCA (Principal Component Analysis) and EFA (Exploratory Factor Analysis): It is mentioned that PCA is for &lt;strong&gt;population&lt;/strong&gt; while EFA is for &lt;strong&gt;sample&lt;/strong&gt;. Is that true? I didn't see that being mentioned on any discussion I read so far. Is it irrelevant? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 3:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I noticed that all those methods seems to impose the normal distribution constraint. I also read that for larger sets this constraint can be ignored. Is that true or PCA, EFA and CFA are sensible to distribution constraint violations?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 4:&lt;/strong&gt; Where from the results of PCA and EFA should I note that one is talking about latent factors (EFA) and the other is just clustering on components (factors) the variables? The outputs from R looks the same to me. Is it just the way I perceive what the factors being shown as output? I noted that both show me the table where I can see which I can observe which of my variables are expressed the most of my factors. &lt;strong&gt;What is the difference on the interpretation I should have on which variable belongs to which factor&lt;/strong&gt; in respect to PCA and EFA? EFA is saying those with higher expression seems to be more explained by that latent factor while PCA is trying to say that factor is holding those variables from what is it observed?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 5&lt;/strong&gt;&#10;Finally the last question is regarding CFA (Confirmatory Factor Analysis). &lt;/p&gt;&#10;&#10;&lt;p&gt;On the same function website the following image is being shown:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/NNrvh.gif&quot; alt=&quot;Confirmatory Factor Analysis&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I read that CFA is usually followed after EFA for hypothesis testing. In that sense, EFA tells you which are the latent factors (which are the output factors) and then you use CFA assuming those factors you observed from EFA for hypothesis testing? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 6&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;For EFA one of the available rotations on the literature is direct oblimium. I heard that it can accounts for both promax and varimax so 'it takes the best of two words'. Is that true? I am also trying to find a function that employs them on R, since the one suggested on the site does not. I would be happy to get any suggestion on this one.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I hope it is noted that this question is way more specific on the doubts regarding EFA and PCA and also adds to CFA so not to get closed for being repeated on the subject. If at least one of the questions is answered I am more than happy too as to clarify the confusion in my head.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-05-14T05:55:32.040" FavoriteCount="3" Id="28437" LastActivityDate="2013-10-23T13:11:02.900" LastEditDate="2012-05-18T19:22:13.883" LastEditorUserId="9887" OwnerUserId="9887" PostTypeId="1" Score="2" Tags="&lt;pca&gt;&lt;factor-analysis&gt;&lt;confirmatory-factor&gt;" Title="Differences on exploratory factor analysis, confirmatory factor analysis and principal component analysis" ViewCount="1279" />
  
  <row Body="&lt;h2&gt;Collinearity&lt;/h2&gt;&#10;&#10;&lt;p&gt;If all the points are collinear, we should be able to explain almost the variance in the sample by one vector. In other words, the leading &lt;em&gt;principal value&lt;/em&gt; should be significantly greater than the other two (since we are working with a total of three dimensions). How much greater it should be is up to you to decide. I would say well above 95%, say 98% or 99% of the total variance, should be explained by the leading &lt;em&gt;principal component&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Alignment with the axes&lt;/h2&gt;&#10;&#10;&lt;p&gt;Once you have calculated the leading principal component, it is a simple matter of taking its dot product of the unit vectors along all three axes to determine the alignment. The closer the absolute value of the dot product is to unity, the more aligned the principal component is to that axis.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Mathematica Example&lt;/h2&gt;&#10;&#10;&lt;p&gt;I'm going to generate points that are randomly dispersed around a randomly-generated quadratic curve. The greater the coefficient of the highest-order term, the tighter the curve, and the closer the points are to being collinear.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v = RandomReal[{-1, 1}, 3]; v /= Norm[v]; &#10;u = RandomReal[{-1, 1}, 3]; &#10;w = RandomReal[{-1, 1}, 3];&#10;x = Table[RandomVariate[&#10; NormalDistribution[0, 1], 3] + u + v t + w t^2, {t, -10, 10, 0.1}];&#10;#/Total[#] &amp;amp; @ Eigenvalues@Covariance@x // First&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is what it looks like with 95% total variance (cumulative energy) in the leading principal component (vector in red):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/n0DK2.png&quot; alt=&quot;Principal component with 95% total variance.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If we were satisfied with the collinearity, we would proceed with alignment verification:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Abs[First@Eigenvectors@Covariance@x.#] &amp;amp; /@ {{1, 0, 0}, {0, 1, 0}, {0,&#10;0, 1}}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The illustrated example returns &lt;code&gt;{0.329372, 0.805966, 0.491867}&lt;/code&gt;, so we would conclude that the first principal component is &lt;strong&gt;not&lt;/strong&gt; well-aligned with any of the axes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-14T08:16:07.467" Id="28441" LastActivityDate="2012-05-14T08:16:07.467" OwnerUserId="4479" ParentId="28230" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="28462" AnswerCount="1" Body="&lt;p&gt;Ok so this may seem like a really stupid question but I'm getting confused by the mathematical notation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am calculating the MSE (mean squared error) between two matrices. I know how to compute this, but do not know how to denote it.&lt;/p&gt;&#10;&#10;&lt;p&gt;For calculating the MSE, you have to subtract &lt;em&gt;every&lt;/em&gt; element of matrix 2 from &lt;em&gt;every&lt;/em&gt; element of matrix 1. It's how to denote &quot;subtract element [i,j] of matrix 2 from element [i,j] of matrix 1&quot; that I can't figure out...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for the help.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-14T14:34:50.863" Id="28461" LastActivityDate="2012-05-14T14:42:35.770" OwnerUserId="9649" PostTypeId="1" Score="1" Tags="&lt;notation&gt;" Title="How to denote element-wise difference of two matrices" ViewCount="342" />
  <row Body="&lt;p&gt;As I believe @whuber is alluding to (+1), it's very important to consider how prices are weighted and combined.  Generally, &lt;a href=&quot;http://en.wikipedia.org/wiki/Inflation&quot;&gt;inflation&lt;/a&gt; reflects the weighted average price of a basket of goods.  But, those weights should reflect amounts that are being purchased.  See &lt;a href=&quot;http://en.wikipedia.org/wiki/Consumer_Price_Index&quot;&gt;Consumer price index&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if, in your example, people buy equal amounts of milk and bread at the original prices (100 each), and, after the price changes, people buy the same amount of bread, but half the amount of milk, the weighted average price stays the same: $(200*.5+50*1)/1.5=100$.  If people buy double the amount of bread, and half the amount of milk, the weighted average price goes down: $(200*.5+50*2)/2.5=80$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-14T20:07:57.403" Id="28487" LastActivityDate="2012-05-14T20:07:57.403" OwnerUserId="11240" ParentId="28465" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;For fun, I tried to replicate the results of &lt;a href=&quot;http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract&quot; rel=&quot;nofollow&quot;&gt;Petersen (2009)&lt;/a&gt; who deals with the correct estimation of standard errors in finance panel data sets. &lt;/p&gt;&#10;&#10;&lt;p&gt;In a nutshell, he estimates the following standard regression for a panel data set:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  
  
  <row Body="&lt;p&gt;I don't know about appropriate, but we have had good results by submitting such data to the network-based clustering method MCL. We (van Dongen and Abreu-Goodger) have written up an elaborate example in PMID 22144159 (using MCL to extract clusters from networks), on E coli expression data across 466 different conditions. In other work we have also applied MCL to time-series data. An important step in this approach is to quantitate the network as it is submitted to edge thresholds of increasing severity, then to choose an appropriate threshold (based on network density, number of singletons, et cetera), transform the network, and finally cluster it. From the book chapter mentioned above, it would look something like this (with some cutoffs made more stringent):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mcxarray -data ecoli.exprs -co 0.5 -skipr 1 -skipc 1 -tf 'abs()' -o ecoli.mci -write-tab ecoli.dict&#10;mcx query -imx ecoli.mci --vary-correlation    # consider attributes that are output.&#10;mcx alter -imx ecoli.mci -tf 'gq(0.8),add(-0.8)' -o ecoli80.mci&#10;mcl ecoli80.mci -use-tab ecoli.dict            # try different granularities.&#10;mcl ecoli80.mci -use-tab ecoli.dict -I 1.4&#10;mcl ecoli80.mci -use-tab ecoli.dict -I 3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-05-15T11:33:34.027" Id="28521" LastActivityDate="2012-05-15T11:33:34.027" OwnerUserId="4495" ParentId="28514" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Yes, python is a good choice for building a machine learning application.  The primary scientific libraries, numpy and scipy, use C and Fortran for core loops, so they are often as fast as libraries in other languages.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that a drawback of python is that each python process can only utilize a single core for computation (because of the &lt;a href=&quot;http://wiki.python.org/moin/GlobalInterpreterLock&quot; rel=&quot;nofollow&quot;&gt;GIL&lt;/a&gt;) and python performance suffers if you try to have multiple cpu-intense threads running simultaneously.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-15T12:20:31.910" Id="28525" LastActivityDate="2012-05-15T12:26:12.683" LastEditDate="2012-05-15T12:26:12.683" LastEditorUserId="11240" OwnerUserId="11240" ParentId="28512" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Assume that you have &lt;em&gt;independent&lt;/em&gt; observations. If the first observation falls in bin $i$ then the probability of obtaining that observation is $P_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the second observation falls in bin $j$, the probability of obtaining those observations in that order is $P_i\cdot P_j$. If the third observation falls into bin $i$ the cumulative probability is now $P_i\cdot P_j\cdot P_i=P_i^2\cdot P_j$, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Continuing in this way, we find that the cumulative probability is $P_i^{n_i}\cdot P_j^{n_j}\cdots=\prod_i P_i^{n_i}$, which is the second formula.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-15T13:26:15.207" Id="28530" LastActivityDate="2012-05-15T13:26:15.207" OwnerUserId="8507" ParentId="28529" PostTypeId="2" Score="4" />
  <row Body="Julia is a high-level, high-performance dynamic programming language for technical computing, with syntax that is familiar to users of other technical computing environments." CommentCount="0" CreationDate="2012-05-15T13:26:57.950" Id="28532" LastActivityDate="2012-05-15T13:33:38.997" LastEditDate="2012-05-15T13:33:38.997" LastEditorUserId="2981" OwnerUserId="2981" PostTypeId="4" Score="0" />
  
  
  <row Body="&lt;p&gt;There actually is an S3 generic &lt;code&gt;simulate&lt;/code&gt; that even returns the data frame (or other list) you want. Type &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;?simulate  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It has methods for classes lm (works also for glm or for your aov example) and glm.nb (in MASS) already. You can now write S3 &lt;code&gt;simulate&lt;/code&gt; methods for other classes of objects, e.g. for objects from lme4. You can check for which classes there are methods by typing &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;getAnywhere(&quot;simulate.class&quot;), getAnywhere(&quot;simulate&quot;)  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;getS3method(&quot;simulate&quot;,&quot;class&quot;), methods(simulate) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2012-05-16T00:38:15.713" Id="28568" LastActivityDate="2012-06-18T21:10:42.833" LastEditDate="2012-06-18T21:10:42.833" LastEditorUserId="8413" OwnerUserId="8413" ParentId="28566" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;I'm not sure if I'm in a position to write a dictionary definition for these terms, but perhaps the following will help. And there probably are other terms used. But here's a basic overview:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt;: The measurement tool that results from taking a function of component items.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Scale Score&lt;/strong&gt;: The actual score of an individual on the scale.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Item or question&lt;/strong&gt;: An individual item that forms part of the scale.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Rating scale or response scale&lt;/strong&gt;: The set of response options available. (e.g., the set &lt;code&gt;{&quot;Yes&quot;, &quot;No&quot;, &quot;Don't Know&quot;}&lt;/code&gt;)(note also that this is sometimes, somewhat confusingly, also referred to as a scale)&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Response option or level of the response scale&lt;/strong&gt;: An element  (e.g., &quot;Strongly Disagree&quot;) of the response scale.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Observation or response&lt;/strong&gt;: the actual response (i.e., one of the response options) given by a particular participant to a particular item.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;And here's it all used in a paragraph:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;John gave the &lt;strong&gt;response&lt;/strong&gt; &quot;strongly agree&quot; to &lt;strong&gt;item&lt;/strong&gt; 7 on the Happiness &lt;strong&gt;scale&lt;/strong&gt;.&#10;  &quot;Strongly agree&quot; was the &lt;strong&gt;response option&lt;/strong&gt; chosen. This was one of five options that made up the &lt;strong&gt;rating scale&lt;/strong&gt; for each item. &#10;  Taking the mean of all the &lt;strong&gt;items&lt;/strong&gt; on the &lt;strong&gt;scale&lt;/strong&gt; yielded a &lt;strong&gt;scale score&lt;/strong&gt; of 3.7.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2012-05-16T04:41:01.703" Id="28584" LastActivityDate="2012-05-16T04:41:01.703" OwnerUserId="183" ParentId="28572" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="28591" AnswerCount="1" Body="&lt;p&gt;I have a time-series of daily measurements of some quantity for 1995-2011. There's about one measurement every three days.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data show a strong seasonality (annual cycle).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I ultimately want is get an estimation for the underlying trend. &lt;a href=&quot;http://www.jos.nu/Articles/abstract.asp?article=613&quot; rel=&quot;nofollow&quot; title=&quot;STL&quot;&gt;STL&lt;/a&gt; decomposition works fine and gives me seperate trend, seasonality, and noise components of my original time series. But how do I get a quantitative measure for the trend component? A simple linear regression won't do due to the autocorrelation in the noise component. Gavin Simpson suggested using &lt;code&gt;gls()&lt;/code&gt; from &lt;strong&gt;R&lt;/strong&gt;'s &lt;em&gt;nlme&lt;/em&gt; package, but I'm not sure how I should do that. Which would be the inputs for &lt;code&gt;gls()&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, are there other suggestions for tackling my problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is greatly appreciated :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-16T08:50:25.893" Id="28590" LastActivityDate="2012-05-16T09:26:04.000" OwnerUserId="11352" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;trend&gt;&lt;seasonality&gt;" Title="Calculate trend slope after STL decomposition" ViewCount="723" />
  <row AnswerCount="0" Body="&lt;p&gt;I used ASreml to estimate genetic parameters for feed intake using bivariate and multivariate models. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am very confused about how to define variance structures (even though I went through the &quot;Users Guide&quot; many times). Is there a quick way to check which models will be used in the R and G structures, such as when using the Unstructured, Identity, and Autoregressive models? How would I determine the initial values?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-16T12:05:16.843" FavoriteCount="0" Id="28597" LastActivityDate="2012-08-15T01:09:27.147" LastEditDate="2012-08-15T01:09:27.147" LastEditorUserId="3826" OwnerUserId="11356" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="Specify variance structure in ASreml v3" ViewCount="87" />
  
  
  <row Body="&lt;p&gt;The parameter $a$ which maximizes the likelihood of the observed data is the maximum likelihood parameter.  It is, in the maximum likelihood sense, the &quot;best&quot; parameter for the observed data.  Note that this will not (necessarily) give you the best generalization performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you note, the ratio should give you the same &quot;best&quot; parameter since $L(a) \propto LR(a)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that it is common to work with logarithms of likelihood and LR (since log doesn't affect maximum).  It might be easier to convince yourself of the equivalence if you work with log-likelihood and log-likelihood-ratio.  It is also conventional to negate and minimize.  You might be interested in this &lt;a href=&quot;http://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/&quot; rel=&quot;nofollow&quot;&gt;discussion of negative log-likelihood&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-16T14:04:47.247" Id="28606" LastActivityDate="2012-05-16T14:04:47.247" OwnerUserId="11240" ParentId="28600" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The Gaussian distribution isn't supported (although as you know, the normal distribution is). Have a look at &lt;code&gt;?fitdistr&lt;/code&gt; after loading the &lt;code&gt;MASS&lt;/code&gt; package and you will find a list of supported distributions. To fit data to a gamma distribution, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(MASS)    &#10;x &amp;lt;- rgamma(100, shape = 5, rate = 0.1)&#10;fitdistr(x, &quot;gamma&quot;)&#10;#     shape         rate   &#10;#  4.43472115   0.09208318 &#10;# (0.60485447) (0.01329746)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-05-16T14:07:28.710" Id="28607" LastActivityDate="2012-05-16T14:13:12.370" LastEditDate="2012-05-16T14:13:12.370" LastEditorUserId="4856" OwnerUserId="9249" ParentId="28602" PostTypeId="2" Score="2" />
  
&#10;P(\text{&amp;quot;fishy&amp;quot;|&amp;quot;all sixes&amp;quot;}) = \frac{P(\text{&amp;quot;all sixes&amp;quot;|&amp;quot;fishy&amp;quot;})P(\text{&amp;quot;fishy&amp;quot;})}{P(\text{&amp;quot;all sixes&amp;quot;})}.
  
  <row Body="&lt;p&gt;Boxplots were really designed for normal data, or at least unimodal data. The Beanplot shows you the actual density curve, which is more informative.&lt;/p&gt;&#10;&#10;&lt;p&gt;The shape is the density, and the short horizontal lines represent each data point. This combines the best of a boxplot, density plot, and rug plot all in one and is very readable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, the example that you've chosen decided to add a bunch of longer lines which clutter the graph beyond recognition (for me). [snip]&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Having now worked with beanplot a bit more, the longer thick lines are the mean (or optionally median) for each bean. The longer thin lines are the data, with a sort of &quot;stacking&quot; where wider lines indicate more duplicate values. (You can also jitter them, which I prefer, but at least the &quot;normal&quot; category already has a fair density of points that jittering might make worse.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I still think the example you chose is a rather cluttered, which could perhaps be cleared up by using jittering instead of stacking.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://www.jstatsoft.org/v28/c01/paper&quot; rel=&quot;nofollow&quot;&gt;paper that describes the R package for making bean plots&lt;/a&gt; is a nice read.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-16T15:17:13.227" Id="28615" LastActivityDate="2012-05-23T20:18:40.493" LastEditDate="2012-05-23T20:18:40.493" LastEditorUserId="1764" OwnerUserId="1764" ParentId="28612" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="28625" AnswerCount="1" Body="&lt;p&gt;I recently read a &lt;a href=&quot;http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/&quot; rel=&quot;nofollow&quot;&gt;fascinating article&lt;/a&gt; describing methods for clustering data without assuming a fixed number of clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The article even includes some sample code, in a mix of Ruby, Python, and R.  However, the meat of the analysis is performed using &lt;a href=&quot;http://scikit-learn.sourceforge.net/dev/index.html&quot; rel=&quot;nofollow&quot;&gt;scikit-learn&lt;/a&gt;'s &lt;a href=&quot;http://scikit-learn.sourceforge.net/dev/modules/mixture.html&quot; rel=&quot;nofollow&quot;&gt;Dirichlet Process Gaussian Mixture Model&lt;/a&gt; to actually find clusters in some sample data taken from McDonald's menu.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously, this a a great excuse to learn some more python, but I'm lazy and would like to find a ready-made R package that can take a dataframe and return clusters, in a manner similar to the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html&quot; rel=&quot;nofollow&quot;&gt;kmeans&lt;/a&gt; function.  &lt;a href=&quot;http://cran.r-project.org/web/views/Cluster.html&quot; rel=&quot;nofollow&quot;&gt;A quick search on CRAN&lt;/a&gt; reveals the packages &lt;a href=&quot;http://cran.r-project.org/web/packages/dpmixsim/index.html&quot; rel=&quot;nofollow&quot;&gt;dpmixsim&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/profdpm/index.html&quot; rel=&quot;nofollow&quot;&gt;profdpm&lt;/a&gt;.  Any suggestions for the best place to start?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-16T16:05:42.860" FavoriteCount="3" Id="28620" LastActivityDate="2012-05-16T16:47:46.953" LastEditDate="2012-05-16T16:11:20.700" LastEditorUserId="2817" OwnerUserId="2817" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;clustering&gt;&lt;dirichlet-process&gt;" Title="Dirichlet process/Chinese restaurant process for clustering in R" ViewCount="2482" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm a little confused about the analysis of normality in a repeated measures ANOVA I'm doing. It's a factorial ANOVA with three rm IVs each of which has only 2 levels. I've read in a few places online and in a book or two that to assess normality in a repeated measures ANOVA, I look at the distributions within each conditions -this would be 8 distributions in my case. However, most of these articles focused on rm ANOVAs in which at least one of the IVs has more than 2 levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;My confusion is coming from the fact that when carrying out a repeated measures t test (i.e. 1 IV with just 2 levels), you don't care about the distribution for responses in the separate conditions. Rather, you focus on the distribution of the difference scores. In other words, in a repeated measures t-test, we're interested in change.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Should I assess normality by looking at the 8 individual distributions associated with each conditions&lt;/li&gt;&#10;&lt;li&gt;If yes, why is it different for a t-test (which is essentially calculated in the same way)&lt;/li&gt;&#10;&lt;li&gt;If I shouldn't look at the 8 distributions mentioned above, what should I run my normality analysis on?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="6" CreationDate="2012-05-16T16:23:00.660" Id="28623" LastActivityDate="2014-01-17T12:14:41.597" LastEditDate="2012-06-13T01:46:56.070" LastEditorUserId="183" OwnerUserId="11364" PostTypeId="1" Score="5" Tags="&lt;anova&gt;&lt;normal-distribution&gt;&lt;repeated-measures&gt;" Title="Whether to assess normality in a factorial repeated measures ANOVA by looking at distributions within cells?" ViewCount="1227" />
  
  
  <row AcceptedAnswerId="28643" AnswerCount="1" Body="&lt;p&gt;I am currently using different procedures to estimate the probability that a $D$-dimensional Gaussian random variable with mean $\mu$ and covariance $\Sigma$ lies within a sphere of radius $R$ that is centered about the origin.  That is, I am estimating $P(|| X ||_2  &amp;lt; R)$  where  $X \sim N(\mu, \Sigma)$  and $X \in \mathbb{R}^D$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering whether there is a way to obtain the &lt;strong&gt;exact&lt;/strong&gt; value of this probability analytically (i.e. without using numerical integration or Monte Carlo)? I currently have two basic approaches to follow:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Approach 1&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Find a way to analytically evaluate the integral:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\int_{x \in S} (2\pi)^{-\frac{D}{2}}|\Sigma|^{-\frac{1}{2}} \exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) dx $&lt;/p&gt;&#10;&#10;&lt;p&gt;over the spherical region:&lt;/p&gt;&#10;&#10;&lt;p&gt;$S = \{||x|| &amp;lt; R \} = \{x^Tx &amp;lt; R^2\}$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Approach 2&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Exploit the fact that if &lt;/p&gt;&#10;&#10;&lt;p&gt;$x \sim N(\mu,\Sigma)$&lt;/p&gt;&#10;&#10;&lt;p&gt;then &lt;/p&gt;&#10;&#10;&lt;p&gt;$(x-\mu)^T \Sigma^{-1} (x-\mu) \sim \chi^2(D) $&lt;/p&gt;&#10;&#10;&lt;p&gt;This implies that&lt;/p&gt;&#10;&#10;&lt;p&gt;$P( (x-\mu)^T \Sigma^{-1} (x-\mu) &amp;lt; R^2 ) = P(\chi^2(D) &amp;lt; R^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is very simple to evaluate... &lt;/p&gt;&#10;&#10;&lt;p&gt;I am hoping that there is a way to use this fact in order to evaluate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P( x^T x &amp;lt; R^2) $&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-16T19:31:28.620" Id="28633" LastActivityDate="2012-05-16T22:38:41.080" LastEditDate="2012-05-16T22:11:49.700" LastEditorUserId="930" OwnerUserId="3572" PostTypeId="1" Score="3" Tags="&lt;probability&gt;" Title="What is the probability that a multivariate Normal RV lies within a sphere of radius R?" ViewCount="273" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a document corpus and I want to estimate the probability of occurrence of a certain word $w$. Simply calculating the frequencies and use such a number as an estimation is not a good choice. Is there any work on this topic describing a better approach?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-17T12:54:23.077" FavoriteCount="1" Id="28665" LastActivityDate="2012-05-21T21:47:34.320" LastEditDate="2012-05-17T21:39:13.617" LastEditorUserId="930" OwnerUserId="7803" PostTypeId="1" Score="6" Tags="&lt;text-mining&gt;&lt;frequency&gt;" Title="What's a good approach to estimate the probability of word frequencies?" ViewCount="565" />
  
  
&#10; \frac{b-\log ((-b-1) (a b-1))}{a b^2} &amp;amp; a&amp;lt;0\land \frac{1}{a}-b&amp;gt;1
  <row Body="&lt;p&gt;&lt;strong&gt;A note:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You want to answer questions about your data, and not create questions about the visualization method itself.  Often, boring is better.  It does make comparisons of comparisons easier to comprehend too.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;An Answer:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The need for simple formatting beyond R's base package probably explains the popularity of Hadley's ggplot package in R.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(sn)&#10;library(ggplot2)&#10;&#10;# Simulate from a normal and skew-normal distributions&#10;x = rnorm(250,0,1)&#10;y = rsn(250,0,1,5)&#10;&#10;&#10;##============================================================================&#10;## I put the data into a data frame for ease of use&#10;##============================================================================&#10;&#10;dat = data.frame(x,y=y[1:250]) ## y[1:250] is used to remove attributes of y&#10;str(dat)&#10;dat = stack(dat)&#10;str(dat)&#10;&#10;##============================================================================&#10;## Density plots with ggplot2&#10;##============================================================================&#10;ggplot(dat, &#10;     aes(x=values, fill=ind, y=..scaled..)) +&#10;        geom_density() +&#10;        opts(title = &quot;Some Example Densities&quot;) +&#10;        opts(plot.title = theme_text(size = 20, colour = &quot;Black&quot;))&#10;&#10;ggplot(dat, &#10;     aes(x=values, fill=ind, y=..scaled..)) +&#10;        geom_density() +&#10;        facet_grid(ind ~ .) +&#10;        opts(title = &quot;Some Example Densities \n Faceted&quot;) +&#10;        opts(plot.title = theme_text(size = 20, colour = &quot;Black&quot;))&#10;&#10;ggplot(dat, &#10;     aes(x=values, fill=ind)) +&#10;        geom_density() +&#10;        facet_grid(ind ~ .) +&#10;        opts(title = &quot;Some Densities \n This time without \&quot;scaled\&quot; &quot;) +&#10;        opts(plot.title = theme_text(size = 20, colour = &quot;Black&quot;))&#10;&#10;##----------------------------------------------------------------------------&#10;## You can do histograms in ggplot2 as well...&#10;## but I don't think that you can get all the good stats &#10;## in a table, as with hist&#10;## e.g. stats = hist(x)&#10;##----------------------------------------------------------------------------&#10;ggplot(dat, &#10;     aes(x=values, fill=ind)) +&#10;        geom_histogram(binwidth=.1) +&#10;        facet_grid(ind ~ .) +&#10;        opts(title = &quot;Some Example Histograms \n Faceted&quot;) +&#10;        opts(plot.title = theme_text(size = 20, colour = &quot;Black&quot;))&#10;&#10;## Note, I put in code to mimic the default &quot;30 bins&quot; setting&#10;ggplot(dat, &#10;     aes(x=values, fill=ind)) +&#10;        geom_histogram(binwidth=diff(range(dat$values))/30) +&#10;        opts(title = &quot;Some Example Histograms&quot;) +&#10;        opts(plot.title = theme_text(size = 20, colour = &quot;Black&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Finally, I've found that adding a simple background helps.&#10;Which is why I wrote &quot;bgfun&quot; which can be called by panel.first&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;bgfun = function (color=&quot;honeydew2&quot;, linecolor=&quot;grey45&quot;, addgridlines=TRUE) {&#10;    tmp = par(&quot;usr&quot;)&#10;    rect(tmp[1], tmp[3], tmp[2], tmp[4], col = color)&#10;    if (addgridlines) {&#10;        ylimits = par()$usr[c(3, 4)]&#10;        abline(h = pretty(ylimits, 10), lty = 2, col = linecolor)&#10;    }&#10;}&#10;plot(rnorm(100), panel.first=bgfun())&#10;&#10;## Plot with original example data&#10;op = par(mfcol=c(2,1))&#10;hist(x, panel.first=bgfun(), col='antiquewhite1', main='Bases belonging to us')&#10;hist(y, panel.first=bgfun(color='darkolivegreen2'), &#10;    col='antiquewhite2', main='Bases not belonging to us')&#10;mtext( 'all your base are belong to us', 1, 4)&#10;par(op)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-05-17T19:35:43.037" Id="28685" LastActivityDate="2012-05-17T19:35:43.037" OwnerUserId="10826" ParentId="28431" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;It is hard to answer your needs without more detail. In text analysis, word frequencies are replaced by &lt;a href=&quot;http://en.wikipedia.org/wiki/Tf%2aidf&quot; rel=&quot;nofollow&quot;&gt;tf*idf&lt;/a&gt; which stands for &quot;term frequency times inverse document frequency&quot;. This is an empirical score that corrects for the occurrence of terms that are frequent in the corpus and thus do not discriminate documents. It is widely used to compare texts, in particular through the &lt;a href=&quot;http://en.wikipedia.org/wiki/Cosine_similarity&quot; rel=&quot;nofollow&quot;&gt;cosine similarity&lt;/a&gt; measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, you compute the frequency of the term in the document (tf) and multiply it by the log of the inverse fraction of documents containing the term (idf). &lt;/p&gt;&#10;&#10;&lt;p&gt;The site of &lt;a href=&quot;http://www.python.org/&quot; rel=&quot;nofollow&quot;&gt;Python&lt;/a&gt;'s &lt;a href=&quot;http://nltk.org/&quot; rel=&quot;nofollow&quot;&gt;NLTK&lt;/a&gt; (natural language toolkit) contains an implementation of it, along with other tools and a good deal of explanations.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, if what you &lt;strong&gt;really&lt;/strong&gt; want is an estimator of the probability of occurrence of the word, I don't know if you can get better than the frequency. And if 0 count is an issue, you can use the the Bayesian estimator (k+1) / (n+1), where k and n are word count and text size respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: for a great read about IDF, take a look at S. Robertson's paper &lt;a href=&quot;http://www.soi.city.ac.uk/~ser/idfpapers/Robertson_idf_JDoc.pdf&quot; rel=&quot;nofollow&quot;&gt;Understanding Inverse Document Frequency&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-17T21:36:35.710" Id="28694" LastActivityDate="2012-05-21T21:47:34.320" LastEditDate="2012-05-21T21:47:34.320" LastEditorUserId="11458" OwnerUserId="10849" ParentId="28665" PostTypeId="2" Score="6" />
&#10;\end{align}
  
  <row Body="&lt;p&gt;Alexander's answer is very good and he makes a good suggestion.  It does seem a little surprising that everything is significant when the sample size is relatively small as in your example.  The Bonferroni bound may be too conservative though if some of the p-values are close to 0.05.  I would suggest p-value adjustment using a bootstrap or permutation approach might do better.  In SAS you can do this with PROC MULTTEST.  If you are not familiar with these methods look at the text by Westfall and Young.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-18T11:50:01.693" Id="28720" LastActivityDate="2012-05-18T11:50:01.693" OwnerUserId="11032" ParentId="28712" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The answer of Elvis relies on permutations but in my opinion it does not make clear what is wrong with the original bootstrap approach. Let me discuss a solution based solely on bootstrap.&lt;/p&gt;&#10;&#10;&lt;p&gt;The crucial problem of your original simulation is that bootstrap always provides you with the TRUE distribution of the test statistic. However, when computing the p-value you have to compare the obtained value of the test statistic to its distribution UNDER H0, i.e. not with the true distribution!&lt;/p&gt;&#10;&#10;&lt;p&gt;[Let's make it clear. For example, it is known that the test statistic T of the classical t-test has the classical &quot;central&quot; t-distribution under H0 and a noncentral distribution in general. However, everyone is familiar with the fact that the observed value of T is compared  to the classical &quot;central&quot; t-distribution, i.e. one does not try to obtain the true [noncenral] t-distribution to make the comparison with T.]&lt;/p&gt;&#10;&#10;&lt;p&gt;Your p-value 0.4804 is so large, because the observed value &quot;t0&quot; of the test statistic Mean[1]-Mean[2] lies very close to the centre of the bootstrapped sample &quot;t&quot;. It is natural and typically it is always so [i.e. irrespective of the validity of H0], because the bootstrapped sample &quot;t&quot; emulates the the ACTUAL distribution of Mean[1]-Mean[2]. But, as noted above [and also by Elvis], what you really need is the distribution of Mean[1]-Mean[2] UNDER H0. It is obvious that&lt;/p&gt;&#10;&#10;&lt;p&gt;1) under H0 the distribution of Mean[1]-Mean[2] will be centered around 0,&lt;/p&gt;&#10;&#10;&lt;p&gt;2) its shape does not depend on the validity of H0.&lt;/p&gt;&#10;&#10;&lt;p&gt;These two points imply that the distribution of Mean[1]-Mean[2] under H0 can be emulated by the bootstrapped sample &quot;t&quot; SHIFTED so that it is centered around 0. In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;b3.under.H0 &amp;lt;- b3$t - mean(b3$t)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the corresponding p-value will be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mean(abs(b3.under.H0) &amp;gt; abs(b3$t0))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives you a &quot;very nice&quot; value of 0.0232. :-)&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me note that the the point &quot;2)&quot; mentioned above is called &quot;translation equivariance&quot; of the test statistic and it does NOT have to hold in general! I.e. for some test statistics, shifting of the bootstrapped &quot;t&quot; does not provide you with a valid estimate of the distribution of the test statistic under HO! Have a look at this discussion and especially at the reply of P. Dalgaard:&#10;&lt;a href=&quot;http://tolstoy.newcastle.edu.au/R/e6/help/09/04/11096.html&quot; rel=&quot;nofollow&quot;&gt;http://tolstoy.newcastle.edu.au/R/e6/help/09/04/11096.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Your testing problem does yields a perfectly symmetric distribution of the test statistic, but keep in mind that there are some problems with obtaining TWO-SIDED p-values in case of skewed bootstrapped distribution of the test statistic. Again, read the above link.&lt;/p&gt;&#10;&#10;&lt;p&gt;[And finally, I would use the &quot;pure&quot; permutation test in your situation; i.e. the second half of Elvis answer. :-)]&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH :-)&lt;/p&gt;&#10;&#10;&lt;p&gt;jan s.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-18T12:42:37.633" Id="28725" LastActivityDate="2012-05-18T12:42:37.633" OwnerUserId="11403" ParentId="20701" PostTypeId="2" Score="4" />
  
&#10;P({\bf x}|{\bf μ},{\bf Σ}) = N({\bf \mu}, {\bf \Sigma}) \\
  
  
  <row Body="&lt;p&gt;To bootstrap in a mixed effects linear model you would do sampling with replacement in a way that maintains the model structure.  So your data is divided into groups and you don't want to mix the data from one group into the data from another.  For any particular group say you have m observations then you would sample m times with replacement from those m observations.  You repeat this process with all the other groups (but the value for m may change).  Once you have done this you have a bootstrap sample.  You fit the model to this bootstrap sample  and then repeat the bootstrapping followed by model fitting many times. This will give you a collection of estimated model parameters (a histogram for each if you will).  Any time you have a bootstrap histogram of estimates you can construct bootstrap confidence intervals from this collection of estimates.  The simplest is Efron's percentile method which takes the 2.5 percentile and the 97.5 percentile from these ordered bootstrap estimate to be the endpoint of a 95% confidence interval.  For more detail on this you can read Efron and Tibshirani's An Introduction to Bootstrap (1993) Chapman and Hall, my book Bootstrap Methods 2nd ed (2007) Wiley or the article by Efron and Tibshirani in Statistical Science (1986).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now in the absence of data you may want to get an understanding of how the model works.  then you can do simulation of the data and look at the results in a way similar to what I described for the bootstrap.  The difference is that instead of sampling from the empirical distribution for the data you have to specify a distribution or distributions whenever you do the sampling.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-18T19:39:04.050" Id="28758" LastActivityDate="2012-05-18T19:39:04.050" OwnerUserId="11032" ParentId="28683" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;One solution would be to use cross-validation methods. This might be a conceptually easy (and elegant) solution because the model you have differs significantly from the model to be compared to. AIC or BIC won't really work here because the functional forms of these two models are very different -- yours is nonlinear and their model is not only linear but also based on binned data. AIC or BIC is insensitive to functional forms.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wouldn't worry about binning vs non-binning too much, since it seems to me that binning is a modeling decision that could make a model better or worse. In other words, it's a feature whose effectiveness should be tested.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, assuming you can implement the other model, you can perform a k-fold cross-validation:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Divide your data into k subsets;&lt;/li&gt;&#10;&lt;li&gt;Iteratively leave one subset out, and train your model (without binning) and the other model (with binning) on the rest of the subsets;&lt;/li&gt;&#10;&lt;li&gt;Compute the sum of loglikelihoods of the subset that was left out in the previous with regard to your model and the other model. This should be relatively straight-forward: in your nonlinear model, error is binomially distributed; in the other model, error is normally distribution since it's a simple linear regression;&lt;/li&gt;&#10;&lt;li&gt;Repeat 2 and 3 until you have used each of the k subsets as the test subset (thus the name k-fold).&lt;/li&gt;&#10;&lt;li&gt;You can then compare which model gives you the better loglikelihood (i.e. the less negative one).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2012-05-19T01:10:04.373" Id="28765" LastActivityDate="2012-05-19T01:10:04.373" OwnerUserId="7618" ParentId="18732" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;1) The authors of the article have written that both $m_i$ and $n_i$ are distributed Poisson, as it's a comparison of simulated to actual data; hence the difference between the two has variance $m_i + n_i$ instead of the usual case, where $m_i$ is the variance (assuming correctness of the model). Consequently, $m_i+n_i$ is the appropriate divisor. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) The test is indeed nonparametric.  No Gaussianity assumed!&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Michael Chernick makes a valuable point in comments, namely that the approximation will be poor if the cells are sparse.  The authors do mention the importance of bin size selection, but it's certainly not as though you can take this statistic, apply it without care, and expect to get good results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit the Second:  Michael Chernick makes another valuable point in comments, which is that there are better ways of validating their model against the star data.  Even if they take the &quot;binning&quot; approach, they'd be better off using model-calculated expected values for bin occupancy counts and doing a more typical $\chi^2$ test than comparing simulated data with actual data; the use of simulated data vs. expected values just adds randomness to the results and thereby reduces the power of the test.  It may be, however, that w/o access to the software tool's code, they can't actually do the needed calculations to get bin occupancy rates, in which case this may be about as well as they can do (I could be wrong about that, though.)  &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-05-19T01:29:24.077" Id="28766" LastActivityDate="2012-05-19T19:46:25.750" LastEditDate="2012-05-19T19:46:25.750" LastEditorUserId="7555" OwnerUserId="7555" ParentId="28726" PostTypeId="2" Score="3" />
  <row AnswerCount="3" Body="&lt;p&gt;What methods are there to fit a model of the form $y=A\mathrm e^{Bx}+C\mathrm e^{Dx}+E$? &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the actual scientific data to be fitted: &lt;a href=&quot;http://dl.dropbox.com/u/39499990/Ben%2C%20real%20data.xlsx&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/39499990/Ben%2C%20real%20data.xlsx&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;B should be in the range of -1 to -100.&lt;/p&gt;&#10;&#10;&lt;p&gt;D should be in the range of -100 to -500.&lt;/p&gt;&#10;&#10;&lt;p&gt;E is a constant.&lt;/p&gt;&#10;&#10;&lt;p&gt;This specific model is of interest as it is an accepted one in the scientific community for describing the biological proccess in hand- Inactivation of a voltage dependent calcium channel. (for reference see for example: A novel molecular inactivation determinant of voltage-gated CaV1.2 L-type Ca2+ channel. A Livneh, R Cohen, and D Atlas; Neuroscience, Jan 2006; 139(4): 1275-87. &#10;&quot; The rate of inactivation was analyzed by a biexponential decay -A1exp(-t/Tao1)-A2exp(-t/Tao2)+C &quot; )&lt;/p&gt;&#10;&#10;&lt;p&gt;Best would be a solution that I could implement in Excel, by the use of build-in functions or VBA code.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-05-19T06:43:54.377" FavoriteCount="1" Id="28769" LastActivityDate="2013-08-13T10:45:34.747" LastEditDate="2013-08-13T10:45:34.747" LastEditorUserId="805" OwnerUserId="11421" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;predictive-models&gt;&lt;curve-fitting&gt;&lt;mixture&gt;&lt;exponential&gt;" Title="Fitting an exponential mixture model with interval constraints on the mixture weights" ViewCount="1360" />
  
  <row Body="Intervention analysis estimates the effect of an external intervention on a time-series." CommentCount="0" CreationDate="2012-05-19T11:41:36.673" Id="28774" LastActivityDate="2012-05-19T14:16:04.717" LastEditDate="2012-05-19T14:16:04.717" LastEditorUserId="179" OwnerUserId="179" PostTypeId="4" Score="0" />
  
  
  <row Body="&lt;p&gt;You might want to look at &lt;a href=&quot;http://www.jstatsoft.org/v34/i02/paper&quot; rel=&quot;nofollow&quot;&gt;beta regression&lt;/a&gt;.  It would be helpful to know what the response variable actually represents, but I would start with logistic regression (O.K. if the proportion represents the average of a large number of Bernoulli trials) and then try beta regression (which is a much more flexible solution, where the residuals are assumed to be from a beta distribution with parameters specified by the regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: It seems that the data generating process for this problem means that it is not a straightforward regression problem, but instead it has three seperate modes of generating the response, one where it is zero, one where it is one and one where it can be any value in the range (0,1).  The way to approach such problems is by using a compond likelihood.  I have used this kind of approach for modelling rainfall, where there are lots of exact zeros for days where it doesn't rain at all.  The solution is to have a model with three outputs, one which gives the probability that it will rain, and the other two giving the shape and scale parameters of a gamma distribution which represents the amount of rain that you would see if it did rain.  The original paper on this was by Peter Williams, but I can't find it on line, so &lt;a href=&quot;http://theoval.cmp.uea.ac.uk/publications/pdf/esann2003c.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; is my paper, which should give you the basic idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this problem, you could try having a model with three outputs, one is the probability that the response is an exact zero, one that is the probability that it is an exact one and one that is a prediction of the response if it isn't an exact zero or an exact one.  I doub't you will be able to get some off-the shelf code for this, but it is the approach I would take.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-05-19T17:31:39.830" Id="28782" LastActivityDate="2012-05-20T16:15:45.563" LastEditDate="2012-05-20T16:15:45.563" LastEditorUserId="887" OwnerUserId="887" ParentId="28756" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The multinomial distribution is a discrete probability distribution used to describe the results of a random experiment where each of $n$ outcomes are placed into one of $k$ nominal categories. It can be thought of as the generalization of the binomial distribution. The binomial distribution is a special case of the multinomial distribution where there are only $k=2$ categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability mass function (pmf) of the distribution is parametrized by $p_i$, the probabilities of $x_i$ ($i=1,2,\ldots k$) outcomes being placed in the $i^\text{th}$ category. The pmf $P(x_i,n;p_i)$ has the following form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \left\{&#10; \begin{array}{l l}&#10;    \frac{n!}{x_1!x_2!\ldots x_n!} p_1^{x_1} p_2^{x_2} \ldots p_n^{x_n} &amp;amp; \quad \text{if $\sum_{n=1}^k x_i =n$ }\\&#10;~ \\&#10;~ \\&#10;    0 &amp;amp; \quad \text{otherwise}&#10;  \end{array}  \right. $$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-19T21:21:25.590" Id="28793" LastActivityDate="2013-08-31T15:38:38.570" LastEditDate="2013-08-31T15:38:38.570" LastEditorUserId="7290" OwnerUserId="27581" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;Often when I want to select the number of features, I tend to look at Principal Component Analysis (PCA).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you consider the different features as the dimensions of your problem, PCA will allow you to create a new set of features (with less dimensions) that preserves most of the information.   Each of the new features has an associated eigenvalue that describes the amount of information preserved from the original formulation of the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;The more of the new features you use, the more information you have (about the original problem).  However, often a few features are enough to give you most of the information that you are seeking and a lot of features give little additional information at all (since they have small eigenvalues).&lt;/p&gt;&#10;&#10;&lt;p&gt;To identify this cut-off point you can use a scree plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/o20Tf.gif&quot; alt=&quot;Scree Plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you notice, after the first 3-4 features (components in this diagram) you get little extra information about the original problem (because the eigenvalues are small).  So if you create a new problem with only the first 4 features, you preserve most of the information that you have available while reducing the dimensionality and complexity of your original problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-19T22:08:18.837" Id="28798" LastActivityDate="2012-05-19T22:08:18.837" OwnerUserId="10322" ParentId="28795" PostTypeId="2" Score="2" />
  
  
  
&#10;\overline x_2 = 24.3, s_2 = 3.45, n_2 = 15$&lt;/p&gt;&#10;&#10;&lt;p&gt;Find the confidence interval for $\mu_1$. Then, separately, find the confidence interval for $\mu_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next part is basically saying: What would be the result of a test of $H_0: \mu_1 = \mu_2$ vs $H_1:\mu_1 \ne \mu_2$?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-05-20T08:22:54.090" Id="28810" LastActivityDate="2012-05-20T08:22:54.090" OwnerUserId="10448" ParentId="28809" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;When looking back, I think what I did not know, was the actual meaning of paired-tests. The ranksum-test in MATLAB is non-paired whereas the ranksign-test is a paired non-parametric test. In the paired test, both of the two possible ways of permuting the data are correct.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-20T14:29:48.743" Id="28818" LastActivityDate="2012-05-20T14:29:48.743" OwnerUserId="7339" ParentId="28727" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Segmented regression is flawed when you have auto-correlated data i.e. time series data. In the case of the interrupted experiment one know the point in time when the interruption occurs. Fitting local trend equations to the before and after assumes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;that there is one and only 1 trend in each of the two groups&lt;/li&gt;&#10;&lt;li&gt;there are no pulses or level shifts in either of the two groups&lt;/li&gt;&#10;&lt;li&gt;the variance of the errors in each of the two groups is constant not only weithin each group but across the two groups &lt;/li&gt;&#10;&lt;li&gt;the errors form the asssumed trend line within each of the groups are independent and identically distributed &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;None of these assumptions go untested with the regression with ARIMA errors approach when conducted properly. Your answer is then use the regression with ARIMA errors approach making sure that you validate the Gaussian assumptions that were discussed here. You might want to read a well written seminal book ( unfortunately using very dated procedures ) by McCleary and Hay (1980) . &lt;/p&gt;&#10;&#10;&lt;p&gt;Care should be  taken to ensure that the software you use has a &lt;strong&gt;intervention detection&lt;/strong&gt; procedure which empirically suggests the de facto breakpoint. Intervention modelling as usually presented assumes that the user knows the true point of the intervention/interruption. Try googling &quot;&lt;em&gt;automatic intervention detection&lt;/em&gt;&quot; to find out recent advances.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-20T18:50:37.037" Id="28829" LastActivityDate="2012-05-21T01:26:45.173" LastEditDate="2012-05-21T01:26:45.173" LastEditorUserId="179" OwnerUserId="3382" ParentId="28772" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Study Design:&lt;/strong&gt; I have a 2x3 factorial design, 2 levels of Time (2050 or 2100) by 3 levels of information (None/Control, Moderate, Extreme).&lt;/p&gt;&#10;&#10;&lt;p&gt;I set up some very specific contrasts when analyzing this design, in particular Control vs. Other, Moderate vs. Extreme, and 2050 vs. 2100. For one DV, the overall ANOVA was not significant but the Control vs. Other contrast was.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is the best way to interpret a nonsignificant ANOVA but a significant contrast? &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that ANOVA is used to lower familywise error rates, and I wouldn't want to fall into the trap of ignoring the overall ANOVA for pairwise comparisons. However, this is one of a select few contrasts that I planned ahead of time. Does that make a difference in interpretation, or should I simply consider the manipulation not significant?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-21T04:09:05.760" FavoriteCount="1" Id="28845" LastActivityDate="2013-09-08T17:20:25.793" LastEditDate="2012-05-21T04:39:00.653" LastEditorUserId="10227" OwnerUserId="10227" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;statistical-significance&gt;&lt;contrasts&gt;" Title="What if an overall ANOVA is not significant, but specific contrasts are?" ViewCount="6919" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm interested to know what a good practice is when generating/simulating data for testing or comparing modelling methods. I'm focusing on linear models and measuring prediction accuracy and measuring parameter estimate accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking at prediction accuracy (PSE) in terms of cross validated squared prediction error and parameter estimate accuracy in terms of $\|\beta-\hat{\beta}\|$ (MSE).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have attempted a bunch of simulations using what I believe to be a 'sound' method, but I am not happy with the results. I'm guessing I'm overlooking something. The issue is that adjacent parameter estimates are highly correlated with each other and I cannot figure out why. Another issue is that while I can generate datasets that show a large difference in parameter estimate accuracy, the prediction performance will be very similar. I am not sure why this happens, I would expect the methods with improved MSE to have much improved PSE over the other methods. Here's a typical example: &lt;a href=&quot;http://ohyur.com/random/typical.png&quot; rel=&quot;nofollow&quot;&gt;http://ohyur.com/random/typical.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I will outline what I have attempted.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Set the true parameter estimates, $\beta$ &lt;/li&gt;&#10;&lt;li&gt;Set correlation structure for the predictor variables&lt;/li&gt;&#10;&lt;li&gt;Generate the predictors with specified correlation structure, $X$. I've tried using mvrnorm() in R to generate N(0,1) columns and mvrunif() function to generate U(-1,1) columns. Both with the specified correlation structure.&lt;/li&gt;&#10;&lt;li&gt;Generate the response using $Y=\mu + X\beta+\epsilon$, where $\epsilon$ is $N(0,\sigma$).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;To me, that sounds like a reasonably method of generating data for a linear model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example of what the generated data might look like: &#10;&lt;a href=&quot;http://ohyur.com/random/data.png&quot; rel=&quot;nofollow&quot;&gt;http://ohyur.com/random/data.png&lt;/a&gt; &#10;($\mu=3,$ $\beta=(3,2,-3,5,2,-3,5,5)$, $\sigma=15$, $\text{cor}(x_i,x_j)=0.9^{|i-j|}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now in my simulations, in each replication I'm generating data according to the above steps. Then fitting some models, estimating the CV PSE and calculating the MSE and storing the results. Here is a boxplot of the results for 3 different modelling methods over 100 replications.&#10;&lt;a href=&quot;http://ohyur.com/random/psemse.png&quot; rel=&quot;nofollow&quot;&gt;http://ohyur.com/random/psemse.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Now&lt;/strong&gt;, most interestingly to me, Here is a pairwise plot of the estimated parameters for a full OLS model. Why are the adjacent predictors correlated?&#10;&lt;a href=&quot;http://ohyur.com/random/estimates.png&quot; rel=&quot;nofollow&quot;&gt;http://ohyur.com/random/estimates.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing I'm coverned about is that in the real world, some predictors are more correlated with the response than others, and not just due to effect size. This isn't really accounted for in my setup. But I'm really just after the simplest possible 'sound' method.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any insight/comments or recommendations on how to properly generate data for testing liner models would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-21T05:38:43.923" Id="28848" LastActivityDate="2012-06-20T08:34:48.997" LastEditDate="2012-05-21T06:23:27.237" LastEditorUserId="845" OwnerUserId="845" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;dataset&gt;&lt;simulation&gt;" Title="Simulating or generating datasets for testing modelling methodologies" ViewCount="399" />
  <row AnswerCount="5" Body="&lt;p&gt;I am writing a program to predict the click through rates of online ads. Two important notes about this problem:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;click through rates are very small (like 0.1%)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;click through rates depend on several parameters (like size of the ad, country in which ad is shown, whether the user has seen this ad earlier etc)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As I employ statistical/machine learning techniques, I would like to measure the performance of my techniques on historical data. I cannot employ metrics like precision or accuracy since all I predict is probability rather than predicting occurrence or non-occurrence of event.&lt;/p&gt;&#10;&#10;&lt;p&gt;In such a case, how should I measure the performance of my algorithm?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-21T07:19:32.167" Id="28852" LastActivityDate="2012-10-05T14:11:39.073" OwnerUserId="11447" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;logistic&gt;" Title="Performance metric for algorithm predicting probability of low probability events" ViewCount="461" />
  
  
  <row AcceptedAnswerId="28907" AnswerCount="1" Body="&lt;p&gt;Suppose we want to cluster a data stream of unknown number of clusters, and estimate them using particle filters. With particle filters, we need to know $P(x_t | x_{t-1})$ and $P(z_t | x_t)$ (where z refer to the data (reports), and x refer to the estimated states (hypothesis)).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then my question is: how can we define $P(x_t | x_{t-1})$ (transition probability) and $P(z_t | x_t)$ (observation/likelihood probability) in this context (online clustering) ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-21T15:28:12.780" FavoriteCount="1" Id="28872" LastActivityDate="2012-06-15T09:25:53.780" LastEditDate="2012-06-15T09:25:53.780" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;nonparametric&gt;&lt;online&gt;&lt;sequential-analysis&gt;&lt;particle-filter&gt;" Title="clustering with particle filters" ViewCount="273" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for some libraries in R that can do incremental learning (also called online or sequential learning). The use case of such learning in comparison to traditional batch methods would be to process large amounts of data. Such practices include streams and data from sensors, where it is not feasible to use always the same model or to rebuild the model from scratch every time. Any machine learning algorithm that can use only single new example to change the model would suffice. However, the model itself must not hold on to old data (as you can imagine it would soon get too big), instead just calculating some statistics about data. &lt;/p&gt;&#10;&#10;&lt;p&gt;For multivariate regression, online approach like &lt;a href=&quot;http://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot; rel=&quot;nofollow&quot;&gt;Stochastic gradient descent&lt;/a&gt; would be a good option. For regression / model trees something like &lt;a href=&quot;http://www.liaad.up.pt/~kdus/DAMI10.pdf&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt; comes to mind. I am looking for such library where relatively good &lt;strong&gt;prediction&lt;/strong&gt; accuracy (with respect to traditional batch methods) could be achieved based on the evolving model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-21T17:38:09.293" Id="28879" LastActivityDate="2012-05-22T13:22:47.620" LastEditDate="2012-05-22T13:22:47.620" LastEditorUserId="88" OwnerUserId="2889" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;online&gt;&lt;sequential-analysis&gt;" Title="Incremental learning methods in R" ViewCount="268" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to understand whether the output of an algorithm I developed agrees with the ratings of external observers. The number of observers is 10 and each of them rated on 2 3-point-Likert items 10 stimuli. The algorithm provides one measure (a real number) for each stimulus. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking about the use of intraclass correlation among observers and then to use this possible agreement (but I don't know if this sounds good and how to do this) to validate the output of the algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-22T09:29:47.263" Id="28918" LastActivityDate="2012-05-22T13:20:18.827" LastEditDate="2012-05-22T13:20:18.827" LastEditorUserId="88" OwnerUserId="11471" PostTypeId="1" Score="1" Tags="&lt;likert&gt;&lt;validation&gt;&lt;intraclass-correlation&gt;" Title="Intraclass correlation to validate a measure" ViewCount="127" />
  
  <row Body="&lt;p&gt;There isn't enough theory to provide a unique answer.  This is one of several reasons to entertain the use of the bootstrap or the double bootstrap.  More information about the bootstrap for model validation may be found in &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/RmS/rms.pdf&quot; rel=&quot;nofollow&quot;&gt;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/RmS/rms.pdf&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-22T11:37:53.087" Id="28922" LastActivityDate="2012-05-22T11:37:53.087" OwnerUserId="4253" ParentId="28921" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Covariance is a quantity used to measure the strength and direction of the linear relationship between two variables. The covariance between $X$ and $Y$ is defined as $${\rm cov}(X,Y) = E \left[ \left( X-E(X) \right) \left( Y-E(Y) \right) \right] = E(XY) - E(X)E(Y) $$ Since the magnitude is difficult to interpret in isolation, the covariance is often scaled by the standard deviations of $X$ and $Y$ to produce the Pearson product-moment correlation coefficient. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-22T12:36:53.030" Id="28928" LastActivityDate="2013-09-02T13:37:33.320" LastEditDate="2013-09-02T13:37:33.320" LastEditorUserId="27581" OwnerUserId="27581" PostTypeId="5" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to sample the Gaussian markov random field  or say multivariate gaussian distribution with some spatial correlation given by the precision matrix Q.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the algorithm that I am using&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Compute the Cholesky factorization Q=LL'&#10;Sample z ~ N(0,I)&#10;Solve L'x = z&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where x is the samples obtained.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a grid of 20x20. And I have precision matrix such that any point in the spatial grid is related to its four neighbors. So if my Q matrix is of size 400x400, then the first row which is for the first point, the Q matrix is nonzero for columns 1,2 and 21 where 1 denotes the first element itself, 2 denotes the element right of it and column 21 denotes the element below it. So, I got the samples x from it and I plotted them in a grid of 20x20. However, I couldn't see much relation. May be I am wrongly interpreting the plot. So anyone with any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Baoki.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, I tried to increase the window size to 5x5Here is my code. However, when I plot the generated data, they don't seem to be spatially correlated or it's getting worse.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;%Initialize the grid&#10;row = 20;&#10;column = 20;&#10;&#10;N = row*column;&#10;linearMatrix = zeros(row*column,3);&#10;delta = 0.0001;&#10;&#10;index=1;&#10;for i=1:row&#10;    for j=1:column&#10;        linearMatrix(index,1) = i;&#10;        linearMatrix(index,2) = j;&#10;        linearMatrix(index,3) = index;&#10;        index = index+1;&#10;    end&#10;end&#10;&#10;sparseIndexX = zeros(N,1);&#10;sparseIndexY = zeros(N,1);&#10;sparseIndexValue = zeros(N,1);&#10;&#10;index = 1;&#10;for j=1:N&#10;    x = linearMatrix(j,1);&#10;    y = linearMatrix(j,2);&#10;    for k=1:N&#10;        x1 = linearMatrix(k,1);&#10;        y1 = linearMatrix(k,2);&#10;&#10;        if(j == k)&#10;            %diagonal element&#10;            sparseIndexX(index,1) = j;&#10;            sparseIndexY(index,1) = k;&#10;&#10;&#10;            % For the four corner elements&#10;            if j == 1 || j == N || (x == 1 &amp;amp;&amp;amp; y == column) || (x == row &amp;amp;&amp;amp; y == 1)&#10;                sparseIndexValue(index,1) = 8;&#10;            elseif (x == 1 &amp;amp;&amp;amp; y == 2)  || (x == 1 &amp;amp;&amp;amp; y == column-1) || (x == 2 &amp;amp;&amp;amp; y == 1)  || (x == 2 &amp;amp;&amp;amp; y == column) || + ...&#10;               (x == row-1 &amp;amp;&amp;amp; y == 1) ||  (x == row-1 &amp;amp;&amp;amp; y == column) || (x == row &amp;amp;&amp;amp; y == 2) || (x == row &amp;amp;&amp;amp; y == column-1)&#10;                % For the elements at the edge but not the corner&#10;                sparseIndexValue(index,1) = 11;&#10;            elseif (x==2 &amp;amp;&amp;amp; y==2) || (x==2 &amp;amp;&amp;amp; y==column-1) || (x==row-1 &amp;amp;&amp;amp; y==2) || (x==row-1 &amp;amp;&amp;amp; y==column-1)&#10;                sparseIndexValue(index,1) = 15;&#10;            elseif (x==2 &amp;amp;&amp;amp; y&amp;gt;2) || (x==2 &amp;amp;&amp;amp; y&amp;lt;column-1) || (x==row-1 &amp;amp;&amp;amp; y&amp;gt;2) || (x==row-1 &amp;amp;&amp;amp; y&amp;lt;column-1) || (x&amp;gt;2 &amp;amp;&amp;amp; y==2) || (x&amp;lt;row-1 &amp;amp;&amp;amp; y==2) || (x&amp;gt;2 &amp;amp;&amp;amp; y==column-1) || (x&amp;lt;row-1 &amp;amp;&amp;amp; y==column-1)&#10;                sparseIndexValue(index,1) = 19;&#10;            elseif x &amp;gt; 2 &amp;amp;&amp;amp; y &amp;gt; 2 &amp;amp;&amp;amp; x &amp;lt; row-1 &amp;amp;&amp;amp; y &amp;lt; column-1&#10;                % For the elements in the middle&#10;                sparseIndexValue(index,1) = 24;&#10;&#10;            else&#10;                sparseIndexValue(index,1) = 14;&#10;            end&#10;&#10;            index = index + 1;&#10;        else&#10;            %if the elements are adjacent &#10;            if (abs(x-x1) &amp;lt;= 2 &amp;amp;&amp;amp; abs(y-y1) == 0) || (abs(x-x1) == 0 &amp;amp;&amp;amp; abs(y-y1) &amp;lt;= 2) || (abs(x-x1) &amp;lt;= 2 &amp;amp;&amp;amp; abs(y-y1) &amp;lt;= 2)&#10;                sparseIndexX(index,1) = j;&#10;                sparseIndexY(index,1) = k;&#10;                sparseIndexValue(index,1) = -1;&#10;                index = index + 1;&#10;            end&#10;        end&#10;    end&#10;end&#10;&#10;%Generate the precision matrix for spatial correlation&#10;QSpatial = sparse(sparseIndexX', sparseIndexY', sparseIndexValue', N, N);&#10;&#10;Q=QSpatial;&#10;&#10;%Make the matrix strictly diagonally dominant so that it is positive definite and&#10;%invertible&#10;for i=1:N&#10;    Q(i,i) = Q(i,i) + delta;&#10;end&#10;&#10;v=zeros(N,1);&#10;&#10;%Get the cholesky decomposition&#10;%Step 1&#10;L=chol(Q,'lower');&#10;&#10;%Sample from the normal distribution&#10;%Step 2&#10;z=randn(1,N);&#10;z=z';&#10;&#10;%Back substitution method to generate the sample&#10;%Step 3&#10;for i=N:-1:1&#10;    total = 0;&#10;    if i~= N&#10;        for j=(i+1):N&#10;            total=total + L(j,i) * v(j,1);&#10;        end&#10;    end&#10;    v(i,1) = 1/L(i,i) * (z(i,1) - total);&#10;end&#10;&#10;%Step 4&#10;x=v;&#10;&#10;&#10;x=reshape(x,20,20);&#10;x=x';&#10;imagesc(x);&#10;colorbar;  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is the plot of the samples &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BBB8k.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;Does anyone have any suggestion?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-05-22T15:56:40.877" Id="28939" LastActivityDate="2012-05-24T04:09:02.910" LastEditDate="2012-05-24T04:09:02.910" LastEditorUserId="11453" OwnerUserId="11453" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;matlab&gt;" Title="Simulating unconditional Gaussian markov random field" ViewCount="664" />
  <row Body="&lt;p&gt;Outlier detection in time series encompasses a large body of literature.  First you would want to have a time series model that fit well to the data when there were no suspect observations.  If for example an ARMA model works you might assume that the noise distribution is Gaussian.  There are at least two types of outliers.  Fox defined them in a 1972 paper. The best source to start with on this subject is the latest edition of Barnett and Lewis' &quot;Outliers in Statistical Data&quot; published by Wiley.  They have a chapter on time series.  My 1982 paper with Downing took the approach of looking at influence functions for autocorrwlation.  Our idea is that if an observation had a big effect on one of more of the lagged correlations it would also affect the model parameters adversely.  Martin, Yohai and others defined influence functionals for time series in a different way that seems to have better theoretical justification but addresses the same issue .  Ruel Tsay, George Tiao and others have also published work on outliers in time series.  I am less familiar with that. But our colleague IrishStat can probably comment on that and more. In the process of improving his autobox software over the years IrishStat and his son Tom have invested time into keeping up on the literature about outliers and level shifts (sometimes called interventions) in order to make their product state-of-the-art.&#10;Just like with outliers in data that are not time dependent any outliers that are detected using time series methods should be studied to see why they occurred. Were they measurement errors?  Maybe a change in the behavior of the process?  Maybe a temporary intervention (like the Federal Reserve changing interest rates as an example)?  The reason if it can be found will dictate how the outlier should be treated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-22T18:13:10.890" Id="28950" LastActivityDate="2013-07-06T21:22:37.420" LastEditDate="2013-07-06T21:22:37.420" LastEditorUserId="17230" OwnerUserId="11032" ParentId="28944" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="28952" AnswerCount="1" Body="&lt;p&gt;I am interested to learn about cognitive robotics and its application in situation assessment using natural language processing esp monitoring of human activities/disaster management.The processing would involve data visualization techniques. Can anyone suggest what is the starting point,which books to follow,the programming language/platform and some links to reference/example codes to begin with.Do I need to use ROS(robotic Operating system)?I came across GOLOG and situational calculus.Can anyone suggest where to download GOLOG &amp;amp; how to use it?Any other help would be highly solicited.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-22T19:09:19.967" FavoriteCount="1" Id="28951" LastActivityDate="2012-05-22T22:24:54.617" OwnerUserId="9266" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;books&gt;&lt;references&gt;" Title="Basic knowledge about cognitive robotics" ViewCount="127" />
  <row Body="&lt;p&gt;If you think there is an interaction and no main effect model it that way and interpret its effect based on the coefficient of the interaction term.  There is guidance to say that interactions should only be looked at if the main effects are significant.  Sometimes there is justification for that.  But it is not a law of statistics that is set in stone.  It would be nice though if you have some subject matter rationale for the existence of the interaction without a main effect.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-22T20:49:04.617" Id="28956" LastActivityDate="2012-05-22T20:49:04.617" OwnerUserId="11032" ParentId="28936" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I rarely do this, but your question is so general, and covers so many topics, that I don't see any other choice than to point you to something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And tell you to simply read.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a newer version for this book here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1584884258&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Linear-Models-Chapman-Statistical-Science/dp/1584884258/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-22T21:00:30.133" Id="28958" LastActivityDate="2012-05-22T21:00:30.133" OwnerUserId="253" ParentId="28957" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I have made some statistics games for teaching. They deal with confidence intervals, hypothesis tests (Neyman-Pearson) and significance tests (Fisherian).&lt;/p&gt;&#10;&#10;&lt;p&gt;They are excellent tools for engaging students in lectures/workshops and a subset of the students spend quite a lot of time playing them.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been going to make an online high scores leader-board to make the game aspect more competitive but, well, I haven't got around to it ;-)&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2012-05-22T21:18:16.023" CreationDate="2012-05-22T21:18:16.023" Id="28961" LastActivityDate="2012-05-22T21:18:16.023" OwnerUserId="1679" ParentId="28925" PostTypeId="2" Score="-5" />
  
  <row Body="&lt;p&gt;From Mood et al. (page 57, 1974):&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;A random variable $X$ will be defined to be discrete if the range of $X$ is countable. If a random variable $X$ is discrete, then its corresponding cumulative distribution function $F_X( . )$ will be defined to be discrete.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0070854653&quot; rel=&quot;nofollow&quot;&gt;Mood, A. M., Graybill, F. A., &amp;amp; Boes, D. C. (1974). Introduction to theory of statistics. (B. C. Harrinson &amp;amp; M. Eichberg, Eds.) (3rd ed., p. 564). McGraw-Hill, Inc.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-23T00:58:04.407" Id="28976" LastActivityDate="2013-07-25T00:23:37.860" LastEditDate="2013-07-25T00:23:37.860" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  
  
  
  
  
  <row AcceptedAnswerId="29030" AnswerCount="1" Body="&lt;p&gt;I am asking to find out why a factor may become non significant after I define the nested design. The following are the two aov analysis in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obs (subject #) is the within subject factor and stim3 &amp;amp; label (1st &amp;amp; 2n stimuli manipulations) are the nested within subject.  stim3 &amp;amp; label are nested within obs. The analysis shows that 'label' is only significant when the analysis is done without defining the nested design. I wanted to ask why this is the case and how sound is the choice to define the nested design.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; &amp;gt; summary(aov(resp2 ~ stim3*label+Error(obs/(stim3*label)), tl ) )&#10;&#10;    Error: obs&#10;              Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;    Residuals 11   5.25   0.478               &#10;&#10;    Error: obs:stim3&#10;              Df Sum Sq Mean Sq F value  Pr(&amp;gt;F)    &#10;    stim3      1  165.4   165.4     425 3.8e-10 ***&#10;    Residuals 11    4.3     0.4                    &#10;    ---&#10;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;    Error: obs:label&#10;              Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  &#10;    label      1   6.77    6.77    4.77  0.051 .&#10;    Residuals 11  15.61    1.42                 &#10;    ---&#10;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;    Error: obs:stim3:label&#10;                Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;    stim3:label  1  0.026  0.0255    0.15   0.71&#10;    Residuals   11  1.893  0.1721               &#10;&#10;    Error: Within&#10;                Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;    Residuals 1872    279   0.149               &#10;&#10;    &amp;gt; summary(aov(resp2 ~ stim3*label+Error(obs), tl ) )&#10;&#10;    Error: obs&#10;              Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;    Residuals 11   5.25   0.478               &#10;&#10;    Error: Within&#10;                  Df Sum Sq Mean Sq F value  Pr(&amp;gt;F)    &#10;    stim3          1  165.4   165.4 1048.60 &amp;lt; 2e-16 ***&#10;    label          1    6.8     6.8   42.95 7.2e-11 ***&#10;    stim3:label    1    0.0     0.0    0.16    0.69    &#10;    Residuals   1905  300.4     0.2                    &#10;    ---&#10;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks for reading!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-23T16:09:35.483" FavoriteCount="1" Id="29013" LastActivityDate="2012-05-23T20:06:18.743" LastEditDate="2012-05-23T20:06:18.743" LastEditorUserId="1084" OwnerUserId="1084" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="How important is it to define the nested design in a within subject ANOVA?" ViewCount="427" />
  
  
  <row Body="&lt;p&gt;This is non-trivial exercise since there's lots of dynamic strategic behavior. Your bid depends on what the other bidders are doing (and vice versa). For example, say you stumbled on an auction for a Suny DVD player. You are probably the only participant, and your bid just has to exceed the minimum, since you're not worried about the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some examples of auctions with &lt;a href=&quot;http://www.heinz.cmu.edu/research/281full.pdf&quot; rel=&quot;nofollow&quot;&gt;new digital cameras (Vogt et al, 2005)&lt;/a&gt;, &lt;a href=&quot;http://www2.ftc.gov/be/workshops/internetauction/06_Robert_Zeithammer_FowardLookingBidding.pdf&quot; rel=&quot;nofollow&quot;&gt;mp3 players and DVDs (Zeithammer, 2005)&lt;/a&gt;, and &lt;a href=&quot;http://www-personal.umich.edu/~backus/AuctionDemand_Jan.pdf&quot; rel=&quot;nofollow&quot;&gt;a method to back out demand from bid data (Backus and Lewis, 2010)&lt;/a&gt;. I think all of these assume a unit demand, meaning that each buyer purchases only a single item, so your problem, where reselling is an option, is considerably harder.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-23T19:38:02.930" Id="29029" LastActivityDate="2012-05-23T19:38:02.930" OwnerUserId="7071" ParentId="29019" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to do power computation for testing differnce between two log normal means with R. Can you write using R code? &#10;best wishes...&lt;/p&gt;&#10;" ClosedDate="2012-08-09T23:16:04.800" CommentCount="1" CreationDate="2012-05-23T20:27:34.640" Id="29033" LastActivityDate="2012-05-23T20:51:47.517" LastEditDate="2012-05-23T20:44:14.177" LastEditorUserId="88" OwnerUserId="11507" PostTypeId="1" Score="-3" Tags="&lt;r&gt;&lt;mean&gt;&lt;power&gt;&lt;lognormal&gt;" Title="How do I make power computation for testing differnce between two log normal means with R? " ViewCount="87" />
  <row Body="&lt;p&gt;Okay they say p-word is is the probability that come about from what is called the likelihood equation.  The data is the given prefix.  The likelihood is given the prefix the probability that to prefix corresponds to a given word.  So for example suppose someone typed yuo.  A very likely word would be you since o and u are close on the keyboard and in typing the key for o may have accidently been hit first.  Another possibility is your.  That could occur if the same mistake was made with the o and the u but the person forgot to type the r or hit it too softly.  Based on this description you would have a higher likelihood than your. But to quantify this statistically what is done is that you collect lots of typing samples that covers many words and many typos for the words.  You find out what the intended word was.  So your likeihood for each word is the number of times the actual word is you when yuo is typed divided by the total numner of times yuo occurs.  Perhaps that would be 0.80 in this case and 0.15 for your when yuo occurs.  Now this could be refined based on sentence structure and context but hat become much more complicated and is not what they are doing here.  In this case we say that the probability that the word is you given that yuo is typed is 0.80 and for your it is 0.15.  Since you has the highest probaility we call it the maximum likelihood estimate for the correct word given the data yuo.  So what they cll p-word would in this case be 0.80 for the word you.  Now for some reason not completely clear to me they choose to score the word log base 2 of 0.80.  That means the score is the value x so that 2 raised to the power x is 0.80.  Using Excel you can determine that x=0.32200 approximately.  Since the log is a monotonic function meaning as p increases x always increases the highest likelihood will correspond to the highest score.  I hope this give you a good feel for what is going on with the statistics.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-23T22:29:13.137" Id="29040" LastActivityDate="2012-05-23T23:09:43.450" LastEditDate="2012-05-23T23:09:43.450" LastEditorUserId="11032" OwnerUserId="11032" ParentId="29037" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="29093" AnswerCount="1" Body="&lt;p&gt;R and Statistics newbie here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## LOGIT MODEL:&#10;library(car)&#10;mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=&quot;logit&quot;))&#10;&#10;## PROBABILITY CURVE:&#10;all.x &amp;lt;- expand.grid(won=unique(won), bid=unique(bid))&#10;y.hat.new &amp;lt;- predict(mod1, newdata=all.x, type=&quot;response&quot;)&#10;plot(bid&amp;lt;-000:1000,predict(mod1,newdata=data.frame(bid&amp;lt;-c(000:1000)),type=&quot;response&quot;), lwd=5, col=&quot;blue&quot;, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-23T23:02:26.973" FavoriteCount="5" Id="29044" LastActivityDate="2013-05-10T21:29:38.090" LastEditDate="2012-05-24T13:01:57.183" LastEditorUserId="88" OwnerUserId="3310" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;logistic&gt;" Title="Plotting confidence intervals for the predicted probabilities from a logistic regression" ViewCount="8990" />
  <row AcceptedAnswerId="29224" AnswerCount="2" Body="&lt;p&gt;I need to prove an induction step. $X_i$ are independently distributed with the distribution function $1-F_i=x^{-\alpha}L_{i}(x)$ where $\alpha \geq 0$ and $L_{i}(x)$ is regularly varying (If the limit $g(a)=\lim\limits_{x\rightarrow\infty}\frac{L(ax)}{L(x)}$ is finite and nonzero for $a &amp;gt;0$, then L is regularly varying).&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lim\limits_{x\rightarrow \infty} \frac{P(X_1+...+X_n&amp;gt;x) }{P(X_1 &amp;gt; x)+...+P(X_n&amp;gt;x)} = 1$ is true.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we have to show:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lim\limits_{x\rightarrow \infty} \frac{P(X_1+...+X_{n+1}&amp;gt;x) }{P(X_1 &amp;gt; x)+...+P(X_{n+1}&amp;gt;x)} = 1.$&lt;/p&gt;&#10;&#10;&lt;p&gt;How do we show this?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-24T03:10:14.073" FavoriteCount="0" Id="29052" LastActivityDate="2012-05-28T02:08:23.950" LastEditDate="2012-05-28T01:34:31.290" LastEditorUserId="10749" OwnerUserId="10749" PostTypeId="1" Score="5" Tags="&lt;convergence&gt;" Title="Limit of a convolution and sum of distribution functions" ViewCount="453" />
  <row Body="&lt;p&gt;I don't know what the perfect solution is here. I might ask others in the same field, or who have experience with that journal what has worked for them or what they suggest.  My initial thought would be to label y-axes on the left hand side of the plots that are in the leftmost column.  Then I might decrease the vertical margins between the plots relative to the horizontal margins between the plots.  The gestalt will make the rows salient and visually highlight that they all share the same y-axis labels on the left.  (You'd have to see how this looks, but I think it might work.)  Furthermore, I would group the plots with the same x-axes by column if possible (even though I gather you're still going to have x-axes for every plot), and definitely point these facts out in the figure caption.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I should say, at this point, that I can't read your figure, I wonder if you could make and submit a high-resolution version and let the typesetting people reduce the resolution or figure out what they want to do with it.  For example, sometimes they can keep a higher-resolution version at the journal's website or something.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-24T03:28:48.870" Id="29054" LastActivityDate="2012-05-24T03:28:48.870" OwnerUserId="7290" ParentId="29025" PostTypeId="2" Score="1" />
  
  <row AnswerCount="4" Body="&lt;p&gt;Given pairs $(x_i, y_i), x_i \in R^n , y_i \in R$ we want to solve minimization&#10;problem (logistic regression):$\min \frac{1}{2} ||w||^2 + \sum_i^{i=m}\log(1+\exp(-y w\cdot x_i))$. How to do that? I know the dual form is:&#10;$ \min_{\alpha} D(\alpha)= \frac{1}{2}\sum_{i, j}\alpha_i \alpha_j y_i y_j x_i \cdot x_j + \sum_i\alpha_i \log(\alpha_i) + (C-\alpha_i)\log(C-\alpha_i)$, subject to: $0\le \alpha_i\le C$. How to get dual?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-24T04:15:21.310" Id="29059" LastActivityDate="2013-03-27T23:37:12.517" OwnerUserId="8361" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;logistic&gt;" Title="logistic regression. How to get dual function?" ViewCount="939" />
  
  <row AcceptedAnswerId="29080" AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Short question:&lt;/strong&gt; &#10;I am looking for books that deal with correlated data in a systematic and theoretical way. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Long version:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Right now I am developing approximation algorithms for time series data (more concrete I want to approximate a function $f:[a,b]\rightarrow\mathbb{R}^3$ where $[a,b]$ is a time interval). The raw approximation of that data shall not be of concern for this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, what has a certain relevance for my works is a proof of correctness of the developed algorithm. That means that I have to take the measurement error of the data that is to be approximated into account. Problem with that is that I do not know a lot on the distribution of the error. Furthermore, I know that high absolute errors are likely to be correlated. Thus, I need to use statistical methods that do not fail completely with correlated data for my theoretical examination of the data. (Intuitively I would guess that quantiles already might be enough for my purpose, if I can justify their use reasonably.)&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem with that is that I do not have a strong knowledge on methods that hold with correlated data. All statistical methods I remember have that small but important &lt;em&gt;iid&lt;/em&gt; among the list of their requirements.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, the question is: Can you recommend any good books that build up statistical methods for correlated data? As I'm looking for methods and used theory, I do need a book dealing with probability theory rather than just practical methods. My background on statistics is a bit rusty but certainly should be enough for understanding theoretical books as well.&#10;For me it is important to gain an understanding on the mathematics used for that kind of problem rather than just to get to know a lot of different tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-24T07:56:19.563" Id="29066" LastActivityDate="2012-05-24T20:03:59.867" OwnerUserId="6246" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;books&gt;&lt;big-list&gt;" Title="Books on non-parametric theory with correlated data" ViewCount="153" />
  <row Body="&lt;p&gt;Andrew Gelman's tentative advice on that is based on the significance and the sign of the predictors:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If predictor is significant: keep it (if it has the unexpected sign: think hard about it!)&lt;/li&gt;&#10;&lt;li&gt;If predictor is not significant but in the expected direction: keep it. It will not improve the prediction dramatically, but won't do much hurt.&lt;/li&gt;&#10;&lt;li&gt;If predictor is not significant and in the unexpected direction: set it to zero&#10;(p. 69 in &quot;Data Analysis Using Regression and Multilevel/Hierarchical Models&quot;, 2007)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This presupposes to think about the expected directions of predictors before running the model ...&lt;/p&gt;&#10;&#10;&lt;p&gt;Gelman, A., &amp;amp; Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-24T07:57:41.297" Id="29067" LastActivityDate="2012-05-24T07:57:41.297" OwnerUserId="6082" ParentId="28936" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Which distance-based clustering methods have you tried?&lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds like you are exactly looking for DBSCAN: &lt;a href=&quot;https://en.wikipedia.org/wiki/DBSCAN&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/DBSCAN&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-24T09:04:47.097" Id="29071" LastActivityDate="2012-05-24T09:04:47.097" OwnerUserId="7828" ParentId="29068" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;In short, no, you don't just add the limits. Maybe if the predictions were perfectly correlated, but that's not usually the case at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;Typically (if the model assumes independence) and you want an interval for a sum of predicted values, you might then think that you can treat the predictions as independent, but they generally aren't independent even when the observations are, because the predictions generally share parameter estimates.&lt;/p&gt;&#10;&#10;&lt;p&gt;In ordinary regression it's fairly straightforward; you can work out the mean and standard deviation of the sum and construct a t interval similar to the way you would for a single prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the model is the multiple regression model $y = X \beta + e$ with $e$ ~ $ N(0, \sigma^2 I)$,  and you're predicting a vector of future values, $y_f$, where you have a set of predictors for those predictions, $X_f$.&lt;/p&gt;&#10;&#10;&lt;p&gt;then you want an interval for $a' y_f$  (where in your case, $a$ is a vector of $1$s)&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $R = a' (y_f - \hat{y}_f)$ ~ $N(0, \sigma^2 m)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $m = a' (I + X_f (X'X)^{-1} X_f') a$ &lt;/p&gt;&#10;&#10;&lt;p&gt;so $Q = R/(\sqrt{m}.s)$ is distributed as (standard) Student t with d.f. the d.f. in the estimate of the variance, $\sigma^2$, which for regression is normally $n-p$, where $p$ is the number of predictors including the constant. From the interval for Q, you can then back out an interval for R and then a prediction interval for $a'y_f$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming I didn't screw up along the way.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-24T09:53:53.317" Id="29076" LastActivityDate="2012-05-24T09:53:53.317" OwnerUserId="805" ParentId="14987" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;The code you used estimates a logistic regression model using the &lt;code&gt;glm&lt;/code&gt; function. You didn't include data, so I'll just make some up.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1234)&#10;mydat &amp;lt;- data.frame(&#10;    won=as.factor(sample(c(0, 1), 250, replace=TRUE)), &#10;    bid=runif(250, min=0, max=1000)&#10;)&#10;mod1 &amp;lt;- glm(won~bid, data=mydat, family=binomial(link=&quot;logit&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A logistic regression model models the relationship between a binary response variable and, in this case, one continuous predictor. The result is a logit-transormed probability as a linear relation to the predictor. In your case, the outcome is a binary response corresponding to winning or not winning at gambling and it is being predicted by the value of the wager. The coefficients from &lt;code&gt;mod1&lt;/code&gt; are given in logged odds (which are difficult to interpet), according to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{logit}(p)=\log\left(\frac{p}{(1-p)}\right)=\beta_{0}+\beta_{1}x_{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To convert logged odds to probabilities, we can translate the above to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p=\frac{\exp(\beta_{0}+\beta_{1}x_{1})}{(1+\exp(\beta_{0}+\beta_{1}x_{1}))}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You can use this information to set up the plot. First, you need a range of the predictor variable: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plotdat &amp;lt;- data.frame(bid=(0:1000))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then using &lt;code&gt;predict&lt;/code&gt;, you can obtain predictions based on your model &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;preddat &amp;lt;- predict(mod1, newdata=plotdat, se.fit=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the fitted values can also be obtained via &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod1$fitted&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By specifying &lt;code&gt;se.fit=TRUE&lt;/code&gt;, you also get the standard error associated with each fitted value. The resulting &lt;code&gt;data.frame&lt;/code&gt; is a matrix with the following components: the fitted predictions (&lt;code&gt;fit&lt;/code&gt;), the estimated standard errors (&lt;code&gt;se.fit&lt;/code&gt;), and a scalar giving the square root of the dispersion used to compute the standard errors (&lt;code&gt;residual.scale&lt;/code&gt;). In the case of a binomial logit, the value will be 1 (which you can see by entering &lt;code&gt;preddat$residual.scale&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;). If you want to see an example of what you've calculated so far, you can type &lt;code&gt;head(data.frame(preddat))&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next step is to set up the plot. I like to set up a blank plotting area with the parameters first:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with(mydat, plot(bid, won, type=&quot;n&quot;, &#10;    ylim=c(0, 1), ylab=&quot;Probability of winning&quot;, xlab=&quot;Bid&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now you can see where it is important to know how to calculate the fitted probabilities. You can draw the line corresponding to the fitted probabilities following the second formula above. Using the &lt;code&gt;preddat data.frame&lt;/code&gt; you can convert the fitted values to probabilities and use that to plot a line against the values of your predictor variable.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with(preddat, lines(0:1000, exp(fit)/(1+exp(fit)), col=&quot;blue&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Finally, answer your question, the confidence intervals can be added to the plot by calculating the probability for the fitted values &lt;code&gt;+/-&lt;/code&gt; 1.96 times the standard error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with(preddat, lines(0:1000, exp(fit+1.96*se.fit)/(1+exp(fit+1.96*se.fit)), lty=2))&#10;with(preddat, lines(0:1000, exp(fit-1.96*se.fit)/(1+exp(fit-1.96*se.fit)), lty=2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The resulting plot (from the randomly generated data) should look something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/29nbh.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For expediency's sake, here's all the code in one chunk:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1234)&#10;mydat &amp;lt;- data.frame(&#10;    won=as.factor(sample(c(0, 1), 250, replace=TRUE)), &#10;    bid=runif(250, min=0, max=1000)&#10;)&#10;mod1 &amp;lt;- glm(won~bid, data=mydat, family=binomial(link=&quot;logit&quot;))&#10;plotdat &amp;lt;- data.frame(bid=(0:1000))&#10;preddat &amp;lt;- predict(mod1, newdata=plotdat, se.fit=TRUE)&#10;with(mydat, plot(bid, won, type=&quot;n&quot;, &#10;    ylim=c(0, 1), ylab=&quot;Probability of winning&quot;, xlab=&quot;Bid&quot;))&#10;with(preddat, lines(0:1000, exp(fit)/(1+exp(fit)), col=&quot;blue&quot;))&#10;with(preddat, lines(0:1000, exp(fit+1.96*se.fit)/(1+exp(fit+1.96*se.fit)), lty=2))&#10;with(preddat, lines(0:1000, exp(fit-1.96*se.fit)/(1+exp(fit-1.96*se.fit)), lty=2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Note: This is a heavily edited answer in an attempt to make it more relevant to stats.stackexchange.)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-24T13:56:14.557" Id="29093" LastActivityDate="2013-05-10T21:29:38.090" LastEditDate="2013-05-10T21:29:38.090" LastEditorUserId="7290" OwnerUserId="9249" ParentId="29044" PostTypeId="2" Score="13" />
  
  <row Body="&lt;p&gt;Overfitting does not always hurt prediction.  In one type of overfitting the variables are highly correlated. So they could be almost functionally related perhaps in a linear fashion.  Suppose that X1=2X2+5X3 exactly without error.  Then you could use any two of the variables in the model and get exactly the same result.  The equations can look very different and still both fit and predict well.  With another type of overfitting, it can lead to a poorer model than one with fewer parameters (when the problem is not collinearity) but rather the inclusion of too many variables results in fitting the noise as well as the signal.  For example suppose we have a response that is a simple linear function of time observed with random mean zero noise.  To fit the line we are given 5 pairs (t, f(t)) at distinct times t, where the function f(t) is &quot;truly&quot; f(t) =at+b and is observed with an additive noise component e(t).  It we fit a straight line we may get a good (but not perfect) linear fit to these five points. The reason it is not perfect is because of the noise.  So the 5 points do not all fall on a single straight line. However if we take a fourth degree polynomial of the form f(t) =a1 +a2 t +a3 t4$^2$ +a4 t$^3$ + a5 t$^4$ we can take the five values of f(t) on the left-hand side of the equation and plug in the five corresponding values of t on the righthand side we will have 5 linear equations in five unknowns that leads to a unique set of values for a1, a2, a3, a4, a5.  So by overfitting we can get a wiggly polynomial to fit the data perfectly.  But this function will not interpolate, extrapolate or predict well. Variable subset selection procedures will take out variables that may be highly correlated with other variables in the model and also remove variables that really have no relationship to the response.  So yes they can reduce or eliminated both types of overfitting problems.  when you drop variables from the model you do not keep the original parameter estimate that came about with the other variables included.  You will refit the new model that contains fewer parameters and the coefficients should change.  So there is no dilemma.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-24T15:26:45.863" Id="29098" LastActivityDate="2012-05-24T20:30:22.770" LastEditDate="2012-05-24T20:30:22.770" LastEditorUserId="11032" OwnerUserId="11032" ParentId="29086" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="29105" AnswerCount="2" Body="&lt;p&gt;From the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/hist.html&quot; rel=&quot;nofollow&quot;&gt;R docs for hist&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;R's default with equi-spaced breaks (also the default) is to plot the&#10;  counts in the cells defined by breaks. Thus the height of a rectangle&#10;  is proportional to the number of points falling into the cell, as is&#10;  the area provided the breaks are equally-spaced.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The default with non-equi-spaced breaks is to give a plot of area one,&#10;  in which the area of the rectangles is the fraction of the data points&#10;  falling in the cells.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So .. how do I get hist to plot non-equi-spaced breaks?  It sounds as if it will calculate the breaks to end up with area one, but I don't see the options.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Also, what are recommended ways (in R) to do non-equi-spaced histograms?  A typical case would be data that is spiky, causing all the action in one or a few cells, no matter how many are given as &quot;breaks&quot;. Another would be two areas of activity separated by a large area of zero, meaning no matter how many breaks, all you see is flat, with two huge narrow spikes.  Or perhaps worse, one area of activity, then another area of much less activity far away that causes the graph to be very wide and flat.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-24T16:28:40.513" FavoriteCount="1" Id="29104" LastActivityDate="2012-05-24T18:41:41.947" LastEditDate="2012-05-24T17:28:07.540" LastEditorUserId="2849" OwnerUserId="2849" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;histogram&gt;" Title="How to perform a non-equi-spaced histogram in R?" ViewCount="977" />
  <row AnswerCount="3" Body="&lt;p&gt;I'm working on highly skewed data, so I'm using the median instead of the mean to summarise the central tendency.  I'd like to have a measure of dispersion&#10;While I often see people reporting &lt;strong&gt;mean $\pm$ standard deviation&lt;/strong&gt; or &lt;strong&gt;median$\pm$quartiles&lt;/strong&gt; to summarise the central tendency, is it ok to report &lt;strong&gt;median $\pm$ &lt;a href=&quot;http://en.wikipedia.org/wiki/Median_absolute_deviation&quot;&gt;median absolute dispersion (MAD)&lt;/a&gt;&lt;/strong&gt;?  Are there potential issues with this approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would find this approach more compact and intuitive than reporting lower and upper quartiles, especially in large tables full of figures.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any hint!&#10;Mulone&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-05-24T21:12:18.660" FavoriteCount="1" Id="29116" LastActivityDate="2013-04-24T18:34:14.537" LastEditDate="2012-06-11T12:38:51.877" LastEditorUserId="4856" OwnerUserId="9344" PostTypeId="1" Score="8" Tags="&lt;mean&gt;&lt;median&gt;&lt;skewness&gt;&lt;mad&gt;" Title="Mean$\pm$SD or Median$\pm$MAD to summarise a highly skewed variable?" ViewCount="443" />
  
  <row AcceptedAnswerId="29129" AnswerCount="3" Body="&lt;p&gt;How would you explain intuitively what is a unit root, in the context of the unit root test? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking in ways of explaining much like I've founded in &lt;a href=&quot;http://stats.stackexchange.com/questions/17537/understanding-variance-intuitively&quot;&gt;this question&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The case with unit root is that I know (little, by the way) that the unit root test is used to test for stationarity in a time series, but it's just it. &lt;/p&gt;&#10;&#10;&lt;p&gt;How would you go to explain it to the layperson, or to a person who has studied a very basic probability and statistics course?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;b&gt;UPDATE&lt;/b&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I accepted whuber's answer as it is what most reflect what I asked here. But I urge everybody that came here to read Patrick's and Michael's answers also, as they are the natural &quot;next step&quot; in understanding the Unit Root. They use mathematics, but in a very intuitive way.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-24T22:07:17.407" FavoriteCount="28" Id="29121" LastActivityDate="2014-07-19T07:03:31.330" LastEditDate="2012-05-30T14:01:00.863" LastEditorUserId="11090" OwnerUserId="11090" PostTypeId="1" Score="38" Tags="&lt;intuition&gt;&lt;unit-root&gt;" Title="Intuitive explanation of unit root" ViewCount="7887" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;He  had  just come to the bridge; and not looking where he was going,&#10;  he  tripped  over  something,  and  the  fir-cone jerked out of his&#10;  paw into the river.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&quot;Bother,&quot;  said  Pooh,  as  it floated slowly under the bridge, and he went back to get another fir-cone  which  had  a rhyme&#10;  to it. But then he thought that he would just look at the river&#10;  instead, because it was a peaceful sort of day, so he lay down and&#10;  looked at it, and it slipped slowly away beneath him . . . and&#10;  suddenly, there was his fir-cone slipping away too.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&quot;That's  funny,&quot;  said Pooh. &quot;I dropped it on the other side,&quot; said Pooh, &quot;and it came out on this side! I wonder if it would do it again?&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;A.A. Milne, &lt;a href=&quot;http://www.lib.ru/MILN/pooh2.txt&quot;&gt;The House at Pooh Corner&lt;/a&gt; (Chapter VI.  In which Pooh invents a new game and eeyore joins in.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a picture of the flow along the surface of the water:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bUO74.png&quot; alt=&quot;Pooh sticks 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The arrows show the direction of flow and are connected by &lt;em&gt;streamlines.&lt;/em&gt;  A fir cone will tend to follow the streamline in which it falls.  But it doesn't always do it the same way each time, even when it's dropped in the same place in the stream: &lt;em&gt;random variations&lt;/em&gt; along its path, caused by turbulence in the water, wind, and other whims of nature kick it onto neighboring stream lines.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RloPR.png&quot; alt=&quot;Pooh sticks 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, the fir cone was dropped near the upper right corner.  It more or less followed the stream lines--which converge and flow away down and to the left--but it took little detours along the way.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;An &quot;autoregressive process&quot; (AR process) is a sequence of numbers thought to behave like certain flows.  The two-dimensional illustration corresponds to a process in which each number is determined by its &lt;em&gt;two&lt;/em&gt; preceding values--plus a random &quot;detour.&quot;  The analogy is made by interpreting each successive pair in the sequence as coordinates of a point in the stream.  Instant by instant, the stream's flow changes the fir cone's coordinates in the same mathematical way given by the AR process.&lt;/p&gt;&#10;&#10;&lt;p&gt;We can recover the original process from the flow-based picture by writing the coordinates of each point occupied by the fir cone and then erasing all but the last number in each set of coordinates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nature--and streams in particular--is richer and more varied than the flows corresponding to AR processes.  Because each number in the sequence is assumed to depend &lt;em&gt;in the same fixed way&lt;/em&gt; on its predecessors--apart from the random detour part--the flows that illustrate AR processes exhibit limited patterns.  They can indeed seem to flow like a stream, as seen here.  They can also look like the swirling around a drain.  The flows can occur in reverse, seeming to gush outwards from a drain.  And they can look like mouths of two streams crashing together: two sources of water flow directly at one another and then split away to the sides.  But that's about it.  You can't have, say, a flowing stream with eddies off to the sides.  AR processes are too simple for that.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L7hcR.png&quot; alt=&quot;Pooh sticks 3&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this flow, the fir cone was dropped at the lower right corner and quickly carried into the eddy in the upper right, despite the slight random changes in position it underwent.  But it will never quite stop moving, due to those same random movements which rescue it from oblivion.  The fir cone's coordinates move around a bit--indeed, they are seen to oscillate, on the whole, around the coordinates of the center of the eddy.  In the first stream flow, the coordinates progressed inevitably along the center of the stream, which quickly captured the cone and carried it away faster than its random detours could slow it down: they &lt;em&gt;trend in time.&lt;/em&gt;  By contrast, circling around an eddy exemplifies a &lt;em&gt;stationary&lt;/em&gt; process in which the fir cone is captured; flowing away down the stream, in which the cone flows out of sight--trending--is non-stationary.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, when the flow for an AR process moves away downstream, it also &lt;em&gt;accelerates.&lt;/em&gt;  It gets faster and faster as the cone moves along it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The nature of an AR flow is determined by a few special, &quot;characteristic,&quot; directions, which are usually evident in the stream diagram: streamlines seem to converge towards or come from these directions.  One can always find as many characteristic directions as there are coefficients in the AR process: two in these illustrations.  Associated with each characteristic direction is a number, its &quot;root&quot; or &quot;eigenvalue.&quot;  When the &lt;em&gt;size&lt;/em&gt; of the number is less than unity, the flow in that characteristic direction is &lt;em&gt;towards&lt;/em&gt; a central location.  When the size of the root is greater than unity, the flow accelerates &lt;em&gt;away&lt;/em&gt; from a central location.  Movement along a characteristic direction with a &lt;em&gt;unit&lt;/em&gt; root--one whose size is $1$--is dominated by the random forces affecting the cone.  It is a &quot;random walk.&quot;  The cone can wander away slowly but without accelerating.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Some of the figures display the values of both roots in their titles.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Even Pooh--a bear of very little brain--would recognize that the stream will capture his fir cone only when all the flow is toward one eddy or whirlpool; otherwise, on one of those random detours the cone will eventually find itself under the influence of that part of the flow with a root greater than $1$ in magnitude, whence it will wander off downstream and be lost forever.  Consequently, &lt;strong&gt;an AR process can be stationary if and only if all characteristic values are less than unity in size&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Economists are perhaps the greatest analysts of time series and employers of the AR process technology.  Their series of data typically do not accelerate out of sight.  They are concerned, therefore, only whether there is a characteristic direction whose value may be as large as $1$ in size: a &quot;unit root.&quot;  Knowing whether the data are consistent with such a flow can tell the economist much about the potential fate of his pooh stick: that is, about what will happen in the future.  That's why it can be important to test for a unit root.  A fine &lt;a href=&quot;http://en.wikipedia.org/wiki/Unit_root#Unit_root_hypothesis&quot;&gt;Wikipedia article&lt;/a&gt; explains some of the implications.&lt;/p&gt;&#10;&#10;&lt;p&gt;Pooh and his friends found an empirical test of stationarity:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Now one day Pooh and Piglet and Rabbit and Roo were all playing&#10;  Poohsticks  together.  They had dropped their sticks in when Rabbit&#10;  said &quot;Go!&quot; and then they had hurried across to  the other  side  of &#10;  the bridge, and now they were all leaning over the edge, waiting to&#10;  see whose stick would come out first.  But it was a long time coming,&#10;  because the river was very lazy that day,  and  hardly seemed to mind&#10;  if it didn't ever get there at all.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&quot;I can  see  mine!&quot;  cried  Roo.  &quot;No,  I  can't,  it's something  else.  Can  you see yours, Piglet? I thought I could see&#10;  mine, but I couldn't. There it is! No, it  isn't.  Can  you see yours,&#10;  Pooh?&quot;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&quot;No,&quot; said Pooh.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&quot;I  expect  my  stick's  stuck,&quot;  said Roo. &quot;Rabbit, my stick's stuck. Is your stick stuck, Piglet?&quot;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&quot;They always take longer than you think,&quot; said Rabbit.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This passage, from 1928, could be construed as the very first &quot;Unit Roo test.&quot;&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-05-25T04:22:38.083" Id="29129" LastActivityDate="2012-06-25T21:38:39.077" LastEditDate="2012-06-25T21:38:39.077" LastEditorUserId="919" OwnerUserId="919" ParentId="29121" PostTypeId="2" Score="57" />
  
  <row Body="A non-negative continuous probability distribution indexed by two strictly positive parameters. " CommentCount="0" CreationDate="2012-05-25T14:30:23.707" Id="29148" LastActivityDate="2012-05-25T14:31:34.493" LastEditDate="2012-05-25T14:31:34.493" LastEditorUserId="4856" OwnerUserId="4856" PostTypeId="4" Score="0" />
  
  
  <row Body="&lt;p&gt;Chi Square doesn't require equal size groups.  In R you can use either prop.test() or chisq.test().    &lt;/p&gt;&#10;&#10;&lt;p&gt;I do this often with A/B direct mail tests with unequal size groups.  For example, 100K donors are split 90% and 10%: the 90% are sent an email appeal, and 10% are sent nothing.  The binary outcome is whether they donated to the appeal.&lt;/p&gt;&#10;&#10;&lt;p&gt;The nice thing about prop.test vs chisq.test is that prop.test will both calculate the p-value of the hypothesis that the groups are equal &lt;em&gt;and&lt;/em&gt; calculate the confidence interval for the difference&lt;/p&gt;&#10;&#10;&lt;p&gt;This page gives an example of prop.test() with two groups:&#10;&lt;a href=&quot;http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_prop.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_prop.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sexsmoke&amp;lt;-matrix(c(70,120,65,140),ncol=2,byrow=T)&#10;rownames(sexsmoke)&amp;lt;-c(&quot;male&quot;,&quot;female&quot;)&#10;colnames(sexsmoke)&amp;lt;-c(&quot;smoke&quot;,&quot;nosmoke&quot;)&#10;prop.test(sexsmoke)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-05-25T17:27:07.277" Id="29163" LastActivityDate="2012-05-25T17:27:07.277" OwnerUserId="8053" ParentId="25299" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;Many of the other answers have covered the main points but you asked for a hierarchy if any exists and the way I see it, although they are each disciplines in their own right, there is hierarchy no one seems to have mentioned yet since each builds upon the previous one.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Statistics&lt;/strong&gt; is just about the numbers, and quantifying the data.  There are many tools for finding relevant properties of the data but this is pretty close to pure mathematics.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Data Mining&lt;/strong&gt; is about using &lt;strong&gt;Statistics&lt;/strong&gt; as well as other programming methods to find patterns hidden in the data so that you can &lt;em&gt;explain&lt;/em&gt; some phenomenon. Data Mining builds intuition about what is really happening in some data and is still little more towards math than programming, but uses both.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt; uses &lt;strong&gt;Data Mining&lt;/strong&gt; techniques and other learning algorithms to build models of what is happening behind some data so that it can &lt;em&gt;predict&lt;/em&gt; future outcomes. Math is the basis for many of the algorithms, but this is more towards programming.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt; uses models built by &lt;strong&gt;Machine Learning&lt;/strong&gt; and other ways to &lt;em&gt;reason&lt;/em&gt; about the world and give rise to intelligent &lt;em&gt;behavior&lt;/em&gt; whether this is playing a game or driving a robot/car. Artificial Intelligence has some goal to achieve by predicting how actions will affect the model of the world and chooses the actions that will best achieve that goal.  Very programming based.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In short&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Statistics&lt;/strong&gt; &lt;em&gt;quantifies&lt;/em&gt; numbers&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Data Mining&lt;/strong&gt; &lt;em&gt;explains&lt;/em&gt; patterns&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt; &lt;em&gt;predicts&lt;/em&gt; with models&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Artificial Intelligence&lt;/strong&gt; &lt;em&gt;behaves&lt;/em&gt; and &lt;em&gt;reasons&lt;/em&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now this being said, there will be some AI problems which fall only into AI and similarly for the other fields but most of the interesting problems today (self driving cars for example) could easily and correctly be called all of these. Hope this clears up the relationship between them you asked about. &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-05-26T06:33:22.270" CreationDate="2012-05-26T06:33:22.270" Id="29186" LastActivityDate="2012-05-26T06:39:51.327" LastEditDate="2012-05-26T06:39:51.327" LastEditorUserId="11559" OwnerUserId="11559" ParentId="5026" PostTypeId="2" Score="11" />
  <row AcceptedAnswerId="29202" AnswerCount="2" Body="&lt;p&gt;I systematically chose rare words from three groups of texts (#1, #2 and #3) (the rare words within each group are different, but some rare words may appear in more than one group).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, for each rare word, I attempted to found out how frequent it is in one corpus (a large, systematic aggregation of texts), called Corpus P, and how frequent it is in another (Corpus B). Using the frequencies, I found out the log-likelihood statistic for each ratio of frequencies. This is a common statistic in corpus linguistics, and the bigger it is, the bigger the measure of &quot;surprise&quot; in the frequency ratio.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was particularly interested in the rare words that were more significantly more frequent in Corpus P than in Corpus B. I want to see if the scores of rare words which meet this requirement significantly differ by text group (#1-3).&lt;/p&gt;&#10;&#10;&lt;p&gt;The scores for this set of rare words can be found, by text group and in descending order, in &lt;a href=&quot;http://pastebin.com/7bvJxNGC&quot; rel=&quot;nofollow&quot;&gt;this dataset&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Group #1 (n = 1362)&lt;/li&gt;&#10;&lt;li&gt;Group #2 (n = 285)&lt;/li&gt;&#10;&lt;li&gt;Group #3 (n = 112)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I looked at Q-Q plots at each of the series and they certainly are not normally distributed. Then, based on a literature review, I reached the hypothesis that these statistics have a chi-square distribution, I believe with one degree of freedom. &lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to examine this assumption, and then find out if these are three significantly different series, i.e. if the rare words from each group have significantly different scores. (I tried to see if they were different &lt;strong&gt;without&lt;/strong&gt; this assumption by applying a non-parametric Wilcoxon rank sum test, but the result was not significant.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How do I see if each column in the data has a distribution similar to chi square, 1 d.f.? If I see that they are each a good fit to the model, how do I find out if the series are different from eachother?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-26T08:17:07.493" FavoriteCount="0" Id="29187" LastActivityDate="2012-05-27T12:59:43.663" LastEditDate="2012-05-27T12:59:43.663" LastEditorUserId="10849" OwnerUserId="11555" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;distributions&gt;&lt;nonparametric&gt;" Title="Assuming my three data series are each $\chi^2(1)$ distributed, are they different from eachother?" ViewCount="225" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to carry out Graph Clustering in a huge &lt;strong&gt;undirected graph&lt;/strong&gt; with millions of edges and nodes. Graph is almost clustered with different clusters joined together only by some nodes (kind of ambiguous nodes which can relate to multiple clusters). There will be very few or almost no edges between two clusters. This problem is almost similar to finding &lt;strong&gt;vertex cut set&lt;/strong&gt; of a graph, with one exception that graph needs to be partitioned into many components(their number being unknown). Please, refer to this picture:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZThlX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Its almost like different strongly connected components sharing a couple of nodes between them and i am supposed to &lt;strong&gt;remove those nodes&lt;/strong&gt; to separate those strongly connected components. Edges are weigthed but this problem is more like finding structures in a graph, so edge weigths won't be of relevance. (Another way to think about the problem would be to visualize Solid Spheres touching each other at some points with Spheres being those strongly connected components and touching points being those ambiguous nodes.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am prototyping something, so am quiet short of time to pick up Graph Clustering Algorithms by myself and to select the best possible. Plus I need a solution that would cut nodes and not edges since different clusters share nodes and not edges in my case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any research paper, blog that addresses this or somewhat related problem? Or can anyone come up with a solution to this problem howsoever dirty.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since millions of nodes and edges are involved, I would need a MapReduce implementation of the solution. Any inputs, links for that too?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any current open source implementation in MapReduce that could be directly used?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this problem is analogous to &lt;strong&gt;Finding Communities in online social networks by removing vertices.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-26T09:01:11.383" Id="29188" LastActivityDate="2012-09-07T07:25:24.820" LastEditDate="2012-07-28T20:16:56.460" LastEditorUserId="930" OwnerUserId="6752" PostTypeId="1" Score="3" Tags="&lt;clustering&gt;&lt;classification&gt;&lt;online&gt;&lt;graph-theory&gt;" Title="Finding communities in online social networks by removing nodes" ViewCount="239" />
  <row Body="&lt;p&gt;As Rob said if it is a sequence collected at time intervals it is by one definition a time series. It is another question as whether or not there is an temporal structure within the sequence. One could consider a sequence of numbers that are independently distributed as being generated from a mean model (0,0,0)(0,0,0) with perhaps some pulses or unusual values. In this light a sequence of numbers that is measured with a fixed frequency e.g. 1 every minute/hour/day/month  could be referred to as a cross-sectional series if there is no evidented structure between successive values such as an ARIMA structure or a deterministic based structure such as level shifts, seasonal-pulses and/or local time trends even though it was collected at specific intervals of time. All of this having been said , if a series can be classified as having no temporal dependency then one might refer to it as being equivalent to a non-time series and treat it accordingly.&#10;I suggest that the OP program or acquire such functionality software that delivers parsimonious automatic ARIMA and the detection of seasonal pulses, level shifts, local time trends and various combinations of these two structures. If he finds that there is no proof of any of these structures being evident then he might classify it as a &quot;non-time series&quot;  or as Michael opined &quot;white noise&quot; or informationless using time structures.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-26T11:50:00.103" Id="29193" LastActivityDate="2012-05-26T13:54:38.153" LastEditDate="2012-05-26T13:54:38.153" LastEditorUserId="3382" OwnerUserId="3382" ParentId="29189" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="29233" AnswerCount="3" Body="&lt;p&gt;In a recent exam, we were asked to justify the use of the $\chi^2(1)$ distribution in performing the Wald or Rao's score test. There was only 1 mark for this (approx. 2.5 mins worth of time). My answer was&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The Wald and score test statistics are based on various approximations to the &#10;  log-likelihood ratio, which are valid and equivalent in large samples when $H_0$ is true. For example, the approximation for the 2nd Wald statistic is&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$ 2\log(LR)\simeq (\hat{\theta}_n-\theta_0)^2E\{-\ell''(\theta)\}|_{\theta_0} =(\hat{\theta}_n-\theta_0)^2I(\theta_0) $$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;where $I(\theta_0)$ is the Fisher information at $\theta_0$. Then, using asymptotic normality, &lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$\hat{\theta}_n\approx N \left ( \theta_0,\frac{1}{ni(\theta_0)}\right )=N \left ( \theta_0,\frac{1}{I(\theta_0)}\right )$$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;which yields&#10;  $(\hat{\theta}_n-\theta_0)\sqrt{I(\theta_0)} \approx N(0,1)$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Since $2\log(LR)$ is approximately equal to the square of the LHS of this&#10;  when $n$ is large, it is approximately distributed as the square of a standard&#10;  normal random variable, that is, as $\chi^2(1)$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The marker wrote &quot;insufficient&quot; and I got zero for this. As it is a summative exam, they won't give any feedback, nor engage in any discussion about it. I was wondering if anyone here can explain what I have missed or where I went wrong. I'm not very good with latex so I hope I didn't make any mistakes in the typing !&lt;/p&gt;&#10;&#10;&lt;p&gt;This is for an elective module in statistical theory in the final year of an undergraduate maths degree. Thanks !&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: There is a formal procedure to have my script remarked, but for the sake of 1 mark, and since I passed quite comfortably, I don't really want to rock the boat.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-26T14:13:20.847" Id="29201" LastActivityDate="2012-05-28T00:24:15.607" LastEditDate="2012-05-26T16:55:15.003" LastEditorUserId="4856" OwnerUserId="6884" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;mathematical-statistics&gt;&lt;asymptotics&gt;" Title="Justification for use of $\chi^2(1)$ in Wald and score test" ViewCount="312" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;This may for sure be a stupid question but suppose a model &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10; y = \alpha  \times r^\gamma \times \varepsilon&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;the parameters $\alpha$ and $\gamma$ are of interst for me (and $\varepsilon$ is the standard error term). so my first intention was do take the $log()$ of the function. however, i know that the &lt;strong&gt;$y$ may be negative&lt;/strong&gt;. any hints for this situation?&lt;/p&gt;&#10;" ClosedDate="2013-01-07T14:29:48.270" CommentCount="10" CreationDate="2012-05-26T22:09:28.233" FavoriteCount="1" Id="29222" LastActivityDate="2013-01-07T14:44:05.987" LastEditDate="2012-05-27T08:49:48.113" LastEditorUserId="8041" OwnerUserId="8041" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;data-transformation&gt;" Title="Log transformation with negative values" ViewCount="2493" />
  
  <row Body="&lt;p&gt;In addition to King's answer that the research organization should retain the IDs but keep them separate from the real identities, there are other measures you should put in place to protect the anonymity of the findings in a situation like this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the client should never have access to the raw data, so even if they could connect the ID to the real identity, they couldn't connect it to the survey findings&lt;/li&gt;&#10;&lt;li&gt;you need to have clear policies when you can report what level of detail about the data. For example, quantitative data shall not be reported on groups of less than 5; open-ended response comments will not be reported on groups of less than 25, etc.&lt;/li&gt;&#10;&lt;li&gt;identifying information disclosed in reports should be kept to a minimum. For example, things like work department, job title groups, etc. should only be included where it is relevant for the purpose at hand.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-05-26T22:15:51.657" Id="29223" LastActivityDate="2012-05-26T22:15:51.657" OwnerUserId="3331" ParentId="29139" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Generally a variance for a mean of $n$ iid $X_i$s is ${\rm var}(X_i)/n$.  Now since the $X_i$s are Bernoulli with success probability $p$, ${\rm var}(X_i)$ has a special form.  It is $p(1-p)$.  So each of the $23$ means have variance of the special form $p(1-p)/n$  and the standard error is the square root of that.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-26T23:19:40.523" Id="29227" LastActivityDate="2012-05-27T03:10:04.977" LastEditDate="2012-05-27T03:10:04.977" LastEditorUserId="4856" OwnerUserId="11032" ParentId="29220" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You can try the &lt;code&gt;survival&lt;/code&gt; package in R-core.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(survival)&#10;fit &amp;lt;- coxph(V1+V2, data=data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Not sure how it compares to &lt;code&gt;rms&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-27T12:26:28.557" Id="29245" LastActivityDate="2012-05-27T12:26:28.557" OwnerUserId="10849" ParentId="29131" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you have no basis for a particular correlation structure and the unstructured correlation option requires too many parameters, I would try two different ones to see if the results are sensitive to the choice .  In my experience they usually aren't sensitive. I would probably compare AR(1) to compound symmetry.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-27T14:14:34.463" Id="29257" LastActivityDate="2012-05-27T23:59:39.487" LastEditDate="2012-05-27T23:59:39.487" LastEditorUserId="11032" OwnerUserId="11032" ParentId="29253" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Can I assume this is not homework? So what you have to calculate are the probabilities for 0, 1, and 2 responses.  Let's take 0 as the simplest case  It will only happen if all five post=paid letters and all five non post-paid letters are answered.  By independence it is (0.90)$^5$ (0.60)$^5$.  Now add to that the probability that only 1 is returned.  There are two ways this can happen,  It can be a post-paid returned or a non-postpaid.  The disjoint events can have their probabilities summed.  For the post-paid case this is&#10;(0.60)$^4$ (0.40)$^1$ (0.90)$^5$ But there are 5 ways that 1 post-paid letter can be answered and only 1 way that all five non-post paid letters will not be answered.  So this term is 5 (0.60)$^4$ (0.40)$^1$ (0.90)$^5$ and similarly for one non post paid 5 (0.90)$^4$ (0.10) (0.60)$^5$.  Last of all you need to add all the cases where 2  letters are answered.  This can happen by having 2 non-post-paid letters returned or 1 non-post-paid and 1 post-paid or 2 post-paid.  You and the results for these possibilities to the others to get the final answer.  The calculations are done in the same way with the number of combinations to get 2 out of 10 letters selected.  When they are both from the post-paid group the factor is number of combinations for choosing 2 out of 5 which is 10.  The same factor when both are from the non-post-paid group.  When it is one from each there are 5 ways for post-paid to match with any one of the 5 non-post-paid.  So that factor is 5x5 =25.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-27T19:19:40.923" Id="29265" LastActivityDate="2012-05-27T19:19:40.923" OwnerUserId="11032" ParentId="29262" PostTypeId="2" Score="0" />
  <row AnswerCount="4" Body="&lt;p&gt;A researcher knows that the probability that a questionnaire will be reponded by mail is 40%.&#10;He wants to be 99% sure he will get at least 200 responded questionnaires back.&#10;How many questionnaires must he send by mail to be 99% sure he will get at least 200 answers?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-27T20:02:18.700" FavoriteCount="2" Id="29268" LastActivityDate="2012-06-14T12:09:28.903" LastEditDate="2012-05-30T21:52:50.703" LastEditorUserId="4856" OwnerUserId="11577" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;binomial&gt;" Title="Each letter has a $40\%$ chance of being replied to; how many letters to send to be $99\%$ sure that you get 200 replies?" ViewCount="330" />
  <row AcceptedAnswerId="35453" AnswerCount="2" Body="&lt;p&gt;I am looking for guidelines on how to interpret residual plots of glm models.  Especially poisson, negative binomial, binomial models.  What can we expect from these plots when the models are &quot;correct&quot;? (for example, we expect the variance to grow as the predicted value increases, for when dealing with a Poisson model)&lt;/p&gt;&#10;&#10;&lt;p&gt;I know the answers depend on the models.  Any references (or general points to consider) will be helpful/appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-27T21:25:17.983" FavoriteCount="7" Id="29271" LastActivityDate="2012-08-31T18:15:04.293" LastEditDate="2012-05-27T21:43:19.600" LastEditorUserId="253" OwnerUserId="253" PostTypeId="1" Score="15" Tags="&lt;generalized-linear-model&gt;&lt;residuals&gt;&lt;diagnostic&gt;&lt;residual-analysis&gt;" Title="Interpreting residual diagnostic plots for glm models?" ViewCount="8174" />
  
  <row Body="&lt;p&gt;I am working my way through &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;Elements of Statistical Learning&lt;/a&gt;. This book covers an incredible range of techniques (so is 700+ pages) but each approach is explained clearly in a very practical, rather than highly theoretical way. It doesn't explicitly contain anything about R, however the plots and graphs are all clearly made with R and there are packages on CRAN for all the topics discussed. The authors have all been involved with the development of R (as well as a fair chunk of modern machine learning techniques).&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-05-27T22:18:30.663" CreationDate="2012-05-27T22:18:30.663" Id="29274" LastActivityDate="2012-05-27T22:18:30.663" OwnerUserId="10354" ParentId="27553" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="29288" AnswerCount="2" Body="&lt;p&gt;I'm reading &lt;a href=&quot;http://www.dspguide.com&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; book. &lt;a href=&quot;http://www.dspguide.com/ch2/2.htm&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt;, he gives me a method for calculating the mean:&#10;&lt;img src=&quot;http://i.stack.imgur.com/ipzQY.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.dspguide.com/ch2/4.htm&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; he introduces a new method of calculating the mean through histogram:&#10;&lt;img src=&quot;http://i.stack.imgur.com/vSuRl.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;He says that now we will count and index  the samples in the histogram, this list:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;{7, 8, 0, 9, 2, 10, 4, 1, 6, 0, 8, 7, 6, 0, 0}&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is going to be counted and it will result in:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;{{7, 2}, {8, 2}, {0, 4}, {9, 1}, {2, 1}, {10, 1}, {4, 1}, {1, 1}, {6, &#10;    2}}&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It means there are two 7's in the list, there are two 8's in the list and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then i've calculated the mean of this list:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;{4, 1, 1, 0, 1, 0, 2, 2, 2, 1, 1}&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Which is almost as the ordered version of the last list, four 0's, one 1 and so on&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that the mean of the first one is different of the second, while the mean of the first list is 4.53333, the second list is 1.36364, do these means have to be identical?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-28T07:48:11.963" Id="29286" LastActivityDate="2012-05-28T11:41:18.333" OwnerUserId="10385" PostTypeId="1" Score="2" Tags="&lt;mean&gt;&lt;histogram&gt;" Title="Does the samples mean and the histogram mean have to be the same?" ViewCount="444" />
  <row Body="&lt;p&gt;If it is one-step forecast then both forecasts are the same. The difference arises when forecasting further: &quot;dynamic forecast&quot; will take previously forecasted values while &quot;&quot;static forecast&quot; will take actual values to make next step forecast.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-28T08:09:48.643" Id="29287" LastActivityDate="2012-05-28T08:09:48.643" OwnerUserId="10908" ParentId="29155" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Check out the &lt;strong&gt;wikipedia&lt;/strong&gt; article &lt;a href=&quot;http://en.wikipedia.org/wiki/Skewness&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Skewness&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I see that the mean and standard deviation are provided, not clear what is the statistic below that. If you are able to infer the median from this histogram, you could also calculate the &lt;strong&gt;Karl Pearson&lt;/strong&gt; coefficient of skewness. This is defined as &lt;strong&gt;3(mean - median) /Standard Deviation&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;If this statistic is greater than zero, the distribution is positively skewed, if negative then distribution is negatively skewed (this test might not work for bi-modal distributions though)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-28T10:38:26.633" Id="29291" LastActivityDate="2012-05-28T10:43:39.917" LastEditDate="2012-05-28T10:43:39.917" LastEditorUserId="11588" OwnerUserId="11588" ParentId="26095" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You could try out read.table or read.csv &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html&quot; rel=&quot;nofollow&quot;&gt;http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html&lt;/a&gt; is the link to the R manual. &lt;/p&gt;&#10;&#10;&lt;p&gt;The important specification parameters include whether the file contains headers (head = T in case file contains headers), the separator specification - sep = &quot;,&quot; for a csv file would work well. The specification on how to handle missing strings is by the na.strings parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;say the variable filename contains the full path to the csv file then&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Eg: read.table(filename, head = T, sep = &quot;,&quot;, na.strings = &quot;&quot;)&lt;/strong&gt; would readin a csv file with headers and replace the missing fields with blanks &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-28T14:14:53.867" Id="29306" LastActivityDate="2012-05-28T14:14:53.867" OwnerUserId="11588" ParentId="29303" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;try to find the confounding variable.. see if you get any interactions or mabe run another test with different methodology (and more variables that may give you a clearer picture.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-28T17:19:57.377" Id="29321" LastActivityDate="2012-05-28T17:19:57.377" OwnerUserId="11601" ParentId="29289" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Linear regression uses the general linear equation $Y=b_0+∑(b_i X_i)+\epsilon$ where $Y$ is a continuous dependent variable and independent variables $X_i$ are &lt;em&gt;usually&lt;/em&gt; continuous (but can also be binary, e.g. when the linear model is used in a t-test) or other discrete domains. $\epsilon$ is a term for the variance that is not explained by the model and is usually just called &quot;error&quot;. Individual dependent values denoted by $Y_j$ can be solved by modifying the equation a little: $Y_j=b_0 + \sum{(b_i X_{ij})+\epsilon_j}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Logistic regression is another generalized linear model (GLM) procedure using the same basic formula, but instead of the continuous $Y$, it is regressing for the probability of a categorical outcome. In simplest form, this means that we're considering just one outcome variable and two states of that variable- either 0 or 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;The equation for the probability of $Y=1$ looks like this:&#10;$$&#10;P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Your independent variables $X_i$ can be continuous or binary. The regression coefficients $b_i$ can be exponentiated to give you the change in odds of $Y$ per change in $X_i$, i.e., $Odds={P(Y=1) \over P(Y=0)}={P(Y=1) \over 1-P(Y=1)}$ and ${\Delta Odds}=  e^{b_i}$.  $\Delta Odds$ is called the odds ratio, $Odds(X_i+1)\over Odds(X_i)$. In English, you can say that the odds of $Y=1$ increase by a factor of $e^{b_i}$ per unit change in $X_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: If you wanted to see how body mass index predicts blood cholesterol (a continuous measure), you'd use linear regression as described at the top of my answer. If you wanted to see how BMI predicts the odds of being a diabetic (a binary diagnosis), you'd use logistic regression.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-05-28T18:45:05.103" Id="29326" LastActivityDate="2012-05-29T00:53:19.070" LastEditDate="2012-05-29T00:53:19.070" LastEditorUserId="8807" OwnerUserId="8807" ParentId="29325" PostTypeId="2" Score="32" />
  <row AnswerCount="2" Body="&lt;p&gt;Five months ago, jbowman posted a very useful answer to estimate the break point in a broken stick model with random effects in R. I never use &quot;computing&quot; like &lt;code&gt;ifelse&lt;/code&gt; and I would like to estimate two break points. I should write two other functions like &lt;code&gt;b1&lt;/code&gt; and &lt;code&gt;b2&lt;/code&gt; but I don't know how. Can someone please tell me how to do that in R? Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;jbowman's code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;str(sleepstudy)&#10;&#10;#Basis functions&#10;bp = 4&#10;b1 &amp;lt;- function(x, bp) ifelse(x &amp;lt; bp, bp - x, 0)&#10;b2 &amp;lt;- function(x, bp) ifelse(x &amp;lt; bp, 0, x - bp)&#10;&#10;#Wrapper for Mixed effects model with variable break point&#10;foo &amp;lt;- function(bp)&#10;{&#10;  mod &amp;lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy)&#10;  deviance(mod)&#10;}&#10;&#10;search.range &amp;lt;- c(min(sleepstudy$Days)+0.5,max(sleepstudy$Days)-0.5)&#10;foo.opt &amp;lt;- optimize(foo, interval = search.range)&#10;bp &amp;lt;- foo.opt$minimum&#10;bp&#10;[1] 6.071932&#10;mod &amp;lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-05-28T19:07:41.427" FavoriteCount="1" Id="29327" LastActivityDate="2013-04-26T15:58:30.950" LastEditDate="2012-09-28T08:42:37.657" LastEditorUserId="2970" OwnerUserId="11604" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;piecewise-linear&gt;" Title="Estimating two break points in a broken stick model with random effects in R" ViewCount="446" />
  
  
  
  <row Body="&lt;p&gt;This sounds like a trivial classification problem. The variables A1,A2,... will be your data and the true categories are dependent to these data. You can start with trying off-the-shelf  classifiers, e.g. &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot; rel=&quot;nofollow&quot;&gt;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&lt;/a&gt; or &lt;a href=&quot;http://www.mathworks.com/help/toolbox/stats/classify.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/help/toolbox/stats/classify.html&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-05-28T23:58:05.820" Id="29344" LastActivityDate="2012-05-28T23:58:05.820" OwnerUserId="8320" ParentId="29343" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;We have 25m XY pairs to be correlated within R.&#10;Data is bank financial data and smooth over Time ; observations for x and y are 32 quarters each.&#10;Testing 25m rships exhaustively will take forever ; this task is easily over-engineered.&#10;We’ll use the best of XY relationships to predict. (fn1)&lt;/p&gt;&#10;&#10;&lt;p&gt;We’re swamped by choice in R packages (forecast, fastVAR, tseries, nlts etc.).  All seem hard to compare.&#10;Can someone help us with which package(s) is/are most apt ?&#10;That is, which can time-efficiently test for correlation given properties of the data ?&#10;We’ll save lots of time with fine advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;(And yes, to conserve time, we’ll employ foreach &amp;amp; doSNOW.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Steve&lt;/p&gt;&#10;&#10;&lt;p&gt;fn1 : X and Y are series both 50 quarters in length.&#10;Y is offset 8 quarters forward, such that X (periods 9 to 50) is compared to Y (1 to 42).&#10;Best fit (x actual vs x fitted using y actual) is determined.&#10;Method identified is used with Y (43 to 50) to predict X for the final 8 quarters.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-05-29T01:05:53.627" Id="29349" LastActivityDate="2012-05-29T01:05:53.627" OwnerUserId="11613" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;predictive-models&gt;" Title="XY correlation, time-efficiently" ViewCount="65" />
  <row AnswerCount="3" Body="&lt;p&gt;If I have done a (binomial) logistic regression. Is it then possible from the coefficients to calculate a percentage of how much each of the variables affects the dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say that we have for example var1: 0.635, var2: 0.245, var3: 1.243. If we know that the depends variable Y in the data is 1 at 0.64 of the time. Can we then use this to calculated something a long the lines of:&lt;/p&gt;&#10;&#10;&lt;p&gt;Variable 1 has a 25% effect on Y, Variable 2 15%, and Variable 3 60%?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-29T06:27:35.933" Id="29363" LastActivityDate="2012-06-28T14:15:11.553" LastEditDate="2012-05-29T07:20:15.433" LastEditorUserId="11294" OwnerUserId="11294" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Is it possible to extrapolate a percentage from given logistic regression coefficients?" ViewCount="464" />
  <row AcceptedAnswerId="29607" AnswerCount="2" Body="&lt;p&gt;I'd like to see an extension of &lt;a href=&quot;http://stats.stackexchange.com/questions/14226/given-the-power-of-computers-these-days-is-there-ever-a-reason-to-do-a-chi-squa&quot;&gt;this discussion&lt;/a&gt; of the age-old chi-sq vs. Fisher's exact test debate, broadening the scope a bit. There are many many tests for interactions in a contingency table, enough to make my head spin. I'm hoping to get an explanation of what test I should use and when, and of course an explanation as to why one test should be preferred over another.&lt;/p&gt;&#10;&#10;&lt;p&gt;My current problem is the classic $n \times m$ case, but answers regarding higher dimensionality are welcome, as are tips for implementing the various solutions in R, at least, in cases where it is non-obvious how to proceed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below I've listed all the tests I'm aware of; I hope by exposing my errors they can be corrected.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$\chi^2$. The old standby. There are three major options here:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The correction built into R for 2x2 tables: &quot;one half is subtracted from all $|O-E|$ differences.&quot; Should I always be doing this?&lt;/li&gt;&#10;&lt;li&gt;&quot;$N-1$&quot; $\chi^2$ Test, not sure how to do this in R.&lt;/li&gt;&#10;&lt;li&gt;Monte Carlo simulation. Is this always best? Why does R not give me df when I do this?&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot;&gt;Fisher's exact test&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Traditionally advised when any cell is expected to be &amp;lt;4, but apparently some dispute this advice.&lt;/li&gt;&#10;&lt;li&gt;Is the (usually false) assumption that the marginals are fixed really the biggest problem with this test?&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Barnard%27s_exact_test&quot;&gt;Barnard's exact test&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Another exact test, except I've never heard of it.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Poisson regression&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;One thing that always confuses me about glms is exactly how to do this significance tests so help on that would be appreciated. Is it best to do nested model comparison? What about a Wald test for a particular predictor?&lt;/li&gt;&#10;&lt;li&gt;Should I really just always be doing Poisson regression? What's the practical difference between this and a $\chi^2$ test?&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-05-29T08:09:51.123" FavoriteCount="5" Id="29367" LastActivityDate="2012-06-04T18:53:21.770" LastEditDate="2012-05-31T17:06:33.923" LastEditorUserId="5186" OwnerUserId="5186" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;contingency-tables&gt;" Title="Contingency tables: what tests to do and when?" ViewCount="1625" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have been reading the book, &lt;em&gt;Good to Great&lt;/em&gt; by Jim Collins.  It says that you should focus on information that &quot;cannot be ignored,&quot; and that you should focus on the right &quot;denominator.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;I thought it would be good to measure &quot;Average Growth Rate per Producing Entity.&quot; If we are providing value and support, we would expect to be helping them &quot;produce more.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;I have production data in an excel spreadsheet that I can summarize. &lt;/p&gt;&#10;&#10;&lt;p&gt;I primarily use &quot;pivot tables&quot; for my data analysis.  I know that you can do &quot;percentage change over previous time period&quot; but I end up with distortions for the first year, when people don't have a full period to produce.  I also am not sure how to summarize it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a better program that I should use for this? Is there a good way to do this in excel? &lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking you could take the geometric mean (what growth rate would you need on the initial production number in order to end up with the final number -- or so I believe it is explained) and besides that I'm not sure where to start.  I would love to use pivot tables or something similar where I can just hit refresh and get the updated version of the numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas out there? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-29T15:48:57.387" Id="29398" LastActivityDate="2013-06-30T00:25:35.543" LastEditDate="2013-06-30T00:25:35.543" LastEditorUserId="805" OwnerUserId="11627" PostTypeId="1" Score="1" Tags="&lt;excel&gt;&lt;summary-statistics&gt;&lt;business-intelligence&gt;&lt;pivot-table&gt;" Title="How can you summarize growth in the &quot;production&quot; rate per person?" ViewCount="168" />
  <row AcceptedAnswerId="29412" AnswerCount="2" Body="&lt;p&gt;I have two random variables: $x = N(0, \sigma^2)$ and $y =U[0, b]$.  I need to compute $E(x/(1+y))$.  How does one go about doing this?  They are independent so the joint pdf is just the product of the two pdfs but can the integral be computed in closed form or is this something that should just be done numerically?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-29T17:39:39.817" Id="29408" LastActivityDate="2012-06-08T14:11:53.967" LastEditDate="2012-06-08T14:11:53.967" LastEditorUserId="4856" OwnerUserId="11408" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;expected-value&gt;&lt;uniform&gt;" Title="What is the expectation of a normal random variable divided by uniform random variable?" ViewCount="1138" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am comparing three different measures comprised of 10-20 items each. Each of these measures is actually a short form of a longer 40-item measure and each of the short forms shares some similar items, and thus they are quite similar. I am interested in seeing how each of these short forms are correlated with the same dependent variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;I come up with &lt;em&gt;r&lt;/em&gt;= .54, .56, and .58. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I assess whether these correlations are significantly different? It seems doing a simple &lt;em&gt;r&lt;/em&gt;-to-&lt;em&gt;z&lt;/em&gt; transformation would not be appropriate since the independent variables are quite similar and the dependent variable is in fact the same for all three correlations. As such, all of these *r*s overlap quite a bit.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found this one paper:&lt;/p&gt;&#10;&#10;&lt;p&gt;Zou, G. (2007). &lt;a href=&quot;http://dx.doi.org/10.1037/1082-989X.12.4.399&quot; rel=&quot;nofollow&quot;&gt;Toward using confidence intervals to compare correlations.&lt;/a&gt; &lt;em&gt;Psychological Methods&lt;/em&gt;, 12(4), 399.&lt;/p&gt;&#10;&#10;&lt;p&gt;But it seems way over my head and I am wondering whether there is an easier way to do it...&lt;/p&gt;&#10;&#10;&lt;p&gt;Any guidance would be appreciated!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-29T19:10:12.573" FavoriteCount="1" Id="29422" LastActivityDate="2013-01-26T01:14:37.363" LastEditDate="2013-01-26T01:14:37.363" LastEditorUserId="2669" OwnerUserId="3262" PostTypeId="1" Score="4" Tags="&lt;correlation&gt;&lt;statistical-significance&gt;" Title="Is there an easy way to calculate significant difference between two largely overlapping correlations from same sample?" ViewCount="570" />
  
  <row Body="&lt;p&gt;LDA finds at most $k - 1$ linear discriminants, where $k$ is the number of classes. In your data you state you have three classes hence only 2 linear discriminants can be resolved.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-29T19:20:37.227" Id="29423" LastActivityDate="2012-05-29T19:20:37.227" OwnerUserId="1390" ParentId="29399" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If the test set has a lot of these points with new factor values then I'm not sure what the best approach is. If it is just a handful of points you might be able to get away with something fudgy like treating the errant factor levels as missing data and imputing them with whatever approach you see fit. The R implementation has a couple of ways to impute missing data, you just need to set these factor levels to NA to indicate they are missing.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-30T00:08:49.817" Id="29448" LastActivityDate="2012-05-30T00:08:49.817" OwnerUserId="10354" ParentId="29446" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Kaelin Colclasure!&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, you are right, PCA works in such a way, that if you have more samples of one person, it would more likely produce eigenfaces that would look like that person. However, if you use the same number of samples for every person, you would get eigenfaces for all of them with no prior. And in this case giving more samples would give you much stable eigenfaces and even get components to describe some specific features of the images.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I would advise to take as much samples of one person as you can, but balance the number of samples of every kind. And that applies not only for images of one person, but for man and woman, for example (if you want to deal with images of both men and women, of course).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-30T10:05:24.090" Id="29467" LastActivityDate="2012-05-30T10:05:24.090" OwnerUserId="11639" ParentId="29174" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;If the explanatory variables have values over the entire real line it makes little sense to express an expectation that is a proportion in $[0,1]$ as a linear function of variable defined over the entire real line.  If the sigmoid shape of the logit transformation doesn't describe the shape then perhaps it is best to search for a different transformation that maps $[0,1]$ into $(-∞ , ∞)$.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-05-30T12:12:15.750" Id="29472" LastActivityDate="2012-05-30T12:14:44.913" LastEditDate="2012-05-30T12:14:44.913" LastEditorUserId="4856" OwnerUserId="11032" ParentId="29469" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;assuming your data are in a &lt;code&gt;data.frame&lt;/code&gt; called dt, you can turn it into a table object using &lt;code&gt;xtabs&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tbl &amp;lt;- xtabs(Freq ~ Species + Eco_region4, data=dt)&#10;tbl&#10;          Eco_region4&#10;   Species A1 A2 A3 B1 B2 B3 C1 C2 C3&#10;        S1 10 10  2 13  9 15 12 12 18&#10;        S2 12  6  9 15  8 12 18  0 10&#10;        S3  8 11 13  7 13 13 20 11 16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can get a three dimensional table by adding Eco_region3 to the end of the formula, but then the correspondence analysis would fail because of the nested structure of your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can perform correspondence analysis with the &lt;code&gt;ca&lt;/code&gt; function from the &lt;code&gt;ca&lt;/code&gt; package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-05-30T16:10:30.843" Id="29496" LastActivityDate="2012-05-30T16:10:30.843" OwnerUserId="697" ParentId="10886" PostTypeId="2" Score="3" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;If I take the sum of a sequence of independent R.V.'s, do they always converge to a stable distribution? (I've heard about generalizations of the CLT, but I'm looking for more precision).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I think we need more restrictions on this statement to say anything useful. You could have a sequence of independent random uniform RVs on the interval [0, i]. The convolution of any two RVs from this sequence certainly does not follow the same distribution &lt;em&gt;and&lt;/em&gt; the asymptotic distribution of the sample mean converges to a non-stable distribution.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-05-30T18:22:21.460" Id="29504" LastActivityDate="2012-05-30T18:22:21.460" OwnerUserId="8013" ParentId="29497" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;I might just make many draws from the distribution and calculate the rate that the event you are interested in occurs. In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N=10^7&#10; x=rnorm(N,mu_x,sig_x)&#10; y=rnorm(N,mu_y,sig_y)&#10; z=rnorm(N,mu_z,sig_z)&#10; sum(x&amp;lt;y &amp;amp; y &amp;gt;z )/N&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It is just an estimation so maybe do it a couple times. Quick and dirty&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-05-31T03:38:48.920" Id="29534" LastActivityDate="2012-05-31T17:11:27.107" LastEditDate="2012-05-31T17:11:27.107" LastEditorUserId="10630" OwnerUserId="10630" ParentId="29051" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;Imagine some paired data ${(x_i,y_i)}_{i=1}^n$ representing the results of two different measurement methods and the question is about the quantification of the bias between the two methods. Let us assume that the normality assumption is reasonable for $x_i-y_i$ hence we simply compute an estimate and a confidence interval of the mean difference. &lt;/p&gt;&#10;&#10;&lt;p&gt;Of course the width of the confidence interval is an indicator of how precise is the quantification of the bias, but what other tool(s) can we use to assess if there enough data for the quantification to be reliable ? I have in mind a &quot;cross-validation&quot; (I quote because I am ignorant on this topic): for example we could assess whether the result changes when we drop a data value. Is there a standard way to perform such an assessment ? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am also interested in performing this assessment in the R software.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-05-31T08:49:49.103" Id="29544" LastActivityDate="2012-05-31T13:49:01.890" OwnerUserId="8402" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;cross-validation&gt;&lt;methodology&gt;" Title="What tool can we use to assess if there are enough data?" ViewCount="129" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have pairs of timeseries that are estimating the same quantity over the years. It is some survival data: number of &quot;dead&quot; subjects during the year over the number of total subjects at the beginning of the year. The underlying data is regularly revised, so I need to assess the effect of the changes. Below an example is given.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SmpCV.png&quot; alt=&quot;the two series&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to specifically test if the mean (which is the parameter of interest) is the same for the timeseries pairs (the analysis will be repeated for a lot of pairs, please do not focus too much on the particular image although it is fairly representative).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have several questions: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Should I use a paired test or not?&lt;/li&gt;&#10;&lt;li&gt;If it is paired, how do I deal with the ties, i.e. zero differences (I will always have a lot of ties)?&lt;/li&gt;&#10;&lt;li&gt;I believe that &lt;code&gt;bootstrapping&lt;/code&gt; can be usefull in this case. I have bootstrapped the sample mean difference. This gives a distribution centered around the observed mean difference. How can I compute a p-value out of this distribution? Would it be reasonable to bootstrap the t-test instead?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-05-31T12:15:33.650" Id="29557" LastActivityDate="2012-06-14T22:14:59.910" OwnerUserId="1443" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;bootstrap&gt;" Title="Hypothesis testing: small timeseries changes" ViewCount="188" />
  
  
  <row Body="&lt;p&gt;I think that one very important application of data mining is genetic exploration.  With microarrays there are huge challenges regarding design of the experiment, the data collection and transformations of the data (general subtracting baseline noise and scaling).  But in the area of inference methods the biggest problem is the multiplicity issue with hypothesis tests.  Simultaneously testing thousands of hypotheses leads to a lot of false alarms.  Adjust by controlling the false discovery rate is one step in the right direction.  Terry Speed, Brad Efron and others have been wotking hard on the microarray problem.  Efron even has a recent monograph out titled &quot;Large Scale Inference&quot;  and an annual conference on this topic was started last year.  An empirical Bayes method is an approach that Efron recommends and illustrates in his monograph. &lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2012-05-31T14:35:14.517" CreationDate="2012-05-31T14:35:14.517" Id="29570" LastActivityDate="2012-05-31T14:35:14.517" OwnerUserId="11032" ParentId="29565" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;If you wanna get the best antibiotic concentration for bacterial cell death, then you need to fit a non linear model for dose-response relationship, for predict the &lt;a href=&quot;http://en.wikipedia.org/wiki/LD50&quot; rel=&quot;nofollow&quot;&gt;median lethal dose 50&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check this tutorial to do this kind of analysis&#10;&lt;a href=&quot;http://www.graphpad.com/prism/tutorials/dose_response/dose_response_curves.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.graphpad.com/prism/tutorials/dose_response/dose_response_curves.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also check this &lt;a href=&quot;http://dl.dropbox.com/u/4828275/dose-response.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; for concepts and to do this kind of analysis in R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-01T01:15:06.307" Id="29602" LastActivityDate="2012-06-01T01:24:30.960" LastEditDate="2012-06-01T01:24:30.960" LastEditorUserId="11397" OwnerUserId="11397" ParentId="29595" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;It seems like you're using $n$ twice in two different ways - both as the sample size and as the number of bernoulli trials that comprise the Binomial random variable; to eliminate any ambiguity, I'm going to use $k$ to refer to the latter. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you have $n$ independent samples from a &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_distribution&quot;&gt;${\rm Binomial}(k,p)$&lt;/a&gt; distribution, the variance of their sample mean is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ {\rm var} \left( \frac{1}{n} \sum_{i=1}^{n} X_{i} \right) = \frac{1}{n^2} \sum_{i=1}^{n} {\rm var}( X_{i} ) = \frac{ n {\rm var}(X_{i}) }{ n^2 } = \frac{ {\rm var}(X_{i})}{n} = \frac{ k pq }{n} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $q=1-p$ and $\overline{X}$ is the same mean. This follows since &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance#Properties&quot;&gt;${\rm var}(cX) = c^2 {\rm var}(X)$,&lt;/a&gt; for any random variable, $X$, and any constant $c$. &lt;/p&gt;&#10;&#10;&lt;p&gt;(2) &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_.28Bienaym.C3.A9_formula.29&quot;&gt;the variance of a sum of independent random variables equals the sum of the variances&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The standard error of $\overline{X}$is the square root of the variance: $\sqrt{\frac{ k pq }{n}}$. Therefore, &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;When $k = n$, you get the formula you pointed out: $\sqrt{pq}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When $k = 1$, and the Binomial variables are just &lt;a href=&quot;http://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;bernoulli trials&lt;/a&gt;, you get the formula you've seen elsewhere: $\sqrt{\frac{pq }{n}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="9" CreationDate="2012-06-01T16:34:36.533" Id="29644" LastActivityDate="2012-06-01T22:45:25.897" LastEditDate="2012-06-01T22:45:25.897" LastEditorUserId="4856" OwnerUserId="4856" ParentId="29641" PostTypeId="2" Score="15" />
  <row AnswerCount="4" Body="&lt;p&gt;The likelihood ratio (a.k.a. deviance) $G^2$ statistic and lack-of-fit (or goodness-of-fit) test is fairly straightforward to obtain for a logistic regression model (fit using the &lt;code&gt;glm(..., family = binomial)&lt;/code&gt; function) in R. However, it can be easy to have some cell counts end up low enough that the test is unreliable. One way to verify the reliability of the likelihood ratio test for lack of fit is to compare its test statistic and &lt;em&gt;P&lt;/em&gt;-value to those of Pearson's chi square (or $\chi^2$) lack-of-fit test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Neither the &lt;code&gt;glm&lt;/code&gt; object nor its &lt;code&gt;summary()&lt;/code&gt; method report the test statistic for Pearson's chi square test for lack of fit. In my search, the only thing I came up with is the &lt;code&gt;chisq.test()&lt;/code&gt; function (in the &lt;code&gt;stats&lt;/code&gt; package): its documentation says &quot;&lt;code&gt;chisq.test&lt;/code&gt; performs chi-squared contingency table tests and goodness-of-fit tests.&quot; However, the documentation is sparse on how to perform such tests:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If &lt;code&gt;x&lt;/code&gt; is a matrix with one row or column, or if &lt;code&gt;x&lt;/code&gt; is a vector and &lt;code&gt;y&lt;/code&gt; is not given, then a &lt;em&gt;goodness-of-fit&lt;/em&gt; test is performed (&lt;code&gt;x&lt;/code&gt; is treated as a one-dimensional contingency table). The entries of &lt;code&gt;x&lt;/code&gt; must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in &lt;code&gt;p&lt;/code&gt;, or are all equal if &lt;code&gt;p&lt;/code&gt; is not given.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'd imagine that you could use the &lt;code&gt;y&lt;/code&gt; component of the &lt;code&gt;glm&lt;/code&gt; object for the &lt;code&gt;x&lt;/code&gt; argument of &lt;code&gt;chisq.test&lt;/code&gt;. However, you can't use the &lt;code&gt;fitted.values&lt;/code&gt; component of the &lt;code&gt;glm&lt;/code&gt; object for the &lt;code&gt;p&lt;/code&gt; argument of &lt;code&gt;chisq.test&lt;/code&gt;, because you'll get an error: &quot;&lt;code&gt;probabilities must sum to 1.&lt;/code&gt;&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I (in R) at least calculate the Pearson $\chi^2$ test statistic for lack of fit without having to run through the steps manually?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-01T18:18:19.507" FavoriteCount="4" Id="29653" LastActivityDate="2014-08-14T20:35:28.577" LastEditDate="2012-07-02T12:47:37.133" LastEditorUserId="4856" OwnerUserId="1583" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;&lt;goodness-of-fit&gt;" Title="How can I compute Pearson's $\chi^2$ test statistic for lack of fit on a logistic regression model in R?" ViewCount="3855" />
  
  <row Body="&lt;p&gt;If I did the math right, you have between &lt;code&gt;19.43%&lt;/code&gt; and &lt;code&gt;21.15%&lt;/code&gt; chance of winning a prize&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;19.43%&lt;/code&gt; is the best-case scenario, where every entrant has 6 tickets&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;21.15%&lt;/code&gt; is the worst-case scenario, where every entrant has 1 ticket except you&lt;/p&gt;&#10;&#10;&lt;p&gt;Both scenarios are extremely unlikely, so your actual odds of winning probably fall somewhere in between, however a roughly 1/5 chance at winning seems like a fairly solid number to go by&lt;/p&gt;&#10;&#10;&lt;p&gt;The details on how those numbers were obtained can be found in &lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0AkStwMIFXZABdDBOYl80N3gwXzFuZWstUy1MZFZGLXc&amp;amp;pli=1#gid=0&quot; rel=&quot;nofollow&quot;&gt;this Google spreadsheet&lt;/a&gt;, however to summarize how they were obtained:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Start with &lt;strong&gt;Total # of Entries&lt;/strong&gt; (784) and &lt;strong&gt;Your Entries&lt;/strong&gt; (6)&lt;/li&gt;&#10;&lt;li&gt;Get chance at winning (&lt;code&gt;6 / 784 = 0.77%&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;Subtract 6 for best-case, or 1 for worst-case from &lt;code&gt;TotalEntries&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;Get chance of winning (&lt;code&gt;6/778&lt;/code&gt; for best case &lt;code&gt;6/783&lt;/code&gt; for worst case)&lt;/li&gt;&#10;&lt;li&gt;Repeat steps 3-4 until you have 25 percentages&lt;/li&gt;&#10;&lt;li&gt;Add the 25 percentages together to find out your overall chance at winnning something&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Here's an alternative way to get the approximate percentage that is simpler, but is not as accurate since you are not removing duplicate entries every time you draw a winner.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;6 (your tickets) / 784 total tickets = 0.00765&#10;0.00765 chance to win * 25 prizes = 19.14 % chance to win&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I'm fairly sure I'm missing something in my math and that you cannot simply add percentages like this (or multiply percent chance to win by # of prizes), although I think I'm close&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/29651/is-it-possible-to-estimate-the-odds-of-winning-a-multi-entry-contest-when-i-don#comment56472_29655&quot;&gt;Whobar's comment&lt;/a&gt; gives a 17.4% chance of winning, although I still need to figure out the formula he gave and make sure it's accurate for the contest. Perhaps a weekend project :)&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-06-01T18:19:37.117" Id="29655" LastActivityDate="2012-06-01T20:17:32.587" LastEditDate="2012-06-01T20:17:32.587" LastEditorUserId="11703" OwnerUserId="11703" ParentId="29651" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have 52 samples daily, weekly and biweekly data for 3 subjects over the course of a 10 month period. At two points during the 10 month period the resolution increases to daily samples as the people begin taking a supplement. I have been considering a percentage of supplement that is (total supplication/total all samples) but I see that that was flawed because the time of the total for all samples is so grossly greater than the time during the supplement. Now I am considering (total supplement/total before) and (average supplement/average before). I think that the average of the supplements and before eliminates the time factor problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a link to the sample dates:&#10;&lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0Agl7ZLsLCsDQdFdzRGdhdU9Tb0hGNWtBOGkyMURnZlE&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/spreadsheet/ccc?key=0Agl7ZLsLCsDQdFdzRGdhdU9Tb0hGNWtBOGkyMURnZlE&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Would someone help me make an accurate comparison? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-06-01T20:54:15.160" Id="29661" LastActivityDate="2012-06-01T22:24:58.073" LastEditDate="2012-06-01T22:24:58.073" LastEditorUserId="88" OwnerUserId="11279" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;multiple-comparisons&gt;" Title="Accurate comparison over time period" ViewCount="149" />
  
  <row AcceptedAnswerId="29670" AnswerCount="2" Body="&lt;p&gt;In R, I have an $N \times K$ matrix $P$ where the $i$'th row of $P$ corresponds to a distribution on $\{1, ..., K\}$. Essentially, I need to sample from each row efficiently. A naive implementation is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X = rep(0, N);&#10;for(i in 1:N){&#10;    X[i] = sample(1:K, 1, prob = P[i, ]);&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is much too slow. In principle I could move this to C but I'm sure there must be an existing way of doing this. I would like something in the spirit of the following code (which does not work):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X = sample(1:K, N, replace = TRUE, prob = P)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; For motivation, take $N = 10000$ and $K = 100$. I have $P_1, ..., P_{5000}$ matrices all $N \times K$ and I need to sample a vector from each of them.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-06-02T00:44:14.983" Id="29669" LastActivityDate="2012-06-03T21:22:10.347" LastEditDate="2012-06-02T17:07:56.047" LastEditorUserId="5339" OwnerUserId="5339" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;mcmc&gt;" Title="How to sample from $\{1, 2, ..., K\}$ for $n$ random variables, each with different mass functions, in R?" ViewCount="423" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I conducted a pre and post survey that consisted of five questions with answers being either agree or disagree. The survey was to assess if a persons perception of Pit Bulls was improved after they interacted with it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The first or pre survey was before the individual interacted with the dog. After they did, the respondents immediately were given the same survey (the post survey).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I now have the results of the two surveys and am uncertain how I am to conduct the $\chi^2$ test to see if there was a significant improvement in the perception of the dog?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically I do not know if I am to assess each question individually or if the numbers are compiled for all questions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-02T14:50:15.173" Id="29691" LastActivityDate="2012-08-14T06:54:57.257" LastEditDate="2012-08-14T06:54:57.257" LastEditorUserId="3826" OwnerUserId="11717" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;survey&gt;" Title="How do I conduct a $\chi^2$ test on two different surveys?" ViewCount="104" />
  <row Body="&lt;p&gt;Your QQ plot bears a strong resemblance to that of a t distribution with 2 degrees of freedom (plot based on 40000 observations):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Wxh1G.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A t(3) and a Cauchy with sample sizes of 40,000 don't look as much like your QQ plot as the t(2), but note I'm not saying this is evidence that your error distribution is a t(2) or is even well-approximated by one.  The point is that your distribution is very fat-tailed indeed.  &lt;/p&gt;&#10;&#10;&lt;p&gt;With the t family, the optimal estimator of location is &lt;a href=&quot;http://en.wikipedia.org/wiki/Redescending_M-estimator&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;redescending&lt;/em&gt;&lt;/a&gt;, meaning, heuristically, that the weight applied to extreme observations goes to zero faster than the observation value goes to infinity.  This means that extreme values receive less weight than with the Huber estimator, for which absolute values greater than the parameter $k$ have weights that, in effect, go to zero as fast as the observation value goes to infinity.  In &lt;a href=&quot;http://en.wikipedia.org/wiki/Robust_statistics#M-estimators&quot; rel=&quot;nofollow&quot;&gt;the Wikipedia page for &quot;Robust statistics&quot;&lt;/a&gt;, a little over halfway down, is a graphical comparison of the influence of values on the Huber (called &quot;Winsorizing at 1.5&quot;) and Tukey biweight (a redescending estimator.)  A little farther down is a plot of the influence function of the optimal estimator for various t distributions.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Consequently, with this data, I'd not use the Huber $\psi$-function at all, instead setting &lt;code&gt;psi=biweight&lt;/code&gt; in your call to &lt;code&gt;rlm&lt;/code&gt;, and accepting the fact that a lot of your observations will have little or no influence on the final estimates.  Note, as @Macro observed in the thread to the other question, that this isn't a question of identifying and downweighting outliers, this is actually trying to get somewhat close to optimal estimates given your errors really do come from a very fat-tailed distribution - no outliers required or assumed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-02T17:40:13.727" Id="29696" LastActivityDate="2012-06-02T17:40:13.727" OwnerUserId="7555" ParentId="29636" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;We're trying to develop a simple tool that will help our teams optimise their clients' Facebook posting strategies. In our experience, time of post can have a big impact on audience response; but those times will differ from audience to audience based on their behaviours (can they access Facebook from work? What times of day are they online? When are they at a loose end, and when are other activities taking up their concentration?) We expect responsiveness to vary throughout the day for these (and other known &amp;amp; unknown) reasons. Furthermore, of course, time of day isn't the only reason that response rates vary; the content of the post is one very significant reason.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know if this background is useful; whether it makes the following question clearer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following R script (I lack sufficient reputation to post charts) contains data that cover posting activity and audience response by hour of post. You'll notice that the 6 am point (for which we have two observations) massively outperforms the rest of the day. We often see these very high response rates for hours that have fewer observations.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;post.hour &amp;lt;- c(0:23)&#10;posts &amp;lt;- c(0,0,0,0,0,0,2,15,16,10,17,13,29,21,23,18,29,24,34,42,51,48,49,17)&#10;response &amp;lt;- c(0,0,0,0,0,0,5282,8627,6080,2716,2831,3258,6291,7756,4008,4614,11838,2611,10527,14706,5416,10970,19098,9505)&#10;mean.response &amp;lt;- response/posts&#10;&#10;d &amp;lt;- data.frame(post.hour,posts,response,mean.response)&#10;&#10;library(ggplot2)&#10;&#10;response.chart &amp;lt;- ggplot(d,aes(post.hour,mean.response)) +&#10; geom_point() + geom_line() +&#10; ylab(&quot;mean response&quot;) +&#10; opts (title=&quot;mean audience response by post hour&quot;)&#10;response.chart&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've tried manually removing hours that have fewer than a certain number of observations:&lt;/p&gt;&#10;&#10;&lt;p&gt;This seems unsatisfactory, and (furthermore) is hard to automate across widely differing data sets.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Should I remove these hours? If so, what statistical tools should I use to perform this repeatedly and automatically across multiple data sets? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If the answer is &quot;no&quot; -- and I suspect that it might be -- what should I do to account for these tiny sample sizes?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Conscious that this is possibly Statistics 101 stuff. It should be clear from this that I have not - in fact - had any statistical training. I'd greatly appreciate you taking this into account if and when you respond to this question. I'm not even sure that the hourly data represent samples...&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-06-02T20:22:55.647" Id="29702" LastActivityDate="2012-06-03T17:44:14.033" LastEditDate="2012-06-03T17:44:14.033" LastEditorUserId="11719" OwnerUserId="11719" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;small-sample&gt;" Title="How should I account for small samples in a larger data set? Should I remove them?" ViewCount="123" />
  <row AcceptedAnswerId="29706" AnswerCount="3" Body="&lt;p&gt;We always say that statistics is just dealing with data. But we also know that informatics is also getting knowledge from data analysis. For example, bioinformatics people can totally go without biostatistics. I want to know what is the essential difference between statistics and informatics.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-02T21:02:58.087" FavoriteCount="1" Id="29703" LastActivityDate="2012-07-10T13:32:08.400" OwnerUserId="4810" PostTypeId="1" Score="8" Tags="&lt;bioinformatics&gt;" Title="What's the difference between statistics and informatics?" ViewCount="3054" />
  
  <row Body="&lt;p&gt;Excellent question!!&lt;/p&gt;&#10;&#10;&lt;p&gt;I heard several times that bioinformaticians can go without biostatistics, or even without statistics. That's perfectly true until it becomes false. In my opinion, general lack of statistical knowledge has disastrous effect in the field, as shown by &lt;a href=&quot;http://www.nytimes.com/2011/07/08/health/research/08genes.html&quot; rel=&quot;nofollow&quot;&gt;Keith Baggerly&lt;/a&gt;. I could also observe that lack of basic knowledge in statistics (and linear algebra) is the cause of stagnation of bioinformaticians in the long run: without a deep knowledge of the theory, they tend to reinvent the wheel and resort to &lt;em&gt;ad hoc&lt;/em&gt; solutions that solve nothing but their own problem.&#10;$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $&lt;/p&gt;&#10;&#10;&lt;p&gt;But now, to answer your question, I agree that overall, statistics can't do without computers those days. Yet, one of the major aspects of statistics is &lt;strong&gt;inference&lt;/strong&gt;, which has nothing to do with computers. &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_inference&quot; rel=&quot;nofollow&quot;&gt;Statistical inference&lt;/a&gt; is actually what makes statistics a science, because it tells you whether or not your conclusions hold up in other contexts.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, you can analyze the hell out of your data, you will still need statistics to know the &lt;strong&gt;validity&lt;/strong&gt; of the predictions or decisions you will make based on your analyses.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-02T21:59:39.697" Id="29706" LastActivityDate="2012-07-10T10:53:44.510" LastEditDate="2012-07-10T10:53:44.510" LastEditorUserId="10385" OwnerUserId="10849" ParentId="29703" PostTypeId="2" Score="14" />
  <row AcceptedAnswerId="29722" AnswerCount="2" Body="&lt;p&gt;I'm conducting a project where I determine which factors (3 in total) contribute to the accuracy in which one would hit a dartboard. Each factor has two levels, low and high. For example, one of the factors is distance and the lowest distance (closest to the dart board) is 2 meters while the highest distance is 4 meters. Another factor I'm using is the left and right hand to throw.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is, how do I measure the response? should I indicate on the dart board two areas where one is considered a bulls-eye (success) and the other a failure? Or should I measure the distance in centimeters between the bulls-eye and the dart to determine which dart has come closest?&lt;/p&gt;&#10;&#10;&lt;p&gt;which is the appropriate method? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-03T03:58:13.687" Id="29712" LastActivityDate="2012-06-03T12:28:02.143" LastEditDate="2012-06-03T04:54:40.383" LastEditorUserId="4856" OwnerUserId="11724" PostTypeId="1" Score="3" Tags="&lt;experiment-design&gt;&lt;measurement&gt;" Title="Experiment involving darts, how should I measure the response?" ViewCount="233" />
  <row Body="&lt;p&gt;In my opinion, there are multiple cut-off options. You might weight sensitivity and specificity differently (for example, maybe for you it is more important to have a high sensitive test even though this means having a low specific test. Or vice-versa).&lt;/p&gt;&#10;&#10;&lt;p&gt;If sensitivity and specificity have the same importance to you, one way of calculating the cut-off is choosing that value that minimizes the Euclidean distance between your ROC curve and the upper left corner of your graph.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way is using the value that maximizes the sum of sensitivity and specificity as a cut-off.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, I do not have references for these two methods as I have learned them from professors or other statisticians. I have only heard referring to the first method as the 'Jouden method').&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-03T12:04:50.617" Id="29727" LastActivityDate="2014-01-02T22:20:05.630" LastEditDate="2014-01-02T22:20:05.630" LastEditorUserId="26708" OwnerUserId="9253" ParentId="29719" PostTypeId="2" Score="14" />
  <row AcceptedAnswerId="29748" AnswerCount="3" Body="&lt;p&gt;There are several threads on this site discussing &lt;a href=&quot;http://stats.stackexchange.com/questions/12053/what-should-i-check-for-normality-raw-data-or-residuals&quot;&gt;how&lt;/a&gt; to determine if the OLS residuals are &lt;a href=&quot;http://stats.stackexchange.com/questions/29709/what-happens-if-you-reject-normality-of-residuals-when-estimating-with-least-squ&quot;&gt;asymptotically&lt;/a&gt; normally distributed. Another way to evaluate the normality of the residuals with R code is provided in this excellent &lt;a href=&quot;http://stats.stackexchange.com/questions/22468/residuals-in-linear-regression&quot;&gt;answer&lt;/a&gt;. This is another &lt;a href=&quot;http://stats.stackexchange.com/questions/12945/standardized-residuals-vs-regular-residuals&quot;&gt;discussion&lt;/a&gt; on the practical difference between standardized and observed residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;But let's say the residuals are definitely not normally distributed, like in this &lt;a href=&quot;http://stats.stackexchange.com/questions/29636/robust-regression-setting-the-limit-between-errors-and-influential-observations&quot;&gt;example&lt;/a&gt;. Here we have several thousand observations and clearly we must reject the normally-distributed-residuals assumption. One way to address the problem is to employ some form of robust estimator as explained in the answer. However I am not limited to OLS and in facts I would like to understand the benefits of other glm or non-linear methodologies.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the most efficient way to model data violating the OLS normality of residuals assumption? Or at least what should be the first step to develop a sound regression analysis methodology?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-06-03T13:24:20.773" FavoriteCount="14" Id="29731" LastActivityDate="2014-08-14T15:25:13.977" LastEditDate="2012-07-18T00:25:53.697" LastEditorUserId="4856" OwnerUserId="7795" PostTypeId="1" Score="14" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;assumptions&gt;&lt;residual-analysis&gt;" Title="Regression when the OLS residuals are not normally distributed" ViewCount="22127" />
  
  <row AcceptedAnswerId="29747" AnswerCount="1" Body="&lt;p&gt;I have three variables some of which are occasionally 0 which is a na and not a sample. I would like to calculate the standard deviation of the remaining two items but there isn't a deviation if like there is an averageif does anyone know how to do an STDEVIF?&#10;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-03T20:55:45.227" Id="29746" LastActivityDate="2012-06-04T10:35:01.237" LastEditDate="2012-06-04T10:35:01.237" LastEditorUserId="88" OwnerUserId="11279" PostTypeId="1" Score="2" Tags="&lt;standard-deviation&gt;&lt;excel&gt;" Title="Standard deviation if in Excel" ViewCount="395" />
  
  
  
  
  
  <row AcceptedAnswerId="29809" AnswerCount="1" Body="&lt;p&gt;I'm looking for an article, program, algorithm that can clearly explain whats going on inside a &lt;strong&gt;Multinomial Naive Bayes classifier&lt;/strong&gt; compared to a &lt;strong&gt;Gaussian Naive Bayes Classifier&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-04T22:50:34.687" Id="29801" LastActivityDate="2012-06-05T09:49:39.087" OwnerUserId="7918" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;classification&gt;&lt;naive-bayes&gt;&lt;pattern-recognition&gt;" Title="Multinomial Naive Bayes" ViewCount="253" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm performing the cox regression analysis, where I have the model with 1 time-independent (treatment (yes/no)) and 1 time-dependent predictor (blood pressure level). &lt;/p&gt;&#10;&#10;&lt;p&gt;Outcome is death (yes/no). Time until death is measured in days and the count starts after the date of treatment prescription (time=0 days) in patients.&lt;/p&gt;&#10;&#10;&lt;p&gt;Time-independent predictor (treatment) have a value 1 (yes) or 0 (no) at time=0 days and remains the same until either death occur or time of observation is over (censored).&lt;/p&gt;&#10;&#10;&lt;p&gt;Time-dependent predictor, in turn, may appear in the model at time 90 (on 90th day after treatment date).&lt;/p&gt;&#10;&#10;&lt;p&gt;I compute the segmented time-dependent predictors in a way as it described in SPSS help (BP - blood pressure):&lt;/p&gt;&#10;&#10;&lt;p&gt;COMPUTE T_COV = (T_ &amp;lt; BP1_time) * BP1 + (T_ &gt;= BP1_time &amp;amp; T_ &amp;lt; BP2_time) * BP2 + (T_ &gt;= BP2_time &amp;amp; T_ &amp;lt; BP3_time) * BP3 + (T_ &gt;= BP3_time &amp;amp; T_ &amp;lt; BP4_time) * BP4....&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The question is:&lt;/strong&gt; how does SPSS deal with the time within the period from 0 to 90 days for the time-dependent predictor? Does it just ignore this time period and start to measure hazard at the day when the BP variable enters to the model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-05T09:18:39.503" Id="29816" LastActivityDate="2012-06-05T12:50:41.053" LastEditDate="2012-06-05T12:50:41.053" LastEditorUserId="919" OwnerUserId="11761" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;survival&gt;&lt;cox-model&gt;" Title="Segmented time-dependent covariate" ViewCount="731" />
  
  <row Body="Models in which a complicated function of data is estimated in the first step, and plugged again into another estimation model of primary interest in the second step" CommentCount="0" CreationDate="2012-06-05T13:52:15.797" Id="29830" LastActivityDate="2012-06-05T14:28:35.767" LastEditDate="2012-06-05T14:28:35.767" LastEditorUserId="5739" OwnerUserId="5739" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;My first thought was a mixed effects model, but that has already been discussed so I won't say any more on that.&lt;/p&gt;&#10;&#10;&lt;p&gt;My other thought is that if it were theoretically possible that you could have measured paired data on all subjects but due to cost, errors, or another reason you don't have all the pairs, then you could treat the unmeasured effect for the unpaired subjects as missing data and use tools like the EM algorithm or Multiple Imputation (missing at random seems reasonable unless the reason a subject was only measured under 1 treatment was related to what their outcome would be under the other treatment).  &lt;/p&gt;&#10;&#10;&lt;p&gt;It may be even simpler to just fit a bivariate normal to the data using maximum likelihood (with the likelihood factored based on the available data per subject), then do a likelihood ratio test comparing the distribution with the means equal vs. different means.&lt;/p&gt;&#10;&#10;&lt;p&gt;It has been a long time since my theory classes, so I don't know how these compare on optimality.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-05T17:27:49.897" Id="29840" LastActivityDate="2013-08-03T20:51:20.723" LastEditDate="2013-08-03T20:51:20.723" LastEditorUserId="17230" OwnerUserId="4505" ParentId="25941" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The quote from Pinker's book and the idea of a deterministic world completely ignore Quantum Mechanics and the Heisenberg Uncertaintly Principle.  Imagine putting a small amount of something radioactive near a detector and arranging the amounts and distances so that there will be a 50% chance of detecting a decay during a pre-determined time interval.  Now connect the detector to a relay that will do something highly significant if a decay is detected and operate the device once and only once.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You have now created a situation where the future is inherently unpredictable.  (This example is drawn from one described by whomever taught sophomore or junior year physics at MIT back in the middle 1960's.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-05T19:01:04.463" Id="29842" LastActivityDate="2012-06-05T19:01:04.463" OwnerUserId="8871" ParentId="29665" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="29919" AnswerCount="1" Body="&lt;p&gt;Which may be good techniques to face this abstract problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;You have a data stream of a continuous signal, as one from a physical sensor. That signal has real (discretized) values, no attribute; addictional features (e.g., power, auto-correlation, entropy) might be extracted. You can assign one label from a finite set to a window of the signal. Let this label be a &lt;em&gt;training&lt;/em&gt; label. You have to choise start and end points of the window as well as the window label.&lt;/p&gt;&#10;&#10;&lt;p&gt;The task is to classify next windows online, as just as the signal is received.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am asking for an incremental algorithm, in the sense that it should increase its detection performance given more training labels. But it must be able to classify even after only one training label.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the problem turns out to be too hard because of windows-boundaries detection, let's say you can fix their size at a small constant. Thus the algorithm classifies little slices of the signal and then it merges adjacent ones with same labels. If you use that simplified approach, please justify why it is reasonable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-06T00:49:51.633" FavoriteCount="1" Id="29862" LastActivityDate="2012-06-06T16:11:12.213" LastEditDate="2012-06-06T04:48:45.907" LastEditorUserId="11583" OwnerUserId="11583" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;signal-processing&gt;&lt;online&gt;" Title="Techniques for incremental online learning of classifier on stream data" ViewCount="184" />
  
  <row Body="&lt;p&gt;A non-parametric, rank-based method to perform two-way ANOVAs with interactions is a method proposed by Brunner, Dette, and Munk (1997), which improves on the Kruskal- Wallis test&#10;(for details, see Wilcox, 2010). This method should be able to deal with all-zero cells.&lt;/p&gt;&#10;&#10;&lt;p&gt;It can be performed using the &lt;code&gt;bdm2way&lt;/code&gt; function from the &lt;code&gt;WRS&lt;/code&gt; package for &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Brunner, E., Domhof, S., &amp;amp; Langer, F. (2002). Nonparametric analysis of longitudinal data in factorial experiments. New York: Wiley.&lt;/p&gt;&#10;&#10;&lt;p&gt;Wilcox, R. R. (2010). Fundamentals of modern statistical methods: Substantially improving power and accuracy. Springer Verlag.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-06T07:09:49.333" Id="29874" LastActivityDate="2012-06-06T07:09:49.333" OwnerUserId="6082" ParentId="29812" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/7200/evaluate-definite-interval-of-normal-distribution&quot;&gt;Evaluate definite interval of normal distribution&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Title was changed and question edited bellow.&lt;/p&gt;&#10;&#10;&lt;p&gt;How is possible that a probability density function defined as following can return some values that are higher than 1 ?! Is there any error in my definition of this pdf ?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;float gauss_pdf = exp( -pow(x-mean, 2.0) / (2.0 * pow(sigma, 2.0)) )   /   ( sigma * sqrt(2*M_PI) );&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For example, for x = 0.0908182, mean = 0.0552096 and sigma = 0.0241953, it will return 5.583&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Well, what I need is the corresponding probability, thus I need to compute the CDF (i.e. P(X &amp;lt; x)) for that mean and sigma. How can I compute the CDF ?&lt;/p&gt;&#10;" ClosedDate="2012-06-06T14:42:25.213" CommentCount="1" CreationDate="2012-06-06T12:28:23.593" FavoriteCount="0" Id="29890" LastActivityDate="2012-06-06T13:41:12.230" LastEditDate="2012-06-06T12:57:23.400" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;normal-distribution&gt;" Title="How to compute CDF probability of normal distribution" ViewCount="47" />
  
  <row Body="&lt;p&gt;Hastie and Tibshirani's paper on &lt;em&gt;&lt;a href=&quot;http://www.stanford.edu/~hastie/Papers/dann_IEEE.pdf&quot; rel=&quot;nofollow&quot;&gt;Discriminative Adaptive Nearest Neighbour Classification&lt;/a&gt;&lt;/em&gt; would be a good place to start.&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple approach would be to choose the weights to minimise the leave-one-out error rate.  However one of the advantages of kNN is that, being a relatively simple method, it is usually quite easy to avoid over-fitting (basically just need to choose k), and this advantage is easily lost if you try to tune the distance metric, so it may well make the performance of the model worse rather than better.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-06T12:51:05.977" Id="29893" LastActivityDate="2012-06-06T12:51:05.977" OwnerUserId="887" ParentId="27013" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;I will just try to share my experience as I did some marketing research. Of course everything depends on the data itself, type of business, frequency constraints, etc... So I want you to understand that there is no exact answer for your question.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A ranking of the various pitches in their effectiveness, and an indication of how much the pitch affects the result.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I would just compare the cumulative results of the customers who were involved into pitch with the cumulative results for all the rest. Then you can see relative success of each pitch and rank them afterwards.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Same for the day of the week and time.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This depends a lot of the nature of your pitches. You can either do the same procedure as described above for every day of the week, or for a pair &quot;pitch+day&quot;.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;And, trickier -- How much of a correlation does their previous business have to do with who converts now? Is it the previous big spenders that convert? Are they the ones buying the most product?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You are right, that's a little trickier. I would first segment your customers into groups (as you said &quot;previous big spenders&quot; can be one of the groups). And after that I will have a look at the impact of the pitch for every group separately. You will see that some groups are converted more (the relative success of the pitch is higher).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;And finally, of all the attributes above, how do they rank in their importance to conversions?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That can vary a lot from one industry to another. So I cannot answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope some of my suggestions would help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-06T23:54:00.103" Id="29953" LastActivityDate="2012-06-06T23:54:00.103" OwnerUserId="11639" ParentId="12924" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;One version of &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;CLT&lt;/a&gt; tells us that the distribution of averages of independent identically distributed random variables will start to look like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gaussian_function&quot; rel=&quot;nofollow&quot;&gt;bell&lt;/a&gt;-shaped &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_distribution&quot; rel=&quot;nofollow&quot;&gt;normal distribution&lt;/a&gt; as the number of variables in the sum ($n$) gets large.  Formal mathematical convergence takes place under mild conditions on the distribution when the average is appropriately normalized. This will work for most population distributions of various shapes including &lt;a href=&quot;http://en.wikipedia.org/wiki/Gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;gamma&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Triangular_distribution&quot; rel=&quot;nofollow&quot;&gt;triangular&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29&quot; rel=&quot;nofollow&quot;&gt;uniform&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;beta&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Chi-squared_distribution&quot; rel=&quot;nofollow&quot;&gt;chi square&lt;/a&gt; and even discrete distributions like &lt;a href=&quot;http://en.wikipedia.org/wiki/Bernoulli_distribution&quot; rel=&quot;nofollow&quot;&gt;Bernoulli&lt;/a&gt;.  This makes it easy to do inference on the mean of a distribution based on a random sample by &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot; rel=&quot;nofollow&quot;&gt;testing hypotheses&lt;/a&gt; or constructing &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval&quot; rel=&quot;nofollow&quot;&gt;confidence intervals&lt;/a&gt; based on the approximating normal distribution. Because the variance of the sample mean goes to $0$ at a rate of $1/n$, the mean will actually converge to a degenerate distribution with all its probability mass at the population mean.  So the appropriate normalization for convergence to a normal requires recentering and multiplication by $\sqrt{n}$. There are other statistics that come up that also converge to the normal.  The fact that the normal distribution can be used to approximate the distribution of various &lt;a href=&quot;http://en.wikipedia.org/wiki/Test_statistic&quot; rel=&quot;nofollow&quot;&gt;test statistics&lt;/a&gt; is the reason for its prominence in statistics.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-07T10:39:35.747" Id="29970" LastActivityDate="2013-06-30T19:11:45.780" LastEditDate="2013-06-30T19:11:45.780" LastEditorDisplayName="user10525" LastEditorUserId="919" OwnerUserId="11032" ParentId="29957" PostTypeId="2" Score="-1" />
  
  <row AcceptedAnswerId="29991" AnswerCount="1" Body="&lt;p&gt;Let's have some linear model, for example just simple ANOVA:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# data generation&#10;set.seed(1.234)                      &#10;Ng &amp;lt;- c(41, 37, 42)                    &#10;data &amp;lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      &#10;fact &amp;lt;- as.factor(rep(LETTERS[1:3], Ng)) &#10;&#10;m1 = lm(data ~ 0 + fact)&#10;summary(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Result is as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = data ~ 0 + fact)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-2.30047 -0.60414 -0.04078  0.54316  2.25323 &#10;&#10;Coefficients:&#10;      Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;factA  -0.9142     0.1388  -6.588 1.34e-09 ***&#10;factB   0.1484     0.1461   1.016    0.312    &#10;factC   1.0990     0.1371   8.015 9.25e-13 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 0.8886 on 117 degrees of freedom&#10;Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 &#10;F-statistic: 36.23 on 3 and 117 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I try two different methods to estimate confidence interval of these parameters&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;c = coef(summary(m1))&#10;&#10;# 1st method: CI limits from SE, assuming normal distribution&#10;cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], &#10;    high = c[,1] + qnorm(p = 0.975) * c[,2])&#10;&#10;# 2nd method&#10;confint(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h2&gt;Questions:&lt;/h2&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the distribution of estimated linear regression coefficients? Normal or $t$?&lt;/li&gt;&#10;&lt;li&gt;Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you very much!&lt;/p&gt;&#10;&#10;&lt;p&gt;data ~ 0 + fact&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT after an answer&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is exact, this will give exactly the same result as &lt;code&gt;confint(m1)&lt;/code&gt;!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# 3rd method&#10;cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], &#10;    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-06-07T12:36:48.833" FavoriteCount="1" Id="29981" LastActivityDate="2012-10-11T23:09:08.823" LastEditDate="2012-10-11T23:09:08.823" LastEditorUserId="5509" OwnerUserId="5509" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;regression&gt;&lt;confidence-interval&gt;" Title="Should confidence intervals for linear regression coefficients be based on the normal or $t$ distribution?" ViewCount="3550" />
  <row AnswerCount="0" Body="&lt;p&gt;during my master thesis I'm working on / towards an OCR for math. For testing purposes I need a massive set of mathematical equations of various formats (font family, font size, used equation formatter etc.) I did some googling and could only find corpora for equations &lt;em&gt;as text&lt;/em&gt;, rather than &lt;em&gt;as images&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since an OCR should render images and retrieve the text, those corpora don't help me. I thought about recreating the images, e.g. if the equation is given as TeX, but that would mean that I get pretty similar looking equations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;" ClosedDate="2012-06-07T16:05:18.090" CommentCount="5" CreationDate="2012-06-07T13:48:23.643" Id="29984" LastActivityDate="2012-06-07T13:48:23.643" OwnerUserId="11824" PostTypeId="1" Score="1" Tags="&lt;dataset&gt;&lt;pattern-recognition&gt;&lt;mathematics&gt;" Title="Where can I find a large corpus of mathematical equations as images?" ViewCount="76" />
  <row AcceptedAnswerId="29989" AnswerCount="3" Body="&lt;p&gt;I have a clustering algorithm, where if I use an euclidian distance as similarity, it works well on any dataset. If I replace it by a cosine similarity (see my code bellow), it will give a degenerate results (will not work at all). Did I do an error in coding this cosine similarity or it is the cosine similarity that should by nature work only on some kind of data ?!&lt;/p&gt;&#10;&#10;&lt;p&gt;And by the way, this is a &quot;similarity&quot;, is there any different between it and the &quot;distance&quot; ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are example vectors from two datasets that I use. The second dataset may contain many repeated vectors:&lt;/p&gt;&#10;&#10;&lt;p&gt;Examples from dataset1: &lt;a href=&quot;http://pastebin.com/6iYcqgWF&quot; rel=&quot;nofollow&quot;&gt;http://pastebin.com/6iYcqgWF&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Examples from dataset2: &lt;a href=&quot;http://pastebin.com/4MtLXwp7&quot; rel=&quot;nofollow&quot;&gt;http://pastebin.com/4MtLXwp7&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: the square is just because the function is called under a root in the main program ..&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;// My squared  euclidean distance similarity &#10;float computeSqrDistance(vector&amp;lt;float&amp;gt; pos1, vector&amp;lt;float&amp;gt; pos2)&#10;{&#10;    float sum = 0;&#10;&#10;    for(unsigned int i = 0; i &amp;lt; pos1.size(); ++i)&#10;    {&#10;        sum += pow( (pos1[i] - pos2[i]), 2.0 );&#10;    }&#10;&#10;    return sum;&#10;}&#10;&#10;&#10;// My squared cosine distance similarity &#10;float computeSqrDistance(vector&amp;lt;float&amp;gt; pos1, vector&amp;lt;float&amp;gt; pos2)&#10;{&#10;    float sum0 = 0, sum1 = 0, sum2 = 0;&#10;&#10;    for(unsigned int i = 0; i &amp;lt; pos1.size(); ++i)&#10;    {&#10;        sum0 += pos1[i] * pos2[i];&#10;        sum1 += (pos1[i]*pos1[i]);&#10;        sum2 += (pos2[i]*pos2[i]);&#10;    }&#10;&#10;    float similarity = sum0 / ( sqrt(sum1) * sqrt(sum2) );&#10;&#10;    similarity = 1 - (acos(similarity) / M_PI);&#10;&#10;    return (similarity*similarity);&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="10" CreationDate="2012-06-07T14:08:26.823" FavoriteCount="1" Id="29987" LastActivityDate="2012-06-07T19:10:12.993" LastEditDate="2012-06-07T16:49:35.250" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;distance-functions&gt;&lt;distance&gt;&lt;similarities&gt;&lt;euclidean&gt;" Title="Using a cosine similarity does not work for any dataset" ViewCount="2221" />
  
  
  <row Body="&lt;p&gt;When you say &lt;em&gt;standard error&lt;/em&gt;, you should be talking about the standard error of something, such as the &lt;em&gt;standard error of the sample mean&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly you could for example talk about the &lt;em&gt;median absolute deviation of the sample median&lt;/em&gt;. It is possible to calculate this, at least as an approximation for large samples.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It is well known that for a continuous random variable with population median $m$, continuous probability density of the median $f(m)$ and a large odd sample size $n$, the sample median is approximately normally distributed with median $m$ and variance $\frac{1}{4 n f(m)^2}$, i.e. with median absolute deviation approximately $\dfrac{\Phi^{-1}\left(\frac34 \right)}{2  \sqrt{n} f(m)}$ where $\frac{\Phi^{-1}\left(\frac34 \right)}{2} \approx 0.337$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to have this as a &lt;em&gt;relative&lt;/em&gt; median absolute deviation of the sample median, then presumably you divide by $m$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-07T20:02:01.577" Id="30015" LastActivityDate="2012-06-08T06:15:04.167" LastEditDate="2012-06-08T06:15:04.167" LastEditorUserId="2958" OwnerUserId="2958" ParentId="30002" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In the setting of multivariate multiple regression (vector regressor and regressand), the four major tests for the general hypothesis (Wilk's Lambda, Pillai-Bartlett, Hotelling-Lawley, and Roy's Largest Root) all depend on the eigenvalues of the matrix $H E^{-1}$, where $H$ and $E$ are the 'explained' and 'total' variation matrices. &lt;/p&gt;&#10;&#10;&lt;p&gt;I had noticed that the Pillai and Hotelling-Lawley statistics could both be expressed as&#10;$$\psi_{\kappa} = \mbox{Tr}\left(H\left[\kappa H + E\right]^{-1}\right),$$&#10;for, respectively, $\kappa = 1, 0$. I am looking at an application where the distribution of this trace, defined for the population analogues of $H$ and $E$, is of interest for the $\kappa = 2$ case. (modulo errors in my work.) I am curious if there is some known unification of the sample statistics for general $\kappa$, or some other generalization that captures two or more of the four classical tests.  I realize that for $\kappa$ not equal to $0$ or $1$, the numerator no longer looks like a Chi-square under the null, and so a central F approximation seems questionable, so maybe this is a dead end.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am hoping that there has been some research on the distribution of $\psi_{\kappa}$ under the null (&lt;em&gt;i.e.&lt;/em&gt; the true matrix of regression coefficients is all zero), and under the alternative. I am interested particularly in the $\kappa = 2$ case, but if there is work on the general $\kappa$ case, I could, of course, use that.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-08T04:58:08.957" FavoriteCount="1" Id="30045" LastActivityDate="2012-08-24T19:00:28.323" LastEditDate="2012-06-25T17:37:02.917" LastEditorUserId="5739" OwnerUserId="795" PostTypeId="1" Score="9" Tags="&lt;regression&gt;&lt;multivariate-analysis&gt;&lt;multiple-regression&gt;&lt;manova&gt;&lt;hotelling&gt;" Title="Is there a generalization of Pillai trace and the Hotelling-Lawley trace?" ViewCount="297" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Suppose $X\beta =Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We don't know $Y$ exactly, only its correlation with each predictor, $X^\mathrm{t}Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The ordinary least-squares (OLS) solution is $\beta=(X^\mathrm{t} X)^{-1} X^\mathrm{t}Y$ and there isn't a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;But suppose $X^\mathrm{t}X$ is near singular (multicollinearity), and you need to estimate the optimal ridge parameter. All the methods seems to need the exact values of $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an alternative method when only $X^\mathrm{t}Y$ is known?&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-06-08T07:36:53.183" FavoriteCount="2" Id="30051" LastActivityDate="2013-10-07T10:06:21.207" LastEditDate="2013-10-07T09:54:16.827" LastEditorUserId="17230" OwnerUserId="11848" PostTypeId="1" Score="13" Tags="&lt;regression&gt;&lt;multicollinearity&gt;" Title="Linear regression when you only know $X^t Y$, not $Y$ directly" ViewCount="366" />
  <row Body="&lt;p&gt;Using Google I found Tommy Franks derogatory quote about Douglas Feith that appears in Bob Woodward's book &quot;Plan of Attack&quot; and is related to the Geneva convention.  Unfrotunately I have not able to find a connection to robust regression yet. See &lt;a href=&quot;http://www.nndb.com/people/100/000047956/&quot; rel=&quot;nofollow&quot;&gt;http://www.nndb.com/people/100/000047956/&lt;/a&gt;.&#10;As I research this more I think this was most likely a joke perhaps related to this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;In December 2001, Secretary of Defense Donald Rumsfeld and then Bush received Franks’s preliminary commander’s estimate, followed by&#10;  the first workable’, in Tenet’s words, Iraq war plan put to Bush on 1&#10;  February 2002 and successive plans through to Bush’s final approval on&#10;  6 September.[22] One of the variables from the outset was the degree&#10;  of international support and involvement. The assumptions about levels&#10;  of allied support ranged from ‘robust’ through ‘reduced’ to nil. If&#10;  the American military had their way, they would operate unilaterally,&#10;  but pressures come from the politicians to make room for allies.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I found this quote of Franks which is most likely connected to the Lancet article:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;In the article below we were wrong to say that a household survey conducted by the World Health Organisation and the Iraqi health&#10;  ministry found that the rate of violent deaths had doubled in Iraq&#10;  after the invasion. The survey did not make this finding. Figures that&#10;  were unadjusted for under-reporting showed a doubling of the rate of&#10;  all deaths and a violence-related death rate about 11 times higher.&#10;  The article said the survey estimated that 151,000 civilians had been&#10;  killed since the invasion. That figure included combatants. The&#10;  article below should have also made clear that the Lancet and Opinion&#10;  Research Business surveys included combatants as well as civilians.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot; Lieutenant General Tommy Franks, who led the invasions of Iraq and&#10;  Afghanistan during his time as head of US Central Command, once&#10;  announced, &quot;We don't do body counts.&quot; This blunt response to a&#10;  question about civilian casualties was an attempt to distance George&#10;  Bush's wars from the disaster of Vietnam. One of the rituals of that&#10;  earlier conflict was the daily announcement of how many Vietnamese&#10;  fighters US forces had killed. It was supposed to convince a sceptical&#10;  American public that victory was coming. But the 'body count' concept&#10;  sounded callous - and never more so than when it emerged that many of&#10;  the alleged guerrilla dead were in fact women, children and other&#10;  unarmed civilians.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2012-06-08T10:26:45.783" Id="30057" LastActivityDate="2012-06-08T15:07:41.540" LastEditDate="2012-06-08T15:07:41.540" LastEditorUserId="11032" OwnerUserId="11032" ParentId="30047" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="30089" AnswerCount="1" Body="&lt;p&gt;The problem of designing a multi-class classifier using &lt;strong&gt;LDA&lt;/strong&gt; can be expressed as a &lt;strong&gt;2 class problem&lt;/strong&gt;(one vs everything else) or a &lt;strong&gt;multi-class&lt;/strong&gt; problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is it that in certain cases &lt;strong&gt;Multi-class LDA classifier&lt;/strong&gt; out-performs &lt;strong&gt;2 class LDA&lt;/strong&gt; (one vs everything else) or &lt;strong&gt;vice-versa&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-08T15:29:37.567" FavoriteCount="2" Id="30075" LastActivityDate="2012-06-08T17:54:50.660" OwnerUserId="7918" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;pattern-recognition&gt;&lt;discriminant-analysis&gt;" Title="Multi class LDA vs 2 class LDA" ViewCount="1108" />
  <row Body="&lt;p&gt;If you keep all the components from a PCA - then the Euclidean distances between patients in the new PCA-space will equal their Mahalanobis distances in the observed-variable space. If you'll skip some components, that will change a little, but anyway. Here I refer to to unit-variance PCA-components, not the kind whose variance is equal to eigenvalue (I am not sure about your PCA implementation).&lt;/p&gt;&#10;&#10;&lt;p&gt;I just mean, that if you want to evaluate Mahalanobis distance between the patients, you can apply PCA and evaluate Euclidean distance. Evaluating Mahalanobis distance after applying PCA seems something meaningless to me.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-08T15:46:29.073" Id="30079" LastActivityDate="2012-06-08T15:46:29.073" OwnerUserId="11639" ParentId="24221" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;You can still do the chi-square test.  Nothing says that the bins have to be 1 dimensional.  Divide the globe into longitude by latitude segments and count the number of cases in each bin for the two samples.  The same chi square test applies.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-06-08T15:55:39.157" Id="30080" LastActivityDate="2012-06-08T15:55:39.157" OwnerUserId="11032" ParentId="30072" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The answer really seems to be that cross-validation is not great because its results are extremely variable but it remains the best option available.  The only other competitive approach seems to be the 0.632 bootstrap estimator which has slightly lower variance but also under-estimates the true performance.  See &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/14960464?dopt=Abstract&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;Is cross-validation valid for small-sample microarray classification?&lt;/strong&gt;&lt;/a&gt;.  Also of relevance - (perhaps obvious) - the more features that are included, the higher the variance of the cv-estimates.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-08T19:09:57.770" Id="30094" LastActivityDate="2012-06-08T19:09:57.770" OwnerUserId="11674" ParentId="29583" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;At first glance this looks like snowball sampling, but snowball sampling usually proceeds to get more referalls from the refered subjects.&lt;/p&gt;&#10;&#10;&lt;p&gt;This could also be considered cluster sampling where a cluster is defined by a working mother, her manager, and her closest co-worker.  You are sampling an entire cluster by choosing the mother.  So you could use techniques based on cluster sampling.  Or it could even be that each triplet is really your observational unit (if you are more interested in relationships between the 3 people than something like the average age of the managers) and could be treated as a simple random sample (but the data is the triplet).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem comes in that it is unlikely that the population of interest matches the population that you are sampling from.  Any employees who are not the closest coworker to a mother have 0 probability of being sampled, any manager who does not supervise a mother has 0 probability of being sampled, and any manager who supervises more than 1 mother has a higher chance of being sampled than those that only supervise 1.  The last part could be handled by weighting, but the under coverage in the other 2 could seriously bias any results.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-08T19:49:48.390" Id="30098" LastActivityDate="2012-06-08T19:49:48.390" OwnerUserId="4505" ParentId="30043" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I've found a variation of the $\chi^2$ statistic that looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\chi^2 = \sum\limits_{i=1}^N\,\chi_i^2 = \sum\limits_{i=1}^N\,\frac{(\log m_{i}- \log n_{i})^2}{\sigma_{i}^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\sigma_{i}=1/n_i$, $m_i$ is the modeled number of counts in bin $i$ and $n_i$ is the observed number (both are of course &lt;strong&gt;positive and integers&lt;/strong&gt;) $log$ is the decimal logarithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $m_i=0$ or $n_i=0$, the author assigns a very small value to avoid inconsistencies evaluating the $log$ function at $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that this parameter is biased in the sense that it will give more weight to $\chi_i$ factors where $n_i\neq0$; ie: it will pretty much disregard $\chi_i$ factors where $n_i=0$ (which will be replaced by a very small number, say $0.0001$) thus allowing the model to assign almost any number $m_i$ to that bin.&lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://www.wolframalpha.com/input/?i=plot%200.0001%2a%28log10%280.0001%29-log10%28x%29%29%2a%2a2%20x%20from%200%20to%2030&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for an example of $\chi_i$ when $n_i=0$ (replaced by $0.0001$) and &lt;a href=&quot;http://www.wolframalpha.com/input/?i=plot%201%2a%28log10%281%29-log10%28x%29%29%2a%2a2%20x%20from%202%20to%2030&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; to see the behavior of $\chi_i$ when $n_i=1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly for what I see in the case where $n_i=1$ the values of $\chi_i$ are much bigger than those obtained by the same parameter when $n_i=0$ (replaced by $0.0001$)&lt;/p&gt;&#10;&#10;&lt;p&gt;I see this as a prove that the statistic is biased. Am I correct here?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-08T23:44:26.227" Id="30110" LastActivityDate="2012-06-08T23:44:26.227" OwnerUserId="10416" PostTypeId="1" Score="0" Tags="&lt;chi-squared&gt;&lt;bias&gt;" Title="Biased variation of $\chi^2$ statistic?" ViewCount="86" />
  
  
  
  <row Body="&lt;p&gt;Let me start by saying that I think your first question and first R model are incompatible with each other.  In R, when we write a formula with either &lt;code&gt;-1&lt;/code&gt; or &lt;code&gt;+0&lt;/code&gt;, we are suppressing the intercept.  Thus, &lt;code&gt;lm(y ~ group + x:group - 1)&lt;/code&gt; &lt;em&gt;prevents&lt;/em&gt; you from being able to tell if the intercepts significantly differ from 0.  In the same vein, in your following two models, th &lt;code&gt;+1&lt;/code&gt; is superfluous, the intercept is automatically estimated in R.  I would advise you to use &lt;em&gt;reference cell coding&lt;/em&gt; (also called 'dummy coding') to represent your groups.  That is, with $g$ groups, create $g-1$ new variables, pick one group as the default and assign 0's to the units of that group in each of the new variables.  Then each new variable is used to represent membership in one of the other groups; units that fall within a given group are indicated with a 1 in the corresponding variable and 0's elsewhere.  When your coefficients are returned, if the intercept is 'significant', then your default group has a non-zero intercept. Unfortunately, the standard significance tests for the other groups will not tell you if they differ from 0, but rather if they differ from the default group.  To determine if they differ from 0, add their coefficients to the intercept and divide the sum by their standard errors to get their t-values.  The situation with the slopes will be similar:  That is, the test of $X$ will tell you if the default group's slope differs significantly from 0, and the interaction terms tell you if those groups' slopes differ from the default groups.  Tests for the slopes of the other groups against 0 can be constructed just as for the intercepts.  Even better would be to just fit a 'restricted' model without any of the group indicator variables or the interaction terms, and test this model against the full model with &lt;code&gt;anova()&lt;/code&gt;, which will tell you if your groups differ meaningfully at all.  &lt;/p&gt;&#10;&#10;&lt;p&gt;These things having been said, your main question is whether doing all of this is &lt;em&gt;acceptable&lt;/em&gt;.  The underlying issue here is &lt;a href=&quot;http://en.wikipedia.org/wiki/Multiple_comparisons&quot; rel=&quot;nofollow&quot;&gt;the problem of multiple comparisons&lt;/a&gt;.  This is a long-standing and thorny issue, with many opinions.  (You can find more information on this topic on CV by perusing the &lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/multiple-comparisons?sort=votes&amp;amp;pagesize=30&quot;&gt;questions tagged with this keyword&lt;/a&gt;.)  While opinions have certainly varied on this topic, I think no one would fault you for running many analyses over the same dataset provided the analyses were orthogonal.  Generally, &lt;a href=&quot;http://en.wikipedia.org/wiki/Contrast_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;orthogonal contrasts&lt;/a&gt; are thought about in the context of figuring out how to compare a set of $g$ groups &lt;em&gt;to each other&lt;/em&gt;, however, that is not the case here; your question is unusual (and, I think, interesting).  So far as I can see, if you simply wanted to partition your dataset into $g$ separate subsets and run a simple regression model on each that should be OK.  The more interesting question is whether the 'collapsed' analysis can be considered orthogonal to the set of individual analyses; I don't think so, because you should be able to recreate the collapsed analysis with a linear combination of the group analyses.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A slightly different question is whether doing this is really meaningful.  Image that you run an initial analysis and discover that the groups differ from each other in a substantively meaningful way; what sense does it make to put these divergent groups together into a discombobulated whole?  For example, imagine that the groups differ (somehow) on their intercepts, then, at least &lt;em&gt;some&lt;/em&gt; group &lt;em&gt;does not&lt;/em&gt; have a 0 intercept.  If there is only one such group, then the intercept for the whole will only be 0 if that group has $n_g=0$ in the relevant population.  Alternatively, lets say that there are exactly 2 groups with non-zero intercepts with one positive and one negative, then the whole will have a 0 intercept only if the $n$'s of these groups are in inverse proportion to the magnitudes of the intercepts' divergences.  I could go on here (there are many more possibilities), but the point is you are asking questions about how the groups sizes relate to the differences in parameter values.  &lt;em&gt;Frankly, these are weird questions to me.&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I would suggest you follow the protocol I outline above.  Namely, dummy code your groups.  Then fit a full model with all the dummies and interaction terms included.  Fit a reduced model without these terms, and perform a nested model test.  If the groups do differ somehow, follow up with (hopefully) a-priori (theoretically driven) orthogonal contrasts to better understand &lt;em&gt;how&lt;/em&gt; the groups differ.  (And plot--always, &lt;em&gt;always&lt;/em&gt; plot.)  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-09T06:55:42.383" Id="30124" LastActivityDate="2012-06-09T14:59:58.210" LastEditDate="2012-06-09T14:59:58.210" LastEditorUserId="7290" OwnerUserId="7290" ParentId="30035" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;SAS is important in the pharmaceutical industry but not necessarily in other disciplines.  in business and marketing time series analysis and survey sampling are particularly important.  Yes big pharma and business use SAS a lot but it is expensive and makes more economical sense with multiple users that you would find in big companies.  in the social sciences SPSS is more commonly used but is also expensive.  R is free and many advanced statistical procedures can be found in the CRAN libraries.&lt;/p&gt;&#10;" CommentCount="10" CommunityOwnedDate="2012-06-09T16:41:00.910" CreationDate="2012-06-09T11:16:49.113" Id="30132" LastActivityDate="2012-06-09T13:21:44.893" LastEditDate="2012-06-09T13:21:44.893" LastEditorUserId="11032" OwnerUserId="11032" ParentId="30129" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I imagine this would depend on the background of the individual.  Someone with a strong statisticsl background would probably have a lower threshold on what is a reasoanbly high percentage than somone who doesn't.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-09T11:25:10.737" Id="30133" LastActivityDate="2012-06-09T11:25:10.737" OwnerUserId="11032" ParentId="30126" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;There was a pretty funny paper in The American Statistician a few years ago: &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/diceRev2.pdf&quot;&gt;You can load a die, but you cannot bias a coin&lt;/a&gt;. As far as I can recall, they flipped beer bottle caps or some other obvious non-coins, still producing results close to 50-50.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the publication bias towards significant results, of any 1000 studies that tossed a coin 1,000,000 times, the 50 that found a significant difference from 0.50 will be published. Meta-analysis can uncover though that 25 would find a positive bias, and 25, a negative bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;Read about &lt;a href=&quot;http://yunwah.wordpress.com/2009/10/29/the-patron-saint-of-experimental-scientists/&quot;&gt;John Kerrich&lt;/a&gt; for the real reasons one would want to toss a coin for a few months.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a class activity, I had my undergrad students sand-paper a few cubes, roll them and prove to me, using Pearson $\chi^2$ test, that they indeed produced a biased die. For the time limits they had (50 to 100 rolls), you had to basically reduce one of the sides to a half to see significant results.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-09T15:10:23.843" Id="30142" LastActivityDate="2012-06-09T15:10:23.843" OwnerUserId="5739" ParentId="30116" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Your estimate $\hat{p}$ is equal to the Mann-Whitney $U$ statistic, and is therefore equivalent to the Wilcoxon rank-sum statistic $W$ (also known as the Wilcoxon-Mann-Whitney statistic):  $W = U + {n(n+1)\over{2}}$, where $n$ is the sample size of $y$ (assuming no ties.)  You can therefore use tables / software of the Wilcoxon test and transform them back to $U$ to get a confidence interval or a $p$-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $m$ be the sample size of $x$, $N$ = $m+n$.  Then, asymptotically, &lt;/p&gt;&#10;&#10;&lt;p&gt;$W^* = \frac{W-\frac{m(N+1)}{2}}{\sqrt{\frac{mn(N+1)}{12}}} \sim \text{N}(0,1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Source:  &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471190454&quot;&gt;Hollander and Wolfe&lt;/a&gt;, Nonparametric Statistical Methods, roughly p. 117, but probably most nonparametric statistics books will get you there.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-09T15:23:02.553" Id="30143" LastActivityDate="2012-06-09T15:23:02.553" OwnerUserId="7555" ParentId="30141" PostTypeId="2" Score="11" />
  
  
  <row Body="&lt;p&gt;Aside from the technical skills, like R or SAS, SQL will be important, and a few other higher level skills, including:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Data Manipulation&lt;/strong&gt;: To be able to analyse data, you will frequently have to spend some considerable time acquiring the data and manipulating it into a form which can be analysed.  Many statisticians will tell you that most of their time on a given project will be spent manipulating the data - so it is important to be good at this!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Understanding&lt;/strong&gt;: Many people vastly under estimate the amount of time that is required just to understand a complex dataset.  In bygone days one had to serve time apprenticed to a master crafts man, with a dataset you have to spend time looking at the various facets of the data and understanding it's dimensions and missing data and talking to people to try to understand the data.  Again you will spend considerable time doing this, it requires practice to build up this skill!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: Going hand in hand with the above is visualization.  Knowing how to plot data to help gain understanding is important. Later when you want to show somebody else what you have found in the dataset, a carefully created picture says a thousand words.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: One of the hardest to learn is requirements gathering.  Your customer will frequently not be clear what they want in their own head, and even less clear in what they say to you.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2012-06-09T16:41:00.910" CreationDate="2012-06-09T16:27:05.647" Id="30151" LastActivityDate="2012-06-09T16:27:05.647" OwnerUserId="6579" ParentId="30129" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Let $y_{ij}, {\boldsymbol x}_{ij}$ denote the response and predictor vector (respectively) of student $i$ in school $j$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(1)&lt;/strong&gt; For binary data, I think the standard way to do variance decompositions analogous to those done for continuous data is what the authors call &lt;strong&gt;Method D&lt;/strong&gt; (I'll comment on the other methods below) in your link - envisioning the binary data as arising from a underlying continuous variable that is governed by a linear model and decompose the variance on that latent scale. The reason is that logistic models (and other GLMs) naturally arises this way--  &lt;/p&gt;&#10;&#10;&lt;p&gt;To see this, define  $y^{\star}_{ij}$ such that it is governed by a linear mixed model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y^{\star}_{ij} = \alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_j + \varepsilon_{ij} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\alpha,\beta$ are regression coefficients, $\eta_j \sim N(0,\sigma^2)$ is the school level random effect and $\varepsilon_{ij}$ is the residual variance term and has a standard &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_distribution&quot;&gt;logistic distribution&lt;/a&gt;. Now let &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;y_{ij} = &#10;\begin{cases} 1 &amp;amp; \text{if} \ \ \ y^{\star}_{ij}≥0\\&#10;\\&#10;0 &amp;amp;\text{if} \ \ \ y^{\star}_{ij}&amp;lt;0&#10;\end{cases}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;let $p_{ij} = P(y_{ij} = 1|{\boldsymbol x}_{ij},\eta_j)$ now, simply using the logistic CDF we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p_{ij} = 1-P(y^{\star}_{ij}&amp;lt;0|{\boldsymbol x}_{ij},\eta_j) = \frac{ \exp \{-(\alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_j) \} }{1+ \exp \{-(\alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_j) \}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;now taking the &lt;a href=&quot;http://en.wikipedia.org/wiki/Logit&quot;&gt;logit transform&lt;/a&gt; of both sides, you have &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \log \left( \frac{ p_{ij} }{1 - p_{ij}} \right) = \alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_j $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is exactly the logistic mixed effects model. So, the logistic model is equivalent to the latent variable model specified above. One important note: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The scale of $\varepsilon_{ij}$ is not identified since, if you were to scale it down but a constant $s$, it would simply change the above to&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$$ \frac{ \exp \{-(\alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_j)/s \} }{1+ \exp \{-(\alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_j)/s \}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ \ \ \ \ \ \ $therefore the coefficients and random effects would simply be scaled up by the&lt;br&gt;&#10;$\ \ \ \ \ \ $ corresponding amount. So, $s=1$ is used, which implies ${\rm var}(\varepsilon_{ij}) = \pi^2/3$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you use this model and then the quantity&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{ \hat{\sigma}^{2}_{\eta} }{\hat{\sigma}^{2}_{\eta} + \pi^2/3 } $$&lt;/p&gt;&#10;&#10;&lt;p&gt;estimates the intraclass correlation of the &lt;strong&gt;underlying latent variables&lt;/strong&gt;. Another important note: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If $\varepsilon_{ij}$ is specified as, instead, having a standard normal distribution, then you have the mixed effects &lt;a href=&quot;http://en.wikipedia.org/wiki/Probit_model&quot;&gt;probit model&lt;/a&gt;. In that case $$ \frac{ \hat{\sigma}^{2}_{\eta} }{\hat{\sigma}^{2}_{\eta} + 1 } $$ estimates the &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Polychoric_correlation&quot;&gt;tetrachoric correlation&lt;/a&gt;&lt;/em&gt; between two randomly selected pupils in the same school, which were shown by Pearson (around 1900 I think) to be statistically identified when the underlying continuous data was normally distributed (this work actually showed these correlations were identified beyond the binary case to the multiple category case, where these correlations are termed &lt;a href=&quot;http://en.wikipedia.org/wiki/Polychoric_correlation&quot;&gt;&lt;em&gt;polychoric correlations&lt;/em&gt;&lt;/a&gt;). &lt;strong&gt;For this reason, it may be preferable (and would be my recommenation) to use a probit model when the primary interest is in estimating the (tetrachoric) intraclass correlation of binary data.&lt;/strong&gt; &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Regarding the other methods mentioned in the paper you linked: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;(A)&lt;/strong&gt; I've never seen the linearization method, but one drawback I can see is that there's no indication of the approximation error incurred by this. In addition, if you're going to linearize the model (through a potentially crude approximation), why not just use a linear model in the first place (e.g. option &lt;strong&gt;(C)&lt;/strong&gt;, which I'll get to in a minute)? It would also be more complicated to present since the ICC would depend on ${\boldsymbol x}_{ij}$. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;(B)&lt;/strong&gt; The simulation method is intuitively appealing to a statistician since it would give you an estimated variance decomposition on the original scale of the data but, depending on the audience, it may (i) be complicated to describe this in your &quot;methods&quot; section and (ii) may turn off a reviewer who was looking for something &quot;more standard&quot; &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;(C)&lt;/strong&gt; Pretending the data is continuous is probably not a great idea, although it won't perform terribly if most of the probabilities are not too close to 0 or 1. But, doing this would almost certainly raise a red flag to a reviewer so I'd stay away. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now finally, &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(2)&lt;/strong&gt; If the fixed effects are very different across years, then you're right to think that it could be difficult to compare the random effect variances across years, since they are potentially on different scales (this is related to the non-identifiability of scaling issue mentioned above). &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to keep the fixed effects over time (however, if you see them changing a lot over time, you may not want to do that) but look at the change in the random effect variance, you can explore this effect using some random slopes and dummy variables. For example, if you wanted to see if the ICCs were different in different years, you culd let $I_k = 1$ if the observation was made in year $k$ and 0 otherwise and then model your linear predictor as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_{1j} I_1 + \eta_{2j} I_2 + &#10;\eta_{3j} I_3 + \eta_{4j} I_4 + \eta_{5j} I_5+ \eta_{6j} I_6$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;this will give you a different ICCs each year but the same fixed effects. It may be tempting to just use a random slope in time, making your linear predictor &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha + {\boldsymbol x}_{ij} {\boldsymbol \beta} + \eta_{1} + \eta_{2} t $$&lt;/p&gt;&#10;&#10;&lt;p&gt;but I don't recommend this, since that will only allow your associations to &lt;em&gt;increase&lt;/em&gt; over time, not &lt;em&gt;decrease&lt;/em&gt;. &lt;/p&gt;&#10;" CommentCount="19" CreationDate="2012-06-09T18:40:03.750" Id="30153" LastActivityDate="2012-06-13T15:48:18.353" LastEditDate="2012-06-13T15:48:18.353" LastEditorUserId="4856" OwnerUserId="4856" ParentId="29986" PostTypeId="2" Score="14" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have time series data recorded at multiple locations, stored in a matrix $Y$.  I have fit a Vector Autoregressive Model to it which forecasts the data pretty well on a test set.  However, if I plot the residuals there is definitely some time series structure left; ideally the residuals would look like noise.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have been learning about boosted trees (package 'gbm' in R) that fits a response vector $Y$ to some input features $X$, pulls the residuals $E = Y - f(X)$, and then fits $E$ to the same input features $X$ again (repeat until the process starts to overfit).  It's my belief that a tree won't be able to model time series structure well, so I am wondering if the same process can be done using an AR model as the weak learner.&lt;/p&gt;&#10;&#10;&lt;p&gt;After fitting my VAR model, I take the residuals and fit an AR process to it, and forecast it using the 'forecast' package in R.  I then take my VAR forecasts and add them to my AR forecasts to get a final prediction.  The error at some locations decrease, but for most locations the error increases.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Below are the plots of the residuals having only fit the VAR model - there is definitely some structure that can still be explained using some model. Any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&#10;The time series represent energy consumption at various locations.  During the night we expect energy consumption to be very low and flat, during the day there may be high volatility so the variance is definitely not constant.  My first step was to remove the daily seasonal component using STL (Seasonal Trend Decomposition Using Loess).  After that, I fit my VAR model, and subsequently another AR model to the residuals.  The time series after removing the Seasonal Component look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qJlPA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The error terms from the VAR look like&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VYttV.png&quot; alt=&quot;VAR Errors&quot;&gt; &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-10T09:20:46.547" FavoriteCount="1" Id="30171" LastActivityDate="2013-12-12T10:39:38.860" LastEditDate="2012-06-11T05:28:35.777" LastEditorUserId="8067" OwnerUserId="8067" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;autoregressive&gt;" Title="Boosted AR for time series forecasting?" ViewCount="415" />
  <row Body="&lt;p&gt;Well, &quot;aimlessly trying different values&quot; is of course not a very good strategy. The primary thing to optimize (from my experience), would be the number of hidden layers, the number of hidden neurons in the hidden layers, and the learning rate. I would try to vary one parameter at the time, and then plot the test-error and the training error, to see which learning rate makes the network converge in the best way.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; As mentioned by bayerj, there are multiple misleading statements in this answer. The first thing is plotting the test-error. You should of course use a training/validation/test-split (often 80%, 10%, 10%) on your data, and then plot your validation error - not the test error. I left this out in my original answer since it was supposed to be a comment.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other thing, is that I said that &quot;aimlessly trying different values&quot; was a bad strategy. What I meant here, was not that a randomized strategy for optimizing performance was a bad idea, but that is not how I read &quot;aimlessly trying different values&quot;. If you implement a simple randomized search for good parameter-values, the article provided in the answer by bayerj shows, that this performs very well.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-10T09:32:58.570" Id="30175" LastActivityDate="2012-06-10T12:15:53.137" LastEditDate="2012-06-10T12:15:53.137" LastEditorUserId="7339" OwnerDisplayName="utdiscant" OwnerUserId="7339" ParentId="30174" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;A few notes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If you look at your data you obtain:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; str(data)&#10;'data.frame':   40 obs. of  5 variables:&#10;$ id    : Factor w/ 20 levels &quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 1 12 14 15 16 17 18 19 20 2 ...&#10;$ sess  : Factor w/ 2 levels &quot;a&quot;,&quot;b&quot;: 1 2 2 1 1 2 2 1 1 2 ...&#10;$ treat : Factor w/ 2 levels &quot;drug&quot;,&quot;placebo&quot;: 1 1 1 1 1 1 1 1 1 1 ...&#10;$ memory: Factor w/ 20 levels &quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 2 12 17 18 1 20 19 15 14 16 ...&#10;$ blood : Factor w/ 20 levels &quot;1&quot;,&quot;100&quot;,&quot;17&quot;,..: 3 11 17 18 20 19 9 10 6 16 ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The dependent variable &lt;code&gt;memory&lt;/code&gt; and the covariate &lt;code&gt;blood&lt;/code&gt; shouldn't be factors, but numeric:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; data$memory &amp;lt;- as.numeric(data$memory)&#10;&amp;gt; data$blood &amp;lt;- as.numeric(data$blood)&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am no expert for regression designs, but why do you add the covariate and all it's interactions? Wouldn't it be sufficient to have something like &lt;code&gt;memory ~ sess*treat + blood&lt;/code&gt;? I think everything on top of this needs justification.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The random effect needs to be stratified for both within-subjects factor &lt;code&gt;sess&lt;/code&gt; and &lt;code&gt;treat&lt;/code&gt; something &lt;a href=&quot;http://stats.stackexchange.com/q/13784/442&quot;&gt;that is cumbersome in &lt;code&gt;lme&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;lme4&lt;/code&gt; is preferable (for the cost of loosing straight forward p-values, but you can still get them via bootstrap). Something between the following extremes should work:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(memory ~ sess*treat + blood + (1|id), data = data)&#10;lmer(memory ~ sess*treat + blood + (1+sess*treat|id), data = data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Have a look at the paper from Baayen and Milin (2010) for different random effects structures. And to obtain p-values from a &lt;code&gt;lmer&lt;/code&gt; model you can use the &lt;code&gt;pvals.fnc&lt;/code&gt; function from the &lt;code&gt;languageR&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Baayen, R. H., &amp;amp; Milin, P. (2010). Analyzing Reaction Times. &lt;em&gt;International Journal of Psychological Research&lt;/em&gt;, 3(2), 12–28.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-10T18:56:29.460" Id="30197" LastActivityDate="2012-06-10T18:56:29.460" OwnerUserId="442" ParentId="30190" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;&quot;Generalization&quot; refers to the ability of a classifier to correctly classify instances that it has not yet seen as part of its training and is always a desirable feature. The opposite of generalization would be over-fitting, which you always want to avoid (Although a classifier might also not generalize well if the training domain is different from the domain it will finally be applied to, which might not be due to over-fitting in the strictest sense).&lt;/p&gt;&#10;&#10;&lt;p&gt;The terms recall and precision have very specific meanings, while your use of &quot;general&quot; and &quot;specific&quot; are not standard terminology I don't think. For binary classifiers, it is customary to refer to one class of instances as &quot;positive&quot; and the other as &quot;negative&quot;. For example, if you are trying to build a classifier that recognizes pictures with faces in them, then you might define the class of all pictures that contain faces as the positive class. The term recall is then defined as the proportion of all positive instances that were classified correctly, while precision is the proportion of all instances classified as positive that were actually positive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example. Lets say of $40$ pictures, $30$ contain faces. If a classifier classifies $25$ pictures as positive, of which $20$ actually do contain faces, then the recall is $\frac{20}{30}=2/3$ while precision is $\frac{20}{25}=0.8$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-11T05:35:02.450" Id="30227" LastActivityDate="2012-06-11T05:35:02.450" OwnerUserId="11195" ParentId="30221" PostTypeId="2" Score="6" />
  
  
  
  
  
  
  
  <row AcceptedAnswerId="30288" AnswerCount="1" Body="&lt;p&gt;I'm checking the conversion rates on some segments of a web application as to figure out which types of customers we should invest in paid acquisition (theory is that these segments see more value in the product and, as such, are willing to pay).&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, on one segment there are 336 users of which 88 converted to paid. This gives a ~26% conversion rate for the segment but how can I have a high degree of confidence that the data is significant enough as to avoid burning precious resources chasing this segment?&lt;/p&gt;&#10;&#10;&lt;p&gt;TIA&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-12T01:00:47.540" FavoriteCount="1" Id="30282" LastActivityDate="2012-06-12T16:10:22.090" OwnerUserId="11928" PostTypeId="1" Score="2" Tags="&lt;estimation&gt;" Title="Calculating confidence for a web app conversion rate" ViewCount="269" />
  <row Body="&lt;p&gt;From a statistical point of view, each user can be thought of as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Bernoulli_trial&quot; rel=&quot;nofollow&quot;&gt;Bernoulli trial&lt;/a&gt; for simplicity.  (There are other ways of thinking about this, but I suspect this will be good enough for your purposes, and won't require going too far into more advanced statistics.)  The success rate achieved over multiple Bernoulli trials is distributed as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_distribution&quot; rel=&quot;nofollow&quot;&gt;binomial&lt;/a&gt; with probability $p$ (in your case 26%), and number of trials $N$ (for you 336).  How much the observed proportion will bounce around can be assessed by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_error&quot; rel=&quot;nofollow&quot;&gt;standard error&lt;/a&gt; of the proportion, which is:&lt;br&gt;&#10;$$&#10;SE_{prop}=\sqrt{\frac{p(1-p)}{N}}&#10;$$ &#10;The &lt;a href=&quot;http://stats.stackexchange.com/questions/3734/what-intuitive-explanation-is-there-for-the-central-limit-theorem&quot;&gt;Central Limit Theorem&lt;/a&gt; should assure us that the distribution of sample proportions will be approximately normal, given that your $N$ is so large.  Thus, you could calculate a decent first approximation of the 95% &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval&quot; rel=&quot;nofollow&quot;&gt;confidence interval&lt;/a&gt; by simply multiplying your SE by 1.96 and adding (and subtracting) the product to your observed proportion.  &lt;/p&gt;&#10;&#10;&lt;p&gt;From there, you need to think about how much this will cost the company and how much revenue will be generated.  HTH, cheers.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;  What will likely happen as the number of users goes from 336 to 336,000?  &lt;/p&gt;&#10;&#10;&lt;p&gt;If 26% really is the true underlying probability, then the sample proportion will bounce around less and less far from that percentage in larger samples.  Note however, that given your current data, a range of values are likely, so it may converge on a higher or lower number than 26%.  That is what the 95% confidence interval tells you--the range of plausible values.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Note also that this is just a model: for example, you are assuming that every user has the same probability of converting into a paid customer, which is certainly not true, but probably a good enough approximation anyway.  Imagine a more sophisticated model with a mix of groups of users, with the groups constituting different proportions of the total, and with each group having a different probability of converting.  It's easy to see how this can amount to the same thing as your current model when you are only working with the aggregate.  Models necessarily are simplifications, and thus are never veridical, as Box famously put it, &quot;All models are wrong, but some are useful&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;When you use the model to plan future actions, you are further assuming that nothing that will change over that time (e.g., the policy changes you implement, or other changes in the larger world) will not influence the underlying probability.  This is also false.  As they say, making predictions is hard, especially about the future.  All you can do is use the best information available to you in the optimal way; the fact that there's always more information that you don't have and that your information is imperfect doesn't change that.  These considerations illustrate how the model &lt;em&gt;helps&lt;/em&gt; you think about what you may want to do, but cannot do your thinking for you.  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-12T02:43:14.167" Id="30288" LastActivityDate="2012-06-12T16:10:22.090" LastEditDate="2012-06-12T16:10:22.090" LastEditorUserId="7290" OwnerUserId="7290" ParentId="30282" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;em&gt;I'm aware that this is a 'homework' question but it has gone unanswered for over six months so I figure the homework has been turned in by now. Also, the hints in the comments (which I use directly here) are useful until you get to the point where you have to calculate the expected value of the sample standard deviation (a non-trivial exercise), which I give a link to in this answer.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a sample $X_1, X_2, ..., X_n$ from a $N(\mu,\sigma^2)$ population, &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_distribution#Estimation_of_parameters&quot; rel=&quot;nofollow&quot;&gt;the $95 \%$ confidence interval for $\mu$ when the variance $\sigma^2$ is unknown is&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \overline{X} \pm t_{n-1} \cdot \frac{s}{\sqrt{n}} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$ and $ s = \sqrt{ \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2}$ and $t_{n-1}$ is the $97.5$th quantile of the $t$-distribution with $n-1$ degrees of freedom. From the expression for the confidence interval, we can see its width is the random variable $$W = 2t_{n-1} \cdot \frac{s}{\sqrt{n}}$$ The only random part of $W$ is $s$, therefore the expected width is&#10;$ E(W) = \frac{2t_{n-1}}{\sqrt{n}} \cdot E(s) $, which reduces the problem to that of calculating $E(s)$ (which is calculated in &lt;a href=&quot;http://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma/27984#27984&quot;&gt;this thread&lt;/a&gt;) and doing some algebra: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E(W) = \sigma  \cdot t_{n-1} \cdot \sqrt{ \frac{8}{n(n-1)} } \cdot \frac{ \Gamma(n/2) }{\Gamma( \frac{n-1}{2} ) } $$&lt;/p&gt;&#10;&#10;&lt;p&gt;From that formula, we can plug in $n=9$ and find that $ E(W) \approx 1.49 \sigma $. We can also plot $E(W)$ as a function of $n$: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\ \ \ \ \ \ \ \ \ $&lt;img src=&quot;http://i.stack.imgur.com/A4D0r.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-12T03:52:34.393" Id="30292" LastActivityDate="2012-06-12T12:14:48.330" LastEditDate="2012-06-12T12:14:48.330" LastEditorUserId="4856" OwnerUserId="4856" ParentId="18379" PostTypeId="2" Score="7" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have been reading a lot on this site and I think I am going to analyze my data using a mixed model ANOVA- random effect= subject and 2 fixed effects- comparison and pricing. All of my subjects were in all 4 conditions (a cross between the 2 factors) and answered 3 items in each condition. The catch is that my DV is dichotomous. I read that it is not a good idea to assign a mean (i.e. proportion in this case) of the 3 items for each subject and then do the ANOVA using that. Does this mean that I will have number of subjects * 3* 4 observations?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-12T09:42:40.783" FavoriteCount="1" Id="30300" LastActivityDate="2012-06-13T06:57:36.400" OwnerUserId="11891" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;binary&gt;" Title="Dichotomous DV and 3 points of measurement per condition within each subject" ViewCount="133" />
  <row AnswerCount="2" Body="&lt;p&gt;I built a neural net in Octave to predict if products are a match or not using a number of features. The net has 8 input features, 1 hidden layer with 8 nodes and outputs either match or no match. I employ feature regualarization and use &lt;code&gt;fmuncg&lt;/code&gt; to perform gradient decend.&lt;/p&gt;&#10;&#10;&lt;p&gt;I trained it with 30,000 examples with +/-10% of those training sets being positve matches.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my problem:&#10;The precision on the net is excellent, around 92%.&#10;Recall is very low at less than 10%. Playing around lambda in the regularization is not helping much, nor is performing more iterations in &lt;code&gt;fmuncg&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I dont want to balance the training set because that would reduce my data a lot. Is there anything else that I can do?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is my cost function:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;J = (-1/m)*sum(sum(y1.*log(r)+(1-y1).*log(1-r)))+ (lambda/(2*m))*((sum(sum(nTheta1.*nTheta1))+sum(sum(nTheta2.*nTheta2))));&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is there a way that I can change it, to penalise false negatives more?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-12T14:32:29.737" Id="30313" LastActivityDate="2012-10-24T12:20:38.463" LastEditDate="2012-06-25T01:59:42.513" LastEditorUserId="3826" OwnerUserId="11940" PostTypeId="1" Score="3" Tags="&lt;neural-networks&gt;&lt;precision-recall&gt;" Title="Improving recall in a neural network" ViewCount="426" />
  <row AnswerCount="3" Body="&lt;p&gt;I have a scenario where both factors are within-subject. One is time (5 levels) and one is condition (2 levels). The time point of 0 was only measured once in each subject, and is a 'resting value' that is to be used as the time point of 0 for both conditions. I cannot find a way of dealing with this accept for entering in the value twice into the data set as t=0 for both conditions. I feel like this is not a valid approach but I don't know what else there is to do. I could run two separate one way ANOVAs but then I couldn't see an interaction between the two conditions. I use SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: Thanks for the responses. The nature of the methodology/study design includes assuming rest is the same for both conditions (to reduce tissue sampling required). Therefore, entering the rest measures twice in the two-way ANOVA is valid and is the nature of the methodology in this case, albeit a limitation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-12T18:28:56.447" Id="30329" LastActivityDate="2012-06-14T10:05:51.567" LastEditDate="2012-06-13T20:43:16.403" LastEditorUserId="11949" OwnerUserId="11949" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="Not sure how to carry out this two-way repeated measures ANOVA" ViewCount="354" />
  <row Body="&lt;p&gt;No matter what model you use very rare events are a problem because you may never see them in your data or if you do you will not see many unless you look for events over a very long time period.  I don't think it is ever a good idea to just conjure up a model that will give some answer that may be accurate when the model is correct but could be sensitive to departures from the model.  For very rare events the model assumptions may be difficult or impossible to check.  It would help if there is a physical basis for the choice of the form of the point process.  Maybe a Poisson model would be appropriate or maybe you have apriori reason to believe there is overdispersion in which case a negative binomial might be more appropriate.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-12T22:46:27.963" Id="30342" LastActivityDate="2012-06-12T22:46:27.963" OwnerUserId="11032" ParentId="30272" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You might want to consider using LibLINEAR instead of (Lib)SVM. It is said to run faster than SVM for cases like document classification, though I'm not sure how it effects memory usage (&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot; rel=&quot;nofollow&quot;&gt;see&lt;/a&gt; section 'When to use LIBLINEAR but not LIBSVM'). &lt;a href=&quot;http://cran.r-project.org/web/packages/LiblineaR/&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is the package for R.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-13T11:51:51.933" Id="30371" LastActivityDate="2012-06-13T11:51:51.933" OwnerUserId="11812" ParentId="30358" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;After clarification in a comment discussion, it appears that the question is about a situation where there is a linear model relating a continuous response variable $Y_i$ and a binary predictor $X_i \in \{0,1\}$, and the OP wants to know whether or not it is defensible to interpolate by plugging in values for $X_i$ that are $\in (0,1)$ and assuming everything is &quot;OK&quot;. That answer is that this is only defensible &lt;strong&gt;if you assume linearity of $E(Y_i | X_i)$ over that interval&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;To see why this is true, consider fitting these data with a simple regression model: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\varepsilon_i$ is a mean $0$ error term. Under this model, $E(Y_i | X_i = 1) = \beta_0 + \beta_1$ and $E(Y_i | X_i = 0) = \beta_0$. When you only use data such that each $X_i \in \{0,1\}$ to fit this model, the coefficient estimates will be exactly&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat \beta_0 = \overline{Y_0}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat \beta_1 = \overline{Y_1} - \overline{Y_0}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\overline{Y_0}$ is the sample mean of the $Y_i$'s when $X_i = 0$ and $\overline{Y_1}$ is the sample mean of the $Y_i$'s when $X_i = 1$. Therefore, the estimated mean for any intermediate value of $X_i$, call it $x$, is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \overline{Y_0} + (\overline{Y_1} - \overline{Y_0})x = \overline{Y_0}(1-x) + \overline{Y_1}x$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, &lt;strong&gt;a function which linearly goes from $\overline{Y_0}$ to $\overline{Y_1}$ as $x$ moves from $0$ to $1$&lt;/strong&gt;. If the true relationship is not this way, then the predictions made will be very wrong. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, suppose the true relationship was a step function: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(Y_i | X_i) = \beta_0 + \beta_1 \mathcal{I}(X_i \geq 1/2) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathcal{I}(X_i \geq 1/2) = 1$ if $X_i \geq 1/2$ and 0 otherwise. This would be compatible with the linear model specified if all you observe are $X_i = 0,1$ but once you start to interpolate that linear relationship, you will get very wrong predictions. At $X_i = 1/2$, the true mean is $\beta_0 + \beta_1$ but the linear model's estimate would be $\beta_0 + \beta_1/2$ (Yikes!). Other examples can be given where the linear model is correct for the binary data, but makes this disagreement arbitrarily large once you interpolate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-13T14:40:09.623" Id="30384" LastActivityDate="2012-06-14T01:43:02.883" LastEditDate="2012-06-14T01:43:02.883" LastEditorUserId="4856" OwnerUserId="4856" ParentId="30339" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;In your situation, I think the appropriate null hypothesis would be $\mu$=0 or $\mu=k$&lt;/p&gt;&#10;&#10;&lt;p&gt;The test statistic would be $t$=${\bar X-\mu }\over{s/\sqrt n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me summarize here, Given the population distribution is normal,&lt;/p&gt;&#10;&#10;&lt;p&gt;when you have only one group, you can test for the mean or/and the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Test for the population mean will be different according to whether you know the population variance or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Test for the population variance will use Chi-squared distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Test for the population mean and variance would use joint distribution of sample mean and variance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-13T17:19:55.177" Id="30393" LastActivityDate="2012-06-13T17:19:55.177" OwnerUserId="11242" ParentId="30390" PostTypeId="2" Score="-1" />
  <row AcceptedAnswerId="30450" AnswerCount="4" Body="&lt;p&gt;Let's say we have the statistics given below&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gender mean sd n&#10;f 1.666667 0.5773503 3&#10;m 4.500000 0.5773503 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do you perform a two-sample t-test (to see if there is a significant difference between the means of men and women in some variable) using statistics like this rather than actual data?&lt;/p&gt;&#10;&#10;&lt;p&gt;I couldn't find anywhere on the internet how to do this. Most of the tutorials and even the manual deal with the test with the actual data set only.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-13T16:15:52.413" FavoriteCount="4" Id="30394" LastActivityDate="2013-07-28T15:05:04.213" LastEditDate="2013-07-28T15:02:01.530" LastEditorUserId="22047" OwnerDisplayName="Alby" OwnerUserId="11013" PostTypeId="1" Score="9" Tags="&lt;r&gt;&lt;t-test&gt;" Title="How to perform two-sample t-tests in R by inputting sample statistics rather than the raw data?" ViewCount="5008" />
  
  
  <row Body="&lt;p&gt;How many trials before getting 200 successes, that's actually the definition of the negative binomial. In &lt;code&gt;R&lt;/code&gt; you calculate this with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;200 + qnbinom(0.99, 200, 0.4)&#10;[1] 567&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A negative binomial density (R: dnbinom) gives you the prob that a certain number x of failures will be necessary to obtain 200 successes. Sum that up to get the CDF (R: pnbinom), i.e., the probability that at most x failures will get you the 200 successes. Conversely, the quantile function tells you that with probability 99%, 367 failures will suffice to get you the 200 successes. Said otherwise, there's (roughly) 1% chance that more than 367 failures (i.e., 567 trials) will be necessary to get your 200 successes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Chap&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-14T02:20:10.940" Id="30426" LastActivityDate="2012-06-14T12:09:28.903" LastEditDate="2012-06-14T12:09:28.903" LastEditorUserId="11976" OwnerUserId="11976" ParentId="29268" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;In the absence of a particular desired form of prior, often people use conjugate priors; the conjugate prior for $\sigma^2$ in the normal would be inverse gamma. You can choose anything from highly uninformative (an improper prior like $1/\sigma^2$ is often used and is a limiting case of the Inverse Gamma), through mildly informative, to a fully informative prior (such as one based on a previous study).&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe the Jeffreys prior is $1/\sigma^2$. The nice thing about Jeffreys priors is they don't depend on your parameterization (if you transform the parameter, the Jeffreys prior 'follows' it so that everything corresponds).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Inverse-gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Inverse-gamma_distribution&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-14T03:19:16.790" Id="30427" LastActivityDate="2013-11-16T21:06:26.060" LastEditDate="2013-11-16T21:06:26.060" LastEditorUserId="17230" OwnerUserId="805" ParentId="30369" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I like Amyunimus's answer.  But as an alternative, how about fitting the model for t = 1 through 4 and both conditions, then conducting post-hoc tests for each of the fitted values against the value of the response variable at t=0?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-14T10:05:51.567" Id="30439" LastActivityDate="2012-06-14T10:05:51.567" OwnerUserId="266" ParentId="30329" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="30452" AnswerCount="1" Body="&lt;p&gt;This is more of a data management question.&lt;/p&gt;&#10;&#10;&lt;p&gt;When merging several waves of a survey where the same questions were asked in multiple years -- but each time of a new sample -- should the data from a question be merged into the same variable or separate variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;Since each wave was administered to a new sample, the unique (person) identifier would be new for each wave and there is no risk of analysing answers from multiple waves as though it were a panel dataset. But might using a single variable (say q1 rather than separate w1_q1 w2_q1 w3_q1) be confusing or helpful?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-14T11:46:40.013" Id="30447" LastActivityDate="2012-06-14T14:26:09.247" OwnerUserId="6654" PostTypeId="1" Score="4" Tags="&lt;dataset&gt;&lt;data-management&gt;" Title="Best practice when merging waves from non-panel survey" ViewCount="92" />
  <row Body="&lt;p&gt;It is much easier to do these evidence combination operations in the natural parametrization, which for the multivariate normal distribution with mean $\mu$ and covariance matrix $\Sigma$ is&#10;$$\begin{bmatrix}\Sigma^{-1}\mu \\ \Sigma^{-1}\end{bmatrix}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Convert $p$ and $q$ so that they're in this parametrization, then pad $q$ with zeroes for the extra component (since the precision there is zero) and just add the natural parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, the resulting distribution is:&#10;\begin{align}&#10;y \mid D \sim \mathcal N\left([\Psi\Phi\mu]_1, \Psi_{11}\right)&#10;\end{align}&#10;where&#10;\begin{align}&#10;\Theta &amp;amp;= \begin{bmatrix} k&amp;amp; \mathbf{v} \\ \mathbf{v}^T &amp;amp; K\end{bmatrix}^{-1} \\&#10;\Phi &amp;amp;= \begin{bmatrix} 0&amp;amp; \mathbf{0} \\ \mathbf{0}^T &amp;amp; \sigma^{-2}I\end{bmatrix} \\&#10;\Psi &amp;amp;=(\Theta + \Phi)^{-1}.&#10;\end{align}&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-14T16:50:05.653" Id="30464" LastActivityDate="2012-06-14T17:00:10.243" LastEditDate="2012-06-14T17:00:10.243" LastEditorUserId="858" OwnerUserId="858" ParentId="30457" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The partial eta squared indicates that gender explains 5% of the variance in drugs and ethnicity explains 6% of the variance in drugs. These are small effect sizes.&#10;I am not sure I understand this because when I look at the ANOVA table produced by SPSS it says that the Partial ETA squared for gender is .045 and for ethnicity is .060. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are these figures just rounded up and moved over a few decimal places? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-14T20:29:59.297" Id="30478" LastActivityDate="2012-06-15T04:08:12.240" LastEditDate="2012-06-15T04:08:12.240" LastEditorUserId="183" OwnerUserId="11995" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;spss&gt;" Title="How can partial eta squared be 5% and yet SPSS says 0.045?" ViewCount="202" />
  <row Body="&lt;p&gt;The correlation coefficient between two variables X and Y is just Cov(X,Y)/[√Var(X)√Var(Y)]&#10;and &lt;/p&gt;&#10;&#10;&lt;p&gt;Cov(X,Y) = E[(X-m$_1$)(Y-m$_2$)] where m$_1$ and m$_2$ are the respective means for X and Y. Given paired observations (X$_i$,Y$_i$) for i=1,2,..,n &lt;/p&gt;&#10;&#10;&lt;p&gt;Cov(X,Y) is estimated by &lt;/p&gt;&#10;&#10;&lt;p&gt;∑ (X$_i$-m$_1$$_b$) (Yi-m$_2$$_b$)/n where m$_1$$_b$ = ∑X$_i$/n  and m$_2$$_b$=∑Y$_i$/n.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The estimate for Var(X) is usually ∑ (X$_i$-m$_1$$_b$)$^2$/(n-1) and the estimate of &lt;/p&gt;&#10;&#10;&lt;p&gt;Var(Y) = ∑ (Y$_i$-m$_2$$_b$)$^2$/(n-1).  &lt;/p&gt;&#10;&#10;&lt;p&gt;This tells you how to calculate the correlation coefficient.  So you could write your own R code to do this.  But first you need to know which Y goes with X.  So you need to have the data paired.  It seems that you logically would pair based on taking them from the same year.  So for example the correlation coefficient for the Congo would be&#10;the estimated covariance: &lt;/p&gt;&#10;&#10;&lt;p&gt;{(1200-1007)(900-616.7) + (1146-1007)(400-616.7) + (675-1007) (550-616.7)}/3  divided by&lt;/p&gt;&#10;&#10;&lt;p&gt;[√{((1200-1007)$^2$ + (1146-1007)$^2$ +(675-1007)$^2$)/2} √{(900-616.7)$^2$ + (400-616.7)$^2$ +(550-616.7)$^2$}/2 &lt;/p&gt;&#10;&#10;&lt;p&gt;The numbers 1007 and 616.7 appear in the formula because they represent m$_1$$_b$ and m$_2$$_b$ respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given Bill Huber's display of the data it is clear that the Congo does not follow the regression line that seems to fit well to the other countries because of one outlier in 1984.  It is so extreme that it is an obvious problem based on the scatter plot.  This may mean that there must be some strange reason why the high smoking rate in the Congo in 1984 does not lead to a high incidence of heart attacks in 1984 or that there is a recording error.  Both possibilities should be looked into.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the other two points we see a low rate of heart attacks in 2010 and a corresponding reduction in smoking over the high rates in the 1980s and a high rate of heart attacks in 1988 associated with a high rate of smoking.  This leads me to conjecture that either (1) heart attack awarness was not great in 1984 and so cases went unreported and that increased awareness between 1984 and 1988 led to better reporting and a higher rate.  It seems this awareness may have led to decline in smoking in the Congo by 2010, or (2) a correct and consistent number of heart attacks occurred in 1984 but there was a data entry error on that number or (3) the much less plausible explanation that the low heart attack rate was correct but the high smoking rate was a recording error in 1984.  I think (3) is doubtful because the smoking rate in 1988 is close to the recorded value for 1984 and it seems less credible that smoking rates would go up dramatically between 1984 and 1988. Nevertheless these three scenarios could explain the problem and there may be other plausible explanations that ideally should be investigated.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is important to recognize that the 1984 outlier should not be ignored.  Points like this tend to dramatically effect the correlation estimate (lowering it) as well as the variances (increasing them in this cas).  In this case the outlier is noticeable without looking at multivariate outlier detection methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my 1982 paper which came out of this ORNL technical report &lt;a href=&quot;http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=6243468&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; which I have referenced on stackexchange &lt;a href=&quot;http://stats.stackexchange.com/questions/32941/examples-of-lurking-variable-and-influential-observation&quot;&gt;Here&lt;/a&gt; shows how to calculate the influence function for the sample correlation at points in the (x,y) plane.  In less obvious cases this could be helpful in identifying such outliers.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-14T21:09:05.250" Id="30482" LastActivityDate="2012-09-14T15:49:10.483" LastEditDate="2012-09-14T15:49:10.483" LastEditorUserId="11032" OwnerUserId="11032" ParentId="30461" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There are a couple of ways you can do this.  The first would be to use the mean of the posterior for each of the $\mu_i$, and calculate a residual using this as the &quot;estimated value&quot; corresponding to $\hat{\beta}X$ in OLS.  You then calculate the variance of the residuals as usual and plug it into the $R^2$ calculation.  You would do this in R, of course.  An alternative would be to use the posterior mean of the variance ($1/\tau$) as the estimate of residual variance in the $R^2$ calculation, again done in R.  The former comes closest to how $R^2$ is calculated in classical statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;No doubt there are other approaches, which (hopefully) others will point out in their answers.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the bigger issues are a) with $R^2$ as a criterion and b) with comparison of OLS estimation to anything else using $R^2$ as a criterion.  I'll skip over the first, pointing you to &lt;a href=&quot;http://www.statisticalengineering.com/r-squared.htm&quot; rel=&quot;nofollow&quot;&gt;statisticalengineering.com&lt;/a&gt; and &lt;a href=&quot;http://andrewgelman.com/2007/08/rsquared_useful/&quot; rel=&quot;nofollow&quot;&gt;Andrew Gelman&lt;/a&gt; as references.  The second issue arises because OLS maximizes $R^2$ (a consequence of the &quot;least squares&quot; property) and therefore no other technique (that is not equivalent to OLS) will generate as high an $R^2$.  Consequently, your Bayesian approach is doomed if maximizing $R^2$ is the criterion of choice.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You might be able to suggest a more out-of-sample criterion instead, for example, a k-fold &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;cross-validation&lt;/a&gt;, which would necessitate multiple runs of JAGS on subsets of the data, then comparing the out-of-sample predicted values to the actual values.  You can generate the predicted values inside JAGS as observed in the answer to &lt;a href=&quot;http://stats.stackexchange.com/questions/22184/missing-values-in-response-variable-in-jags&quot;&gt;Missing values in response variable in JAGS&lt;/a&gt;, or in R of course.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll also point out that the &lt;code&gt;dgamma(0.01,0.01)&lt;/code&gt; distribution has largely fallen out of favor, as it is actually quite informative near zero.  The answers to &lt;a href=&quot;http://stats.stackexchange.com/questions/30369/priors-for-log-normal-models/30381#30381&quot;&gt;priors for lognormal models&lt;/a&gt; might help with that.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-14T21:47:22.423" Id="30485" LastActivityDate="2012-06-15T16:03:01.430" LastEditDate="2012-06-15T16:03:01.430" LastEditorUserId="7555" OwnerUserId="7555" ParentId="30377" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;It's a bit hard to get a p-value below 5% when each sample represents 16.7% of the data ! But even with a very large sample size, there's no such thing as a &quot;universal&quot; acceptable error rate that would be suitable for all applications. The expected MSE of an estimator can be decomposed as bias^2 + variance + noise. So even a &quot;perfect&quot; learning machine will not allow you to get rid of the noise term which is application dependent. Intuitively, noise comes from the fact that the underlying data generating process is non-deterministic, i.e., $y = f(x) + \epsilon$, i.e., you may get different values of y (the target) for two samples with the exact same x (vector of inputs). The best (in MSE terms) predictor will be    $\hat{y} = f(x)$ with error equal to $Var(\epsilon)$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-15T00:56:00.007" Id="30488" LastActivityDate="2012-06-15T00:56:00.007" OwnerUserId="11976" ParentId="30487" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="30494" AnswerCount="1" Body="&lt;p&gt;There are 26 participants who have participated in my research. Each participant treated with a lab module (Hands on Robotics Session). Now each participant will be evaluated using a rubric on scale of Excellent, Good, Fair and Poor.&lt;/p&gt;&#10;&#10;&lt;p&gt;For my research i want to evaluate the following questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 1&lt;/p&gt;&#10;&#10;&lt;p&gt;Null Hypothesis: Students learn about computational thinking (programming basics and algorithmic thinking) with the help of robotics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternate Hypothesis: students do not learn about computational thinking (programming basics and algorithmic thinking) with the help of robotics.&lt;/p&gt;&#10;&#10;&lt;p&gt;To evaluate the above question, the categories i will be considering are Plan, Implementation, Testing and Precision on a scale of Excellent, Good, Fair and Poor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 2:&lt;/p&gt;&#10;&#10;&lt;p&gt;Null Hypothesis: Participants effectively translate the given scenario to a flowchart&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternate Hypothesis: Participants do not effectively translate the given scenario to a flowchart&lt;/p&gt;&#10;&#10;&lt;p&gt;I will also evaluate the question 2 on a scale of Excellent, Good, Fair and Poor.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am planning to use CHI SQUARE to test the above two hypothesis. Is CHI SQUARE a feasible option? Is Yes what should i consider for rows and columns of chi square.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-15T01:43:32.060" Id="30490" LastActivityDate="2012-06-15T08:29:08.263" LastEditDate="2012-06-15T08:29:08.263" LastEditorUserId="88" OwnerUserId="12001" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;chi-squared&gt;" Title="Statistical analysis for rubric-based research" ViewCount="227" />
  <row Body="&lt;p&gt;(This isn't really an answer but it's 300 characters too long for a comment.)&lt;/p&gt;&#10;&#10;&lt;p&gt;What do you do about the infinity of distributions that aren't in the list? &lt;/p&gt;&#10;&#10;&lt;p&gt;What do you do when &lt;em&gt;none&lt;/em&gt; of the ones in your list fit adequately? e.g. if your distribution is strongly bimodal &lt;/p&gt;&#10;&#10;&lt;p&gt;How are you going to deal with the fact that the exponential is just a special case of the gamma, and as such, the gamma must always fit any set of data better, since it has an additional parameter, and hence must have a better likelihood? &lt;/p&gt;&#10;&#10;&lt;p&gt;How do you deal with the fact that the likelihood is only defined up to a multiplicative constant and that the likelihood for different distributions might not automatically be comparable unless defined consistently? &lt;/p&gt;&#10;&#10;&lt;p&gt;It's not that these are necessarily insoluble, but doing this stuff in a sensible way is nontrivial; certainly more thought is required than just bunging everything through the calculation of a MLE and comparison of likelihoods.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-06-14T10:00:12.923" Id="30492" LastActivityDate="2012-06-15T00:51:47.523" OwnerDisplayName="Glen_b" OwnerUserId="805" ParentId="30491" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;Here are a few observations to add to the existing answers.&#10;I think it's important to think through conceptually why you are getting a group with zero variance.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Floor and ceiling effects&lt;/h3&gt;&#10;&#10;&lt;p&gt;In my experience in psychology, this example comes up most often when there is a floor or ceiling on a scale, and you have some groups that fall in the middle of the scale and others who fall on the extreme. For example, If your dependent variable is proportion of items correct out of five questions, then you might find that your &quot;smart&quot; group gets 100% correct or that your &quot;clinical group&quot; gets 0% correct. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this case:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You might want to fall back on ordinal non-parametric tests if you have no  variance in one of your groups. &lt;/li&gt;&#10;&lt;li&gt;Although it may not help you after the fact, you may also want to think conceptually about whether a different measure that did not have floor or ceiling effects would have been better to use. In some cases it wont matter. For example, the point of the analysis may have been to show that one group could perform a task and another could not. In other cases, you may want to model individual differences in all groups, in which case you may need a scale that does not suffer from floor or ceiling effects.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Very small group size&lt;/h3&gt;&#10;&#10;&lt;p&gt;Another case where you can get no group variance is where you have a group with a really small sample size (e.g., $n\lt5$), usually in combination with a dependent variable that is fairly discrete.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, you may be more inclined to put the lack of variance down to chance, and proceed with a standard t-test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-15T05:14:44.963" Id="30503" LastActivityDate="2012-06-16T07:20:14.450" LastEditDate="2012-06-16T07:20:14.450" LastEditorUserId="183" OwnerUserId="183" ParentId="30388" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;In the context of online clustering, I often find many papers talking about: &quot;dirichlet process&quot; and &quot;finite/infinite mixture models&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that I've never used or read about dirichlet process or mixture models. Do you know any suggestions of introductory lectures or papers that are easy to understand, about that ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-15T11:29:22.677" FavoriteCount="4" Id="30521" LastActivityDate="2014-10-20T02:45:33.507" LastEditDate="2012-06-15T11:42:56.777" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="7" Tags="&lt;clustering&gt;&lt;inference&gt;&lt;mixture&gt;&lt;dirichlet-distribution&gt;&lt;dirichlet-process&gt;" Title="Mixture Models and Dirichlet Process Mixtures (beginner lectures or papers)" ViewCount="1771" />
  
  
  
  <row AnswerCount="4" Body="&lt;p&gt;I'm currently experimenting with Gaussian processes.&#10;I decided to use matlab + gpml (&lt;a href=&quot;http://www.gaussianprocess.org/gpml/code/&quot; rel=&quot;nofollow&quot;&gt;http://www.gaussianprocess.org/gpml/code/&lt;/a&gt;) for playing around with Gaussian processes  a bit.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to do Gaussian process regression on 2d data. For that I created some simple test data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    #x y z&#10;    0 0 -1&#10;    0 7 -1&#10;    3 7 -1&#10;    8 3 -1&#10;    5 5 -1&#10;    8 8 5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result looks like this (I used a unit lengthscale and magnitude with the squared exponential cov function):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FSnTb.jpg&quot; alt=&quot;x dimension fixed to 8&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I wanted to add some variance at position (8,8), so I added the following to the training data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    8 8 -1&#10;    8 8 10&#10;    8 8 -10&#10;    8 8 -8&#10;    ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I expected the variance at this point to increase a lot. While the mean increased at this point, the variance hardly changed at all. This is the result:&#10;&lt;img src=&quot;http://i.stack.imgur.com/FnRvE.jpg&quot; alt=&quot;again x is fixed to 8. Data has added variance in (8,8)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to play around with the hyperparameters, but I couldn't get a result that looked like the one I expected: the variance increasing at (8,8)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm really stuck here, so I'd really appreciated, if anyone could explain this behavior to.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-15T13:48:59.777" Id="30527" LastActivityDate="2012-07-24T12:43:09.113" LastEditDate="2012-06-16T07:58:36.523" LastEditorUserId="183" OwnerUserId="12013" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;normal-distribution&gt;&lt;matlab&gt;&lt;gaussian-process&gt;" Title="How to increase variance in Gaussian Process regression?" ViewCount="631" />
  
  <row Body="&lt;p&gt;There is no good answer to this. Let us consider a big trio of survey statistics: stratification, clustering/multistage selection procedures, and unequal probabilities of selection/sampling weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason survey statisticians utilize &lt;em&gt;stratification&lt;/em&gt; is because they don't want to sample little bit of everything and avoid repeatedly sampling similar units; a perfect stratification is when you find a group of highly similar objects and just take a handful of these. So you should a priori expect the units within the same stratum to be similar; I don't know if you can come up with a measure of distance that accounts for that -- e.g., by discounting the distances of units if they are within the same stratum?&lt;/p&gt;&#10;&#10;&lt;p&gt;Likewise, the units within the same &lt;em&gt;primary sampling unit&lt;/em&gt; may be similar, although that is a parasitic effect for survey estimates that increases variances.&lt;/p&gt;&#10;&#10;&lt;p&gt;The effect of the sample &lt;em&gt;weights&lt;/em&gt; on cluster analysis is totally unclear. The weights are attached to individual units, so how do you come up with a weight for their distance? Survey statisticians sometimes work with pairwise selection probability weights, but this is only relevant if you are trying to estimate a population quantity that is a U-statistic of higher order (Gini index), or involves the second order moments of the design (the variance of Horwitz-Thompson estimator). Distances in cluster analysis are second order moments of the data, so it is not entirely clear whether they should be accompanied with survey weights, and if they should be, how these weights should be expressed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Oversampling may lead to some strange effects in terms of the sensitivity of your algorithm and the number of detected clusters. If you have few observations with high weights that represent a significant fraction of the population (either because few responded, or by design), your algorithm may miss all of them if it has specification of the smallest number of points it will want to consider as an interesting cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, you would want to purge the survey design information from your data and analyze the data that have the underlying dependencies only. However, yet another feature of good surveys is that the design information is often related to the outcomes of interest, often in inexplicable ways.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a bottom line, I am happy that I don't have to do this work :)&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW, your unequivocal faith in the ability of the bootstrap to fix up anything may not be well justified. I can name 3-5 or so complex survey bootstrap schemes &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/9101177&quot; rel=&quot;nofollow&quot;&gt;off the top of my head&lt;/a&gt;, and another 5-8 if I look up my notes. &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/1036970&quot; rel=&quot;nofollow&quot;&gt;Shao (1996)&lt;/a&gt; gave a great review of the existing resampling methods; nothing absolutely major came up since then.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-15T20:24:26.960" Id="30550" LastActivityDate="2012-06-15T20:24:26.960" OwnerUserId="5739" ParentId="30543" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;In building logistic regression, you have to bear in mind that the dependent value must assume exactly two values on the cases being processed. In your question , you did not provide enough information on your dependent variable or if you are using binary or multi logistic regression. Nevertheless,if you are using Gender as your dependent variable, then it must assume exactly two values representing male and female, and must not include unknown as you pointed out &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-16T13:44:07.467" Id="30582" LastActivityDate="2012-06-16T13:44:07.467" OwnerUserId="12026" ParentId="8058" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Or an even more condensed version, which helps really break it down into how the different elements contribute to the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;120000 * (0.3*1.1 + 0.2* 0.95 + 0.5*1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-06-16T22:29:10.833" Id="30598" LastActivityDate="2012-06-16T22:29:10.833" OwnerUserId="7972" ParentId="18587" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you have the number of hospitals per district that received extra-funding, and the majority size of those districts, you can check if both are correlated using Pearson correlation, and then check the significance of said correlation using a permutation test (which would handle non-normality). It sounds more complex of what is, really, so, I have prepared a &lt;a href=&quot;http://downloads.neustats.com/media/examples/MajoritySizeAndExtraFunding.xlsx&quot; rel=&quot;nofollow&quot;&gt;quick example&lt;/a&gt; of how to do it in Excel.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-17T07:11:20.150" Id="30614" LastActivityDate="2012-06-17T07:11:20.150" OwnerUserId="6624" ParentId="30599" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Say I have four populations of samples, and I'd like to know if they are distinct or sampling the same process. Each sample is multivariate, and the distributions of the variables are non-normal. Furthermore the data set is quite noisy, so kernel-based PCA on all the samples gives no discernible structure. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to select those samples that best represent the underlying process(es), so I did the following: for each population I bootstrapped the mean for each variable, giving me a mean vector. To rate the samples, I computed the Pearson correlation coefficient between sample and corresponding mean vector. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Those samples that correlate the highest with their respective mean vectors, are they suitable for further analysis (e.g. clustering methods) or am I deluding myself? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-17T11:27:02.483" Id="30620" LastActivityDate="2012-06-17T11:27:02.483" OwnerUserId="8374" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;&lt;sample&gt;" Title="Selecting samples in noisy data based on correlation with a multivariate mean vector?" ViewCount="68" />
  
  
  <row Body="&lt;p&gt;You include them as you would a level-1 predictor, like so:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;lmer(level1_depend ~ level1_feat1 + level1_feat2 + level2_feat1 + level2_feat2 + (1 | level2_ID), data = example_data)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This would be a model in which &lt;code&gt;level1_depend&lt;/code&gt; is a function of two level-1 predictors, two level-2 predictors, with intercepts for level-2 varying. Note if your data are not already in this form, you will need to have each level-1 observation within each cluster of level-2 have the same values for the level-2 predictors.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-17T17:08:37.283" Id="30632" LastActivityDate="2012-06-17T17:08:37.283" OwnerUserId="4600" ParentId="30576" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="30683" AnswerCount="2" Body="&lt;p&gt;I am reviewing my time series knowledge and looking for a document that has the commonly-used time series tests, what they are used for, how to use them, etc. e.g. Augmented Dickey–Fuller test, PACF tests, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;I found a wikipedia page of common statistical tests, but I am looking for a list of such that is specific for time series analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Common_test_statistics&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Common_test_statistics&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-17T23:13:55.110" FavoriteCount="4" Id="30640" LastActivityDate="2013-07-30T15:24:56.443" OwnerUserId="8086" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;hypothesis-testing&gt;" Title="A list of common time series tests?" ViewCount="1481" />
  <row Body="&lt;p&gt;If the points in space are measured with normal random errors the measured points in the plane for Gaussian spatial process.  When the process is Gaussian the kriging estimates are maximum likelihood as well as minimum variance unbiased.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-18T03:28:12.373" Id="30648" LastActivityDate="2012-06-18T03:28:12.373" OwnerUserId="11032" ParentId="30644" PostTypeId="2" Score="1" />
  
  <row AnswerCount="4" Body="&lt;p&gt;Apology if this is too simple. I couldn't get the more advanced r-help group to respond.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am planning to characterize workloads by measuring the correlation coefficient of two sets of real values but before that I wish to generate two sets of sample values that have a high coefficient and a low coefficient. I want to plot both in the same graph so that I can see the highly correlated values' together(peaks and troughs). I use R and know about rseek. &lt;/p&gt;&#10;&#10;&lt;p&gt;If there is any particular R book that could help my capcaity planning efforts I will buy it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variable&quot;&gt;Generate a random variable with a defined correlation to an existing variable&lt;/a&gt; is a tad too advanced for me at this time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note : The two sets of values that I am about to plot are related because I am plotting CPU usage and a througput number. So if the no: of bytes increases the CPU usage may increase. Both are postitive values. So if the correlation is high both will either increase together or decrease together.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-18T07:43:41.507" FavoriteCount="3" Id="30653" LastActivityDate="2012-06-19T07:43:35.487" LastEditDate="2012-06-19T07:43:35.487" LastEditorUserId="12051" OwnerUserId="12051" PostTypeId="1" Score="7" Tags="&lt;correlation&gt;" Title="Generate sets of values with high correlation coefficient" ViewCount="659" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking for the right statistical terminology to describe the following problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to characterize an electronics device that has a linear response&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y = \beta_0 + \beta_1 X + \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\epsilon \sim N(0,\sigma^2_{ro})$ is a term due to the read-out noise of the device. In order to determine $\beta_0, \beta_1, \sigma^2_{ro}$ I would measure a series of responses $\{X_i,Y_i\}$ and apply the standard linear regression toolbox. But I don't know what the $X_i$ are exactly, because I use a source that is affected by shot noise. That is I know that if I set the dial on the source to a certain value $J_i$ then $X_i \sim N(\mu, \mu)$ (a Gaussian with average $\mu$ and variance $\mu$).&lt;/p&gt;&#10;&#10;&lt;p&gt;This looks like an errors-in-variables model of linear regression (&lt;a href=&quot;http://en.wikipedia.org/wiki/Errors-in-variables_models&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Errors-in-variables_models&lt;/a&gt;), where not for the fact that in order to characterize my device over its entire input range, during the measurements I have to change the value of $J_i$, and now the variance of the $X_i$ is not fixed, but it depends on $X_i$ (through J_i), although because of the shot noise if $X_i=X_j$ this does not mean that the variance of $X_i$ is the same as the variance of $X_j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is this model called, and are there articles where I can find out such a problem is approached? Or am I formulating in the wrong manner?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-18T13:51:00.873" Id="30673" LastActivityDate="2012-06-18T18:39:52.283" LastEditDate="2012-06-18T18:39:52.283" LastEditorUserId="919" OwnerUserId="12057" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;maximum-likelihood&gt;&lt;heteroscedasticity&gt;&lt;errors-in-variables&gt;" Title="Linear regression with shot noise" ViewCount="252" />
  <row AcceptedAnswerId="30727" AnswerCount="3" Body="&lt;p&gt;I need to classify URLs into categories. Say I have 15 categories that I'm planning to zero down every URL to. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is a 15-way classifier better? Where I have 15 labels and generate features for each data point. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or building 15 binary classifiers, say: Movie or Non-Movie, and use the numbers I get from these classifications to build a ranker, to pick the best category, going to be better? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-18T15:12:49.837" FavoriteCount="2" Id="30680" LastActivityDate="2012-06-20T15:08:15.367" LastEditDate="2012-06-20T15:08:15.367" LastEditorUserId="4856" OwnerUserId="12060" PostTypeId="1" Score="15" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;categorical-data&gt;&lt;svm&gt;&lt;feature-selection&gt;" Title="Is building a multiclass classifier better than several binary ones?" ViewCount="265" />
  
  
  
  <row Body="&lt;p&gt;You can set the zeros of the $i^{th}$ variable to the ${\rm mean}(x_i) - n\times{\rm stddev}(x_i)$ where $n$ is large enough to distinguish these cases from the rest (e.g., 6 or 10). &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that any such artificial setup will affect your analyses so you should be careful with your interpretation and in some cases discard these cases to avoid artifacts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the detection limit is also a reasonable idea.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-19T13:24:29.803" Id="30738" LastActivityDate="2012-06-19T13:24:29.803" OwnerUserId="5919" ParentId="30728" PostTypeId="2" Score="3" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;As the zeros merely indicate concentrations below the detection limit, maybe setting them to (detection limit)/2 would be appropriate&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I was just typing that the thing that comes to my mind where log does (frequently) make sense and 0 may occur are concentrations when you did the 2nd edit. As you say, for measured concentrations the 0 just means &quot;I couldn't measure that low concentrations&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Side note: do you mean LOQ instead of LOD?&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether setting the 0 to $\frac{1}{2}$LOQ is a good idea or not depends:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;from the point of view that $\frac{1}{2}\mathrm{LOQ}$ is your &quot;guess&quot; expressing that c is anywhere between 0 and LOQ, it does make sense.&lt;br&gt;&#10;But consider the corresponding calibration function: &#10;&lt;img src=&quot;http://i.stack.imgur.com/msFn6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Zl1BL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;On the left, the calibration function yields c = 0 below the LOQ. On the right, $\frac{1}{2}\mathrm{LOQ}$ is used instead of 0.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;However, if the original measured value is available, that may provide a better guess. After all, LOQ usually just means that the relative error is 10%. Below that the measurement still carries information, but the relative error becomes huge.&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/33wa1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;(blue: LOD, red: LOQ)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;An alternative would be to exclude these measurements. That can be reasonable, too&lt;br&gt;&#10;e.g. think of a calibration curve. In practice you often observe a sigmoid shape: for low c, signal ≈ constant, intermediate linear behaviour, then detector saturation. &#10;&lt;img src=&quot;http://i.stack.imgur.com/2TpgL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;In that situation you may want to restrict yourself to statements about concentrations that are clearly in the linear range as both below and above other processes heavily influence the result.&lt;br&gt;&#10;Make sure you explain that the data was selected that way and why.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;edit: What is sensible or acceptable, depends of course on the problem. Hopefully, we're talking here about a small part of the data that does not influence the analyis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe a quick and dirty check is: run your data analysis with and without excluding the data (or whatever treatment you propose) and see whether anything changes substantially.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you see changes, then of course you're in trouble. However, from the analytical chemistry point of view, I'd say your trouble does not primarily lie in which method you use to deal with the data, but the underlying problem is that the analytical method (or its working range) was not appropriate for the problem at hand. There is of course a zone where the better statistical approach can save your day, but in the end the approximation &quot;garbage in, garbage out&quot; usually holds also for the more fancy methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;Quotations for the topic:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;A statistican once told me:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The problem with you (chemists/spectroscopists) is that your problems are either so hard that they cannot be solved or so easy that there is no fun in solving them.&lt;/p&gt;&#10;&lt;/blockquote&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/a/739/4598&quot;&gt;Fisher about the statistical post-mortem of experiments&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-06-19T13:29:22.027" Id="30739" LastActivityDate="2012-06-20T02:17:32.200" LastEditDate="2012-06-20T02:17:32.200" LastEditorUserId="2970" OwnerUserId="4598" ParentId="30728" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;My quick answer would be yes, but I am not sure about the scale parameter. You can view a Gaussian random walk as a subset of random walks with stable distributions.  All stable distributions have the property that a linear combination of two i.i.d. stable distributions is also stable. (All this is related to a generalized central limit theore and functional analysis, but that's too much to deal with here.) &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-19T14:02:00.663" Id="30742" LastActivityDate="2012-06-19T14:02:00.663" OwnerUserId="10346" ParentId="30740" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Minimizing $(Ax-b)^2$ is wrong; minimizing $(y-(Ax+b))^2$ is right.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-19T16:56:47.720" Id="30752" LastActivityDate="2012-06-19T17:22:42.930" LastEditDate="2012-06-19T17:22:42.930" LastEditorUserId="919" OwnerUserId="1397" ParentId="30751" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I would suggest two approaches to assessing whether or not a deterministic mathematical model is performing well - neither of which actually involve a statistical test, and which especially do not involve trying to reduce model performance to a p-value.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How well does your model predict parameters? If your model estimates parameters from data, how well does this estimate agree with observed parameters &lt;em&gt;from data other than what you fit the model to&lt;/em&gt;?&lt;/li&gt;&#10;&lt;li&gt;Does it generate the correct answer when confronted with parameters that result in a known change. For example, if your model is given all the parameters that occur before a heat wave, does it correctly produce said heat wave?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;As someone else has suggested, you could also compare the error between your predicted output and the actual output of the system, though this just gives you a number that you're trying to minimize, not actually a statistical estimate. Designing mathematical models to be tested statistically is very hard to do backwards - the elements you need generally need to be discussed in the model design step, just like with studies.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-19T22:37:02.753" Id="30776" LastActivityDate="2012-06-19T22:37:02.753" OwnerUserId="5836" ParentId="30763" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to develop a Generalized Linear Model (GLM) in which the dependent data is a&#10;proportion data with 4 categories. I want to develop this model as an&#10;alternative method to Multinomial Logit Model for comparison of&#10;results. Here is the sample data. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Category SharesY x1 x2&#10;1 0.80 7480 628&#10;2 0.03 5178 1011&#10;3 0.16 6711 1391&#10;4 0.01 4177 1845&#10;1 0.79 7562 628&#10;2 0.05 5260 1011&#10;3 0.15 6779 1391&#10;4 0.01 5048 1693&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By running this as a GLM model with family as binomial, and link&#10;function as logit I am getting a warning message:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!&quot;. &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also, the coefficients I am getting are not comparable to that of the MNL model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am very new to R, and not sure whether it is even possible to run&#10;GLM model using proportion data but I read it somewhere and hence&#10;tried it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, is it required to do any transformation on the dependent variable before running the GLM? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas/ thoughts/ alternate ways to run GLM using proportion data&#10;then please let me know. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-19T23:29:47.233" FavoriteCount="1" Id="30778" LastActivityDate="2012-07-14T20:34:54.320" LastEditDate="2012-07-14T20:34:54.320" LastEditorUserId="930" OwnerUserId="12091" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;multinomial&gt;" Title="Building GLM Model with dependent variable as a proportion for 4 categories using R software" ViewCount="385" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Can you use composite z-scores to perform a t-test. If so, can you use a composite z-scores in a repeated measures t-test?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-06-20T00:38:09.237" Id="30781" LastActivityDate="2012-11-22T05:44:44.377" LastEditDate="2012-06-20T05:35:59.440" LastEditorUserId="183" OwnerUserId="12092" PostTypeId="1" Score="1" Tags="&lt;t-test&gt;&lt;composite&gt;" Title="Can you use composite z-scores in a repeated measures t-test?" ViewCount="1754" />
  
  
  <row Body="&lt;p&gt;It sounds like you are looking for the semi-NMF algorithm, which relaxes the positive restriction on the input matrix (and one of the output matrixes).  Mike Jordan has a paper discussing this technique including a description of an algorithm to compute it. &lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/papers/ding-li-jordan.pdf&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to ask if this is really what you want to do.  Gene expression data is inherently non-negative, which is why NMF is popular for gene expression analysis. Your large negative values are probably the result of a log-transform which you could undo and then apply a regular NMF.  Ask whoever generated the data if any transformations were applied and consider undoing it and then running regular NMF.  This is probably easier then rolling your own implementation of an exotic NMF variant.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; If it's just a few negative entries, you could also consider removing the genes with negative entries. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-20T01:35:24.737" Id="30784" LastActivityDate="2012-06-20T14:48:31.307" LastEditDate="2012-06-20T14:48:31.307" LastEditorUserId="12046" OwnerUserId="12046" ParentId="29804" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="30800" AnswerCount="3" Body="&lt;p&gt;I'm very new with R and stats in general, but I need to make a scatterplot that I think might be beyond its native capacities.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a couple of vectors of observations and I want to make a scatterplot with them, and each pair falls into one out of three categories. I would like to make a scatterplot that separates each category, either by colour or by symbol. I think this would be better than generating three different scatterplots.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have another problem with the fact that in each of the categories, there are large clusters at one point, but the clusters are larger in one group than in the other two.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know a good way to do this? Packages I should install and learn how to use? Anyone done something similar?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-20T04:30:12.157" FavoriteCount="18" Id="30788" LastActivityDate="2012-06-20T15:03:47.327" OwnerUserId="12096" PostTypeId="1" Score="28" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;scatterplot&gt;" Title="What's a good way to use R to make a scatterplot that separates the data by treatment?" ViewCount="12683" />
  <row AnswerCount="0" Body="&lt;p&gt;I will try to explain my data as good as possible.&#10;So we taged 13 different whales with a tag that records time, depth, speed, angle of descent and ascent 25 samples every second.&#10;The normal diving behaviour of these animals is one deep dive of one hour to 1200 meters followed by a series of 3-7 shallow dives of 20 minutes up to 300 m. Because the tag not always stays the same time in each animal, my data is unbalanced and some tag records have one deep dive and 6 shallow dives while other records have 7 deep dives and 26 shallow dives.&#10;I divided each dive in units of 30 seconds. for each unit I have the next data:&#10;whale number, dive number, total number of fluke strokes in the 30 seconds unit of analysis, mean of the sin of the angle during the 30 seconds unit, swim speed, dive type(ascent or descent), dive direction( if it is a descent or an ascent) and time since the start of the dive and finally my variable response which is presence or absence of one type of fluke stroke called stroke type B.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think there has to be some autocorrelation between each 30 seconds unit of analyis and need to include it in my model but do not know how!&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested to know what affects the presence or absence of the type B stroke (which is a binomial variable with 0 and 1)&#10;so I decide to use a  binomial glmm with whale number as a random effect. I included as well dive number within whale as a random effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;here is the model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glmm114&amp;lt;-lmer (StrokeB~ Time * Depth+SINP+flukes*Depth+speed+(1|whale_number)+(0+dive_number|whale_number),data=Luciadeepas, family = binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;this is my final model after taking out the non significative variables, the problem is that due to the interaction a problem appears saying&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;The false convergence warning message (8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I looked on internet and it says is a common problem, and some people says that it doesnt make any change in the output while others says that each variable has to be&#10;divided by 100. but when I do this then the variables that become significant doesnt make any sense. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-20T10:47:33.460" Id="30803" LastActivityDate="2012-06-20T11:45:28.573" LastEditDate="2012-06-20T11:45:28.573" LastEditorUserId="8507" OwnerUserId="12102" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;" Title="False convergence warning message in lmer" ViewCount="631" />
  
  <row Body="&lt;p&gt;I don't understand how you're bootstrapping, but you're probably doing it wrong. Bootstrapping would entail simulating observations conditional on the outcome process, the censoring processes, and the distribution of covariates, of which joint estimation is impossible. Additionally, you have correlated data. Subjects which &quot;weave&quot; in and out of the study cannot be observed during their censored times. You may alternately be aggregating data and using a log linear model approach in which bootstrapping might be a sensible approach, but I still don't know how that would account for repeat observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me suggest a completely different strategy--&lt;/p&gt;&#10;&#10;&lt;p&gt;Your inference sounds very much like a motivating description for a proportional hazards model. By adjusting for the gene expression you're interested in, you can compute and compare risk sets of individuals averaged over instantaneous time and estimate a hazard ratio, which is an approximation of the risk ratio between those with and without the gene. By looking at &quot;risk sets&quot; at failure times, you can eliminate censored individuals from the denominator and have true apples-to-apples comparisons of instantaneous &quot;at-risk&quot; populations. &lt;/p&gt;&#10;&#10;&lt;p&gt;How many repeat illnesses does an average individual in this sample have as a proportion of the total illnesses? If there are only few, you may consider only the first such failure, or entering all failures as independent observations. If there are many such failures, you would want to use a frailty model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-20T15:43:58.920" Id="30812" LastActivityDate="2012-06-20T16:01:29.010" LastEditDate="2012-06-20T16:01:29.010" LastEditorUserId="8013" OwnerUserId="8013" ParentId="30380" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a dataset that has 12 explanatory variables for every 1 observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hypothesize that the data is generated by underlying process which undergoes several different phases, and that the 12 explanatory variables would somehow help in identifying clusters. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to write a machine learning algorithm that can:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;At first pass, identify the clusters within the data&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The data is dynamic in that new (additional) data is generated periodically. After classification (step 1 above), I want to be able to correctly label (i.e. classify) any new data not previously seen into one of the previously identified classes/clusters (or fail gracefully).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I assume that&lt;/p&gt;&#10;&#10;&lt;p&gt;Y(c) ~ X(c) + error&lt;/p&gt;&#10;&#10;&lt;p&gt;where:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Y(c) is a nx1 vector of observations belonging to cluster C&lt;/li&gt;&#10;&lt;li&gt;X(c) is a nx12 vector of explanatory factors that 'belong' to cluster C&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Observations in different clusters will differ from each other by having different distribution shapes. That is to say observations WITHIN a cluster will have a different distribution shape compared to observations from another cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am relatively new to machine learning, and would like some guidance on how to implement such an algorithm (or perhaps one already exists?)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be particularly interested in finding out how to 'classify' observations based on determining the &lt;em&gt;shape&lt;/em&gt; of the empirical distribution of the observed data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Last but not the least, I would also appreciate some advice on whether to implement this in R, or Octave (and why).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-20T15:48:50.223" FavoriteCount="1" Id="30813" LastActivityDate="2013-02-16T07:20:50.330" LastEditDate="2012-06-20T17:28:22.293" LastEditorUserId="8333" OwnerUserId="8333" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;" Title="How to implement this classification/labelling problem?" ViewCount="141" />
  <row Body="&lt;p&gt;Nope! Both can be used.. interchangeably :-) it's the same.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-20T16:48:42.507" Id="30817" LastActivityDate="2012-06-20T16:48:42.507" OwnerUserId="12060" ParentId="30816" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;So..if you dont have the data already into clusters or a set of training data that are already classified ( with their respective labels ) then the first attempt towards solving this problem is an &quot;unsupervised approach&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could look up.. K-Means clustering algorithms or Expectation Maximization (EM algorithms) or even LDA(latent Dirichlet allocation). These are algorithms that would help you perform clustering based on the features you generate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Once you get your clusters from these, you get find a way to measure the precision/recall to get an idea of how good/bad the clusters are. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then you could probably proceed to use this as a training data to develop a model, and classify the &quot;unobserved&quot; data points or &quot;new data&quot; as you call it, you come across using this model. &lt;/p&gt;&#10;&#10;&lt;p&gt;my two cents!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-20T17:03:33.587" Id="30819" LastActivityDate="2012-06-20T17:03:33.587" OwnerUserId="12060" ParentId="30813" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;A much slower (at least in my implementation) alternative to Procrastinator's answer is the brute-force method: form the likelihood as the convolution of a Gaussian and $\alpha$-stable distribution, calculated by numerical integration, then maximize it.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(stabledist)&#10;library(MASS)&#10;library(stats4)&#10;&#10;# True parameter values&#10;alpha0 &amp;lt;- 1.75&#10;sigma0 &amp;lt;- 1&#10;c0 &amp;lt;- 0.5&#10;set.seed(2)&#10;&#10;# Simulated sample&#10;Z &amp;lt;- rnorm(100) + rstable(n=100, alpha=alpha0 , beta=0, gamma = c0)&#10;&#10;# -log likelihood &#10;#   (uses log of scale parameters as input, so range is (-Inf,Inf))&#10;ll = function(lsigma, lsc) {&#10;    fconv &amp;lt;- function(x, zi, sigma, sc) {&#10;        dnorm(x, 0, sigma)*dstable(zi-x, alpha=alpha0, beta=0, gamma=sc)&#10;    }&#10;&#10;    sigma &amp;lt;- exp(lsigma)&#10;    sc &amp;lt;- exp(lsc)&#10;    f &amp;lt;- 0&#10;    for (zi in Z) {&#10;        f &amp;lt;- f + log(integrate(fconv, lower=-5*sigma, upper=5*sigma, zi=zi, sigma=sigma, sc=sc)$value)&#10;    }&#10;    -f&#10;}&#10;&#10;&#10;# optimisation &#10;# Note: reltol should probably be set larger than the accuracy of integrate &#10;#    or you may have convergence problems&#10;foo &amp;lt;- mle(ll, start=list(lsigma=0, lsc=log(0.5)), &#10;   control=list(reltol=4*(.Machine$double.eps^0.25)))&#10;summary(foo)&#10;&#10;... blah blah blah ...&#10;&#10;Coefficients:&#10;         Estimate Std. Error&#10;lsigma  0.1703237  0.9844097&#10;lsc    -0.6680191  2.0703820&#10;&#10;-2 log L: 361.0518 &#10;&amp;gt; exp(foo@coef)&#10;   lsigma       lsc &#10;1.1856886 0.5127232 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Runtime, however, is an issue; on my reasonably fast computer, this took about 25 minutes to run.  Larger samples, or starting well away from the MLE, would no doubt take longer.  You clearly wouldn't want to form confidence intervals by bootstrapping the procedure...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-20T21:36:22.147" Id="30831" LastActivityDate="2012-06-20T21:36:22.147" OwnerUserId="7555" ParentId="30743" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="30854" AnswerCount="1" Body="&lt;p&gt;Let's say I got a set of models $M = \{M_1, M_2, \dots M_n\}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now say I got some data $x$ and I would like to know, which model represents the data best.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know how to calculate the likelihood $L(\theta | x)$, with $\theta$ being the parameters of any of those models. I realize that the likelihood value of one model alone won't tell me anything useful. But what I can do is compare them to each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I know which of the given models is the most likely.&#10;&lt;strong&gt;But&lt;/strong&gt;: I would also like to know, how likely it is none of the models represents a model well enough? That is, I'm interested in a statistical sound way to tell, that I should create a new model for that data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers on how I could calculate this?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-06-20T23:13:19.003" FavoriteCount="0" Id="30835" LastActivityDate="2012-06-21T10:22:43.530" LastEditDate="2012-06-21T10:22:43.530" LastEditorUserId="12013" OwnerUserId="12013" PostTypeId="1" Score="3" Tags="&lt;maximum-likelihood&gt;&lt;likelihood-function&gt;" Title="Likelihood based model selection" ViewCount="224" />
  
  
  <row AcceptedAnswerId="30894" AnswerCount="3" Body="&lt;p&gt;Normally when somebody finds an association in an epidemiological study people are quick to point out that it doesn't prove causality, that there are problems of missing co-founders, that it is at best hypothesis generating and at worst spurious. This leads to people not putting much weight on associations found in epidemiological studies. &lt;/p&gt;&#10;&#10;&lt;p&gt;What if it goes the other way around? Say I already have a theory, maybe with some small earlier studies to back it up and even a good theoretical explanation for the effect. Then I do a big, well powered, epidemiological study and fail to find the association the theory predicts. How much weight can I put on the result now?&lt;/p&gt;&#10;&#10;&lt;p&gt;Intuitively it seems to me that the result would be quite damning, despite it being only a epidemiological study. But I have long since learned not to trust my intuition when it comes to statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Do all the weaknesses of associations found in epidemiological studies also apply to when you fail to find an association?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-21T11:43:49.517" FavoriteCount="1" Id="30869" LastActivityDate="2012-06-22T08:58:16.250" LastEditDate="2012-06-22T08:33:08.740" LastEditorUserId="183" OwnerUserId="9728" PostTypeId="1" Score="6" Tags="&lt;epidemiology&gt;&lt;causal-inference&gt;&lt;confounding&gt;" Title="What to conclude when you fail to find an association in an epidemiological study?" ViewCount="200" />
  
  
  <row Body="&lt;p&gt;Ok, here is the answer i came up with (it's essentially taken from &lt;a href=&quot;http://www.jameskeirstead.ca/r/gaussian-process-regression-with-r/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &#10;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CFQQFjAA&amp;amp;url=http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf&amp;amp;ei=rl_jT6n4FoK_0QW7ltSxBg&amp;amp;usg=AFQjCNGBT5oUZFxMjhw7n62yoqYCR6kl0Q&amp;amp;sig2=QkjUw20LtrNOe9afwM-7tA&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). The idea is to project some random pairs $\{x_i,y_i\}$ unto a spline basis. Then, we are assured to get a draw from a (smooth) GP. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(&quot;MASS&quot;)&#10;calcSigma&amp;lt;-function(X1,X2,l=1){&#10;    Sigma&amp;lt;-matrix(rep(0,length(X1)*length(X2)),nrow=length(X1))&#10;    for(i in 1:nrow(Sigma)){&#10;        for (j in 1:ncol(Sigma)) Sigma[i,j]&amp;lt;-exp(-1/2*(abs(X1[i]-X2[j])/l)^2)&#10;    }&#10;    return(Sigma)&#10;}&#10;# The standard deviation of the noise&#10;n.samples&amp;lt;-50&#10;n.draws&amp;lt;-50&#10;x.star&amp;lt;-seq(-5,5,len=n.draws)&#10;nval&amp;lt;-3&#10;f&amp;lt;-data.frame(x=seq(-5,5,l=nval),y=rnorm(nval,0,10))&#10;sigma.n&amp;lt;-0.2&#10;# Recalculate the mean and covariance functions&#10;k.xx&amp;lt;-calcSigma(f$x,f$x)&#10;k.xxs&amp;lt;-calcSigma(f$x,x.star)&#10;k.xsx&amp;lt;-calcSigma(x.star,f$x)&#10;k.xsxs&amp;lt;-calcSigma(x.star,x.star)&#10;f.bar.star&amp;lt;-k.xsx%*%solve(k.xx+sigma.n^2*diag(1,ncol(k.xx)))%*%f$y&#10;cov.f.star&amp;lt;-k.xsxs-k.xsx%*%solve(k.xx+sigma.n^2*diag(1,ncol(k.xx)))%*%k.xxs&#10;values&amp;lt;-matrix(rep(0,length(x.star)*n.samples),ncol=n.samples)&#10;for (i in 1:n.samples)  values[,i]&amp;lt;-mvrnorm(1,f.bar.star,cov.f.star)&#10;values&amp;lt;-cbind(x=x.star,as.data.frame(values))&#10;matplot(x=values[,1],y=values[,-1],lty=1,type=&quot;l&quot;,col=&quot;black&quot;)&#10;lines(x.star,f.bar.star,col=&quot;red&quot;,lwd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aIMaO.jpg&quot; alt=&quot;A trial. Smooth functions&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-21T18:01:15.760" Id="30891" LastActivityDate="2012-06-21T22:44:34.287" LastEditDate="2012-06-21T22:44:34.287" LastEditorUserId="603" OwnerUserId="603" ParentId="30652" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Yes, the weaknesses of associations found in epidemiological studies also apply to a failure to find an association. You've already eliminated the first go-to problem, that of a study being underpowered, so at the moment we're just talking about bias. Two issues that may mean your study is failing to find a true association:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Confounding&lt;/strong&gt;. There are conditions where a confounding variable will drive a result toward the null. For positive effects, this is the confounding variable being negatively associated with both the exposure and the outcome. For negative effects, the reverse. This could easily drive, depending on the strength of the confounding, a real relationship downward enough that you cannot find it.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Misclassification&lt;/strong&gt;. Non-differential misclassification is when everyone in your study has an equal probability of being in the wrong category. This &lt;em&gt;tends&lt;/em&gt; to drive estimates toward the null. Differential misclassification, where particular categories are more prone to being misclassified, can drive results toward &lt;em&gt;or&lt;/em&gt; away from the null.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So no, the results of a single study should never be taken as definitive &quot;proof&quot;, one way or the other, of a causal relationship.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-21T18:25:11.300" Id="30894" LastActivityDate="2012-06-21T18:25:11.300" OwnerUserId="5836" ParentId="30869" PostTypeId="2" Score="3" />
  
  
  
  
  <row Body="&lt;p&gt;I would vote for &lt;a href=&quot;http://lucene.apache.org/core/&quot; rel=&quot;nofollow&quot;&gt;http://lucene.apache.org/&lt;/a&gt; and &lt;a href=&quot;http://mahout.apache.org/&quot; rel=&quot;nofollow&quot;&gt;http://mahout.apache.org/&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-22T08:19:18.740" Id="30917" LastActivityDate="2012-06-22T08:19:18.740" OwnerUserId="10908" ParentId="30916" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I think it's worth distinguishing a few aspects of the problem:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt;: If you have a bigger sample size, you will typically be able to estimate parameters with greater precision whether you define this in a frequentist sense in terms of smaller confidence intervals or in a Bayesian sense in terms of smaller credibility intervals. Thus, if you conduct an epidemiological observational study with a bigger sample you will have greater precision in describing the magnitude of a parameter of interest. This is true whether the parameter is a simple correlation coefficient or a regression coefficient in a broader model with many other predictors. It's also true whether the parameter of interest is derived from an observational study or an experimental study. So for example, you might get a very accurate estimate of the correlation between eating chocolate and Body Mass Index. Associations, whether causal or not, are real and interesting.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Generalisation&lt;/strong&gt;: However, even if you know the value of a parameter in your particular sample, there are still issues of generalisation. In epidemiology there are plenty of issues related to generalising across time, culture, social groups, and so on. Often we have theories and empirical evidence to guide us in this process of generalising. For example, we may argue that is safe to generalise a chocolate-BMI association over reasonable periods of time, but that perhaps across nations it is more complex, perhaps because of different eating and exercise habits, etc.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Causality versus association&lt;/strong&gt;: However, you seem to be particular interested in causal inference. At a basic level, &lt;strong&gt;the absence of an association in an observational study does not prove the absence of causality, just as the presence of an association in an observational study does not prove causality&lt;/strong&gt;. Even if observational studies showed no relationship between, for example, chocolate and BMI, this would not prevent experimental studies from showing that when kids were fed more chocolate, they put on weight. &lt;strong&gt;The association or lack of association in an observational study may be informative as to causal processes, but it is not definitive.&lt;/strong&gt; You still need to think hard about the theorised underlying causal processes.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-06-22T08:58:16.250" Id="30920" LastActivityDate="2012-06-22T08:58:16.250" OwnerUserId="183" ParentId="30869" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Let's say I got a Gaussian Process model $M$ based on some training data. Now I get a stream of sample data of a certain batch size coming in.&lt;/p&gt;&#10;&#10;&lt;p&gt;The GP does not model a time series, but it's trying to regress the value at certain locations $x$, that will be visited mutliple times.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that at some point there will be an abrupt change in the distribution the data batches are generated from. (At least on certain locations $x$)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I'm looking for a statistically sound way of detecting this change, that is I want to find the point, when the GP doesn't model the data well enough anymore.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought I would base this on detecting &quot;big&quot; changes in the likelihood function $L(\theta | x)$. However I'm not sure how to interpret &quot;big&quot; here, as the values of the likelihood function only have a meaning in comparison, but on their own.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I'm asking this, because some people said my original &lt;a href=&quot;http://stats.stackexchange.com/questions/30835/likelihood-based-model-selection&quot;&gt;question&lt;/a&gt; was too abstract. I didn't want to change the whole question, though, as there already were some answers.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-22T11:28:10.330" FavoriteCount="1" Id="30930" LastActivityDate="2012-06-26T06:46:44.543" LastEditDate="2012-06-26T06:46:44.543" LastEditorUserId="12013" OwnerUserId="12013" PostTypeId="1" Score="5" Tags="&lt;model-selection&gt;&lt;maximum-likelihood&gt;&lt;gaussian-process&gt;&lt;likelihood-function&gt;" Title="Gaussian Process goodness of fit" ViewCount="246" />
  <row AnswerCount="1" Body="&lt;p&gt;When running a logit/probit model for particular sets of outcomes on a set of participants (whether they did a certain behavior at least two times), how can one best control for differences when the individual observations began? In particular, one person may have started to participate 3 years ago, whereas another person may have joined just last year. I'm using STATA for the analysis and do know when the individuals began observations. Related to this question, what is the most effective way to refine the functional form of a the model?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-06-22T12:38:13.963" Id="30936" LastActivityDate="2012-06-23T14:21:27.437" LastEditDate="2012-06-22T21:13:40.100" LastEditorUserId="11137" OwnerUserId="11137" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;stata&gt;&lt;probit&gt;" Title="Logistic regression across groups" ViewCount="184" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://williamlowe.net/software/jfreq/&quot; rel=&quot;nofollow&quot;&gt;JFreq&lt;/a&gt; might do what you want.  There's a command line version too.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-22T14:30:22.683" Id="30940" LastActivityDate="2012-06-22T14:30:22.683" OwnerUserId="1739" ParentId="30916" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Both my independent and dependent variables are binary. My result for classification table is 72% for predicted, and my ROC curve area is 0.389. Since &amp;lt;0.5 for ROC area is the worst for accuracy model, what should I do?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-06-22T17:47:50.877" Id="30954" LastActivityDate="2012-06-22T19:52:58.860" LastEditDate="2012-06-22T19:52:58.860" LastEditorUserId="88" OwnerUserId="12158" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;" Title="Binary classification and ROC curve area less than .5" ViewCount="548" />
  
  
  
  <row AcceptedAnswerId="30996" AnswerCount="4" Body="&lt;p&gt;I'm interesting in learning about how to rate and rank individuals in a group who only interact/compete in a pairwise fashion (i.e., systems like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Elo_rating_system&quot;&gt;ELO&lt;/a&gt; rating system for chess). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Are there are any go-to methods or more accurate and advanced methods out there? &lt;/li&gt;&#10;&lt;li&gt;Are there any R packages that make implementation easy? &lt;/li&gt;&#10;&lt;li&gt;Are there methods that can use auxiliary information as well as the outcome of a match/game? &lt;/li&gt;&#10;&lt;li&gt;Are there methods that can better use the information of winning margin as opposed to dichotomous win/lose?&lt;/li&gt;&#10;&lt;li&gt;What should I be looking for in the literature?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-06-23T01:58:16.163" FavoriteCount="3" Id="30976" LastActivityDate="2014-06-25T01:58:41.757" LastEditDate="2012-06-23T04:53:27.380" LastEditorUserId="183" OwnerUserId="845" PostTypeId="1" Score="15" Tags="&lt;ranking&gt;&lt;rating&gt;" Title="How to get started with rating and ranking based on pairwise competition data?" ViewCount="1196" />
  <row Body="&lt;p&gt;If you use &lt;code&gt;ggplot2&lt;/code&gt; (the third plotting system, in R, after base R and lattice), this becomes:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2)&#10;ggplot(Data, aes(x,y)) + geom_point() + geom_smooth()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oDIoY.png&quot; alt=&quot;plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can choose how the data is smoothed: see &lt;code&gt;?stat_smooth&lt;/code&gt; for details and examples.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-23T15:30:55.083" Id="30990" LastActivityDate="2012-06-23T15:30:55.083" OwnerUserId="8539" ParentId="30975" PostTypeId="2" Score="13" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm looking for way to visualize subjective rankings, separate from my non-parametric tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've asked 12 participants to rank 8 different items according to different subjective criterion (separate rankings for each one). For any individual set of rankings, I'm looking for a good way to visualized the high-level trends of the rankings.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried both bar and radar plots on the average rankings, and I've seen one other person use a scatter/balloon plot over the number of responses per ranking, but I'm not quite sure what conveys the best overview. Either I can use the 8 mean rankings, or the the 8 counts of each ranking per item.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&#10;Each column is an item, each row is a person's ranking of each of the eight items. Not a particularly strong agreement in this example, but in general would like to understand the best way to convey the overall trends.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                        Item:&#10;            A   B   C   D   E   F   G   H&#10;Rater:  &#10;  1         6   8   1   7   3   4   2   5&#10;  2         1   3   8   7   6   5   2   4&#10;  3         5   8   7   6   1   4   2   3&#10;  4         5   8   7   6   4   2   1   3&#10;  5         1   2   8   7   4   3   5   6&#10;  6         1   7   8   5   6   2   4   3&#10;  7         5   1   8   4   7   3   6   2&#10;  8         4   2   8   7   6   1   5   2&#10;  9         6   3   8   4   7   1   5   2&#10;  10        3   2   8   7   4   1   5   6&#10;  11        2   3   7   8   1   5   4   5&#10;  12        8   5   6   7   2   3   1   4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2012-06-23T17:21:42.880" FavoriteCount="1" Id="30995" LastActivityDate="2013-10-04T22:20:06.053" LastEditDate="2012-06-23T17:38:38.190" LastEditorUserId="7290" OwnerUserId="12172" PostTypeId="1" Score="10" Tags="&lt;data-visualization&gt;&lt;nonparametric&gt;&lt;excel&gt;&lt;ranking&gt;" Title="How would one graph the results of subjective rank order?" ViewCount="5612" />
  <row Body="&lt;p&gt;What is firm size measured by -- market capitalization?  If so, could you index market cap, and simply rank the firms in terms of size.  The regressor would then be &quot;size_rank&quot;.  Or, bucket firms into small, medium, and large groups.  This might mitigate some of the multicollinearity.&lt;/p&gt;&#10;&#10;&lt;p&gt;The broader issue is that because most of your focus variables include firm size in the denominator, firm size is already implicitly controlled for in your model.  In other words, you've already 'normalized' the regressors by firm size.  If you think hard about why you need to control for &lt;em&gt;additional&lt;/em&gt; variation due to firm size, you might come up with a solution, or toss it out altogether.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you think that the coefficient on &quot;focus_variable_1&quot; should be different based on the size of the firm, you could add an additional interaction term (firm_size*focus_variable_1).  This is along the lines of your suggestion (1) above, however you would want to keep the existing non-interaction term and not also control for firm size.  Then to calculate the full impact of a focus_variable_1 on the dependent variable, you would add the coefficient on (focus_variable_1) to the coefficient on the interaction term multiplied by the mean firm size, then maybe +/- one standard deviation.  As you can see, interpretation gets difficult quickly, so it is good to have the theory solid before blindly dropping in additional interaction terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;For additional discussion on interpreting continuous*continuous interaction terms, see: &lt;a href=&quot;http://www.nd.edu/~rwilliam/stats2/l55.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.nd.edu/~rwilliam/stats2/l55.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-23T20:28:21.550" Id="31001" LastActivityDate="2012-06-23T20:28:21.550" OwnerUserId="9420" ParentId="30986" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;For completeness's sake, here's an answer and some references.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because each sample has equal probability, it suffices to count the number of samples with the desired property and divide by the total number of samples.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Let's tackle the &lt;strong&gt;denominator&lt;/strong&gt; first, because it's easier to count.  A &quot;sample&quot; can be uniquely described by the sequence of results.  Abstractly, that's a function $s$ from $\{1,2,\ldots,N\}$ to the set of items or, equivalently, an $N$-vector with coefficients in a set of $K$ things.  Because there are $K$ ways to specify each coefficient, the total number of samples equals $K^N$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The &lt;strong&gt;numerator&lt;/strong&gt; is the number of samples in which each item appears at least once.  In mathematical terminology, $s$ is a &lt;em&gt;surjection&lt;/em&gt; (or &quot;onto&quot; function).  Combinatorics texts explain how to count surjections and they provide formulas.  For instance, Wagner's &lt;a href=&quot;http://www.math.utk.edu/~wagner/papers/comb.pdf&quot; rel=&quot;nofollow&quot;&gt;Basic Combinatorics&lt;/a&gt; covers this on the first page of Chapter 9 and the first page of Chapter 10.  The upshot is that the numerator equals $K!$ times the &lt;a href=&quot;http://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind&quot; rel=&quot;nofollow&quot;&gt;Stirling number of the second kind&lt;/a&gt;, $S(N,K)$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The desired probability therefore equals $K! S(N,K) /K^N$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This technique of first reducing a probability to a counting problem and then expressing that counting problem in terms of a set of functions with specific properties gives a general method for characterizing problems of this type. (Gian-Carlo Rota's &lt;a href=&quot;http://en.wikipedia.org/wiki/Twelvefold_way&quot; rel=&quot;nofollow&quot;&gt;Twelvefold Way&lt;/a&gt; provides a unified approach to such problems.) With such a characterization in hand you can often then look up the answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;As an example of this particular formula,&lt;/strong&gt; consider the case $K=2$ and $N=4$. We can look up or compute that $S(N,K)=7$, whence the probability is $2! \times 7 / 2^4$ = $14/16$. We can check by enumerating all samples of size $4$ from a set of $2$ items, such as $\{A,B\}$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$AAAA, AAAB, AABA, AABB, ABAA, ABAB, ABBA, ABBB, BAAA, BAAB, BABA, BABB, BBAA, BBAB, BBBA, BBBB.$&lt;/p&gt;&#10;&#10;&lt;p&gt;Of these, the first and last omit one of the items but the remaining $14$ out of the $16$ samples include both.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, the question asks for the answer when $N=10^4$ and $K=365$. It differs from $1$ by $4.44104 \times 10^{-10}$.  It is interesting to see how this probability depends on $N$ (keeping $K$ fixed at $365$):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aVKIV.png&quot; alt=&quot;Plot of probabilities vs. N&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-23T21:10:41.480" Id="31005" LastActivityDate="2012-06-23T21:10:41.480" OwnerUserId="919" ParentId="30483" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.cs.waikato.ac.nz/ml/weka/&quot; rel=&quot;nofollow&quot;&gt;Weka&lt;/a&gt; offers this functionality in Java. Start Weka and open the &lt;code&gt;Explorer&lt;/code&gt;. Then load your dataset and apply the &lt;code&gt;StringToWordVector&lt;/code&gt; filter. This filter can create a doc term matrix (either binary or by frequency), do IDF, stopword removal, stemming, normalization, punctuation removal and more.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-24T09:57:50.447" Id="31025" LastActivityDate="2012-06-25T17:36:58.917" LastEditDate="2012-06-25T17:36:58.917" LastEditorUserId="11812" OwnerUserId="11812" ParentId="30916" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The problem you'll have here is that each of your F's is calculated across different terms and therefore really has different underlying effect variances and different violations of sphericity.  A violation on the treatment effect doesn't mean you have a violation on the words.  Unless you recalculate sphericity for your specific terms leading to your Fmin' you're not going to solve this.&lt;/p&gt;&#10;&#10;&lt;p&gt;There isn't a calculator out that that does this for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the bright side, people have pretty much ignored the sphericity assumption for many decades.  Perhaps you can as well.  The degree of violation is a random variable, if you haven't violated it by much then perhaps you haven't really violated it.  Furthermore, the G-G and H-F corrections don't solve the problem, they just ameliorate it (sometimes).  It's possible to over correct and go from Type I to Type II errors, or not quite correct enough.  Some argue you only use the epsilons as an indicator of whether you should even trust the ANOVA at all, not just to correct the F's.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-24T14:15:50.423" Id="31029" LastActivityDate="2012-06-24T14:15:50.423" OwnerUserId="601" ParentId="31004" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;When the standard error of the difference is 0 then you might go with a non-parameteric test.  A sign test would be good.  Rather than give you the probability on a t-distribution, it gives you the probability of that many successes (differences in the same direction).  That would typically be a meaningful and useful kind of p-value to describe.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variance of 0 means that all of the differences were the same.  This can happen for a variety of reasons, for example, insensitive measurement, or genuinely extremely low variance of the effect.  Perhaps you even rounded off the variability.  It's very rare though, if you have data that follow the assumption of the t-test.  Your data probably require a non-parametric test in the first place.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-24T14:19:23.460" Id="31030" LastActivityDate="2012-06-24T14:24:49.990" LastEditDate="2012-06-24T14:24:49.990" LastEditorUserId="601" OwnerUserId="601" ParentId="31014" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I need help with a probability problem and how to simulate it in a program.&lt;/p&gt;&#10;&#10;&lt;p&gt;A large number of people are individually asked a series of difficult questions. Statistics show that 40% of people who answered a question correctly also answered the next question correctly. 35% of those who answered a question correctly also answered the question that followed the next question correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the probability of someone answering a question correctly, if:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    a) they answered both previous questions correctly  &#10;    b) they answered the previous question correctly, but the one before incorrectly.&#10;    c) they answered the previous question incorrectly and the one before correctly.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="12" CreationDate="2012-06-24T16:01:57.723" Id="31034" LastActivityDate="2012-06-25T03:26:51.640" LastEditDate="2012-06-24T16:27:38.533" LastEditorUserId="919" OwnerUserId="12185" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;conditional-probability&gt;" Title="Simulating answering a series of questions correctly in a row" ViewCount="117" />
  <row Body="&lt;p&gt;Each Poisson arrival will see the system either in state $E$ or not-$E$ ($\bar{E}$).   One might suspect that we can construct confidence intervals for the long run fraction of the time in state $E$ (LRF($E$)) by treating this as a sample from a binomial distribution for which we are estimating the probability $p$, but there are two problems with a straightforward approach.  First, the constructed CIs only apply to the sample interval $D$.  To see why this is a problem, imagine we sample over 10 seconds, 5 of which are in $E$ and 5 not.  Increasing sampling frequency while holding $D$ fixed will cause our estimate to converge on 0.5 and our naive confidence intervals to shrink towards a width of 0, but the long run fraction of the time spent in $E$ might well not equal 0.5; $D$ is itself a sample from the long run.     &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, successive observations are not independent.  Imagine the system alternates between one hour in $E$ and one hour in $\bar{E}$, forever.  If the sample interval $D$ is, say, 10 seconds long with start time uniformly distributed over (0,2) hours, with high probability we will see exactly 1 sojourn in either $E$ or $\bar{E}$ and we will estimate LRF($E$) to be either 0 or 1.  Our estimate will be very inaccurate, even with a sample size of 1000 over the 10 seconds.  If, on the other hand, we have the duration of $D$ = 10 years with 1000 samples in $D$, we will see close to 500 sojourns in $E$ and 500 in $\bar{E}$ and our estimate will be close to 0.5.  Our estimate, although based on the same sample size as the previous example's estimate, will be much more accurate.  In either case, though, the estimator is unbiased, as its expected value is 0.5.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The other factor (besides sample size) that counts for constructing CIs for the LRF($E$) is evidently the number of distinct sojourns in $E$ and in $\bar{E}$ we see while in the steady state.  The ideal case is when the sojourns are tagged, so our sample not only counts the frequencies of $E$ and $\bar{E}$ but the number of distinct sojourns into each state.  Otherwise, if the mean duration of a sojourn in $E$ and $\bar{E}$ is much shorter than the mean inter-arrival time of our Poisson process, then we can assume almost all the observed $E$s are distinct sojourns, in which case the Binomial sampling approach gives only slightly too small confidence intervals.  (This is pretty much the best case; it wastes the fewest Poisson samples.)  If, on the other hand, the mean duration of a sojourn in $E$ and $\bar{E}$ is much longer than the mean inter-arrival time of our Poisson process, we might assume that a run of observed $E$s represents a single sojourn of the system in $E$, and likewise for $\bar{E}$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Either way, by adding the sojourn counts for $E$ and $\bar{E}$ together, we get an estimated (or calculated) total number of sojourns $N$.  Let $\hat{p}$ be the fraction of samples which saw state $E$; if $N\hat{p}$ and $N(1-\hat{p})$ are both large enough, say &gt; 5 (rule of thumb), then we can construct an approximate CI using $\hat{p}$ and $\sqrt{\hat{p}(1-\hat{p})/N}$ as the mean and standard deviation in a Normal distribution, similar to what we would do with true Binomial sampling.  Otherwise, we can construct an approximate CI using $N$ and, for the lower bound, $x = \lfloor N\hat{p} \rfloor$, for the upper bound, $x = \lceil N\hat{p} \rceil$, and pretending that we observed $x$ from a Binomial sample of size $N$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that neither of these approximate CIs is likely to be any good when you're not in one of the two extreme cases described above.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a little simulation that will illustrate the point.  The duration of $E$ and $\bar{E}$ are both distributed Exponential(1), so the long run fraction of the time the system is in $E = 1/2$.  I sample 1000 times at rates of every 0.01, 0.1, 1, and 10 time units, and repeat 1000 times, estimating $p$ (the long run average) each time.  In the first case, it's easy to see we expect to see about 5 sojourns each in $E$ and $\bar{E}$ for an effective sample size of 10; in the latter, about 500 each, for an effective sample size of 1000.  Here's the code and results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MTBSamples &amp;lt;- c(0.01, 0.1, 1, 10)&#10;EOn &amp;lt;- function(st) {min(which(E.Transition&amp;gt;st)) %% 2 == 1}&#10;      # Odd = not in E, Even = in E&#10;&#10;phat &amp;lt;- matrix(0, nrow=1000, ncol=length(MTBSamples))&#10;for (i in 1:length(MTBSamples)) {&#10;  for (j in 1:nrow(phat)) {&#10;    E.Transition &amp;lt;- cumsum(rexp(20000,1))   #20000 = Lots more than needed&#10;    SamplePoints &amp;lt;- cumsum(rexp(1000,1/MTBSamples[i]))&#10;    phat[j,i] &amp;lt;- mean(sapply(SamplePoints, EOn))&#10;  }&#10;}&#10;&#10;# Mean of estimates of long run occupancy fraction&#10;colMeans(phat)&#10;[1] 0.530586 0.502062 0.500477 0.499649&#10;&#10;# Standard deviations of estimates of long run occupancy fraction&#10;apply(phat,2,sd)&#10;[1] 0.15686412 0.05153150 0.02290577 0.01681441&#10;&#10;# (Estimated) effective sample size&#10;0.25/apply(phat, 2, var)&#10;[1]  10.15998  94.14438 476.48622 884.25297&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-06-24T23:12:26.867" Id="31042" LastActivityDate="2012-06-26T02:37:09.570" LastEditDate="2012-06-26T02:37:09.570" LastEditorUserId="7555" OwnerUserId="7555" ParentId="31024" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I can think of no better place than Owen's book to learn about empirical likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;One practical way to think about $L = L(p_1, \ldots, p_n)$ is as the likelihood for a multinomial distribution on the observed data points $x_1, \ldots, x_n$. The likelihood is thus a function of the probability vector $(p_1, \ldots, p_n)$, the parameter space is really the $n$-dimensional simplex of probability vectors, and the MLE is putting weight $1/n$ on each of the observations (supposing they are all different). The dimension of the parameter space increases with the number of observations. &lt;/p&gt;&#10;&#10;&lt;p&gt;A central point is that empirical likelihood gives a method for computing confidence intervals by profiling without specifying a parametric model. If the &lt;em&gt;parameter of interest&lt;/em&gt; is the mean, $\mu$, then for any probability vector $p = (p_1, \ldots, p_n)$ we have that the mean is&lt;br&gt;&#10;$$\mu(p) = \sum_{i=1}^n x_i p_i,$$&#10;and we can compute the profile likelihood as &#10;$$L_{\text{prof}}(\mu) = \max \{ L(p) \mid \mu(p) = \mu \}.$$&#10;Then we can compute confidence intervals of the form &#10;$$I_r = \{ \mu \mid L_{\text{prof}}(\mu) \geq r L_{\text{prof}}(\bar{x}) \}$$&#10;with $r \in (0,1)$. Here $\bar{x}$ is the empirical mean and $L_{\text{prof}}(\bar{x}) = n^{-n}$. The intervals $I_r$ should perhaps just be called (profile) likelihood intervals since no statement about coverage is made upfront. With decreasing $r$ the intervals $I_r$ (yes, they are intervals) form a nested, increasing family of confidence intervals. Asymptotic theory or the bootstrap can be used to calibrate $r$ to achieve 95% coverage, say. &lt;/p&gt;&#10;&#10;&lt;p&gt;Owen's book covers this in detail and provides extensions to more complicated statistical problems and other parameters of interest.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-25T07:20:27.170" Id="31057" LastActivityDate="2012-06-25T19:16:10.153" LastEditDate="2012-06-25T19:16:10.153" LastEditorUserId="4376" OwnerUserId="4376" ParentId="31053" PostTypeId="2" Score="12" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Lets say I have the following data frame&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(survival)&#10;library(multcomp)&#10;data(cml)&#10;cml$group&amp;lt;-sample(1:5, 507, replace=T)&#10;    plot(survfit(Surv(time=cml$time, cml$status)~factor(cml$group)))&#10;(survdiff(Surv(time=cml$time, cml$status)~factor(cml$group)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to perform multiple comparison test  comparing (logrank) for example group0 vs. all other groups or even every group with each other? Is it necessary to correct for multiple comparisons? If yes, is there a nice way of plotting these multiple comparisons (as for example in &lt;code&gt;plot.TukeyHSD()&lt;/code&gt; in &lt;code&gt;aov()&lt;/code&gt;)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have posted the same question in &lt;a href=&quot;http://stackoverflow.com/questions/11176762/kaplan-meier-multiple-group-comparisons&quot;&gt;http://stackoverflow.com/questions/11176762/kaplan-meier-multiple-group-comparisons&lt;/a&gt; but the answer presents multiple comparison test with parametric survival not with non parametric (as in my case)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-25T15:50:29.100" Id="31077" LastActivityDate="2014-09-08T18:07:07.227" OwnerUserId="6015" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;nonparametric&gt;&lt;multiple-comparisons&gt;&lt;survival&gt;" Title="Kaplan-Meier multiple group comparisons" ViewCount="730" />
  <row AnswerCount="1" Body="&lt;p&gt;Participants were rated twice, with the 2 ratings separated by 3 years. For most participants the ratings were done by different raters, but for some (&amp;lt; 10%) the same rater performed both ratings. There were 8 raters altogether, with 2 doing ratings at both time points. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, since the ratings were of an aspect of ability with a hypothetical &quot;correct&quot; value, then absolute agreement between raters is of interest, rather than consistency. However, since the ratings were taken 3 years apart, there might have been (and probably was) some real change in the ability. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;What would be the best test of reliability in this case?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;I'm leaning towards an intra-class correlation, but is ICC1 the best I can do with these data?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-06-25T15:51:42.873" Id="31078" LastActivityDate="2012-06-26T06:38:42.290" LastEditDate="2012-06-26T06:38:42.290" LastEditorUserId="183" OwnerUserId="12197" PostTypeId="1" Score="7" Tags="&lt;reliability&gt;&lt;psychometrics&gt;&lt;inter-rater&gt;&lt;intraclass-correlation&gt;" Title="How to perform inter-rater reliability with multiple raters, different raters per participant, and possible changes over time?" ViewCount="974" />
  
  
  <row Body="&lt;p&gt;Exponential smoothing would be appropriate if the data tend to drift.  Since exponential smoothing is a special case of an ARIMA Model namely IMA(1,1) you probably should look at fitting ARIMA models and find  a &quot;best fitting&quot; model.  That could turn out to be something different from exponetial smotthing and as Rob suggests you have a long enough series to find out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-25T22:39:55.347" Id="31105" LastActivityDate="2012-06-25T22:39:55.347" OwnerUserId="11032" ParentId="31073" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I've done something similar for visualizing similar rankings.  The method I used gave me a quick snapshot of how the rankings related-nothing more.  My solution used Excel 2010 sparklines to create a small-multiples view of the rankings (this can be done in other Excel versions, but it takes a bit more work).  Also, I generally use Excel's Table functionality just to speed things along.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Transpose your matrix so the rows hold all the values per rated item.&lt;/li&gt;&#10;&lt;li&gt;If rank 1 is best, invert your ranks so 1=8,2=7... this just helps you visualize the chart's columns (rather than blank space) as better.&lt;/li&gt;&#10;&lt;li&gt;Sum the results of each question's raters.  Then sort by this total.  This will put the item with the best overall scores first and worst overall scores last.  This ranking will help you visually rank the charts when the overall pattern may not be obvious (as was the case in your sample data.&lt;/li&gt;&#10;&lt;li&gt;Insert Sparklines using your ranking data (without the totals column) next to your data table.  Visually, the more/bigger bars there are, the better the overall ranking. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There's nothing particularly analytical about this approach, but it's a pretty quick way to visualize the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/n0zZn.png&quot; alt=&quot;Subjective Rankings&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, if you don't have Excel 2010, you can create stripped down, cell sized column charts for each row that look about the same.  Or, you can use a third-party add-on to create them.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:  Table and Chart utilizing Gung's suggestion for average measure.  As pointed out, since the scale is similar, it was added to the chart as an additional point of comparison (I used gray to help differeniate it from the raw data plots).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RQgVi.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-26T12:01:10.097" Id="31125" LastActivityDate="2012-06-26T17:42:42.993" LastEditDate="2012-06-26T17:42:42.993" LastEditorUserId="10217" OwnerUserId="10217" ParentId="30995" PostTypeId="2" Score="5" />
  <row AnswerCount="5" Body="&lt;p&gt;I want to do a survey to determine how satisfied the (16) employees are with the company training program. The survey has 30 questions, each using a 5 point Likert scale for responses. The questions are divided into different groups (9 for the utility of the program, 6 regarding the trainer, 7 regarding the balance of the program, 2 about training content, 4 about the training facilities, and 2 about the implementation of training. I have calculated the mean, mode, frequency, percent and score of each question, but I need a result for a group of questions. Specifically, a result for the 9 questions regarding the utility of the program. I am currently thinking of using the mean or score of the group of the questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;What else I can do with this data? Please give suggestions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-26T15:36:13.207" FavoriteCount="2" Id="31139" LastActivityDate="2012-06-27T21:17:37.440" LastEditDate="2012-06-26T16:47:41.383" LastEditorUserId="11779" OwnerUserId="12215" PostTypeId="1" Score="3" Tags="&lt;mean&gt;&lt;survey&gt;&lt;likert&gt;" Title="Likert scale question divided into different group. How to calculate mean of different group?" ViewCount="8990" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;This question is sort of general and long-winded, but please bear with me.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my application, I have many datasets, each consisting of ~20,000 datapoints with ~50 features and a single dependent binary variable. I am attempting to model the datasets using regularized logistic regression (R package &lt;a href=&quot;http://cran.r-project.org/web/packages/glmnet/glmnet.pdf&quot;&gt;glmnet&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;As part of my analysis, I've created residual plots as follows. For each feature, I sort the datapoints according to the value of that feature, divide the datapoints into 100 buckets, and then compute the average output value and the average prediction value within each bucket. I plot these differences. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example residual plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/GXKYq.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the above plot, the feature has a range of [0,1] (with a heavy concentration at 1). As you can see, when the feature value is low, the model appears to be biased towards overestimating the likelihood of a 1-output. For example, in the leftmost bucket, the model overestimates the probability by about 9%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Armed with this information, I'd like to alter the feature definition in a straightforward manner to roughly correct for this bias. Alterations like replacing&lt;/p&gt;&#10;&#10;&lt;p&gt;$x \rightarrow \sqrt{x}$&lt;/p&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;p&gt;$x \rightarrow f_a(x) = \cases{a &amp;amp; if $x&amp;lt;a$ \cr x &amp;amp; else}$&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I do this? I'm looking for a general methodology so that a human could quickly scroll through all ~50 plots and make alterations, and do this for all datasets and repeat often to keep models up-to-date as the data evolves over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a general question, is this even the right approach? Google searches for &quot;logistic regression residual analysis&quot; don't return many results with good practical advice. They seem to be fixated on answering the question, &quot;Is this model a good fit?&quot; and offer various tests like Hosmer-Lemeshow to answer. But I don't care about whether my model is good, I want to know how to make it better!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-26T19:45:36.973" FavoriteCount="2" Id="31166" LastActivityDate="2012-06-27T15:37:47.507" LastEditDate="2012-06-27T15:37:47.507" LastEditorUserId="88" OwnerUserId="2221" PostTypeId="1" Score="8" Tags="&lt;logistic&gt;&lt;residuals&gt;" Title="Logistic regression residual analysis" ViewCount="1234" />
  <row Body="&lt;pre&gt;&lt;code&gt;A1 = matrix(data=c(-0.4, 0.4, 0.15, 0.3, -0.55, 0.6, 0.1, 0.15, -0.75), nrow=3, ncol=3, byrow=TRUE)&#10;A2&amp;lt;-eigen(A)$vector&#10;A%*%A2[,3]&#10;              [,1]&#10;[1,]  2.461139e-17&#10;[2,] -1.083796e-16&#10;[3,]  3.932943e-17&#10;1&amp;gt; A2[,3]&#10;[1] 0.7298860 0.6450156 0.2263212&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I suppose you mean the norm of the variables should be 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1&amp;gt; A2[,3]/sum(A2[,3])&#10;[1] 0.4558304 0.4028269 0.1413428&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2012-06-26T20:13:00.177" Id="31168" LastActivityDate="2012-06-26T20:13:00.177" OwnerUserId="603" ParentId="31164" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;What I would like to know it is the precise use of both statistics -- the sample mean and the mean of various samples. I'm getting a bit confused with the notation in my textbook and sometimes I cannot make a clear distinction between both statistics. For instance, when we talk about &quot;Xi&quot; (i=1,2...) aleatory variables from one population that approaches a normal distribution, I can assume we are talking about various samples from one population that approaches a normal distribution? I would be glad if someone could give me a clear difference in the use of both statistics (it would be nice a distinction of the variance in both approaches too) and its properties.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks o/ &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-26T20:46:20.390" Id="31171" LastActivityDate="2013-02-22T00:23:36.650" OwnerUserId="12223" PostTypeId="1" Score="3" Tags="&lt;sampling&gt;&lt;mathematical-statistics&gt;" Title="About sample mean and the mean of various samples" ViewCount="108" />
  
  
  <row Body="&lt;p&gt;I believe the correct approach here is to compare the fit of a model where IV(a) and IV(b) are allowed to vary - that is, your present model - with the fit of a model where IV(a) and IV(b) are fit to the same value (in which case the mediator is just an average of the two).  The two models can be compared using a Chi-Square difference test.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This is simple enough to be performed by hand - I am not quite sure how to do that last, final step in SPSS.  But all of the requisite values for calculating the Chi-Square difference will be available to you in SPSS, and there are several online calculators that could be used for determining the value of this test statistic and its p-value. I hope that helps!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-26T23:32:08.630" Id="31185" LastActivityDate="2012-06-26T23:32:08.630" OwnerUserId="12229" ParentId="31116" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The key assumption in the MNL is that the errors are independently and identically distributed with a Gumbel extreme value distribution. The problem with &lt;em&gt;testing&lt;/em&gt; this assumption is that it is made &lt;em&gt;a priori&lt;/em&gt;. In standard regression you fit the least-squares curve, and measure the residual error. In a logit model, you assume that the error is already in the measurement of the point, and compute a likelihood function from that assumption.&lt;/p&gt;&#10;&#10;&lt;p&gt;An important assumption is that the sample be exogenous. If it is choice-based, there are corrections that need to be employed.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as assumptions on the model itself, &lt;a href=&quot;http://elsa.berkeley.edu/books/choice2nd/Ch03_p34-75.pdf&quot; rel=&quot;nofollow&quot;&gt;Train&lt;/a&gt; describes three:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Systematic, and non-random, taste variation.&lt;/li&gt;&#10;&lt;li&gt;Proportional substitution among alternatives (a consequence of the IIA property).&lt;/li&gt;&#10;&lt;li&gt;No serial correlation in the error term (panel data).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The first assumption you mostly just have to defend in the context of your problem. The third is largely the same, because the error terms are purely random.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second is testable to a certain extent, however. If you specify a nested logit model, and it turns out that the inter-nest substitution pattern is entirely flexible ($\lambda = 1$) then you could have used the MNL model, and the IIA assumption is valid. But remember that the log-likelihood function for the nested logit model has local maxima, so you should make sure that you get $\lambda =1$ consistently.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as doing any of this in SPSS, I can't help you other than suggest you use the &lt;code&gt;mlogit&lt;/code&gt; package in R instead. Sorry.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-27T01:08:09.217" Id="31187" LastActivityDate="2012-06-27T01:55:50.183" LastEditDate="2012-06-27T01:55:50.183" LastEditorUserId="10026" OwnerUserId="10026" ParentId="30959" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to use a 5-item subscale to measure the effects of a behavioural intervention program on the use of behavioural strategies. I obtain significant results when I analyse it in models however the reliability of this measure is very low at .5 and the mean inter-item correlation is below .2. Can I use this measure or should I exclude it from all subsequent analyses?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-27T07:01:28.100" Id="31197" LastActivityDate="2015-02-16T21:23:18.863" OwnerUserId="11560" PostTypeId="1" Score="3" Tags="&lt;reliability&gt;" Title="Can I use a measure when it has low reliability?" ViewCount="2058" />
  <row AnswerCount="0" Body="&lt;p&gt;we have a process in which, at each step, a set of elements are presented to user, the user choses one, his choice is recorded and next round starts with a new set of elements.&lt;br&gt;&#10;For example:&lt;br&gt;&#10;1. {20,50,80} and the user chose 80&lt;br&gt;&#10;2. {80,110,140} and the user chose 80 again&lt;br&gt;&#10;3. ...&lt;/p&gt;&#10;&#10;&lt;p&gt;We want to verify the hypothesis whether the user prefers large elements.&#10;You can assume we have many steps.&lt;/p&gt;&#10;&#10;&lt;p&gt;How does the following suggestion fare ?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;at each step compute MEAN, and STD of the presented values.&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;normalize elements and the choice to be in standard units, i.e.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; standard_value = (value-MEAN) / STD&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for example: in (1) above the used chose -1 and in (2) he chose +1&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;count how many times each standard unit is chosen and draw a scatter plot.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So for example we'll plot points like: (-3.5,0), (-1.3, 2), (0,4), (1,7), (2,10), &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this too simplistic ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-27T08:18:15.687" Id="31198" LastActivityDate="2013-02-12T11:39:45.203" LastEditDate="2013-02-12T11:39:45.203" LastEditorUserId="12235" OwnerUserId="12235" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;bayesian&gt;&lt;predictive-models&gt;&lt;model-selection&gt;" Title="Analysing choices pattern" ViewCount="54" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two different sources for order / purchase related data, one is at individual (purchase) level, the other is aggregated daily. Both sources contain different &lt;em&gt;types&lt;/em&gt; of data points, albeit all related to purchases. I want to use this data to model changes in purchase patterns. My question is can I use individual vs daily records as seperate levels? Does it make sense to use time itself as a level in a hierarchy? Until now I've mostly seen schools, states, cities, etc. being used as levels, I never saw examples using time. &lt;/p&gt;&#10;&#10;&lt;p&gt;The alternative, I guess, is to summarize points in the specific source so it becomes daily, but I am guessing I could be losing some valueable information that way. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas would be appreciated, &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-27T10:56:43.513" Id="31207" LastActivityDate="2012-06-27T15:36:10.050" LastEditDate="2012-06-27T15:36:10.050" LastEditorUserId="88" OwnerUserId="9577" PostTypeId="1" Score="4" Tags="&lt;multilevel-analysis&gt;" Title="Using days and months as levels in hierarchy" ViewCount="39" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have seen many places where they have input/output datasets where they first create a linear regression line, correct the bias, and then only use that data for their model. I didn't get what this bias correction is?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-27T21:54:51.950" FavoriteCount="1" Id="31253" LastActivityDate="2014-08-25T06:01:02.757" LastEditDate="2012-06-28T11:46:03.467" LastEditorUserId="930" OwnerUserId="11475" PostTypeId="1" Score="4" Tags="&lt;bias&gt;" Title="What is bias correction?" ViewCount="4592" />
  <row Body="&lt;p&gt;One very important thing to remember is that multiple testing correction assumes independent tests. If the data your analyzing isn't independent, things get a little more complicated than simply correcting for the number of tests performed, you have to account for the correlation between the data being analyzed or your correction will probably be way too conservative and you will have a high type II error rate. I've found cross-validation, permutation tests, or bootstrapping can be effective ways to deal with multiple comparisons if used properly. Others have mentioned using FDR, but this can give incorrect results if there's a lot of non-independence in your data as it assumes p-values are uniform across all tests under the null. The distribution of p-values across tests under the null can be very skewed if a lot of non-independence exists.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-28T03:28:38.520" Id="31261" LastActivityDate="2012-06-28T03:28:38.520" OwnerUserId="12261" ParentId="16779" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Unfortunately, the maximum likelihood estimate of the rate parameter for a Poisson process that's sampled over a predetermined interval $T$ does not have a finite mean (or higher moments).  This is because there's a nonzero probability of seeing no arrivals ($k=0$) in $T$, leading to an estimate of $T/0$.  We can fix that problem in any number of ways, an obvious one being to use $\max\{k, c\}$ instead of $k$ where $c$ is some constant such as 1/2.  The estimate is no longer maximum likelihood, nor is it unbiased, but at least it has moments of all order, and it is consistent. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also unfortunately, we can't really describe an estimator as &quot;unbiased given $k=1$&quot;, as after we've observed the data, there's no randomness left (since $T$ is fixed also.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-28T04:12:42.050" Id="31263" LastActivityDate="2012-06-28T04:12:42.050" OwnerUserId="7555" ParentId="31138" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Practical concerns like memory and processor time aside, I can't imagine any situation where having &lt;em&gt;more&lt;/em&gt; representative training data leads to a worse outcome. Overfitting is essentially learning spurious correlations that occur in your training data, but not the real world. For example, if you considered only my colleagues, you might learn to associate &quot;named Matt&quot; with &quot;has a beard.&quot; It's 100% valid ($n=4$, even!), but it's obviously not true in general. Increasing the size of your data set (e.g., to the entire building or city) should reduce these spurious correlations and improve the performance of your learner.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, one situation where more data does not help---and may even hurt---is if your additional training data is noisy or doesn't match whatever you are trying to predict. I once did an experiment where I plugged different language models[*] into a voice-activated restaurant reservation system. &lt;/p&gt;&#10;&#10;&lt;p&gt;I varied the amount of training data as well as its relevance: at one extreme, I had a small, carefully curated collection of people booking tables, a perfect match for my application. At the other, I had a model estimated from &lt;em&gt;huge&lt;/em&gt; collection of classic literature, a more accurate language model, but a much worse match to the application. To my surprise, the small-but-relevant model &lt;strong&gt;vastly&lt;/strong&gt; outperformed the big-but-less-relevant model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, I don't think there are any hard and fast rules for this sort of trade-off. You'll have to try it and see how it works.&lt;/p&gt;&#10;&#10;&lt;p&gt;[*]A language model is just the probability of seeing a given sequence of words e.g. $P(w_n = \textrm{'quick', } w_{n+1} = \textrm{'brown', } w_{n+2} = \textrm{'fox'})$. They're vital to building halfway decent speech/character recognizers. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-28T05:42:36.277" Id="31264" LastActivityDate="2012-06-28T05:42:36.277" OwnerUserId="7250" ParentId="31249" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Do you really need the maximum likelihood estimate?  Can you get away with the Bayesian estimator of the intensity of the Poisson process?  For $k$ observed events in a period of length $T$, the likelihood over the intensity is gamma-distributed with shape $k$ and rate $T$.  This is correct even if $k$ is one or zero (although you might want to introduce a gamma-distributed prior).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-28T05:44:23.353" Id="31265" LastActivityDate="2012-06-28T05:44:23.353" OwnerUserId="858" ParentId="31138" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;$D(f_n\|\phi)\rightarrow 0$ assures that there's no &quot;distance&quot; between the distribution of the sum of random variables and the gaussian density as $n\rightarrow\infty$ just because of the definition of K-L divergence, so it's the proof itself. Perhaps I misunderstood your question.&lt;/p&gt;&#10;&#10;&lt;p&gt;About the second point as you appointed, it's responded in your paragraph.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-28T07:01:55.053" Id="31268" LastActivityDate="2012-07-28T14:53:38.207" LastEditDate="2012-07-28T14:53:38.207" LastEditorUserId="10849" OwnerUserId="12264" ParentId="31266" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Why are you calling this ANCOVA?  If it were an ANCOVA then you would be adding in some covariate that explains variability in the response variable but that is not a predictor in the model.  That covariate would not require a linear relationship.  This is just a 2-way regression with an interaction.  From your description it's not even clear why you would want an ANCOVA.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to see if the slopes depend upon the group you've already got that.  There is no interaction between group and slope, therefore slope does not depend on group.&lt;/p&gt;&#10;&#10;&lt;p&gt;How did you assess normality?  You want to do it on the residuals, not on the data.  Try&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(ancova)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These plots will help you assess whether the data meet the assumptions of the regression.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-06-28T07:57:04.937" Id="31269" LastActivityDate="2012-06-28T14:35:55.440" LastEditDate="2012-06-28T14:35:55.440" LastEditorUserId="601" OwnerUserId="601" ParentId="31262" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;CART stands for &lt;em&gt;Classification And Regression Trees&lt;/em&gt;.  This is a technique for developing a tree model (T) to predict categories (C) and/or continuous values (R) by recursive partitioning. It does not make restrictive parametric assumptions. &lt;/p&gt;&#10;&#10;&lt;p&gt;(Note that &quot;CART&quot; is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Synecdoche&quot; rel=&quot;nofollow&quot;&gt;synecdoche&lt;/a&gt; for the general data mining technique of using &lt;a href=&quot;http://en.wikipedia.org/wiki/Decision_tree_learning&quot; rel=&quot;nofollow&quot;&gt;decision trees&lt;/a&gt; to predict outcomes.  Strictly speaking, &quot;CART&quot; refers to a specific algorithm for forming trees that was popularized by the work of Leo Breiman.  However, CART is commonly used to refer to &lt;em&gt;any&lt;/em&gt; predictive tree algorithm, and the tag may be used similarly on Cross Validated.)  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-28T14:15:14.173" Id="31287" LastActivityDate="2014-12-31T18:23:16.090" LastEditDate="2014-12-31T18:23:16.090" LastEditorUserId="919" OwnerUserId="7290" PostTypeId="5" Score="0" />
  
  
  <row AcceptedAnswerId="31332" AnswerCount="1" Body="&lt;p&gt;I have a dependent variable made up of 3 categories and 14 binary predictor variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried using &lt;code&gt;mlogit&lt;/code&gt; and &lt;code&gt;nnet/multinom&lt;/code&gt; packages in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there a better approach than multinomial logistic regression for this particular scenario?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-06-28T17:14:49.807" FavoriteCount="2" Id="31299" LastActivityDate="2012-06-29T14:50:55.993" LastEditDate="2012-06-29T14:50:55.993" LastEditorUserId="5880" OwnerUserId="3591" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;multinomial&gt;" Title="Whether to use multinomial logistic regression with a three-category outcome variable and many binary predictors?" ViewCount="1186" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm a bit of a stats newbie so take it easy on me if this ends up being somehow trivial. I'm working on a problem that involves parameter estimation for the sum of two independent gamma distributions (which do not necessarily have the same scale or shape parameters.) I'm interested in what would be the most convenient way to numerically estimate the parameters of the model. I'm primarily interested in approximating MLEs (though I would be interested in hearing alternative methods.) The convolution expression for the density seems a bit messy, and while I found a &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=3&amp;amp;ved=0CFoQFjAC&amp;amp;url=http%3A%2F%2Fwww.math.utep.edu%2FFaculty%2Fmoschopoulos%2FPublications%2F1985-The_Distribution_of_the_Sum_of_Independent_Gamma_Random_Variables.pdf&amp;amp;ei=myXtT_imHMfM2gWs3eCCAg&amp;amp;usg=AFQjCNFG3m5zmnN9jwMKQd9cNZNXiAP7CA&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; giving an expression for the density in terms of an infinite series with a nice truncation error estimate, the objective function is still pretty messy. Particularly, I'm curious to know where to start looking in terms of a) asymptotic approximation for the MLE and b) software that is well-suited for this type of application. Any links or references that you find useful will be most appreciated. Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-06-29T03:52:40.727" Id="31333" LastActivityDate="2012-06-29T08:50:00.220" LastEditDate="2012-06-29T08:50:00.220" LastEditorUserId="88" OwnerUserId="11837" PostTypeId="1" Score="3" Tags="&lt;estimation&gt;&lt;maximum-likelihood&gt;&lt;software&gt;&lt;asymptotics&gt;" Title="Parameter estimation for the sum of two Independent (not necessarily i.d.) Gamma RVs" ViewCount="167" />
  <row Body="&lt;p&gt;The learning rate is a parameter that determines how much an updating step influences the current value of the weights. While weight decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled.&lt;/p&gt;&#10;&#10;&lt;p&gt;So let's say that we have a cost or error function $E(\mathbf{w})$ that we want to minimize. Gradient descent tells us to modify the weights $\mathbf{w}$ in the direction of steepest descent in $E$:&#10;\begin{equation}&#10;w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i},&#10;\end{equation}&#10;where $\eta$ is the learning rate, and if it's large you will have a correspondingly large modification of the weights $w_i$ (in general it shouldn't be too large, otherwise you'll overshoot the local minimum in your cost function).&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to effectively limit the number of free parameters in your model so as to avoid over-fitting, it is possible to regularize the cost function. An easy way to do that is by introducing a zero mean Gaussian prior over the weights, which is equivalent to changing the cost function to $\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\frac{\lambda}{2}\mathbf{w}^2$. In practice this penalizes large weights and effectively limits the freedom in your model. The regularization parameter $\lambda$ determines how you trade off the original cost $E$ with the large weights penalization.&lt;/p&gt;&#10;&#10;&lt;p&gt;Applying gradient descent to this new cost function we obtain:&#10;\begin{equation}&#10;w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i}-\eta\lambda w_i.&#10;\end{equation}&#10;The new term $-\eta\lambda w_i$ coming from the regularization causes the weight to decay in proportion to its size.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-29T03:58:39.660" Id="31334" LastActivityDate="2012-06-29T03:58:39.660" OwnerUserId="12290" ParentId="29130" PostTypeId="2" Score="20" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two sets of equal sized and normally distributed data, which are output from two different experiments. I wish to do a two-sample t-test with these two data sets to see if the results from the two experiments agree.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is it appropriate to mean (or median) centralize the data sets before doing the t-test?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The variance of the data are equal and passes the var.test. My understanding is that if I centralize the data and the mean will be the same for the two data sets and the result of a t-test will always favor the null hypothesis of equal means.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can the same thing be said about centralizing data or not for a one-way ANOVA if I have more than two datasets?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-29T05:37:48.590" Id="31338" LastActivityDate="2012-07-15T19:24:23.023" LastEditDate="2012-07-15T19:24:23.023" LastEditorUserId="930" OwnerUserId="12292" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;" Title="Two sample t-test with mean centralized data" ViewCount="145" />
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471584959&quot; rel=&quot;nofollow&quot;&gt;Johnson, Kotz, and Balakrishnan&lt;/a&gt; describe several estimators of the parameters of the Pareto distribution, including the linear regression one, method of moments, and MLE, and note that the MLE is the uniform minimum variance estimator of the two parameters jointly, and is unbiased.  On the other hand, a technique which is specifically designed to fit tails well will naturally tend to fit the tails better than a technique which fits the entire distribution weighted by likelihood (so to speak.)  What estimator is best for you depends upon your loss associated with misestimating the distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-29T15:14:28.270" Id="31368" LastActivityDate="2012-06-29T15:14:28.270" OwnerUserId="7555" ParentId="31352" PostTypeId="2" Score="3" />
  
  
  
  
  <row AcceptedAnswerId="31409" AnswerCount="1" Body="&lt;p&gt;I'm trying to build a recommendation system, and have a bunch of (item,item_features,liked) triplets, where liked is binary.  Most items are not liked.  So I'm running a logistic regression with glmnet of the form &#10;    &lt;code&gt;liked ~ item_features&lt;/code&gt;&#10;This yields an AUC of around 0.75 (it doesn't vary much with the regularization parameter).  However, the error rate (also doesn't vary much) is only a tiny,tiny bit better than what you would get if you just always predicted &quot;don't like.&quot;  What is the best way (or any way, really!) to think about the value or lack thereof this recommender?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-06-29T21:46:03.300" Id="31408" LastActivityDate="2012-09-07T08:27:58.107" LastEditDate="2012-08-29T11:39:03.310" LastEditorUserId="264" OwnerUserId="12090" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;interpretation&gt;&lt;recommender-system&gt;&lt;auc&gt;" Title="AUC vs error rate for classification" ViewCount="240" />
  <row AcceptedAnswerId="31421" AnswerCount="1" Body="&lt;p&gt;These two functions exist in R but I don't know their differences. It seems that they only return the same p-values when calling &lt;code&gt;wilcox.test&lt;/code&gt; with &lt;code&gt;correct=FALSE&lt;/code&gt;, and &lt;code&gt;wilcox_test&lt;/code&gt; (in the coin package) with &lt;code&gt;distribution=&quot;aymptotic&quot;&lt;/code&gt;. For other values they return different p-values. Also &lt;code&gt;wilcox.test&lt;/code&gt; is always returning W=0 for my dataset, independently of the settings of its parameters: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;x = c(1, 1, 1, 3, 3, 3, 3)&lt;/code&gt; and &lt;code&gt;y = c(4, 4, 6, 7, 7, 8, 10)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, when I try using different tools other than R (some available online, others as Excel add-ons), sometimes they report different p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;So how can I know which tool is giving the &quot;correct&quot; p-value? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a &quot;correct&quot; p-value, or if a few tools give a p-value &amp;lt; 0.05 should I be happy? (Sometimes these tools do not offer so many parametrization possibilities like R.)&lt;/p&gt;&#10;&#10;&lt;p&gt;What am I missing here? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-30T00:51:58.897" Id="31417" LastActivityDate="2012-06-30T16:55:19.080" LastEditDate="2012-06-30T16:55:19.080" LastEditorUserId="7290" OwnerUserId="12218" PostTypeId="1" Score="11" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;p-value&gt;&lt;wilcoxon&gt;" Title="What is the difference between wilcox.test and wilcox_test in R" ViewCount="1861" />
  <row Body="&lt;p&gt;I'm pretty sure you are using wrong tools here.&lt;/p&gt;&#10;&#10;&lt;p&gt;ML methods are created for interpolation (like predicting time series A from time series B and C); for extrapolations we have Markov chains and friends.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with your approach is that it is terribly easy to overfit the model in this conditions and, what's worse, it is hard to spot this (normal cross-validation will fail, so it is very hard to fit parameters the proper way, etc.).&lt;br&gt;&#10;Adding explicit time to predictors is also a bad idea -- I have seen models fitted &lt;em&gt;only&lt;/em&gt; on time and decision with 90% accuracy on cross-validation and random guessing on post-training-data tests. If you need time, it is better to include it as a series of cycle descriptors like day of week or seconds past midnight, obviously never exceeding or even going near the length of your training series.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-06-30T10:00:51.153" Id="31429" LastActivityDate="2012-06-30T10:00:51.153" OwnerUserId="88" ParentId="31387" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;How are you fitting the regression equation?  Knowing that will help us to help you.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are doing the regression by hand, then use the formula in the book that gave you the formula for the regression (or use the wikipedia article in the comments above).  Or better yet, get a real statistical software package to help you.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are using Excel or another spreadsheet then you should really switch to a real statistics program.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are using a statistics program then it may have an option or command that will compute the interval for you (but we don't know what program you are using, so we can't tell you what that command is).  Even if the software does not compute the interval for you, it may give you the proper standard errors that you just need to multiply by the proper table value and add and subtract from the coefficient estimate (that formula should be in your textbook or on the wikipedia article).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-30T19:29:27.873" Id="31444" LastActivityDate="2012-06-30T19:29:27.873" OwnerUserId="4505" ParentId="31442" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: How can I construct a test to determine if the observed &quot;mountain&quot;-allele frequency (Fig 1) is significantly lower in the central to southern mountains than predicted (Fig 2) by the ecological selection model (&lt;em&gt;see below for details&lt;/em&gt;)? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: My initial thought was to regress the model residuals against latitude:longitude and altitude (which results in only the interaction between latitude and longitude as significant). The problem is that the residuals (Fig 3) may reflect variation not explained by the model and/or that their is something biological happening, e.g. allele has not had time to spread south to its potential or there is some barrier to gene flow. If you compare the observed (Fig1) versus expected (Fig 2) mountain-allele frequencies there is an obvious difference especially in the central to southern mountains of Sweden and Norway. I accept that the model may not explain all of the variation, but can I come up with a reasonable test to explore the idea that mountain-allele has not reached its potential in the central to southern mountains?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: I have an bi-allelic AFLP marker whose frequency distribution appears to be associated with mountain (and latitude:longitude) versus lowland habitats on the Scandinavian peninsula (Fig 1). The &quot;mountain&quot;-allele is nearly fixed in the north, which is mountainous. It is nearly absent or fixed for the &quot;lowland&quot;-allele in the south, which lacks mountains. As one moves north to south in the mountains the &quot;mountain&quot;-allele occurs in lower frequency. This difference in the &quot;mountain&quot;-allele frequency from north to south may simply be due to phylogeography or historical processes, as the region was colonized from both the north and the south. For example, if the mountain-allele originates in the northern population, maybe it has not had time to expand fully into the southern population, assuming there are no barriers to gene flow (the northern and southern populations are panmictic for mtDNA and 12 microsatellites).&lt;/p&gt;&#10;&#10;&lt;p&gt;My working hypothesis is that the &quot;mountain&quot;-allele frequency is a result of ecological selection (null hypothesis is neutral selection).&lt;/p&gt;&#10;&#10;&lt;p&gt;For my ecological selection model, I have used a generalized additive model (GAM) with the binomial allele &lt;em&gt;frequency&lt;/em&gt; as the response variable (129 sites sampled across Fennoscandinavia with typically 10 to 20 individuals sampled at each site) and several climatic and growing season variables as the predictor variables. The model results are as follows (TMAX04-06 = max temperature in April-June, Phen_NPPMN = mean growing season vegetation productivity, PET_HE_YR = annual potential evapotranspiration, Dist_Coast = distance to coast):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Family: binomial &#10;Link function: logit &#10;&#10;Formula: Binomial_WW1 ~ s(TMAX_04) + s(TMAX_05) + s(TMAX_06) + s(Phen_NPPMN) + &#10;s(PET_HE_YR) + s(Dist_Coast)&#10;&#10;Parametric coefficients:&#10;             Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -0.74372    0.04736   -15.7   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Approximate significance of smooth terms:&#10;             edf Ref.df Chi.sq  p-value    &#10;s(TMAX_04)    3.8100  4.812 25.729 9.43e-05 ***&#10;s(TMAX_05)    0.8601  1.000  5.887  0.01526 *  &#10;s(TMAX_06)    0.8862  1.000  7.644  0.00569 ** &#10;s(Phen_NPPMN) 6.2177  7.375 39.028 3.16e-06 ***&#10;s(PET_HE_YR)  3.1882  4.147 18.039  0.00145 ** &#10;s(Dist_Coast) 2.2882  2.857  9.725  0.01906 *  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;R-sq.(adj) =  0.909   Deviance explained = 89.7%&#10;REML score = 326.73  Scale est. = 1         n = 129&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/uJO5H.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Figure 1. Observed &quot;mountain&quot;-allele frequency for bi-allelic AFLP marker. Contour lines 0.1 frequency intervals, color-shading is altitude with blues for lowest and reds highest.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sDN5x.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Figure 2. Predicted&quot;mountain&quot;-allele frequency for bi-allelic AFLP marker. Contour lines 0.1 frequency intervals, color-shading is altitude with blues for lowest and reds highest.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nOv1P.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Figure 3. Ecological selection model (using GAM) residuals broken down by the entire study area (Fennoscandinavia) and separately for Norway, Sweden and Finland. The red dashed lines represent a secondary contact zone between the northern and southern populations inferred from other AFLP markers and stable isotope analysis of feathers grown on their separate wintering grounds in Africa. The thin black dotted line is the center of the zone.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-06-30T20:46:23.953" Id="31449" LastActivityDate="2015-02-17T02:57:30.997" LastEditDate="2012-07-10T14:53:05.613" LastEditorUserId="7928" OwnerUserId="7928" PostTypeId="1" Score="8" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;residuals&gt;&lt;general-additive-model&gt;" Title="Is the observed allele frequency significantly less than the predicted?" ViewCount="313" />
  
  
  
  <row Body="&lt;p&gt;If you want to generate events from a Poisson process by generating interarrival times  you choose X such that $F(X)=U$ where $U$ is uniform on $U[0,1]$ and $F(X)=1-\exp(-\lambda X)$.  So you set $U=1-\exp(-\lambda X)$ or $1-U=\exp(-\lambda X)$ or $-\log(1-U)/\lambda =X$.  Generate $10000$ $X's$ this way and then  count how many $X's$ are less than $60$ seconds.  I don't think that is what your code is doing.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-01T21:00:46.847" Id="31482" LastActivityDate="2012-07-02T09:39:04.267" LastEditDate="2012-07-02T09:39:04.267" LastEditorDisplayName="user10525" OwnerUserId="11032" ParentId="31478" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;After a bit og Googling I found the following package which does the trick:&#10;&lt;a href=&quot;http://cran.r-project.org/web/packages/ChoiceModelR/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/ChoiceModelR/index.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-02T06:12:16.357" Id="31497" LastActivityDate="2012-07-02T06:12:16.357" OwnerUserId="2704" ParentId="31279" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Very interesting question, I'll have to read the papers you give... But maybe this will start us in direction of an answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;I usually tackle this problem in a very pragmatic way: I iterate the k-fold cross validation with new random splits and calculate performance just as usual for each iteration. The overall test samples are then the same for each iteration, and the differences come from different splits of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;This I report e.g. as the 5th to 95th percentile of observed performance wrt. exchanging up to $\frac{n}{k} - 1$ samples for new samples and discuss it as a measure for model instability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Side note: I anyways cannot use formulas that need the sample size. As my data are clustered or hierarchical in structure (many similar but not repeated measurements of the same case, usually several [hundred] different locations of the same specimen) I don't know the effective sample size.&lt;/p&gt;&#10;&#10;&lt;h3&gt;comparison to bootstrapping:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;iterations use new random splits. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the main difference is resampling with (bootstrap) or without (cv) replacement. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;computational cost is about the same, as I'd choose no of iterations of cv $\approx$ no of bootstrap iterations / k, i.e. calculate the same total no of models.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;bootstrap has advantages over cv in terms of some statistical properties (asymptotically correct, possibly you need less iterations to obtain a good estimate)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;however, with cv you have the advantage that you are guaranteed that &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the number of distinct training samples is the same for all models (important if you want to calculate learning curves)&lt;/li&gt;&#10;&lt;li&gt;each sample is tested exactly once in each iteration&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;some classification methods will discard repeated samples, so bootstrapping does not make sense&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Variance for the performance&lt;/h3&gt;&#10;&#10;&lt;p&gt;short answer: yes it does make sense to speak of variance in situation where only {0,1} outcomes exist. &lt;/p&gt;&#10;&#10;&lt;p&gt;Have a look at the binomial distribution (k = successes, n = tests, p = true probability for success = average k / n):&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sigma^2 (k) = np(1-p)$&lt;/p&gt;&#10;&#10;&lt;p&gt;The variance of proportions (such as hit rate, error rate, sensitivity, TPR,..., I'll use $p$ from now on and $\hat p$ for the observed value in a test) is a topic that fills whole books...&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Fleiss: Statistical Methods for Rates and Proportions&lt;/li&gt;&#10;&lt;li&gt;Forthofer and Lee: Biostatistics has a nice introduction.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now, $\hat p = \frac{k}{n}$ and therefore:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sigma^2 (\hat p) = \frac{p (1-p)}{n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;This means that the uncertainty for measuring classifier performance depends only on the true performance p of the tested model and the number of test samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;In cross validation you assume &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;that the k &quot;surrogate&quot; models have the same true performance as the &quot;real&quot; model you usually build from all samples. (The breakdown of this assumption is the well-known pessimistic bias).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;that the k &quot;surrogate&quot; models have the same true performance (are equivalent, have stable predictions), so you are allowed to pool the results of the k tests.&lt;br&gt;&#10;Of course then not only the k &quot;surrogate&quot; models of one iteration of cv can be pooled but the ki models of i iterations of k-fold cv. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;h3&gt;Why iterate?&lt;/h3&gt;&#10;&#10;&lt;p&gt;The main thing the iterations tell you is the model (prediction) instability, i.e. variance of the predictions of different models for the same sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can directly report instability as e.g. the variance in prediction of a given test case regardless whether the prediction is correct or a bit more indirectly as the variance of $\hat p$ for different cv iterations.&lt;/p&gt;&#10;&#10;&lt;p&gt;And yes, this is important information. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if your models are perfectly stable, all $n_{bootstrap}$ or $k \cdot n_{iter.~cv}$ would produce exactly the same prediction for a given sample. In other words, all iterations would have the same outcome. The variance of the estimate would not be reduced by the iteration (assuming $n - 1 \approx n$). In that case, assumption 2 from above is met and you are subject only to $\sigma^2 (\hat p) = \frac{p (1-p)}{n}$ with n being the total number of samples tested in all k folds of the cv.&lt;br&gt;&#10;In that case, iterations are not needed (other than for demonstrating stability). &lt;/p&gt;&#10;&#10;&lt;p&gt;You can then construct confidence intervals for the true performance $p$ from the observed no of successes $k$ in the $n$ tests. So, strictly, there is no need to report the variance uncertainty if $\hat p$ and $n$ are reported. However, in my field, not many people are aware of that or even have an intuitive grip on how large the uncertainty is with what sample size. So I'd recommend to report it anyways.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you observe model instability, the pooled average is a better estimate of the true performance. The variance between the iterations is an important information, and you could compare it to the expected minimal variance for a test set of size n with true performance average performance over all iterations.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-02T11:59:04.857" Id="31507" LastActivityDate="2012-07-04T12:05:56.530" LastEditDate="2012-07-04T12:05:56.530" LastEditorUserId="4598" OwnerUserId="4598" ParentId="31190" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;I don't think you're going to find what you're looking for.  First of all, there is no real concept of &quot;OOB Predictions&quot; for a full &lt;code&gt;gbm&lt;/code&gt; fit.  It does save the OOB decrease (or increase) in error after each tree, but that does not equate to an OOB prediction.  Since the trees are in sequence (boosted) instead of in parallel (bagged) there is no way to get &quot;untainted&quot; predictions for the training data.&lt;/p&gt;&#10;&#10;&lt;p&gt;It sound like you are actually looking for the Out-Of-Fold (OOF) predictions.  Calling gbm with Cross Validation enabled will make k+1 fits, but I don't think it saves anything other than the mean cross-validation error metric at each iteration.  I've moved away from using the internal cross-validation functionality for this reason.  I fold it (or bag it) myself and save the predictions from the folds.&lt;/p&gt;&#10;&#10;&lt;p&gt;And yes, these OOF predictions are valuable if you want to see untainted predictions of the training data or if you want to ensemble a gbm with another algorithm.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-02T13:40:08.037" Id="31511" LastActivityDate="2012-07-02T13:40:08.037" OwnerUserId="8120" ParentId="31509" PostTypeId="2" Score="3" />
  
  
  
  
  
  
  
  <row AcceptedAnswerId="31774" AnswerCount="2" Body="&lt;p&gt;I would love to perform a TukeyHSD post-hoc test after my two-way Anova with R, obtaining a table containing the sorted pairs grouped by significant difference. (Sorry about the wording, I'm still new with statistics.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to have something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2RhiS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So, grouped with stars or letters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea? I tested the function &lt;code&gt;HSD.test()&lt;/code&gt; from the &lt;code&gt;agricolae&lt;/code&gt; package, but it seems it doesn't handle two-way tables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-03T02:30:30.697" FavoriteCount="5" Id="31547" LastActivityDate="2014-10-29T16:08:28.757" LastEditDate="2012-07-03T19:39:19.167" LastEditorUserId="930" OwnerUserId="12339" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;anova&gt;&lt;multiple-comparisons&gt;&lt;post-hoc&gt;&lt;tukey-hsd&gt;" Title="How to obtain the results of a Tukey HSD post-hoc test in a table showing grouped pairs?" ViewCount="19511" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a problem with the &lt;code&gt;ezANOVA&lt;/code&gt; package using for rm ANOVA or a problem with my statistic design of the experiment (or  both ;-))&lt;/p&gt;&#10;&#10;&lt;p&gt;My file looks like this, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dput(head(reg,20))&#10;&#10;structure(list(sample = c(13L, 14L, 15L, 13L, 14L, 15L, 13L, &#10;14L, 15L, 13L, 14L, 15L, 13L, 14L, 15L, 13L, 14L, 15L, 1L, 2L&#10;), days = c(7L, 7L, 7L, 14L, 14L, 14L, 21L, 21L, 21L, 28L, 28L, &#10;28L, 35L, 35L, 35L, 42L, 42L, 42L, 7L, 7L), treat = structure(c(1L, &#10;1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, &#10;1L, 2L, 2L), .Label = c(&quot;asynchron&quot;, &quot;control&quot;, &quot;synchron&quot;), class = &quot;factor&quot;), &#10;repl = c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, &#10;1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L), disp = structure(c(2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L), .Label = c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;), class = &quot;factor&quot;), &#10;gbiov = c(205088809L, 236043290L, 182258079L, 356710654L, &#10;519036032L, 423556500L, 265916788L, 260495312L, 424304021L, &#10;456717288L, 412530454L, 495216491L, 360012083L, 246070292L, &#10;351623075L, 99075488L, 141565279L, 109303015L, 157719021L, &#10;275901505L), rich = c(15L, 15L, 15L, 12L, 12L, 10L, 10L, &#10;10L, 10L, 8L, 7L, 9L, 9L, 9L, 8L, 6L, 8L, 8L, 15L, 15L), &#10;evenness = c(0.810893754, 0.766130198, 0.82401103, 0.422734873, &#10;0.365797654, 0.406629574, 0.317890623, 0.434262493, 0.475385016, &#10;0.311819454, 0.307131166, 0.321269955, 0.365280751, 0.505205296, &#10;0.43643584, 0.484220445, 0.455471176, 0.423730082, 0.811921742, &#10;0.833306048)), .Names = c(&quot;sample&quot;, &quot;days&quot;, &quot;treat&quot;, &quot;repl&quot;, &#10;&quot;disp&quot;, &quot;gbiov&quot;, &quot;rich&quot;, &quot;evenness&quot;), row.names = c(NA, 20L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Again, my dataframe looks like (column sorted)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sample   days  treat    repl   disp   gbiov richness evenness&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have measured totalbiovolume(gbiov), richness and evenness over six weeks with a weekly sampling interval. I have 2x2 factorail design, where I have three treatments (treat): Synchron, asynchron and control and three different dispersal rate (disp) low, medium and high.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I decided to use a rm ANOVA by using &lt;code&gt;ezANOVA&lt;/code&gt;, because I took a sample every week from the same erlenmeyer flask. My within SSw is (treatment and disp) and between (days?). I used repl. column as the specifying case/Ss identifier for the replicates of each combination ($2\times 2$ design).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My input:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;test&amp;lt;-ezANOVA(data=reg, dv=.(gbiov), wid=.(repl), within=.(treat, disp, days))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Error message:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Warnung: Converting &quot;repl&quot; to factor for ANOVA.&#10;Warnung: &quot;days&quot; will be treated as numeric.&#10;Fehler in ezANOVA_main(data = data, dv = dv, wid = wid, within = within,  : &#10;One or more cells is missing data. Try using ezDesign() to check your data.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My problem in my dataframe is, that for the combination (asynchron $\times$ low) I have only three replicates (one is missing during the experiment periode), I got the advice to use &lt;code&gt;ezDesign&lt;/code&gt; to check my dataframe and after I did so, I add the missing replicates by reseting the missing value to zero for each time point, but still I get the same error message, except when I add days to the within-SSw and omit between-SSb. But I don´t know if that is correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps, the SS identifier makes no sence?! Or my design is wrong and I have no within and between conditions Or it is a problem of one missing replicate&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for solving the problem&lt;/p&gt;&#10;&#10;&lt;p&gt;Best,&#10;Nils&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-03T08:43:05.240" FavoriteCount="1" Id="31557" LastActivityDate="2012-07-03T09:38:02.963" LastEditDate="2012-07-03T09:38:02.963" LastEditorUserId="8507" OwnerUserId="12366" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="ezANOVA package in R: right method for my experimental design?" ViewCount="672" />
  
  <row Body="&lt;p&gt;Some suggestions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Choose the minimum class size, M, select the M nearest instances to the class means.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Find the maximum class size, N. Add Gaussian random noise to a random sample with replacement of N-Si instances for each class, other than the largest. S is the class size for the ith class.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For each class, except the smallest, run k-means clustering.  k is the number of instances in the smallest class. Use the cluster centers as the instances for the class.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;It may be worth mentioning that with option 2 you want to take care not to let the generated additive nose samples leak into the test set for your evaluation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-03T16:22:17.177" Id="31580" LastActivityDate="2012-07-05T11:25:57.507" LastEditDate="2012-07-05T11:25:57.507" LastEditorUserId="10065" OwnerUserId="10065" ParentId="31566" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Lets say you want to use Accuracy (or % correct) to evaluate &quot;optimal,&quot; and you have time to look at 25 values for k.  The following R code will answer your question using 15 repeats of 10-fold cross-validation.  It will also take a long time to run.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(caret)&#10;model &amp;lt;- train(&#10;    Species~., &#10;    data=iris, &#10;    method='knn',&#10;    tuneGrid=expand.grid(.k=1:25),&#10;    metric='Accuracy',&#10;    trControl=trainControl(&#10;        method='repeatedcv', &#10;        number=10, &#10;        repeats=15))&#10;&#10;model&#10;plot(model)&#10;&amp;gt; confusionMatrix(model)&#10;Cross-Validated (10 fold, repeated 15 times) Confusion Matrix &#10;&#10;(entries are percentages of table totals)&#10;&#10;            Reference&#10;Prediction   setosa versicolor virginica&#10;  setosa       33.3        0.0       0.0&#10;  versicolor    0.0       31.9       1.2&#10;  virginica     0.0        1.4      32.1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JAPq7.png&quot; alt=&quot;Accuracy&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So, by this criteria, I get an answer of 17, but it looks like the &quot;true&quot; value could lie anywhere between 5 and 20.  You can substitute &quot;Kappa&quot; or some other metric if you want, and add more cv-folds as well.  You can also try different methods of cross validation, such as leave-one-out, or bootstrap re-sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;/Edit: in response for your request for variety, I wrote this function to calculate a variety of metrics for multi-class problems:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Multi-Class Summary Function&#10;#Based on caret:::twoClassSummary&#10;require(compiler)&#10;multiClassSummary &amp;lt;- cmpfun(function (data, lev = NULL, model = NULL){&#10;&#10;  #Load Libraries&#10;  require(Metrics)&#10;  require(caret)&#10;&#10;  #Check data&#10;  if (!all(levels(data[, &quot;pred&quot;]) == levels(data[, &quot;obs&quot;]))) &#10;    stop(&quot;levels of observed and predicted data do not match&quot;)&#10;&#10;  #Calculate custom one-vs-all stats for each class&#10;  prob_stats &amp;lt;- lapply(levels(data[, &quot;pred&quot;]), function(class){&#10;&#10;    #Grab one-vs-all data for the class&#10;    pred &amp;lt;- ifelse(data[, &quot;pred&quot;] == class, 1, 0)&#10;    obs  &amp;lt;- ifelse(data[,  &quot;obs&quot;] == class, 1, 0)&#10;    prob &amp;lt;- data[,class]&#10;&#10;    #Calculate one-vs-all AUC and logLoss and return&#10;    cap_prob &amp;lt;- pmin(pmax(prob, .000001), .999999)&#10;    prob_stats &amp;lt;- c(auc(obs, prob), logLoss(obs, cap_prob))&#10;    names(prob_stats) &amp;lt;- c('ROC', 'logLoss')&#10;    return(prob_stats) &#10;  })&#10;  prob_stats &amp;lt;- do.call(rbind, prob_stats)&#10;  rownames(prob_stats) &amp;lt;- paste('Class:', levels(data[, &quot;pred&quot;]))&#10;&#10;  #Calculate confusion matrix-based statistics&#10;  CM &amp;lt;- confusionMatrix(data[, &quot;pred&quot;], data[, &quot;obs&quot;])&#10;&#10;  #Aggregate and average class-wise stats&#10;  #Todo: add weights&#10;  class_stats &amp;lt;- cbind(CM$byClass, prob_stats)&#10;  class_stats &amp;lt;- colMeans(class_stats)&#10;&#10;  #Aggregate overall stats&#10;  overall_stats &amp;lt;- c(CM$overall)&#10;&#10;  #Combine overall with class-wise stats and remove some stats we don't want &#10;  stats &amp;lt;- c(overall_stats, class_stats)&#10;  stats &amp;lt;- stats[! names(stats) %in% c('AccuracyNull', 'Prevalence', 'Detection Prevalence')]&#10;&#10;  #Clean names and return&#10;  names(stats) &amp;lt;- gsub('[[:blank:]]+', '_', names(stats))&#10;  return(stats)&#10;})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It's a doozy of a function, so it's going to slow down caret a bit, but I'd be very happy if you posted the results of your 1000 repeats of 10-fold CV (I have neither the time not the computational capacity to attempt this at present).  Here's my code for 15 repeats of 10-fold CV.  Note that you can easily modify this code to try other re-sampling methods, such as bootstrap sampling:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(caret)&#10;set.seed(19556)&#10;model &amp;lt;- train(&#10;  Species~., &#10;  data=iris, &#10;  method='knn',&#10;  tuneGrid=expand.grid(.k=1:30),&#10;  metric='Accuracy',&#10;  trControl=trainControl(&#10;    method='repeatedcv', &#10;    number=10, &#10;    repeats=15,&#10;    classProbs=TRUE,&#10;    summaryFunction=multiClassSummary))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Both &lt;a href=&quot;http://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve&quot;&gt;ROC&lt;/a&gt; and LogLoss seem to peak around 8:&#10;&lt;img src=&quot;http://i.stack.imgur.com/1z3vF.png&quot; alt=&quot;ROC&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/wAczt.png&quot; alt=&quot;logLoss&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;While &lt;a href=&quot;http://en.wikipedia.org/wiki/Specificity_%28statistics%29&quot;&gt;sensitivity and specificity&lt;/a&gt; seem to peak around 15:&#10;&lt;img src=&quot;http://i.stack.imgur.com/zO068.png&quot; alt=&quot;Sens&quot;&gt; &#10;&lt;img src=&quot;http://i.stack.imgur.com/Au38S.png&quot; alt=&quot;Spec&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's some code to output all the plots as a pdf:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dev.off()&#10;pdf('plots.pdf')&#10;for(stat in c('Accuracy', 'Kappa', 'AccuracyLower', 'AccuracyUpper', 'AccuracyPValue', &#10;              'Sensitivity', 'Specificity', 'Pos_Pred_Value', &#10;              'Neg_Pred_Value', 'Detection_Rate', 'ROC', 'logLoss')) {&#10;&#10;  print(plot(model, metric=stat))&#10;}&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you put a gun to my head, I'd probably say 8...&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-07-03T16:50:32.530" Id="31583" LastActivityDate="2013-03-05T18:25:13.303" LastEditDate="2013-03-05T18:25:13.303" LastEditorUserId="2817" OwnerUserId="2817" ParentId="31579" PostTypeId="2" Score="15" />
  <row AnswerCount="0" Body="&lt;p&gt;I am a programmer, not a statistician, so pardon my botched use of the terms.  My basic problem is this: I am wanting to calculate  $R^2$ between a known concentration (which can be any non-negative value) and a discrete measurement (where the values are all integers).  There are 92 possible observations, and each of these 92 has a known actual concentration.  We wish to measure the accuracy of the measurements by looking at the  $R^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The different cases (each a different molecule) have concentrations which vary across orders of magnitude, so we use the log (base 2) of the values when calculating the  $R^2$.  This is an industry standard convention for this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;In most cases, this works well.  However, in cases where the measurements are relatively low, I am thinking that it may cause an artificially low  $R^2$.  For example, if the known concentration should result in a measurement of 0.1, since the only measurements possible are 0 or 1, then it will interpret this as error.  Even if there are around 10 cases with concentrations that should give a measurement around 0.1, and we get one detection of one of them and 0 for the other 9, this will be interpreted as error.&lt;/p&gt;&#10;&#10;&lt;p&gt;For higher concentrations, this is obviously less of an issue (if the detections should have been 118.5 and we got either 118 or 119, it will correctly interpret this as not much error).  However, I don't want to just make up my own correction for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;My guess is there is some standard way of handling the calculation of  $R^2$ between a continuous and a discrete variable.  Can you point me at it?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm doing my calculations in Python using the scipy.stats module, but if you just know the name of the proper calculation method and don't know the python code that's perfectly fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;p.s. To be more clear, there are 92 molecules of known concentration, and we are measuring their concentration using a technique.  We want to know if a given measurement run went ok, and so the  $R^2$ of the measurement run (which is discrete, i.e. how many counts do you have for that molecule) vs. the known true concentration (which is continuous) is being used to determine how this run's accuracy compares to other runs (for the exact same set of molecules).  Hopefully the fact that it is always the same 92 x axis values (where y is the measurement), and we are comparing only one measurement run of this type to another, makes  $R^2$ not too bad a metric to use here.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-03T19:43:24.507" Id="31589" LastActivityDate="2012-07-04T04:23:30.860" LastEditDate="2012-07-04T04:23:30.860" LastEditorUserId="3277" OwnerUserId="2917" PostTypeId="1" Score="2" Tags="&lt;discrete-data&gt;&lt;r-squared&gt;" Title="Calculating $R^2$ when one variable can only take integer values" ViewCount="160" />
  
  <row Body="&lt;p&gt;When measuring the accuracy of a measurement system you are always faced with the problem that you can't know the &quot;truth&quot; exactly and so have to compare to a standard.  As you say your estimate of the accuracy is off because you are unable to take into account the accuracy of the standard.  Hopefully you used the the method as a gold standard because it is highly accurate. If you take repeated measurements with the standard you could then get an estimate of its accuracy.  For the moment assume both instrument provide unbiased estimates.  Then MSE=variance and Var(X-Y) =Var(X)+ Var(Y)  where X is the new measurement and Y is the standard.  You have an estimate of Var(X-Y) which you have been using as a rough estimate for Var(X) now by repeated measurements or some other means you have an estimate of the variance of the standard.  Subtraction the estimate of Var(Y) from the estimate of Var(X-Y) and you will have your estimate of Var(X).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-03T20:35:39.233" Id="31595" LastActivityDate="2012-07-03T20:35:39.233" OwnerUserId="11032" ParentId="31593" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="31626" AnswerCount="1" Body="&lt;p&gt;I have a linear regression problem with about 120 predictors and I tried to remove a number of predictors from it.&#10;First I tried to remove multi-collinearities by calculating the variance inflation factor. This left me with about 20 different (hopefully not collinear anymore) predictors.&#10;Then I used a PCA to reduce dimensionality even further. Because the predictors' variances are very different to one another I used the correlation matrix for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can get the 'final' data when I multiply the eigenvectors with the largest eigenvalues with my original data, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;In the end I want to find out which original predictors are left and how I can recover the 'new' original data. But for some reason I am not able to recover correct numbers.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-04T10:00:03.230" Id="31622" LastActivityDate="2012-07-04T19:51:30.417" OwnerUserId="12395" PostTypeId="1" Score="2" Tags="&lt;pca&gt;" Title="Recover data after PCA" ViewCount="862" />
  
  <row Body="&lt;p&gt;PCA does not get rid of any of the variables, although some may be very unimportant. Suppose you use the first three components from your PCA, each of these is a linear combination of the 20 variables you put into it.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to be able to interpret the importance of the original variables in the regression, I don't think PCA is the way to go. I would consider one of the penalized regression methods such as LASSO or LAR&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-04T10:49:09.380" Id="31626" LastActivityDate="2012-07-04T10:49:09.380" OwnerUserId="686" ParentId="31622" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;You can normalize the values so that you use, for example,  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{AP - AP_0}{AP_1-AP_0}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $AP$ is the current air pressure, $AP_0$ is the air pressure value you want sent to $0$, and $AP_1$ is the air pressure value you want sent to $1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is ok if your inputs occasionally go a bit outside $[-1,1]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is dangerous if an input is usually small, but has some occasional extreme values. Then it might be better to split the input into more than one input value, or to remove the outliers and accept that the neural network has a restricted context of applicability. Rescaling so that the outliers are between $-1$ and $1$ won't fix the problem.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-07-04T19:22:15.267" Id="31644" LastActivityDate="2012-07-04T21:36:59.203" LastEditDate="2012-07-04T21:36:59.203" LastEditorUserId="11981" OwnerUserId="11981" ParentId="31641" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="31691" AnswerCount="1" Body="&lt;p&gt;I'm doing some research into respiratory physiology but have come stuck at the analysis stage (should've paid more attention to my stats lectures).&lt;/p&gt;&#10;&#10;&lt;p&gt;I've collected two simultaneous measures of ventilation: the volume of air passing through the mouth and the change in resistance of a chest band to measure chest expansion. I'd like compare the signals and ultimately hope to derive volume from the chest expansion signal. But first I have to align/synchronise my data.&lt;/p&gt;&#10;&#10;&lt;p&gt;As recording doesn't start at precisely the same time and chest expansion is captured for longer periods I need to find the data that corresponds to my volume data within the chest expansion data set. While I could eyeball it, I'll be having to repeat this process god-knows how many times and would like to have a measure of how well they are aligned. I'm not quite sure how to go about this without an anchorpoint (eg. same start time) and with the data in different scales and at different resolutions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've attached an example of the two signals (&lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0As4oZTKp4RZ3dFRKaktYWEhZLXlFbFVKNmllbGVXNHc&quot;&gt;https://docs.google.com/spreadsheet/ccc?key=0As4oZTKp4RZ3dFRKaktYWEhZLXlFbFVKNmllbGVXNHc&lt;/a&gt;), please let me know if there's anything further I could provide.&lt;/p&gt;&#10;&#10;&lt;p&gt;Apologies if this question is a little vague or convoluted, it's been a long, long day and my words are failing me. I'll attempt to clean it up tomorrow morning, but would welcome any suggestions you had in the meantime. Thanks very much!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-05T02:42:02.020" FavoriteCount="5" Id="31666" LastActivityDate="2012-07-06T12:43:25.173" LastEditDate="2012-07-05T15:24:41.950" LastEditorUserId="919" OwnerUserId="12417" PostTypeId="1" Score="9" Tags="&lt;r&gt;&lt;time-series&gt;&lt;signal-processing&gt;&lt;measurement&gt;" Title="How can I align/synchronize two signals? " ViewCount="3331" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am watching &lt;a href=&quot;http://www.youtube.com/watch?v=9iOgAlZOf7k&quot; rel=&quot;nofollow&quot;&gt;the seminar&lt;/a&gt; (around min 20) from CERN about Higgs' Boson discovery.&lt;/p&gt;&#10;&#10;&lt;p&gt;They quickly go on event classification talking about &lt;em&gt;boosting decision trees&lt;/em&gt;. I am not really aware on what they need to classify -I guess they assume the audience knows enough the project in depth. I wonder if someone knows the whole detection and classification process in subject. The problem statement is fundamental for me to understand it. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am investigating on what features, classes and statistical measures have been used in the physics experiment and then feed to decision trees and multivariate analysis.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-05T14:47:56.037" Id="31687" LastActivityDate="2012-07-07T09:24:20.137" LastEditDate="2012-07-07T09:24:20.137" LastEditorUserId="11583" OwnerUserId="11583" PostTypeId="1" Score="4" Tags="&lt;classification&gt;&lt;cart&gt;&lt;boosting&gt;" Title="Event classification in Higgs' Boson discovery" ViewCount="205" />
  
  
  
  <row AcceptedAnswerId="31704" AnswerCount="2" Body="&lt;p&gt;I want to perform a test to determine (with 95% confidence) whether at least 70% of a population can perform some task.&lt;/p&gt;&#10;&#10;&lt;p&gt;The test involves sitting a randomly chosen person down and them attempting a task, which they either pass or fail. Equivalently, I flip a weighted coin.&lt;/p&gt;&#10;&#10;&lt;p&gt;The tests are expensive, so we will only perform a dozen or so of them.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is to calculate pairs of (sample size, number of passes) that would give the required significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to be able to say something of the form &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Ask 10 people. If 8 or more pass then you can say with 95 confidence&#10;  that the true ratio in the population is greater than 70%&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2012-07-05T16:38:28.243" Id="31702" LastActivityDate="2012-07-05T17:24:40.743" LastEditDate="2012-07-05T17:06:45.430" LastEditorUserId="919" OwnerUserId="12425" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;&lt;sample-size&gt;&lt;basic-concepts&gt;" Title="Sample size to tell if more than X% of the population can do &lt;thing&gt;" ViewCount="110" />
  <row AnswerCount="3" Body="&lt;p&gt;If I do the PCA on the whole dataset I get 7 components that can explain 90% of the variance, if I split the dataset into 2 (sorted by time), the number of significant components in the first half goes to 5 (with 15 variables present in one or more components) and in the second half goes to 8 (with 21 variables present in one or more components), can we infer that some of these variables become more significant in the latter half compared to first half?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-05T19:21:51.307" Id="31720" LastActivityDate="2012-08-05T15:48:10.003" LastEditDate="2012-08-05T09:46:38.063" LastEditorUserId="88" OwnerUserId="8849" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;&lt;modeling&gt;&lt;pca&gt;" Title="PCA components changing over time" ViewCount="340" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;If we get something which looks like a familiar distribution, can we&#10;  assume that our error term has this distribution?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I would argue that you can't, since the model you have just fit is invalid if the normality assumption about the errors does not hold. (in the sense that the shape of the distribution is distinctly non-normal such as Cauchy etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The usual approach instead of assuming f.e. Poisson distributed errors, is to perform some form of data transformation such as log y, or 1/y in order to normalize the residuals. &#10;(also the true model might not be linear which would make the plotted residuals appear weirdly distributed even though they are in fact normal)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Say, if we found out that residuals resemble normal distribution, does&#10;  it make sense to assume normality of error term in population?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You assumed the normality of errors once you have fit an OLS regression. Whether you have to provide arguments for that claim, depends on the type and level of your work. (it is often useful to look at what is the accepted practice in the field)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if the residuals do in fact appear to be normally distributed, you can pet yourself on the back, since you can use it as an empirical proof of your previous assumptions. :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-05T23:05:50.590" Id="31738" LastActivityDate="2012-07-12T20:42:01.787" LastEditDate="2012-07-12T20:42:01.787" LastEditorUserId="930" OwnerUserId="12436" ParentId="22468" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://stata.com/help.cgi?arch_postestimation&quot; rel=&quot;nofollow&quot;&gt;ARCH postestimation&lt;/a&gt; help file explains it all. You will most likely need&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     predict hat_volatility, variance&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     predict hat_volatility_factor, het&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;depending on what exactly you mean by ``volatility''. The former is the full prediction, the latter is the multiplier that goes in front of the $\hat\sigma^2$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-06T00:29:32.523" Id="31741" LastActivityDate="2012-07-06T00:29:32.523" OwnerUserId="5739" ParentId="31680" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;When I determine my lambda through cross-validation, all coefficients become zero. But I have some hints from the literature that some of the predictors should definitely affect the outcome. Is it rubbish to arbitrarily choose lambda so that there is just as much sparsity as one desires?&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to select the top 10 or so predictors out of 135 for a cox model and effect sizes unfortunately are small.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-06T07:19:17.500" FavoriteCount="1" Id="31751" LastActivityDate="2012-07-06T17:00:29.613" LastEditDate="2012-07-06T07:25:05.887" LastEditorUserId="10064" OwnerUserId="10064" PostTypeId="1" Score="8" Tags="&lt;lasso&gt;" Title="How defensible is it to choose $\lambda$ in a LASSO model so that it yields the number of nonzero predictors one desires?" ViewCount="949" />
  <row AnswerCount="2" Body="&lt;p&gt;I performed a test to study the joint effects of hardness and alkalinity (independent variables, IV) on reproduction (number of offspring, dependent variable, DV) of water fleas, but I’m not sure what statistical test is appropriate to analyze data.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, I’m not sure whether both IV are independent. They are often correlated in natural waters, thus I kept them autocorrelated in my experiments (they are positively related). However, I manipulated them independently (by adding different salts to water). I may say “they are independent but autocorrelated”?! So are they really independent? &lt;/p&gt;&#10;&#10;&lt;p&gt;Second question: how to analyze data? The design is not factorial: I tested 4 levels of hardness, 4 levels of alkalinity (this would give 16 waters, but I tested only 10 waters). Graphically, data suggest an interaction between hardness and alkalinity on reproduction of the organisms, but I cannot test it. Supposing the assumptions for ANOVA are met, can I use ANOVA?  I first thought of performing ANOVA following the GLM procedure and using simultaneously hardness level and alkalinity level as fixed factors (although they vary to a little extent, as a consequence of the measurement technique). But I was told to perform ANOVA (GLM) for hardness level and, separately, for alkalinity level, but I don’t understand why. Anyway, how should I analyze data?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-06T12:06:51.940" Id="31765" LastActivityDate="2013-05-14T18:36:36.870" LastEditDate="2012-07-06T15:54:28.470" LastEditorUserId="88" OwnerUserId="12444" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;independence&gt;" Title="Independence of variables and ANOVA" ViewCount="442" />
  
  
  <row Body="&lt;p&gt;${\rm Var}(s)\leq \lambda_1$. To see this, note that we have&#10;$${\rm Var}(s)=\frac{\sum_{i=1}^p \lambda_i^3}{(\lambda_1+\cdots+\lambda_p)^2}=\lambda_1\cdot\frac{\lambda_1^2+\sum_{i=2}^p\frac{\lambda_i^3}{\lambda_1}}{(\lambda_1+\cdots+\lambda_p)^2}.$$&#10;But&#10;$$(\lambda_1+\cdots+\lambda_p)^2\geq \lambda_1^2+\cdots+\lambda_p^2\geq \lambda_1^2+\sum_{i=2}^p\frac{\lambda_i^3}{\lambda_1}$$&#10;since $\lambda_1\geq\lambda_i$ for $i=2,3,\ldots,p$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus&#10;$$\frac{\lambda_1^2+\sum_{i=2}^p\frac{\lambda_i^3}{\lambda_1}}{(\lambda_1+\cdots+\lambda_p)^2}\leq 1$$&#10;and&#10;$${\rm Var}(s)=\lambda_1\cdot\frac{\lambda_1^2+\sum_{i=2}^p\frac{\lambda_i^3}{\lambda_1}}{(\lambda_1+\cdots+\lambda_p)^2}\leq \lambda_1\cdot 1=\lambda_1.$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-06T15:24:12.443" Id="31785" LastActivityDate="2012-07-06T15:38:07.077" LastEditDate="2012-07-06T15:38:07.077" LastEditorUserId="4856" OwnerUserId="8507" ParentId="31727" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Assuming that your model is correctly specified, then it looks like the likelihood is highly non linear or unreliable at neighborhood of the last iteration which seemed to be maxima (likely a local one). My strategy would be to try several starting values by using the STARTING option which allows you to set the number of starting value sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suppose that your are not modeling the censoring. In that case your estimates risk to be biased but your model will stay numerically simpler (modeling the censorship may exacerbate your numerical issues). If you are modeling the censoring, it might be a good idea to pick up starting values from a simpler model, the one without censoring, and then rerun the complete model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-06T15:53:21.543" Id="31786" LastActivityDate="2012-07-06T15:53:21.543" OwnerUserId="12265" ParentId="31645" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;Suppose that $\{Y_{t}: t \in \mathbb{Z} \}$ is a stationary zero mean time series. Consider the Hilbert space $\mathcal{H}$ generated by the random variables $\{Y_t: t \in \mathbb{Z} \}$ with inner product $$ \langle X, Y \rangle = E(XY)$$ and norm $$||X||^2 = E|X|^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the subspace $\mathcal{M}$ generated by the random variables $\{Y_u: u \leq t \}$. Why are future values found by projecting onto the subspace $\mathcal{M}$? For example, why is $Y_{t+1}$ found by $\mathcal{P}_{\mathcal{M}}Y_{t+1}$?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-06T16:19:47.053" Id="31788" LastActivityDate="2013-12-23T00:39:05.070" LastEditDate="2012-07-06T16:57:56.287" LastEditorUserId="88" OwnerUserId="12449" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;" Title="Hilbert spaces and time series" ViewCount="309" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data that have been collected using case-control procedures, in which the population of positive cases is collected with a random sample of negative cases. This yields 62 positive cases and 179 controls. There are 58 possible predictor variables (mostly numeric, two factors).&lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is not classification per se--I am in the social sciences and significance testing is a big deal in my discipline, for better or worse. Rather, my collaborators want to make some inferences about the predictors that increase the probability of being a positive case. Obviously, the data are unbalanced, non-randomly sampled, and I have low power. I am trying to select variables for a logistic regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;As an exploratory option, I have run a random forest with various specifications to try and find important variables. My false positive rate is, as expected, only about .03, as there are many more positive cases than negative. My false negative rate is quite high, about .70. Adjusting the weights doesn't seem to help with this.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be a good procedure for variable selection that could then be used to estimate rare events logistic regressions using the reduced set of variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-06T16:59:48.890" Id="31790" LastActivityDate="2012-07-06T16:59:48.890" OwnerUserId="4600" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;feature-selection&gt;&lt;random-forest&gt;" Title="Procedure for variable selection + logistic regression when n is small, p is large, and data are unbalanced?" ViewCount="265" />
  
  <row AcceptedAnswerId="31797" AnswerCount="1" Body="&lt;p&gt;I'm trying to write an R script to simulate the repeated experiments interpretation of a 95% confidence interval. I've found that it overestimates the proportion of times in which the true population value of a proportion is contained within the sample's 95% CI.  Not a big difference - about 96% vs 95% but this interested me nonetheless.   &lt;/p&gt;&#10;&#10;&lt;p&gt;My function takes a sample &lt;code&gt;samp_n&lt;/code&gt; from a Bernoulli distribution with probability &lt;code&gt;pop_p&lt;/code&gt;, and then calculates a 95% confidence interval with &lt;code&gt;prop.test()&lt;/code&gt; using continuity correction, or more exactly with &lt;code&gt;binom.test()&lt;/code&gt;.  It returns 1 if the true population proportion &lt;code&gt;pop_p&lt;/code&gt; is contained within the 95% CI.  I've written two functions, one which uses &lt;code&gt;prop.test()&lt;/code&gt; and one which uses &lt;code&gt;binom.test()&lt;/code&gt; and have had similar results with both: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;in_conf_int_normal &amp;lt;- function(pop_p = 0.3, samp_n = 1000, correct = T){&#10;    ## uses normal approximation to calculate confidence interval&#10;    ## returns 1 if the CI contain the pop proportion&#10;    ## returns 0 otherwise&#10;    samp &amp;lt;- rbinom(samp_n, 1, pop_p)&#10;    pt_result &amp;lt;- prop.test(length(which(samp == 1)), samp_n)&#10;    lb &amp;lt;- pt_result$conf.int[1]&#10;        ub &amp;lt;- pt_result$conf.int[2]&#10;    if(pop_p &amp;lt; ub &amp;amp; pop_p &amp;gt; lb){&#10;        return(1)&#10;    } else {&#10;    return(0)&#10;    }&#10;}&#10;in_conf_int_binom &amp;lt;- function(pop_p = 0.3, samp_n = 1000, correct = T){&#10;    ## uses Clopper and Pearson method&#10;    ## returns 1 if the CI contain the pop proportion&#10;    ## returns 0 otherwise&#10;    samp &amp;lt;- rbinom(samp_n, 1, pop_p)&#10;    pt_result &amp;lt;- binom.test(length(which(samp == 1)), samp_n)&#10;    lb &amp;lt;- pt_result$conf.int[1]&#10;        ub &amp;lt;- pt_result$conf.int[2] &#10;    if(pop_p &amp;lt; ub &amp;amp; pop_p &amp;gt; lb){&#10;        return(1)&#10;    } else {&#10;    return(0)&#10;    }&#10; }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've found that when you repeat the experiment a few thousand times, the proportion of times when the &lt;code&gt;pop_p&lt;/code&gt; is within the 95% CI of the sample is closer to 0.96 rather than 0.95.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1234)&#10;times = 10000&#10;results &amp;lt;- replicate(times, in_conf_int_binom())&#10;sum(results) / times&#10;[1] 0.9562&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My thoughts so far about why this may be the case are&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;my code is wrong (but I've checked it a lot)&lt;/li&gt;&#10;&lt;li&gt;I initially thought that this was due to the normal approximation issue, but then found &lt;code&gt;binom.test()&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-07-06T17:30:59.427" Id="31795" LastActivityDate="2013-08-08T23:08:49.833" LastEditDate="2013-08-08T23:08:49.833" LastEditorUserId="17230" OwnerUserId="1991" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;binomial&gt;&lt;theory&gt;" Title="Problems with a simulation study of the repeated experiments explanation of a 95% confidence interval - where am I going wrong?" ViewCount="606" />
  <row Body="&lt;p&gt;There are so many good possibilities and your vague description makes it difficult to narrow it down to just a couple.&#10;But here is a short list.&#10;1.  Humourous but also clear and accurate by Gonick &quot;The Cartoon Guide to Statistics 1993&quot;&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0062731025&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Cartoon-Guide-Statistics-Larry-Gonick/dp/0062731025/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1341601837&amp;amp;sr=1-1&amp;amp;keywords=the+cartoon+guide+to+statistics&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Clearly written in the style of David Moore &quot;The Basic Practice of Statistics 5th Edition 2010.&quot;&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1429201215&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Basic-Practice-Statistics-David-Moore/dp/1429201215/ref=sr_1_2?s=books&amp;amp;ie=UTF8&amp;amp;qid=1341601954&amp;amp;sr=1-2&amp;amp;keywords=the+basic+practice+of+statistics&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Good college level book by Hogg and Tanis &quot;Probability and Statistical Inference 8th Edition 2009.&quot; Now Published by Prentice-Hall.  Was published by Macmillian when I studied out of it in the 1970s  The authors were Hogg and Craig then and the title was different too.  I had it as &quot;Introduction to Mathematical Statistics 3rd Edition 1970.&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0321584759&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Probability-Statistical-Inference-Robert-Hogg/dp/0321584759/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1341601657&amp;amp;sr=1-1&amp;amp;keywords=hogg+tanis&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The classic by Mood, Graybill and Boes 1974 &quot;Introduction to the Theory of Statistics&quot;.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0070854653&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Introduction-Theory-Statistics-3rd-Edition/dp/0070854653/ref=la_B002880BCE_1_1?ie=UTF8&amp;amp;qid=1341601600&amp;amp;sr=1-1&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Very modern first year undergraduate introductory text.  One of my favorites because it includes resampling methods.  Chihara and Hesterberg &quot;Mathematical Statistics with Resampling and R, 2011&quot; &#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1118029852&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Mathematical-Statistics-Resampling-Laura-Chihara/dp/1118029852/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1341602206&amp;amp;sr=1-1&amp;amp;keywords=Chihara+and+Hesterberg&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;This is the only good one that is concise &quot;pocketbook&quot; size. Silvey's &quot;Statistical Inference 1975&quot;&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0412138204&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Statistical-Inference-Monographs-Statistics-Probability/dp/0412138204/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1341602312&amp;amp;sr=1-1&amp;amp;keywords=silvey+s+d&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="7" CreationDate="2012-07-06T19:19:49.053" Id="31802" LastActivityDate="2012-07-06T19:19:49.053" OwnerUserId="11032" ParentId="31800" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;This is an answer to the last part of your question, how to quickly generate the data you want for the pollinator hypothesis:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n = 16&#10;max = 4&#10;p1 = 0.1&#10;p2 = 0.9&#10;Y1 = rbinom(10000*n,1,p1)&#10;Y2 = matrix(Y1*rbinom(10000*n,4,p2),ncol=16)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can also use &lt;code&gt;rzibinom()&lt;/code&gt; in package VGAM. Although I'm not sure what you want to do with it. You have 2 free parameters, p1 and p2, that need to be estimated. Why not use a zero inflated binomial model to estimate them from the data?&lt;/p&gt;&#10;&#10;&lt;p&gt;You should look at package VGAM, which fits ZIB models among others. In fact, you can get the expected distribution for a ZIB from the VGAM function &lt;code&gt;dzibinom()&lt;/code&gt;, which you could use to compare your observed distribution with if you know the parameters of visitation and pollination. Again, you really should fit the ZIB model. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your partial selfing hypothesis is exclusive to insect pollination, then the expected distribution is simply binomial, and you could estimate the parameters with a binomial family glm or perhaps a glmm with plant id as a random effect. However, if they can partial self AND receive insect pollination, then you're back to needing a mixture of two binomial distributions. In that case I would investigate using OpenBUGS or JAGS to fit the model using MCMC.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have the two models fitted to your data you then compare the models to see which one fits better, using AIC or BIC or some other metric of your choice. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-06T21:31:28.160" Id="31812" LastActivityDate="2012-07-07T14:16:53.133" LastEditDate="2012-07-07T14:16:53.133" LastEditorUserId="12258" OwnerUserId="12258" ParentId="31747" PostTypeId="2" Score="1" />
  
  <row Body="" CommentCount="0" CreationDate="2012-07-07T05:36:41.043" Id="31826" LastActivityDate="2012-07-07T05:36:41.043" LastEditDate="2012-07-07T05:36:41.043" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row AcceptedAnswerId="31829" AnswerCount="2" Body="&lt;p&gt;I've got a set of football results and I want to make a probabilty model of football scores as described in Dixon, Coles (1997,  &lt;a href=&quot;http://www.math.ku.dk/~rolf/teaching/thesis/DixonColes.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.math.ku.dk/~rolf/teaching/thesis/DixonColes.pdf&lt;/a&gt;). They estimate the parameters based on maximum likelihood and the model assumes the variables are independent Poisson.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I have 2 questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How should I transform my data to input them in the model?&lt;/li&gt;&#10;&lt;li&gt;What packages in R are best for maximum likelihood estimation of this nature?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2012-07-07T06:13:49.453" FavoriteCount="3" Id="31828" LastActivityDate="2012-07-08T19:37:49.863" LastEditDate="2012-07-08T19:37:49.863" LastEditorUserId="12455" OwnerUserId="12455" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;estimation&gt;&lt;poisson&gt;&lt;maximum-likelihood&gt;&lt;em-algorithm&gt;" Title="Maximum likelihood estimation in a Poisson model for football (soccer) scores" ViewCount="2013" />
  <row Body="&lt;p&gt;Remember: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;I  True&#10;II False&#10;or&#10;I  TRue&#10;II FAlse&#10;or&#10;I  TR&#10;II FA&#10;or&#10;I  T         R.&#10;II F         A&#10;or &#10;Type  I error: True Ho is Rejected.&#10;Type II error: False Ho is Accepted.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So remember&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;I  True&#10;II False&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-07-07T11:59:39.417" Id="31838" LastActivityDate="2012-07-07T12:48:14.180" LastEditDate="2012-07-07T12:48:14.180" LastEditorUserId="2970" OwnerUserId="12458" ParentId="1610" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;This response will discuss possible models from a &lt;em&gt;measurement perspective&lt;/em&gt;, where we are given a set of observed (manifest) interrelated variables, or measures, whose shared variance is assumed to measure a well-identified but not directly observable construct (generally, in a reflective manner), which will be considered as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Latent_variable&quot;&gt;latent variable&lt;/a&gt;.&#10;If you are unfamiliar with latent trait measurement model, I would recommend the following two articles: &lt;a href=&quot;https://sites.google.com/site/borsboomdenny/BorsboomPM2006.pdf&quot;&gt;The attack of the psychometricians&lt;/a&gt;, by Denny Borsbooom, and &lt;a href=&quot;http://www.gllamm.org/Skrondal07SJS.pdf&quot;&gt;Latent Variable Modelling: A Survey&lt;/a&gt;, by Anders Skrondal and Sophia Rabe-Hesketh. I will first make a slight digression with binary indicators before dealing with items with multiple response categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to transform ordinal level data into interval scale is to use some kind of &lt;a href=&quot;http://en.wikipedia.org/wiki/Item_response_theory&quot;&gt;Item Response&lt;/a&gt; model. A well-known example is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rasch_model&quot;&gt;Rasch model&lt;/a&gt;, which extends the idea of the parallel test model from &lt;a href=&quot;http://en.wikipedia.org/wiki/Classical_test_theory&quot;&gt;classical test theory&lt;/a&gt; to cope with &lt;strong&gt;binary-scored items&lt;/strong&gt; through a generalized (with logit link) mixed-effect linear model (in some of the 'modern' software implementation), where the probability of endorsing a given item is a function of 'item difficulty' and 'person ability' (assuming there's no interaction between one's location on the latent trait being measured and item location on the same logit scale--which could be captured through an additional item discrimination parameter, or interaction with individual-specific characteristics--which is called &lt;a href=&quot;http://en.wikipedia.org/wiki/Differential_item_functioning&quot;&gt;differential item functioning&lt;/a&gt;). The underlying construct is assumed to be unidimensional, and the logic of the Rasch model is just that the respondent has a certain 'amount of the construct'--let's talk about subject's liability (his/her 'ability'), and call it $\theta$, as does any item that defines this construct (their 'difficulty'). What is of interest is the difference between respondent location and item location on the measurement scale, $\theta$. To give a concrete example, consider the following question: &quot;I found it hard to focus on anything other than my anxiety&quot; (yes/no). A person suffering from &lt;a href=&quot;http://en.wikipedia.org/wiki/Anxiety_disorder&quot;&gt;anxiety disorders&lt;/a&gt; is more likely to answer positively to this question compared to a random individual taken from the general population and having no past history of depression or anxiety-related disorder. &lt;/p&gt;&#10;&#10;&lt;p&gt;An illustration of 29 item response curves derived from a large-scale US study that aims to build a calibrated item bank assessing anxiety-related disorders&lt;sup&gt;(1,2)&lt;/sup&gt; is shown below. The sample size is $N=766$; exploratory factor analysis confirmed the unidimensionality of the scale (with first eigenvalue largely above the second eigenvalue (by a 17-fold amount), and unreliable 2nd factor axis (eigenvalue juste above 1) as confirmed by parallel analysis), and this scale shows reliability index in the acceptable range, as assessed by Cronbach's alpha ($\alpha=0.971$, with 95% bootstrap CI $[0.967;0.975]$). Initially, five response categories were proposed (1 = 'Never', 2 = 'Rarely', 3 = 'Sometimes', 4 = 'Often', and 5 = 'Always') for each item. We will here only consider binary-scored responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qO3VZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;&lt;sub&gt;(Here, responses to Likert-type items have been recoded as binary responses (1/2=0, 3-5=1), and we consider that each item is equally discriminative across individuals, hence the parallelism between item curve slopes (Rasch model).)&lt;/sub&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As can be seen, people located to the right of the $x$-axis, which reflects the latent trait (anxiety), who are thought to express more of this trait are more likely to answer positively to questions like &quot;I felt terrified&quot; (terrific) or &quot;I had sudden feelings of panic&quot; (panic) than people located to the left (normal population, unlikely to be considered as cases); on the other hand, it is not unlikely than someone from the general population would report having trouble to get asleep (sleeping): for someone located at intermediate range of the latent trait, say 0 logit, his/her probability of scoring 3 or higher is about 0.5 (which is the item difficulty).&lt;/p&gt;&#10;&#10;&lt;p&gt;For &lt;strong&gt;polytomous items&lt;/strong&gt; with ordered categories, there are several choices: the &lt;em&gt;partial credit model&lt;/em&gt;, the &lt;em&gt;rating scale model&lt;/em&gt;, or the &lt;em&gt;graded response model&lt;/em&gt;, to name but a few that are mostly used in applied research. The first two belong to the so-called &quot;Rasch family&quot; of IRT models and share the following properties: (a) &lt;em&gt;monotonicity&lt;/em&gt; of the response probability function (item/category response curve), (b) &lt;em&gt;sufficiency&lt;/em&gt; of total individual score (with latent parameter considered as fixed), (c) &lt;em&gt;local independence&lt;/em&gt; meaning that responses to items are independent, conditional on the latent trait, and (d) &lt;em&gt;absence of differential item functioning&lt;/em&gt; meaning that, conditional on the latent trait, responses are independent of external individual-specific variables (e.g., gender, age, ethnicity, SES). &lt;/p&gt;&#10;&#10;&lt;p&gt;Extending the previous example to the case where the five response categories are effectively accounted for, a patient will have a higher probability of choosing response category 3 to 5, compared to someone sampled from the general population, without any antecedent of anxiety-related disorders. Compared to the modeling of dichotomous item described above, these models consider either cumulative (e.g., odds of answering 3 vs. 2 or less) or adjacent-category threshold (odds of answering 3 vs. 2), which is also discussed in Agresti's &lt;a href=&quot;http://www.stat.ufl.edu/~aa/cda/cda.html&quot;&gt;Categorical Data Analysis&lt;/a&gt; (chapter 12). The main difference between the aforementioned models lies in the way transitions from one response category to the other are handled: the partial credit model does not assume that difference between any given threshold location and the mean of the threshold locations on the latent trait is equal or uniform across items, contrary to the rating scale model. Another subtle difference between those models is that some of them (like the unconstrained graded response or partial credit model) allows for unequal discrimination parameters between item. See &lt;a href=&quot;http://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=graded%20response%20model%20threshold&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CFcQFjAA&amp;amp;url=http://www.logobook.ru/af/11033974/2414/0198527691_sample.pdf&amp;amp;ei=rhn4T4O-CYzS8QOKmfSUBw&amp;amp;usg=AFQjCNGDGgaVhCHa-LMBkgC9OePvSSWTGA&quot;&gt;Applying item response theory modeling for evaluating questionnaire item and scale properties&lt;/a&gt;, by Reeve and Fayers, or &lt;a href=&quot;http://info.worldbank.org/etools/docs/library/117765/Item%20Response%20Theory%20-%20F%20Baker.pdf&quot;&gt;The basis of item response theory&lt;/a&gt;, by Frank B. Baker, for more details. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because in the preceding case we discussed the interpretation of responses probability curves for dichotomously scored items, let's look at item response curves derived from a graded response model, highlighting the same target items:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ltBSy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;&lt;sub&gt;(Unconstrained graded response model, allowing for unequal discrimination among items.)&lt;/sub&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, the following observations deserve some consideration:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Response categories for the 'sleeping' item are less discriminative than, say, the ones attached to 'terrific': in the case of 'sleeping', for two persons located at the two extrema of the interval $[2;2.5]$ on the latent trait (in logit units), their probability of choosing the fourth response (&quot;&lt;em&gt;often&lt;/em&gt; had difficulty sleeping&quot;) goes from approx. 0.35 to 0.4; with 'terrific', that probability goes from less than 0.1 to about 0.25 (dashed blue line). If you want to discriminate between two patients showing signs of anxiety, the latter item is more informative.&lt;/li&gt;&#10;&lt;li&gt;There is an overall shift, from the left to the right, between item assessing sleep quality and those assessing more severe conditions, although sleeping disorders are not uncommon. This is expected: after all, even people in the general population might experience some difficulty falling asleep, independent of their health state, and people severely depressed or anxious are likely to exhibit such problems. However, 'normal persons' (if this ever had any meaning) are unlikely to show some signs of panic disorder (the probability they choose the highest response category is zero for people located up to the intermediate range or more of the latent trait, [0;1]).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In both cases discussed above, &lt;strong&gt;this $\theta$ scale which reflects individual liability on the assumed latent trait has the property of an &lt;a href=&quot;http://en.wikipedia.org/wiki/Level_of_measurement&quot;&gt;interval scale&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides being thought of as truly &lt;a href=&quot;http://www.rasch.org/memo62.htm&quot;&gt;measurement&lt;/a&gt; models, what makes Rasch models attractive is that sum scores, as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Sufficient_statistic&quot;&gt;sufficient statistic&lt;/a&gt;, can be used as surrogates for the latent scores. Moreover, the sufficiency property readily implies the separability of model (persons and items) parameters (in the case of polytomous items, one should not forget that everything applies at the level of item response category), hence conjoint additivity. &lt;/p&gt;&#10;&#10;&lt;p&gt;A good review of IRT model hierarchy, with R implementation, is available in Mair and Hatzinger's article published in the &lt;em&gt;Journal of Statistical Software&lt;/em&gt;: &lt;a href=&quot;http://www.jstatsoft.org/v20/i09/&quot;&gt;Extended Rasch Modeling: The eRm Package for the Application of IRT Models in R&lt;/a&gt;. Other models include &lt;a href=&quot;http://www.rasch.org/rmt/rmt222f.htm&quot;&gt;log-linear models&lt;/a&gt;, non-parametric model, like the &lt;a href=&quot;http://arno.uvt.nl/show.cgi?fid=81040&quot;&gt;Mokken model&lt;/a&gt;, or &lt;a href=&quot;http://staff.pubhealth.ku.dk/~kach/Christensen_iscb2010.pdf&quot;&gt;graphical models&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Apart from R, I am not aware of Excel implementations, but several statistical packages were proposed on this thread: &lt;a href=&quot;http://stats.stackexchange.com/q/15565/930&quot;&gt;How to get started with applying item response theory and what software to use?&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, if you want to study the relationships between a set of items and a response variable without resorting on a measurement model, some form of variable quantization through &lt;a href=&quot;http://stats.stackexchange.com/a/3812/930&quot;&gt;optimal scaling&lt;/a&gt; can be interesting too. Apart from R implementations discussed in those threads, SPSS solutions were also proposed on &lt;a href=&quot;http://stats.stackexchange.com/a/31297/930&quot;&gt;related threads&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;h2&gt;References&lt;/h2&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Pilkonis, P., Choi, S., Reise, S., Stover, A. and Riley, W. et al. (2011). &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/21697139&quot;&gt;Item banks for mea- suring emotional distress from the patient-reported outcomes measurement information system (PROMIS): Depression, anxiety, and anger&lt;/a&gt;. &lt;em&gt;Assessment&lt;/em&gt;, 18(3), 263–283.&lt;/li&gt;&#10;&lt;li&gt;Choi, S., Gibbons, L. and Crane, P. (2011). &lt;a href=&quot;http://www.jstatsoft.org/v39/i08/&quot;&gt;lordif: An R package for detecting differential item functioning using iterative hybrid ordinal logistic regression/Item Response Theory and monte carlo simulations&lt;/a&gt;. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, 39(8).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2012-07-07T13:09:49.783" Id="31842" LastActivityDate="2012-07-09T17:02:05.623" LastEditDate="2012-07-09T17:02:05.623" LastEditorUserId="930" OwnerUserId="930" ParentId="27927" PostTypeId="2" Score="20" />
  <row AnswerCount="1" Body="&lt;p&gt;Two terms normal distribution and standard normal distribution are used in statistics. Does standard term contribute to the normal distribution anything? Please give a simple-however a substantive reasoning in the back of these terms.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-07T15:30:39.873" Id="31851" LastActivityDate="2014-04-27T15:21:05.583" LastEditDate="2012-07-07T21:53:20.237" LastEditorUserId="8507" OwnerUserId="10619" PostTypeId="1" Score="-3" Tags="&lt;normal-distribution&gt;" Title="What is the difference between normal distribution and standard normal distribution?" ViewCount="24289" />
  
  <row Body="&lt;p&gt;I would use &quot;fully Bayesian&quot; to mean that any nuissance parameters had been marginalised from the analysis, rather than optimised (e.g. MAP estimates).  For example a Gaussian process model, with hyper-parameters tuned to maximise the marginal likelihood would be Bayesian, but only partially so, whereas if the hyper-parameters defining the covariance function were integrated out using a hyper-prior, that would be fully Bayesian.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-07T21:07:23.963" Id="31861" LastActivityDate="2012-07-07T21:07:23.963" OwnerUserId="887" ParentId="31849" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;In the &lt;strong&gt;frequentist&lt;/strong&gt; approach, it is asserted that the only sense in which probabilities have meaning is as the limiting value of the number of successes in a sequence of trials, i.e. as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p = \lim_{n\to\infty} \frac{k}{n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $k$ is the number of successes and $n$ is the number of trials. In particular, it doesn't make any sense to associate a probability distribution with a &lt;em&gt;parameter&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, consider samples $X_1, \dots, X_n$ from the Bernoulli distribution with parameter $p$ (i.e. they have value 1 with probability $p$ and 0 with probability $1-p$). We can define the &lt;em&gt;sample success rate&lt;/em&gt; to be&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{p} = \frac{X_1+\cdots +X_n}{n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and talk about the distribution of $\hat{p}$ conditional on the value of $p$, but it doesn't make sense to invert the question and start talking about the probability distribution of $p$ conditional on the observed value of $\hat{p}$. In particular, this means that when we compute a confidence interval, we interpret the ends of the confidence interval as random variables, and we talk about &quot;the probability that the interval includes the true parameter&quot;, rather than &quot;the probability that the parameter is inside the confidence interval&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the &lt;strong&gt;Bayesian&lt;/strong&gt; approach, we interpret probability distributions as quantifying our uncertainty about the world. In particular, this means that we can now meaningfully talk about probability distributions of parameters, since even though the parameter is fixed, our knowledge of its true value may be limited. In the example above, we can invert the probability distribution $f(\hat{p}|p)$ using Bayes' law, to give&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\overbrace{f(p|\hat{p})}^{\rm posterior} = \underbrace{\frac{f(\hat{p}|p)}{f(\hat{p})}}_{\rm likelihood\, ratio} \overbrace{f(p)}^{\rm prior}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The snag is that we have to introduce the &lt;em&gt;prior&lt;/em&gt; distribution into our analysis - this reflects our belief about the value of $p$ before seeing the actual values of the $X_i$. The role of the prior is often criticised in the frequentist approach, as it is argued that it introduces subjectivity into the otherwise austere and object world of probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Bayesian approach one no longer talks of confidence intervals, but instead of credible intervals, which have a more natural interpretation - given a 95% credible interval, we can assign a 95% probability that the parameter is inside the interval.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-05T15:45:46.660" Id="31868" LastActivityDate="2012-07-05T15:45:46.660" OwnerDisplayName="Chris Taylor" OwnerUserId="2425" ParentId="31867" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;You're right about your interpretation of Frequentist probability: randomness in this setup is merely due to incomplete sampling. From the Bayesian viewpoint probabilities are &quot;subjective&quot;, in that they reflect an agent's uncertainty about the world. It's not quite right to say that the parameters of the distributions &quot;change&quot;. Since we don't have complete information about the parameters, our &lt;em&gt;uncertainty&lt;/em&gt; about them changes as we gather more information. &lt;/p&gt;&#10;&#10;&lt;p&gt;Both interpretations are useful in applications, and which is more useful depends on the situation. You might check out &lt;a href=&quot;http://andrewgelman.com/&quot;&gt;Andrew Gelman's&lt;/a&gt; blog for ideas about Bayesian applications. In many situations what Bayesians call &quot;priors&quot; Frequentists call &quot;regularization&quot;, and so (from my perspective) the excitement can leave the room rather quickly. In fact, according to the Bernstein-von Mises theorem, Bayesian and Frequentist inference are actually asymptotically equivalent under rather weak assumptions (though notably the theorem fails for infinite-dimensional distributions). You can find a slew of references about this &lt;a href=&quot;http://cscs.umich.edu/~crshalizi/notebooks/bayesian-consistency.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you asked for interpretations: I think the Frequentist viewpoint makes great sense when modeling scientific experiments as it was designed to do. For some applications in machine learning or for modeling inductive reasoning (or learning), Bayesian probability makes more sense to me. There are many situations in which modeling an event with a fixed, &quot;true&quot; probability seems implausible. &lt;/p&gt;&#10;&#10;&lt;p&gt;For a toy &lt;a href=&quot;http://en.wikipedia.org/wiki/Sunrise_problem&quot;&gt;example going back to Laplace&lt;/a&gt;, consider the probability that the sun rises tomorrow. From the Frequentist perspective, we have to posit something like infinitely-many universes to define the probability. As Bayesians, there is only one universe (or at least, there needn't be many). Our uncertainty about the sun rising is squelched by our very, very strong prior belief that it will rise again tomorrow. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-05T16:27:40.843" Id="31870" LastActivityDate="2012-07-05T16:27:40.843" OwnerDisplayName="sydeulissie" OwnerUserId="8485" ParentId="31867" PostTypeId="2" Score="9" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I am trying to reach the answers &quot;How likely would that be? ¿How can I know if that is X% likely?&quot; numbers using Mathematica, so far it seems I can answer: How likely would that be? by writing this:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Probability[x&amp;lt;1.54,x[Distributed]PERTDistribution[0.25,5,1]]&#10;  I get a 0.55 probability of X being smaller than 1.54. ¿Correct?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes your assumptions are correct and you have used the Mathematica function ( $Probability[x &amp;lt; 1.54, x \[Distributed] PERTDistribution[{0.25, 5}, 1]]$ ) in the right way. &lt;/p&gt;&#10;&#10;&lt;p&gt;You also have a 0.45 chance of your storage needs being larger than 60GB as well as 0.55 chance of it being smaller.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you specify 128GB of storage you will have a 0.964 chance of your storage not being exceeded.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you specify 256GB of storage you will have a  probability of almost 1 that  your storage will not be exceeded.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless you are using RAM or SSD why not specify 1 TB and be done with it :)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;But it does not, it seems like estimating a single files size with 95% confidence and then multiplying it for 40,000 is very different... ¿where is the mistake? also ¿is there a way to deal with this mistake &quot;symbolically&quot; that is, without having to wait for slow generation of sizes?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes there is. Use the Mathematica function &lt;code&gt;Probability&lt;/code&gt; and your chosen PERTDistribution, which will integrate the probability distribution correctly for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your report perhaps you could consider including a graph such as this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FMoFB.png&quot; alt=&quot;Mathematica graphics&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Produced using the following Mathematica command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;With[{uLim = 5}, &#10; Plot[CDF[PERTDistribution[{0.25, 5}, 1], x], {x, 0, uLim}, &#10;  FrameTicks -&amp;gt; {With[{ts = Range[0, uLim, 0.25]}, {ts, &#10;       40 ts}\[Transpose]], Automatic}, &#10;  GridLines -&amp;gt; {Range[0, uLim, 0.25], Range[0, 1, 0.05]}, &#10;  Frame -&amp;gt; True, LabelStyle -&amp;gt; Directive[Bold, Larger], &#10;  FrameLabel -&amp;gt; {&quot;Space (GB) &quot;, &#10;    &quot;Probability that Space is Sufficient&quot;}]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which uses the Cumulative Distribution Function for the PERT distribution, &lt;code&gt;CDF&lt;/code&gt; in Mathematica, which is effectively an aggregate of the probability that is below some value ( think of it as a plot of instantaneous values of the &lt;code&gt;Probability&lt;/code&gt; function ).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then pose the question &quot;What probability of storage exhaustion is acceptable to the organisation ?&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be a little wary of the values of storage size for probabilities approaching unity, as the real distribution of file sizes is likely to have heavier tails than accounted for by the PERT distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;You may also want to consider your estimate of user numbers. You have a nice distribution estimate for file size, accounting for the potential variability in that quantity. However,you have a single fixed estimate for user numbers, 40,000, with no equivalent allowance for the potentially significant variation in this value. It might be wise to account for this uncertainty on your model.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-07-08T01:38:36.423" Id="31872" LastActivityDate="2012-07-09T10:30:25.777" LastEditDate="2012-07-09T10:30:25.777" LastEditorUserId="10065" OwnerUserId="10065" ParentId="31830" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You need another assumption. There are $4$ consistent ways to deal out C1, C3, and C4 to P1, P4, and P5 respectively: P1P4P5, P4P1P5, P4P5P1, and P5 P4P1. If you add the assumption that all consistent deals are equally likely, then the probability P5 gets C1 is $1/4$; C3, $1/4$; and C4, $1/2$. In many practical situations, the assumption that all consistent deals are equally likely is not correct because the hidden cards held have affected the information which has been revealed. &lt;/p&gt;&#10;&#10;&lt;p&gt;The technique you used to assign probabilities by looking at individual columns or rows is not correct. You will come up with positive probabilities for impossible assignments. For example, if there are only $2$ cards and $2$ players, and you have no direct restriction on the card held by P1, but you know P2 can't hold C1, then you can deduce that P2 has C2 so P1 has C1. Your method would assign probabilities of $1/2$ to the cards for P1, or for the locations of C2.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-08T05:34:07.183" Id="31880" LastActivityDate="2012-07-08T05:34:07.183" OwnerUserId="11981" ParentId="31873" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;One simple approach is to use an &lt;a href=&quot;https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average&quot; rel=&quot;nofollow&quot;&gt;exponentially weighted moving average&lt;/a&gt;.  This tends to emphasize recent data, while retaining some history.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, each hour you could update the estimated popularity $y$ for a page by updating your estimate as follows: $y := (1-\alpha) y + \alpha x$, where $x$ denotes some measure of its popularity over the past hour (e.g., number of visits) and $\alpha$ is a constant that determines how rapidly the estimate discounts older values.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could then sort pages by their estimated popularity.  Pages with a high estimate are &quot;trending&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-08T05:37:25.643" Id="31881" LastActivityDate="2012-07-08T05:37:25.643" OwnerUserId="2921" ParentId="31878" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There is the Cochran-Armitage trend test for linear models but along the lines of D.W.s answer you can view the data as a time series.  The generalization of exponential smoothing is the ARIMA model structure.  The approach to identify trends in ARIMA models is to calculate the estimate of the autocorrelatioon function.  If the function is positive and slowly declining that is an indication of trend (informal).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-08T13:11:50.840" Id="31889" LastActivityDate="2012-07-08T13:11:50.840" OwnerUserId="11032" ParentId="31878" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;For binary classification problems where the data is highly imbalanced, i.e. much more negative samples than positive samples, it is recommended to evaluate the performance of a classifier using the ROC curve because it does not depend on the actual ratio between positive and negative class (see e.g. &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5128907&quot; rel=&quot;nofollow&quot;&gt;He et al&lt;/a&gt;). Yet, I recently came across an article that stated that rare events are usually &quot;better predicted&quot; when looking at the ROC curve. Unfortunately, I did not save the article and was unable to find it again so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I decided to ask here if someone could point me to a paper that demonstrates this behaviour or can give an explanation where this comes from. My follow-up questions would then be, what the preferred way is to evaluate a classifier under these circumstances.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-08T17:59:32.930" FavoriteCount="1" Id="31897" LastActivityDate="2012-07-09T09:14:26.883" OwnerUserId="12431" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;roc&gt;&lt;rare-events&gt;" Title="Evaluation of classifier using ROC curve in the presence of rare events" ViewCount="611" />
  <row Body="&lt;p&gt;You asked if your requirements look reasonable.  My opinion: No, your requirements are not reasonable.  In particular, you proposed:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Ratings above and below the median should be equivalent.&lt;/p&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;{1,1,3,4,4} = {2,2,3,4,4} = {2,2,3,5,5}&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is not reasonable.  {1,1,3,4,4} is a significantly worse score than {2,2,3,5,5}.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have studied app market ratings, and found that there is a significant difference between what a 4 vs a 5 (in terms of what this tends to mean to the reviewer).  For instance, a common meaning of 5 is &quot;I love it&quot;; a common meaning of 4 is &quot;I love it, though it would be great if the developer would add feature X&quot;.  There is also a significant difference between a 1 and a 2.  For instance, a common meaning of 1 is &quot;it does not work at all for me&quot;; a common meaning of 2 is &quot;it works under some situations, or some beneficial qualities, but overall is very poor&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your requirement insists that we must throw away this information.  That is not a reasonable thing to insist upon.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Added 7/9: Here is another way to look at it.&lt;/p&gt;&#10;&#10;&lt;p&gt;By your criteria, {1,1,4,4} &amp;lt; {2,2,5,5}, since the median is lower.  Now suppose a new user walks by and rates each of these two apps a 3.  I would expect this to maintain any existing relationship, so we should have {1,1,3,4,4} &amp;lt; {2,2,3,5,5} -- but your proposal violates this expectation.  So the mere act of rating two products, with the same rating, can actually change their relative ranking.  That is surprising, to say the least!&lt;/p&gt;&#10;&#10;&lt;p&gt;To formalize it a bit, I am proposing the following criteria as one that seems reasonable:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If we have $S &amp;lt; T$, then we also have $S \cup U &amp;lt; T  \cup U$ for any set $U$.  If we have $S = T$, then we also have $S \cup U = T  \cup U$ for any set $U$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;However, this is not consistent with the set of criteria you listed, so we can't have them all: we have to throw something out.  I would argue that your third criteria should be thrown out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-08T19:25:34.103" Id="31902" LastActivityDate="2012-07-09T07:46:31.597" LastEditDate="2012-07-09T07:46:31.597" LastEditorUserId="2921" OwnerUserId="2921" ParentId="19115" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AcceptedAnswerId="31935" AnswerCount="1" Body="&lt;p&gt;I am trying to fit a three parameter inverse gamma distribution to my data in either R or Python. I would like to do this using maximum likelihood estimation (MLE). &lt;/p&gt;&#10;&#10;&lt;p&gt;The pdf of the three parameter inverse gamma is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nOrUa.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Where Γ is the gamma function, ρ is the shape, α is the scale and s is the location parameter&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't spotted an R package that can perform MLE to this distribution directly (if you know of one, please let me know!). So I think this leaves either:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;em&gt;(A) working out the log-likelihood function of the formula&lt;/em&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;(B) transforming the data to a gamma distribution. However, this distribution only has two parameters so I'm not clear on how I would calculate the third parameter (I'm not a very mathematical person!).&lt;/em&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any help on a method to use MLE to fit an inverse gamma distribution to my data would be much appreciated! Many thanks in advance. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-09T11:45:04.623" Id="31934" LastActivityDate="2012-07-09T15:33:58.530" OwnerDisplayName="Faith" OwnerUserId="12490" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;pdf&gt;&lt;curve-fitting&gt;&lt;gamma-distribution&gt;" Title="Maximum Likelihood Estimation of Inverse Gamma Distribution in R or RPy" ViewCount="1511" />
  
  <row Body="&lt;p&gt;1.) I think nonparametric correlation methods Spearman's or Kendall's can be used.&#10;2.) Reversing the order in the code only changes the sign of the correlation not the magnitude.  So changing order is not necessary.&#10;3.) The nonparametric methods require that the data be ordered.  So they can be applied when one variable is ordinal and the other is interval scale.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-09T15:59:07.203" Id="31940" LastActivityDate="2012-07-09T15:59:07.203" OwnerUserId="11032" ParentId="31936" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;You can use a credible interval (or HPD region) for Bayesian hypothesis testing.  I don't think it is common; though, to be fair I do not see much nor do I use formal Bayesian Hypothesis testing in practice.  Bayes factors are occasionally used (and in Robert's &quot;Bayesian Core&quot; somewhat lauded) in hypothesis testing set up.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-07-09T21:53:51.953" Id="31968" LastActivityDate="2012-07-09T21:53:51.953" OwnerUserId="10346" ParentId="31679" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; following dutch&lt;/p&gt;&#10;&#10;&lt;p&gt;In mijn afstandmaat voor mengelingen van variabele mag ik alleen een combinatie van continue en ordinale variabele gebruiken. Mijn data bestaat echter uit een mengeling van ordinale, nominale en continue variabele. Mag ik aannemen dat al mijn nominale variabele ook ordinaal zijn of maak ik dan een gigantische fout in mijn waarneemingen?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;English:&lt;/strong&gt;&#10;In my distance measurement for mixed I can only use a combination of continuous and ordinal variables. But my data has a combination of nominal, ordinal and continuous variables. Would it make sense to assume that my nominal variables are ordinal or do I make a huge mistake in that way?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-10T07:15:18.410" Id="31992" LastActivityDate="2012-07-13T09:02:10.167" LastEditDate="2012-07-13T09:02:10.167" LastEditorUserId="930" OwnerUserId="12508" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;categorical-data&gt;&lt;ordered-variables&gt;" Title="Whether to assume nominal variables are ordinal when computing distance measurement for mixed?" ViewCount="149" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for a free CAS (Computer Algebra System) that is most able to do mathematical statistics. I thought of SAGE, but it doesn't seem to be very powerful.&lt;/p&gt;&#10;&#10;&lt;p&gt;It can't derive t-distribution : &lt;a href=&quot;http://ask.sagemath.org/question/1485/student-t-pdf&quot; rel=&quot;nofollow&quot;&gt;http://ask.sagemath.org/question/1485/student-t-pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And it seems to have trouble calculating even the mean and the variance of Laplace distribution!&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's sage input/output.&lt;/p&gt;&#10;&#10;&lt;p&gt;INPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def f1(x,mu,sigma):&#10;  return 1/sqrt(2*pi*sigma^2)*exp(-1/2*((x-mu)/sigma)^2)&#10;def f2(x,mu,sigma):&#10;  return 1/sqrt(2*sigma^2)*exp(-sqrt(2)*abs((x-mu)/sigma))&#10;&#10;var(&quot;mu sigma&quot;)&#10;&#10;integral(x*f2(x,mu,sigma), x, -oo,oo)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OUTPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1/2*sqrt(2)*integrate(x*e^(-sqrt(2)*abs(-mu + x)/sigma), x, -Infinity,&#10;+Infinity)/sigma&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;INPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;integral(x*f1(x,mu,sigma), x, -oo,oo)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OUTPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mu&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;INPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;integral((x-mu)^2*f2(x,mu,sigma), x, -oo,oo)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OUTPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1/2*sqrt(2)*integrate((mu - x)^2*e^(-sqrt(2)*abs(-mu + x)/sigma), x,&#10;-Infinity, +Infinity)/sigma&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;INPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;integral((x-mu)^2*f1(x,mu,sigma), x, -oo,oo)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OUTPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sigma^2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does anyone know free CAS which can calculate t-distribution and the mean and the variance of Laplace distribution with parameters unkown? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-10T07:35:27.010" Id="31994" LastActivityDate="2012-07-10T10:54:21.840" LastEditDate="2012-07-10T10:54:21.840" LastEditorUserId="8507" OwnerUserId="11242" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;software&gt;" Title="What would be the most potent free CAS for mathematical statistics?" ViewCount="136" />
  
  <row Body="&lt;p&gt;Mathematical statistics concentrates on theorems and proofs and mathematical rigor, like other branches of math. It tends to be studied in math departments, and mathematical statisticians often try to derive new theorems.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Statistics&quot; includes mathematical statistics, but the other parts of the field tend to concentrate on more practical problems of data analysis and so on. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-10T10:29:09.380" Id="32004" LastActivityDate="2012-07-10T10:29:09.380" OwnerUserId="686" ParentId="32001" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;There are other discussions on this site where the inclusion of interaction terms in regression without main effect is discussed.  The issue is the same in logistic regression.  Nothing prevents this from being possible.  It is just a common rule without thorough justification that itneractions should not be included without main effects. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-10T11:56:48.257" Id="32009" LastActivityDate="2012-07-10T11:56:48.257" OwnerUserId="11032" ParentId="32000" PostTypeId="2" Score="0" />
  <row AnswerCount="4" Body="&lt;p&gt;My team and me would like to give a presentation to the non-statisticians of the company about the utility of the design of experiments. These non-statisticians are also our clients and they usually don't consult us before collecting their data. Do you know some real examples which would well illustrate Fisher's famous quote &lt;em&gt;&quot;To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he only may be able to say what the experiment died of.&quot;&lt;/em&gt; ? Preferably we are looking for an illustration in an industrial/pharmaceutical/biological context. We think of an example of an inconclusive statistical analysis which could have been successful if it had been preliminary well designed, but maybe there are other possible illustrations.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-10T13:53:56.693" FavoriteCount="2" Id="32018" LastActivityDate="2012-07-11T03:34:05.757" OwnerUserId="8402" PostTypeId="1" Score="12" Tags="&lt;experiment-design&gt;" Title="Seeking a real illustration of Fisher's quote about DoE" ViewCount="331" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a couple of stacked histograms which I need to compare/evaluate for similarity or difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JK2GG.png&quot; alt=&quot;Stacked histogram generated from one dataset&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe rather than evaluating histograms is will be east to work with dataset used to plot these stacked histograms, which is in format: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;RED                PURPLE       BLUE        GREY       YELLOW&#10;&#10;22.0640569395   16.9483985765   0   60.987544484    0&#10;&#10;8.1850533808    8.8523131673    0   82.962633452    0&#10;&#10;6.8505338078    6.8950177936    0.756227758 85.4982206406   0.5338078292&#10;&#10;6.7615658363    5.2491103203    1.6459074733    86.3434163701   0.6672597865&#10;&#10;5.8274021352    7.384341637 2.1352313167    84.653024911    1.1565836299&#10;&#10;7.8736654804    6.628113879 1.5569395018    83.9412811388   1.2010676157&#10;&#10;7.1619217082    8.1850533808    1.2455516014    83.4074733096   1.3790035587&#10;&#10;5.5604982206    10.2758007117   1.0676156584    83.0960854093   1.0231316726&#10;&#10;7.1174377224    7.6067615658    0.7117437722    84.5640569395   0.756227758&#10;&#10;7.8736654804    3.9590747331    0.6672597865    87.5    0.3113879004&#10;&#10;7.6512455516    7.8736654804    0.5338078292    83.9412811388   0.5338078292&#10;&#10;7.6067615658    8.9857651246    1.4679715302    81.9395017794   0.3558718861&#10;&#10;8.9412811388    8.0071174377    1.3790035587    81.6725978648   0.5782918149&#10;&#10;19.0836298932   9.2081850534    2.1352313167    69.5729537367   1.3790035587&#10;&#10;14.9911032028   11.0765124555   3.2028469751    70.7295373665   1.0676156584&#10;&#10;15.3914590747   10.8985765125   3.024911032 70.6850533808   1.2900355872&#10;&#10;17.4822064057   12.5444839858   2.4911032028    67.4822064057   1.334519573&#10;&#10;15.8362989324   13.0338078292   2.0017793594    69.128113879    1.334519573&#10;&#10;17.037366548    10.4537366548   2.4021352313    70.1067615658   1.2010676157&#10;&#10;20.2846975089   10.0088967972   0   69.706405694    1.0676156584&#10;&#10;28.7366548043   12.6334519573   0   58.6298932384   0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there any possible way I can compare such dataset from multiple experiments (n=8) and visually show (plot) that these datasets are in consensus or differ from each other? Can QQplot be used to evaluate such datasets. &lt;/p&gt;&#10;&#10;&lt;p&gt;Awaiting reply, &lt;/p&gt;&#10;&#10;&lt;p&gt;Atul&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-06T21:39:51.780" Id="32019" LastActivityDate="2012-07-10T14:13:51.397" OwnerDisplayName="Atul Kakrana" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;dataset&gt;&lt;data-visualization&gt;" Title="Evaluate/Compare multiple numerical dataset for consensus or difference in R" ViewCount="140" />
  
  <row Body="&lt;p&gt;I agree with Peter &lt;a href=&quot;http://stats.stackexchange.com/a/32002/5739&quot;&gt;here&lt;/a&gt;. You probably have a miscommunication issue with your consultant. What a survey is supposed to give you are the &lt;em&gt;estimates&lt;/em&gt; of the &lt;strong&gt;population&lt;/strong&gt; parameters. If your consultant just talked about the sample as if they were a population, that would be bad. But if they computed sampling error, then he probably understands the distinction between the two. You, on the other hand, might want to read more on sampling statistics, although you do demonstrate the basic understanding, judging from your first paragraph. &lt;a href=&quot;http://whatisasurvey.info/&quot; rel=&quot;nofollow&quot;&gt;What Is a Survey&lt;/a&gt; by Fritz Scheuren, past president of the American Statistical Association, is one starting point; a good book such as &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0495105279&quot; rel=&quot;nofollow&quot;&gt;Lohr (2009)&lt;/a&gt; is another.&lt;/p&gt;&#10;&#10;&lt;p&gt;What might have thrown you off is that the consultant computed the weights that sum up to the population total, and that is the appropriate practice (unlike the practice of providing the weights that sum up to the sample size). So now all counts look like they relate to the population -- and that's a good thing, especially if your &quot;number of females who smoke in the country&quot; is accompanied with a margin of sampling error.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would also comment that it is probably a good practice to double check on the work of your consultant, especially if you feel that you don't quite understand what they have done. Make sure to not piss them off though by questioning their qualifications. I understand it when my clients check my work in the peer-review way; but I doubt I would return to do business with a client who would ask somebody else to redo my work, especially behind my back without telling me.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-10T14:49:06.693" Id="32023" LastActivityDate="2012-07-10T14:49:06.693" OwnerUserId="5739" ParentId="31981" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="32042" AnswerCount="1" Body="&lt;p&gt;I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). &lt;strong&gt;The simplest model:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)&#10;&amp;gt; summary(fix1)&#10;&#10;Call:&#10;lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-5.5835 -0.3543 -0.0024  0.3944  4.7294 &#10;&#10;Coefficients:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;Group1       4.6395740  0.0466217  99.515  &amp;lt; 2e-16 ***&#10;Group2       4.8094268  0.0534118  90.044  &amp;lt; 2e-16 ***&#10;Group3       4.5607287  0.0561066  81.287  &amp;lt; 2e-16 ***&#10;Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** &#10;Group2:Year  0.0032369  0.0031098   1.041  0.29802    &#10;Group3:Year  0.0006081  0.0032666   0.186  0.85235    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 0.7926 on 2981 degrees of freedom&#10;Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 &#10;F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Clearly the individual should be random effect, so I introduce random intercept effect for each individual:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)&#10;&amp;gt; summary(mix1a)&#10;Linear mixed model fit by REML &#10;Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) &#10;   Data: mydata &#10;  AIC  BIC logLik deviance REMLdev&#10; 4727 4775  -2356     4671    4711&#10;Random effects:&#10; Groups     Name        Variance Std.Dev.&#10; Individual (Intercept) 0.39357  0.62735 &#10; Residual               0.24532  0.49530 &#10;Number of obs: 2987, groups: Individual, 103&#10;&#10;Fixed effects:&#10;              Estimate Std. Error t value&#10;Group1       4.6395740  0.1010868   45.90&#10;Group2       4.8094268  0.1158095   41.53&#10;Group3       4.5607287  0.1216522   37.49&#10;Group1:Year -0.0084165  0.0016963   -4.96&#10;Group2:Year  0.0032369  0.0019433    1.67&#10;Group3:Year  0.0006081  0.0020414    0.30&#10;&#10;Correlation of Fixed Effects:&#10;            Group1 Group2 Group3 Grp1:Y Grp2:Y&#10;Group2       0.000                            &#10;Group3       0.000  0.000                     &#10;Group1:Year -0.252  0.000  0.000              &#10;Group2:Year  0.000 -0.252  0.000  0.000       &#10;Group3:Year  0.000  0.000 -0.252  0.000  0.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The individuals are also different in slope so I also introduced the random slope effect:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)&#10;&amp;gt; summary(mix1c)&#10;Linear mixed model fit by REML &#10;Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) &#10;   Data: mydata &#10;  AIC  BIC logLik deviance REMLdev&#10; 2941 3001  -1461     2885    2921&#10;Random effects:&#10; Groups     Name        Variance  Std.Dev. Corr   &#10; Individual (Intercept) 0.1054790 0.324775        &#10;            Year        0.0017447 0.041769 -0.246 &#10; Residual               0.1223920 0.349846        &#10;Number of obs: 2987, groups: Individual, 103&#10;&#10;Fixed effects:&#10;              Estimate Std. Error t value&#10;Group1       4.6395740  0.0541746   85.64&#10;Group2       4.8094268  0.0620648   77.49&#10;Group3       4.5607287  0.0651960   69.95&#10;Group1:Year -0.0084165  0.0065557   -1.28&#10;Group2:Year  0.0032369  0.0075105    0.43&#10;Group3:Year  0.0006081  0.0078894    0.08&#10;&#10;Correlation of Fixed Effects:&#10;            Group1 Group2 Group3 Grp1:Y Grp2:Y&#10;Group2       0.000                            &#10;Group3       0.000  0.000                     &#10;Group1:Year -0.285  0.000  0.000              &#10;Group2:Year  0.000 -0.285  0.000  0.000       &#10;Group3:Year  0.000  0.000 -0.285  0.000  0.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;&lt;strong&gt;But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!&lt;/strong&gt;&lt;/h3&gt;&#10;&#10;&lt;p&gt;How is this possible? I would expect that the random effect will &quot;eat&quot; the unexplained variability and increase &quot;sureness&quot; of the estimate!&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the residual SE behaves as expected - it is lower than in the random intercept model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R&quot; rel=&quot;nofollow&quot;&gt;Here is the data&lt;/a&gt; if needed.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Edit&lt;/h2&gt;&#10;&#10;&lt;p&gt;Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, &lt;strong&gt;I get exactly the same result as the random slope model!&lt;/strong&gt; Would you know why?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;indivSlope = c()&#10;for (indiv in 1:103) {&#10;    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])&#10;    indivSlope[indiv] = coef(mod1)['Year']&#10;}&#10;&#10;indivGroup = unique(mydata[,c(&quot;Individual&quot;, &quot;Group&quot;)])[,&quot;Group&quot;]&#10;&#10;&#10;anova1 = lm(indivSlope ~ 0 + indivGroup)&#10;summary(anova1)&#10;&#10;Call:&#10;lm(formula = indivSlope ~ 0 + indivGroup)&#10;&#10;Residuals:&#10;      Min        1Q    Median        3Q       Max &#10;-0.176288 -0.016502  0.004692  0.020316  0.153086 &#10;&#10;Coefficients:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)&#10;indivGroup1 -0.0084165  0.0065555  -1.284    0.202&#10;indivGroup2  0.0032369  0.0075103   0.431    0.667&#10;indivGroup3  0.0006081  0.0078892   0.077    0.939&#10;&#10;Residual standard error: 0.04248 on 100 degrees of freedom&#10;Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 &#10;F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R&quot; rel=&quot;nofollow&quot;&gt;Here is the data&lt;/a&gt; if needed.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-10T18:58:53.483" FavoriteCount="3" Id="32040" LastActivityDate="2012-07-11T01:01:41.217" LastEditDate="2012-07-11T01:01:41.217" LastEditorUserId="88" OwnerUserId="5509" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;random-effects-model&gt;" Title="Why the introduction of a random slope effect enlarged the slope's SE?" ViewCount="705" />
  <row Body="&lt;p&gt;To be able to talk about the significance of the coefficients you need to know the distribution under the null hypothesis that the parameter is actually 0.  For the least squares estimate we can do that if we can estimate its distribution from a parametric family.  The most common approach is to assume a normally distributed error term for the model with mean 0 and constant variance.  If you want to get away from the parametric approach the bootstrap works in this case.  You can bootstrap residuals or bootstrap the vectors.  The bootstrap distribution for the regression parameter, serves the role of the t statistic in the parametirc approach, to decide on the significance of the regression parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be generalized to other fit criteria such as MAD (minimizing the median absolute deviation)..&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-07-10T20:33:52.797" Id="32053" LastActivityDate="2012-07-10T20:37:01.733" LastEditDate="2012-07-10T20:37:01.733" LastEditorUserId="603" OwnerUserId="11032" ParentId="32052" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there a good expository account of Sheppard's correction, written in a way that any ordinary mathematician can readily follow? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://mathworld.wolfram.com/SheppardsCorrection.html&quot; rel=&quot;nofollow&quot;&gt;http://mathworld.wolfram.com/SheppardsCorrection.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(I've thought of writing a Wikipedia article titled &quot;Sheppard's correction&quot;, but I haven't done enough homework on it yet.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Sheppard's correction can be done for various functionals of a probability distribution.  The Wolfram article treats cumulants.  Curiously, each correction is $-1$ times the correction I'd expect to do for a uniform distribution.  I'm not sure whether to believe this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Think of this: split the interval $(0,\theta)$ into bins of equal length.  For a sample from the uniform distribution on the interval, suppose only the midpoint of the bin is reported.  Then the variance of the reported data would be too small, since it would fail to report variability &lt;em&gt;within&lt;/em&gt; bins.  The variability within bins would be just $1/12$ of the square of the bin length.  But Sheppard's correction for normally distributed data tells you to add $-1/12$ of the square of the bin length.  With the normal distribution, the error from binning is negatively correlated with the observation; with the uniform distribution it is uncorrelated with the observation.  If we believe Wolfram, this multiplication by $-1$ seems to apply to all cumulants, not just the second cumulant.  If so, that seems like a mystery to be investigated.  I hesitate to believe Wolfram on this.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-10T22:29:00.443" Id="32060" LastActivityDate="2012-07-11T00:40:45.277" LastEditDate="2012-07-11T00:40:45.277" LastEditorUserId="5176" OwnerUserId="5176" PostTypeId="1" Score="3" Tags="&lt;references&gt;" Title="Sheppard's correction" ViewCount="154" />
  
  <row Body="&lt;p&gt;The lines in the boxplots are medians not means.  When you have outliers the mean and medians can be very different.  Did you include the outliers when fitting the model?  If you excldued them what was your justification for removing them?  Also if the outliers are not used in the model you get a misleading picture including them in the boxplot. The distributions look skewed, particularly the one on the far right and far left.  Observations should not be arbitrarily removed for exceeding the 3 sigma limit. When the data is very nonnormal the F tests in the analysis of variance are not valid and you should consider nonparametric alternatives such as the Kruskal-Wallis test.  Also keep in mind that even for a normal distribution approxiamtely 3 out of 1000 observations will fall outside the 3 sigma limit.  With a sample of over 650 observations it would not be surprising to have one or two outside the 3 sigma limit.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-11T01:44:05.753" Id="32074" LastActivityDate="2012-07-11T01:44:05.753" OwnerUserId="11032" ParentId="32072" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;The Component score coefficient matrix holds weights of the regression &lt;a href=&quot;http://pareonline.net/pdf/v14n20.pdf&quot; rel=&quot;nofollow&quot;&gt;factor scores&lt;/a&gt;. However, you can save factor scores as new variables in your working SPSS data sheet. See, for example, this Annotated SPSS Output on &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/output/factor1.htm&quot; rel=&quot;nofollow&quot;&gt;Factor Analysis&lt;/a&gt; on UCLA server, or this &lt;a href=&quot;http://psych.unl.edu/psycrs/statpage/pc_score.pdf&quot; rel=&quot;nofollow&quot;&gt;short tutor&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-11T10:02:38.777" Id="32091" LastActivityDate="2012-07-11T10:02:38.777" OwnerUserId="930" ParentId="32088" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I don't know if I could list them all.  The exponential, normal and binomial come to mind and they all fall into the class of exponential families.  The exponential family has its sufficient statistic in the exponent and the mle is often a nice function of this sufficient statistic.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-11T14:01:13.637" Id="32104" LastActivityDate="2012-07-12T01:58:53.923" LastEditDate="2012-07-12T01:58:53.923" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32103" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;@Joke&#10;A fixed-effects model implies that the effect-size generated by a study(or experiment) is fixed i.e. repeat measurements for an intervention turn out same effect-size.Presumably, the external and internal conditions for the experiment do not change. If you have a number of trials and or studies under different condtions, you will have different effect-sizes. The parametric estimates of mean and variance for a set of effect-sizes can be realised by either presuming that these are fixed-effects or these are random-effects(realised from a super-population). If you have data or could explain a bit of it, I shall try to persist. What is the real theme in your mind, I think that it is matter that can be resolved with the help of mathematical statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-11T14:43:58.157" Id="32107" LastActivityDate="2013-11-28T15:21:35.667" LastEditDate="2013-11-28T15:21:35.667" LastEditorUserId="10619" OwnerUserId="10619" ParentId="26230" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;There is no linearity or normality assumed in PCA.  The idea is just decomposing the variation in a p-dimensional dataset into orthogonal components that are ordered according to amount of variance explained.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-11T15:25:49.323" Id="32110" LastActivityDate="2012-07-11T15:25:49.323" OwnerUserId="11032" ParentId="32105" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;If you fit a distribution to a small sample a good fit will only really measure how well the body of the distribution fits the data because you won't see much of the tail of a distribution unless you have a very large dataset.  Distributions can look very similar in the body and yet very different in the tails. Comparing the tails of the distributions as Procrastinator pointed out can pretty much be done from knowing the particular distributions being selected without knowing anything about the data and in a small sample what you know from the data won't tell much if anthing about the tails.  So I don't see where comparing the tails of competing distributions makes any sense in data analysis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-11T15:33:34.247" Id="32111" LastActivityDate="2012-07-11T15:49:25.403" LastEditDate="2012-07-11T15:49:25.403" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32106" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose that we have a list of relevant numbers (e.g. maximum revenues for various years). If the leading digits are $1$ for all the years, can we say that trivially the maximum revenues satisfy &lt;a href=&quot;http://en.wikipedia.org/wiki/Benford%27s_law#Explanations&quot; rel=&quot;nofollow&quot;&gt;Benford's Law&lt;/a&gt;? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-11T17:37:12.447" Id="32120" LastActivityDate="2012-07-12T11:31:54.237" LastEditDate="2012-07-12T11:31:54.237" LastEditorUserId="183" OwnerUserId="5111" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;" Title="If the most common leading digit is one, is Benford's Law satisfied?" ViewCount="151" />
  
  <row AcceptedAnswerId="32142" AnswerCount="1" Body="&lt;p&gt;Trying to &quot;understand&quot; a time series' patterns it is intuitively tempting to use STL decomposition as the concept of distinguishing between trend, season and the rest makes sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;But my experience tells me that no static algorithm will lead under all circumstances to useful results. &lt;/p&gt;&#10;&#10;&lt;p&gt;So my general question/s is/are when should you not apply STL decomposition and if you do, what observation in the STL result might in you experience indicate a faulty/useless decomposition?&lt;/p&gt;&#10;&#10;&lt;p&gt;Like you wouldn't blindly trust a correlation analysis of two variables without having a look at a scatter plot, b/c outliers might lead to a high correlation coefficient indicating a non-existant relation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm a newbie in this area, so a more extensive answer would be great.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-11T22:10:13.300" FavoriteCount="1" Id="32140" LastActivityDate="2012-07-12T21:02:17.290" LastEditDate="2012-07-12T05:33:16.847" LastEditorUserId="8507" OwnerUserId="3077" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;trend&gt;&lt;seasonality&gt;" Title="Contraindication for STL decompostion" ViewCount="142" />
  
  
  
  <row AcceptedAnswerId="32170" AnswerCount="1" Body="&lt;p&gt;I am building a Cox PH model for recurrence-free survival from breast cancer.  One of my variables, the number of involved lymph nodes, is highly skewed with many more &quot;0&quot; values than positive integers.  It seems to me that this would make any transformation ineffective because this variable cannot be negative.  Here is a stem plot of the lymph node data:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 0 | 00000000000000000000000000000000000000000000000000000000000000000000+195&#10; 2 | 00000000000000000000000000000000000000000000000000&#10; 4 | 00000000000000000000000000000&#10; 6 | 000000000000000&#10; 8 | 000000000000&#10;10 | 0000000000&#10;12 | 00000&#10;14 | 0000&#10;16 | 0&#10;18 | &#10;20 | 0&#10;22 | &#10;24 | 00&#10;26 | 0&#10;28 | 0&#10;30 | &#10;32 | 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A log(X+1) transformation doesn't do anything to normalize the data because there is no data less than 0.  How important is it to transform this variable?  Would using only values &amp;lt;20 and log-transforming them help more?  This must be a common problem; are there any creative solutions to it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help!    &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-12T14:21:23.857" FavoriteCount="1" Id="32168" LastActivityDate="2013-03-16T02:47:46.590" LastEditDate="2013-03-16T02:47:46.590" LastEditorUserId="805" OwnerUserId="12561" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;cox-model&gt;" Title="How important is it to transform variable for Cox Proportional Hazards?" ViewCount="714" />
  <row AnswerCount="3" Body="&lt;p&gt;I’m trying to generate correlated random sequence with mean = $0$, variance = $1$, correlation coefficient = $0.8$. In the code below, I use &lt;code&gt;s1&lt;/code&gt; &amp;amp; &lt;code&gt;s2&lt;/code&gt; as the standard deviations, and &lt;code&gt;m1&lt;/code&gt; &amp;amp; &lt;code&gt;m2&lt;/code&gt; as the means. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p = 0.8 &#10;u = randn(1, n)&#10;v = randn(1, n)&#10;x = s1 * u + m1&#10;y = s2 * (p * u + sqrt(1 - p^2) * v) + m2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me the correct &lt;code&gt;corrcoef()&lt;/code&gt; of 0.8 between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. My question is how can I generate a series means if I want &lt;code&gt;z&lt;/code&gt; that is also correlated with &lt;code&gt;y&lt;/code&gt; (with the same correlation $r=0.8$), but not with &lt;code&gt;x&lt;/code&gt;. Is there a particular formula I need to know? I found one but couldn't understand it. See this &lt;a href=&quot;http://www.goddardconsulting.ca/option-pricing-monte-carlo-basket.html#generalprocedure&quot;&gt;link&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-12T14:31:01.303" FavoriteCount="5" Id="32169" LastActivityDate="2014-10-16T15:42:00.833" LastEditDate="2013-09-10T20:04:59.420" LastEditorUserId="7290" OwnerUserId="12562" PostTypeId="1" Score="12" Tags="&lt;correlation&gt;&lt;matlab&gt;&lt;simulation&gt;&lt;random-generation&gt;" Title="How can I generate data with a prespecified correlation matrix?" ViewCount="5911" />
  
  <row Body="&lt;p&gt;As for AR model, you may refer to &lt;a href=&quot;http://www.statoek.wiso.uni-goettingen.de/veranstaltungen/zeitreihen/sommer03/ts_r_intro.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;. For R, the &lt;a href=&quot;http://cran.r-project.org/web/packages/TSA/index.html&quot; rel=&quot;nofollow&quot;&gt;TSA&lt;/a&gt; package is a good one to use. The writer of the package has easily understandable &lt;a href=&quot;http://homepage.stat.uiowa.edu/~kchan/TSA.htm&quot; rel=&quot;nofollow&quot;&gt;class notes&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;If you have a specific example, I can provide you further help.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-12T15:04:03.687" Id="32173" LastActivityDate="2012-07-12T15:04:03.687" OwnerUserId="9583" ParentId="32158" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="136072" AnswerCount="1" Body="&lt;p&gt;The PCA algorithm can be formulated in terms of the correlation matrix (assume the data $X$ has already been normalized and we are only considering projection onto the first PC). The objective function can be written as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \max_w (Xw)^T(Xw)\; \: \text{s.t.} \: \:w^Tw = 1. $$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is fine, and we use Lagrangian multipliers to solve it, i.e. rewriting it as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \max_w [(Xw)^T(Xw) - \lambda w^Tw], $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is equivalent to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \max_w \frac{ (Xw)^T(Xw) }{w^Tw},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and hence (&lt;a href=&quot;http://mathworld.wolfram.com/Point-LineDistance2-Dimensional.html&quot; rel=&quot;nofollow&quot;&gt;see here on Mathworld&lt;/a&gt;) seems to be equal to $$\max_w \sum_{i=1}^n \text{(distance from point $x_i$ to line $w$)}^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But this is saying to maximize the distance between point and line, and from what I've read &lt;a href=&quot;http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues&quot;&gt;here&lt;/a&gt;, this is incorrect -- it should be $\min$, not $\max$. Where is my error?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, can someone &lt;strong&gt;show me the link between maximizing variance in projected space and minimizing distance between point and line?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-12T15:09:55.693" Id="32174" LastActivityDate="2015-02-03T12:50:15.567" LastEditDate="2015-02-03T12:50:15.567" LastEditorUserId="28666" OwnerUserId="11867" PostTypeId="1" Score="5" Tags="&lt;pca&gt;&lt;optimization&gt;" Title="PCA objective function: what is the connection between maximizing variance and minimizing error?" ViewCount="743" />
  <row AcceptedAnswerId="32178" AnswerCount="1" Body="&lt;p&gt;I have done the following measurements:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Independent Variables IV1 and IV2 with a ordinal scale&lt;/li&gt;&#10;&lt;li&gt;Dependent Variable 'DV' with an ordinal scale&lt;/li&gt;&#10;&lt;li&gt;Tested 8 different values for IV1 and 6 different values for IV2 in a full factorial design with 5 repetition each and recorded the dependent variable. So in total 240 measurements were done.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now an ANOVA test on these measurements show that with a very high probability (&gt; 1 - 2e-16) both independent variables contribute to the variance of 'DV'. Thats what I expected, so everything OK until here.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; aov(DV~IV1+IV2, data=d)&#10;Terms:&#10;                IV1                  IV2 Residuals&#10;Sum of Squares  1721.6289       918.6018  198.5668&#10;Deg. of Freedom         7              5       227&#10;&#10;Residual standard error: 0.9352772 &#10;Estimated effects may be unbalanced&#10;&#10;&amp;gt; summary(aov(DV~IV1+IV2, data=d))&#10;                Df Sum Sq Mean Sq F value Pr(&amp;gt;F)    &#10;IV1              7 1721.6  245.95   281.2 &amp;lt;2e-16 ***&#10;IV2              5  918.6  183.72   210.0 &amp;lt;2e-16 ***&#10;Residuals      227  198.6    0.87                   &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I now want to quantify how much influence a change in IV1 and a change in IV2 has on the dependent variable DV. How can I do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure whether I'm asking the right question at all. &lt;/p&gt;&#10;&#10;&lt;p&gt;A solution which came to my mind was fitting a linear model to &lt;code&gt;DV~IV1+IV2&lt;/code&gt; (R syntax) an taking the coefficients as the influence. I'm not sure if this is the right way of looking at the data because I can't be sure that there is a linear dependency between DV and IV1, IV2.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; lm(DV~IV1+IV2, data=d)&#10;Coefficients:&#10;   (Intercept)       IV1            IV2  &#10;     6.2580969       0.0002853      -0.0703225  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-07-12T15:23:07.090" FavoriteCount="1" Id="32175" LastActivityDate="2012-07-12T18:26:11.240" LastEditDate="2012-07-12T17:26:03.887" LastEditorUserId="7119" OwnerUserId="7119" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;anova&gt;" Title="Quantify influence in ANOVA" ViewCount="153" />
  
  <row AcceptedAnswerId="32184" AnswerCount="1" Body="&lt;p&gt;Let's say $X$ has a log-normal distribution and there is one real positive number $c$. then is it right to say that $(X -c)$ also has some log-normal distribution? My feeling is that, it can't be, because $(X - c)$ may take negative value whereas a log-normal distribution is only defined on the positive domain. Can somebody disprove that?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-12T18:08:39.013" FavoriteCount="2" Id="32183" LastActivityDate="2012-07-13T13:25:35.023" LastEditDate="2012-07-13T02:39:40.730" LastEditorUserId="4856" OwnerUserId="5738" PostTypeId="1" Score="8" Tags="&lt;lognormal&gt;" Title="If $X$ has a log-normal distribution, does $X-c$ also have a log-normal distribution?" ViewCount="739" />
  <row Body="&lt;p&gt;&lt;strong&gt;The answer to your question is (essentially) no&lt;/strong&gt; and your argument has the right idea. Below, we formalize it a bit. (For an explanation of the caveat above, see @whuber's comment below.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If $X$ has a &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot; rel=&quot;nofollow&quot;&gt;lognormal distribution&lt;/a&gt; this means that $\log(X)$ has a normal distribution. Another way of saying this is that $X = e^{Z}$ where $Z$ has a $N(\mu, \sigma^2)$ distribution for some $\mu \in \mathbb{R}, \sigma^2 &amp;gt;0$. Note that &lt;em&gt;by construction&lt;/em&gt;, this implies that $X \geq 0$ with probability one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, $X-c = e^Z - c$ cannot have a lognormal distribution because&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(e^Z - c &amp;lt; 0 ) = P(e^Z &amp;lt; c) = P(Z &amp;lt; \log(c)) = \Phi \left( \frac{ \log(c) - \mu }{\sigma} \right) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;which is strictly positive for any $c &amp;gt; 0$. Therefore, $e^Z - c$ has a positive probability of taking on negative values, which precludes $e^Z - c$ from being lognormally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, the lognormal distribution is not closed under subtraction of a positive constant. It is, however, closed under &lt;em&gt;multiplication&lt;/em&gt; by a (positive) constant, but that's an entirely different question. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-12T18:27:51.580" Id="32184" LastActivityDate="2012-07-13T13:25:35.023" LastEditDate="2012-07-13T13:25:35.023" LastEditorUserId="4856" OwnerUserId="4856" ParentId="32183" PostTypeId="2" Score="13" />
  <row AnswerCount="2" Body="&lt;p&gt;I am doing a one way ANOVA (per species) with custom contrasts.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3] [,4]&#10;0.5    -1    0    0    0&#10;5       1   -1    0    0&#10;12.5    0    1   -1    0&#10;25      0    0    1   -1&#10;50      0    0    0    1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where I compare intensity 0.5 against 5, 5 against 12.5 and so on. These is the data I'm working on&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L7uVk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;with the following results&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Generalized least squares fit by REML&#10;  Model: dark ~ intensity &#10;  Data: skofijski.diurnal[skofijski.diurnal$species == &quot;niphargus&quot;, ] &#10;       AIC      BIC    logLik&#10;  63.41333 67.66163 -25.70667&#10;&#10;Coefficients:&#10;            Value Std.Error  t-value p-value&#10;(Intercept) 16.95 0.2140872 79.17334  0.0000&#10;intensity1   2.20 0.4281744  5.13809  0.0001&#10;intensity2   1.40 0.5244044  2.66970  0.0175&#10;intensity3   2.10 0.5244044  4.00454  0.0011&#10;intensity4   1.80 0.4281744  4.20389  0.0008&#10;&#10; Correlation: &#10;           (Intr) intns1 intns2 intns3&#10;intensity1 0.000                      &#10;intensity2 0.000  0.612               &#10;intensity3 0.000  0.408  0.667        &#10;intensity4 0.000  0.250  0.408  0.612 &#10;&#10;Standardized residuals:&#10;       Min         Q1        Med         Q3        Max &#10;-2.3500484 -0.7833495  0.2611165  0.7833495  1.3055824 &#10;&#10;Residual standard error: 0.9574271 &#10;Degrees of freedom: 20 total; 15 residual&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;16.95 is the global mean for &quot;niphargus&quot;. In intensity1, I'm comparing means for intensity 0.5 against 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I understood this right, the coefficient for intensity1 of 2.2 should be half the difference between means of intensity levels 0.5 and 5. However, my hand calculations don't match those of the summary. Can anyone chip in what am I doing wrong?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ce1 &amp;lt;- skofijski.diurnal$intensity&#10;    levels(ce1) &amp;lt;- c(&quot;0.5&quot;, &quot;5&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;)&#10;    ce1 &amp;lt;- as.factor(as.character(ce1))&#10;    tapply(skofijski.diurnal$dark, ce1, mean)&#10;       0    0.5      5 &#10;  14.500 11.875 13.000 &#10;diff(tapply(skofijski.diurnal$dark, ce1, mean))/2&#10;      0.5       5 &#10;  -1.3125  0.5625 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-07-12T20:00:55.800" FavoriteCount="3" Id="32188" LastActivityDate="2012-11-10T12:58:50.317" LastEditDate="2012-07-12T20:11:19.197" LastEditorUserId="930" OwnerUserId="144" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;anova&gt;&lt;contrasts&gt;&lt;gls&gt;" Title="How to interpret these custom contrasts?" ViewCount="892" />
  <row Body="&lt;p&gt;As I mentioned when I answered your other question, you need to take the ratio of the sum of squares to the total to get percentage of variance explained.  But the F tests use ratios of mean squares. So th mean squares play a role in testing differences in group means.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-13T01:23:39.590" Id="32207" LastActivityDate="2012-07-14T11:34:01.503" LastEditDate="2012-07-14T11:34:01.503" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32206" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let's say I am trying to estimate $E(y | x_1, x_2,...)$. The dataset has two subcategories, say URBAN and RURAL. The descriptive stats of the two subsets are very different, as are the coefficients from regressions run separately. For example, I can see that $E(y|x_1)$, $E(y|x_2)$ etc. are all positive for URBAN and all negative for rural. In this case, is it still justified to run a pooled regression with interactions thrown in? Or, do we assume that two samples are really from two different populations and hence the models should be estimated separately? Is there any way to test (frequentist or bayesian) if the two samples came from the same underlying population?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-13T03:46:25.120" Id="32210" LastActivityDate="2015-02-10T17:45:17.193" LastEditDate="2014-05-10T23:20:55.700" LastEditorUserId="26338" OwnerUserId="3671" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;sampling&gt;" Title="Pool samples which &quot;look&quot; too different?" ViewCount="66" />
  <row AnswerCount="2" Body="&lt;p&gt;I am wondering if there are any good rules of thumb for how to go about selecting an approximate inference algorithm for a problem/model (specifically when exact inference is intractable)? When you are faced with a problem, what are the things you consider when selecting an approach for inference (e.g. MCMC, belief propagation, variational, etc.)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-13T06:04:13.763" Id="32214" LastActivityDate="2012-07-18T06:35:06.150" OwnerUserId="1913" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;inference&gt;&lt;approximation&gt;" Title="How to go about selecting an algorithm for approximate Bayesian inference" ViewCount="132" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have two time series a and b, which I want to compare. Due to their range difference I normalize them first.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a(i), b(i) are natural numbers for i=1,...,N&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;two different normalizations:&lt;/p&gt;&#10;&#10;&lt;p&gt;( mean and std_dev both refer to the whole time series )&lt;/p&gt;&#10;&#10;&lt;p&gt;1) &lt;code&gt;a'(i) := a(i) / mean( a )&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;goal&lt;/em&gt;: mean( a' ) = 1&lt;/p&gt;&#10;&#10;&lt;p&gt;2) &lt;code&gt;a'(i) := [ a(i) - mean( a ) ] / std_dev( a )&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;goal&lt;/em&gt;: usual normalization &lt;/p&gt;&#10;&#10;&lt;p&gt;what confuses me is how do the meanings after those transformations differ?&lt;/p&gt;&#10;&#10;&lt;p&gt;does the first transformation make any sense at all?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-13T09:31:50.203" FavoriteCount="1" Id="32221" LastActivityDate="2014-07-22T15:30:24.450" LastEditDate="2012-07-13T09:59:08.107" LastEditorUserId="930" OwnerUserId="3077" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;normalization&gt;" Title="How to normalize two time series for comparison?" ViewCount="6030" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;As described in Merlo et al (&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/16537344&quot; rel=&quot;nofollow&quot;&gt;J Epidem Comm Health 2006&lt;/a&gt;), the 95% credible interval for MOR is calculated using MCMC. MOR is defined as $\exp(\sqrt{2\sigma^2}\times 0.675)$, where $\sigma$ is the level-2 variance of the random intercept $u$ from a null model of a hierarchical logistic regression.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have an idea of how to write a program for an Markov chain Monte Carlo to calculate the standard error of the  median odds ratio (MOR) using &lt;a href=&quot;http://cran.r-project.org/web/packages/rjags/index.html&quot; rel=&quot;nofollow&quot;&gt;rjags&lt;/a&gt;?&lt;br&gt;&#10;My dependent variable is outcome(alive/dead) and the clustering (level2)variable is Hospital. There are 140 hospitals and would like to see variations in outcome between hospitals. Other risk factors will be included later as independent level1 variables.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-07-13T15:55:13.753" FavoriteCount="1" Id="32239" LastActivityDate="2012-07-15T12:03:48.410" LastEditDate="2012-07-15T12:03:48.410" LastEditorUserId="12629" OwnerUserId="12629" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;jags&gt;&lt;multilevel-analysis&gt;" Title="How to implement credible 95% interval for median odds ratio using JAGS?" ViewCount="648" />
  <row AcceptedAnswerId="32244" AnswerCount="1" Body="&lt;p&gt;Suppose we have weekly data for some attribute (e.g. housing prices). Say that we have $500$ weeks worth of housing price data. Suppose some major event happened on week $256$. If we want to detect any significant changes in housing prices after week $256$ versus before week $256$, would it be better to use a longer time scale? Maybe convert weeks into months? What other models would you suggest?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-13T17:15:27.727" Id="32242" LastActivityDate="2012-07-13T23:15:31.127" LastEditDate="2012-07-13T18:13:27.430" LastEditorUserId="88" OwnerUserId="5111" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;predictive-models&gt;" Title="Time intervals for modeling" ViewCount="73" />
  
  <row AnswerCount="1" Body="&lt;p&gt;When there are more treated than control, is it possible to do 1-to-1 matching (without replacement)?&lt;/p&gt;&#10;&#10;&lt;p&gt;It makes sense to do 1-to-1 when there are less treated than control but I am not sure if one can do that if there are more treated. It looks like a good way to introduce bias and doesn't make sense to selectively throw out treated.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I guess full matching is a good possibility when there are less control than treated but what if I want to do 1-to-1 in the same case? &lt;/p&gt;&#10;&#10;&lt;p&gt;The only way I can think of that may be valid is to randomly sample smaller number of treated than control and do 1-to-1, and do this over many times... &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-13T17:26:17.953" FavoriteCount="1" Id="32243" LastActivityDate="2012-07-14T21:11:43.713" LastEditDate="2012-07-14T21:11:43.713" LastEditorUserId="12357" OwnerUserId="12357" PostTypeId="1" Score="3" Tags="&lt;econometrics&gt;&lt;propensity-scores&gt;&lt;matching&gt;" Title="How to perform 1-to-1 matching when there are more treated than control subjects?" ViewCount="260" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I was tutoring a Stat student taking a probability course and I came across a problem dealing with variance of exponential that has left me confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;$X \sim \mathrm{Exp}(\mathrm{rate}=\lambda)$ so $E(X)=1/\lambda$ and $\newcommand{\Var}{\mathrm{Var}}\Var(X)=1/\lambda^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Customers enter a store at a rate of 2 per hour, a customer has just arrived, what is the variability of the amount of time until the next customer arrives in minutes?&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lambda=2, \; E(X)=1/2, \; \Var(X)=1/2^2=1/4$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Here the expected value = 30 minutes (1/2 of an hour) and the variance = 15 minutes (1/4 of an hour)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now work on minutes scale:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lambda=2/60=1/30, \; E(X)=30, \; \Var(X)=30^2=900$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here the expected value = 30 minutes but the variance = 900 minutes&lt;/p&gt;&#10;&#10;&lt;p&gt;I am confused why I am not getting the same variance when I switch from minutes to hour and vice versa.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-13T20:00:00.153" Id="32257" LastActivityDate="2013-01-10T13:07:15.467" LastEditDate="2013-01-10T13:07:15.467" LastEditorUserId="17230" OwnerUserId="2310" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;exponential&gt;" Title="Variance of exponential distribution when changing units" ViewCount="438" />
  <row AcceptedAnswerId="32289" AnswerCount="3" Body="&lt;p&gt;Suppose are three time series, $X_1$, $X_2$ and $Y$&lt;/p&gt;&#10;&#10;&lt;p&gt;Running ordinary linear regression on $Y$ ~ $X_1$ ($Y = b X_1 + b_0 + \epsilon$ ), we get $R^2 = U$. The ordinary linear regression $Y$ ~ $X_2$ get $R^2 = V$. Assume $U &amp;lt; V$&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the minimum and maximum possible values of $R^2$ on regression $Y$ ~ $X_1 + X_2$ ($Y = b_1 X_1 + b_2 X_2 + b_0 + \epsilon$ )? &lt;/p&gt;&#10;&#10;&lt;p&gt;I believe the minimum $R^2$ should be $V$ + a small value, since adding new variables always increases $R^2$, but I don't know how to quantify this small value, and I don't know how to obtain the maximum range. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-14T03:40:48.643" FavoriteCount="1" Id="32272" LastActivityDate="2012-07-18T12:53:47.437" LastEditDate="2012-07-15T12:03:02.403" LastEditorUserId="88" OwnerUserId="8086" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;multiple-regression&gt;" Title="Possible range of $R^2$" ViewCount="264" />
  
  
  <row Body="&lt;p&gt;I'm sure there is, but I don't know it.  Moreover it would be fairly complex, because you would also have to account for the correlations amongst the covariates.  That is, you would need $corr(X_1,X_2)$ as well.  The resulting formula would need to be expressed in matrix notation to make it possible to fit on one line and be able to account for any number of covariates, but that might also make it more opaque for most people unless they are very literate with matrix notation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;One simple thing I can say is that your regression model will yield a single vector of predicted values, $\hat y$.  &lt;em&gt;Under the assumption that your model has an intercept&lt;/em&gt;, the correlation between these values and the observed ones, $corr(\hat y,y)$, will always equal the square root of $R^2$, no matter how many covariates you have.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-14T18:33:13.667" Id="32296" LastActivityDate="2012-07-14T18:54:04.703" LastEditDate="2012-07-14T18:54:04.703" LastEditorUserId="7290" OwnerUserId="7290" ParentId="32294" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="32298" AnswerCount="3" Body="&lt;p&gt;What is the difference between Superpopulation and Infinite population? Please explain this with examples. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-14T19:01:40.673" Id="32297" LastActivityDate="2014-05-22T12:08:44.680" LastEditDate="2012-07-15T12:05:10.093" LastEditorUserId="88" OwnerUserId="11889" PostTypeId="1" Score="6" Tags="&lt;population&gt;&lt;definition&gt;" Title="Difference between superpopulation and infinite population" ViewCount="2293" />
  <row Body="&lt;p&gt;For two predictors, it is easy to write out the equation in algebraic form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R^2 = \frac{r^2_{x1,y} + r^2_{x2,y} - 2r_{x1,y}r_{x2,y}r_{x1,x2}}{1-r^2_{x1,x2}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As pointed out by @gung, you also need to know the correlation between $x1$ and $x2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Just a quick example (in R) to illustrate this equation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(12873)&#10;&#10;x1 &amp;lt;- rnorm(20)&#10;x2 &amp;lt;- .1*x1 + rnorm(20)&#10;y  &amp;lt;- .8*x1 + .2*x2 + rnorm(20)&#10;&#10;summary(lm(y ~ x1 + x2))$r.square&#10;(cor(x1,y)^2 + cor(x2,y)^2 - 2*cor(x1,y)*cor(x2,y)*cor(x1,x2))/(1-cor(x1,x2)^2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives the exact same answer of &lt;code&gt;0.2928677&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-14T19:42:52.043" Id="32301" LastActivityDate="2012-07-15T17:13:34.810" LastEditDate="2012-07-15T17:13:34.810" LastEditorUserId="1934" OwnerUserId="1934" ParentId="32294" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;If you compute the inverse of the correlation matrix of a set of variables, then take one minus the reciprocal of the diagonal elements ($1-\frac{1}{r_{ii}}$) then the results are the same as the $R^2$ value for the given term as response variable and all others as predictors.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(lm( Sepal.Length ~ .-Species, data=iris ))$r.squared&#10;    [1] 0.8586117&#10;    &amp;gt; summary(lm( Sepal.Width ~ .-Species, data=iris ))$r.squared&#10;[1] 0.5240071&#10;&amp;gt; summary(lm( Petal.Length ~ .-Species, data=iris ))$r.squared&#10;    [1] 0.9680118&#10;    &amp;gt; summary(lm( Petal.Width ~ .-Species, data=iris ))$r.squared&#10;[1] 0.9378503&#10;&amp;gt; 1-1/diag(solve(cor(iris[,-5])))&#10;Sepal.Length  Sepal.Width Petal.Length  Petal.Width &#10;   0.8586117    0.5240071    0.9680118    0.9378503 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-07-14T23:07:45.377" Id="32305" LastActivityDate="2012-07-14T23:07:45.377" OwnerUserId="4505" ParentId="32294" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;When setting up the two regressions, R treats them differently. In the first (with intercept) case, it takes the first factor and re-frames it as a smaller number of factors, each representing the difference between a level and the first level (so there are 2 coefficients to estimate in your case instead of 3.)  This avoids the multicollinearity that would be caused by the &quot;full&quot; representation + an intercept.  It then does the same for the second. Hence you get an intercept and 3 total coefficients. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the second case, it knows there isn't an intercept, so it doesn't re-frame the first factor, instead leaving it as is.  It still re-frames the second factor, though, to avoid the multicollinearity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's your example; we can look at the coefficient values to see what's happening:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x1=factor(rep(1:3, 100))&#10;x2=factor(rep(1:2, 150))&#10;y=rnorm(300)&#10;&#10;summary(lm(y~x1+x2+1))&#10; ... stuff ... &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;(Intercept)  0.13537    0.11742   1.153   0.2499  &#10;x12         -0.20787    0.14381  -1.445   0.1494  &#10;x13         -0.28883    0.14381  -2.008   0.0455 *&#10;x22          0.05656    0.11742   0.482   0.6304  &#10;---&#10;&#10;summary(lm(y~x1+x2-1))&#10; ... stuff removed ...&#10;&#10;Coefficients:&#10;    Estimate Std. Error t value Pr(&amp;gt;|t|)&#10;x11  0.13537    0.11742   1.153    0.250&#10;x12 -0.07250    0.11742  -0.617    0.537&#10;x13 -0.15346    0.11742  -1.307    0.192&#10;x22  0.05656    0.11742   0.482    0.630&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see that the &lt;code&gt;Intercept&lt;/code&gt; in the first regression has the same value as &lt;code&gt;x11&lt;/code&gt; in the second.  &lt;code&gt;x12&lt;/code&gt; in the first equals &lt;code&gt;x12-x11&lt;/code&gt; in the second, and &lt;code&gt;x13&lt;/code&gt; in the first equals &lt;code&gt;x13-x11&lt;/code&gt; in the second, both as a consequence of representing the factors as differences in the first regression.  &lt;code&gt;x22&lt;/code&gt; is the same in both, because including all the levels of the second factor will result in multicollinearity in both regressions.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-07-15T01:23:43.310" Id="32308" LastActivityDate="2012-07-15T01:23:43.310" OwnerUserId="7555" ParentId="32241" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;I have an annual time series of data of a growth-rate variable $X$ for 50 years. Most of the values for the variable $X$ are less than 10%. The exception are two values that are around 30%. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I treat these two values in the regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to seek your suggestions in this regard. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-15T03:28:30.460" Id="32312" LastActivityDate="2013-06-05T04:18:02.950" LastEditDate="2013-06-05T04:18:02.950" LastEditorUserId="805" OwnerUserId="12625" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;outliers&gt;" Title="Treatment of outliers in annual time series data" ViewCount="319" />
  
  
  <row AcceptedAnswerId="32330" AnswerCount="1" Body="&lt;p&gt;I Have a data set containing about 40 categorical variables. I am trying to factor analyze them. But each categorical variable contains a good number of missing values. Some of them are simply because of non-response. The respondent did not filled up any answer option for that question. Some of them are due to questions of the following type:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;5) Do you create formal work teams in your institution? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1=&quot;NO&quot; 2=&quot;YES&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;  &#10;  &lt;p&gt;(Please skip question number 6 and 7 whose answer to this question&#10;  is 1=&quot;NO&quot;)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;6) How many members form the work team? (for example)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;7) What is the criterion of selecting team members? (for example)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now those who answered &quot;NO&quot; for question number 5 will not answer 6 and 7. He will again start from 8. This is another source of missing information or gap in the data set. Because of specially this type of missing values if I omit missings listwise a lots of information is missed.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I am looking for adjusting these missing values. I don't know how to adjust these missing values (of both non-response and the second type I mentioned) for categorical variables. Taking mean, median or even EM algorithm may not be appropriate for categorical variable I guess. So, what should be done and how?&lt;/p&gt;&#10;&#10;&lt;p&gt;My actual number of observations is 212, but it reduces to only 42 when I use &lt;code&gt;na.omit(data).&lt;/code&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-15T09:25:55.960" FavoriteCount="1" Id="32317" LastActivityDate="2012-07-15T14:06:30.863" LastEditDate="2012-07-15T14:06:30.863" LastEditorUserId="3277" OwnerUserId="12603" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;&lt;missing-data&gt;&lt;data-imputation&gt;" Title="Adjustment for missing values of the categorical variables in a data set" ViewCount="745" />
  <row Body="&lt;p&gt;This would depend on what you know about the outliers. They should not be removed unless there is some obvious error.  They could be indicative of interventions.  First step is to understand why you have them.  Then you can decide how to treat them.  In addition to the covariate you might want to include autoregressive terms in the model. You may also want to investigate how the outliers affect the regression parameter for X (perhaps by using influence functions). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-15T12:54:29.830" Id="32326" LastActivityDate="2012-07-15T12:54:29.830" OwnerUserId="11032" ParentId="32312" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Prz.  In general an ARIMA model is an optimization of the number of previous periods to use in the weighting scheme AND the values of the coefficients . You are assuming 3 periods and the coefficients are equal. Care must be taken to identify level shifts , seasonal pulses, pulses and Local time trends while ensuring that the parameters (coefficients) haven't changed over tome and that the error variance is homogeneous. For more on ARIMA modelling and Intervention Detection please see work by Tsay &lt;a href=&quot;http://www.unc.edu/~jbhill/tsay.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.unc.edu/~jbhill/tsay.pdf&lt;/a&gt; and perhaps some of my previous posts here at SE &lt;a href=&quot;http://stats.stackexchange.com/users/3382/irishstat?tab=activity&quot;&gt;http://stats.stackexchange.com/users/3382/irishstat?tab=activity&lt;/a&gt;. Hope this helps. If you wish to post an example time series, I will try and give you some pointers on this data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-15T21:13:47.100" Id="32352" LastActivityDate="2012-07-15T21:13:47.100" OwnerUserId="3382" ParentId="32247" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to create a function to determine annual chance of flooding given a specific ground elevation.  The data I have available to me are water surface elevations at 5 specific recurrence intervals &lt;code&gt;(10yr, 25yr, 50yr, 100yr, and 500yr)&lt;/code&gt;.  My initial approach was to run a linear regression using a log10 of the water surface values as the y axis and a log10 value for each interval.  This did not yield very reliable results.  Since then I have began exploring a piecewise linear interpolation solution. &lt;code&gt;y = f(x)&lt;/code&gt; as it is described in the scipy interp1d. I can not seem to get this to work either.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen this solution in Excel using Trend and some other functions, but I will need to run this over well over 1 million records.  Again the idea is if you have a ground elevation of 556.345, where does it best fit between 5 intervals. How can I do this?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-26T02:23:03.060" Id="32358" LastActivityDate="2012-07-16T08:25:22.737" LastEditDate="2012-07-16T08:25:22.737" LastEditorUserId="8507" OwnerDisplayName="jhuffines" PostTypeId="1" Score="0" Tags="&lt;python&gt;&lt;interpolation&gt;" Title="Pythonic solution for determining annual chance flooding" ViewCount="43" />
  <row Body="&lt;p&gt;@Blain. Your scoring method for regular or no meetings is not correct. Probaly, you do not have a reason for 2,1 or 1,2. The profitablity is continuous variable. May be you can look for possibility of applying point-biserial correlation. Also, the objeicve you have in mind may be realised through a linear multinomial type regression or a mixed model (nominal preferably, simple regression with some control variable.The factor analysis is a data-reduction technique and it should be used with care.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-16T03:48:04.977" Id="32362" LastActivityDate="2012-07-16T03:48:04.977" OwnerUserId="10619" ParentId="31936" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="32366" AnswerCount="1" Body="&lt;p&gt;I have 3 random variables, A, B and C. Given A, variables B and C are independent.  I have estimates of the variance of A, B and C and the covariance between A and B and A and C. Is it possible calculate an implied covariance between B and C?&lt;/p&gt;&#10;&#10;&lt;p&gt;My application is that I have a large number of random variables (N) and need to calculate an empirical covariance matrix from a limited number (M) of samples, where M &amp;lt; N. However, most of the random variables are independent of eachother given a few of the other variables, and it seems like I should be able to estimate the covariance matrix more efficiently by using the fact that many of the variables are conditionally independent.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-16T05:46:10.777" FavoriteCount="1" Id="32364" LastActivityDate="2012-07-16T07:22:26.067" OwnerUserId="1146" PostTypeId="1" Score="1" Tags="&lt;covariance&gt;" Title="Calculating induced covariances" ViewCount="86" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I know there are some other questions out there on book recommendations, but I've looked into a lot of the books in those answers and haven't quite found what I'm looking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am going to be starting a Masters program in Predictive Analytics this fall.  My background is Bachelor's in CS with a couple years experience doing software development in the industry.  I had an introductory statistics course in college, but it was rather worthless and, having not used it for a couple years, I've forgotten most of the information I did learn.&lt;/p&gt;&#10;&#10;&lt;p&gt;The topics for the prereq introductory course (for which I will likely receive a waiver, given my background) include: descriptive statistics, elementary probability rules, sampling, distributions, confidence intervals, correlation, regression and hypothesis testing.  A book that provides at a minimum a solid understanding of these topics would be great.  I imagine this is more of an applied statistics situation, versus deep mathematical foundations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for some recommendations on books that can get me prepared for the Masters program again.  Introductory level and beyond.  What I really want is some books that have problem sets and availability of corresponding answers.  In order to learn, I really need to put the concepts to work, and would like to be able to check the answers I come up with.&lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2013-02-12T17:26:33.383" CreationDate="2012-07-16T15:36:35.617" Id="32392" LastActivityDate="2013-05-13T09:04:03.263" LastEditDate="2012-07-16T16:31:12.390" LastEditorUserId="88" OwnerUserId="12596" PostTypeId="1" Score="4" Tags="&lt;books&gt;" Title="Textbooks with problem sets to prepare for predictive analytics masters" ViewCount="506" />
  
  <row Body="&lt;p&gt;Since a motif is a pattern that gets repeated in a time series if you observe the motif repeating itself after a certain time lag in the series then you can forecast a future occurrence of the motify in the future one time lag ahead of where it last occurred.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you clarified that you have labels for your series and based on an observed motif you want to identify the right label, I would say that if each time series has different motifs then observing a particular motif would match it to a particular series and thereby identify the correct label.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-16T19:12:53.263" Id="32404" LastActivityDate="2012-07-16T19:50:38.550" LastEditDate="2012-07-16T19:50:38.550" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32403" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In several situations, I have two unbiased estimators, and I know one of them is better (lower variance) than the other. However, I would like to get as much information as possible, and I would like to do better than throwing out the weaker estimator.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\newcommand{\Outcome}{\text{Outcome}}\newcommand{\Skill}{\text{Skill}}\newcommand{\Luck}{\text{Luck}}\Outcome = \Skill + \Luck$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Outcome$ is observed. $\Skill$ is what I would like to determine. $\Luck$ is known to have the average value $0$. From other observables, I can estimate $\Luck$ by $L$ so that $\mathbb E(L) = 0$ and $\mathrm{Var}(\Luck-L) \lt \mathrm{Var}(\Luck)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Outcome$ is an unbiased estimator for $\Skill$. A better estimate from variance reduction  is $\Outcome - L$, which is also unbiased. For example, in one situation $\Outcome$ is the average of repeated trials, and I might produce a $95\%$ confidence interval of $[-5.0,13.0]$ without using variance reduction. Using variance reduction, I might get a confidence interval of $[-2.0,4.0]$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The typical practice is for people to use $\Outcome-L$ instead of $\Outcome$. However, this is unsatisfactory to me because in my experience, there is more information in the pair $(\Outcome, \Outcome-L)$ than in just $\Outcome-L$. Specifically, in some situations I know that if $\Outcome$ is low, then $\Outcome-L$ tends to be an underestimate for $\Skill$, and if $\Outcome$ is high, then $\Outcome-L$ tends to be an overestimate for $\Skill$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What's a good way to take advantage of the extra information from knowing both estimators?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-17T02:34:07.143" Id="32430" LastActivityDate="2012-07-17T20:28:04.547" LastEditDate="2012-07-17T13:59:31.303" LastEditorUserId="2970" OwnerUserId="11981" PostTypeId="1" Score="4" Tags="&lt;estimation&gt;&lt;unbiased-estimator&gt;" Title="Two unbiased estimators for the same quantity" ViewCount="264" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of univariate data, where the response variable in a trial is a sequence of time intervals between button presses (my experimental subjs have to press a button intermittently to react to some task conditions).   Eventually I have to run a within-subject regression to see if  my hypothesized predictors are indeed significant.  However, the first thing I want to do is to show that the subjects didn't just press the button randomly. In other words, I need to make sure that the person didn't just press a button at a regular pace, (averaged response times + random variability).  &lt;/p&gt;&#10;&#10;&lt;p&gt;One idea I have is to check if my data resemble a Poisson process, in which arrival times occur randomly at a certain rate. Based on what I've looked up on using Poisson process so far, there are two approaches to fit the poisson process to my time interval data: &#10;1)  count how many times the person pressed a button in each trial of fixed time period. Then I can follow the Poisson distribution fitting process to count data.&lt;br&gt;&#10;2) the wait times between events in a poisson process is said to follow an exponential distribution.  So I can try to fit &quot;exponential&quot; distribution to the time intervals.  In that case, should I try to fit a bunch of different distributions such as Exponential, Gamma, and Weibull?  &lt;/p&gt;&#10;&#10;&lt;p&gt;In either case, I would then use a chi-square goodness of fit test and/or a QQ-plot to assess whether the distribution fit can be rejected. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice would be greatly appreciated. Is fitting to Poisson Process to my data even the right way to go? I would be happy to hear about any other suggestions. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-17T04:12:12.483" FavoriteCount="1" Id="32436" LastActivityDate="2012-07-17T22:06:16.260" LastEditDate="2012-07-17T22:06:16.260" LastEditorUserId="12083" OwnerUserId="12083" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;poisson&gt;&lt;exponential-family&gt;" Title="How to test whether button pressing behaviour is random or is in response to a stimulus?" ViewCount="133" />
  <row AcceptedAnswerId="32461" AnswerCount="2" Body="&lt;p&gt;I am researching Sentiment Analysis over social media, particularly classifying online texts such as blog posts as positive, negative or neutral. &lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the approaches I have found for sentiment analysis are supervised (they need labeled data to train a classifier). However, I have also found a couple of papers that do it using joint topic-sentiment models (unsupervised) like &lt;a href=&quot;http://people.kmi.open.ac.uk/yulan/PAPERS/JST_CIKM09.pdf&quot; rel=&quot;nofollow&quot;&gt;this one&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;According to the results in the topic model papers, the main advantage of unsupervised approaches based on topic models is that they do no need any labeled data (apart from prior &quot;general&quot; sentiment information, i.e. a dictionary of positive/negative words). However, they do not reach the accuracy of a supervised approach (2% less of accuracy).&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any other advantages/disadvantages for using topic-sentiment models for sentiment classification instead of supervised approaches?   &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-17T07:47:38.733" FavoriteCount="4" Id="32441" LastActivityDate="2012-10-11T12:53:58.860" LastEditDate="2012-10-11T12:53:58.860" LastEditorUserId="88" OwnerUserId="7556" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;unsupervised-learning&gt;&lt;topic-models&gt;&lt;sentiment-analysis&gt;" Title="Supervised approaches vs. topic models in sentiment analysis" ViewCount="1536" />
  <row Body="&lt;p&gt;Let me add to Bogdanovist's excellent answer that&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;cross validation is unbiased for what it measures: the predicitive abilities of &quot;surrogate&quot; models with respect to the data at hand (&quot;drawn from the same population&quot;).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the often stated pessimistic bias arises in situations where the surrogate models are on average worse than the real model, usually because of the smaller training sample size (even if the drawn from the same population assumption is true)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;This paper stresses the &quot;drawn from the same population&quot; problems, particularly the drift over time:&#10;&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/cem.1310/abstract&quot; rel=&quot;nofollow&quot;&gt;Esbensen and Geladi: Principles of Proper Validation: use and abuse of re-sampling for validation, Journal of Chemometrics, Volume 24, Issue 3-4, pages 168-187, March-April 2010&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-07-17T08:41:57.587" Id="32443" LastActivityDate="2012-07-17T08:41:57.587" OwnerUserId="4598" ParentId="32426" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I think that the answer here might be that you're comparing apples and oranges.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $F(x)$ denote the cdf of the Mann-Whitney $U$ statistic. &lt;code&gt;qwilcox&lt;/code&gt; is the quantile function $Q(\alpha)$ of $U$. By definition, it is therefore&#10;$$Q(\alpha)=\inf \{x\in \mathbb{N}: F(x)\geq \alpha\},\qquad \alpha\in(0,1).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Because $U$ is discrete, there is usually no $x$ such that $F(x)=\alpha$, so typically $F(Q(\alpha))&amp;gt;\alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, consider the critical value $C(\alpha)$ for the test. In this case, you want $F(C(\alpha))\leq \alpha$, since you otherwise will have a test with a &lt;a href=&quot;http://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error&quot; rel=&quot;nofollow&quot;&gt;type I error rate&lt;/a&gt; that is &lt;em&gt;larger&lt;/em&gt; than the nominal one. This is usually considered to be undesirable; &lt;em&gt;conservative&lt;/em&gt; tests tend to be prefered. Hence,&#10;$$C(\alpha)=\sup \{x\in \mathbb{N}: F(x)\leq \alpha\},\qquad \alpha\in(0,1).$$&#10;Unless there is an $x$ such that $F(x)=\alpha$, we therefore have $C(\alpha)=Q(\alpha)-1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason for the discrepancy is that &lt;code&gt;qwilcox&lt;/code&gt; has been designed to compute quantiles and not critical values!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-17T12:04:47.020" Id="32451" LastActivityDate="2012-07-17T15:20:57.053" LastEditDate="2012-07-17T15:20:57.053" LastEditorUserId="8507" OwnerUserId="8507" ParentId="32445" PostTypeId="2" Score="13" />
  
  <row Body="&lt;p&gt;Well, there is one ad hoc method that I've used before. I'm not sure if this procedure has a name but it makes sense intuitively. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose your goal is to fit the model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + \varepsilon_i $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where the two predictors - $X_i, Z_i$ - are highly correlated. As you've pointed out, using them both in the same model can do strange things to the coefficient estimates and $p$-values. An alternative is to fit the model &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Z_i = \alpha_0 + \alpha_1 X_i + \eta_i $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Then the residual $\eta_i$ will be uncorrelated with $X_i$ and can, in some sense, be thought of as the part of $Z_i$ that is not subsumed by its linear relationship with $X_i$. Then, you can proceed to fit the model &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_i = \theta_0 + \theta_1 X_i + \theta_2 \eta_i + \nu_i $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which will capture all of the effects of the first model (and will, indeed, have the exact same $R^2$ as the first model) but the predictors are no longer collinear. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; &lt;em&gt;The OP has asked for an explanation of why the residuals do not, definitionally, have a sample correlation of zero with the predictor when you omit intercept like they do when the intercept is included. This is too long to post in the comments so I made an edit here. This derivation is not particularly enlightening (unfortunately I couldn't come up with a reasonable intuitive argument) but it does show what the OP requested&lt;/em&gt;: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_linear_regression#Linear_regression_without_the_intercept_term&quot; rel=&quot;nofollow&quot;&gt;When the intercept is omitted in simple linear regression&lt;/a&gt;, $\hat \beta = \frac{ \sum x_i y_i}{\sum x_i^2}$, so $e_i = y_i - x_i \frac{ \sum x_i y_i}{\sum x_i^2}$. The sample correlation between $x_i$ and $e_i$ is proportional to $$\overline{xe} - \overline{x}\overline{e}$$ where $\overline{\cdot}$ denotes the sample mean of the quantity under the bar. I'll now show this is not necessarily equal to zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;First we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\overline{xe} = \frac{1}{n} \left( \sum x_i y_i - x_{i}^2 \cdot \frac{ \sum x_i y_i}{\sum x_i^2} \right) = \overline{xy} \left( 1 - \frac{ \sum x_{i}^2}{ \sum x_{i}^2 } \right) = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;but &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\overline{x} \overline{e} = \overline{x} \left( \overline{y} - \frac{\overline{x} \cdot \overline{xy}}{\overline{x^2}} \right) = \overline{x}\overline{y} - \frac{\overline{x}^2 \cdot \overline{xy}}{\overline{x^2}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;so in order for the $e_i$ and $x_i$ to have a sample correlation of exactly 0, we need $\overline{x}\overline{e}$ to be $0$. That is, we need $$ \overline{y} = \frac{ \overline{x} \cdot \overline{xy}}{\overline{x^2}} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which does not hold in general for two arbitrary sets of data $x, y$. &lt;/p&gt;&#10;" CommentCount="19" CreationDate="2012-07-17T15:46:33.293" Id="32473" LastActivityDate="2012-07-18T17:19:25.467" LastEditDate="2012-07-18T17:19:25.467" LastEditorUserId="4856" OwnerUserId="4856" ParentId="32471" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;You are not going to get just whole numbers this way.  Displaying values with no decimals is just a formatting choice.  The fractional values arising from your denominator will still be present.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you just sum the three variables, you are certainly likely to get some results that are the same for multiple cases.  Try a histogram of your new variable to see what the distribution looks like (Graphs&gt;Chart Builder).&lt;/p&gt;&#10;&#10;&lt;p&gt;But thinking ahead, what are you planning to do with this variable?  Why do you need to boil it down to one?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-17T16:19:22.087" Id="32476" LastActivityDate="2012-07-17T16:19:22.087" OwnerUserId="6768" ParentId="32397" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;This is a Likert type scale and involves ordinal numbers and since intervals are not defined nor are ratios there is really not a meaningful concept for it.  However when scores are averaged there are models such as the Rasch model which allow these averages to be viewed as interval data.  I really know nothing about these models but Peter Flom and others at this site have discussed these ideas here yesterday and prior to that. Gung and Peter Flom provide some information on this at this question. &lt;a href=&quot;http://stats.stackexchange.com/questions/32389/is-0-a-valid-value-in-a-likert-scale&quot;&gt;Is 0 a valid value in a likert scale?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If we accept that the average score is interval then the standard formula for the standard error of the mean could be applied to give you an estimate of the standard deviation for the mean.  I still would not think an standard deviation for the sample responses makes sense but you could calculate something that looks like a standard deviation by multiplying the standard error of the mean by square root of n where n is the number of independent responders.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-17T19:08:24.237" Id="32487" LastActivityDate="2012-07-17T19:08:24.237" OwnerUserId="11032" ParentId="32486" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Can I interpret the coefficients in a VAR model in the same way as I do in a normal OLS regression?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-17T19:09:44.867" FavoriteCount="1" Id="32488" LastActivityDate="2013-07-06T07:52:42.390" LastEditDate="2012-07-17T19:30:28.117" LastEditorUserId="4856" OwnerUserId="12682" PostTypeId="1" Score="6" Tags="&lt;interpretation&gt;&lt;regression-coefficients&gt;&lt;autoregressive&gt;" Title="How to interpret coefficients in a vector autoregressive model?" ViewCount="1382" />
  
  
  <row Body="Refers to the variables used in a model to predict a response. This tag can also be used for $X$ variables in explanatory &amp; descriptive modeling, not just predictive modeling. This same construct goes by many names in different contexts, including: independent variable, explanatory variable, regressor variable, covariate, etc. This tag can be used for any of these synonymous terms." CommentCount="0" CreationDate="2012-07-18T01:14:36.580" Id="32514" LastActivityDate="2012-07-18T01:49:53.827" LastEditDate="2012-07-18T01:49:53.827" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Michael Chernick points you in the right direction. I would also look at Ruey Tsay's work as that added to this body of knowledge. See more &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CE4QFjAA&amp;amp;url=http://www.unc.edu/~jbhill/tsay.pdf&amp;amp;ei=NKoGUI2fBoqm6wGMwu3ECA&amp;amp;usg=AFQjCNFyhuiw1FmwWs_jXa3sX6BkWP5B3Q&amp;amp;sig2=yaWLl9Ne5dfPNaz74tbCzw&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can't compete against today's automated computer algorithms.  They look at many ways to approach the time series that you haven't considered and often not documented in any paper or book. When one asks how to do an ANOVA, a precise answer can be expected when comparing against different algorithms. When one asks the question how do I do pattern recognition, many answers are possible as heuristics are involved. Your question involves the use of heuristics.&lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to fit an ARIMA model, if outliers exist in the data is to evaluate possible states of nature and to select that approach that is deemed optimal for a particular data set. One possible state of nature is that the ARIMA process is the primary source of explained variation. In this case one would &quot;tentatively identify&quot; the ARIMA process via the acf/pacf function and then examine the residuals for possible outliers. Outliers can be Pulses, i.e., one-time events OR seasonal pulses which are evidenced by systematic outliers at some frequency (say, 12 for monthly data). A third type of outlier is where one has a contiguous set of pulses, each having the same sign and magnitude, this is called a step or level shift.  After examining the residuals from the tentative ARIMA process one can then tentatively add the empirically identified deterministic structure to create a tentative combined model. Nor if the primary source of variation is one of the 4 kinds or &quot;outliers&quot; then one would be better served by identifying them ab initio (first) and then using the residuals from this &quot;regression model&quot; to identify the stochastic (ARIMA) structure. Now these two alternative strategies get a little more complicated when one has a &quot;problem&quot; where the ARIMA parameters change over time or the error variance changes over time due to a number of possible causes, possibly the need for weighted least squares or a power transform like logs / reciprocals, etc. Another complication / opportunity is how and when to form the contribution of user-suggested predictor series to form a seamlessly integrated model incorporating memory, causals and empirically identified dummy series. This problem is further exacerbated when one has trending series best modeled with indicator series of the form $0,0,0,0,1,2,3,4,...$, or $1,2,3,4,5,...n$ and combinations of level shift series like $0,0,0,0,0,0,1,1,1,1,1$. You might want to try and write such procedures in R, but life is short. I would be glad to actually solve your problem and demonstrate in this case how the procedure works, please post the data or send it to sales@autobox.com &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Additional comment after receiving / analyzing the data / daily data for a foreign exchange rate / 18=765 values starting 1/1/2007&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/p0dSu.jpg&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;The data had an acf of: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ja5Q3.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Upon identifying an arma model of the form $(1,1,0)(0,0,0)$ and a number of outliers the acf of the residuals indicates randomness since the acf values are very small. AUTOBOX identified a number of outliers: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VAr3a.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The final model:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BYr19.jpg&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;included the need for a variance stabilization augmentation a la TSAY where variance changes in the residuals were identified and incorporated. The problem that you had with your automatic run was that the procedure you were using, like an accountant, believes the data rather than challenging the data via Intervention Detection (a.k.a., Outlier Detection). I have posted a complete analysis &lt;a href=&quot;http://www.autobox.com/se/sajeeka.zip&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/uhCY9.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-07-18T12:21:33.590" Id="32537" LastActivityDate="2014-03-01T18:27:31.520" LastEditDate="2014-03-01T18:27:31.520" LastEditorUserId="7290" OwnerUserId="3382" ParentId="32528" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;The bootstrap is not needed here because the null distribution is well defined.  Bootstrap should not be used just because you are comfortable with simulation.  The bootstrap works in this situation and will give essentally the same results as the straightforward method.  So your motivation is not a factor here.  Sounds like you handled the first question properly.&lt;/p&gt;&#10;&#10;&lt;p&gt;For question 2 do a pairwise comparison of the proportions (two sample binomial or corresponding bootstrap).  Adjust the p-values for multiplicity.  The children with signifiicant p-values in the adjusted comparison can be considered to be treated differently with respect to the sweets received.&lt;/p&gt;&#10;&#10;&lt;p&gt;This answer was to the initial question before it was changed.  Random selection of a number to give a child muddies the waters (adds to the uncertainty).  Statistically significant differences between children can be due to preferentially handing to some children more than others (the effect of preference which you would be interested in) or just because by chance certain children happened to get the luck of the draw and get more sweets when selected (a random event not of interest).  As Aniko points out the problem is only well defined if you specify a probability distribution for the number drawn.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-18T13:23:33.123" Id="32545" LastActivityDate="2012-07-20T11:54:45.313" LastEditDate="2012-07-20T11:54:45.313" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32541" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;As you've described the problem, ${\boldsymbol Y} = \{Y_1,  Y_2, Y_3, Y_4\}$ will have a multivariate normal distribution with mean ${\boldsymbol \mu} = (\beta, \beta, \beta, \beta)'$ and covariance matrix &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \Sigma = \sigma^2 \left( \begin{array}{cccc} &#10;2 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \\ &#10;0 &amp;amp; 2 &amp;amp; 1 &amp;amp; 1 \\ &#10;1 &amp;amp; 1 &amp;amp; 3 &amp;amp; 1 \\&#10;1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 3 \\ \end{array} \right) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Normally this type of &lt;em&gt;covariance structure model&lt;/em&gt; would require some kind of software like MPLUS but I believe it may be simple enough to &quot;trick&quot; &lt;code&gt;lme&lt;/code&gt; into fitting a model like this but it is simple enough to &quot;build-your-own&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure about getting the unbiased minimum variance estimator (although I'm sure the ordinary sample mean would be competitive), but I can describe how to get the maximum likelihood estimator (MLE), which is desirable for the reasons mentioned by Michael Chernick. &lt;a href=&quot;http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Density_function&quot;&gt;The log-likelihood for a single observation of ${\boldsymbol Y}$ is&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;$$L(\beta, \sigma^2) = \log \left( \frac{1}{(2\pi)^2 |\Sigma|^{1/2}} \right) -\frac{1}{2} ({\boldsymbol Y}-{\boldsymbol \mu})' \Sigma^{-1} ({\boldsymbol Y}-{\boldsymbol \mu}) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;which is only a function of $\beta$ and $\sigma^2$ since ${\boldsymbol \mu}$ only depends on $\beta$ and $\Sigma$ only depends on $\sigma^2$. We sum over the observations and optimize the resulting function as a function of $\beta, \sigma^2$ to get the MLE. I'll use the &lt;code&gt;dmnorm()&lt;/code&gt; function from the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;mnormt&lt;/code&gt; to do this and give a rather crudely programmed example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1234) &#10;N &amp;lt;- 100 &#10;s = matrix(0,4,4)&#10;s[1,]=c(2,0,1,1)&#10;s[2,]=c(0,2,1,1)&#10;s[3,]=c(1,1,3,1)&#10;s[4,]=c(1,1,1,3)&#10;&#10;# generate data where true values are beta=1, sigma^2 = 3. &#10;y &amp;lt;- list()&#10;for(i in 1:N) y[[i]] &amp;lt;- rmnorm(1,mean=c(1,1,1,1),varcov=3*s)&#10;&#10;# P[1] is beta, P[2] is sigma squared&#10;L &amp;lt;- function(P)&#10;{&#10;   # crude barrier to prevent sigma squared being negative&#10;   if( P[2] &amp;lt;= 0 ) return(Inf) &#10;&#10;   like &amp;lt;- 0 &#10;   for(i in 1:N) &#10;   {&#10;      like &amp;lt;- like + dmnorm(y[[i]], mean=rep(P[1],4), varcov=P[2]*s, log=TRUE)&#10;   }&#10;   return(-like)&#10;}&#10;# chose arbitrary starting values of beta=1,sigma^2=1 for the optimization&#10;optim(c(1,1),L)$par&#10;[1] 0.9109401 3.0786393&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can get approximate confidence intervals either by bootstrapping or using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher_information&quot;&gt;fisher information&lt;/a&gt;, which will require derivatives of the log-likelihood either numerically (which is returned by &lt;code&gt;optim()&lt;/code&gt;) or analytically, which you may find &lt;a href=&quot;http://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density&quot;&gt;this thread&lt;/a&gt; helpful for. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-18T13:46:28.010" Id="32549" LastActivityDate="2012-07-18T13:56:08.903" LastEditDate="2012-07-18T13:56:08.903" LastEditorUserId="4856" OwnerUserId="4856" ParentId="32527" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;I think it might be helpful to your intuition to think of a mixed model as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Multilevel_model&quot; rel=&quot;nofollow&quot;&gt;hierarchical or multilevel model&lt;/a&gt;. At least to me, it makes more sense when I think of nesting and how the model is working within and across categories in a hierarchical manner.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Macro, I had left this a little open-ended because it does help me view it more intuitively, but I'm not sure it's correct. But to expand it in possibly incorrect directions...&lt;/p&gt;&#10;&#10;&lt;p&gt;I look at it as fixed effects averaging across categories and random effects distinguishing between categories. In some sense, the random effects are &quot;clusters&quot; that share some characteristics, and larger and more compact clusters will have greater influence over the average at the higher level.&lt;/p&gt;&#10;&#10;&lt;p&gt;With OLS doing the fitting (in phases, I believe), larger and more compact random effect &quot;clusters&quot; will thus pull the fit more strongly towards themselves, while smaller or more diffused &quot;clusters&quot; will pull the fit less. Or perhaps the fit begins closer to larger and more compact &quot;clusters&quot; since the higher-level average is closer to begin with&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry I can't be clearer, and may even be wrong. It makes sense to me intuitively, but as I try to write it I'm not sure if it's a top-down or bottom-up thing, or something different. Is it a matter of lower-level &quot;clusters&quot; pulling fits towards themselves more strongly, or of having greater influence over the higher-level averaging -- and thus &quot;ending up&quot; nearer to the higher-level average -- or neither?&lt;/p&gt;&#10;&#10;&lt;p&gt;In either case, I feel that it explains why smaller, more diffuse categories of random variables will be pulled farther towards the mean than larger, more compact categories.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-18T14:26:28.973" Id="32553" LastActivityDate="2012-07-19T14:17:57.543" LastEditDate="2012-07-19T14:17:57.543" LastEditorUserId="1764" OwnerUserId="1764" ParentId="32522" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;It is generally understood that likelihood ratio tests have better statistical properties than Wald tests.  (Edited:) However, as @Macro reminds me, the generalized estimating equations are not a form of maximum likelihood estimation, thus likelihood ratio tests are not available.  So you can go ahead with the Wald test that is reported.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It is true that betas are log odds, however, you can exponentiate them and then interpret the result as an &lt;em&gt;odds ratio&lt;/em&gt;.  If odds ratios aren't sufficiently intuitive (in my experience, people aren't born with the ability to think in odds ratios, but you can learn to use them), you can solve for two cases that have covariate values that seem typical, or are of interest to you, and that are identical except that in the first case CONDITION=0 and the other CONDITION=1. Exponentiating both will yield the two odds; computing $odds/(odds+1)$ in each case will yield two probabilities.  Remember that these probabilities and their difference hold only for that exact combination of covariate values.  Thus, if you want to know about what happens with a different set of covariate values, you have to go through the process again.  &lt;/p&gt;&#10;&#10;&lt;p&gt;One last point about the interpretation of a model fit by the GEE: this model will describe how the population &lt;em&gt;as a whole&lt;/em&gt; behaves, &lt;em&gt;not how an individual within that population&lt;/em&gt; will behave.  For example, consider a study that looks at students within a classroom taking (and possibly passing) a test.  When the model is fit with GEE it is telling you about the class, if it had been fit with a GLiMM instead, it would have told you about an individual student conditional on that student's attributes.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-18T18:46:55.480" Id="32572" LastActivityDate="2012-07-18T19:07:28.993" LastEditDate="2012-07-18T19:07:28.993" LastEditorUserId="7290" OwnerUserId="7290" ParentId="32566" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I like both of the answers given thus far.  Let me add a few things.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Another option is that you can also &lt;em&gt;combine&lt;/em&gt; the variables.  This is done by standardizing both (i.e., turning them into z-scores), averaging them, and then fitting your model with only the composite variable.  This would be a good approach when you believe they are two different measures of the same underlying construct.  In that case, you have two measurements that are contaminated with error.  The most likely true value for the variable you &lt;em&gt;really&lt;/em&gt; care about is in between them, thus averaging them gives a more accurate estimate.  You standardize them first to put them on the same scale, so that nominal issues don't contaminate the result (e.g., you wouldn't want to average several temperature measurements if some are Fahrenheit and some are Celsius).  Of course, if they are already on the same scale (e.g., several highly-correlated public opinion polls), you can skip that step.  If you think one of your variables might be more accurate than the other, you could do a weighted average (perhaps using the reciprocals of the measurement errors).  &lt;/p&gt;&#10;&#10;&lt;p&gt;If your variables are just different measures of the same construct, and are sufficiently highly correlated, you really could just throw one out without losing much information.   As an example, I was actually in a situation once, where I wanted to use a covariate to absorb some of the error variance and boost power, but where I didn't care about that covariate--it wasn't germane substantively.  I had several options available and they were all correlated with each other $r&amp;gt;.98$.  I basically picked one at random and moved on, and it worked fine.  I suspect I would have &lt;em&gt;lost&lt;/em&gt; power burning two extra degrees of freedom if I had included the others as well by using some other strategy.  Of course, I &lt;em&gt;could&lt;/em&gt; have combined them, but why bother?  &lt;strong&gt;However, this depends critically on the fact that your variables are correlated &lt;em&gt;because&lt;/em&gt; they are two different versions of the same thing; if there's a different reason they are correlated, this could be totally inappropriate.&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;As that implies, I suggest you think about what lies behind your correlated variables.  That is, you need a theory of &lt;em&gt;why&lt;/em&gt; they're so highly correlated to do the best job of picking which strategy to use.  In addition to different measures of the same latent variable, some other possibilities are a causal chain (i.e., $X_1\rightarrow X_2\rightarrow Y$) and more complicated situations in which your variables are the result of multiple causal forces, some of which are the same for both.  Perhaps the most extreme case is that of a suppressor variable, which @whuber describes in his comment below.  @Macro's suggestion, for instance, assumes that you are primarily interested in $X$ and wonder about the &lt;em&gt;additional&lt;/em&gt; contribution of $Z$ &lt;em&gt;after&lt;/em&gt; having accounted for $X$'s contribution.  Thus, thinking about why your variables are correlated and what you want to know will help you decide which (i.e., $x_1$ or $x_2$) should be treated as $X$ and which $Z$.  The key is to use &lt;em&gt;theoretical insight&lt;/em&gt; to inform your choice.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I agree that ridge regression is arguably better, because it allows you to use the variables you had originally intended and is likely to yield betas that are very close to their true values (although they will be biased--see &lt;a href=&quot;http://stats.stackexchange.com/questions/20295/what-problem-do-shrinkage-methods-solve/20303#20303&quot;&gt;here&lt;/a&gt; or &lt;a href=&quot;http://stats.stackexchange.com/questions/22566/when-will-a-less-true-model-predict-better-than-a-truer-model/22570#22570&quot;&gt;here&lt;/a&gt; for more information).  Nonetheless, I think is also has two potential downsides:  It is more complicated (requiring more statistical sophistication), and the resulting model is more difficult to interpret, in my opinion.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I gather that perhaps the ultimate approach would be to fit a structural equation model.  That's because it would allow you to formulate the exact set of relationships you believe to be operative, including latent variables.  However, I don't know SEM well enough to say anything about it here, other than to mention the possibility.  (I also suspect it would be overkill in the situation you describe with just two covariates.)  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-18T19:56:04.443" Id="32577" LastActivityDate="2012-07-21T04:33:02.530" LastEditDate="2012-07-21T04:33:02.530" LastEditorUserId="7290" OwnerUserId="7290" ParentId="32471" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering about this, especially because I was checking this thing called &quot;clarify&quot; which apparently does some montecarlo simulations, but I don't understand very well if it serves my purpose (assesing goodness of fit).  Greetings and thanks so much. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-18T20:09:57.457" Id="32579" LastActivityDate="2012-07-19T06:52:22.733" LastEditDate="2012-07-19T06:52:22.733" LastEditorUserId="11748" OwnerUserId="11748" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;stata&gt;&lt;goodness-of-fit&gt;&lt;logit&gt;" Title="Assesing goodness of fit of ologit model and correcting endogenity in STATA" ViewCount="172" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am doing some information extraction study on some data. We extract a &quot;word&quot; list from the data. And for each &quot;word&quot;, it has a n dimensional feature vector in R ^n. Next, We want to rank the list according to a score. The question is, how can I combine all the feature values to build a score function for my ranking. The difficulty is we can only work in an unsupervised way. I think many people are facing to this question in bioinformatic or other area when doing IE, right? I had thought about max entropy, but it usually works in supervised way. Is there any good method to solve this problem in unsupervised way ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-19T03:11:18.270" Id="32592" LastActivityDate="2012-07-19T03:11:18.270" OwnerUserId="10899" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;information-theory&gt;" Title="How to combine different feature values to build a score for ranking" ViewCount="248" />
  <row Body="&lt;p&gt;I disagree with @guy since the &quot;&amp;lt;-&quot; assignment works for matrices. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think you have to &quot;fill&quot; the matrix mu.vector starting from the first line, you cannot start from the second line. Then type :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; for(i in 2:NPERIODS1){         &#10;    mu.vector[i-1,1:2]&amp;lt;-vector[i-1,1:2]&#10;    vector[i,1:2]~dmnorm(mu.vector[i-1,1:2], omega[1:2,1:2])&#10;    }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm not sure because I haven't used WinBUGS since a couple of years. Another possibility is that it does not allow operations on the indices inside the brackets &quot;[...]&quot;. If that holds true, then define a vector j such that j[i]=i-1 and type &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   for(i in 2:NPERIODS1){         &#10;        mu.vector[j[i],1:2]&amp;lt;-vector[j[i],1:2]&#10;        vector[i,1:2]~dmnorm(mu.vector[j[i],1:2], omega[1:2,1:2])&#10;        }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By the way your code is incomplete (how is vector[1,] defined ?..). Hence this is difficult to help ! The error is possibly due to wrong dimensions when you load initial values or data.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-19T05:04:41.677" Id="32595" LastActivityDate="2012-07-19T06:52:44.510" LastEditDate="2012-07-19T06:52:44.510" LastEditorUserId="8402" OwnerUserId="8402" ParentId="11347" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;When applying a naive Bayes classifier to document classification, the classifier (at least in the applications I've seen) always iterates only over the words actually present in the document.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, when computing (say)&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(SPAM|d) \propto P(d|SPAM) * P(SPAM) = [P(w_1 | SPAM)  \cdots P(w_n | SPAM)] P(SPAM)$&lt;/p&gt;&#10;&#10;&lt;p&gt;the $w_i$ only include words actually present in the document $d$. So $P(w_1|SPAM)$ is always $P(w_1 \text{ present}|SPAM)$, and never $P(w_1 \text{ not present} | SPAM).$&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is this? I can't think of any a priori reason why including the missing features would hurt the classifier's performance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-19T11:20:37.750" Id="32614" LastActivityDate="2012-07-19T12:03:52.283" LastEditDate="2012-07-19T11:47:53.520" LastEditorUserId="264" OwnerUserId="1106" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;naive-bayes&gt;" Title="Including missing words when applying Naive Bayes in document classification" ViewCount="268" />
  <row AcceptedAnswerId="32623" AnswerCount="1" Body="&lt;p&gt;I have two random variables $s\sim \mathcal{N}(\nu,\sigma)$ and $a\sim \mathcal{U}(0,A)$, $0&amp;lt;A&amp;lt;1$, calculate a third r.v. $t=(1-a)/s$ and want to find its distribution $p(t|\nu,\sigma,A)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;My reasoning is, that for any value of $a$, there is exactly one $s=(1-a)/t$ such that the $(a,s)$ pair will produce $t$ and I will therefore have to integrate over the probabilities for these values:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(t|\nu,\sigma,A) = \int_0^A p_a(x)p_s((1-x)/t)\,\mathrm dx\\$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which can be expressed as the sum of two error functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;But already when checking this step numerically, I get a discrepancy between estimated (by sampling) and calculated distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i45.tinypic.com/2da0ew5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Why do the histogram and the calculated distribution do not match?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the code I used:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n=10000000&#10;&#10;A=0.7&#10;nu=.7&#10;sigma=.1&#10;&#10;# sampling from the target distribution&#10;s=rnorm( n, mean=nu, sd=sigma )&#10;a=runif( n, min=0, max=A )&#10;t=(1-a)/s&#10;hist(t,200, freq=F, xlim=c(0,5), ylim=c(0,2.5))&#10;&#10;&#10;# plot the analytical result&#10;analytic &amp;lt;- function(t, A, nu, sigma ){&#10;  tmp &amp;lt;- function(x, A, t, nu, sigma){&#10;     return (1/A*dnorm( (1-x)/t, mean=nu, sd=sigma) )&#10;  }&#10;  return( integrate( tmp, 0, A, A=A, t=t, nu=nu, sigma=sigma)$value)&#10;}&#10;&#10;x=seq(0,5,by=.01)&#10;y=rep(0,length(x))&#10;for( i in seq(1,length(x)) ){&#10;  y[i]=analytic(x[i], A, nu, sigma)&#10;}&#10;lines( x, y, col=&quot;red&quot;, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="11" CreationDate="2012-07-19T13:00:17.593" Id="32620" LastActivityDate="2012-07-20T08:21:11.480" LastEditDate="2012-07-19T16:44:13.223" LastEditorUserId="2970" OwnerUserId="6577" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;random-variable&gt;" Title="Dividing a uniform by a normal random variable: What's the distribution?" ViewCount="511" />
  
  
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I did hierarchical regression analysis on my data due to having moderation effects in my research model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/imZJq.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;R2 increased from .695 in model1 (main effect only) to .734 in model2 (main &amp;amp;interaction effects)(sig. F change = .000). All the assumptions for the regression analysis have been met.&#10;I have two problems with the &quot;coefficients&quot; table: &lt;img src=&quot;http://i.stack.imgur.com/teR03.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;As u can see in the table, the insignificant beta value of &lt;strong&gt;ZSC&lt;/strong&gt; in model1 became significant in model2! Is it ok? I'm confused! Which value should i consider to reject/accept the related hypothesis? B of model1 (which rejects the hypothesis) or 2 (which confirms it!!)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Although the Beta value for &lt;strong&gt;ZSC_X_CS&lt;/strong&gt; is significant, its positive sign is against the hypothesis! it's supposed to have a negative sign according to the literature &amp;amp; also logic! How should i treat this hypothesis? Accept? Reject? Partially accept?!!!&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;TQ.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dENFs.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-07-19T20:25:06.470" Id="32653" LastActivityDate="2012-07-21T20:56:00.203" LastEditDate="2012-07-20T23:47:59.870" LastEditorUserId="12481" OwnerUserId="12481" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;beta-regression&gt;" Title="How to interpret inconsistent Beta values in different steps of hierarchical regression analysis?" ViewCount="2093" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have two vectors $x,y \in \mathbb{R}.$ Based on the count and vector length I can compute $p(x)$ and $p(y)$ but I have no information on the joint density. How can I calculate mutual information in this case?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-19T22:55:39.737" FavoriteCount="2" Id="32662" LastActivityDate="2012-07-20T20:29:26.967" OwnerUserId="12733" PostTypeId="1" Score="3" Tags="&lt;mutual-information&gt;" Title="Question about calculating mutual information" ViewCount="2038" />
  <row AcceptedAnswerId="32687" AnswerCount="1" Body="&lt;p&gt;I'm working from &lt;a href=&quot;http://stats.stackexchange.com/questions/31948/looking-for-a-step-through-an-example-of-a-factor-analysis-on-dichotomous-data/32136#32136&quot;&gt;caracal's great example&lt;/a&gt; conducting a factor analysis on dichotomous data using R and I'm now struggling to understand the difference between &lt;em&gt;VSS complexity 1&lt;/em&gt; and &lt;em&gt;VSS complexity 2&lt;/em&gt; in the Very Simple Structure &lt;a href=&quot;http://personality-project.org/r/vss.html&quot; rel=&quot;nofollow&quot;&gt;(&lt;code&gt;vss&lt;/code&gt;) from the &lt;code&gt;psych&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read &lt;a href=&quot;http://personality-project.org/r/vss.html&quot; rel=&quot;nofollow&quot;&gt;the help page here&lt;/a&gt; and the &lt;a href=&quot;http://cran.r-project.org/web/packages/psych/psych.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;psych&lt;/code&gt; manual&lt;/a&gt;, but I think it is some statistical concept I do not get. The thing is that only the recommended number of factors from &lt;em&gt;complexity 2&lt;/em&gt; makes sense to me (&lt;em&gt;complexity 1&lt;/em&gt; suggest 1 factor ...).&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help me understand these two complexity levels from the &lt;code&gt;vss&lt;/code&gt; function in R's &lt;code&gt;psych&lt;/code&gt; package?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please see &lt;a href=&quot;http://stats.stackexchange.com/questions/31948/looking-for-a-step-through-an-example-of-a-factor-analysis-on-dichotomous-data/32136#32136&quot;&gt;caracal's example&lt;/a&gt; for the working example.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-20T00:17:58.540" FavoriteCount="3" Id="32669" LastActivityDate="2012-07-20T13:39:19.277" LastEditDate="2012-07-20T07:03:49.350" LastEditorUserId="3277" OwnerUserId="10275" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;factor-analysis&gt;&lt;psychometrics&gt;" Title="VSS criterion for the number of factors (in R's psych package)" ViewCount="657" />
  <row Body="&lt;p&gt;You can do a one-sided test but even if you do a two-sided test the side to which you exceed the threshold clearly tells you if you are better or worse.&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard hypothesis test only tells you that you are significantly better but not by the magnitude. To show that the magnitude is greater than a specified $\Delta$ requires a larger sample size.  Instead of testing $p_1-p_2&amp;gt;0$ you test $p_1-p_2&amp;gt;\Delta$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-20T01:53:08.067" Id="32674" LastActivityDate="2012-07-20T06:00:42.093" LastEditDate="2012-07-20T06:00:42.093" LastEditorUserId="8507" OwnerUserId="11032" ParentId="32673" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;If we know the Cholesky decomposition $V^{-1} = L^TL$, say, then &#10;$$(y - X\beta)^T V^{-1} (y - X\beta) =   (Ly - LX\beta)^T (Ly - LX\beta)$$&#10;and we can use standard algorithms (with whatever penalization function one prefers) by replacing the response with the vector $Ly$ and the predictors with the matrix $LX$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-20T11:08:27.453" Id="32690" LastActivityDate="2012-07-20T11:08:27.453" OwnerUserId="4376" ParentId="32682" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="32761" AnswerCount="3" Body="&lt;p&gt;I have a set of observed values (shown with black dots in the figure) that I would like to compare to some simulated data (100 simulated datasets shown as box plots with quartiles, extremes (excluding outliers) as whiskers, and outliers as white dots).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UARxG.png&quot; alt=&quot;Box plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The observed values are outside of the 95% confidence intervals of the simulated dataset so it seems pretty obvious that there is a significant difference between the observed and the simulated; but in some cases, the difference might not be so obvious. Which statistical test (preferably in R) can I do on this data to get a P-value for the significant difference between the observed and the simulated sets?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-20T15:28:14.073" Id="32701" LastActivityDate="2012-07-21T23:23:14.173" LastEditDate="2012-07-20T16:26:05.303" LastEditorUserId="8024" OwnerUserId="8024" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;statistical-significance&gt;&lt;categorical-data&gt;&lt;simulation&gt;" Title="Comparing observed data to simulated data?" ViewCount="987" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Can a distribution with finite mean and infinite variance have a moment generating function? What about a distribution with finite mean and finite variance but infinite higher moments? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-20T16:30:49.167" FavoriteCount="9" Id="32706" LastActivityDate="2015-01-27T00:34:11.040" LastEditDate="2012-07-22T08:25:39.830" LastEditorUserId="8507" OwnerUserId="12754" PostTypeId="1" Score="13" Tags="&lt;variance&gt;&lt;moments&gt;&lt;mgf&gt;" Title="Existence of the moment generating function and variance" ViewCount="4011" />
  <row AnswerCount="3" Body="&lt;p&gt;Suppose an ARIMA(p,d,q) model fits the data very poorly. What are some ways to improve the fit? Is there any way to do it without guessing and checking?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit.&lt;/strong&gt; I am using auto.arima() to search for an ARIMA model. But it is a poor fit.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-20T20:27:48.523" Id="32716" LastActivityDate="2012-07-24T15:21:55.383" LastEditDate="2012-07-20T20:39:25.987" LastEditorUserId="7290" OwnerUserId="12757" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;" Title="ARIMA model improvement" ViewCount="346" />
  
  <row Body="&lt;p&gt;In R you can use the &lt;code&gt;as.data.frame.table&lt;/code&gt; and &lt;code&gt;rep&lt;/code&gt; commands to get back to original data from a crosstabs type object:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mydf &amp;lt;- data.frame( one= sample( letters[1:5], 100, TRUE), &#10;+ two= sample(LETTERS[1:5], 100, TRUE) )&#10;&amp;gt; &#10;&amp;gt; tab1 &amp;lt;- table(one=mydf$one, two=mydf$two)&#10;&amp;gt; tab1&#10;   two&#10;one A B C D E&#10;  a 5 7 3 2 0&#10;  b 6 6 7 7 3&#10;  c 6 5 3 1 5&#10;  d 3 9 4 3 4&#10;  e 5 0 2 1 3&#10;&amp;gt; &#10;&amp;gt; mydf2 &amp;lt;- as.data.frame.table(tab1)&#10;&amp;gt; head(mydf2)&#10;  one two Freq&#10;1   a   A    5&#10;2   b   A    6&#10;3   c   A    6&#10;4   d   A    3&#10;5   e   A    5&#10;6   a   B    7&#10;&amp;gt; &#10;&amp;gt; mydf3 &amp;lt;- mydf2[ rep( 1:nrow(mydf2), mydf2$Freq ), -3 ]&#10;    &amp;gt; head(mydf3)&#10;        one two&#10;    1     a   A&#10;    1.1   a   A&#10;    1.2   a   A&#10;    1.3   a   A&#10;    1.4   a   A&#10;    2     b   A&#10;    &amp;gt; &#10;    &amp;gt; rownames(mydf3) &amp;lt;- NULL&#10;    &amp;gt; mydf &amp;lt;- mydf[ order( mydf$two, mydf$one ), ]&#10;&amp;gt; rownames(mydf) &amp;lt;- NULL&#10;&amp;gt; &#10;&amp;gt; all.equal(mydf, mydf3)&#10;[1] TRUE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-07-21T17:56:22.877" Id="32749" LastActivityDate="2012-07-21T17:56:22.877" OwnerUserId="4505" ParentId="32733" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;These methods are examples of application of exploratory and confirmatory data analysis. Exploratory data analysis looks for patterns while confirmatory data analysis does statistical hypothesis testing on proposed models.  It really should not be viewed in terms of which method to use it is more a matter of what stage in the data analysis you are at.  If you are unsure of what factors to include in your model you apply EFA.  Once you have eliminated some factors and settled on what to include in your model you do CFA to test the model formally to see if the chosen factors are significant.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-22T10:58:57.980" Id="32780" LastActivityDate="2012-07-22T10:58:57.980" OwnerUserId="11032" ParentId="32770" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Note that the linearity assumption you're speaking of only says that &lt;strong&gt;&lt;em&gt;the conditional mean of $Y_i$ given $X_i$ is a linear function&lt;/em&gt;&lt;/strong&gt;.  You cannot use the value of $R^2$ to test this assumption. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is because $R^2$ is merely the squared correlation between the observed and predicted values and &lt;a href=&quot;http://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_and_linearity&quot;&gt;the value of the correlation coefficient does not uniquely determine the relationship between $X$ and $Y$ (linear or otherwise) &lt;/a&gt; and &#10;both of the following two scenarios are possible:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;High $R^2$ but the linearity assumption is still be wrong in an important way&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Low $R^2$ but the linearity assumption still satisfied&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I will discuss each in turn:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(1) High $R^2$ but the linearity assumption is still be wrong in an important way:&lt;/strong&gt; The trick here is to manipulate the fact that &lt;strong&gt;correlation is very sensitive to outliers&lt;/strong&gt;. Suppose you have predictors $X_1, ..., X_n$ that are generated from a mixture distribution that is standard normal $99\%$ of the time and a point mass at $M$ the other $1\%$ and a response variable that is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_i = \begin{cases} &#10;Z_i &amp;amp; {\rm if \ } X_i \neq M \\ &#10;M &amp;amp; {\rm if \ } X_i = M \\&#10;\end{cases} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $Z_i \sim N(\mu,1)$ and $M$ is a positive constant much larger than $\mu$, e.g. $\mu=0, M=10^5$. Then $X_i$ and $Y_i$ will be almost perfectly correlated: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;u = runif(1e4)&amp;gt;.99&#10;x = rnorm(1e4)&#10;x[which(u==1)] = 1e5&#10;y = rnorm(1e4)&#10;y[which(x==1e5)] = 1e5&#10;cor(x,y)&#10;[1] 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;despite the fact that the expected value of $Y_i$ given $X_i$ is not linear - in fact it is a discontinuous step function and the expected value of $Y_i$ doesn't even depend on $X_i$ except when $X_i = M$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(2) Low $R^2$ but the linearity assumption still satisfied:&lt;/strong&gt; The trick here is to make the amount of &quot;noise&quot; around the linear trend large. Suppose you have a predictor $X_i$ and response $Y_i$ and the model &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i $$&lt;/p&gt;&#10;&#10;&lt;p&gt;was the correct model. Therefore, the conditional mean of $Y_i$ given $X_i$ is a linear function of $X_i$, so the linearity assumption is satisfied. If ${\rm var}(\varepsilon_i) = \sigma^2$ is large relative to $\beta_1$ then $R^2$ will be small. For example, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x = rnorm(200)&#10;y = 1 + 2*x + rnorm(200,sd=5)&#10;cor(x,y)^2&#10;[1] 0.1125698&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Therefore, &lt;strong&gt;assessing the linearity assumption is not a matter of seeing whether $R^2$ lies within some tolerable range&lt;/strong&gt;, but it is more a matter of examining scatter plots between the predictors/predicted values and the response and making a (perhaps subjective) decision. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Re: What to do when the linearity assumption is not met and transforming the IVs also doesn't help?!!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When non-linearity is an issue, it may be helpful to look at plots of the residuals vs. each predictor - if there is any noticeable pattern, this can indicate non-linearity in that predictor. For example, if this plot reveals a &quot;bowl-shaped&quot; relationship between the residuals and the predictor, this may indicate a missing quadratic term in that predictor. Other patterns may indicate a different functional form. In some cases, it may be that you haven't tried to right transformation or that the true model isn't linear in any transformed version of the variables (although it may be possible to find a reasonable approximation). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Regarding your example:&lt;/strong&gt; Based on the predicted vs. actual plots (1st and 3rd plots in the original post) for the two different dependent variables, it seems to me that the linearity assumption is tenable for both cases. In the first plot, it looks like there may be some heteroskedasticity, but the relationship between the two does look pretty linear. In the second plot, the relationship looks linear, but the strength of the relationship is rather weak, as indicated by the large scatter around the line (i.e. the large error variance) - this is why you're seeing a low $R^2$. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-22T17:39:26.013" Id="32785" LastActivityDate="2012-07-23T12:28:53.577" LastEditDate="2012-07-23T12:28:53.577" LastEditorUserId="4856" OwnerUserId="4856" ParentId="32781" PostTypeId="2" Score="12" />
  <row AcceptedAnswerId="32801" AnswerCount="2" Body="&lt;p&gt;Here &lt;a href=&quot;http://en.wikipedia.org/wiki/Truncation&quot;&gt;'truncating'&lt;/a&gt; implies reducing precision of the random numbers and not truncating the series of random numbers. For example, if I have $n$ truly random numbers (drawn from any distribution, e.g., normal, uniform, etc.) with arbitrary precision and I truncate all the numbers so that finally I end up with a set of $n$ numbers, each with exactly 2 digits after the decimal. Can I call this new set of numbers 'random'?&lt;/p&gt;&#10;&#10;&lt;p&gt;I came up with this question when I was reading about &lt;a href=&quot;http://en.wikipedia.org/wiki/Hardware_random_number_generator&quot;&gt;&lt;strong&gt;hardware generated random numbers&lt;/strong&gt;&lt;/a&gt;. The Wikipedia article says &lt;em&gt;they generate random numbers by measuring a physical process.&lt;/em&gt; But since this measurement has its limitations (measurement error, finite precision, etc) can we call these hardware generated numbers random?  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-22T20:28:36.397" FavoriteCount="4" Id="32794" LastActivityDate="2012-07-24T21:25:14.197" LastEditDate="2012-07-22T21:07:26.157" LastEditorUserId="7290" OwnerUserId="12786" PostTypeId="1" Score="16" Tags="&lt;random-generation&gt;&lt;measurement-error&gt;&lt;truncation&gt;" Title="Are truncated numbers from a random number generator still 'random'?" ViewCount="918" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am using generalized linear models with one response variable and 6 predictors (1 covariate and 5 factors). I want to assess the effect of smoking on my response variable. When I split my participants in sex groups and then in three smoking groups (smokers, ex-smokers, non-smokers) I find weaker associations than when I add smoking in the model (as a factor since it is a categorical variable) and split the participants in sex groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone please explain why is there this difference and what would be the best practice in this model? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;&#10;&lt;p&gt;ps: when I split in three smoking groups, the associations are similar per smoking group, they are just weaker.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-23T09:57:09.533" Id="32816" LastActivityDate="2015-02-22T20:31:40.167" LastEditDate="2012-07-23T10:23:50.030" LastEditorUserId="12795" OwnerUserId="12795" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;" Title="Generalized linear models (covariates and splitting files)" ViewCount="405" />
  <row Body="&lt;p&gt;If I am following you, you are doing one of two things:&#10;1) Controlling for what you are trying to analyze. In the non-smoking group, there can be no association between smoking and BMI (or anything else). &#10;2) Comparing the three groups (non-smokers, ex-smokers, current smokers) somehow, after separating them. I am not sure how you would then find associations: At most you could find different levels of BMI.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-23T11:25:23.213" Id="32819" LastActivityDate="2012-07-23T11:25:23.213" OwnerUserId="686" ParentId="32816" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a list of values in [0,1] that I want to fit to a Beta distribution in order to get the corresponding  alpha parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't use a beta fitting function because my values might be 0's and 1's.&lt;/p&gt;&#10;&#10;&lt;p&gt;On &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution#Parameter_estimation&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt; I found this formula for the method-of-moments estimation of alpha given the sample mean and sample variance:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dw4pi.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't need any exact fitting, just a good approximation. By using sample mean and variance, I should get rid of all the problems arising with 0s and 1s in my sample, right? Will this still be acceptable?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-07-23T12:02:41.880" Id="32820" LastActivityDate="2012-07-23T12:17:09.233" LastEditDate="2012-07-23T12:17:09.233" LastEditorUserId="8507" OwnerUserId="12287" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;estimation&gt;&lt;beta&gt;&lt;method-of-moments&gt;" Title="Estimating the parameters of a beta distribution with zeroes and ones in the sample" ViewCount="753" />
  <row AcceptedAnswerId="32823" AnswerCount="1" Body="&lt;p&gt;I am hoping someone can help me with which model (frailty, strata or cluster) I should use for my data. I have paired data so I need to take that into account when modelling the Cox PH and am unsure which model will give me a more accurate result.&lt;/p&gt;&#10;&#10;&lt;p&gt;My study was looking at the time it took for a person to become calm after being subjected to a particular stimulus. Each person was subjected to two different stimuli, on separate days. They were randomly assigned which stimulus was first. I have modeled this with survival analysis (time-to-event) but I now need to take into account that the data is paired.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help in regards to when you would use frailty, strata or cluster models would be great.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-23T12:19:22.183" FavoriteCount="3" Id="32821" LastActivityDate="2012-07-23T16:42:13.183" LastEditDate="2012-07-23T12:28:39.200" LastEditorUserId="8507" OwnerUserId="12796" PostTypeId="1" Score="7" Tags="&lt;survival&gt;&lt;cox-model&gt;&lt;frailty&gt;&lt;paired-data&gt;" Title="Which model should I use for Cox proportional hazards with paired data?" ViewCount="2675" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Unfortunately, the data is a bit difficult to deal with, since it&#10;  consists of mostly &quot;soft evidence&quot;, so the parameter estimation&#10;  doesn't seem to have an easy analytic solution such as a direct update&#10;  of Dirichlet counts.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;— why does that follow?  Why not just scale the counts based on the amount of evidence supporting each one?&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have samples of the posterior distribution, why can't you use the sufficient statistics to turn those samples into a maximum likelihood distribution?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Update after your recent edits:&lt;/p&gt;&#10;&#10;&lt;p&gt;The likelihood on $\theta$ given $o$ is true  has density&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;T_1(x) \propto 0.9 x + 0.1(1-x),&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;and false has density, say&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;T_2(x) \propto 0.2 x + 0.8(1-x).&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, the final density after $\eta_1$ observations of true and $\eta_2$ observations of false is proportional to&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;T_1(x)^{\eta_1}T_2(x)^{\eta_2}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;which is nevertheless an exponential family, although not a Beta distribution as you rightly point out.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-23T13:38:24.623" Id="32827" LastActivityDate="2012-07-23T17:18:31.493" LastEditDate="2012-07-23T17:18:31.493" LastEditorUserId="858" OwnerUserId="858" ParentId="32815" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;After your comments I think the sample size is not a big issue.  But like Peter I would be concerned about the missing data and understanding why so many samples have missing information.  If your software won't let you fit some data because if missing covariates see which covariates are missing.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If it is just a couple that cause you to lose so much data drop them and then fit the model.  Now many observations will enter the model because they will not have missing covariates. Maybe based on what you observed smoking doesn't really have much effect and any relationship you might see with a model that relate smoking to BMI where smoking is the only covariate may be because it is the smoker that drink that tend to be obese.  I have seen many a smoker that is pretty thin. &lt;/p&gt;&#10;&#10;&lt;p&gt;Compare a model that includes smoking, alcohol and an interaction term between the two and also look at a model with alcohol alone.  If the more detailed model is not doing much better fit to the same data then maybe it is okay to drop the smoking groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;But before taking any of these recommendation get to understand better why covariates are missing in so many cases.  Maybe you can looked at the dropped cases and compare the demographics (gender, age etc) for the fitted sample with what it is for the missing data.  There could be bias due to covariate imbalance.  Vance Berger's book which I have previously cited here on CV could help you with that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-23T13:56:16.630" Id="32829" LastActivityDate="2012-07-23T13:56:16.630" OwnerUserId="11032" ParentId="32816" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I sometimes have the situation where I have from several dozen to over 100 linear models to perform hypothesis tests on. They have the same predictor variables, but different response variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I have 100 models and each model has four p-values-- one for the intercept, one each for two main effects, and one for the interaction effect. If I want to calculate false discovery rates, should I calculate one set of FDRs based on the 400 p-values, or should I calculate a separate set of FDRs for each term in the model, based on the 100 p-values for that one term? I've been told by a more experienced colleague that it is the latter, but I don't understand why.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case it matters, usually one of the main effects and its interactions with the other effects is of primary interest, and the other terms are included because they might influence the response and therefore must be taken into account.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-23T16:33:00.043" FavoriteCount="2" Id="32845" LastActivityDate="2012-07-23T17:29:08.613" OwnerUserId="4829" PostTypeId="1" Score="3" Tags="&lt;multiple-comparisons&gt;" Title="If I'm calculating FDR or q-value for a collection of linear models, should each term be considered separately?" ViewCount="172" />
  <row AcceptedAnswerId="32956" AnswerCount="1" Body="&lt;p&gt;I have a bunch of data consisting of pairs (i,j), with both i and j drawn from the same set S of size k. I would like to do an independence test similar to a chi-square test.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I don't care very much about full independence. I'm really only interested in seeing if pairs of the form (i,i) show up more (or less) often than pairs of the form (i,j) with j not equal to i. So, it seems to make sense to aggregate the data, and to look at a 2 by k table with rows given by the first entry in a pair, and columns given by whether or not the second entry equals the first entry.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can one do something like a chi-square test in this way? What might it look like/might I find references?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDITED in response to whuber: Thanks for replying. I'm not sure I fully understand the question, but here goes. For me, the elements of S represent different groups (e.g. each element of S is a bank or government agency). If not a 2 by k table, what size would seem reasonable? I don't know what the labels would be for, say, a 2 by 2 table. I can guess rows for a 1 by 2 table, but then I can't imagine what the test would be - there's been too much aggregation. The 2 by k was there just because a) it seems clear that it contains all the data I'm interested in, and b) it contains substantially fewer entries than the full k by k table.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDITED in response to gui11aume: Thanks for the help! The different elements of S represent e.g. different banks. A pair (i,j) might represent a certain type of trade involving bank i and bank j. In principle, it seems reasonable to just ask if a bank is more likely to deal with another part of itself, and so aggregate all of the data into the two blocks (both entries the same) and (entries are different). In practice, I'm a little worried about this, primarily due to scale issues. Some groups are involved in many orders of magnitudes more stuff than other groups. Normal contingency table tests seem to deal with this well, giving a separate normalization to each group. I don't know how to Aggregate into two blocks without making all but the largest groups effectively invisible. Of course, I'm not exactly an expert!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-23T16:49:06.763" Id="32850" LastActivityDate="2012-07-25T23:00:25.370" LastEditDate="2012-07-24T12:52:13.430" LastEditorUserId="12806" OwnerUserId="12806" PostTypeId="1" Score="2" Tags="&lt;chi-squared&gt;&lt;maximum-likelihood&gt;&lt;independence&gt;&lt;aggregation&gt;" Title="Independence test with some aggregation" ViewCount="203" />
  
  
  
  <row Body="&lt;p&gt;I would check the options in PROC glm to see how to get that output.  The help function in SAS is very good and you can find what you want with keyword searches or by skimming through the SAS/STAT users guide. But it is easy enough to program it yourself. SAS gives you the regression coefficients and the intercept term. Just right down the regression equation with the coefficients plugged in and the values of the covariates at the point(s) where you want to make the prediction and sum the squared differences..&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-23T20:50:53.587" Id="32865" LastActivityDate="2012-07-26T13:03:10.763" LastEditDate="2012-07-26T13:03:10.763" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32864" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Another viable alternative that the modeling shop I work for routinely employs, is binning the continuous independent variables and substituting the 'bad rate'.  This forces a linear relationship.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-24T03:07:40.837" Id="32878" LastActivityDate="2012-07-24T03:07:40.837" OwnerUserId="11342" ParentId="32477" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think one of the key reasons why you usually can't do what you are suggesting is sample size vs variability. A p-value tells you whether or not the available data can tell you that you have detected a difference.  It does not directly tell you the magnitude of the difference.  You can have correctly selected the hypothesis test and have a situation where the null hypothesis should be rejected but your sample size is too small to detect the actual effect size.  This could mean that you need a very large sample because of large random error or that the chosen sample size turned out to be too small.  When you try to compare p-values from different studies the differences in sample size muddies the comparison. Of course I agree with Procrastinator's comments as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-24T10:58:37.533" Id="32898" LastActivityDate="2012-07-24T13:17:50.627" LastEditDate="2012-07-24T13:17:50.627" LastEditorUserId="11032" OwnerUserId="11032" ParentId="32890" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;em&gt;Bayesian Statistics for Social Scientists&lt;/em&gt;. Phillips, Lawrence D. (1973), Thomas Crowell &amp;amp; Co. It's very clear, very accessible, assumes no statistics knowledge, and, unlike Bolstad which I found dry, has some personality.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-07-24T11:40:10.337" CreationDate="2012-07-24T11:40:10.337" Id="32900" LastActivityDate="2012-07-24T11:40:10.337" OwnerUserId="2669" ParentId="125" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The auto.arima AIC procedure might work ok if there were no pulses, level shifts, seasonal pulses, local time trends, transient(changes) in parameters at particular points in time, transient (changes) in error variance at particular points in time. If you have any of these circumstances you may be at risk. While standard acf and pacf identification schemes are also subject to these &quot;requirements&quot; , modern procedures using robust EACF and other aggressively analytical procedures should be investigated. Your post is similar to many others reflecting on poor automatic identification leading to poor estimation results e.g. &lt;a href=&quot;http://stats.stackexchange.com/questions/32528/how-to-fit-a-model-for-a-time-series-that-contains-outliers/32537#32537&quot;&gt;How to fit a model for a time series that contains outliers&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/32852/what-type-of-time-series-model-would-be-good/32855#32855&quot;&gt;What type of time series model would be good?&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/32742/auto-arima-vs-autobox-do-they-differ/32763#32763&quot;&gt;Auto.arima vs autobox do they differ?&lt;/a&gt; for recent activity on this important subject. You might want to post your data ,your potentially deficient model identification results and your potentially deficient estimation results and see if there are any takers to help you to come up with a potentially quality result.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-24T13:23:45.373" Id="32911" LastActivityDate="2012-07-24T15:21:55.383" LastEditDate="2012-07-24T15:21:55.383" LastEditorUserId="3382" OwnerUserId="3382" ParentId="32716" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="32955" AnswerCount="1" Body="&lt;p&gt;I've proposed using an anomaly detection algorithm in a project. &lt;/p&gt;&#10;&#10;&lt;p&gt;The algorithm would consist of choosing some features we think might be indicative of anomalous examples.  Then using a training set of anomalous and non-anomalous examples to fit parameters to a Gaussian distribution.  We would then use these parameters to create a probability function p where p(x) &amp;lt; epsilon for some epsilon would indicate an anomaly.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would I answer this challenge?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;“We love this concept, but how does this differ from us just doing some statistics on a set of results?”&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;By just doing some statistics, I assume the challenger means detecting the anomalies manually in old data and coming up with a set of conditions that would indicate anomalies in future data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-24T15:55:23.387" Id="32916" LastActivityDate="2012-07-25T00:17:59.143" LastEditDate="2012-07-24T16:08:59.267" LastEditorUserId="1618" OwnerUserId="1618" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;outliers&gt;" Title="Anomaly Detection Algorithm versus &quot;just doing some statistics&quot;" ViewCount="211" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using an analysis for an experiment that I consider suboptimal. I am asking for any suggestions to improve this. I will briefly outline the experiment and what I’ve done, then ask your advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;The experiment is a two-alternative forced-choice task. Participants view two images side by side. Immediately following, a probe dot occurs on either the left or the right. Participants respond as quickly as possible, indicating which side the probe was on. You can view an image of the trial structure here. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JtcsC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The underlying assumption is that if one of the images draws the participant’s attention, they should be faster to respond to a probe dot on that side of the screen.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 4 categories of images with 9 exemplars each, in this example, hand tools, hands, sea stars, and toasters. All 12 possible permutations of images categories are represented (not including comparisons of a category with itself). Thus, hands on the left are paired with hand tools, sea stars and toasters on the right and the same pairings for hands on the right. And so on for all categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can view what the data would look like using the following R code.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reactionTime&amp;lt;-rnorm(2400, mean = 330, sd = 100)&#10;participant&amp;lt;-rep(1:20, each = 120)&#10;toaster&amp;lt;-rep(c(rep(&quot;L&quot;,3), rep(&quot;R&quot;,3), rep(NA,6)),200)&#10;handTool&amp;lt;-rep(c(&quot;R&quot;,NA,NA,&quot;L&quot;,NA,NA,&quot;R&quot;,&quot;R&quot;,&quot;L&quot;,&quot;L&quot;,NA,NA),200)&#10;hand&amp;lt;-rep(c(NA,&quot;R&quot;,NA,NA,&quot;L&quot;,NA,&quot;L&quot;,NA,&quot;R&quot;,NA,&quot;R&quot;,&quot;L&quot;),200)&#10;seastar&amp;lt;-rep(c(NA,NA,&quot;R&quot;, NA,NA, &quot;L&quot;,NA,&quot;L&quot;,NA, &quot;R&quot;,&quot;L&quot;,&quot;R&quot;),200)&#10;probe&amp;lt;-rep(c(rep(&quot;L&quot;,12), rep(&quot;R&quot;,12)),100)&#10;handedness&amp;lt;-rnorm(2400, 50, 10)&#10;data&amp;lt;-as.data.frame(cbind(participant,toaster,handTool,hand,seastar,probe,reactionTime,handedness))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have treated each image category as a factor, with the levels Left, Right or NA, with NA indicating that the image is not one if the two displayed. The probe factor indicates the side of the screen the prove dot occurs on. Participant is the participant number and a handedness measure is used as a covariate. &lt;/p&gt;&#10;&#10;&lt;p&gt;The question of interest is this: are participants faster when a hand is coincident with the probe dot, irrespective of the other image?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus far, I have analyzed this data using a 2 (image category, L or R) x 2  (Probe location, L or R) linear mixed-model using handedness as a covariate and treating participants as a random factor. I have had to do 4 separate models, using a subset of the data for each of the image categories, due to the NAs within the factors. R code example is below.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(“nlme”)&#10;lme.hand&amp;lt;-lme(RT ~ hand * probe + handedness, data=subset(data, is.na(data$hand) == FALSE), &#10;              random = ~1 | participant)&#10;anova(lme.hand, type=&quot;marginal&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I have 2 questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Can anyone think of a more elegant solution to this problem, either by reorganizing the data or selecting an alternate model? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If not, how would be an appropriate confidence interval be calculated for the fixed effect?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any assistance. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-24T16:03:25.023" Id="32918" LastActivityDate="2012-07-25T09:58:13.053" LastEditDate="2012-07-25T09:58:13.053" LastEditorUserId="930" OwnerUserId="7172" PostTypeId="1" Score="2" Tags="&lt;mixed-model&gt;&lt;methodology&gt;" Title="How to analyze factors that have missing by design values for certain observations?" ViewCount="69" />
  <row AcceptedAnswerId="32942" AnswerCount="7" Body="&lt;p&gt;I want to perform k-means clustering on some objects I have, but the objects aren't described by &quot;points&quot;.  However, I am able to compute the distance between any two objects (it is based on a similarity function).  I've implemented K-means before, but it's not clear to me how to update the clusters to be the cluster &quot;centers&quot; without a point-representation.  How would this normally be done?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-24T17:02:08.440" FavoriteCount="9" Id="32925" LastActivityDate="2013-09-19T07:48:51.040" LastEditDate="2013-09-01T16:44:12.680" LastEditorUserId="7290" OwnerUserId="12828" PostTypeId="1" Score="14" Tags="&lt;clustering&gt;&lt;k-means&gt;&lt;distance&gt;" Title="How to perform k-means clustering with only a distance function, not euclidean points?" ViewCount="2986" />
  
  
  
  
  
  <row Body="&lt;p&gt;I wouldn't do a test of normality (on the residuals).  Make a quantile quantile plot of the residuals (&lt;code&gt;qqnorm()&lt;/code&gt; in R).  Now, put that aside and make some more but this time of just random normal data with the same N as your residuals.  How does your plot look compared to the simulations?  If it's in the range of what you might expect to see it's normal.  If not then you may have some concerns.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-25T01:19:47.463" Id="32964" LastActivityDate="2012-07-25T01:19:47.463" OwnerUserId="601" ParentId="32957" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is one of those areas where people use the jargon to mean quite different things and I fear there is no correct answer.  Two quite different meanings are:&#10;a) The variables are correlated.&#10;b) The variables are highly correlated to the point where it is impossible to estimate a good statistical model that includes these variables as independent variables (i.e., due to multicollinearity).&#10;To work out which, if either, of these meanings is being employed you will need to ask whoever it is who used the term.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-25T01:47:56.270" Id="32968" LastActivityDate="2012-07-25T01:47:56.270" OwnerUserId="12833" ParentId="30903" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Check out the University of California Los Angeles &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/topics/logistic_regression.htm&quot; rel=&quot;nofollow&quot;&gt;site&lt;/a&gt; for detailed help on logistic regression using SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further the &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/output/logistic.htm&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt; contains the annotated SPSS output with interpretations.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-25T04:31:29.310" Id="32973" LastActivityDate="2012-07-25T07:35:24.900" LastEditDate="2012-07-25T07:35:24.900" LastEditorUserId="3826" OwnerUserId="11588" ParentId="32971" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;At the moment I have $5$ paired samples for correlation. Spearman's R is $0.2$ and the $p = 0.78$. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I calculate the number to extra samples I would need to get a more significant p-value?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-25T07:07:13.190" Id="32982" LastActivityDate="2012-07-25T14:23:45.977" LastEditDate="2012-07-25T14:23:45.977" LastEditorUserId="8507" OwnerUserId="12837" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;statistical-significance&gt;&lt;sample-size&gt;&lt;spearman-rho&gt;" Title="Increase sample size for significant correlation" ViewCount="2081" />
  
  
  
  <row Body="&lt;p&gt;Scipy lognormal fit returns shape, location, and scale. I just ran the following on an array of sample price data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;shape, loc, scale = st.lognorm.fit(d_in[&quot;price&quot;])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me reasonable estimates 1.0, 0.09, 0.86, and when you plot it, you should take into account all three parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The shape parameter is the standard deviation of the underlying normal distribution, and the scale is the exponential of the mean of the normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-25T21:21:51.570" Id="33039" LastActivityDate="2012-07-25T21:21:51.570" OwnerUserId="11186" ParentId="33036" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/users/11032/michael-chernick&quot;&gt;Michael&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/users/10346/fraijo&quot;&gt;Fraijo&lt;/a&gt; suggested that simply checking whether the parameter value of interested was contained in some credible region was the Bayesian equivalent of inverting confidence intervals. I was a bit skeptical about this at first, since it wasn't obvious to me that this procedure really resulted in a Bayesian test (in the usual sense).&lt;/p&gt;&#10;&#10;&lt;p&gt;As it turns out, it does - at least if you're willing to accept a certain type of loss functions. Many thanks to &lt;a href=&quot;http://stats.stackexchange.com/users/9394/zen&quot;&gt;Zen&lt;/a&gt;, who provided references to two papers that establish a connection between HPD regions and hypothesis testing:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Pereira &amp;amp; Stern (1999), &lt;a href=&quot;http://www.mdpi.org/entropy/papers/e1040099.pdf&quot; rel=&quot;nofollow&quot;&gt;Evidence and credibility: full Bayesian significance test for precise hypotheses&lt;/a&gt;, &lt;em&gt;Entropy&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;Madruga, Esteves &amp;amp; Wechsler (2001), &lt;a href=&quot;http://w.ime.usp.br/~jstern/miscellanea/citacoes/swtest1.pdf&quot; rel=&quot;nofollow&quot;&gt;On the Bayesianity of Pereira-Stern tests&lt;/a&gt;, &lt;em&gt;Test&lt;/em&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'll try to summarize them here, for future reference. In analogue with the example in the original question, I'll treat the special case where the hypotheses are $$H_0: \theta\in\Theta_0=\{\theta_0\}\qquad \mbox{and}\qquad H_1: \theta\in\Theta_1=\Theta\backslash \Theta_0,$$ where $\Theta$ is the parameter space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Pereira &amp;amp; Stern proposed a method for testing said hypotheses &lt;strong&gt;without having to put prior probabilities on $\Theta_0$ and $\Theta_1$&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\pi(\cdot)$ denote the density function of $\theta$ and define $$T(x)=\{ \theta:\pi(\theta|x)&amp;gt;\pi(\theta_0|x)\}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This means that &lt;strong&gt;$T(x)$ is a HPD region&lt;/strong&gt;, with credibility $P(\theta\in T(x)|x)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Pereira-Stern test rejects $\Theta_0$ when $P(\theta\notin T(x)|x)$ is &quot;small&quot; ($&amp;lt;0.05$, say). For a unimodal posterior, this means that $\theta_0$ is far out in the tails of the posterior, making this criterion somewhat similar to using p-values. In other words, &lt;strong&gt;$\Theta_0$ is rejected at the $5~\%$ level if and only if it is not contained the in $95~\%$ HPD region.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let the test function $\varphi$ be $1$ if $\Theta_0$ is accepted and $0$ if $\Theta_0$ is rejected. Madruga et al. proposed the loss function&#10;$$&#10;L(\theta,\varphi,x) = \begin{cases} a(1-\mathbb{I}(\theta\in T(x)), &amp;amp; \mbox{if } \varphi(x)=0 \\&#10;b+c\mathbb{I}(\theta\in(T(x)), &amp;amp; \mbox{if } \varphi(x)=1, \end{cases}&#10;$$&#10;with $a,b,c&amp;gt;0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Minimization of the expected loss leads to the Pereira-Stern test where $\Theta_0$ is rejected if $P(\theta\notin T(x)|x)&amp;lt;(b+c)/(a+c).$&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, all is well. The Pereira-Stern test is equivalent to checking whether $\theta_0$ is in an HPD region and there is a loss function that generates this test, meaning that it is founded in decision theory.&lt;/p&gt;&#10;&#10;&lt;p&gt;The controversial part though is that &lt;strong&gt;the loss function depends on $x$&lt;/strong&gt;. While such loss functions have appeared in the literature a few times, they don't seem to be generally accepted as being very reasonable.&lt;/p&gt;&#10;&#10;&lt;p&gt;For further reading on this topic, see a &lt;a href=&quot;http://scholar.google.se/scholar?cites=5678603314102954427&quot; rel=&quot;nofollow&quot;&gt;list of papers that cite the Madruga et al. article&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update October 2012:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I wasn't completely satisfied with the above loss function, as its dependence on $x$ makes the decision-making more subjective than I would like. I spent some more time thinking about this problem and ended up writing a short note about it, &lt;a href=&quot;http://arxiv.org/abs/1210.1066&quot; rel=&quot;nofollow&quot;&gt;posted on arXiv earlier today&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $q_{\alpha}(\theta|x)$ denote the posterior quantile function of $\theta$, such that $P(\theta\leq q_{\alpha}(\theta|x))=\alpha$. Instead of HPD sets we consider the central (equal-tailed) interval $(q_{\alpha/2}(\theta|x),q_{1-\alpha/2}(\theta|x))$. &lt;strong&gt;To test $\Theta_0$ using this interval can be justified in the decision-theoretic framework without a loss function that depends on $x$.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The trick is to reformulate the problem of testing the point-null hypothesis $\Theta_0=\{\theta_0\}$ as a three-decision problem with directional conclusions. $\Theta_0$ is then tested against both $\Theta_{-1}=\{\theta:\theta&amp;lt;\theta_0\}$ and $\Theta_{1}=\{\theta:\theta&amp;gt;\theta_0\}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let the test function $\varphi=i$ if we accept $\Theta_i$ (note that this notation is the opposite of that used above!). It turns out that under the weighted $0-1$ loss function&#10;$$L_2(\theta,\varphi) = \begin{cases} 0, &amp;amp; \mbox{if } \theta\in\Theta_i\mbox{ and }\varphi=i, \quad i\in \{-1,0,1\}, \\&#10;\alpha/2, &amp;amp; \mbox{if } \theta\notin\Theta_0 \mbox{ and }\varphi=0,\\&#10;1, &amp;amp; \mbox{if } \theta\in\Theta_{i}\cup\Theta_0 \mbox{ and }\varphi=-i,\quad i\in\{-1,1\},\end{cases}$$&#10;the Bayes test is to reject $\Theta_0$ if $\theta_0$ is not in the central interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;This seems like a quite reasonable loss function to me. I discuss this loss, the Madruga-Esteves-Wechsler loss and testing using credible sets further in the manuscript on arXiv.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2012-07-26T12:21:14.153" CreationDate="2012-07-26T12:21:14.153" Id="33077" LastActivityDate="2012-10-04T22:46:29.097" LastEditDate="2012-10-04T22:46:29.097" LastEditorUserId="8507" OwnerUserId="8507" ParentId="31679" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;To solve your problem, a good approach is to define a probabilistic model that matches the assumptions about your dataset. In your case, you probably want a mixture of linear regression models. You can create a &quot;mixture of regressors&quot; model similar to a gaussian mixture model by associating different data points with different mixture components.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have included some code to get you started. The code implements an EM algorithm for a mixture of two regressors (it should be relatively easy to extend to larger mixtures). The code seems to be fairly robust for random datasets. However, unlike linear regression, mixture models have non-convex objectives, so for a real dataset, you may need to run a few trials with different random starting points.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;import matplotlib.pyplot as plt &#10;import scipy.linalg as lin&#10;&#10;#generate some random data&#10;N=100&#10;x=np.random.rand(N,2)&#10;x[:,1]=1&#10;&#10;w=np.random.rand(2,2)&#10;y=np.zeros(N)&#10;&#10;n=int(np.random.rand()*N)&#10;y[:n]=np.dot(x[:n,:],w[0,:])+np.random.normal(size=n)*.01&#10;y[n:]=np.dot(x[n:,:],w[1,:])+np.random.normal(size=N-n)*.01&#10;&#10;&#10;rx=np.ones( (100,2) )&#10;r=np.arange(0,1,.01)&#10;rx[:,0]=r&#10;&#10;#plot the random dataset&#10;plt.plot(x[:,0],y,'.b')&#10;plt.plot(r,np.dot(rx,w[0,:]),':k',linewidth=2)&#10;plt.plot(r,np.dot(rx,w[1,:]),':k',linewidth=2)&#10;&#10;# regularization parameter for the regression weights&#10;lam=.01&#10;&#10;def em():&#10;    # mixture weights&#10;    rpi=np.zeros( (2) )+.5&#10;&#10;    # expected mixture weights for each data point&#10;    pi=np.zeros( (len(x),2) )+.5&#10;&#10;    #the regression weights&#10;    w1=np.random.rand(2)&#10;    w2=np.random.rand(2)&#10;&#10;    #precision term for the probability of the data under the regression function &#10;    eta=100&#10;&#10;    for _ in xrange(100):&#10;        if 0:&#10;            plt.plot(r,np.dot(rx,w1),'-r',alpha=.5)&#10;            plt.plot(r,np.dot(rx,w2),'-g',alpha=.5)&#10;&#10;        #compute lhood for each data point&#10;        err1=y-np.dot(x,w1)&#10;        err2=y-np.dot(x,w2)&#10;        prbs=np.zeros( (len(y),2) )&#10;        prbs[:,0]=-.5*eta*err1**2&#10;        prbs[:,1]=-.5*eta*err2**2&#10;&#10;        #compute expected mixture weights&#10;        pi=np.tile(rpi,(len(x),1))*np.exp(prbs)&#10;        pi/=np.tile(np.sum(pi,1),(2,1)).T&#10;&#10;        #max with respect to the mixture probabilities&#10;        rpi=np.sum(pi,0)&#10;        rpi/=np.sum(rpi)&#10;&#10;        #max with respect to the regression weights&#10;        pi1x=np.tile(pi[:,0],(2,1)).T*x&#10;        xp1=np.dot(pi1x.T,x)+np.eye(2)*lam/eta&#10;        yp1=np.dot(pi1x.T,y)&#10;        w1=lin.solve(xp1,yp1)&#10;&#10;        pi2x=np.tile(pi[:,1],(2,1)).T*x&#10;        xp2=np.dot(pi2x.T,x)+np.eye(2)*lam/eta&#10;        yp2=np.dot(pi[:,1]*y,x)&#10;        w2=lin.solve(xp2,yp2)&#10;&#10;        #max wrt the precision term&#10;        eta=np.sum(pi)/np.sum(-prbs/eta*pi)&#10;&#10;        #objective function - unstable as the pi's become concentrated on a single component&#10;        obj=np.sum(prbs*pi)-np.sum(pi[pi&amp;gt;1e-50]*np.log(pi[pi&amp;gt;1e-50]))+np.sum(pi*np.log(np.tile(rpi,(len(x),1))))+np.log(eta)*np.sum(pi)&#10;        print obj,eta,rpi,w1,w2&#10;&#10;        try:&#10;            if np.isnan(obj): break&#10;            if np.abs(obj-oldobj)&amp;lt;1e-2: break&#10;        except:&#10;            pass&#10;&#10;        oldobj=obj&#10;&#10;    return w1,w2&#10;&#10;&#10;#run the em algorithm and plot the solution&#10;rw1,rw2=em()&#10;plt.plot(r,np.dot(rx,rw1),'-r')&#10;plt.plot(r,np.dot(rx,rw2),'-g')&#10;&#10;plt.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-07-25T19:46:59.843" Id="33080" LastActivityDate="2012-07-26T14:06:46.250" LastEditDate="2012-07-26T14:06:46.250" LastEditorUserId="9595" OwnerDisplayName="user1149913" OwnerUserId="9595" ParentId="33078" PostTypeId="2" Score="20" />
  <row Body="&lt;p&gt;I would go for co-inertia analysis, which is an unspoken variant of &lt;a href=&quot;http://en.wikipedia.org/wiki/Canonical_analysis&quot; rel=&quot;nofollow&quot;&gt;canonical analysis&lt;/a&gt;. This would give you a linear combination of the 21 variables that has the highest co-inertia with a linear combination of childcare data (or with child care if it is a single quantitative variable). The trick of working with co-inertia instead of correlation is that you can still perform the computations when there are more variables than observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, CIA is not very wide-spread. It was developed for ecology, where there is usually more variables than observation sites. You can find some technical information in &lt;a href=&quot;http://pbil.univ-lyon1.fr/R/articles/arti113.pdf&quot; rel=&quot;nofollow&quot;&gt;Dray, Chessel and Thioulouse, Ecology 84(11), 3078-89, 2003&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, the other comments/answers are right that 12 is a relatively small number and you will have to live with that...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-26T12:55:18.180" Id="33081" LastActivityDate="2012-07-26T15:17:18.863" LastEditDate="2012-07-26T15:17:18.863" LastEditorUserId="10849" OwnerUserId="10849" ParentId="33067" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;What you've described are heteroscedastic errors and regarding your question about bias: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Heteroscedasticity does not bias least squares estimators of regression coefficients&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you have a response variable $Y_i$ and and $p$-length vector of predictors ${\bf X}_{i}$ such that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Y_i = {\bf X}_i {\boldsymbol \beta} + \varepsilon_i $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where ${\boldsymbol \beta} = \{ \beta_0, ..., \beta_p \}$ is the vector of regression coefficients and the errors, $\varepsilon_i$ are such that $E(\varepsilon_i)=0$ with no restrictions on the variance except that it is finite for each $i$. Then &lt;a href=&quot;http://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation&quot;&gt;the least squares estimator of ${\boldsymbol \beta}$&lt;/a&gt; is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat {\boldsymbol \beta} = ( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} {\bf Y}  $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $$ {\bf X} = \left( \begin{array}{c}&#10;{\bf X}_1 \\ &#10;{\bf X}_2 \\ &#10;\vdots \\ &#10;{\bf X}_n \\ &#10;\end{array} \right) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;is a matrix where the rows are the predictor vectors for each individual, including $1$s for the intercept and ${\bf Y}$, ${\boldsymbol \varepsilon}$ are similarly defined as the vector of response values and errors, respectively. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding the expected value of $\hat {\boldsymbol \beta}$, it helps to replace ${\bf Y}$ with $({\bf X} {\boldsymbol \beta} + {\boldsymbol \varepsilon})$ to get that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat {\boldsymbol \beta}  = ( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} ({\bf X} {\boldsymbol \beta} + {\boldsymbol \varepsilon}) = \underbrace{( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} {\bf X} {\boldsymbol \beta}}_{= {\boldsymbol \beta}} + ( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} {\boldsymbol \varepsilon} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, $E(\hat {\boldsymbol \beta}) = {\boldsymbol \beta} + E \left( ( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} {\boldsymbol \varepsilon} \right ) $, so we just need the right hand term to  be  0. We can derive this by conditioning on ${\bf X}$ and averaging over ${\bf X}$ using the law of total expectation: &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align*}&#10;E \left( ( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} {\boldsymbol \varepsilon} \right ) &amp;amp;= E_{ {\bf X} }  \left( E \left( ( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} {\boldsymbol \varepsilon} \right | {\bf X}) \right) \\ &#10;&amp;amp; = E_{ {\bf X} }  \left( {\bf X}^{{\rm T}} {\bf X} )^{-1} {\bf X}^{{\rm T}} E  (  {\boldsymbol \varepsilon} | {\bf X} ) \right) \\&#10;&amp;amp;= 0 &#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;where the final line follows from the fact that $E(  {\boldsymbol \varepsilon} | {\bf X} )=0$, the so-called &lt;a href=&quot;http://en.wikipedia.org/wiki/Ordinary_least_squares#Classical_linear_regression_model&quot;&gt;strict exogeneity assumption&lt;/a&gt; of linear regression. Nothing here has relied on homoscedastic errors. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; While heteroscedasticity does not bias the parameter estimates, useful results including the Gauss-Markov Theorem and the covariance matrix of the $\hat {\boldsymbol \beta}$ being given by $\sigma^2 ({\bf X}^{\rm T} {\bf X})^{-1}$ &lt;strong&gt;do&lt;/strong&gt; require homoscedasticity. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-26T13:29:24.533" Id="33087" LastActivityDate="2012-07-26T13:29:24.533" OwnerUserId="4856" ParentId="33038" PostTypeId="2" Score="7" />
  <row AnswerCount="3" Body="&lt;p&gt;I am looking at the probability of opting into a program based on a continuous variables $X_1$, $X_2$, $X_3$, etc.  When I divide the sample into people who opted in and did not opt in and do a t-test I find that the mean of all the $X$'s for those opting in is significantly different from the mean for those not opting in.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I look at a correlation matrix for the $X$'s and opting in I find really low correlation coefficients -- from between $.04$ and $.13$.  But my t-tests are significant at the 95%+ confidence level.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I square these two results?  &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-26T14:39:40.657" Id="33091" LastActivityDate="2012-07-27T02:04:48.180" LastEditDate="2012-07-26T14:42:59.773" LastEditorUserId="8507" OwnerUserId="8616" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;t-test&gt;" Title="Correlation coefficient contradicts t-test" ViewCount="1113" />
  
  <row Body="&lt;p&gt;user1149913 has an excellent answer (+1), but it looks to me that your data collection fell apart in late 2011, so you'd have to cut that part of your data off, and then still run things a few times with different random starting coefficients to see what you get.&lt;/p&gt;&#10;&#10;&lt;p&gt;One straightforward way to do things would be to separate your data into two sets by eye, then use whatever linear model technique you're used to. In R, it would be the &lt;code&gt;lm&lt;/code&gt; function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or fit two lines by eye. In R you would use &lt;code&gt;abline&lt;/code&gt; to do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data's jumbled, has outliers, and falls apart at the end, yet by-eye has two fairly obvious lines, so I'm not sure a fancy method is worth it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-26T15:08:54.927" Id="33094" LastActivityDate="2012-07-26T15:08:54.927" OwnerUserId="1764" ParentId="33078" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am currently running a multiple regression model using imputed data and have a few questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Using SPSS 18. My data appears to be MAR. Listwise deletion of cases leaves me with only 92 cases, multiple imputation leaves 153 cases for analysis. All assumptions met - one variable log transformed. 9 IV's 5 - 5 categorical, 3 scale, 1 interval. DV-scale. Using the enter method of standard multiple regression. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;My DV is the difference of scores between a pre- score and a post score measure, both of these variables are missing a number of cases - should I impute missing values for each of these and then work out the differnce between them to calculate my DV (how do I go about doing this), or can I just impute data for my DV? Which is the most appropriate approach?&lt;/li&gt;&#10;&lt;li&gt;Should I run imputations on transformed data or skewed untransformed data?&lt;/li&gt;&#10;&lt;li&gt;Should I enter all variables into the imputation process, even if they are not missing data, or should I just impute data for the variables missing more than 10% of cases?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have run the regression on the listwise deleted cases and my IV's account for very little of the variance in my DV, subsequently I have run the regression on a complete file following multiple imputation - The results are very similar, in that my 9 IV's still predict only approx 12% of the variance in my DV, however, now one of my IV'S indicates that it is making a significant contribution (this happens to be a log transformed variable)... &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Should I report original data if there is little difference between my conclusions - i.e my IV's poorly predict the dv, or report the complete data?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2012-07-26T15:33:24.210" FavoriteCount="1" Id="33098" LastActivityDate="2013-07-27T03:49:13.130" LastEditDate="2012-07-27T20:39:38.233" LastEditorUserId="1352" OwnerUserId="12873" PostTypeId="1" Score="7" Tags="&lt;spss&gt;&lt;multiple-regression&gt;&lt;data-transformation&gt;&lt;multiple-imputation&gt;" Title="Multiple imputation questions for multiple regression in SPSS" ViewCount="957" />
  <row Body="&lt;p&gt;Found in &lt;a href=&quot;http://norvig.com/experiment-design.html&quot;&gt;Warning Signs in Experimental Design and Interpretation &lt;/a&gt; by Peter Norvig&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Most of the time, when you get an amazing, counterintuitive result, it means you have screwed up the experiment&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;(Michael Wigler)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;in the sense of &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Extraordinary claims require extraordinary evidence&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;(Carl Sagan)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;which is based on a similar quote by Pierre Laplace&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-07-26T15:51:08.057" CreationDate="2012-07-26T15:51:08.057" Id="33099" LastActivityDate="2012-07-26T15:51:08.057" OwnerUserId="264" ParentId="726" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Spearman's rho is nonparametric.  There are no distributional assumptions.  Interpreting the Pearson correlation as strength of linearity and tests for statistical significance rely on bivariate normality for Pearson. Homoskedasticity never enters in for any correlation.  Remember it only has to do with the bivariate pair (X,Y) that you are calculating correlations for.  Regression doesn't enter in unless in the case of Pearson you want to the slope of the regression of X on Y.  When you say you are plotting with time is time a variable included in the correlation?  If you are just looking at pairs that are from a time dependent sequence, there is no need to make any assumptions about time.  If you are correlating X with time T then Spearman's rho is used to see if there is a relationship between X and T and then clearly you don't want to assume T is independent of X.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-26T17:00:58.657" Id="33106" LastActivityDate="2012-07-26T17:00:58.657" OwnerUserId="11032" ParentId="33096" PostTypeId="2" Score="4" />
  
  
  
  
  <row Body="&lt;p&gt;You should read about multi-task learning. A good place to start is this page: &lt;a href=&quot;http://www.public.asu.edu/~jye02/Software/MALSAR/index.html&quot; rel=&quot;nofollow&quot;&gt;http://www.public.asu.edu/~jye02/Software/MALSAR/index.html&lt;/a&gt; (both the manual and tutorial).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many, many formulations and regularizers that people have devised for the class of problems you are interested in.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-27T02:10:36.620" Id="33147" LastActivityDate="2012-07-27T02:10:36.620" OwnerUserId="9595" ParentId="23292" PostTypeId="2" Score="2" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;fit2a &amp;lt;- glmer(mrsa_result ~ age + ltcf_type + (1|ltcf), family=binomial(&quot;logit&quot;),data=data.mrsa)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The first line is correct. Each unit on L2 is represented in multiple data rows, and lme4 recognizes that ltcf_type is constant for each entry within a L2 unit. So it is treated automatically as a fixed effect on L2.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-27T07:37:41.473" Id="33159" LastActivityDate="2012-07-27T07:37:41.473" OwnerUserId="6082" ParentId="33151" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;If I do regression in two stages:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Stage 1: $y\sim x_1 + 1$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Stage 2: resid_1st_stage $\sim x_2 + 1$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Will the &lt;code&gt;resid_2nd_stage&lt;/code&gt; be orthogonal to $x_1$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-27T15:17:41.093" Id="33190" LastActivityDate="2012-12-05T17:09:50.150" LastEditDate="2012-07-28T09:03:54.557" LastEditorUserId="3826" OwnerUserId="8783" PostTypeId="1" Score="-1" Tags="&lt;regression&gt;&lt;residuals&gt;" Title="Two-stage linear regression" ViewCount="256" />
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Some books on Likelihood Estimation&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.lnms/1215467056&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Amari, Barndorff-Nielsen, Kass, Lauritzen and Rao, &lt;em&gt;Differential geometry in statistical inference&lt;/em&gt;.&#10;$-\small{\text{Geometrical approach for proving existence, uniqueness and other properties of MLE.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Saddlepoint-Approximations-Applications-Statistical-Probabilistic/dp/0521872502&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Butler, &lt;em&gt;Saddlepoint Approximations with Applications&lt;/em&gt;.&lt;br&gt;&#10;$-\small{\text{Saddlepoint approximations to the MLE on complicated models.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521866731&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Cox, &lt;em&gt;Principles of Statistical Inference&lt;/em&gt;.&lt;br&gt;&#10;$-\small{\text{A basic reference on MLE.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Inference-Asymptotics-Monographs-Statistics-Probability/dp/041249440X&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Cox and Barndorff-Nielsen, &lt;em&gt;Inference and Asymptotics&lt;/em&gt;.&#10;$-\small{\text{Likelihood, pseudo-likelihood, approximation theorems and asymptotics explained by}}$&#10;$ \small{\text{two exponents in this area.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0801844436&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Edwards, &lt;em&gt;Likelihood&lt;/em&gt;.&lt;br&gt;&#10;$-\small{\text{A reference for a general discussion on this concept.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Course-Sample-Chapman-Statistical-Science/dp/0412043718&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Ferguson, &lt;em&gt;A Course in Large Sample Theory&lt;/em&gt;.&#10;$-\small{\text{Contains classical results on asymptotic properties of point estimators.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387904581&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Kalbfleisch, &lt;em&gt;Probability and Statistical Inference II&lt;/em&gt;. $\spadesuit$&lt;br&gt;&#10;$-\small{\text{Introductory book containing interesting basic results such as the continuous }}$&#10;$\small{\text{approximation to the likelihood which is not always explained.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Theory-Point-Estimation-Springer-Statistics/dp/0387985026&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Lehmann and Casella, &lt;em&gt;Theory of Point Estimation&lt;/em&gt;.&lt;br&gt;&#10;$-\small{\text{Classical results on point estimation, an essential reference.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/9812386947&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Pace and Salvan, &lt;em&gt;Principles of Statistical Inference: From a Neo-Fisherian Perspective&lt;/em&gt;.&#10;$-\small{\text{A good reference on a school of thought becoming more and more popular:}}$&#10;$\small{\text{the Neo-Fisherian.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0198507658&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Pawittan, &lt;em&gt;In All Likelihood: Statistical Modelling and Inference Using Likelihood&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471024031&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Serfling, &lt;em&gt;Approximation Theorems of Mathematical Statistics&lt;/em&gt;.&#10;$-\small{\text{More rigorous book, here you can find the mystical &quot;regularity conditions&quot;.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Likelihood-Methods-Statistics-Statistical-Science/dp/0198506503/ref=pd_sim_b_1&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Severini, &lt;em&gt;Likelihood Methods in Statistics&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387953825&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Shao, &lt;em&gt;Mathematical Statistics&lt;/em&gt;.&lt;br&gt;&#10;$-\small{\text{Classical results, good as a textbook.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B000PY3NU0&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Sprott, &lt;em&gt;Statistical Inference in Science&lt;/em&gt;. $\spadesuit$&#10;$-\small{\text{Basic reference on likelihood, profile likelihood and classical statistical modelling.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Asymptotic-Statistics-Statistical-Probabilistic-Mathematics/dp/0521784506&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; van der Vaart, &lt;em&gt;Asymptotic Statistics&lt;/em&gt;.&lt;br&gt;&#10;$-\small{\text{A general reference on: modes of convergence, properties of MLE, delta method,}}$&#10;$\small{\text{ moment estimators, efficiency and tests.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521839718&quot; rel=&quot;nofollow&quot;&gt;*&lt;/a&gt; Young and Smith, &lt;em&gt;Essentials of Statistical Inference&lt;/em&gt;.&#10;$-\small{\text{A more recent book on: Likelihood, pseudolikelihood, saddlepoint approximations,}}$&#10;$\small{p^*\text{ formula, modified profile likelihoods and more.}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$\spadesuit$ Suggestion for the OP&lt;/p&gt;&#10;" CommentCount="14" CommunityOwnedDate="2012-07-27T17:51:25.627" CreationDate="2012-07-27T16:44:35.960" Id="33203" LastActivityDate="2013-06-11T07:57:56.763" LastEditDate="2013-06-11T07:57:56.763" LastEditorUserId="22047" OwnerDisplayName="user10525" ParentId="33197" PostTypeId="2" Score="12" />
  
  
  
  <row Body="&lt;p&gt;The Nonlinear Models books that I like and rely on are (1) &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470139005&quot; rel=&quot;nofollow&quot;&gt;Bates and Watts&lt;/a&gt; and (2) &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471802603&quot; rel=&quot;nofollow&quot;&gt;Gallant&lt;/a&gt;.  Both are published by Wiley.  &lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2012-07-28T01:19:18.513" CreationDate="2012-07-28T01:19:18.513" Id="33229" LastActivityDate="2012-07-28T01:31:34.127" LastEditDate="2012-07-28T01:31:34.127" LastEditorUserId="686" OwnerUserId="11032" ParentId="33197" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Maximum likelihood: &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0198507658&quot; rel=&quot;nofollow&quot;&gt;In all Likelihood&lt;/a&gt; (Pawitan). Moderately clear book and the most clear (IMO) with respect to books dealing with likelihood only. Also has R code.&lt;/p&gt;&#10;&#10;&lt;p&gt;GLMs: Categorical Data Analysis (Agresti, 2002) is one of the best written stat books I have read (also has R code available). This text will also help with maximum likelihood. The third edition is coming out in a few months.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second on my list for the above two is Collett's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1584883243&quot; rel=&quot;nofollow&quot;&gt;Modelling Binary Data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;PCA: I find Rencher's writing clear in &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471418897&quot; rel=&quot;nofollow&quot;&gt;Methods of multivariate analysis&lt;/a&gt;. This is a graduate level text, but it is introductory.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2012-07-28T01:40:34.823" CreationDate="2012-07-28T01:40:34.823" Id="33230" LastActivityDate="2013-06-11T07:54:53.673" LastEditDate="2013-06-11T07:54:53.673" LastEditorUserId="22047" OwnerUserId="12548" ParentId="33197" PostTypeId="2" Score="12" />
  
  <row Body="&lt;p&gt;&lt;em&gt;Setting&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X$ be the categorical predictor and suppose it has 3 levels ($X = 1$, $X = 2$, and $X = 3$). Let the third level be the reference category.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define $X_1$ and $X_2$ as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;X_1 = \left\{ &#10;\begin{array}{ll} &#10;1 &amp;amp; \textrm{if } X = 1 \\ &#10;0 &amp;amp; \textrm{otherwise;}&#10;\end{array} \right.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;X_2 = \left\{ &#10;\begin{array}{ll} &#10;1 &amp;amp; \textrm{if } X = 2 \\ &#10;0 &amp;amp; \textrm{otherwise.}&#10;\end{array} \right.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you know both $X_1$ and $X_2$ then you know $X$. In particular, if $X_1 = 0$ and $X_2 = 0$ then $X = 3$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Logistic regression model&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The model is written&#10;$$&#10;\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}&#10;$$&#10;where $\pi_i$ denotes the probability of success of individual $i$ with covariate information $(x_{1i}, x_{2i})$.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If individual $i$ falls in category $1$ then $x_{1i} = 1$, $x_{2i} =&#10;   0$ and $\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 +&#10;   \beta_1$.&lt;/li&gt;&#10;&lt;li&gt;If individual $i$ falls in category $2$ then $x_{1i} = 0$, $x_{2i} =&#10;   1$ and $\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 +&#10;   \beta_2$.&lt;/li&gt;&#10;&lt;li&gt;If individual $i$ falls in category $3$ then $x_{1i} = 0$, $x_{2i} =&#10;   0$ and $\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;odds ratio&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Odds ratios are computed with respect to the reference category. For example, for 'category 1 vs category 3' we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\exp(\beta_0 + \beta_1)}{\exp(\beta_0)} = \exp(\beta_1).&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-28T07:39:58.410" Id="33242" LastActivityDate="2014-06-02T12:46:19.230" LastEditDate="2014-06-02T12:46:19.230" LastEditorUserId="17230" OwnerUserId="3019" ParentId="33240" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Interpretation is always part speculation, but I think the implied meaning is that often you can get the result you want without estimating the standard deviation explicitly. In other words, I think the author is referring to situations where you would use &lt;strong&gt;no&lt;/strong&gt; estimate of the standard deviation, rather than a biased estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, if you can construct an estimate of the whole distribution of a statistic, you can compute confidence intervals without using the standard deviation. In fact, for many (non-normal) distributions the standard deviation itself (and the mean) is not sufficient to compute an estimate of the confidence interval. In other cases, such as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Sign_test&quot; rel=&quot;nofollow&quot;&gt;sign test&lt;/a&gt;, you do not need an estimate for the standard deviation either.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Of course, it is non-trivial to construct an &lt;strong&gt;unbiased&lt;/strong&gt; estimate of a full distribution, and in Bayesian statistics it is actually quite common to introduce bias explicitly through the prior.)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-28T11:07:27.210" Id="33247" LastActivityDate="2012-07-28T11:07:27.210" OwnerUserId="11915" ParentId="33235" PostTypeId="2" Score="4" />
  <row Body="Survival analysis is concerned with modelling the time before subjects change state, typically time until death or failure. One key feature of such data is that they can be censored, that is, some subjects will not have changed state before the study ends. " CommentCount="0" CreationDate="2012-07-28T12:46:50.377" Id="33253" LastActivityDate="2012-10-01T11:36:28.813" LastEditDate="2012-10-01T11:36:28.813" LastEditorUserId="686" OwnerUserId="10849" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;I'm not an expert on the subject, but you can estimate the standard deviation either by calculating it from entire population (if it is feasible) or calculate it from a sample as large as possible. Size &lt;code&gt;sqrt(population size)&lt;/code&gt; should be usually enough, but the larger, the better. Just use the following formula:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rdSUK.png&quot; alt=&quot;sample standard deviation&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;(taken from &lt;a href=&quot;http://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;) to estimate unbiased standard deviation.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-28T05:00:46.637" Id="33255" LastActivityDate="2012-07-28T05:00:46.637" OwnerDisplayName="Timo" OwnerUserId="11274" ParentId="33234" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You will need a little more information than &lt;code&gt;summary(reg)&lt;/code&gt; provides, namely, the covariance matrix of the estimates.  &lt;code&gt;vcov(reg)&lt;/code&gt; will give that to you:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x1 &amp;lt;- rnorm(100)&#10;x2 &amp;lt;- 0.7*rnorm(100) + 0.7*x1&#10;y &amp;lt;- x1 + x2 + rnorm(100)&#10;&#10;reg &amp;lt;- lm(y~x1+x2)&#10;vcov(reg)&#10;             (Intercept)           x1           x2&#10;(Intercept)  0.009780556 -0.002229766  0.001652152&#10;x1          -0.002229766  0.016996594 -0.012423096&#10;x2           0.001652152 -0.012423096  0.018662900&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The covariance between the coefficient of &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; is in the cell with row label &lt;code&gt;x1&lt;/code&gt; and column label &lt;code&gt;x2&lt;/code&gt; (or vice versa), and the variance terms are on the diagonal.&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference between &lt;code&gt;vcov(reg)&lt;/code&gt; and &lt;code&gt;summary(reg)$cov&lt;/code&gt; is that the latter is not scaled by $\hat{\sigma}^2$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-28T16:52:25.380" Id="33263" LastActivityDate="2012-07-28T16:52:25.380" OwnerUserId="7555" ParentId="33260" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="33289" AnswerCount="3" Body="&lt;p&gt;I have a lot of data and I want to do something which seems very simple. In this large set of data, I am interested in how much a specific element clumps together. Let's say my data is an ordered set like this: {A,C,B,D,A,Z,T,C...}. Let's say I want to know whether the A's tend to be found right next to each other, as opposed to being randomly (or more evenly) distributed throughout the set. This is the property I am calling &quot;clumpiness&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, is there some simple measurement of data &quot;clumpiness&quot;? That is, some statistic that will tell me how far from randomly distributed the As are? And if there isn't a simple way to do this, what would the hard way be, roughly? Any pointers greatly appreciated! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-29T00:38:41.463" FavoriteCount="6" Id="33283" LastActivityDate="2013-11-14T22:20:27.517" LastEditDate="2013-11-14T21:44:01.663" LastEditorUserId="8451" OwnerUserId="52" PostTypeId="1" Score="10" Tags="&lt;summary-statistics&gt;" Title="Standard measure of clumpiness?" ViewCount="731" />
  
  <row AcceptedAnswerId="33294" AnswerCount="1" Body="&lt;p&gt;I am trying to write my own ML library. For speed reasons I started out writing things in C using BLAS, but then I learned that NumPy and Theano also use BLAS. I am wondering if there are huge speed differences between implementations of ML algorithms in C/Python/Matlab/Octave. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody have some experience or can provide some data for the comparison? If there is no really good reason to write in pure C, I would rather not.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-29T04:50:36.060" Id="33290" LastActivityDate="2012-07-29T09:31:34.337" LastEditDate="2012-07-29T09:31:34.337" LastEditorUserId="10849" OwnerUserId="12895" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;maximum-likelihood&gt;&lt;algorithms&gt;&lt;software&gt;" Title="What are speed differences beetwen ML implementations in different languages?" ViewCount="187" />
  <row Body="&lt;p&gt;There was a fairly good commentary in the Journal of Wildlife Management concerning uninformative parameters within the AIC framework.&lt;/p&gt;&#10;&#10;&lt;p&gt;Arnold, T. W.  2010.  Uninformative parameters and model selection using Akaike’s Information Criterion.  Journal of Wildlife Management 74:1175–1178.  &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1937-2817.2010.tb01236.x/abstract&quot; rel=&quot;nofollow&quot;&gt;[Link]&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;We usually consider models within 2 delta AIC as competitive.  However, if a model has an addition of only one parameter to its competitor and that parameter is not significant, that parameter is likely spurious.  AIC = –2LL + 2K so the penalty for adding one parameter is +2 AIC.  If only one parameter is added but the AIC is within 2 delta AIC, the model fit was not improved enough to overcome the penalty.  Therefore, that parameter is uninformative and should not be included in the model or interpreted as having an effect.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-29T05:46:30.223" Id="33291" LastActivityDate="2012-11-09T15:04:54.253" LastEditDate="2012-11-09T15:04:54.253" LastEditorUserId="12318" OwnerUserId="12318" ParentId="33179" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Yes, in practice those values are skipped. In your description in terms of a Frobenius norm, this corresponds to minimising the components of the norm which can be measured, i.e. those which have known ratings. The regularisation term can be seen as a Bayesian prior on the components of the feature vectors, with the SVD calculating the maximum likelihood estimator, subject to this prior and the known values.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's probably best to think of the SVD as a method for inferring the missing values. If you've already got a better way of doing this, why do you need the SVD? If you don't, then the SVD will happily fill in the gaps for you.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-07-30T00:09:34.107" Id="33314" LastActivityDate="2012-07-30T00:09:34.107" OwnerUserId="9975" ParentId="33103" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="33318" AnswerCount="2" Body="&lt;p&gt;I was wondering which software statistical package do you guys recommend for performing Bayesian Inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I know that you can run openBUGS or winBUGS as standalones or you can also call them from R. But R also has several of its own packages (MCMCPack, BACCO) which can do bayesian analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody have any suggestions as to which bayesian statistics package in R is best or about other alternatives (Matlab or Mathematica?)&lt;/p&gt;&#10;&#10;&lt;p&gt;The main features I am looking to compare are performance, ease of use, stability and flexibility&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-30T00:24:10.843" FavoriteCount="4" Id="33315" LastActivityDate="2012-09-27T08:49:23.483" LastEditDate="2012-07-30T02:19:31.840" LastEditorUserId="12347" OwnerUserId="12347" PostTypeId="1" Score="12" Tags="&lt;r&gt;&lt;probability&gt;&lt;bayesian&gt;&lt;inference&gt;&lt;bugs&gt;" Title="Optimal software package for bayesian analysis" ViewCount="1035" />
  <row Body="&lt;p&gt;If the difference between the logarithms is constant then the variables differ by a constant ratio. That is probably an easier thing to explain than anything to do with log-constant or log-difference.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-30T00:32:33.847" Id="33316" LastActivityDate="2012-07-30T00:32:33.847" OwnerUserId="1679" ParentId="33313" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;External BUGS variants are the standard. Working within R may be convenient, but I'd be surprised if those packages are as mature and perform as well. Using a library which bridges R and the external program is usually the most common compromise.&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the jags/rjags combo (jags might be roughly considered a dialect of bugs). I haven't tried the other bugs variants, but the reports I've heard are that jags's performance and ability to deal with numerical issues is a bit better than the other bugs variants. I find jags easy to use, but of course, you need some knowledge of bayesian data analysis to know how to use it.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-30T03:32:14.573" Id="33318" LastActivityDate="2012-07-30T13:51:33.123" LastEditDate="2012-07-30T13:51:33.123" LastEditorUserId="4733" OwnerUserId="4733" ParentId="33315" PostTypeId="2" Score="10" />
  
  <row AnswerCount="0" Body="&lt;p&gt;According to my modest understanding, the cox model is all about estimating hazard ratios, relative measures of risk.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it is possible (in R using e.g. the packages pec or peperr) to estimate absolute risks by a cox model.&lt;/p&gt;&#10;&#10;&lt;p&gt;How is this done mathematically?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-30T12:46:29.867" FavoriteCount="1" Id="33335" LastActivityDate="2012-07-30T12:46:29.867" OwnerUserId="10064" PostTypeId="1" Score="4" Tags="&lt;cox-model&gt;&lt;relative-risk&gt;&lt;absolute-risk&gt;" Title="How are absolute risks estimated by the cox model?" ViewCount="218" />
  <row Body="&lt;p&gt;In general when comparing two different fitting curves the way to go is a performance measure.&#10;It does not allow you to draw strong conclusions, but if you have two options you can calculate the R squared for example and at least have an idea of how good your fit can be.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-30T14:47:46.547" Id="33340" LastActivityDate="2012-07-30T14:47:46.547" OwnerUserId="8205" ParentId="31215" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;Suppose you have&#10;$$x_t = x_{t-1} + \delta_t, y_t = y_{t-1} + \epsilon_t$$&#10;with obvious assumptions ($\epsilon_t$ are i.i.d., $\delta_t$ are i.i.d. and independent of $\epsilon_t$, $x_0$ and $y_0$ are independent of each other and $\epsilon$ and $\delta$). Then no matter how hard you try, for $a^2+b^2 \neq 0$, $ax_t + by_t$ will be an $I(1)$ process: it is itself non-stationary, while its first difference is $a\delta_t + b\epsilon_t$ is white noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if anything, cointegration should be viewed as a much rarer phenomenon than lack of cointegration. Examples of cointegration would require special constructions. Define $z_t = z_{t-1} + \gamma \delta_t + \nu_t$ for some $\gamma\neq0$, where $\nu_t \sim ARMA(p,q)$. Then $z_t$ and $x_t$ are cointegrated, as $z_t - \gamma x_t = z_0 - \gamma x_0 + \nu_t$ is now stationary.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-30T16:58:03.853" Id="33351" LastActivityDate="2012-07-30T16:58:03.853" OwnerUserId="5739" ParentId="27414" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a graduate-level background in pure mathematics (Measure Theory, Functional Analysis, Operator Algebra, etc.) I also have a job that requires some knowledge of probability theory (from basic principles to machine learning techniques).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question: Can someone provide some canonical reading and reference materials that:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Self-contained introduction to Probability theory&lt;/li&gt;&#10;&lt;li&gt;Don't shy away from measure theoretic methodologies and proofs&lt;/li&gt;&#10;&lt;li&gt;Provide a heavy emphasis on applied techniques.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Basically, I want a book that will teach me applied probability theory geared towards pure mathematicians. Something starting with the basic axioms of probability theory and introducing applied concepts with mathematical rigor. &lt;/p&gt;&#10;&#10;&lt;p&gt;As per the comments, I'll elaborate on what I need. I am doing basic-to-advanced data mining. Logistic Regression, Decision Trees, basic Stats and Probability (variance, standard deviation, likelihood, probability, likelihood, etc.), Supervised and Unsupervised machine learning (mainly clustering (K-Means, Hierarchal, SVM)).&lt;/p&gt;&#10;&#10;&lt;p&gt;With the above in mind, I want a book that will start at the beginning. Defining probability measures, but then also showing how those result in basic summation probabilities (which I know, intuitively, happen by integration over discrete sets). From there, it could go into: Markov Chains, Bayesian.... all the while discussing the foundational reasoning behind the theory, introducing the concepts with rigorous mathematics, but then showing how these methods are applied in the real world (specifically to data mining). &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Does such a book or reference exist?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;&#10;&lt;p&gt;PS - I realize this is similar in scope to &lt;a href=&quot;http://stats.stackexchange.com/questions/414/introduction-to-statistics-for-mathematicians&quot;&gt;this question&lt;/a&gt;. However, I'm looking for Probability theory and not statistics (as similar as the two fields are).&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-07-30T19:23:17.440" Id="33359" LastActivityDate="2012-08-01T18:46:44.583" LastEditDate="2012-07-31T13:37:19.573" LastEditorUserId="12078" OwnerUserId="12078" PostTypeId="1" Score="8" Tags="&lt;probability&gt;&lt;books&gt;" Title="Introduction to applied probability for pure mathematicians?" ViewCount="783" />
  
  
  
  <row AcceptedAnswerId="44179" AnswerCount="3" Body="&lt;p&gt;I have the &lt;code&gt;plm&lt;/code&gt; package and would like to run unit root tests on some variables. I get the following error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; purtest(data$tot.emp)&#10;Error in data.frame(baldwin = c(59870, 61259, 60397, 58919, 57856, 57227,  : &#10;  arguments imply differing number of rows: 14, 19, 11, 12, 1, 20, 18, 10, 13&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I assume that I'm getting this error because my panel is unbalanced. Two questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Can you use panel unit root tests (Levin, Lin and Chu (2002), Im, Pesaran and Shin (2003), or others) for unbalanced panels?&lt;/li&gt;&#10;&lt;li&gt;If so, is it implemented in R?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2012-07-30T22:23:10.430" FavoriteCount="2" Id="33364" LastActivityDate="2012-11-28T14:12:35.457" LastEditDate="2012-11-21T11:54:09.167" LastEditorUserId="9249" OwnerUserId="401" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;panel-data&gt;&lt;unit-root&gt;" Title="Unit root tests for panel data in R" ViewCount="1422" />
  <row Body="&lt;p&gt;In general when you are using a nonparametric bootstrap normality assumptions are not made so you don't have to worry about the distribution of residuals. If they are iid with finite variance that is sufficient. No transformations of the data is required.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-31T00:59:39.787" Id="33375" LastActivityDate="2012-07-31T00:59:39.787" OwnerUserId="11032" ParentId="33370" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Reading this one now:&#10;&lt;strong&gt;Predictive Analytics: Microsoft Excel&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;By Conrad Carlberg&lt;/p&gt;&#10;&#10;&lt;p&gt;Published Jul 2, 2012 by Que. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;ISBN-10: 0-7897-4941-6 &#10;ISBN-13: 978-0-7897-4941-3&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm not done reading it yet, but so far its a good introduction to the topic for a non-stat person.  It starts pretty basic with both stat concepts and Excel functionality and is building from there.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the Stats front, its going into a pretty healthy discussion of of using moving averages and smoothing to help determine signal/noise in time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the Excel front, its explaining how to build models using the above concepts (rather than just plunking a typical Excel trendline on a chart), and using some of Excels add-on functionality (e.g. Solver and Data Analysis).&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-07-31T01:06:30.090" CreationDate="2012-07-31T01:06:30.090" Id="33376" LastActivityDate="2012-07-31T11:24:45.553" LastEditDate="2012-07-31T11:24:45.553" LastEditorUserId="10217" OwnerUserId="10217" ParentId="2032" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;I have data sets of the returns of two indexes in the same market (two different sets of stocks constituting each index), with 496 observations for each. I want to compare if the means are statistically different. I believe the variances are different, so I think I have to check if the variances are statistically different first. How would I do these things?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-07-31T01:37:57.607" Id="33377" LastActivityDate="2012-07-31T11:27:12.057" LastEditDate="2012-07-31T11:27:12.057" LastEditorUserId="8507" OwnerUserId="12975" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;anova&gt;&lt;t-test&gt;&lt;heteroscedasticity&gt;" Title="Comparing the means of two time series" ViewCount="1979" />
  <row Body="&lt;p&gt;I see at least three reasonable options, although there is one I tend to do.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Compute the difference score, $D = Pre - Post$ and then predict that. The regression equation being something like: $$\hat{D}_{i} = b_0 + b_1X_{i} + b_2Z_{i} + b_3Genderism_{i} + b_4Z_{i}*Genderism_{i}$$ One thing that is attractive about this is that it is straightforward to do.&lt;/li&gt;&#10;&lt;li&gt;Predict the post scores using your model, but also controlling for pre scores.  This regression equation would look something like: $$\hat{Post}_i = b_0 + b_1X_{i} + b_2Z_{i} + b_3Genderism_{i} + b_4Z_{i}*Genderism_{i} + b_5Pre_{i}$$ This tends to be my preferred approach. It does not require you to compute any new variables (which is not a big issue but can be slightly annoying). It also includes in the results an estimate of the strength of association between pre and post scores. It works because your treatment, $X$, predicts that portion of the post scores that are not explained by pre scores. The only drawback I see is that if you have a very small sample size, you lose an additional degree of freedom controlling for pre scores. That seems reasonable to me; however, because there is measurement error at both time points anyway, so it is not like computing the difference scores has zero error.&lt;/li&gt;&#10;&lt;li&gt;The last approach I see is to reshape the data from wide to long, and fit a mixed effects model. This would look something like: $$\hat{Outcome}_{ij} = b_0 + u_{0i} + b_1X_{i} + b_2Z_{i} + b_3Genderism_{i} + b_4Z_{i}*Genderism_{i} + b_5time_{ij} + b_6time_{ij}*X_{i}$$ The outcome is the score at the &lt;em&gt;jth&lt;/em&gt; time point for the &lt;em&gt;ith&lt;/em&gt; individual. The model includes a random intercept (captured by $u_{0i}$) for each individual. The time captures the change over time, and the interaction between time and the group variable is the &quot;treatment&quot; effect. Although kind of cool, I think this model is far too much work in the simple case where you only have pre and post scores (if you had 3+ time points, it would make sense).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;By the way, I do not particularly think that you need to center your variables prior to using them as moderators. I know many people teach that, but the models should work out the same. It can be slightly convenient as it makes the simple effects potentially more interpretable (0 = mean, therefore they are the effect of the variable at the mean of the other), but aside from that I see little gain. In more complex models, the reduction in collinearity between the variables and their product can also be helpful, but I have only ever see that matter in complicated random effects models or in some parallel latent growth models I fit once.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-07-31T05:29:02.243" Id="33378" LastActivityDate="2012-07-31T05:29:02.243" OwnerUserId="12521" ParentId="33367" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="33423" AnswerCount="1" Body="&lt;p&gt;Say you run a regression (Y = Beta_0 + Beta_1 + GDP + ... ), and obtain a coefficient estimate for &quot;GDP&quot; of -6 with a standard error = 4.  Assume sample size is 250.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do you answer the question:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What is the probability that the true (population) coefficient for GDP is greater&#10;  than +1?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Normally, we set up a null hypothesis that the true coefficient is equal to zero and reject/fail to reject -- the &quot;significance test&quot; -- however, this approach doesn't directly answer the question I posed above.  All p-values from a significance test are only relevant when you first assume that the null hypothesis is true, which I do not want to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any/all code welcome.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-07-31T15:47:43.620" Id="33419" LastActivityDate="2012-07-31T17:02:06.453" OwnerUserId="9420" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;probability&gt;&lt;hypothesis-testing&gt;&lt;regression-coefficients&gt;" Title="What is the probability that the true effect is greater than z?" ViewCount="109" />
  
  
  <row Body="&lt;p&gt;Trivially, if you have $N$ data points, they will be linearly separable in $N-1$ dimensions. Any structure in the data may reduce the required dimensionality for linear separation further. You might say that (a projection of) a data set either &lt;strong&gt;is&lt;/strong&gt; or &lt;strong&gt;is not&lt;/strong&gt; completely linearly separable, in which using any (projection into) dimensionality lower than $N-1$ requires either additional properties of the data, of the projection into this higher dimensionality, or can be viewed as a heuristic (for instance in the case of random projections). In general we usually do not care to much about precise separability, in which case it is sufficient that we can meaningfully separate &lt;em&gt;more&lt;/em&gt; data points correctly in higher dimensions.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-07-31T22:18:56.773" Id="33441" LastActivityDate="2012-07-31T23:37:47.283" LastEditDate="2012-07-31T23:37:47.283" LastEditorUserId="11915" OwnerUserId="11915" ParentId="33437" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I have seen two arguments advanced that Bayesian analysis is a generalization of a frequentist analysis. Both were somewhat tongue-in-cheek, and more getting people to recognize the assumptions about regression models by using priors as a context.&lt;/p&gt;&#10;&#10;&lt;p&gt;Argument 1: Frequentist analysis is Bayesian analysis with a purely uninformative prior centered on zero (yes, it doesn't matter where its centered, but ignore that). This provides both the context for which a Bayesian might extract the results of a frequentist analysis, explains why you can get away with using some &quot;Bayesian&quot; techniques like MCMC to extract frequentist estimates in situations where say, maximum likelihood convergence is tough, and gets people to recognize that when they say &quot;The data speak for themselves&quot; and the like, what they're actually saying is that beforehand, all values are equally likely.&lt;/p&gt;&#10;&#10;&lt;p&gt;Argument 2: Any regression term you &lt;em&gt;don't&lt;/em&gt; include in a model has, in effect, been assigned   a prior centered on zero with &lt;em&gt;no&lt;/em&gt; variance. This one isn't so much a &quot;Bayesian analysis is a generalization&quot; as much as &quot;There are priors &lt;em&gt;everywhere&lt;/em&gt;, even in your frequentist models&quot; argument.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-07-31T22:36:26.697" Id="33443" LastActivityDate="2012-07-31T22:36:26.697" OwnerUserId="5836" ParentId="33435" PostTypeId="2" Score="11" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to create a model for debt collections.  In the past I have used logistic regression to predict pay/no-pay.  This has worked well but has a few unfortunate consequences.  People are more likely to pay when the outstanding balance is low.  Thus, the model tends to push low balances to the good tail.&lt;/p&gt;&#10;&#10;&lt;p&gt;The real objective, then, is to maximize dollars collected at a particular depth of file.  The default strategy is to order accounts by the debt amount which is considered a &quot;dummy&quot; solution.  Improving on this dummy solution has been more difficult than I anticipated.  The problem is that the dollars collected is contingent on the debt amount!&lt;/p&gt;&#10;&#10;&lt;p&gt;When I try to model dollars collected, the debt amount completely overpowers everything else in the model.  Is there another way to &quot;subtract&quot; the influence of debt amount or otherwise control for it?  The desired result is to have the model rank order dollars collected better than simply using the debt amount.&lt;/p&gt;&#10;&#10;&lt;p&gt;These are things I have tried:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;2-stage model using Heckman correction predicting pay/no-pay and then dollars collected&lt;/li&gt;&#10;&lt;li&gt;Built separate models predicting pay/no-pay on high, medium and low debt amounts using logistic regression.  Then centered each model using their segment odds.&lt;/li&gt;&#10;&lt;li&gt;Linear regression on just the payers predicting dollars paid and applied the model to the entire population.&lt;/li&gt;&#10;&lt;li&gt;Straight logistic regression predicting pay/no-pay&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-07-31T22:43:20.513" Id="33444" LastActivityDate="2012-07-31T22:43:20.513" OwnerUserId="11342" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;continuous-data&gt;" Title="Dependent variable maximum value contingent on independent variable" ViewCount="135" />
  <row AnswerCount="0" Body="&lt;p&gt;We inherited a turnover prediction process from a previous analyst. The tool we are using is SPSS Modeler. The model being used is CHAID decision tree. The target variable is binary: &quot;stayer&quot; vs &quot;departer&quot; (e.g. person stays with company, person leaves the company). The process is detailed below under BACKGROUND. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I am looking for this specific feedback from the CV community:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What do you see inheritly wrong with the below process?&lt;/li&gt;&#10;&lt;li&gt;What are some better ways of arriving at a predication using a decision tree classification model? ( I am only looking at decision tree models because I am new to data mining and predictive modeling and these models are easier to understand)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I should mention that accuracy has so not been a problem thus far. But what prompted this question is the fact that everyone is classified as &quot;departer&quot; (100% false positives). I posted a separate question about that &lt;a href=&quot;http://stats.stackexchange.com/questions/33450/how-is-it-possible-to-turn-out-with-a-highly-accurate-prediction-when-all-record&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I can post a link to a SPSS Modeler .str file (and related text files, all anonimized) if anyone wants to see it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;BACKGROUND&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Task: Predict number of terminations for three of our directorates (e.g ~ “business units”) for upcoming fiscal year using two years historical terminations and headcount data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Datafile: Employee data with several demographic fields. Every record is an employee that either stayed during the entire two year period or terminated (i.e. &quot;stayer&quot; vs &quot;departer&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Our current process involves runing a CHAID Model interactively using SPSS Modeler for a recent known time period. For example 2009-2010 (file setup as decribed above). &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Sidenote: This is the &quot;training&quot; data. The data is not partioned into&#10;  &quot;training&quot; and &quot;testing&quot; data in the traditional classification sense.&#10;  This is mainly because the count of &quot;departers&quot; is relatively small&#10;  compared (1:14) to &quot;stayers&quot;.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;We do this as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) CHAID interactive model is run iteratively (trial and error), each time measuring the performance by this subprocess:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;a. Apply the model to beginning of year population (e.g. 2011 in this&#10;   case) to come up with propensity scores for each individual.  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;b. Sum the propensity scores by organization and divide by two. We divide by&#10;   two since terminations being used to train the model cover a two year&#10;   period but we are only interested in projecting a year out. This&#10;   produces predictions by directorate. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;c. Once the accuracy is above 85% move on to next step.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;2) Build new model using model from #1 as a guide . New model will be similar to one in #1--business knowledge and some guesswork is applied to adjust splits...but this part is minimal. &lt;/p&gt;&#10;&#10;&lt;p&gt;3) Apply final model to beginning of year population (e.g. 2012..assuming that's the upcoming year) to come up with final predications by directorate.&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW&#10;I tried looking for similar posts on model deployment process but only found posts such as the following which didn't hit the mark for me &lt;a href=&quot;http://stats.stackexchange.com/questions/8276/help-with-deploying-a-model&quot;&gt;help-with-deploying-a-model&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-01T00:56:39.663" Id="33448" LastActivityDate="2012-08-01T01:14:01.360" LastEditDate="2012-08-01T01:14:01.360" LastEditorUserId="11944" OwnerUserId="11944" PostTypeId="1" Score="1" Tags="&lt;modeling&gt;&lt;predictive-models&gt;" Title="Deployment process for Classification models (i.e. decision trees)" ViewCount="145" />
  
  <row Body="&lt;p&gt;The question looks simple, but your reflection around it shows that it is not that simple.&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually, p-values are a relatively late addition to the theory of statistics. Computing a p-value without a computer is very tedious; this is why the only way to perform a statistical test until recently was to use tables of statistical tests, as I explain in &lt;a href=&quot;http://blog.thegrandlocus.com/2012/04/Why-p-values-are-crap&quot;&gt;this blog post&lt;/a&gt;. Because those tables were computed for fixed $\alpha$ levels (typically 0.05, 0.01 and 0.001) you could only perform a test with those levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;Computers made those tables useless, but the logic of testing is still the same. You should:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Formulate a null hypothesis.&lt;/li&gt;&#10;&lt;li&gt;Formulate an alternative hypothesis.&lt;/li&gt;&#10;&lt;li&gt;Decide a maximum type I error (the probability of falsely rejecting the null hypothesis) error you are ready to accept.&lt;/li&gt;&#10;&lt;li&gt;Design a rejection region. The probability that the test statistic falls in the rejection region given that the null hypothesis is your level $\alpha$. As @MånsT explains, this should be no smaller than your acceptable type I error, and in many cases use asymptotic approximations.&lt;/li&gt;&#10;&lt;li&gt;Carry out the random experiment, compute the test statistic and see whether it falls in the rejection region.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In theory, there is a strict equivalence between the events &lt;em&gt;&quot;the statistic falls in the rejection region&quot;&lt;/em&gt; and &lt;em&gt;&quot;the p-value is less than $\alpha$&quot;&lt;/em&gt;, which is why it is felt that you can report the p-value &lt;strong&gt;instead&lt;/strong&gt;. In practice, it allows you to skip step 3. and evaluate the type I error &lt;strong&gt;after the test is done&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;To come back to your post, the statement of the null hypothesis is incorrect. The null hypothesis is that the probability of flipping a head is $1/2$ (the null hypothesis cannot pertain to the results of the random experiment).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you repeat the experiment again and again with a threshold p-value of 0.05, yes, you should have &lt;em&gt;approximately&lt;/em&gt; 5% rejection. And if you set a p-value cut-off of 0.06, you should end up with roughly 6% rejection. More generally, for continuous tests, by definition of the p-value $p$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ Prob(p &amp;lt; x) = x, \, (0 &amp;lt; x &amp;lt; 1), $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is only approximately true for discrete tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some R code that I hope can clarify this a bit. The binomial test is relatively slow, so I do only 10,000 random experiments in which I flip 1000 coins. I perform a binomial test and collect the 10,000 p-values.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;# Generate 10,000 random experiments of each 1000 coin flipping&#10;rexperiments &amp;lt;- rbinom(n=10000, size=1000, prob=0.5)&#10;all_p_values &amp;lt;- rep(NA, 10000)&#10;for (i in 1:10000) {&#10;    all_p_values[i] &amp;lt;- binom.test(rexperiments[i], 1000)$p.value&#10;}&#10;# Plot the cumulative density of p-values.&#10;plot(ecdf(all_p_values))&#10;# How many are less than 0.05?&#10;mean(all_p_values &amp;lt; 0.05)&#10;# [1] 0.0425&#10;# How many are less than 0.06?&#10;mean(all_p_values &amp;lt; 0.06)&#10;# 0.0491&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see that the proportions are not exact, because the sample size is not infinite and the test is discrete, but there is still an increase of roughly 1% between the two.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-08-01T09:38:29.853" Id="33462" LastActivityDate="2012-08-04T07:44:01.043" LastEditDate="2012-08-04T07:44:01.043" LastEditorUserId="10849" OwnerUserId="10849" ParentId="33453" PostTypeId="2" Score="13" />
  <row AnswerCount="0" Body="&lt;p&gt;I conducted listening experiment in which 16 participants had to rate the audio stimuli along 5 scales representing an emotion (sad, tender, neutral, happy and aggressive). Each audio stimulus was synthesized in order to represent a particular emotion. Participants had to move 5 sliders each of which corresponded to one of the 5 emotions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The sliders range was [0,10] but participants were only informed about the extremities of the sliders (not at all - very much).&#10;There was not a force choice, therefore potentially each audio stimulus could be rated with all the scales (e.g. sad = 0.1, tender = 2.5, neutral = 2., happy = 8.3, aggressive = 1.7).&lt;/p&gt;&#10;&#10;&lt;p&gt;There were 40 stimuli, each stimulus was repeated twice, for a total of 80 trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;To analyze the data I want to use a MANOVA with repeated measures (in R).&#10;For this purpose I use the audio stimulus as independent variable having 40 levels, while the 5 responses as dependent variables. Since each individual has been measured twice, I include a within-subjects factor for trial number. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to demonstrate that the created stimuli were actually correctly classified in the corresponding emotion. For example I expect that happy sounds result in happy ratings by participants and that these happy ratings are greater than the other 4 responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;I studied the R documentation and examples, but I encountered the following error while adapting those examples to my case:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model.emotions&amp;lt;-lm(cbind(Sad,Tender,Neutral,Happy,Aggressive) ~ Trial,data=scrd)&#10;idata&amp;lt;-data.frame(scrd$Trial_number)&#10;aov.emotions&amp;lt;-anova(model.emotions,idata=idata, idesign=~Trial, type=&quot;III&quot;)&#10;&#10;&amp;gt; aov.emotions&amp;lt;-anova(model.emotions,idata=idata, idesign=~Trial, type=&quot;III&quot;)&#10;Error in cbind(M, X) : number of rows of matrices must match (see arg 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am not fully sure of the above formula, since I am not an expert in R. Can you please correct them showing the correct R code?&lt;/p&gt;&#10;&#10;&lt;p&gt;The .csv table with all the data can be downloaded here: &lt;a href=&quot;https://dl.dropbox.com/u/3288659/Results_data_listening_Test.csv&quot; rel=&quot;nofollow&quot;&gt;https://dl.dropbox.com/u/3288659/Results_data_listening_Test.csv&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition I was not able to find any post hoc test to apply to the result of the MANOVA in case of a significant main effect. Any suggestion?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-08-01T12:16:26.827" Id="33467" LastActivityDate="2012-08-01T19:33:11.793" LastEditDate="2012-08-01T19:33:11.793" LastEditorUserId="4701" OwnerUserId="4701" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;manova&gt;" Title="MANOVA with repeated measures in R, error" ViewCount="1519" />
  
  <row Body="&lt;p&gt;Though I am sure that @cardinal will also put together an excellent program, let me mention a couple of books that might cover some of the things the OP is asking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;I recently came across &lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4419-9633-6&quot; rel=&quot;nofollow&quot;&gt;Probability for Statistics and Machine Learning&lt;/a&gt; by Anirban DasGupta, which appears to me to cover many of the probabilistic topics asked for. It is fairly mathematical in its style, though it does not seem to be &quot;hard core&quot; measure theoretic. The best &quot;hard core&quot; books are, in my opinion, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521007542&quot; rel=&quot;nofollow&quot;&gt;Real Analysis and Probability&lt;/a&gt; by Dudley and &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387953132&quot; rel=&quot;nofollow&quot;&gt;Foundations of Modern Probability&lt;/a&gt; by Kallenberg.&#10;These two very mathematical books should be accessible given the OPs background in functional analysis and operator algebra $-$ they may even be enjoyable. Neither of them has much to say about applications though.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the more applied side I will definitely mention &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;Elements of Statistical Learning&lt;/a&gt; by Hastie et al., which provides a treatment of many modern topics and applications from statistics and machine learning. Another book that I will recommend is &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0198507658&quot; rel=&quot;nofollow&quot;&gt;In All Likelihood&lt;/a&gt; by Pawitan. It deals with more standard statistical material and applications and is fairly mathematical too. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-01T15:32:32.387" Id="33476" LastActivityDate="2012-08-01T15:32:32.387" OwnerUserId="4376" ParentId="33359" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Foreword : this is &lt;strong&gt;not&lt;/strong&gt; homework, but a real problem : in a Bayesian model comparison context, I am trying to work out the correct prior density of mixed-model parameters which are, for computational reasons, computed as the centered and standardized means of other &quot;raw&quot; parameters, which are themselves given prior distributions. See Gelman A, Hill J. &lt;em&gt;Data Analysis Using Regression and Multilevel/Hierarchical Models&lt;/em&gt;. 1st ed. Cambridge University Press; 2006. This textbook uses this trick (or variants thereof) quite often, and various simulations have convinced me of its efficacy, at least in selected cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;I currently do not have on hand any reference material (textbook, etc...) for this question, whose answer is quite probably a standard result. Could someone lend me a hand (if only by pointing me to a Web-accessible reference, which my Google attempts have been unsuccessful at referring me to) ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X=X_1,\ldots X_N$ be a sample (i. e.  vector of $N$ &lt;em&gt;independent&lt;/em&gt; random variables sampled from the same) &lt;strong&gt;normal&lt;/strong&gt; distribution ${\textrm N}(\mu_x, \sigma^2_x)$. Let $m={{\sum_{i=1}^n X_i} \over N}$ and $s^2={{\sum (X_i)^2} \over {N-1}}$  the unbiased estimators of the mean and variance of this distribution. What is the (joint) distribution of the &lt;em&gt;components&lt;/em&gt; $Y_i$ of $Y={{X-m} \over s}$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sorely tempted to answer that they are $N$ independent normal standard random variables (i. e. distributed as ${\textrm N}(0, 1)$), and my intuition is seconded by various simulation results, but I have trouble &lt;strong&gt;&lt;em&gt;proving&lt;/em&gt;&lt;/strong&gt; it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I know (or prove to my satisfaction) can be summarized as follows :&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$m$ has a ${\textrm N}(\mu_x, {{\sigma^2_x} \over N})$ distribution ; therefore, the common mean of the $Y_i$ is 0 (trivial from the definition of the mean). &lt;/li&gt;&#10;&lt;li&gt;The common variance of the $Y_i$ is 1 (a trifle less trivial but can be worked out from the definition).&lt;/li&gt;&#10;&lt;li&gt;${s^2} \over {\sigma^2_x}$ has a $\chi^2_{N-1}$ distribution  (classical  result that I remember having worked out).&lt;/li&gt;&#10;&lt;li&gt;Any textbook will state that $m \over s$ has a $t_{N-1}(0, 1)$&#10;distribution (&lt;em&gt;Not&lt;/em&gt; trivial, but I remember having worked it out, the crucial point being the independence of $m$ and $s$. I think that this historically is the origin of the distinction of the $t$ distribution, BTW...).&lt;/li&gt;&#10;&lt;li&gt;Obviously, $Y_i={{X_i} \over s}-{m \over s}$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;However, neither $m$ nor $s$ are independent of the $X_i$. Therefore &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;the distribution of ${X_i} \over s$ is unknown (to me), and&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the distribution of the sum is &lt;em&gt;not&lt;/em&gt; the convolution of the distribution of its terms.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Where am I goofing ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-01T20:18:59.783" Id="33505" LastActivityDate="2013-08-24T16:13:42.797" OwnerUserId="13024" PostTypeId="1" Score="7" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;proof&gt;" Title="Distribution of a centered standardized sample" ViewCount="165" />
  
  <row AcceptedAnswerId="33527" AnswerCount="1" Body="&lt;p&gt;I have a data set I would like to normalize in two different ways before building the multiple linear regression model. My data set looks as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;x_{1}  y_{1,1}  y_{1,2}...y_{1,n-1}y_{1,n}$$&#10;$$x_{2}  y_{2,1}  y_{2,2}...y_{2,n-1}y_{2,n}$$&#10;$$...  $$&#10;$$x_{m}  y_{m,1}  y_{m,2}...y_{m,n-1}y_{m,n}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;...where each $x_{i}, y_{i,j}$ is a count, and each row $i$ represents a data set collected from a video with a variable length $k$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To make it so that all the rows have values with equivalent meanings, I normalize each row by dividing all of the counts by $k$, the length of the video. Now, instead of counts, I have counts per minutes. I also want to normalize across each column (variable) to be from 0 to 1, with the idea that I can then compare the relative importance of each variables' coefficient to other variable coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering if this is even a valid normalization. Normalizing across each row is fine, but I'm having trouble figuring out whether normalizing across each column using a different normalization factor is valid. My instinct is that it isn't. If it is not valid, is there another way to achieve what I want with being able to relatively compare the importance of variables?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-02T03:03:01.293" FavoriteCount="1" Id="33523" LastActivityDate="2012-08-02T07:03:06.347" LastEditDate="2012-08-02T06:49:54.577" LastEditorUserId="8507" OwnerUserId="8590" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;normalization&gt;" Title="Normalization across columns in linear regression" ViewCount="2599" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;PSU, or the primary sampling unit, is the object or a group of objects what you sample in the first stage of a multi-stage sample. Typically in large scale national studies, this could be a county or a census tract. Then you go down to the level of city blocks (secondary sampling units), dwellings, households, and individuals. So when you sampled Autauga County, Alabama (one of 3K+ counties in the US, the first that comes on the standard lists), you have to think of the 50,000 people that live in it as a single unit for variance estimation purposes. Of course, you would likely subsample this county, and end up interviewing may be 10 people. However, most of the contribution to the variance comes from the first stage, especially when observations with the PSU are similar to one another.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;This is the standard formula for the variance of a clustered sample; a common knowledge, if you like. There is no simple explanation for it sans the derivation from the first principles. You would have to look at a standard survey statistics book, such as &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0495105279&quot;&gt;Lohr 2009&lt;/a&gt;, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471137731&quot;&gt;Korn &amp;amp; Graubard 1999&lt;/a&gt; or &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/041231780X&quot;&gt;Thompson 1997&lt;/a&gt; (in an increasing order of complexity and mathematical rigor).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The first principles of finite population sampling are really orthogonal to anything you've learned in statistics (be it mainstream or Bayesian or machine learning). What you measure on the sample elements is considered fixed (someone's weight or height or color of their eyes; and that make sense, except for some measurement error: your height tomorrow should not differ from your height today, so how can it be random?). What is random, however, are the indicators of the finite population elements being taken into the sample. In other words, if you talk about sampling 1000 people from US population, you are talking about a 300-million dimensional vector that has zeroes for most people who did not make it to the sample, and ones for the 1000 people who were sampled. Thus, the probability spaces that you would encounter in the world of sample survey are discrete (although combinatorially huge), and so are the sampling distributions of the sample statistics, although the latter would sometimes be well approximated by the normal distributions. The CLT-type justifications, however, are way more complicated in survey statistics, as appropriate CLTs have only been proven in limited contexts of specific sampling designs. You would need to get used to thinking in terms of the totals (because they are the only linear statistics of the random elements); to the weighted mean being a biased estimator of the population mean (as it is a ratio estimator, i.e., a non-linear statistic); and to variance estimation being an order of magnitude more complex than point estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;While Phil Kott is a very wise guy who writes quite well, I doubt that this paper is a good starting point on survey statistics. You must have been thrown into this quite harshly, I gather, to have to read this out of blue sky.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-02T04:21:15.040" Id="33524" LastActivityDate="2012-08-02T04:21:15.040" OwnerUserId="5739" ParentId="33513" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm confused about the independence of the product of independent random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; be independent of each other and &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;D&lt;/code&gt; also be independent of each other, then I understand that &lt;code&gt;AC&lt;/code&gt; and &lt;code&gt;CB&lt;/code&gt; are not necessarily independent each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt; are mutually independent of each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;In that case, are &lt;code&gt;AC&lt;/code&gt; and &lt;code&gt;BD&lt;/code&gt; independent of each other?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-02T05:00:30.527" Id="33525" LastActivityDate="2012-08-02T13:04:50.520" LastEditDate="2012-08-02T13:04:50.520" LastEditorUserId="7290" OwnerUserId="13032" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;random-variable&gt;&lt;independence&gt;" Title="Are the products of different independent random variables independent?" ViewCount="121" />
  
  <row Body="&lt;p&gt;I am not exactly sure what you mean by &quot;contribute most to the difference&quot;. If you just mean, which have the most errors, you have that in the table: Just subtract the number correct from the row total; or, if it better suits your needs you can do it in terms of proportion that are correct in each row.&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking overall accuracy assumes that all errors are equally bad. Without context, it's not possible to know if this is reasonable. But, if you can quantify the 'badness' of each type of mistake, you can pretty easily create an index of that. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-02T10:10:17.070" Id="33536" LastActivityDate="2012-08-02T10:10:17.070" OwnerUserId="686" ParentId="33534" PostTypeId="2" Score="1" />
  
  
  
</parent>
