<parent>
  <row Body="&lt;p&gt;Ironically, the MDS actually wasn't my problem.  The preprocessing I was doing was the issue.  I'm used to coding in lower-level languages and forgot how slow looping is in R.  I rewrote the preprocessing code using vector ops and the MDS actually only takes a few seconds.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-17T18:24:11.260" Id="10903" LastActivityDate="2011-05-17T18:24:11.260" OwnerUserId="1347" ParentId="10892" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="10922" AnswerCount="1" Body="&lt;p&gt;What are the pros and cons of learning about a distribution's properties algorithmically (via computer simulations) versus mathematically?&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems like computer simulations can be an alternative learning method, especially for those new students who do not feel strong in calculus.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also it seems that coding simulations can offer an earlier and more intuitive grasp of the concept of a distribution.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-17T18:24:27.733" Id="10904" LastActivityDate="2011-05-17T21:57:27.727" OwnerUserId="4329" PostTypeId="1" Score="10" Tags="&lt;distributions&gt;&lt;algorithms&gt;&lt;education&gt;" Title="What are the pros and cons of learning about a distribution algorithmically (simulations) versus mathematically?" ViewCount="735" />
  
  <row Body="&lt;p&gt;You only have 2 failures Why were you thinking you could estimate more than two coefficients?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-17T20:09:15.070" Id="10919" LastActivityDate="2011-05-17T20:09:15.070" OwnerDisplayName="DWin" OwnerUserId="2129" ParentId="10918" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here are two posts where I describe the process of computing scale scores for multiple-item, multiple-scale tests:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://jeromyanglim.blogspot.com/2009/10/scale-construction-item-reversal-scale.html&quot; rel=&quot;nofollow&quot;&gt;Scale construction and item reversal for multiple item scale in SPSS&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://jeromyanglim.blogspot.com/2009/10/calculating-scale-scores-for.html&quot; rel=&quot;nofollow&quot;&gt;Calculating scale scores in SPSS&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There are many things to consider when creating scales (e.g., should the items be equally weighted? do you have missing data? do you want a mean or sum? etc.), and there are several tricks for doing it efficiently and reliably (e.g., using loops, using syntax, automatically generating syntax from metadata), but the posts above describe this in detail.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-18T05:03:38.017" Id="10927" LastActivityDate="2011-05-18T05:03:38.017" OwnerUserId="183" ParentId="10910" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The p-values for the true null hypothesis (Ha) should be uniformly distributed (see amongst others &lt;a href=&quot;http://stats.stackexchange.com/questions/10613/why-p-values-are-uniformly-distributed&quot;&gt;q10613&lt;/a&gt;). If your two tests are independent (which they seem to be from your example), the chance of the p-value of Hb, given that for Ha's (the non-true one) is a, is simply a.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if you know the distribution of a, you may be able to integrate this out to find an analytical solution. But this depends upon your alternative, and upon which test you are using (for the false null hypothesis).&lt;/p&gt;&#10;&#10;&lt;p&gt;Extending the comment by @Henry and abusing notation somewhat:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(a&amp;lt;b) = \int p(a&amp;lt;b) da = \int (1-a) da = E(1-a)$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-18T07:45:35.023" Id="10932" LastActivityDate="2011-05-18T08:21:47.847" LastEditDate="2011-05-18T08:21:47.847" LastEditorUserId="4257" OwnerUserId="4257" ParentId="10928" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;No it should not. e.g. for logistic regression, which appears to be the case here, it may be that the greater p-value (I'm assuming you mean the right kind of p-value here, like from a likelihood ratio test) comes from increasing the odds for (correctly predicted) observations that are already relatively extreme (e.g. all observations that had predicted probability 0.8 become 0.9 and all those that had 0.2 become 0.1), while the ones that were only just on the right side of the 50% threshold are now just on the other side. As a result, the extremities are now predicted with more confidence, but there are more misclassifications.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, good fit does not guarantee good prediction (or the other way around) - even though that's the way most 'scientific' publications work these days :-(&lt;/p&gt;&#10;&#10;&lt;p&gt;I would advise you to look into a more evolved technique like LASSO or elastic net for variable selection... It will also easily allow you to optimize for some predictive measure like missclassification. In R, try glmnet.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-19T06:51:10.817" Id="10959" LastActivityDate="2011-05-19T06:51:10.817" OwnerUserId="4257" ParentId="10958" PostTypeId="2" Score="6" />
  
  
  
  
  <row AcceptedAnswerId="11028" AnswerCount="2" Body="&lt;p&gt;I'd like to regress a vector B against each of the columns in a matrix A. This is trivial if there are no missing data, but if matrix A contains missing values, then my regression against A is constrained to include only rows where all values are present (the default &lt;em&gt;na.omit&lt;/em&gt; behavior). This produces incorrect results for columns with no missing data. I can regress the column matrix B against individual columns of the matrix A, but I have thousands of regressions to do, and this is prohibitively slow and inelegant. The &lt;em&gt;na.exclude&lt;/em&gt; function seems to be designed for this case, but I can't make it work. What am I doing wrong here? Using R 2.13 on OSX, if it matters.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A = matrix(1:20, nrow=10, ncol=2)&#10;B = matrix(1:10, nrow=10, ncol=1)&#10;dim(lm(A~B)$residuals)&#10;# [1] 10 2 (the expected 10 residual values)&#10;&#10;# Missing value in first column; now we have 9 residuals&#10;A[1,1] = NA  &#10;dim(lm(A~B)$residuals)&#10;#[1]  9 2 (the expected 9 residuals, given na.omit() is the default)&#10;&#10;# Call lm with na.exclude; still have 9 residuals&#10;dim(lm(A~B, na.action=na.exclude)$residuals)&#10;#[1]  9 2 (was hoping to get a 10x2 matrix with a missing value here)&#10;&#10;A.ex = na.exclude(A)&#10;dim(lm(A.ex~B)$residuals)&#10;# Throws an error because dim(A.ex)==9,2&#10;#Error in model.frame.default(formula = A.ex ~ B, drop.unused.levels = TRUE) : &#10;#  variable lengths differ (found for 'B')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-05-19T21:03:54.640" FavoriteCount="3" Id="11000" LastActivityDate="2011-05-20T22:46:53.610" LastEditDate="2011-05-20T19:00:34.397" LastEditorUserId="1699" OwnerUserId="1699" PostTypeId="1" Score="12" Tags="&lt;r&gt;&lt;missing-data&gt;&lt;linear-model&gt;" Title="How does R handle missing values in lm?" ViewCount="11945" />
  <row Body="&lt;p&gt;&quot;True positives&quot;, like proportion classified correctly, requires arbitrary and information losing categorization of the predicted values.  These are improper scoring rules.  An improper scoring rule is a criterion that when optimized leads to a bogus model.  Also, watch out when using P-values in any way to guide model selection.  This will greatly distort the inference from the final model.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-19T21:55:58.340" Id="11002" LastActivityDate="2011-05-19T21:55:58.340" OwnerUserId="4253" ParentId="10958" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You could use something like &lt;code&gt;formals()&lt;/code&gt; or &lt;code&gt;args()&lt;/code&gt;, e.g. &lt;code&gt;formals(glm)&lt;/code&gt; gives:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; formals(glm)&#10;$formula&#10;&#10;&#10;$family&#10;gaussian&#10;&#10;$data&#10;&#10;&#10;$weights&#10;&#10;&#10;$subset&#10;&#10;&#10;$na.action&#10;&#10;&#10;$start&#10;NULL&#10;&#10;$etastart&#10;&#10;&#10;$mustart&#10;&#10;&#10;$offset&#10;&#10;&#10;$control&#10;list(...)&#10;&#10;$model&#10;[1] TRUE&#10;&#10;$method&#10;[1] &quot;glm.fit&quot;&#10;&#10;$x&#10;[1] FALSE&#10;&#10;$y&#10;[1] TRUE&#10;&#10;$contrasts
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QwNgS.png&quot; alt=&quot;Strong Law of Large Numbers&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The SLLN (convergence almost surely) says that we can be 100% sure that &lt;em&gt;this&lt;/em&gt; curve stretching off to the right will eventually, at some finite time, fall entirely within the bands forever afterward (to the right).&lt;/p&gt;&#10;&#10;&lt;p&gt;The R code used to generate this graph is below (plot labels omitted for brevity).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 1000;  m &amp;lt;- 50; e &amp;lt;- 0.05&#10;s &amp;lt;- cumsum(2*(rbinom(n, size=1, prob=0.5) - 0.5))&#10;plot(s/seq.int(n), type = &quot;l&quot;, ylim = c(-0.4, 0.4))&#10;abline(h = c(-e,e), lty = 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rtcqE.png&quot; alt=&quot;Weak Law of Large Numbers&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The WLLN (convergence in probability) says that a large proportion of the sample paths will be in the bands on the right-hand side, at time $n$ (for the above it looks like around 48 or 9 out of 50).  We can never be sure that any &lt;em&gt;particular&lt;/em&gt; curve will be inside at any finite time, but looking at the mass of noodles above it'd be a pretty safe bet.  The WLLN also says that we can make the proportion of noodles inside as close to 1 as we like by making the plot sufficiently wide.&lt;/p&gt;&#10;&#10;&lt;p&gt;The R code for the graph follows (again, skipping labels).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- matrix(2*(rbinom(n*m, size=1, prob=0.5) - 0.5), ncol = m)&#10;y &amp;lt;- apply(x, 2, function(z) cumsum(z)/seq_along(z))&#10;matplot(y, type = &quot;l&quot;, ylim = c(-0.4,0.4))&#10;abline(h = c(-e,e), lty = 2, lwd = 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-05-20T02:47:18.010" Id="11013" LastActivityDate="2011-05-20T12:06:18.283" LastEditDate="2011-05-20T12:06:18.283" LastEditorUserId="1108" OwnerUserId="1108" ParentId="2230" PostTypeId="2" Score="14" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Let's aim for some at an introductory level, some articles and some textbooks.  Applied is more helpful, including R code is great.  Thanks!&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2011-05-20T09:23:56.153" CreationDate="2011-05-20T03:54:39.277" FavoriteCount="4" Id="11018" LastActivityDate="2011-07-15T19:58:50.013" LastEditDate="2011-05-20T09:21:52.803" LastEditorUserId="88" OwnerUserId="3748" PostTypeId="1" Score="7" Tags="&lt;sampling&gt;&lt;books&gt;&lt;weighted-sampling&gt;" Title="Recommend references on survey sample weighting" ViewCount="1171" />
  <row AcceptedAnswerId="11046" AnswerCount="3" Body="&lt;p&gt;I am doing text classification, and have been playing around with different classifiers. However I have a pretty basic question: what if a new unseen document comes in and it happens to not belong to any of the pre-existing classes? The classifiers that I have seen (in WEKA, libsvm etc.) still go ahead and put the unseen document in one of the existing classes anyway.&lt;/p&gt;&#10;&#10;&lt;p&gt;This situation comes up pretty frequently in my work. How can I handle it sensibly?&lt;/p&gt;&#10;&#10;&lt;p&gt;On a related note, is there a way I could get a sense of how confident a classifier is regarding it's classification decision?&lt;/p&gt;&#10;&#10;&lt;p&gt;================================Edit 1======================================&lt;/p&gt;&#10;&#10;&lt;p&gt;After reading the comments here, I think I am going to try one of the two things:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Use the approach described in &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.7457&quot; rel=&quot;nofollow&quot;&gt;Zadrozny,Elkan&lt;/a&gt; paper that Steffen pointed me to, in order to get the probability estimates. If the probabilities are less than a magic threshold, I could then simply discard the unseen instance as noise. &lt;/p&gt;&#10;&#10;&lt;p&gt;(2) I am increasingly starting to think that I could instead handle this as &lt;em&gt;n&lt;/em&gt; 1-class problems. let's say that I have &lt;em&gt;n&lt;/em&gt; classes in my data. I could build and train &lt;em&gt;n&lt;/em&gt; 1-class classifiers that give a yes/no decision (whether an instance belongs to a particular class or not). So that way when a new instance comes in, I could just pass it through each of there 1-class classifiers. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thoughts? Any implementations/packages anyone is aware of that would let me do (2)?&lt;/p&gt;&#10;&#10;&lt;p&gt;===========================================================================&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-20T04:07:25.990" Id="11019" LastActivityDate="2011-05-20T15:25:44.807" LastEditDate="2011-05-20T14:37:42.537" LastEditorUserId="3301" OwnerUserId="3301" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" Title="How does a classifier handle unseen documents that do not belong to any of the pre-existing classes?" ViewCount="383" />
  
  
  
  
  <row Body="&lt;p&gt;As I have to explain variable selection methods quite often, not in a teaching context, but for non-statisticians requesting aid with their research, I love this extremely simple example that illustrates why single variable selection is not necessarily a good idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have this dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y      X1     x2&#10;1       1      1&#10;1       0      0&#10;0       1      0&#10;0       0      1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It doesn't take long to realize that both X1 and X2 individually are completely noninformative for y (when they are the same, y is 'certain' to be 1 - I'm ignoring sample size issues here, just assume these four observations to be the whole universe). However, the combination of the two variables is &lt;em&gt;completely&lt;/em&gt; informative. As such, it is more easy for people to understand why it is not a good idea to (e.g.) only check the p-value for models with each individual variable as a regressor.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my experience, this really gets the message through.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-05-20T20:15:32.913" CreationDate="2011-05-20T20:15:32.913" Id="11059" LastActivityDate="2011-05-20T20:15:32.913" OwnerUserId="4257" ParentId="11050" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;I just googled &quot;R getOption(&quot;max.print&quot;)&quot;, and found: &lt;code&gt;options(max.print=5.5E5)&lt;/code&gt;...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-20T22:38:56.957" Id="11067" LastActivityDate="2011-05-20T22:38:56.957" OwnerUserId="4257" ParentId="11066" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I need an help because I don´t know if the command for the ANOVA analysis I am &#10;performing in R is correct. Indeed using the function aov I get the following error: &lt;code&gt;In aov (......) Error() model is singular&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The structure of my table is the following: subject, stimulus, condition, sex, response&lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subject  stimulus condition sex    response&#10;subject1    gravel  EXP1    M      59.8060&#10;subject2    gravel  EXP1    M      49.9880&#10;subject3    gravel  EXP1    M      73.7420&#10;subject4    gravel  EXP1    M      45.5190&#10;subject5    gravel  EXP1    M      51.6770&#10;subject6    gravel  EXP1    M      42.1760&#10;subject7    gravel  EXP1    M      56.1110&#10;subject8    gravel  EXP1    M      54.9500&#10;subject9    gravel  EXP1    M      62.6920&#10;subject10   gravel  EXP1    M      50.7270&#10;subject1    gravel  EXP2    M      70.9270&#10;subject2    gravel  EXP2    M      61.3200&#10;subject3    gravel  EXP2    M      70.2930&#10;subject4    gravel  EXP2    M      49.9880&#10;subject5    gravel  EXP2    M      69.1670&#10;subject6    gravel  EXP2    M      62.2700&#10;subject7    gravel  EXP2    M      70.9270&#10;subject8    gravel  EXP2    M      63.6770&#10;subject9    gravel  EXP2    M      72.4400&#10;subject10   gravel  EXP2    M      58.8560&#10;subject11   gravel  EXP1    F      46.5750&#10;subject12   gravel  EXP1    F      58.1520&#10;subject13   gravel  EXP1    F      57.4490&#10;subject14   gravel  EXP1    F      59.8770&#10;subject15   gravel  EXP1    F      55.5480&#10;subject16   gravel  EXP1    F      46.2230&#10;subject17   gravel  EXP1    F      63.3260&#10;subject18   gravel  EXP1    F      60.6860&#10;subject19   gravel  EXP1    F      59.4900&#10;subject20   gravel  EXP1    F      52.6630&#10;subject11   gravel  EXP2    F      55.7240&#10;subject12   gravel  EXP2    F      66.4220&#10;subject13   gravel  EXP2    F      65.9300&#10;subject14   gravel  EXP2    F      61.8120&#10;subject15   gravel  EXP2    F      62.5160&#10;subject16   gravel  EXP2    F      65.5780&#10;subject17   gravel  EXP2    F      59.5600&#10;subject18   gravel  EXP2    F      63.8180&#10;subject19   gravel  EXP2    F      61.4250&#10;.....&#10;.....&#10;.....&#10;.....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can notice each subject repeated the evaluation in 2 conditions (EXP1 and EXP2).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am interested in is to know if there are significant differences between &#10;the evaluations of the males and the females.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the command I used to perform the ANOVA with repeated measures:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)&#10;summary(aov1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get the following error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)&#10;Warning message:&#10;In aov(response ~ stimulus * sex + Error(subject/(stimulus * sex)),  :&#10;Error() model is singular&#10;&amp;gt; summary(aov1)&#10;&#10;Error: subject&#10;          Df  Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;sex        1  166.71  166.72   1.273  0.274&#10;Residuals 18 2357.29  130.96               &#10;&#10;Error: subject:stimulus&#10;              Df Sum Sq Mean Sq F value Pr(&amp;gt;F)    &#10;stimulus       6 7547.9 1257.98 35.9633 &amp;lt;2e-16 ***&#10;stimulus:sex   6   94.2   15.70  0.4487 0.8445    &#10;Residuals    108 3777.8   34.98                   &#10;---&#10;Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 &#10;&#10;Error: Within&#10;           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;Residuals 420 9620.6  22.906               &#10;&amp;gt; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The thing is that looking at the data it is evident for me that there is a &#10;difference between male and females, because for each stimulus I always get&#10;a mean higher for the males rather than the females. &#10;Therefore the ANOVA should indicate significant differences....&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anyone who can suggest me where I am wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I know that in R there are two libraries on linear mixed models called &#10;nlme and lme4, but I have never used it so far and I don´t know if I have to utilize it for my case.&#10;Is it the case to utilize it? If yes, could you please provide a quick R example&#10;of a command which could solve my problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;Best regards&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Dear all, &#10;I am stuck now ;-( Indeed I understood everything you suggested me but still I don´t get significance in the ANOVA results, and definitively there is an error, because results cannot be non-significant. Indeed looking at the means for each stimulus, it is possible to notice that males gave always higher evaluations than females.&lt;/p&gt;&#10;&#10;&lt;p&gt;To prove this I discarded for a moment the effect of the repeated measures, and I performed an ANOVA separately on both the two conditions (EXP1 and EXP2) during which the evaluations were given.&#10;What I get is significant differences between males and female, in both EXP1 and EXP2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, why when I perform the ANOVA with repeated measures I don´t get the same behavior?&lt;/p&gt;&#10;&#10;&lt;p&gt;My design is the following:&#10;-sex is a between-subjects factor (with two levels)&#10;-stimulus is a within-subjects factor (with 3 assumed levels)&#10;-condition is a within-subjects factor (with 2 levels)&#10;-all factors are fully crossed&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried, both the ways suggested but without achieving significance: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mDf &amp;lt;- aggregate(response ~ subject + sex, data=scrd, FUN=mean)&#10;summary(aov(response ~ sex, data=mDf))     # ANOVA with just the between-effect&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov1 = aov(response ~ sex*stimulus*condition + Error(subject/(stimulus*condition)), data=scrd)&#10;summary(aov1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Instead if I perform the ANOVA on the two subtables of EXP 1 and 2 I get significant differences. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;table_EXP1 &amp;lt;- subset(scrd, condition == &quot;EXP1&quot;)&#10;table_EXP2 &amp;lt;- subset(scrd, condition == &quot;EXP2&quot;)&#10;&#10;&#10;fit_table_EXP1 &amp;lt;- lm(response ~ stimulus*sex, data=table_EXP1) &#10;summary(fit_table_EXP1 )&#10;anova(fit_table_EXP1 )&#10;&#10;&#10;fit_table_EXP2 &amp;lt;- lm(response ~ stimulus*sex, data=table_EXP2) &#10;summary(fit_table_EXP2)&#10;anova(fit_table_EXP2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;....how can this be possible?...it is a contraddiction....&lt;/p&gt;&#10;&#10;&lt;p&gt;HELP!&lt;/p&gt;&#10;&#10;&lt;p&gt;Please enlighten me!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-05-21T12:24:06.390" FavoriteCount="2" Id="11079" LastActivityDate="2011-05-28T20:12:23.450" LastEditDate="2011-05-28T20:12:23.450" LastEditorUserId="919" OwnerUserId="4701" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;&lt;mixed-model&gt;" Title="Problem with ANOVA repeated measures: &quot;Error() model is singular&quot;" ViewCount="5536" />
  <row AcceptedAnswerId="11089" AnswerCount="1" Body="&lt;p&gt;I was just wondering why regression problems are called &quot;regression&quot; problems. What is the story behind the name? &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One definition for regression:&#10;  &quot;Relapse to a less perfect or&#10;  developed state.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2011-05-21T18:25:00.283" FavoriteCount="5" Id="11087" LastActivityDate="2011-05-21T18:54:18.390" LastEditDate="2011-05-21T18:44:13.433" LastEditorUserId="930" OwnerUserId="3541" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;terminology&gt;" Title="Why are regression problems called &quot;regression&quot; problems?" ViewCount="1267" />
  <row AnswerCount="2" Body="&lt;p&gt;Apologies for what is probably a very basic question. I have looked around both here and in the usual places and haven't had any luck.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have read that there are at least two methods for linearly transforming data so that you can give your distribution a certain desired standard deviation. What are they and are there cases where you'd want to use one method rather than another?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just for concreteness's sake, let's say you have test scores from 0-50, a mean of 35 and sd of 10, and you wanted to rescale so the sd is 15.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-21T21:07:11.437" Id="11093" LastActivityDate="2011-05-21T22:38:27.183" OwnerUserId="52" PostTypeId="1" Score="4" Tags="&lt;data-transformation&gt;&lt;standard-deviation&gt;" Title="Rescaling for desired standard deviation" ViewCount="1345" />
  
  
  
  
  <row AcceptedAnswerId="11110" AnswerCount="4" Body="&lt;p&gt;If you have a variable which perfectly separates zeroes and ones in target variable, R will yield the following &quot;perfect or quasi perfect separation&quot; warning message:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Warning message:&#10;glm.fit: fitted probabilities numerically 0 or 1 occurred &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We still get the model but the coefficient estimates are inflated. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do you deal with this in practice?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-22T10:37:08.303" FavoriteCount="26" Id="11109" LastActivityDate="2014-04-04T11:10:31.900" LastEditDate="2013-11-04T17:02:43.997" LastEditorUserId="22047" OwnerUserId="333" PostTypeId="1" Score="36" Tags="&lt;r&gt;&lt;logistic&gt;" Title="How to deal with perfect separation in logistic regression?" ViewCount="13729" />
  
  
  <row AcceptedAnswerId="11119" AnswerCount="1" Body="&lt;p&gt;Given a data frame in R is there a way to export it in &lt;em&gt;R syntax&lt;/em&gt; such that executing this code would re-create the data frame? I would find this useful to store results in R files along with calculations without depending on external files.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-22T16:37:29.693" Id="11118" LastActivityDate="2011-06-03T22:30:59.033" LastEditDate="2011-06-03T22:30:59.033" LastEditorUserId="930" OwnerUserId="4195" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;dataframe&gt;" Title="How to export data in R syntax?" ViewCount="288" />
  
  
  <row Body="&lt;p&gt;To answer your question , one would be advised to build a single equation model which captured day-of-the-week effects (6 dummy indicators) and an indicator for the &quot;event&quot;. Software exists to capture any lead, contemporaneous and.or lag effects around known event. In the absence of such software you might try and &quot;roll your own&quot; in order to identify the &quot;window of response&quot; around your event variable. In addition you might want to include week-of-the-year and/or month-of-the-year variables to handle annual seasonality. Furthermore since this is daily data you might want to include other events such as the Holidays and also incorporating any required &quot;window of response&quot; around these Holidays. You should also try to identify &quot;particular days of the month&quot; irrespective of what day of the week they fall on as important contributors to explaining daily demand such as the end-of-the-month or the day seniors get their social security checks et al . Other factors that might be important in the &quot;discovery model phase&quot; might be the empirical identification of mean shifts and/or local time trends. Before congratulating yourself for either writing such a &quot;tour de force&quot; you might (should !) validate that neither the model parameters of the error variance have changed over time. By sharing your data and inviting the list to provide quality analysis of your data we could all learn what others are doing in a precise manner. Upon completion of the modelling phase one could then eliminate the event variables effect , essentially scrubbing the data by obtaining a realization of the daily sales series without the events coefficients/effects being incorporated. The difference between the originally observed series and this scrubbed series provides an estimate of the events effects.This recommended approach uses all of the data as compared to the incorrect approach of trying to forecast data values at a point prior to the events effects which of course is the unknown ( but to be found ) point in time where the effect &quot;starts&quot;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-24T01:19:16.530" Id="11171" LastActivityDate="2011-05-24T01:52:15.880" LastEditDate="2011-05-24T01:52:15.880" LastEditorUserId="3382" OwnerUserId="3382" ParentId="11161" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You actualy want to select the maximum element from the elements with the same id. For that you can use &lt;code&gt;ddply&lt;/code&gt; from package &lt;strong&gt;plyr&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dt&amp;lt;-data.frame(id=c(1,1,2,2,3,4),var=c(2,4,1,3,4,2))&#10;&amp;gt; ddply(dt,.(id),summarise,var_1=max(var))&#10;   id var_1&#10;1  1   4&#10;2  2   3&#10;3  3   4&#10;4  4   2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;unique&lt;/code&gt; and &lt;code&gt;duplicated&lt;/code&gt; is for removing duplicate records, in your case you only have duplicate ids, not records.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Here is the code when there are additional variables:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dt&amp;lt;-data.frame(id=c(1,1,2,2,3,4),var=c(2,4,1,3,4,2),bu=rnorm(6))&#10;&amp;gt; ddply(dt,~id,function(d)d[which.max(d$var),])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2011-05-24T14:33:45.407" Id="11194" LastActivityDate="2011-05-24T19:43:38.453" LastEditDate="2011-05-24T19:43:38.453" LastEditorUserId="2116" OwnerUserId="2116" ParentId="11193" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The eta-square ($\eta^2$) value you are describing is intended to be used as a measure of &lt;em&gt;effect size&lt;/em&gt; in the observed data (i.e., your sample), as it amounts to quantify how much of the total variance can be explained by the factor considered in the analysis (that is what you wrote in fact, BSS/TSS). With more than one factor, you can also compute partial $\eta^2$ that reflect the percentage of variance explained by one factor when holding constant the remaining ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;The F-ratio (BSS/WSS) is the right &lt;em&gt;test statistic&lt;/em&gt; to use if you want to test the null hypothesis ($H_0$) that there is no effect of your factor (all group means are equal), that is your factor of interest doesn't account for a sufficient amount of variance compared to the residual (unexplained) variance. In other words, we test whether the added explained variance (BSS=TSS-RSS) is large enough to be considered as a &quot;significant quantity&quot;. The distribution of the ratio of these two sum of squares (scaled by their corresponding degrees of freedom--this answers one of your question, about why we don't use directly SSs), which individually follow a $\chi^2$ distribution, is known as the &lt;a href=&quot;http://en.wikipedia.org/wiki/F-distribution&quot; rel=&quot;nofollow&quot;&gt;Fisher-Snedecor&lt;/a&gt; distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know which software you are using, but &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you have R, everything you need for basic modeling is given in the &lt;code&gt;aov()&lt;/code&gt; base function ($\eta^2$ might be computed with &lt;code&gt;etasq&lt;/code&gt; from the &lt;a href=&quot;http://cran.r-project.org/web/packages/heplots/index.html&quot; rel=&quot;nofollow&quot;&gt;heplots&lt;/a&gt; package; and there's a lot more to see for diagnostics and plotting in other packages).&lt;/li&gt;&#10;&lt;li&gt;If you're more versed into C programming, you may have a look at the &lt;a href=&quot;http://apophenia.sf.net/&quot; rel=&quot;nofollow&quot;&gt;apophenia&lt;/a&gt; library which features a nice set of statistical functions with bindings for MySQL and Python.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2011-05-24T14:34:40.337" Id="11195" LastActivityDate="2011-05-24T14:34:40.337" OwnerUserId="930" ParentId="10111" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="11201" AnswerCount="1" Body="&lt;p&gt;Today I opened two STATA windows and ran the following command in both:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set obs 100&#10;gen x = rnormal()&#10;sort x&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(the difference is that on the second window I generated a variable called y).  Summing up: I asked STATA to give me 100 pseudo-random numbers taken from a standard normal distribution, then I sorted it.  To my surprise, the numbers of the x and y vectors are the same!  I did this at home, and then at work, and my impression is that all of these vectors are the same.  Is there an explanation for this, to me, strange behavior?&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is a problem in STATA, does R have a better pseudo-random number generator procedure?&lt;/p&gt;&#10;&#10;&lt;p&gt;A side question.  I came up to this &quot;problem&quot; because I was trying to generate two pseudo-random columns in Stata (x and y, say), and then sort then separately.  But the two commands I know for sorting (sort and gsort) sort the whole database, not separate columns.  Would you know of a Stata command that allows me to sort a column while keeping the other columns fixed?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-24T15:53:05.780" Id="11200" LastActivityDate="2011-05-24T16:07:17.587" LastEditDate="2011-05-24T16:07:17.587" LastEditorUserId="919" OwnerUserId="2929" PostTypeId="1" Score="4" Tags="&lt;stata&gt;&lt;random-generation&gt;" Title="Generating sorted pseudo-random numbers in Stata" ViewCount="1076" />
  <row Body="&lt;p&gt;Holt's or Winter-Holt's exponential smoothing methods can give negative values for purely non-negative input values because of the &lt;em&gt;trend factor&lt;/em&gt; which acts as a kind of inertia, which can drive the time series below zero. Normal exponential smoothing doesn't have this problem, it's always smoothing &lt;em&gt;inwards&lt;/em&gt;, it never overshoots.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-24T17:14:00.257" Id="11205" LastActivityDate="2011-05-24T17:14:00.257" OwnerUserId="4360" ParentId="11202" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Yes, you should absolutely compare your predicted values with actual values.  This is good practice with any kind of statistical modeling, not just time series analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;If certain months are consistently off, you should use a seasonal model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-24T17:20:16.913" Id="11207" LastActivityDate="2011-05-24T17:20:16.913" OwnerUserId="2817" ParentId="11203" PostTypeId="2" Score="5" />
&#10;\newcommand{\e}{\mathbb{E}}
  <row AnswerCount="2" Body="&lt;p&gt;I have been reading about appropriate measures of central tendency for ordinal level data. &#10;So far I have learned that the median and mode can be used but that the latter can only be used in some cases. Some sources state that the median can only be used with Likert questions when there is an odd number of scores. It is not clear to me what this means and also which cases the median cannot be used. &lt;/p&gt;&#10;&#10;&lt;h2&gt;Example:&lt;/h2&gt;&#10;&#10;&lt;p&gt;An example may illustrate. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If there was a question: &quot;Climate change is England’s most serious environmental problem&quot; on a response scale: 1=strongly agree 2=agree 3=unsure 4=disagree 5=strongly disagree. Would the median be 3=unsure?&lt;/li&gt;&#10;&lt;li&gt;What if no respondents stated disagree or strongly disagree and all 100 respondents stated either 1, 2, or 3, is the median then 2? &lt;/li&gt;&#10;&lt;li&gt;what if respondents only stated 2 or 3. In this case is it not possible to identify the median?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-05-25T00:29:48.363" FavoriteCount="1" Id="11219" LastActivityDate="2012-09-04T18:44:50.493" LastEditDate="2011-05-25T03:22:21.527" LastEditorUserId="4498" OwnerUserId="4498" PostTypeId="1" Score="3" Tags="&lt;median&gt;" Title="Median value on ordinal scales" ViewCount="8759" />
  <row Body="&lt;h3&gt;Definitional issues:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The median is the middle value of the data; it is not by definition the middle value of the scale.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When the sample size is even, then the median is the mean of the values either side of middle most point after rank ordering all values (&lt;a href=&quot;http://en.wikipedia.org/wiki/Median#For_an_even_number_of_values&quot; rel=&quot;nofollow&quot;&gt;see wikipedia description&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;When to use median on ordinal data&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;In theory the median can be used on data from any variable where the values can be ordered.&lt;/li&gt;&#10;&lt;li&gt;In practice, the median is  often not the most useful summary of central tendency with ordinal variables. &#10;This partially depends on what you want to get out of your measure of central tendency.&#10;When you are describing the central tendency of data on an ordinal variable with only a small number of response options (i.e., perhaps less than 20 or 50 or 100), the median can be quite gross (e.g., 1,1,3,3,3 and 1,3,3,5,5 both have a median of 3, but the second example would have a higher mean). &#10;When it comes to summarising the central tendency of Likert items, I find the mean to be much more useful and sensitive to meaningful differences.&#10;Ordinal variables that are ranks do not suffer from this problem of &quot;grossness&quot;.&lt;/li&gt;&#10;&lt;li&gt;Interpolated medians are another way of overcoming the gross nature of the median on ordinal data with few values.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2011-05-25T05:41:03.343" Id="11226" LastActivityDate="2011-05-25T05:41:03.343" OwnerUserId="183" ParentId="11219" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;I am trying to compare the difference between two means with two pairwise samples. Unfortunately, my data are very far of being normal. What test would you recommend to use in this situation? Should I revert to a nonparametric test?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-25T11:30:44.473" FavoriteCount="1" Id="11232" LastActivityDate="2011-05-25T13:44:31.233" LastEditDate="2011-05-25T11:40:42.647" LastEditorUserId="2116" OwnerUserId="6245" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;" Title="Testing difference between two means with pairwise data and absence of normality" ViewCount="433" />
  
  
  
  <row Body="&lt;p&gt;The &lt;strong&gt;normality&lt;/strong&gt; assumption is a &lt;em&gt;convenient&lt;/em&gt; property of model's residuals, since it enables correct inferences about the estimated parameters and critical values of many other tests are also dependent on this assumption (therefore some corrections should be made, or you may roughly take more strict rule-of-thumb criteria, increasing the acceptable range of your tests), however it &lt;em&gt;doesn't ruin&lt;/em&gt; the regression estimators. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thus it &lt;em&gt;may&lt;/em&gt; (you still need to check the other assumptions) produce well behaved &lt;em&gt;predictions&lt;/em&gt;, but &lt;em&gt;data-mining&lt;/em&gt; and &lt;em&gt;hypothesis testing&lt;/em&gt; would be a bit more difficult. At this point I do agree with Huber that you need to clarify the purpose of the model.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Regarding some &lt;strong&gt;tips&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;At first glance it seems that your distribution after $Y-10$ transformation, could be approximated by some truncated versions of continuous distributions: exponential (&lt;a href=&quot;http://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;Gamma&lt;/a&gt;), &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot;&gt;log-normal&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Pareto_distribution&quot;&gt;Pareto&lt;/a&gt; or some other. So in log-normal case you still may move to something close to normality.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another option could be to try something like fitting the combo of &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalised_logistic_function&quot;&gt;generalized logistic function&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt;. Since you DO know the upper and lower limits it seems feasible.   &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-05-26T05:26:42.400" Id="11260" LastActivityDate="2011-05-26T05:26:42.400" OwnerUserId="2645" ParentId="11256" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Just to add to Frank's points and paint a somewhat finer picture: CART/RPART is indeed highly exploratory and adding a p-value is difficult. I have seen some rare cases where people tried to use bootstrapping to get such p-value but I agree with Frank that it's not worth the effort.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for combining statistical inference with recursive partitioning more generally: The CTree and MOB methods implemented in the &quot;party&quot; package as well as several other tools outside R (especially the work of Loh &amp;amp; co-authors) tries to combine standard statistical tests (nonparametric in case of CTree, parametric for MOB) for growing the trees. They also control their error level in the sense of a closed testing procedure. However, the inference for the final fitted tree is still not trivial. The situation is similar to a regression model where you have selected interactions between your regressors in a forward search (and we know how Frank feels about that). Hence, also the &quot;party&quot; package has no anova() methods. We do provide AIC() for &quot;mob&quot; objects, though. It is not strictly valid (because we haven't done full optimization of the log-likelihood but only forward search) but it would be conservative. So the error is into the preferable direction.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-05-26T07:49:23.200" Id="11265" LastActivityDate="2011-05-26T07:49:23.200" OwnerUserId="4760" ParentId="11246" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;There are two main types of traditional treatments of missing data.&#10;These are:&#10;1) listwise&#10;2) pairwise&lt;/p&gt;&#10;&#10;&lt;p&gt;Listwise is (from what you have said) the default in SAS. It means that you exclude any observation that has missing values in any of the terms in your model.&#10;The advantage of this is that it ensures that all variables have the same n in the model. The disadvantage is that if you have one variable with a high proportion of missing data, much of the rest of the data can be ignored. &lt;/p&gt;&#10;&#10;&lt;p&gt;Pairwise, by contrast, only excludes cases where the value is missing for that particular variable. This has the advantage of keeping more data in the model that listwise does. However, the disadvantage is that some of your results will be based on different subsets of the data, and this can cause problems with p values and confidence intervals. &lt;/p&gt;&#10;&#10;&lt;p&gt;A better approach is probably multiple imputation where you attempt to predict the missing values using all of the data you have. This normally involves simulating a number of datasets with the missing data filled in, and then performing all analyses on each of the datasets individually, and averaging the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good paper on treatment of missing data is Graham 2009 &lt;a href=&quot;http://www.iapsych.com/articles/graham2009.pdf&quot; rel=&quot;nofollow&quot;&gt;Missing Data: Making it work in the real world&lt;/a&gt; which goes into much more detail than I have here. You also need to determine if the missingness in your data is random or not, as if the probability of missingness depends on exogenous variables to your dataset, then none of these approaches will work correctly. &lt;/p&gt;&#10;&#10;&lt;p&gt;A good resource on multiple imputation is &lt;a href=&quot;http://www.multiple-imputation.com/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/modules/missing.htm&quot; rel=&quot;nofollow&quot;&gt;This&lt;/a&gt; article on missing data in SAS may prove helpful, or someone else may answer as to how to do this in SAS. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-26T16:51:15.040" Id="11284" LastActivityDate="2011-05-26T16:51:15.040" OwnerUserId="656" ParentId="11283" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for some probability inequalities for sums of unbounded random variables. I would really appreciate it if anyone can provide me some thoughts.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is to find an exponential upper bound over the probability that the sum of unbounded i.i.d. random variables, which are in fact the multiplication of two i.i.d. Gaussian, exceeds some certain value, i.e., $\mathrm{Pr}[ X  \geq \epsilon\sigma^2 N] \leq \exp(?)$, where $X = \sum_{i=1}^{N} w_iv_i$, $w_i$ and $v_i$ are generated i.i.d. from $\mathcal{N}(0, \sigma)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to use the Chernoff bound using moment generating function (MGF), the derived bound is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray}
  <row AcceptedAnswerId="11314" AnswerCount="1" Body="&lt;p&gt;I have come across the sampling method called &quot;Propensity Weighting Sampling/RIM&quot;, but I do not have a good idea of what these survey methods are all about. &lt;/p&gt;&#10;&#10;&lt;p&gt;What references in the literature cover this topic?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-26T20:06:04.120" FavoriteCount="2" Id="11290" LastActivityDate="2012-12-16T09:36:23.267" LastEditDate="2012-12-16T09:36:23.267" LastEditorUserId="3826" OwnerUserId="4278" PostTypeId="1" Score="3" Tags="&lt;sampling&gt;&lt;weighted-sampling&gt;" Title="What is a propensity weighting sampling / RIM?" ViewCount="2442" />
  
  <row Body="&lt;p&gt;From the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/lattice/html/qqmath.html&quot; rel=&quot;nofollow&quot;&gt;documentation for qqmath&lt;/a&gt; it seems that the default behavior is to compare the empirical quantiles to those of a normal distribution. So what the QQ plot for $\sigma^2$ (which is the error variance) is telling you is that its marginal posterior distribution is not normal, which you would fully expect. So it's not really informative or meaningful. &lt;/p&gt;&#10;&#10;&lt;p&gt;The remaining panels are telling you that the marginal posterior distributions of the regression coefficients are basically normal, perhaps with slightly heavier tails because they are marginalized over $\sigma^2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The MCMC regression does give you more information in that you have a full posterior distribution over all the parameters, so you can answer questions like &quot;Given the data I've seen and my prior information, what is the probability that the tempwarmer coefficient is positve&quot;. You can also look at joint distributions of two or more parameters, or functions of the coefficients, and on and on. &lt;/p&gt;&#10;&#10;&lt;p&gt;I should mention that these QQ plots aren't really giving you an indication of goodness of fit. For that you would want to look at some other measures - posterior predictive distributions, for example, or MCMCregress may be able to do formal Bayesian model comparisons for you too. I'm not familiar with that particular function but a cursory Googling turned up some potential leads.&lt;/p&gt;&#10;&#10;&lt;p&gt;All that said, if the lm() fit is good I wouldn't loose much sleep over it. But I would most wholeheartedly encourage any forays into Bayesian analysis :)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-26T20:15:53.020" Id="11291" LastActivityDate="2011-05-26T20:15:53.020" OwnerUserId="26" ParentId="11236" PostTypeId="2" Score="4" />
  
  
  
  <row AnswerCount="4" Body="&lt;p&gt;I have a table  with four groups (4 BMI groups) as the independent variable (factor). I have a dependent variable that is &quot;percent mother smoking in pregnancy&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it permissible to use ANOVA for this or do I have to use chi-square or some other test? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-27T00:39:52.903" FavoriteCount="6" Id="11296" LastActivityDate="2012-11-08T22:33:41.670" LastEditDate="2011-05-27T02:16:45.603" LastEditorUserId="183" OwnerUserId="4774" PostTypeId="1" Score="5" Tags="&lt;anova&gt;" Title="Using ANOVA on percentages?" ViewCount="12707" />
  <row Body="&lt;p&gt;The classic problem with PCR is that principal components corresponding to small eigenvalues (and hence discarded) can be significant for explaining the dependent variable. One of the solutions to this problem is to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Partial_least_squares_regression&quot; rel=&quot;nofollow&quot;&gt;PLS regression&lt;/a&gt;. In PLS regression the principal components are picked  to have maximal correlation with dependent variable.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-27T06:59:20.680" Id="11303" LastActivityDate="2011-05-27T06:59:20.680" OwnerUserId="2116" ParentId="11231" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;See also the &lt;a href=&quot;http://cran.r-project.org/web/packages/glmulti/index.html&quot; rel=&quot;nofollow&quot;&gt;glmulti&lt;/a&gt; package on CRAN and the accompanying &lt;a href=&quot;http://www.jstatsoft.org/v34/i12/paper&quot; rel=&quot;nofollow&quot;&gt;JSS paper&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;glmulti&lt;/code&gt; provides a wrapper for &lt;code&gt;glm&lt;/code&gt; and similar functions (&lt;code&gt;glm.nb&lt;/code&gt;, etc.), automatically generating all possible models (under constraints set by the user) with the specified response and explanatory variables, and finding the best models in terms of some Information Criterion (AIC, AICc or BIC). It can handle very large numbers of candidate models and features a Genetic Algorithm to find the best models when an exhaustive screening of the candidates is not feasible.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-27T09:30:47.783" Id="11307" LastActivityDate="2011-05-27T09:30:47.783" OwnerUserId="103" ParentId="11280" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You may know that weighting generally aims at ensuring that a given sample is representative of its target population. If in your sample some attributes (e.g., gender, SES, type of medication) are less well represented than in the population from which the sample comes from, then we may adjust the weights of the incriminated statistical units to better reflect the hypothetical target population.&lt;/p&gt;&#10;&#10;&lt;p&gt;RIM weighting (or raking) means that we will equate the sample marginal distribution to the theoretical marginal distribution. It bears some idea with post-stratification, but allows to account for many covariates. I found a good overview in this handout about &lt;a href=&quot;http://sekhon.berkeley.edu/causalinf/sp2010/section/week9.pdf&quot; rel=&quot;nofollow&quot;&gt;Weighting Methods&lt;/a&gt;, and here is an example of its use in a real study: &lt;a href=&quot;http://www.fcsm.gov/01papers/Greene.pdf&quot; rel=&quot;nofollow&quot;&gt;Raking Fire Data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Propensity weighting is used to compensate for unit non-response in a survey, for example, by increasing the sampling weights of the respondents in the sample using estimates of the probabilities that they responded to the survey. This is in spirit the same idea than the use of propensity scores to adjust for treatment selection bias in observational clinical studies: based on external information, we estimate the probability of patients being included in a given treatment group and compute weights based on factors hypothesized to influence treatment selection. Here are some pointers I found to go further: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.statistics.su.se/modernsurveys/publ/11.pdf&quot; rel=&quot;nofollow&quot;&gt;The propensity score and estimation in nonrandom surveys - an overview&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.amstat.org/sections/srms/proceedings/y2009/Files/304345.pdf&quot; rel=&quot;nofollow&quot;&gt;A Simulation Study to Compare Weighting Methods for Nonresponses in the National Survey of Recent College Graduates&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.jds-online.com/file_download/94/JDS-233.pdf&quot; rel=&quot;nofollow&quot;&gt;A Comparison of Propensity Score and Linear Regression Analysis of Complex Survey Data&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As for a general reference, I would suggest &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Kalton G, Flores-Cervantes I.&#10;  Weighting Methods. J. Off. Stat.&#10;  (2003) 19: 81-97. Available on&#10;  &lt;a href=&quot;http://www.jos.nu/&quot; rel=&quot;nofollow&quot;&gt;http://www.jos.nu/&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2011-05-27T16:14:16.013" Id="11314" LastActivityDate="2011-05-27T16:14:16.013" OwnerUserId="930" ParentId="11290" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="11323" AnswerCount="4" Body="&lt;p&gt;So when I assume that the error terms are normally distributed in a linear regression, what does it mean for the response variable, $y$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-27T16:14:56.817" FavoriteCount="3" Id="11315" LastActivityDate="2011-05-28T23:34:21.947" LastEditDate="2011-05-27T18:37:16.203" LastEditorUserId="930" OwnerUserId="4496" PostTypeId="1" Score="9" Tags="&lt;regression&gt;&lt;distributions&gt;" Title="How does the distribution of the error term affect the distribution of the response?" ViewCount="2745" />
  
  
  
  
  
  
  <row AcceptedAnswerId="11382" AnswerCount="1" Body="&lt;p&gt;I've calculated the proportion of chicks fledged out of the number of eggs hatched in each year using &lt;code&gt;prop.test()&lt;/code&gt; in R. I see that it gives me the proportion fledged, but also the 95% confidence interval, which is what I'm after. &#10;Having read the excellent information from another question on this site &lt;a href=&quot;http://stats.stackexchange.com/questions/4713/binomial-confidence-interval-estimation-why-is-it-not-symmetric&quot;&gt;here&lt;/a&gt;, I understand why I don't have symmetry in my 95% CIs! &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;However, how should I report this in a paper? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I've seen people report values as 38% (±0.2%), with an indication that the value in brackets is 95% CI. Obviously this won't work for asymmetrical CIs. Must I report the upper and lower values in these cases?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-31T00:43:26.973" FavoriteCount="1" Id="11381" LastActivityDate="2011-10-06T00:52:53.107" LastEditDate="2011-10-06T00:52:53.107" LastEditorUserId="183" OwnerUserId="4238" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;binomial&gt;" Title="How to report asymmetrical confidence intervals of a proportion?" ViewCount="1212" />
  
  <row Body="&lt;p&gt;I would not use any feature detectors but recurrent neural networks. They are very good for symbolic sequences: for example, they are able to recognize context sensitive languages.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check out &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.5339&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;Biologically Phoneme Classification with LSTM neural nets (Graves, Schmidhuber)&lt;/a&gt; for an explanation of how to use RNNs for classification. See &lt;a href=&quot;http://www.google.de/url?sa=t&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CCIQFjAA&amp;amp;url=http://www.icml-2011.org/papers/524_icmlpaper.pdf&amp;amp;ei=b6_kTcXmLMPPsga8wrSABg&amp;amp;usg=AFQjCNHGCGk9fYM9owhkFX6VahYzLadLAw&amp;amp;sig2=isZS8pi_fMoRm99Q4tzGmw&quot; rel=&quot;nofollow&quot;&gt;Generating Text with Recurrent Neural Networks (Martens, Sutskever, Hinton)&lt;/a&gt; for an impressive symbolic application of RNNs.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-31T09:06:53.763" Id="11388" LastActivityDate="2011-05-31T09:06:53.763" OwnerUserId="2860" ParentId="11387" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Kernel methods (such as the support vector machine) are likely to be quite good for this kind of problem as you can use kernel functions that operate directly on strings of symbols of variable length.  Examples include the &lt;a href=&quot;http://psb.stanford.edu/psb-online/proceedings/psb02/leslie.pdf&quot; rel=&quot;nofollow&quot;&gt;spectrum kernel&lt;/a&gt; (which projects the strings into an implicit feature space where each dimension records the number of ocurrences of all possible substrings of a given length - or less) and the &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.9441&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;mismatch kernel&lt;/a&gt;, which is similar, but the counting of sub-strings allows a certain amount of mis-matches.  There is also the &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/19/15/1964.short&quot; rel=&quot;nofollow&quot;&gt;sequence  alignment&lt;/a&gt; kernel,  which might be of interest.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-31T15:16:16.697" Id="11400" LastActivityDate="2011-05-31T15:16:16.697" OwnerUserId="887" ParentId="11387" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Regardless of the graph's coloring, associated with each of its nodes is a pair $(k, k_{nn})$.  You have used these pairs to classify the nodes into two groups.  The coloring separately classifies the nodes by color.  This is the situation of a $c$ by $2$ contingency table with fixed margins.  To assess whether color is associated with node group, use an appropriate test of association: &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher's Exact Test&lt;/a&gt; if the counts are not too large; otherwise a chi-squared test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-31T15:20:53.820" Id="11401" LastActivityDate="2011-05-31T15:20:53.820" OwnerUserId="919" ParentId="11398" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose $x_{1}, x_{2} \dots x_{N}$ are gaussian RVs with variance $S$ and mean $1$.  What is the density function of&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{ |\sum_{n=1}^{N}x_{n}|^{2}}{\sum_{n=1}^{N}|x_{n}|^{2}}\text{?}$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-06-01T16:39:02.153" FavoriteCount="1" Id="11435" LastActivityDate="2011-06-04T06:29:01.780" LastEditDate="2011-06-01T17:01:41.037" LastEditorUserId="919" OwnerUserId="99" PostTypeId="1" Score="2" Tags="&lt;density-function&gt;" Title="Density function question" ViewCount="102" />
  
  
  
  <row Body="&lt;p&gt;The appropriate analysis would depend on whether you have multiple participants or just one participant and the number of blocks that you have.&#10;In general, with more blocks, you can more precisely characterise the functional relationship between practice and performance.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Small number of blocks (e.g., 3 to perhaps 10)&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Linear and quadratic contrasts as part of a repeated measures ANOVA would provide a basic test of the improvement in d-prime with practice. Or you could implement the same model within a mixed-model framework.&#10;Another simple option would be to compare the first one or two blocks with the last one or two blocks either using a repeated measures t-test or using a contrast with appropriate weights as part of the repeated measure ANOVA.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Many blocks (e.g., perhaps 15 or 30 or more)&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You could start to characterise the change in d-prime with practice more precisely perhaps with some non-linear functions. &#10;Practice effects are usually more rapid at the start of practice and monotonically decelerate and approach an asymptote.&#10;Thus, non-linear regression per participant or non-linear multilevel modelling for a more integrated approach represent two major options.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Distribution of residuals for d-prime&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you had concerns about the distribution of residuals of d-prime then you could consider a transformation; I'm not sure what's standard practice in signal detection research, but at a guess I thought d-primes might be relatively normal without transformation.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-06-02T06:23:07.207" Id="11469" LastActivityDate="2011-06-02T08:24:09.440" LastEditDate="2011-06-02T08:24:09.440" LastEditorUserId="183" OwnerUserId="183" ParentId="11448" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This is merely a comment but it won't fit as such, so I'll leave it as a CW response.&lt;/p&gt;&#10;&#10;&lt;p&gt;Everything is already available in Frank Harrell's &lt;a href=&quot;http://cran.r-project.org/web/packages/rms/index.html&quot; rel=&quot;nofollow&quot;&gt;rms&lt;/a&gt; package (which model to choose, how to evaluate its predictive performance or how to validate it, how not to fall into the trap of overfitting or stepwise approach, etc.), with formal discussion in his textbook, &lt;em&gt;Regression Modeling Strategies&lt;/em&gt; (Springer, 2001), and a nice set of handouts on his &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/Main/RmS&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I would recommend the following papers if you're interested in predictive modeling:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Aliferis, C.F., Statnikov, A., Tsamardinos, I., Schildcrout, J.S., Shepherd, B.E., and Harrell, F.E. Jr (2009). &lt;a href=&quot;http://www.plosone.org/article/info%3adoi/10.1371/journal.pone.0004922&quot; rel=&quot;nofollow&quot;&gt;Factors Influencing the Statistical Power of Complex Data Analysis Protocols for Molecular Signature Development from Microarray Data&lt;/a&gt;. &lt;em&gt;PLoS ONE 4(3)&lt;/em&gt;: e4922.&lt;/li&gt;&#10;&lt;li&gt;Harrell, F.E. Jr, Margolis, P.A., Gove, S., Mason, K.E., Mulholland, E.K., Lehmann, D., Muhe, L., Gatchalian, S., and Eichenwald, H.F.. (1998). &lt;a href=&quot;http://www.lcc.uma.es/~jja/recidiva/064.pdf&quot; rel=&quot;nofollow&quot;&gt;Development of a clinical prediction model for an ordinal outcome: the World Health Organization Multicentre Study of Clinical Signs and Etiological agents of Pneumonia, Sepsis and Meningitis in Young Infants. WHO/ARI Young Infant Multicentre Study Group&lt;/a&gt;. &lt;em&gt;Statistics in Medicine&lt;/em&gt;, 17(8): 909-44.&lt;/li&gt;&#10;&lt;li&gt;Harrell, F.E. Jr, Lee, K.L., and Mark, D.B. (1996). &lt;a href=&quot;http://www.google.fr/url?sa=t&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CCUQFjAA&amp;amp;url=http://www.unt.edu/rss/class/Jon/MiscDocs/Harrell_1996.pdf&amp;amp;ei=CUe5TcbIFM2v8QPa87lA&amp;amp;usg=AFQjCNHuArhbPMUIgFFKkTE2eqqKLEodyg&amp;amp;sig2=zxnYgUuvIRj3zdAu0w-T5Q&quot; rel=&quot;nofollow&quot;&gt;Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors&lt;/a&gt;. &lt;em&gt;Statistics in Medicine&lt;/em&gt;, 15(4): 361-87.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-06-02T11:49:17.280" CreationDate="2011-06-02T11:23:20.110" Id="11479" LastActivityDate="2011-06-02T11:49:17.280" LastEditDate="2011-06-02T11:49:17.280" LastEditorUserId="930" OwnerUserId="930" ParentId="11457" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I really appreciate the pointers to my book and papers and R package.  Briefly, stepwise regression is invalid as it destroys all statistical properties of the result as well as faring poorly in predictive accuracy.  There is no reason to use ROC curves to guide model selection (if model selection is even a good idea), because we have the optimum measure, the log-likelihood and its variants such as AIC.  Thresholds for the dependent variable should be dealt with using ordinal regression instead of making a series of binary models.  The Hosmer-Lemeshow test is now considered obsolete by many statisticians as well as the original authors.  See the reference below (which proposes a better method, implemented in the rms package).&lt;/p&gt;&#10;&#10;&lt;p&gt;@ARTICLE{hos97com,&#10;  author = {Hosmer, D. W. and Hosmer, T. and {le Cessie}, S. and Lemeshow, S.},&#10;  year = 1997,&#10;  title = {A comparison of goodness-of-fit tests for the logistic regression&#10;          model},&#10;  journal = Statistics in Medicine,&#10;  volume = 16,&#10;  pages = {965-980},&#10;  annote = {goodness-of-fit for binary logistic model;difficulty with&#10;           Hosmer-Lemeshow statistic being dependent on how groups are&#10;           defined;sum of squares test (see cop89unw);cumulative sum test;invalidity of naive&#10;           test based on deviance;goodness-of-link function;simulation setup;see sta09sim}&#10;}&lt;/p&gt;&#10;&#10;&lt;p&gt;See also&lt;/p&gt;&#10;&#10;&lt;p&gt;@Article{sal09sim,&#10;  author =       {Stallard, Nigel},&#10;  title =        {Simple tests for the external validation of mortality prediction scores},&#10;  journal =      Statistics in Medicine,&#10;  year =         2009,&#10;  volume =   28,&#10;  pages =    {377-388},&#10;  annote =   {low power of older Hosmer-Lemeshow test;avoiding grouping of predicted risks;logarithmic and quadratic test;scaled $\chi^2$ approximation;simulation setup; best power seems to be for the logarithmic (deviance) statistic and for the chi-square statistics that is like the sum of squared errors statistic except that each observation is weighted by $p(1-p)$}&#10;}&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-06-02T12:41:01.310" Id="11482" LastActivityDate="2011-06-02T12:41:01.310" OwnerUserId="4253" ParentId="11457" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;As @IrishStat said it depends on the model. One way of recovering constant is to use the mean value of the residuals. Note that this method relies strongly on certain assumptions. Here is the illustration. Suppose your model is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_t=\alpha + \beta X_t + \varepsilon_t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(\varepsilon_t|X_t)=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and you estimate it by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Delta Y_t=\beta \Delta X_t+\Delta \varepsilon_t.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose your estimate $\hat\beta$ is unbiased (or at least consistent). Define&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{e}_t=Y_t-\hat\beta X_t.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Substituting the true model we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{e}_t=\alpha+\varepsilon_t+(\beta-\hat\beta)X_t,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;hence&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(\hat{e}_t|X_t)=\alpha,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;if $\hat\beta$ is unbiased or  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{T}\sum_{t=1}^T\hat{e}_t\to \alpha,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;if $\hat\beta$ is consistent.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the natural estimate for $\alpha$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\alpha}=\frac{1}{T}\sum_{t=1}^T(Y_t-\hat\beta X_t).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the model assumption is critical here. However with care this trick can be applied in general.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-02T13:05:09.017" Id="11484" LastActivityDate="2011-06-02T13:05:09.017" OwnerUserId="2116" ParentId="11478" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;No, since for linear regression log likelihood is a sum of squared residuals plus some other terms, log likelihood is scale dependent. So for the same model multiplying the regressors by some constant will change log likelihood but R squared will remain the same.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-02T16:13:21.030" Id="11491" LastActivityDate="2011-06-02T16:13:21.030" OwnerUserId="2116" ParentId="11490" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;You say that you need to solve an ordinary least squares problem on 2400 variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two assumptions that I think you need to revisit:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt;: that you need to compute the inverse of $X^TX$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Assumption 2&lt;/strong&gt;: that solving ordinary least squares on 2400 variables requires specialized methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll examine them in turn:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Assumption 1: that you need to compute the inverse of $X^TX$.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A better way to solve OLS using normal equations is by computing the Cholesky factorization of $X^TX$. See section 5.3.2 of &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0801854148&quot; rel=&quot;nofollow&quot;&gt;Golub and van Loan&lt;/a&gt; for details. They state that the entire algorithm, including computing $X^TX$, $X^Ty$ as well as performing the Cholesky factorization and back-substitution, requires $(m+n/3)n^2$ floating-point operations.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Assumption 2: that solving ordinary least squares on 2400 variables requires specialized methods.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't say what kind of hardware you have at your disposal, so I'll assume that you have access to a typical mainstream PC.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, a 2400x2400 of 64-bit floats requires just 44MB of memory.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, computing Cholesky decomposition of a matrix of this size takes half a second on my desktop PC using Numerical Python (&lt;a href=&quot;http://numpy.scipy.org/&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/a&gt;). This is the dominant computation once you have computed $X^TX$ and $X^Ty$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-02T16:56:47.763" Id="11493" LastActivityDate="2011-06-02T19:06:27.027" LastEditDate="2011-06-02T19:06:27.027" LastEditorUserId="439" OwnerUserId="439" ParentId="11487" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Fully data-driven model selection will result in standard errors and P-values that are too small, confidence intervals that are too narrow, and overstated effects of remaining terms in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;For time effects I usually model using restricted cubic splines.  A detailed case study in the context of generalized least squares for correlated serial data may be found at &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/RmS&quot; rel=&quot;nofollow&quot;&gt;http://biostat.mc.vanderbilt.edu/RmS&lt;/a&gt; - see the two attachments at the bottom named course2.pdf and rms.pdf.  This uses the R rms package.  The case study contains information about the choice of basis functions for the time component.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-06-03T12:18:18.957" Id="11514" LastActivityDate="2011-06-03T12:18:18.957" OwnerUserId="4253" ParentId="11498" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="11520" AnswerCount="1" Body="&lt;p&gt;I am doing Cox regression models and KM plots for a data set where the end point is death.  In addition there is information about whether the death was cancer-specific or not.  So I have three categories:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;No death, last seen is the date of censor&lt;/li&gt;&#10;&lt;li&gt;Death - cancer-specific&lt;/li&gt;&#10;&lt;li&gt;Death - other cause&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What I would like to know is what to do with category 3 data when I am looking at cancer-specific survival as my endpoint.  Do I censor that data at the date of death OR do I just remove that data from the dataset? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-03T14:04:30.007" FavoriteCount="1" Id="11516" LastActivityDate="2011-06-04T10:52:30.077" LastEditDate="2011-06-04T10:52:30.077" LastEditorUserId="88" OwnerUserId="1150" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;cox-model&gt;" Title="Shall I censor or rather remove other causes in cause-specific survival analysis?" ViewCount="201" />
  
  <row Body="&lt;h3&gt;Solution&lt;/h3&gt;&#10;&#10;&lt;p&gt;When you assume the residuals (vertical deviations in a graph of $n$ data) are independently and identically distributed with some normal distribution of zero mean, the estimate of the slope will have a Student t distribution with $n-2$ degrees of freedom, scaled by the standard error.  Because the theoretical value has essentially zero error, we can ignore this complication and treat the theoretical value as a constant.  Therefore we refer the ratio&lt;/p&gt;&#10;&#10;&lt;p&gt;$$t = (0.0106623 - 0.0075) / 0.0011 = 2.88$$&lt;/p&gt;&#10;&#10;&lt;p&gt;to &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution#Table_of_selected_values&quot; rel=&quot;nofollow&quot;&gt;Student's t distribution&lt;/a&gt; (as a &lt;em&gt;two sided test,&lt;/em&gt; because in principle the slope could have been greater or less than the theoretical value and you just want to see whether the difference could be attributed to chance).&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether this deviation is &quot;significant&quot; depends on your criterion for significance and on the degrees of freedom.  For example, if you want 95% or greater significance, then this difference will be significant if and only if you have six or more data values.  This conclusion follows from noting that the 95% two-sided critical value with $5-2 = 3$ degrees of freedom is $3.182$, greater than $2.88$, and the critical value with $6-2 = 4$ d.f. is $2.776$, less than $2.88$.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Discussion&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;If&lt;/em&gt; the uncertainty in the theoretical value were appreciable compared to the standard error of the slope ($0.0011$) &lt;em&gt;and&lt;/em&gt; you had relatively few data points (perhaps 10 or fewer), the problem would become more difficult:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;First, you don't know the distribution of the theoretical error.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Second, you probably don't know for sure that it is a standard error (people often report confidence limits or two or three standard errors or even standard deviations without clearly specifying what they have computed).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Third, the sum of a t-distributed value (your error) and another distribution (the theoretical error) can have a mathematically less tractable distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Mitigating these complications, though, is a simple consideration: if the theoretical uncertainty were largish, then it would add to the overall uncertainty in the difference between the theoretical and estimated values, thereby lowering the t-statistic.  In some cases such a semi-quantitative result might be good enough.  (The addition is in terms of variances: you sum the squares of the two standard errors, obtaining the square of the standard error of the difference, and (therefore) take its square root.)&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, if the theoretical uncertainty were equal to the uncertainty of the estimate, the t-statistic would be reduced to $2.03$.  The distribution of the difference would be approximately Normal, but with slightly longer tails, so referring the value of $2.03$ to a standard Normal distribution would slightly overestimate the significance.  Well, we can compute that $4.2\%$ of the standard Normal distribution is more extreme than $\pm 2.03$.  Thus--still in this hypothetical situation with a largish standard error for the theoretical result--you would &lt;em&gt;not&lt;/em&gt; conclude the difference is significant if your criterion for significance exceeds $100 - 4.2 = 95.8\%$.  Otherwise, the picture is murky and the determination depends on the resolution of the difficulties enumerated above.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-03T14:42:22.673" Id="11518" LastActivityDate="2011-06-03T14:42:22.673" OwnerUserId="919" ParentId="11481" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;You can't interpret the $p$-values. The long-tailed errors you're describing often act to underestimate the standard errors, making your $p$-values too small (not to mention that $\hat{\beta}$ isn't normally distributed in finite samples). I suggest a non-parametric bootstrap so you can characterize the sampling distribution of your coefficient estimates without making an unwarranted assumptions about the error distribution. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-03T16:57:22.990" Id="11529" LastActivityDate="2012-05-26T04:07:59.580" LastEditDate="2012-05-26T04:07:59.580" LastEditorUserId="4856" OwnerUserId="4856" ParentId="11508" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;I am looking at timeseries data in foreign exchange and bond markets (to test for reversion on extreme moves). Unfortunate &quot;tick&quot; data, namely high frequency data, is prone to many problems, and they obviously can significantly mess with the analysis. I'd like to know which R library can help with the following type of fairly frequent data cleaning problems:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) one spike:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GCGja.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is typically created when one market maker prints a wrong quote in one tick, but there would have been no tradability at that price because it lasted for a split second. I'd like to eliminate the spike (but only if there is only one (or maybe 2) prints)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) bid ask gapping:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bAN4d.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case the market is fairly illiquid and the data algorithm is jumping between bids and asks (in this case 2bps wide) causing this weird cloud. &lt;/p&gt;&#10;&#10;&lt;p&gt;Where should I start to clean this stuff, obviously trying to throw out the least amount of real data. I realise that the maxim of &quot;look at the data&quot; applies here, but when you're looking at 1000 series each with 100 days of data, you can see how this will become quickly impractical so I need some automated help. I'll also look at Python language methods if they're available or better.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-03T19:02:33.940" FavoriteCount="5" Id="11531" LastActivityDate="2011-06-07T23:25:43.797" LastEditDate="2011-06-03T19:07:57.977" LastEditorUserId="4705" OwnerUserId="4705" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;finance&gt;" Title="High frequency data series cleaning in R" ViewCount="1724" />
  
  <row Body="&lt;p&gt;Unless I missed something, you want to convert your data.frame into a suitable time-indexed series of measurement. In this case, you can use the &lt;a href=&quot;http://cran.r-project.org/web/packages/zoo/index.html&quot; rel=&quot;nofollow&quot;&gt;zoo&lt;/a&gt; package as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(zoo)&#10;&amp;gt; memdata.ts &amp;lt;- with(memdata, zoo(vsize, date))&#10;&amp;gt; str(memdata.ts)&#10;‘zoo’ series from 2011-04-22 to 2011-04-30&#10;  Data: Factor w/ 9 levels &quot;3535.178&quot;,&quot;4403.515&quot;,..: 1 9 8 4 2 3 5 6 7&#10;  Index:  Factor w/ 9 levels &quot;2011-04-22&quot;,&quot;2011-04-23&quot;,..: 1 2 3 4 5 6 7 8 9&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-06-03T19:55:36.677" Id="11534" LastActivityDate="2011-06-03T19:55:36.677" OwnerUserId="930" ParentId="11532" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;This is a question of testing if several proportions are equal and equal to a specific value. This is quite standard, and you can do this by a likelihood-ratio test or a &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test&quot; rel=&quot;nofollow&quot;&gt;$\chi^2$-test&lt;/a&gt;. In R, the $\chi^2$-test can be computed using &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/prop.test.html&quot; rel=&quot;nofollow&quot;&gt;prop.test&lt;/a&gt;, and you can specify that you want the vector of proportions to be equal to the vector $(0.5, \ldots, 0.5)$. The computations are, however, not complicated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-03T21:41:23.837" Id="11538" LastActivityDate="2011-06-03T21:41:23.837" OwnerUserId="4376" ParentId="11537" PostTypeId="2" Score="4" />
  
  
  
  <row AcceptedAnswerId="11599" AnswerCount="3" Body="&lt;p&gt;I have one data sample of non-negative random variable $X$ with unknown distribution and predefined expected value $y$. Is there any test able to check null hypothesis $\mathbb{E}[X]\geq y$ or $\mathbb{E}[X]\leq y$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Actual data samples are gathered in realtime. More specifically, it's an intervals between HTTP-requests coming to web-server from one client. Pearson's test shown that it is not normally distributed variable.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-04T17:29:53.320" Id="11568" LastActivityDate="2011-06-06T17:19:51.677" LastEditDate="2011-06-05T11:36:40.143" LastEditorUserId="4883" OwnerUserId="4883" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;expected-value&gt;" Title="How to check hypothesis about estimation of random variable with unknown distribution?" ViewCount="152" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of images of same color cars which I have users rate on a scale from 1-5 (integers only) based on how attractive they think the car design is. For each image I have a set of parameters about the cars in question, mostly various ratios of dimensions (say height at middle, width at trunk, curviness of hood, etc). I first give the user a set of training images and have them rate them. I would like to use this training set to predict future ratings by a particular user based on the ratios of the car in question. While a number prediction would be great (ie a prediction of what the user will rate the car), I am also willing to settle on a predicting of whether the rating is above or below say, 3. I don't really have a background in stats, but I think this has something to do with logistic regressions and discrete choice? I was wondering what a good reference for this would be.&lt;/p&gt;&#10;&#10;&lt;p&gt;To add a little more info, there are no strong individual correlations amongst the ratios and the users rating. Moreover, a linear regression is out of the question because there isn't a simple relationship between changing a particular ratio and the attractiveness. Moreover, I also do not want to intoduce too many ratios because then they become correlated with each other (say, height/width and height/length can give length/width upon division). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-04T20:27:53.267" Id="11573" LastActivityDate="2011-06-07T20:09:17.800" LastEditDate="2011-06-04T21:53:09.463" LastEditorUserId="88" OwnerUserId="4886" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;multivariate-analysis&gt;" Title="Discrete choice prediction" ViewCount="236" />
  
  
  
  <row Body="&lt;p&gt;This is a simple, but not so simple concept to understand.  I personally find using the example of a table to illustrate what is going on.  So we have a $2\times 2$ table with counts in each cell.  To keep from the &quot;abstract&quot; nature of the concept, I'll use real numbers instead of letters.  So we have a table of &quot;being sick&quot; against &quot;having a sore throat&quot;:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{c|c}
  
  
  <row Body="&lt;p&gt;A few simple points not directly addressing your question about offsets:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I'd have a look at whether number of games is correlated with mean goals scored. In many elite goal scoring sports that I can think of (e.g., soccer, Australian rules football, etc.) I would predict that longevity of a career is related to the success of a career. And at least for players in goal scoring roles, success is related to number of goals scored.&#10;If this is true, then number of games would capture two effects. One would relate to the mere fact that more games played means more opportunities to score goals; and the other would capture skill-related effects.&#10;You could examine the relationship between number of games and mean goals scored (e.g., goals / number of games) to explore this. I think this has substantive implications for any modelling that you do.&lt;/li&gt;&#10;&lt;li&gt;My instincts are to convert the dependent variable into mean goals per game. I realise that you would have more precise measurement of a player's skill for those who played more games, so maybe that would be an issue. Depending on the precision in your model that you desire, and the resulting distribution of player means, you might be able to rely on standard linear modelling techniques. But perhaps this is a bit too applied for your purposes, and perhaps you have reasons for wanting to model total goals scored.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2011-06-05T15:14:42.277" Id="11598" LastActivityDate="2011-06-05T15:14:42.277" OwnerUserId="183" ParentId="11595" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The problem with allowing &lt;em&gt;any&lt;/em&gt; distribution is that it could have a tiny chance of yielding a huge value.  That eliminates any possibility of testing the mean with satisfactory confidence.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the details.  Choose a unit of measurement in which $y$ is hugely greater than $1$. Let $\alpha$ be the desired significance for the hypothesis test ($0 \lt \alpha \lt 1$) and $n$ be the sample size.   Choose any $p$ for which $0 \lt p \lt 1 - \alpha^{1/n}$ and define $\mu = 1 + y/p$.  Consider the two-point distribution for which $1$ has probability $1-p$ and $\mu$ has probability $p$.  The chance that a sample of size $n$ from this distribution consists entirely of $1$s is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(1-p)^n \gt (\alpha^{1/n})^n = \alpha,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;yet its expectation is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$1(1-p) + \mu (p) = (1-p) + (1 + y/p)p = y+1 \gt y.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Because $y$ can be made arbitrarily large compared to $1$, no hypothesis test of any positive power will conclude that the true mean exceeds $y$ when $n$ $1$s are observed.  Therefore the test will &lt;em&gt;fail&lt;/em&gt; to detect that the mean exceeds $y$ with probability greater than $\alpha$ when this two-point distribution is the true distribution.  Because this analysis places no restrictions on $\alpha$ or $n$, this proves that &lt;strong&gt;no test with positive power, with any amount of sampling, can achieve &lt;em&gt;any&lt;/em&gt; positive level of significance&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-05T15:18:15.483" Id="11599" LastActivityDate="2011-06-05T15:18:15.483" OwnerUserId="919" ParentId="11568" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Simply build an ARIMA MODEL that separate signal from noise incorporating any identifiable deterministic structure such as changes in levels/trends/seaonal pulses/parameter or variance change over time. Develop a prediction for the next 5 days and use the uncertainty in that sum to create possible bounds. Compare the actual sum of the &quot;new five readings&quot; and compute the pobability of yielding a value as &quot;high&quot; or as diverse as this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-05T17:20:43.337" Id="11604" LastActivityDate="2011-06-06T18:39:55.273" LastEditDate="2011-06-06T18:39:55.273" LastEditorUserId="3382" OwnerUserId="3382" ParentId="11548" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The reason that the confidence interval doesn't specify &quot;the probability that the true parameter lies in the interval&quot; is because once the interval is specified, the paramater either lies in it or it doesn't. However, for a 95% confidence interval for example, you have a 95% chance of creating a confidence interval that does contain the value. This is a pretty difficult concept to grasp, so I may not be articulating it well. See &lt;a href=&quot;http://frank.itlab.us/datamodel/node39.html&quot; rel=&quot;nofollow&quot;&gt;http://frank.itlab.us/datamodel/node39.html&lt;/a&gt; for further clarification. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-06-06T03:21:50.793" Id="11613" LastActivityDate="2011-06-06T03:21:50.793" OwnerUserId="4897" ParentId="11609" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Intervals between things like requests are often modeled well with exponential, gamma, and Weibull distributions. These can have pretty fat tails, so @whuber's concern is already accounted for, to some extent, when you calculate your confidence intervals. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-06T17:19:51.677" Id="11638" LastActivityDate="2011-06-06T17:19:51.677" OwnerUserId="4862" ParentId="11568" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Using notions like entropy (like in Ashok's answer) only work if you believe the message is coming from a specific distribution. If all you have a single message, then the only measure of complexity that's meaningful is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov_complexity&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov complexity&lt;/a&gt; of the message, which is sadly uncomputable. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-07T04:18:42.860" Id="11654" LastActivityDate="2011-06-07T04:18:42.860" OwnerUserId="139" ParentId="11594" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;The KL divergence is a difference of integrals of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  <row Body="&lt;p&gt;I'm not sure about the metric of the ICC itself (I have never seen anyone report this metric for inferential purposes, only for description), but I do not believe many modelling strategies will be greatly impacted by the instance of many small groups. This is because random effects modelling takes into account the sample size of the groups, by &quot;shrinking&quot; the estimated group variances by their sample sizes. As a note when I refer to fixed or random effects, it is in line with the definitions layed out &lt;a href=&quot;http://www.stata.com/support/faqs/stat/xtreg.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to assess this is to examine the outcome of interest as deviations from the group means as oppossed to the original metric. So if $y_{ij}$ is the variable $y$ for observation $i$ within group $j$, from this variable subtract the mean of group $j$, and graph a scatterplot of those deviations versus the group size. You would expect this scatterplot to show heteroscedasticity (as the mean of smaller group sizes should be less representative of the observations within the group), and have a wider variance for smaller groups. If the opposite occurs, this suggests that the smaller groups are more homogenous, and might be evidence that the independence of observations is violated and is directly related to group size (e.g. those in smaller groups tend to be more similar to each other than those in larger groups).&lt;/p&gt;&#10;&#10;&lt;p&gt;If anything small groups should inflate the ICC. When there is only 1 observation, all of the variance for that observation is attributed to the group level mean, and if all groups only had 1 observation, the ICC would be 1 (i.e. there is no within group variation, only between groups).&lt;/p&gt;&#10;&#10;&lt;p&gt;Also as a note, frequently to estimate some relationship it is not that the observations need to be independent, it is simply that the model residuals need to be independent. Hence the whole reason to fit multi-level models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Stephen Raudenbush has a book chapter, &lt;a href=&quot;http://psycnet.apa.org/psycinfo/2008-02337-004&quot; rel=&quot;nofollow&quot;&gt;Many Small Groups&lt;/a&gt; that may be of interest (I see a PDF of the whole book can be found &lt;a href=&quot;http://gen.lib.rus.ec/get?md5=3fb54f61e667475a9cbad9e9c6c13ec6&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). The chapter is mainly about how to estimate models with many small groups and potential problems that can arise. This is really only pertinent though if you want to estimate random effects models. If you are simply interested in fixed effects models it is largely unproblematic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I have found the tutorials developed by the &lt;a href=&quot;http://www.bristol.ac.uk/cmm/learning/&quot; rel=&quot;nofollow&quot;&gt;Centre for Multilevel Modelling&lt;/a&gt; to be very useful introductions to the subject material (very gentle, especially compared to the Raudenbush chapter I just cited!)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-07T16:24:48.803" Id="11671" LastActivityDate="2011-06-07T16:24:48.803" OwnerUserId="1036" ParentId="11616" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;@whuber makes great suggestions; I would only add this: Plots, plots, plots, plots. Scatterplots, histograms, boxplots, lineplots, heatmaps and anything else you can think of. Of course, as you've found there are errors that won't be apparent on any plots but they're a good place to start. Just make sure you're clear on how your software handles missing data, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on the context you can get creative. One thing I like to do With multivariate data is fit some kind of factor model/probabilistic PCA (something that will do multiple imputation for missing data) and look at scores for as many components as possible. Data points which score highly on the less important components/factors are often outliers you might not see otherwise. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-07T17:04:52.570" Id="11673" LastActivityDate="2011-06-07T17:04:52.570" OwnerUserId="26" ParentId="11659" PostTypeId="2" Score="20" />
  <row Body="&lt;p&gt;Jeromy Anglim and IrishStat both give great answers, but they sound maybe a little more complex than what you're looking for.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;A simpler method could could be to perform a linear regression on your data, to get &lt;code&gt;PageViews = a * Date + b&lt;/code&gt; for some constants &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;; the constant &lt;code&gt;a&lt;/code&gt; is then a measure of the linear &quot;slope&quot; of your data, which you could use to measure how much the link is trending. However, this might not work so well if your data doesn't follow a linear trend (the example in your other link looks pretty linear, but you could imagine that your link has instead been growing exponentially lately).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;So another approach could be to convert your pageviews into ranks (e.g., in article 1, 100 is the lowest value, so convert that into a 1; 80 is the 2nd-lowest value, so convert that into a 2; 60 is the highest value, so convert that into a 3), and then take the correlation of these ranks with &lt;code&gt;(1,2,...,n)&lt;/code&gt; (where &lt;code&gt;n&lt;/code&gt; is the total number of dates you have).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For example, if your article behaves like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Date, PageViews, Rank&#10;June 1, 100, 1&#10;June 2, 120, 3&#10;June 3, 115, 2&#10;June 4, 125, 4&#10;June 5, 150, 5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then you would take the correlation between &lt;code&gt;(1,3,2,4,5)&lt;/code&gt; and &lt;code&gt;(1,2,3,4,5)&lt;/code&gt; to get a trending score of 0.9. (Note that under this method, though, pageviews of &lt;code&gt;(100, 120, 115, 125, 150)&lt;/code&gt; have the same trending score as &lt;code&gt;(100, 300, 299, 7000, 35000)&lt;/code&gt;, which may or may not be what you want, since the latter is growing faster. In other words, this method tells you how strong the &lt;em&gt;direction&lt;/em&gt; of the trend is, but not the magnitude. If you do want to get a sense of the magnitude, then you could just repeat these methods on the day-by-day changes of pageviews, i.e., determine whether the day-by-day changes are trending upwards or downwards.)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-07T17:31:27.397" Id="11675" LastActivityDate="2011-06-07T17:31:27.397" OwnerUserId="1106" ParentId="11548" PostTypeId="2" Score="1" />
  
  
&#10;\frac{6}{(n+1)(2n+1)} &amp;amp; \mbox{ if } \omega = \omega_0 \mbox{ and } \omega&amp;#39; = \omega_0\\
  <row Body="&lt;p&gt;There isn't really a minimum number of observations.  Essentially the more observations you have the more the parameters of your model are constrained by the data, and the more confident the model becomes.  How many observations you need depends on the nature of the problem and how confident you need to be in your model.  I don't think it is a good idea to rely too much on &quot;rules of thumb&quot; about this sort of thing, but use the all the data you can get and inspect the confidence/credible intervals on your model parameters and on predictions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-08T19:10:58.603" Id="11731" LastActivityDate="2011-06-08T19:10:58.603" OwnerUserId="887" ParentId="11724" PostTypeId="2" Score="7" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;A treatment was given to one hand of a subject, and a single outcome metric is measured for both hands, twice pre and several times post treatment. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is best practice for assessing effectiveness of treatment?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Treated and Untreated &quot;groups&quot; really are paired.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-08T23:58:58.097" Id="11739" LastActivityDate="2011-06-09T02:02:48.033" LastEditDate="2011-06-09T02:02:48.033" LastEditorUserId="183" OwnerUserId="4944" PostTypeId="1" Score="3" Tags="&lt;repeated-measures&gt;&lt;clinical-trials&gt;" Title="Pre and Post, treated and un treated but from same subject" ViewCount="106" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like simulate appearance of publications in a forum and I need know what is the probability distribution of new question being asked in a forum. In my first simulation I used to normal distribution, but I think that the best distribution can be exponential distribution.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-09T09:05:26.687" Id="11749" LastActivityDate="2011-06-10T16:21:24.647" LastEditDate="2011-06-10T06:46:57.370" LastEditorUserId="2116" OwnerUserId="4953" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;probability&gt;" Title="Probability distribution of questions in a forum" ViewCount="103" />
  <row AcceptedAnswerId="11757" AnswerCount="2" Body="&lt;p&gt;How do you estimate degrees of freedoms for derived measurements?&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to assess the significance of the distance of an independent data point to a regression line. I can easily calculate the (vertical) distance between the data point and the regression line, and I get the uncertainty of the distance from the uncertainties of slope and intercept of the linear regression via Gaussian error propagation. However, what are the degrees of freedom?&lt;/p&gt;&#10;&#10;&lt;p&gt;The linear regression line has been calculated from &lt;code&gt;n&lt;/code&gt; data points, thus its degrees of freedom is &lt;code&gt;n-2&lt;/code&gt;. The additional measurement is independent, so I get another degree of freedom, bringing the total to &lt;code&gt;n-1&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, should I estimate the uncertainty of the independent measurement using the variance of the residuals of the regression, since the measurement process is the same for both the data that went into the fit and the independent data point? I guess this would reduce the degrees of freedom again?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-09T11:55:47.333" FavoriteCount="1" Id="11753" LastActivityDate="2011-06-09T13:16:40.683" LastEditDate="2011-06-09T12:18:20.830" LastEditorUserId="198" OwnerUserId="198" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;degrees-of-freedom&gt;" Title="Distance to a regression line, and degrees of freedom" ViewCount="420" />
  
  
  <row AnswerCount="4" Body="&lt;p&gt;Neural networks are often treated as &quot;black boxes&quot; due to their complex structure. This is not ideal, as it is often beneficial to have an intuitive grasp of how a model is working internally. What are methods of visualizing how a trained neural network is working? Alternatively, how can we extract easily digestible descriptions of the network (e.g. this hidden node is primarily working with these inputs)? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am primarily interested in two layer feed-forward networks, but would also like to hear solutions for deeper networks. The input data can either be visual or non-visual in nature. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-09T17:19:19.360" FavoriteCount="6" Id="11764" LastActivityDate="2012-05-09T15:03:50.533" OwnerUserId="2965" PostTypeId="1" Score="22" Tags="&lt;data-visualization&gt;&lt;neural-networks&gt;" Title="How to visualize/understand what a neural network is doing?" ViewCount="2077" />
  
  
  
  
  
  <row AcceptedAnswerId="11824" AnswerCount="2" Body="&lt;p&gt;I have tested a regression framework's robustness to noise and I have noticed in some cases that adding noise improves the prediction performance and in other cases the performance degrades.&lt;/p&gt;&#10;&#10;&lt;p&gt;What could be the reasons for this? If there are multiple reasons, how to I determine which is the cause?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;Some more details about what I am doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The framework uses ridge regression. The inputs are vectors of extracted image features. The outputs are vectors of angles (in degrees, -180 to 180). To test for robustness to noise I am applying 3 levels of noise (white additive Gaussian noise) to the angles (targets) proportional to the individual angle variances (2%, 5%, and 10% of the variance of each angle).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed that in some observations, adding a small amount of noise (2-5%) leads to a small improvement in performance and in one case, all levels of noise give improvement. In my tests, the regularisation term is fixed across all noise levels, and I have ran each noise level test several times to take into account the fluctuations of the random noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I have two broad sets of observation data. The first set was observed relatively accurately, however the second set was more complex (significantly more heterogeneous, leading to notable performance degradation relative to the first set) and exhibited a number of minor errors due to the observation technique being more limited than that which was used with the first set.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the first set, the phenomena of better performance through adding noise did not occur. However, sometimes more noise was better than less noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;If more information is required to better answer the question, I'd be happy to provide it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-11T11:51:39.733" Id="11823" LastActivityDate="2013-02-09T07:22:06.400" LastEditDate="2011-06-11T14:16:58.987" LastEditorUserId="3052" OwnerUserId="3052" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;white-noise&gt;" Title="Why does noisy data result in better prediction performance?" ViewCount="549" />
  
  
  <row AnswerCount="7" Body="&lt;p&gt;I teach an introductory economic geography course. To help my students develop a better understanding of the kinds of countries found in the contemporary world economy and an appreciation of data reduction techniques, I want to construct an assignment that creates a typology of different kinds of countries (e.g, high-income high-value added mfg long life expectancy; high income natural resource exporter mid-high life expectancy; with Germany being an element of the first type, and Yemen an example of the second type). This would use publicly available UNDP data (which if I recall correctly contains socioeconomic data on a bit less than 200 countries; sorry no regional data are available).&lt;/p&gt;&#10;&#10;&lt;p&gt;Prior to this assignment would be another which asks them (using the same --- largely interval or ratio level --- data) to examine correlations between these same variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;My hope is that they would first develop an intuition for the kinds of relationships between different variables (e.g., a positive relationship between life expectancy and [various indicators of] wealth; a positive relationship between wealth and export diversity). Then, when using the data reduction technique, the components or factors would make some intuitive sense (e.g., factor / component 1 captures the importance of wealth; factor / component 2 captures the importance of education).&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that these are second to fourth year students, often with limited exposure to analytical thinking more generally, what single data reduction technique would you suggest as most appropriate for the second assignment? These are population data, so inferential statistics (p-vlaues, etc.) are not really necessary. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-11T17:37:52.547" FavoriteCount="2" Id="11833" LastActivityDate="2011-12-06T13:53:19.163" LastEditDate="2011-12-05T21:13:34.090" LastEditorUserId="930" OwnerUserId="4980" PostTypeId="1" Score="10" Tags="&lt;pca&gt;&lt;factor-analysis&gt;&lt;dimensionality-reduction&gt;" Title="Data reduction technique to identify types of countries" ViewCount="480" />
  
  
  <row Body="&lt;p&gt;I don't think the Fligner-Killeen test (nor the Brown-Forsythe) test is appropriate since you don't know the median in the published data (if you do have it and simply didn't mention it then never mind).&lt;/p&gt;&#10;&#10;&lt;p&gt;I wouldn't suggest simulation of the data either unless you're sure the samples follow a specific distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you don't have the median and the distribution is uncertain &lt;a href=&quot;http://en.wikipedia.org/wiki/Levene%27s_test&quot; rel=&quot;nofollow&quot;&gt;Levene's Test&lt;/a&gt; would be appropriate. I've never ran the test in R before, but there is a description of it &lt;a href=&quot;http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/car/html/levene.test.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. If you're having a lot of trouble getting the R code to work though I'd just compute it by hand given the summary statistics from the literature and your own data. As the wikipedia indicates that statistic is F distributed so &lt;a href=&quot;http://www.statsoft.com/textbook/distribution-tables/#f&quot; rel=&quot;nofollow&quot;&gt;you'll need a table&lt;/a&gt; if you don't have one.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-11T21:17:49.697" Id="11838" LastActivityDate="2011-06-11T21:40:33.070" LastEditDate="2011-06-11T21:40:33.070" LastEditorUserId="4325" OwnerUserId="4325" ParentId="11788" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One usually estimates probabilities with &lt;em&gt;frequencies&lt;/em&gt;: &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_interpretations&quot; rel=&quot;nofollow&quot;&gt;according to Laplace&lt;/a&gt; (1814),&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The ratio of this number [of &quot;favorable cases&quot;] to that of all the cases possible is the measure of this probability...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is justified by an &lt;em&gt;urn model&lt;/em&gt; (or &quot;tickets in a box&quot; model) of probability: print the text on paper, cut out each word, and drop them into a box.  Imagine creating &quot;sentences&quot; by randomly drawing one piece of paper from the box, writing the word seen on it, returning the paper to the box (to leave its contents unchanged), and repeating.  The number of &quot;favorable cases&quot; for any word is the number of slips of paper on which it is written.  The number of &quot;all cases possible&quot; is the total number of slips of paper in the box.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, to compute $p_i$, you count two things: $n$, the number of words in a text, and $n_i$, the number of words that match word $i$.  Then $p_i = n_i/n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in the sentence&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I once had a girl; or should I say, she once had me.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;there are $n=13$ words.  We would compute $p_{\text{once}} = 2/13$, $p_{\text{she}} = 1/13$, $p_{\text{boy}} = 0/13$, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;With this model we can compute the probability of a &quot;co-occurrence.&quot;  This (in your situation) is the chance that word $i$ is followed by word $j$ in a random sequence of &lt;em&gt;three&lt;/em&gt; words drawn as described.&lt;/p&gt;&#10;&#10;&lt;p&gt;Continuing the example, let's compute the probability of co-occurrence of &quot;she had.&quot;  This can be done with a probability tree:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The chance that the first word is &quot;she&quot; equals $1/13$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Conditional on the first word being &quot;she&quot;, there is a $2/13$ chance that the second word is &quot;had&quot;.  Thus there is a $1/13 \cdot 2/13$ chance of &quot;she had ...&quot;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Conditional on the first word being &quot;she&quot; and the second &lt;em&gt;not&lt;/em&gt; being &quot;had&quot;, there is a $2/13$ chance that the third word is &quot;had&quot;.  Because the chance of the second word not being &quot;had&quot; is $11/13$, this conditional probability equals $1/13 \cdot 11/13 \cdot 2/13$.  It is the chance of &quot;she ... had&quot; where &quot;...&quot; is &lt;em&gt;not&lt;/em&gt; &quot;had.&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Conditional on the first word &lt;em&gt;not&lt;/em&gt; being &quot;she&quot;, the chance that the second is &quot;she&quot; and the third is &quot;had&quot; equals $1/13 \cdot 2/13$.  Because the chance of the first word not being &quot;she&quot; is $12/13$, the conditional chance equals $12/13 \cdot 1/13 \cdot 2/13$.  It is the chance of &quot;... she had&quot; where &quot;...&quot; is &lt;em&gt;not&lt;/em&gt; &quot;she.&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;These three conditional events are mutually exclusive, allowing us to add their chances.  Whence the chance of co-occurrence of &quot;she had&quot; is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p_{ij} = 1/13 \cdot 2/13 + 1/13 \cdot 11/13 \cdot 2/13 + 12/13 \cdot 1/13 \cdot 2/13 = 72/2197 = 0.032772.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that $p_i \cdot p_j = 1/13 \cdot 2/13 = 2/169 = 26/2197 = 0.0118343.$  In particular, it is definitely &lt;em&gt;not&lt;/em&gt; the case that $p_{ij} = p_i \cdot p_j$ under this probability model.  Moreover, the ratio $p_{ij} / (p_i p_j) = 36/13$ is none of the intuitively &quot;obvious&quot; values $1$, $3$, or $4$ (it is slightly less than $3$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Comparing a co-occurrence probability to frequencies within an actual text is challenging because the $n-2$ sequences of three words that do appear in the text are &lt;em&gt;not independent.&lt;/em&gt;  For instance, consider co-occurrences of &quot;once had&quot; in the preceding example.  The initial sequence &quot;I once had&quot; is a co-occurrence.  It &lt;em&gt;guarantees&lt;/em&gt; that the second sequence, &quot;once had a&quot; also is a co-occurrence.  However, it lowers the chances that the third sequence is a co-occurrence, because the third sequence must begin with &quot;had,&quot; making it impossible to begin with &quot;once.&quot;  People often address this by computing &lt;em&gt;expected values&lt;/em&gt; of the frequencies.  Returning to the probability tree calculation, we find the expected number of co-occurrences of &quot;she had&quot; in a random sequence of three words, counting &quot;she she had&quot; as just &lt;em&gt;one&lt;/em&gt; co-occurrence, is $72/2179$.  Therefore the expected number of such co-occurrences in a sentence of 13 words, which contains 11 such sequences, equals $11 \cdot 72/2179 = 792/2179 = 0.36$.  That does not depart significantly from the observed number, $1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What these considerations teach us is that &lt;em&gt;any&lt;/em&gt; research that applies probability to co-occurrence networks needs to be clear and specific about (a) how probability is being applied: that is, what probability model is used; and (b) how co-occurrences will be identified and their frequencies computed.  It is evident, though, that the formula $p_{ij} = p_i \cdot p_j$ (for independently drawn words $i$ and $j$) is unlikely to be even approximately true when a &quot;co-occurrence&quot; can include one intermediate word between $i$ and $j$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-12T00:19:51.727" Id="11841" LastActivityDate="2011-06-12T00:19:51.727" OwnerUserId="919" ParentId="11797" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Thanks for the clarification. I agree with @Greg Snow that any transformation should make sense in the context of the problem. Why are you considering a log transform? Have you tried standardizing your predictors?&lt;/p&gt;&#10;&#10;&lt;p&gt;You want to keep in mind how the transformation changes the assumptions in your model. I'll use $\beta = (\beta_2, \dots, \beta_p)&amp;#39;$ and $X = (X_2, \dots, X_p)$. Your two models are&lt;/p&gt;&#10;&#10;&lt;p&gt;Log transform model: $E(Y|X_1,X) = \exp(\tilde\beta_1\log(X_1) + X\beta) = X_1^{\tilde\beta_1}\exp(X\beta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Original model: $E(Y|X_1, X) = \exp(\beta_1X_1 + X\beta)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;For convenience I've overloaded $\beta$ slightly, in that their estimates would obviously be different under each model (in general).&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple way to compare the two models is through their relative risk. Suppose we have two observations $y_i, y_j$ with the same covariate values except that $x_{i1} - 1 = x_{j1}$ ($x_{i1}$ is one unit greater than $x_{j1}$). The relative risk $RR=E(Y|X_1=x_{i1},X)/E(Y|X_1=x_{i1}-1,X)$ is then the multiplicative change in the rate caused by increasing $x_1$ by one unit. The $RR$ is given by&lt;/p&gt;&#10;&#10;&lt;p&gt;Log transform: $RR = \left(\frac{x_{i1}}{x_{i1} - 1}\right)^{\tilde\beta_1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Original model: $RR = \exp(\beta_1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;RR under the log transform varies over the range of $x_{i1}$ (unless of course $\tilde \beta_1 = 0$). Does that make sense in your problem? In the original model the effect of a unit change in $x_1$ doesn't vary with its magnitude (i.e. increasing $X_1$ one unit has the same effect on the rate whether we move from 4 to 5, or 0 to 1, or 100 to 101, etc). Does that make sense in your problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;The coefficient in the log transformed model is harder to interpret, so unless there is a good reason for the transformation I would pass. &lt;/p&gt;&#10;&#10;&lt;p&gt;You didn't say by what criterion the results are getting better, so it's hard to know for sure than any improvement in fit is &quot;real&quot;. But even if it is, it might just be an indication that a Poisson regression is inappropriate. In particular the log transform removes the implicit proportional hazards assumption in the original model. Unfortunately it does so in a very rigid way, so while the overall fit might improve that doesn't necessarily mean you have a good model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;A couple of points re: your comments. Your reference gives another way to interpret the coefficients via partial derivatives. Here, to compare the two models above we would look at $\frac{dE(Y|X)}{dx_1}$. So let's do that:&lt;/p&gt;&#10;&#10;&lt;p&gt;Log transform: $\frac{dE(Y|X_1,X)}{dx_1} = \frac{\tilde\beta_1}{x_1}\exp(\tilde\beta_1\log(x_1)+X\beta) = \tilde\beta_1x_1^{\tilde\beta-1}\exp(X\beta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Original: $\frac{dE(Y|X_1,X)}{dx_1} = \beta_1\exp(\beta x_1+X\beta) = \beta_1\exp(\beta_1 x_1)\exp(X\beta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, these are different models: compare the terms $\tilde\beta_1x_1^{\tilde\beta-1}$ and $\beta_1\exp(\beta_1 x_1)$. You can't interpret the log transformed model in the same way as the original model. However, you &lt;em&gt;could&lt;/em&gt; apply that interpretation to $\log(X_1)$; the question is whether or not that's meaningful/reasonable/etc (a percent/unit change on the log scale is very different, obviously). (basically @Greg Snow's original point). If the only reason for the transformation is to reduce the excess variance or improve the residuals then I would look at other aspects of the model first. &lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of decreasing the Pearson residuals: This isn't always a plus. You may be overfitting the data for one, and for another my original point applies - the log transformed predictor might be compensating for a misspecified model , perhaps in a less-than-obvious way. What are the sample mean and variance of $Y$ - are the data over/underdispersed? Have you considered another model, a negative binomial regression for example?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-06-12T17:03:27.230" Id="11851" LastActivityDate="2011-06-12T19:58:56.200" LastEditDate="2011-06-12T19:58:56.200" LastEditorUserId="26" OwnerUserId="26" ParentId="11829" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;My Interpretation: If you conduct the experiment N times ( where N tends to infinity) then out of these large number of experiments 95% of the experiments will have confidence intervals which lie within these 95% limits. More clearly, lets say those limits are &quot;a&quot; and &quot;b&quot; then 95 out of 100 times your sample mean difference will lie between &quot;a&quot; and &quot;b&quot;.I assume that you understand that different experiment can have different samples to cover out of the whole population.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2011-06-13T05:16:57.427" Id="11858" LastActivityDate="2011-06-13T05:16:57.427" OwnerUserId="1763" ParentId="11856" PostTypeId="2" Score="-2" />
  
  <row AcceptedAnswerId="11880" AnswerCount="2" Body="&lt;p&gt;The &quot;&lt;a href=&quot;http://www.springer.com/statistics/computanional+statistics/book/978-0-387-79053-4&quot; rel=&quot;nofollow&quot;&gt;Introductory Statistics with R&lt;/a&gt;&quot; book contains a section that deals with correlations (section 6.4 in the second edition). The book shows Pearson, Spearman and Kendall correlation coefficients computed on the &lt;code&gt;blood.glucose&lt;/code&gt; and &lt;code&gt;short.velocity&lt;/code&gt; columns of the &lt;a href=&quot;http://www.oga-lab.net/RGM2/func.php?rd_id=ISwR%3athuesen&quot; rel=&quot;nofollow&quot;&gt;thuesen&lt;/a&gt; data set. The p-values associated with these coefficients are 0.048, 0.139 and 0.119, correspondingly. The book then says the following:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Notice that neither of the two nonparametric correlations is significant at the 5% level, which the Pearson correlation is, albeit only borderline significant.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I have several problems with this paragraph. &lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, my naive guess would be that since the non-parametric coefficients do not imply linearity, they will tend to be &quot;significant&quot; more frequently than Pearson's r. Am I right?&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, and more importantly, is such a comparison between p-values of different tests applied on the same data legit? (I'm talking about real-life comparisons and not about trivial examples in a text book) If it is, how one need to interpret the notion that linear correlation is &quot;significant&quot;, while rank or concordance correlation isn't?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-13T13:31:51.697" Id="11871" LastActivityDate="2011-06-13T18:36:35.977" OwnerUserId="1496" PostTypeId="1" Score="5" Tags="&lt;correlation&gt;&lt;statistical-significance&gt;&lt;books&gt;&lt;theory&gt;" Title="Interpreting p-values associated with correlation measurements" ViewCount="442" />
  <row Body="&lt;p&gt;There are several things to keep in mind here. Lasso for linear regression is optimization of a quadratic function with an $\ell_1$-norm penalty term. The latter is non-smooth, and one can put the problem into the context of quadratic optimization with linear constraints. General purpose solvers turn out to be less than optimal for this particular problem, which has special properties. The coordinate descent algorithm relies on these properties for convergence, see e.g. &lt;a href=&quot;http://www.springerlink.com/content/36132n1667120303/&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; for the concept of separability. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because the optimization is optimization of a non-smooth function the methods that rely on smoothness, such as Quasi-Newton, are not appropriate. For the linear regression problem the coordinate descent algorithm is particularly fast because there are several tricks that save computations. However, there are other fast algorithms like &lt;a href=&quot;http://www.stanford.edu/~hastie/Papers/LARS/&quot; rel=&quot;nofollow&quot;&gt;lars&lt;/a&gt; aimed directly at the regression problem. In the comparisons I have seen, the implementation of coordinate descent in the R package &lt;code&gt;glmnet&lt;/code&gt; is faster than the implementation in the &lt;code&gt;lars&lt;/code&gt; package, but the former is also really optimized Fortran code. &lt;/p&gt;&#10;&#10;&lt;p&gt;For more general $\ell_1$-penalized optimization problems I have found the following to be important for speed:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Warm starts. For a decreasing sequence of penalty parameter values ($\lambda$'s) we  use the parameter from a previous $\lambda$-value as start guess for the next $\lambda$-value. &lt;/li&gt;&#10;&lt;li&gt;The coordinate wise update splits into two parts. First we check if the coordinate should stay zero with minimal computations (happens a lot, no extra computations if that is the case). If not, find non-zero update of the coordinate.  &lt;/li&gt;&#10;&lt;li&gt;Sparseness is preserved. The algorithm does not only produce a result with many zeroes but keeps many coordinates at zero for the entire algorithm. Whether this matters for speed depends on the problem, but often sparseness can be exploited in the other computations. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Whether the non-zero update of one coordinate at a time is best for a particular problem will depend on the problem. It works very well for the linear regression problem and  generalized linear models because each update is the minimization of a simple quadratic function, which is very fast. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-14T06:47:25.070" Id="11891" LastActivityDate="2011-06-14T06:47:25.070" OwnerUserId="4376" ParentId="11888" PostTypeId="2" Score="2" />
  
&#10;0 &amp;amp; 0 &amp;amp; \sigma_{3}\end{array}\right]\right)=\prod_{i=1}^{3}\ p(X_{i};\mu_{i},\sigma_{i}).$$&#10;The sample complexity of a univariate distribution is less than that of a full-covariance $M$-variate normal distribution, and thus fewer observations are required to get a good approximation of this model, since only univariate distributions must be estimated.&lt;/p&gt;&#10;&#10;&lt;p&gt;With zero-mean observations $\{x^{(1)},\cdots,x^{(N)}\}$, the covariance MLE for the case where &lt;em&gt;the covariance matrix is isotropic (i.e. all observation components -- denoted with subscripts -- are pairwise independent)&lt;/em&gt; is simply $\hat{\Sigma}=\mathrm{diag}(\hat{\sigma}_{1,}\hat{\sigma}_{2},\cdots,\hat{\sigma}_{M})$ where $\hat{\sigma}_{i}=\frac{1}{N}\sum_{j=1}^{N}\left(x_{i}^{(j)}\right)^{2}$. This pairwise independence is represented as zeros in the &lt;em&gt;precision matrix&lt;/em&gt; $\Sigma^{-1}$, which is diagonal when $\Sigma$ is diagonal. As mentioned above, this is identical to (i.e. gives the same result as) separately estimating $M$ univariate distributions, and taking their product.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: In the &lt;em&gt;in between&lt;/em&gt; case, where there is some (not fully connected) dependency graph $\mathcal{G}$ between observation components, what does the covariance MLE look like then? As a specific example, if we say that $X_{1},X_{2}\perp X_{3}$, then the precision matrix must always have the form $$\Sigma^{-1} = \left[\begin{array}{ccc}
&#10;0 &amp;amp; 0 &amp;amp; \lambda_{33}\end{array}\right].$$ &#10;Therefore, what would the decomposition, and the covariance (or inverse covariance) MLE look like for a normal distribution such as the following? :&#10;$$ p\left(X_{1},X_{2},X_{3};[\mu_{1},\mu_{2},\mu_{3}]^{T},\left[\begin{array}{ccc}
  
  
  
  
  <row Body="&lt;p&gt;The problem may stem from the fitness ratio you use in your code. The inverse of the distances may give very large differences in the resampling probabilities of the new generation of the samples. For example, let &lt;code&gt;x&lt;/code&gt; stand for the &lt;code&gt;rms&lt;/code&gt; values in your design and let them be&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x = [0.1 1 1 3 10 50]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When you use the inverses, that leads to &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = (1./x) ./ norm(1./x)&#10;y = &#10;    0.9896    0.0990    0.0990    0.0330    0.0099    0.0020&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you see, the first sample dominates the next generation. It may converge to wrong proposals very easily. Alternatively, you may use a sigmoid-like function. For example,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;z = exp(-x) ./ norm(exp(-x))&#10;z = &#10;    0.8659    0.3521    0.3521    0.0476    0.0000    0.0000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, the particles in the next generation will probably involve some samples similar to the second and third ones. So, the generation scheme will be more robust to erronous proposals.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Moreover, after dividing by the norms, the sum of the result is not 1. You might use &lt;code&gt;sum&lt;/code&gt; instead.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = (1./x) ./ sum(1./x)&#10;y = &#10;    0.8030    0.0803    0.0803    0.0268    0.0080    0.0016&#10;&#10;z = exp(-x) ./ sum(exp(-x))&#10;z =&#10;    0.5353    0.2176    0.2176    0.0295    0.0000    0.0000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-06-15T12:53:37.533" Id="11938" LastActivityDate="2011-06-15T13:00:42.117" LastEditDate="2011-06-15T13:00:42.117" LastEditorUserId="5025" OwnerUserId="5025" ParentId="8399" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;I like the Predict.Plot and TkPredict functions in the TeachingDemos package for R, but my opinion may be slightly biased.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example (the TkPredict function allows you to dynamically change values to see how they compare):&#10;&lt;img src=&quot;http://i.stack.imgur.com/XOXZ0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-16T00:14:12.240" Id="11962" LastActivityDate="2011-06-16T00:14:12.240" OwnerUserId="4505" ParentId="11953" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Your approach is fine and appropriate.  The power of mixed effects models is that they can be used to analyze data like yours or where treatments were at the individual level, or both.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-16T04:24:22.030" Id="11966" LastActivityDate="2011-06-16T04:24:22.030" OwnerUserId="4505" ParentId="11164" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Data transformation: rescale your data to lie in $[0,1]$ and model it using a glm model with a logit link.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: When you re-scale a vector (ie divide all the elements by  the largest entry), as a rule, before you do that, screen (eyeballs) for outliers. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming you have access to R, i would carry the modeling part with a &lt;em&gt;robust&lt;/em&gt; glm routine, see $\verb+glmrob()+$ in package $\verb+robustbase+.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-06-16T13:35:19.083" Id="11986" LastActivityDate="2011-06-18T08:21:01.550" LastEditDate="2011-06-18T08:21:01.550" LastEditorUserId="603" OwnerUserId="603" ParentId="11985" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Your intuition is correct, the same principles apply.  I looked in Pinheiro/Bates section 5.4, where &lt;code&gt;gls&lt;/code&gt; is introduced, but it doesn't say so explicitly, so you'll just have to trust me, I guess. :)  &lt;/p&gt;&#10;&#10;&lt;p&gt;In Chapter 2 they go through the theory of REML and ML and you'll notice that none of the theory depends on there being any random effects, and that actually, you could write any random effect model using just correlation structure instead and fit with gls, though for complex random effects it would be quite complex.  The simplest example is that a random intercept model is equivalent to a compound symmetry model.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-16T15:26:56.980" Id="11993" LastActivityDate="2011-06-16T15:26:56.980" OwnerUserId="3601" ParentId="11987" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;You should read &lt;a href=&quot;http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html&quot; rel=&quot;nofollow&quot;&gt;Spreadsheet Adiction&lt;/a&gt; and the links from that page before trusting any results from Excel.&lt;/p&gt;&#10;&#10;&lt;p&gt;From your question it appears that you don't have a firm grasp on what confidence intervals and prediction intervals are.  You should really consult a good intro stats book, and/or take a class or meet with a consultant to get these concepts down.  But here is a short explanation:&lt;/p&gt;&#10;&#10;&lt;p&gt;The condifence interval is a statement about where we believe the true population parameter (the mean above) to be based on the sample data.  So not knowing the population mean does not mean that you cannot do a confidence interval.  If your sample is large and you are willing to assume that the population is not overly skewed or would produce outliers, then the Central Limit Theorem says that a confidence interval on the mean based on the assumption of a normal population will be a good approximation even if the population is not normal.  So you can use normal based theory without knowing if the population is normal as long as you are willing to make the above assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The prediction interval is a statement about where we expect future individual data points to be.  This prediction will depend much more on the shape of the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The big difference in concept is whether you are talking about the mean of all future data, or individual data points (I could not tell which you are interested in from the question).&lt;/p&gt;&#10;&#10;&lt;p&gt;The norminv function in Excel does not fit a normal distribution, but gives the x-value for a given area under the curve (probability) for a normal with the specified mean and standard deviation.  That function could be used as part of the computations to get either of the intervals, but that assumes that you know the population standard deviation, if you are using the sample standard deviation then it is more appropriate to use the t distribution rather than the normal.  Also note that the prediction interval takes into account the uncertainty in you estimate of the mean and standard deviation in addition to the randomness of the individual data points, so norminv probably is not what you want.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-06-16T20:55:11.027" Id="12011" LastActivityDate="2011-06-16T20:55:11.027" OwnerUserId="4505" ParentId="12007" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I just read: &lt;a href=&quot;http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the example from the blog post:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let’s make up a little story: let’s say we have three types of wine (A, B and C), and we would like to know which one is the best one (in a scale of 1 to 7). We asked 22 friends to taste each of the three wines (in a blind fold fashion), and then to give a grade of 1 till 7 (for example sake, let’s say we asked them to rate the wines 5 times each, and then averaged their results to give a number for a persons preference for each wine. This number which is now an average of several numbers, will not necessarily be an integer).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Why let them rate the wine &quot;5 times each&quot;? This is just an arbitrary number. More importantly, how do you know that &quot;5&quot; is enough? How should you define &quot;enough&quot;? Is &quot;4&quot; or &quot;2&quot; also enough? Are there methods to quantify how good a sample size is?&lt;/p&gt;&#10;&#10;&lt;p&gt;For my personal problem, I have to test with datapoints, where the mean is around 100 and the standard deviation is 300. This is averaged over 500 samples, but is this enough, for such a huge variance?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-17T08:57:09.077" FavoriteCount="1" Id="12023" LastActivityDate="2011-06-18T14:29:18.473" LastEditDate="2011-06-17T11:24:47.397" LastEditorUserId="5058" OwnerUserId="5058" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;sample-size&gt;" Title="How many samples is enough?" ViewCount="718" />
  <row Body="&lt;p&gt;The most related technique I know of is described in a &lt;a href=&quot;http://fora.tv/2009/10/14/ACM_Data_Mining_SIG_Ted_Dunning&quot; rel=&quot;nofollow&quot;&gt;talk at ACM Data Mining SIG by Ted Dunning&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-17T10:15:41.077" Id="12027" LastActivityDate="2011-06-17T10:15:41.077" OwnerUserId="2150" ParentId="12026" PostTypeId="2" Score="2" />
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $N_0$ is the start concentration of AB, $\lambda=\ln(2)/t_{1/2}$ and $t$ is time. For example when $N_0=1000$ and $t_{1/2}=2$ we get as below, which when log transformed these form a nice straight line on which real data can easily be fit, see left hand images &lt;a href=&quot;https://spreadsheets.google.com/spreadsheet/pub?hl=en_GB&amp;amp;hl=en_GB&amp;amp;key=0ApdtrT02Tv8sdGpRQmROZFhIOE1vMU1xQ1hGODllNkE&amp;amp;single=true&amp;amp;gid=0&amp;amp;output=html&quot; rel=&quot;nofollow&quot;&gt;in this Google spreadsheet&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, because I am measuring B, and the parent start concentration is unknown (probably much bigger than B), I am getting a graph like (top-right graph in link), which is a mirror of the decay curve of AB, starting at the start concentration of B (known, and in this example 100) and increasing by the decay of AB: ($N_0 - N(t)$) at each $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, log transforming this does not produce a straight line, because the concentration is not halving or doubling, but increasing by an amount to do with the unknown start concentration of AB, producing the not very useful graph (bottom-right graph in link).&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I am finding it hard to fit a line to my data. I have tried converting concentration of B into something that looks like concentration of AB which can then be converted easily into a straight line by log transforming, but had no luck as I don't know $N_0$ for AB.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am hoping I've missed something and there is a nice way transform my data to a straight line, or model it some other way. Any help and ideas would be very much appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&#10;Nick&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-17T13:33:55.553" Id="12032" LastActivityDate="2011-06-17T18:16:37.890" LastEditDate="2011-06-17T18:16:37.890" LastEditorUserId="88" OwnerUserId="5061" PostTypeId="1" Score="2" Tags="&lt;modeling&gt;&lt;exponential&gt;" Title="Fitting child molecule concentration in parent molecule exponential decay" ViewCount="88" />
  <row Body="&lt;p&gt;Kruskal-Wallis' test is a non parametric one way anova.  While Friedman's test can be thought of as a (non parametric) repeated measure one way anova.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't understand the difference, I compiled a list of tutorials I found about doing repeated measure anova with R, you can find them &lt;a href=&quot;http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/&quot;&gt;here&lt;/a&gt;...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-17T16:08:34.800" Id="12037" LastActivityDate="2011-06-17T16:08:34.800" OwnerUserId="253" ParentId="12030" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;You basically need to differentiate the sigmoid to get some bell-like curve; this is how it looks like for standard sigmoid $1/(1+e^{-t})$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/TE5f3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then it will be only a peak finding. You can do this either analytically from the fit, or calculate the differences from the data and look for a peak there.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-18T10:02:02.043" Id="12064" LastActivityDate="2011-06-18T10:02:02.043" OwnerUserId="88" ParentId="12060" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="12130" AnswerCount="4" Body="&lt;p&gt;I am using libsvm (which is meant for solving binary classification problems) for multi-class classification. How can I get classification scores / confidences for each class to effectively compare them given that libsvm can only produces scores for two classes. Desired output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Class 1: score1&#10;&#10;Class 2: score2&#10;&#10;Class 3: score3&#10;&#10;Class 4: score4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2011-06-18T12:28:31.430" Id="12068" LastActivityDate="2011-07-07T18:24:41.640" LastEditDate="2011-06-20T07:09:25.333" LastEditorUserId="264" OwnerUserId="4317" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;" Title="Classification score: SVM" ViewCount="2615" />
  
  
  
  
  
  <row AcceptedAnswerId="12121" AnswerCount="1" Body="&lt;p&gt;Why $z=f(x)$ does not imply $E[z]=f(E[x])$ when f is not linear?&lt;/p&gt;&#10;&#10;&lt;p&gt;I can give an example, but I couldn't derive the general form.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $z = {x^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $g(x)$ be the pdf of $x$. Then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ E\left[ z \right] = E\left[ {{x^2}} \right] = \int\limits_{ - \infty }^\infty  {\left( {{x^2}} \right)g\left( x \right)dx} $&lt;/p&gt;&#10;&#10;&lt;p&gt;$f\left( {E\left[ x \right]} \right) = {\left( {\int\limits_{ - \infty }^\infty  {\left( x \right)g\left( x \right)dx} } \right)^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \Rightarrow E\left[ z \right] \ne f\left( {E\left[ x \right]} \right)$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-19T17:44:50.197" Id="12103" LastActivityDate="2011-06-20T09:30:31.577" LastEditDate="2011-06-20T06:27:22.157" LastEditorUserId="2116" OwnerUserId="4898" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;expected-value&gt;" Title="Why z=f(x) does not imply E[z]=f(E[x]) when f is not linear?" ViewCount="104" />
  <row AcceptedAnswerId="12108" AnswerCount="1" Body="&lt;p&gt;I'm reading an article, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/19925207&quot; rel=&quot;nofollow&quot;&gt;The commonality of neural networks for verbal and visual short-term memory&lt;/a&gt; (Majerus et al., J Cogn Neurosci 2010 22(11): 2570), about brain imaging in which the results are analysed with multiple analyses.&#10;One of them is a null &lt;em&gt;conjunction analysis&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried googling a bit but I didn't come up with something useful.&#10;I did find some results on 'conjunction analysis', but I'm not sure if that's the exact same thing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain to me what this analysis is (Goals, method...)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-19T19:48:20.773" Id="12107" LastActivityDate="2011-06-19T21:44:05.837" LastEditDate="2011-06-19T21:35:55.297" LastEditorUserId="930" OwnerUserId="3140" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;neuroimaging&gt;" Title="What is a null conjunction analysis in an fMRI study?" ViewCount="2742" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm trying to interpret a significant three-way interaction. Basically, I've used hierarchical regression to analyse my data, and I have come up with a significant three-way interaction.&lt;/p&gt;&#10;&#10;&lt;p&gt;My DV is continuous. My 3 IVs are continuous, categorical (2 levels), and another categorical (3 levels). My sample size is 194.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that I can do up graphs to eyeball the interactions, but I need a statistical method in order to figure out whether or not a slope is significant. I'm aware of &lt;a href=&quot;http://www.jeremydawson.co.uk/slopes.htm&quot; rel=&quot;nofollow&quot;&gt;Jeremy Dawson's template&lt;/a&gt; to figure out significant slope differences, but they only work for 3 continuous variables. Is there a method I can use to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've also had a read through the UCLA's SPSS guide to &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/faq/threeway_hand.htm&quot; rel=&quot;nofollow&quot;&gt;interpreting three-way interactions&lt;/a&gt;. Would this be the way to go?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any help you can provide.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-20T10:09:58.720" Id="12124" LastActivityDate="2012-03-27T13:59:42.490" LastEditDate="2011-06-20T10:35:45.807" LastEditorUserId="930" OwnerUserId="3998" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;interaction&gt;" Title="Dissecting three-way interactions" ViewCount="3107" />
  <row Body="&lt;p&gt;I am a little confused by your question, since you say that you have already found a significant 3 way interaction and then say you want to find whether slope differences are significant, but I think you want to see which of the levels are different in terms of their slopes, is that right?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know SPSS, but in SAS you can request particular tests of different hypotheses.  In SAS you can do this with EFFECT statements.  You can also do this inside a LSMEANS statement.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But I would shy away from these statements; first, they usually have low power (unless all your variables are perfectly measured and perfectly reliable).  Second, significance just isn't that significant.  Effect size is more important.  Third, the graphs say more (especially in interaction interpretation) than any p-value could.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To quote my favorite professor in grad school &quot;When an article is full of significance tests, the authors are p-ing all over the research&quot;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-20T10:33:12.957" Id="12126" LastActivityDate="2011-06-20T10:33:12.957" OwnerUserId="686" ParentId="12124" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to estimate the distribution of a very large population of known size but unknown mean and variance. I cannot assume anything about the shape of the underlying distribution (although I am relatively certain that it is not normal). However, I am certain that the values in the population are non-negative, non-zero integers. I believe that the distribution is naturally lumped in some way discretely rather than being continuously distributed over the entire range. I cannot sample the entire population but I would like to estimate the probability density function. I would also like some level of assurance of the correctness of the estimated distribution. What is the best way to go about this?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-20T16:58:38.517" FavoriteCount="0" Id="12134" LastActivityDate="2011-06-20T18:44:04.457" LastEditDate="2011-06-20T17:19:06.420" LastEditorUserId="88" OwnerUserId="5095" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;computational-statistics&gt;" Title="Estimating the distribution of a very large population of known size and unknown variance" ViewCount="101" />
  <row Body="&lt;p&gt;As in ocram's answer,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \Pr(\textrm{params} \mid \textrm{data}) = \frac{\Pr(\textrm{data} \mid \textrm{params}) \Pr(\textrm{params})}{\Pr(\textrm{data})} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually while applying Bayes' rule, we wish to infer 'params' and the 'data' is already given.Thus, $\Pr(\textrm{data})$ is a constant and we can assume that it is just a normalizing factor.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-20T18:04:29.430" Id="12135" LastActivityDate="2011-06-20T18:04:29.430" OwnerUserId="2072" ParentId="12112" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Since the &lt;a href=&quot;http://en.wikipedia.org/wiki/Empirical_distribution_function&quot; rel=&quot;nofollow&quot;&gt;empirical distribution function of the sample&lt;/a&gt; should converge to the true population distribution function as the sample size approaches the population size, it seems to me that best estimate of the population pdf is the empirical pdf of the largest sample you an afford to draw. &lt;/p&gt;&#10;&#10;&lt;p&gt;Depending upon what the sample pdf looks like, you could consider summarizing the distribution with some functional form, particularly if you have some ideas about what generated the population.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-20T18:27:21.577" Id="12136" LastActivityDate="2011-06-20T18:44:04.457" LastEditDate="2011-06-20T18:44:04.457" LastEditorUserId="82" OwnerUserId="82" ParentId="12134" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="12289" AnswerCount="1" Body="&lt;p&gt;Say I have n sets of data where each set of data has a binomial distribution. The n sets of data are then combined into one set. &#10;If I take the combined set of data, how would I go about detecting that this set is made up from n individual sets ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have only a limited background in statistics, so here is an example to clarify the question:&#10;I collect the amount of energy used by an appliance at 3pm each day for a number of months&#10;Generally the amount of energy used follows these rules:&#10;- On Monday to Friday, the appliance uses 3000 kWh (mean value)&#10;- On Saturday, the appliance uses 200 kWh (mean value)&#10;- On Sunday, the appliance uses 50 kWh (mean value)&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to be able to take all the 3pm readings every day for a year and then be able to detect that the readings can be broken into the 3 sets as described above.  After I deduce that there are 3 sets of data, I plan to calculate the mean value with std dev etc. for each individual set.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-06-20T21:17:43.987" FavoriteCount="0" Id="12138" LastActivityDate="2011-06-23T17:36:35.253" LastEditDate="2011-06-21T06:51:21.003" LastEditorUserId="930" OwnerUserId="5091" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;" Title="How to detect a number of binomial distributions from a set of data?" ViewCount="174" />
  
  <row Body="&lt;p&gt;First off, I agree.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect that you can create a different sort of graph; you're not using a lot of the two-dimensionality of the current display because everything is clustered about the x=y line. Try plotting the pressure along the x axis and the ratios along the y axis. If this is too messy, try taking the difference in pressure. You could also use some measure of effect size, like Cohen's d, but then viewers would have to know what that is. You can probably come up with something better than what I suggested, but my suggestion might help you think of other approaches. As you'll read below, my approach might mislead viewers because it would make pressure look like an independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would help to know what sort of story you're telling from this graph. My interpretation is that the ratios are independent variables and the pressure is a dependent variable. The change that I suggested above makes it look like the pressure is independent and the ratios are dependent. (That might not be a problem.)&lt;/p&gt;&#10;&#10;&lt;p&gt;But here are a ideas that use your current graph.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.google.com/search?client=ubuntu&amp;amp;channel=fs&amp;amp;q=python%20random%20list%20order&amp;amp;ie=utf-8&amp;amp;oe=utf-8&quot; rel=&quot;nofollow&quot;&gt;Sorting a list randomly in python&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;It looks like the pressures might be clustered a bit. I'm not sure whether this is what you were saying was bell-shaped. But if they are clustered, you could try assigning different dot types to each of a small number of clusters&lt;/li&gt;&#10;&lt;li&gt;For each of the axes, plot a histogram of that variable with the pressure colors stacked on top of each other. Even if you don't change the main three-variable plot, these two-variable modified histograms would help point out the bias in the display.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-06-21T00:49:24.017" Id="12145" LastActivityDate="2011-06-21T00:49:24.017" OwnerUserId="3874" ParentId="11984" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The mean is the coefficient in the regression of the data against the constant $1$.  Your statistic, in this regression context, is the simplest possible example of the &lt;strong&gt;DFBETA&lt;/strong&gt; diagnostic defined in Belsley, Kuh, &amp;amp; Welsch, &lt;em&gt;Regression Diagnostics&lt;/em&gt; (J Wiley &amp;amp; Sons, 1980):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;...we look first at the change in the estimated regression coefficients that would occur if the $i^\text{th}$ row were deleted.  Denoting the coefficients estimated with the  $i^\text{th}$ row deleted by $\mathbf{b}(i)$, this change is easily computed from the formula&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$DFBETA_i = \mathbf{b} - \mathbf{b}(i) = \frac{(X^T X)^{-1} x_i^T e_i}{1 - h_i}$$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;where&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$ h_i = x_i (X^T X)^{-1} x_i^T \ldots$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;[pp 12-13, formulas (2.1) and (2.2)].&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case the design matrix $X$ is the $n$ by $1$ matrix of ones, whence $(X^T X)^{-1} = 1/n$.  The numbers $e_i$ are the residuals,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$e_i = x_i - \bar{x}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  
  
  
  <row Body="&lt;p&gt;The Kruskal-Wallis test is a special case of the proportional odds model.  You can use the proportional odds model to model multiple factors, adjust for covariates, etc.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-21T13:26:08.720" Id="12171" LastActivityDate="2011-06-21T13:26:08.720" OwnerUserId="4253" ParentId="12151" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;&lt;em&gt;Before I answer: whether you have heard of nested ANOVA, or even think of designs as 'nested,' depends to some extent on what software you use and how you were taught stats.  There's more than one way to skin a cat, as they say.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If your design includes nested factors, then the model you use to analyse the data using ANOVA should reflect this nested design.  In your design, &lt;code&gt;mouse&lt;/code&gt; is nested within &lt;code&gt;genotype&lt;/code&gt;, as each mouse can be of only one genotype.  The individual &lt;code&gt;sample&lt;/code&gt;s (which are effectively nested within &lt;code&gt;mouse&lt;/code&gt;) represent the bottom layer in this design and are effectively your 'error' term. It is important to note that the group of &lt;code&gt;sample&lt;/code&gt;s as a whole is not a set of independent data points because they are grouped by &lt;code&gt;mouse&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;To test for an effect of &lt;code&gt;genotype&lt;/code&gt;  on your measured variable of interest, you should calculate an F-ratio that corresponds to (variance between genotypes / variance within genotypes).  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you fit the model &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;measurement = genotype&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or the model &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;measurement = mouse + genotype&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then the F-ratio for genotype will be calculated as MS(genotype)/MS(error).  This is incorrect, because it does not accurately reflect (variance between genotypes / variance within genotypes).  The variance within a genotype is due to variance among the mice in each genotype, not among all the individual samples.  If you fit the model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;measurement = genotype + mouse(genotype)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And (this is important) also declare &lt;code&gt;mouse&lt;/code&gt; as a random factor, then the F-ratio for genotype will be calculated as MS(genotype)/MS(mouse), which is correct.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So if you fit the incorrect model that ignores the nesting, you use the wrong denominator term for your F-ratio - which will usually be artifically large because you have used a denominator that has too many DF (you have more samples than mice) and therefore your chances of making a Type 1 error are increased.&lt;/p&gt;&#10;&#10;&lt;p&gt;You do also have an alternative to the nested model, which is to calculate the mean measurement for each mouse, rendering one datapoint per mouse.  You can then fit the model &lt;code&gt;measurement = genotype&lt;/code&gt; because you have now made MS(error) the correct denominator for MS(genotype).  The results will be identical to the nested analysis of the original data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nested ANOVA is useful for telling you where most variation lies in your design - i.e. which level of the nesting is most variable.  It is also the correct type of ANOVA to use when your experiemtal design is intrinsically nested.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nesting is very nicely covered in Chapter 12 of Grafen &amp;amp; Hails's &lt;em&gt;Modern Statistics for the Life Sciences&lt;/em&gt; (Oxford University Press), I highly recommend reading over this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-21T15:30:12.390" Id="12173" LastActivityDate="2011-06-22T08:48:31.287" LastEditDate="2011-06-22T08:48:31.287" LastEditorUserId="266" OwnerUserId="266" ParentId="8352" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Necessary conditions might be hard, at least for the question as wide-open as it currently stands.  A pretty simple sufficient condition is for $X_{1},X_{2},\ldots,X_{n}$ to be a simple random sample from an &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_family&quot; rel=&quot;nofollow&quot;&gt;exponential family&lt;/a&gt;.  This covers your example above because the $N(0,\sigma)$ family can be written in the form&#10;$$
  
  <row Body="&lt;p&gt;Just to add to answer by @NRH. The general idea follows the &lt;a href=&quot;http://www.google.co.uk/search?q=goldilocks%20MCMC&quot;&gt;Goldilocks principal&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If the jumps are &quot;too large&quot;, then the chain sticks;&lt;/li&gt;&#10;&lt;li&gt;If the jumps are &quot;too small&quot;, then the chain explores the parameter space very slower;&lt;/li&gt;&#10;&lt;li&gt;We want the jumps to be just right.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Of course the question is, what do we mean by &quot;just right&quot;. Essentially, for a particular case they minimise the expected square jump distance. This is equivalent to minimising the lag-1 autocorrelations. Recently, Sherlock and Roberts showed that the magic 0.234 holds for other target distributions:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;C. Sherlock, G. Roberts&#10;  (2009);&lt;a href=&quot;http://www.maths.lancs.ac.uk/~sherlocc/Publications/rwm.final.pdf&quot;&gt;Optimal scaling of the random&#10;  walk Metropolis on elliptically&#10;  symmetric unimodal targets&lt;/a&gt;;&#10;  Bernoulli 15(3)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2011-06-21T21:06:20.710" Id="12185" LastActivityDate="2011-06-21T21:06:20.710" OwnerUserId="8" ParentId="12155" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="12188" AnswerCount="3" Body="&lt;p&gt;Nearly every decision tree example I've come across happens to be a binary tree.  Is this pretty much universal?  Do most of the standard algorithms (C4.5, CART, etc.) only support binary trees?  From what I gather, &lt;a href=&quot;http://en.wikipedia.org/wiki/CHAID&quot;&gt;CHAID&lt;/a&gt; is not limited to binary trees, but that seems to be an exception.&lt;/p&gt;&#10;&#10;&lt;p&gt;A two-way split followed by another two-way split on one of the children is not the same thing as a single three-way split.  This might be an academic point, but I'm trying to make sure I understand the most common use-cases.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-21T21:29:43.933" FavoriteCount="2" Id="12187" LastActivityDate="2011-06-22T15:49:36.863" OwnerUserId="2485" PostTypeId="1" Score="7" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;cart&gt;" Title="Are decision trees almost always binary trees?" ViewCount="2000" />
  <row Body="&lt;p&gt;How many top floor units are there? How many corner units? It's possible these variables are being thrown off by a couple of outliers, which isn't hard when you have so few samples. &lt;/p&gt;&#10;&#10;&lt;p&gt;One thing you can do is look for dependencies between variables. Maybe all/most of the top floor units in your dataset happened to be from cheaper quality buildings, or smaller units. You won't see a dependence if the units are cheaper for a reason that isn't reflected in your independent variables, like location (probably one of the most predictive variables in housing price models). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-21T21:55:25.177" Id="12190" LastActivityDate="2011-06-21T21:55:25.177" OwnerUserId="2965" ParentId="12182" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="12204" AnswerCount="2" Body="&lt;p&gt;If one wanted to use Kernel Regression in a Bayesian Framework, any ideas on how one would go about it? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_regression&quot; rel=&quot;nofollow&quot;&gt;Kernel Regression&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-21T22:43:16.417" FavoriteCount="1" Id="12195" LastActivityDate="2011-06-22T07:11:11.133" LastEditDate="2011-06-21T22:48:34.257" LastEditorUserId="88" OwnerUserId="2310" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;bayesian&gt;&lt;kernel&gt;" Title="Viewing kernel regression in a Bayesian framework" ViewCount="110" />
  <row Body="&lt;p&gt;&lt;strong&gt;I like to draw pictures.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/uMGVl.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;These diagrams are &lt;a href=&quot;http://en.wikipedia.org/wiki/Finite-state_machine&quot; rel=&quot;nofollow&quot;&gt;finite state automata&lt;/a&gt; (FSAs).  They are tiny children's games (like &lt;a href=&quot;http://en.wikipedia.org/wiki/Snakes_and_ladders&quot; rel=&quot;nofollow&quot;&gt;Chutes and Ladders&lt;/a&gt;) that &quot;recognize&quot; or &quot;accept&quot; the HTT and HTH sequences, respectively, by moving a token from one node to another in response to the coin flips.  The token begins at the top node, pointed to by an arrow (line &lt;em&gt;i&lt;/em&gt;).  After each toss of the coin, the token is moved along the edge labeled with that coin's outcome (either H or T) to another node (which I will call the &quot;H node&quot; and &quot;T node,&quot; respectively).  When the token lands on a terminal node (no outgoing arrows, indicated in green) the game is over and the FSA has accepted the sequence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Think of each FSA as progressing vertically down a linear track.  Tossing the &quot;right&quot; sequence of heads and tails causes the token to progress towards its destination.  Tossing a &quot;wrong&quot; value causes the token to back up (or at least stand still).  &lt;em&gt;The token backs up to the most advanced state corresponding to the most recent tosses.&lt;/em&gt;  For instance, the HTT FSA at line &lt;em&gt;ii&lt;/em&gt; stays put at line &lt;em&gt;ii&lt;/em&gt; upon seeing a head, because that head could be the initial sequence of an eventual HTH.  It does &lt;em&gt;not&lt;/em&gt; go all the way back to the beginning, because that would effectively ignore this last head altogether.&lt;/p&gt;&#10;&#10;&lt;p&gt;After verifying these two games indeed correspond to HTT and HTH as claimed, and comparing them line by line, and it should now be obvious that &lt;strong&gt;HTH is harder to win&lt;/strong&gt;.  They differ in their graphical structure only on line &lt;em&gt;iii&lt;/em&gt;, where an H takes HTT back to line &lt;em&gt;ii&lt;/em&gt; (and a T accepts) but, in HTH, a T takes us all the way back to line &lt;em&gt;i&lt;/em&gt; (and an H accepts).  &lt;strong&gt;The penalty at line &lt;em&gt;iii&lt;/em&gt; in playing HTH is more severe than the penalty in playing HTT.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;This can be quantified.&lt;/strong&gt;  I have labeled the nodes of these two FSAs with the &lt;em&gt;expected number of tosses needed for acceptance.&lt;/em&gt;  Let us call these the node &quot;values.&quot;  The labeling begins by &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(1) writing the obvious value of 0 at the accepting nodes.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Let the probability of heads be p(H) and the probability of tails be 1 - p(H) = p(T).  (For a fair coin, both probabilities equal 1/2.)  Because each coin flip adds one to the number of tosses,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(2) the value of a node equals one plus p(H) times the value of the H node plus p(T) times the value of the T node.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;These rules determine the values&lt;/strong&gt;.  It's a quick and informative exercise to verify that the labeled values (assuming a fair coin) are correct.  As an example, consider the value for HTH on line &lt;em&gt;ii&lt;/em&gt;.  The rule says 8 must be 1 more than the average of 8 (the value of the H node on line &lt;em&gt;i&lt;/em&gt;) and 6 (the value of the T node on line &lt;em&gt;iii&lt;/em&gt;): sure enough, 8 = 1 + (1/2)*8 + (1/2)*6.  You can just as readily check the remaining five values in the illustration.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-21T23:17:46.987" Id="12196" LastActivityDate="2013-07-26T16:01:10.617" LastEditDate="2013-07-26T16:01:10.617" LastEditorUserId="919" OwnerUserId="919" ParentId="12174" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;A common technique before applying PCA is to subtract the mean from the samples. If you don't do it, the first eigenvector will be the mean. I'm not sure whether you have done it but let me talk about it. If we speak in MATLAB code: this is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;clear, clf&#10;clc&#10;%% Let us draw a line&#10;scale = 1;&#10;x = scale .* (1:0.25:5);&#10;y = 1/2*x + 1;&#10;&#10;%% and add some noise&#10;y = y + rand(size(y));&#10;&#10;%% plot and see&#10;subplot(1,2,1), plot(x, y, '*k')&#10;axis equal&#10;&#10;%% Put the data in columns and see what SVD gives&#10;A = [x;y];&#10;[U, S, V] = svd(A);&#10;&#10;hold on&#10;plot([mean(x)-U(1,1)*S(1,1) mean(x)+U(1,1)*S(1,1)], ...&#10;     [mean(y)-U(2,1)*S(1,1) mean(y)+U(2,1)*S(1,1)], ...&#10;     ':k');&#10;plot([mean(x)-U(1,2)*S(2,2) mean(x)+U(1,2)*S(2,2)], ...&#10;     [mean(y)-U(2,2)*S(2,2) mean(y)+U(2,2)*S(2,2)], ...&#10;     '-.k');&#10;title('The left singular vectors found directly')&#10;&#10;%% Now, subtract the mean and see its effect&#10;A(1,:) = A(1,:) - mean(A(1,:));&#10;A(2,:) = A(2,:) - mean(A(2,:));&#10;&#10;[U, S, V] = svd(A);&#10;&#10;subplot(1,2,2)&#10;plot(x, y, '*k')&#10;axis equal&#10;hold on&#10;plot([mean(x)-U(1,1)*S(1,1) mean(x)+U(1,1)*S(1,1)], ...&#10;     [mean(y)-U(2,1)*S(1,1) mean(y)+U(2,1)*S(1,1)], ...&#10;     ':k');&#10;plot([mean(x)-U(1,2)*S(2,2) mean(x)+U(1,2)*S(2,2)], ...&#10;     [mean(y)-U(2,2)*S(2,2) mean(y)+U(2,2)*S(2,2)], ...&#10;     '-.k');&#10;title('The left singular vectors found after subtracting mean')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As can be seen from the figure, I think you should subtract the mean from the data if you want to analyze the (co)variance better.  Then the values will not be between 10-100 and 0.1-1, but their mean will all be zero. The variances will be found as the eigenvalues (or square of the singular values ). The found eigenvectors are not affected by the scale of a dimension for the case when we subtract the mean  as much as the case when we do not. For instance, I've tested and observed the following that tells subtracting the mean might matter for your case. So the problem may result not from the variance but from the translation difference.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% scale = 0.5, without subtracting mean&#10;U =&#10;&#10;-0.5504   -0.8349&#10;-0.8349    0.5504&#10;&#10;&#10;% scale = 0.5, with subtracting mean&#10;U =&#10;&#10;-0.8311   -0.5561&#10;-0.5561    0.8311&#10;&#10;&#10;% scale = 1, without subtracting mean&#10;U =&#10;&#10;-0.7327   -0.6806&#10;-0.6806    0.7327&#10;&#10;% scale = 1, with subtracting mean&#10;U =&#10;&#10;-0.8464   -0.5325&#10;-0.5325    0.8464&#10;&#10;&#10;% scale = 100, without subtracting mean&#10;U =&#10;&#10;-0.8930   -0.4501&#10;-0.4501    0.8930&#10;&#10;&#10;% scale = 100, with subtracting mean&#10;U =&#10;&#10;-0.8943   -0.4474&#10;-0.4474    0.8943&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Cbyez.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-06-22T04:44:47.347" Id="12202" LastActivityDate="2011-06-22T04:44:47.347" OwnerUserId="5025" ParentId="12200" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="12224" AnswerCount="1" Body="&lt;p&gt;Let's say that I have a population of 10000, and I have a sample of 300 that have responded to an online survey. Of 10000 invitations, 300 hundred have replied. Would I be able to use the finite population correction for margin of error? &lt;/p&gt;&#10;&#10;&lt;p&gt;Population: 10000&lt;br&gt;&#10;Sample: 300&lt;br&gt;&#10;Response: 3%  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-22T07:40:02.643" Id="12205" LastActivityDate="2011-11-20T04:13:19.317" LastEditDate="2011-06-22T08:32:55.290" LastEditorUserId="88" OwnerUserId="776" PostTypeId="1" Score="1" Tags="&lt;survey&gt;&lt;standard-error&gt;" Title="Finite population correction for calculating margin of error" ViewCount="1122" />
  <row Body="&lt;p&gt;If the singular values are precisely equal, then the singular vectors can be just about any set of orthonormal vectors, therefore they carry no information.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Generally, if two singular values are equal, the corresponding singular vectors can be rotated in the plane defined by them, and nothing changes.  It will not be possible to distinguish between direction in that plane based on the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;To show a 2D example similar to yours, ${(1, 1), (1, -1)}$ are just two orthogonal vectors, but your numerical method could just as easily have given you ${(1,0), (0,1)}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-22T08:05:44.943" Id="12210" LastActivityDate="2011-06-22T08:05:44.943" OwnerUserId="4764" ParentId="10094" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I don't know if there is an obvious standard way of doing this, but:&lt;/p&gt;&#10;&#10;&lt;p&gt;First, you find the intersection points between the two densities. This can be easily achieved by equating both densities, which, for the normal distribution, should result in a quadratic equation for x.&lt;/p&gt;&#10;&#10;&lt;p&gt;Something close to:&#10;$$
  
  <row Body="&lt;p&gt;This is also often called the &quot;overlapping coefficient&quot; (OVL). Googling for this will give you lots of hits. You can find a nomogram for the bi-normal case here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.rasch.org/rmt/rmt101r.htm&quot;&gt;http://www.rasch.org/rmt/rmt101r.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A useful paper may be:&lt;/p&gt;&#10;&#10;&lt;p&gt;Henry F. Inman; Edwin L. Bradley Jr (1989). The overlapping coefficient as a measure of agreement between probability distributions and point estimation of the overlap of two normal densities. Communications in Statistics - Theory and Methods, 18(10), 3851-3874. (&lt;a href=&quot;http://www.informaworld.com/smpp/content~db=all~content=a780118141&quot;&gt;Link&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you got me interested in this more, so I went ahead and created R code to compute this (it's a simple integration). I threw in a plot of the two distributions, including the shading of the overlapping region:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;min.f1f2 &amp;lt;- function(x, mu1, mu2, sd1, sd2) {&#10;    f1 &amp;lt;- dnorm(x, mean=mu1, sd=sd1)&#10;    f2 &amp;lt;- dnorm(x, mean=mu2, sd=sd2)&#10;    pmin(f1, f2)&#10;}&#10;&#10;mu1 &amp;lt;- 2&#10;mu2 &amp;lt;- 1&#10;sd1 &amp;lt;- 2&#10;sd2 &amp;lt;- 1&#10;&#10;xs &amp;lt;- seq(min(mu1 - 3*sd1, mu2 - 3*sd2), max(mu1 + 3*sd1, mu2 + 3*sd2), .01)&#10;f1 &amp;lt;- dnorm(xs, mean=mu1, sd=sd1)&#10;f2 &amp;lt;- dnorm(xs, mean=mu2, sd=sd2)&#10;&#10;plot(xs, f1, type=&quot;l&quot;, ylim=c(0, max(f1,f2)), ylab=&quot;density&quot;)&#10;lines(xs, f2, lty=&quot;dotted&quot;)&#10;ys &amp;lt;- min.f1f2(xs, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)&#10;xs &amp;lt;- c(xs, xs[1])&#10;ys &amp;lt;- c(ys, ys[1])&#10;polygon(xs, ys, col=&quot;gray&quot;)&#10;&#10;### only works for sd1 = sd2&#10;SMD &amp;lt;- (mu1-mu2)/sd1&#10;2 * pnorm(-abs(SMD)/2)&#10;&#10;### this works in general&#10;integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For this example, the result is: 0.6099324 with absolute error &amp;lt; 1e-04. Figure below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6eFin.png&quot; alt=&quot;Example&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-22T10:31:05.137" Id="12216" LastActivityDate="2011-06-22T22:20:06.457" LastEditDate="2011-06-22T22:20:06.457" LastEditorUserId="1934" OwnerUserId="1934" ParentId="12209" PostTypeId="2" Score="19" />
  
  
&#10;\mu=\frac{\alpha}{\alpha+\beta}
&#10;$$&#10;and&#10;$$
  <row Body="&lt;p&gt;Rank based tests work by transforming the data to a uniform distribution then relying on the central limit theorem to justify approximate normality (the clt kicks in for the uniform around n=5 or 6), this helps counter the effects of skewness or outliers.  Your data has the opposite problem and the rank transform is unlikely to help (the 100's will still all be ties in the ranks).  For your sample size and the restrictions on the data, the normal theory tests are probably fine due to the clt.  I would be more concerned about unequal variances if some combinations have only 100's or mostly 100's.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really want to you could do a &lt;a href=&quot;http://stats.stackexchange.com/questions/12151/is-there-an-equivalent-to-kruskal-wallis-one-way-test-for-a-two-way-model/12154#12154&quot;&gt;permutation test&lt;/a&gt;, but I doubt that it will tell you much more than what you have already done, possibly using some statistic based on medians rather than the F-stat may help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-22T19:54:54.857" Id="12250" LastActivityDate="2011-06-22T19:54:54.857" OwnerUserId="4505" ParentId="12231" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;I disagree that there is any program that can do just what you want. Of the automated methods, LASSO is probably best, but it does not allow certain variables to be forced into the equation. It also does not use your substantive knowledge.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is also no algorithm to replace an automated method with a manualized version.&lt;/p&gt;&#10;&#10;&lt;p&gt;The right method depends on number of variables and their interrelationship, but, if there are not a huge number, then I suggest coming up with several sets of variables, based on your knowledge, and comparing them based on AIC, BIC or some other similar measure (I don't have a strong preference among these).&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a case where our human brains are better than computers. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-23T10:38:57.130" Id="12278" LastActivityDate="2011-06-23T10:38:57.130" OwnerUserId="686" ParentId="12260" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;We know that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\widehat{Var}(\mathbf{a}) = \frac{1}{n-1}\left(\sum_{i=1}^n a_i^2 - \frac{1}{n}\left(\sum_{i=1}^n a_i \right)^2 \right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;is an unbiased estimator of the population variance, which is easily computed as $(m+1)m/12$.  This, therefore, answers the first question concerning the expected variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will only sketch how to maximize the variance.  I claim it is maximized when the $a_i$ are in two contiguous blocks: that is, $\mathbf{a}$ is in the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf{a} = (1, 2, \ldots, k, m-l+1, m-l+2, \ldots, m).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Evidently $k+l = n$.)  To prove this claim, suppose $\mathbf{a}$ is not in this form: then you can find a gap in one of the end sequences and increase the variance by changing one of the components of $\mathbf{a}$ to that gap.  It remains only to maximize the variance among these special forms of $\mathbf{a}$; this is done by making the end sequence lengths as balanced as possible; that is, by setting $k=l$ when $n$ is even and otherwise by setting either $k=l+1$ or $l=k+1$.  When $n=2k$ is even, the maximum variance equals&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ n \frac{\left(3 m^2-3 m n+n^2 -1\right)}{12 (n-1)}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;When $n=2k+1$ is odd, the maximum variance is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(n+1) \frac{\left(3 m^2-3 m n+n^2\right)}{12 n}.$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-23T15:00:25.477" Id="12284" LastActivityDate="2011-06-23T15:00:25.477" OwnerUserId="919" ParentId="12258" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;With only just these two cases, you cannot reliably estimate a treatment effect, but you can summarize your data as follows, assuming that the the number of deaths is a draw from a Poisson distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;&amp;amp;\beta_{0i} = \gamma_{00} + \gamma_{01}\text{State}_i \\
&#10;&amp;amp;\beta_{1i} = \gamma_{10} + \gamma_{11}\text{State}_i \\
  <row AcceptedAnswerId="12312" AnswerCount="1" Body="&lt;p&gt;In a cross-validation setting (LASSO penalized logistic regression), I'm calculating AUC. However, I'm interested in the variability of these estimates over the folds (this will give me &lt;em&gt;an&lt;/em&gt; indication of the stability of my model selection over the folds). &lt;/p&gt;&#10;&#10;&lt;p&gt;As such, I want to find the empirical AUC in each of the 10 validation sets, and then calculate the variance over them. This poses a problem, as sometimes a validation set only holds only observations that have true outcome 1 or only observations that have true outcome 0. I don't know of a way to calculate the AUC in such a setting.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be the sensible approach here?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Ignore this 'fold' in the&#10;calculations regarding AUC&lt;/li&gt;&#10;&lt;li&gt;Give it some value anyway, like 0.5&lt;/li&gt;&#10;&lt;li&gt;Perhaps you can suggest a way of&#10;approximating the AUC in such cases&#10;(adding 1 fake observation of the&#10;other kind and assume its predicted&#10;probability is either 0, 0.5 or 1?)&lt;/li&gt;&#10;&lt;li&gt;Don't try this variance over the&#10;folds idea.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-06-24T10:03:44.583" Id="12309" LastActivityDate="2011-06-24T12:05:28.183" OwnerUserId="4257" PostTypeId="1" Score="4" Tags="&lt;model-selection&gt;&lt;cross-validation&gt;&lt;auc&gt;" Title="Empirical AUC in validation set when no TRUE zeroes" ViewCount="165" />
  <row AnswerCount="0" Body="&lt;p&gt;I tried to test if I should normalize the dataset before doing distribution fitting.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; a1 &amp;lt;- rgev(2000, loc= 0.449, scale=0.7423, shape=0)&#10;&amp;gt; a1_scale &amp;lt;- scale(a1)&#10;&amp;gt; fgev(a1, shape=0)&#10;&#10;Call: fgev(x = a1, shape = 0) &#10;Deviance: 5159.472 &#10;&#10;Estimates&#10;   loc   scale  &#10;0.4619  0.7495  &#10;&#10;Standard Errors&#10;    loc    scale  &#10;0.01764  0.01307  &#10;&#10;Optimization Information&#10;  Convergence: successful &#10;  Function Evaluations: 24 &#10;  Gradient Evaluations: 5 &#10;&#10;&amp;gt; fgev(a1_scale, shape=0)&#10;&#10;Call: fgev(x = a1_scale, shape = 0) &#10;Deviance: 5288.283 &#10;&#10;Estimates&#10;    loc    scale  &#10;-0.4476   0.7740  &#10;&#10;Standard Errors&#10;    loc    scale  &#10;0.01822  0.01350  &#10;&#10;Optimization Information&#10;  Convergence: successful &#10;  Function Evaluations: 24 &#10;  Gradient Evaluations: 5 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To check the agreement between these two results, I generate model b from the parameters obtained above and do the qq-plot.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; b1 &amp;lt;- rgev(2000, loc=0.4619, scale=0.7495, shape=0)&#10;&amp;gt; b1_scale &amp;lt;- rgev(2000, loc=-0.4476, scale=0.7440, shape=0)&#10;&amp;gt; qqplot(b1, b1_scale)&#10;&amp;gt; abline(0,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is what I found:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Gzd7X.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Does it mean that I should never normalize the dataset before doing distribution fitting?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-24T10:31:12.687" Id="12310" LastActivityDate="2011-06-24T11:21:17.047" LastEditDate="2011-06-24T11:21:17.047" LastEditorUserId="88" OwnerUserId="5137" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;distributions&gt;&lt;normalization&gt;" Title="Should we normalize the dataset before fitting the Gumbel distribution?" ViewCount="366" />
  
&#10;\begin{align}
  <row AnswerCount="2" Body="&lt;p&gt;If I have missing values in a time series that has 40 quarters (ten cycles or ten years) of data, what is the best SAS procedure to use to impute the missing values? &lt;/p&gt;&#10;&#10;&lt;p&gt;Part 2:  I have 390 series (40 quarters each) that follow similar patterns -- most have missing data points (2-3 each), how do I make use of the other  390 series to help impute missing values in any one series? What SAS procedure would I use for that?  In the end I want a complete set of 15600 data points. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-24T15:17:03.557" Id="12316" LastActivityDate="2012-03-02T05:44:26.847" LastEditDate="2011-06-24T16:29:19.000" LastEditorUserId="88" OwnerUserId="5164" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;sas&gt;&lt;data-imputation&gt;" Title="Imputing missing values in time series using SAS" ViewCount="629" />
  <row Body="&lt;p&gt;Unless you want to know what features are relevant, or there is a cost associated in collecting all of the attributes, rather than just some of them, then don't perform feature selection at all, and just use regularisation (e.g. ridge-regression) to prevent over-fitting instead.  This is essentially the advice given by Miller in his &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1584881712&quot; rel=&quot;nofollow&quot;&gt;monograph&lt;/a&gt; on subset selection in regression.  Feature selection is tricky and often makes predictive performance worse, rather than better.  The reason is that it is easy to over-fit the feature selection criterion, as there is esentially one degree of freedom for each attribute, so you get a subset of attributes that works really well on one particular sample of data, but not necessarily on any other.  Regularisation is easier, as you generally only have one degree of freedom, hence you tend to get less over-fitting (although the problem doesn't go away completely).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-24T15:35:31.857" Id="12318" LastActivityDate="2011-06-24T15:35:31.857" OwnerUserId="887" ParentId="12260" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'm not sure if this is what you are looking for, as it is mostly algebraic and not really leveraging the properties of p being a probability distribution, but here is something you can try.&lt;/p&gt;&#10;&#10;&lt;p&gt;Due to the bounds on pmi, clearly $\frac{p(x,y)}{p(x)p(y)}\leq e^k$ and thus $p(x,y)\leq p(x)p(y)\cdot e^k$.  We can substitute for $p(x,y)$ in $I(X;Y)$ to get $I(X;Y)\leq \sum_{x,y}p(x)p(y)\cdot e^k\cdot log(\frac{p(x)p(y)\cdot e^k}{p(x)p(y)}) = \sum_{x,y}p(x)p(y)\cdot e^k\cdot k$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure if that's helpful or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Upon further review I believe this is actually less useful than the original upper bound of k.  I won't delete this though in case it might hint at a starting point.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-24T19:39:22.427" Id="12329" LastActivityDate="2011-06-24T19:53:21.790" LastEditDate="2011-06-24T19:53:21.790" LastEditorUserId="2485" OwnerUserId="2485" ParentId="12322" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;A quick simulation in R indicates the increase is proportional to the number of sides of the dice&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;means = NULL&#10;for(i in seq(6,72,by=6)) {&#10;  dice1 = sample(1:i, 10000, replace=T)&#10;  dice2 = sample(1:i, 10000, replace=T)&#10;  m = mean(pmax(dice1, dice2))&#10;  means = c(means, m)&#10;}&#10;&#10;plot(means)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-06-25T09:41:45.333" Id="12338" LastActivityDate="2011-06-25T15:41:43.557" LastEditDate="2011-06-25T15:41:43.557" LastEditorUserId="8" OwnerUserId="5160" ParentId="12334" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;KL divergence is another good test statistic to use.  It has very similar properties to the chi-square for large expected cell counts, but retains these &quot;good&quot; properties even when expected cell counts are small.&lt;/p&gt;&#10;&#10;&lt;p&gt;The KL divergence is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{i}O_{i}\log\left(\frac{O_{i}}{E_{i}}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And this statistic never breaks down unless your hypothesis (the $E_{i}$ contradict the data by specifying $E_i=0$ when $O_i&amp;gt;0$.  For independence we have $E_A=\frac{(A+B)(A+C)}{A+B+C+D}$, etc. as in the chi-square test.  This is also called &quot;likelihood ratio chi-square&quot; in some stats packages.  For the purpose of calculating a p-value, its no different to pearson chi-square (they both require large $E_i$ for the asymptotic chi-square distribution to apply).  But you don't need a p-value, as this is a log likelihood ratio, so you can interpret the numerical value directly.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-26T01:43:59.280" Id="12350" LastActivityDate="2011-06-26T01:43:59.280" OwnerUserId="2392" ParentId="8052" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.quora.com/&quot; rel=&quot;nofollow&quot;&gt;Quora&lt;/a&gt; might be one option.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-26T18:54:54.627" Id="12379" LastActivityDate="2011-06-26T18:54:54.627" OwnerUserId="22" ParentId="12373" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The easiest thing to do is just look at how &lt;code&gt;qqplot&lt;/code&gt; works. So in R type:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R&amp;gt; qqplot&#10;function (x, y, plot.it = TRUE, xlab = deparse(substitute(x)), &#10;    ylab = deparse(substitute(y)), ...) &#10;{&#10;    sx &amp;lt;- sort(x)&#10;    sy &amp;lt;- sort(y)&#10;    lenx &amp;lt;- length(sx)&#10;    leny &amp;lt;- length(sy)&#10;    if (leny &amp;lt; lenx) &#10;        sx &amp;lt;- approx(1L:lenx, sx, n = leny)$y&#10;    if (leny &amp;gt; lenx) &#10;        sy &amp;lt;- approx(1L:leny, sy, n = lenx)$y&#10;    if (plot.it) &#10;        plot(sx, sy, xlab = xlab, ylab = ylab, ...)&#10;    invisible(list(x = sx, y = sy))&#10;}&#10;&amp;lt;environment: namespace:stats&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So to generate the plot we just have to get &lt;code&gt;sx&lt;/code&gt; and &lt;code&gt;sy&lt;/code&gt;, i.e:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rnorm(10);y &amp;lt;- rnorm(20)&#10;&#10;sx &amp;lt;- sort(x); sy &amp;lt;- sort(y)&#10;lenx &amp;lt;- length(sx)&#10;leny &amp;lt;- length(sy)&#10;if (leny &amp;lt; lenx)sx &amp;lt;- approx(1L:lenx, sx, n = leny)$y&#10;if (leny &amp;gt; lenx)sy &amp;lt;- approx(1L:leny, sy, n = lenx)$y&#10;&#10;require(ggplot2)&#10;g = ggplot() + geom_point(aes(x=sx, y=sy))&#10;g&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/01LkW.png&quot; alt=&quot;qqplot using ggplot2&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-06-27T10:47:51.237" Id="12394" LastActivityDate="2011-06-27T10:47:51.237" OwnerUserId="8" ParentId="12392" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="12399" AnswerCount="1" Body="&lt;p&gt;I play a game online (Heroes of Newerth) which has a large ladder of players, each player having a couple of different ratings. I've manually gathered the rating data for all percentiles of players but am blanking at how to turn this into a histogram and compute the Mean and Standard Deviation. I'd also like to have a graph to show the curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could easily do so using my TI-83+ and I consider myself very proficient at Excel, but I don't see an obvious way to do this. Below is a link to the data, or if you'd prefer to give me instructions on how to create the distribution I'd be happy to do so myself.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://spreadsheets.google.com/spreadsheet/ccc?key=0Atwn_lcLizk9dDZ4WW1YV3BZQnFSaGNoQVdRa0JVZ3c&amp;amp;hl=en_US&amp;amp;authkey=CKyI38QJ#gid=0&quot; rel=&quot;nofollow&quot;&gt;https://spreadsheets.google.com/spreadsheet/ccc?key=0Atwn_lcLizk9dDZ4WW1YV3BZQnFSaGNoQVdRa0JVZ3c&amp;amp;hl=en_US&amp;amp;authkey=CKyI38QJ#gid=0&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2011-06-27T14:30:43.850" Id="12397" LastActivityDate="2011-07-05T21:46:58.533" LastEditDate="2011-07-05T21:46:58.533" LastEditorUserId="5193" OwnerUserId="5193" PostTypeId="1" Score="4" Tags="&lt;excel&gt;&lt;histogram&gt;&lt;quantiles&gt;" Title="How can I create a Graph of a Probability Density Function from Percentiles?" ViewCount="5437" />
  
  <row Body="&lt;p&gt;Here's a package that might help:  &lt;a href=&quot;http://cran.r-project.org/web/packages/DEoptim/index.html&quot; rel=&quot;nofollow&quot;&gt;DEoptim&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice the vignettes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-27T20:17:48.303" Id="12419" LastActivityDate="2011-06-27T20:17:48.303" OwnerUserId="2775" ParentId="12417" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="12442" AnswerCount="2" Body="&lt;p&gt;Suppose people take a 10-item exam. For each item, $k = 1,2,…,10$, exactly one rater assigns a score to exactly one person, with the constraint that no person sees the same rater twice. There are $i = 1,2,…,5000$ people and $j = 1,2,…,500$ raters. So people and raters are partially crossed, with most person-rater pairs having 0 data points, and the rest having exactly 1 data point. &lt;/p&gt;&#10;&#10;&lt;p&gt;Update: items were randomly assigned to positions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I want to estimate a random slopes model where scores vary linearly with item position and the variability in scores is due to person intercept variability, person slope variability, rater intercept variability, rater slope variability, and residual error variability – in R,&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;lmer(rating ~ itemPosition + (itemPosition | personID) + (itemPosition | raterID))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Firstly, is there a better way to write the model down in non-matrix form than what I have below? Because there are two grouping factors, I am confused about the indices. If I let $q$ index data points, I want something like&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_q = \beta_0 + \beta_1 ItemPosition_q + p_{0i} + p_{1i} ItemPosition_q + r_{0j} + r_{1j} ItemPosition_q + e_q$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p_{0i} \sim N(0, \sigma_{p0}^2)$ denotes a person-level random intercept, $r_{0j} \sim N(0, \sigma_{r0}^2)$ denotes a rater-level random intercept, $p_{1i} \sim N(0, \sigma_{p1}^2)$ denotes a person-level random slope, $r_{1i} \sim N(0, \sigma_{r1}^2)$ denotes a rater-level randoms slope, $e_q \sim N(0, \sigma^2)$ is the residual error, the person random effects are uncorrelated with the rater random effects, the random effects are uncorrelated with the error term, $cov(p_{0i}, p_{1i}) = \sigma_{p01}$, and $cov(r_{0i}, r_{1i}) = \sigma_{r01}$. More specifically, can I replace the index $q$ with $ijk$?&#10;Secondly, do I have the correct expression for $$Var(p_{0i} + p_{1i} ItemPosition_q + {r_0j} + r_{1j} ItemPosition_q + e_q) ?$$ I get &#10;$(\sigma_{0p}^2 + 2\sigma_{p01} ItemPosition_q + \sigma_{1p}^2 ItemPosition_q^2)$ + $(\sigma_{0r}^2 + 2\sigma_{r01} ItemPosition_q + \sigma_{1r}^2 ItemPosition_q^2)$ + $\sigma^2.$&#10;I want to use this as the denominator to calculate the proportion of variance due to persons, for example.&#10;Lastly, any general comments about the appropriateness of the model are also welcome but not at all necessary (as I haven’t provided any information about the data).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-27T20:25:49.450" FavoriteCount="3" Id="12420" LastActivityDate="2011-06-29T20:34:26.753" LastEditDate="2011-06-29T18:53:31.243" LastEditorUserId="3432" OwnerUserId="3432" PostTypeId="1" Score="3" Tags="&lt;random-effects-model&gt;&lt;inter-rater&gt;&lt;rating&gt;&lt;intraclass-correlation&gt;&lt;mixed-model&gt;" Title="Specification of a random slopes model with two grouping factors" ViewCount="192" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have 5 variables for 1000 observations. Given the data, I need to&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Calculate probability of hitting top 5 pct in each of the distributions separately. For example, for the value of 123.45 I need to calculate how probable is that this vale is from the top 5pct.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Calculate probability that a particular observation (given with 5 values for each variables) will be in the tails of each of the distributions... In the same time&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Calculate probability that a particular observation will hit at least one 5pct tail.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;How would you go about this?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-06-28T16:53:24.230" FavoriteCount="1" Id="12443" LastActivityDate="2011-06-28T22:41:51.270" OwnerUserId="333" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;probability&gt;" Title="Probability of hitting tails" ViewCount="145" />
  
  <row Body="&lt;p&gt;Your point 2. Kappa around 0.4 is actually quite a large value indicating a good agreement. Kappa attains 1 when all off-diagonal frequencies (which tell of disagreement) in the table are 0. Kappa is about 0 when diagonal and off-diagonal frequencies are about the same which tells that the agreement is at chance-level (that is, 50%). Kappa is negative when disagreement prevails. So, your 0.2-0.4 values aren't &quot;poor&quot; and, probably, correspond to your percents correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your point 3. Kappa is for categorical data - nominal or ordinal. You could treat your Likert scale as ordinal. But if you treat it as interval scale you should look for other test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-28T16:58:36.610" Id="12444" LastActivityDate="2011-06-28T17:16:25.887" LastEditDate="2011-06-28T17:16:25.887" LastEditorUserId="3277" OwnerUserId="3277" ParentId="12415" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I'm not aware of any.  It's unlikely any are going to arise either.  The hope is that lme4 will be expanded from handling just simple logistic models to multinomial models.  Then you can analyze your within subjects design.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing you might try is random coefficients analysis (RCA).  This was essentially the main analysis of early FMRI work.  You estimate coefficients of the model for each individual subject and then do an analysis across subjects of the coefficients.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-28T20:44:54.170" Id="12451" LastActivityDate="2011-06-29T00:52:44.113" LastEditDate="2011-06-29T00:52:44.113" LastEditorUserId="601" OwnerUserId="601" ParentId="12446" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;@RyanB thank you for your quick reply and your help!&lt;/p&gt;&#10;&#10;&lt;p&gt;I conducted an experiment supporting that men are attracted by women in red. I recruited 30 men and 25 women who followed exactly the same procedure: They had to listen to 4 dialogues between a man and a woman which one of them lasts for 30-40 sec. They hear the first dialogue and then I ask them if they believe that the man is attracted to woman, their answer shall be yes/no. Then I show them to photos, a woman with green shirt and a woman with a red one and ask them to choose the photo which, they think, depicts the woman of the dialogue they just heard. Then they listen to the second dialogue and they do exactly the same until they finish. (1st dialogue- attracted/not attracted- red/green). &#10;The reason why I do the same experiment to women (they have to rate the attractiveness of other women instead of men) is to show the difference between the color preferences between men and women and to replicate one part of the previous experiment which showed exactly the same. I used three different girls, and I randomized my data (dialogues, colors, girls) in excel.&#10;So, if A is red and B is green, I've got A1, A2 and A3 and B1, B2 and B3. And my combinations might be A1-B2, B3-A2, A3-B2, B1-A2 etc.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-06-28T21:01:50.410" FavoriteCount="2" Id="12453" LastActivityDate="2011-09-23T15:27:14.083" LastEditDate="2011-06-29T09:59:08.927" LastEditorUserId="5218" OwnerUserId="5218" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="What test do I use in order to analyze a within participants repeated measure experiment?" ViewCount="325" />
  
&#10;\text{Rating}_{ijk} = \beta_0 + p_{0i} + p_{1i}\text{Item Position}_k + r_{0j} + r_{1j}\text{Item Position}_k + \varepsilon_{ijk}
  
  <row AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/7527/change-point-analysis-using-rs-nls&quot;&gt;Change point analysis using R&amp;#39;s nls()&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;I want to do a nonlinear regression with nls() but also include a specific type of segmented or piecewise regression. The Formula I want to implement is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;S ~ b0 + (A &amp;gt; T) * b1 * (A - T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;T&lt;/code&gt; should be the threshold value or breakpoint as identified by the nonlinear-segmented regression. I know that I can use &lt;code&gt;&quot;algorithm = plinear&quot;&lt;/code&gt; but that does not work at all.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The data I have is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A   S&#10;0.000809371 1&#10;0.003642171 3&#10;0.009712455 4&#10;0.010521827 2&#10;0.004046856 4&#10;0.015378054 5&#10;0.000404686 0&#10;0.000404686 0&#10;0.000404686 0&#10;0.000809371 0&#10;0.000809371 3&#10;0.037635765 3&#10;0.008903084 2&#10;0.016187426 5&#10;0.043301364 1&#10;0.000404686 1&#10;0.002428114 1&#10;0.003642171 1&#10;0.013759312 4&#10;0.051395077 9&#10;0.394568501 9&#10;0.005665599 1&#10;0.013354626 1&#10;0.028732681 3&#10;0.026304567 2&#10;0.004451542 1&#10;0.050585705 2&#10;0.00647497  1&#10;0.010926512 0&#10;0.013354626 1&#10;1.695632841 4&#10;0.013354626 2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2011-06-29T12:49:22.803" CommentCount="3" CreationDate="2011-06-29T10:27:26.677" FavoriteCount="2" Id="12469" LastActivityDate="2011-07-04T12:06:28.767" LastEditDate="2011-06-29T12:47:50.493" LastEditorUserId="919" OwnerDisplayName="Jens" OwnerUserId="5280" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;nonlinear-regression&gt;&lt;change-point&gt;" Title="Segmented nonlinear regression in R?" ViewCount="974" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I understand the Max likelihood estimators for mu and sigma for the lognormal distribution when data are actual values.  However I need to understand how these formulas are modified when data are already grouped or binned (and actual values are not available).  Specifically, for mu, the mle estimator is the sum of the logs of each X (divided by n which is the number of points).  For sigma squared, the mle estimator is the sum of (each log X minus the mu, squared); all divided by n. (Order of operations is taking each log X minus the mu; square that; sum that over all X's; then divide by n). Now suppose data in bins b1, b2, b3, and so on where b1 to b2 is the first bin; b2 to b3 second bin and so on.  What are the modified mu and sigma squared? thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-29T22:00:11.343" Id="12490" LastActivityDate="2011-07-01T14:23:50.713" LastEditDate="2011-06-29T22:56:40.273" LastEditorUserId="919" OwnerUserId="5229" PostTypeId="1" Score="4" Tags="&lt;maximum-likelihood&gt;&lt;lognormal&gt;" Title="Lognormal distribution using binned or grouped data" ViewCount="819" />
  <row Body="&lt;p&gt;You could turn your matrix of distances into raw data and input these to K-Means clustering. The steps would be as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Distances between your N points must be squared euclidean ones. Perform &quot;double centering&quot; of the matrix: Substract row mean from each element; in the result, substract column mean from each element; in the result, add matrix mean to each element; divide by minus 2. The matrix you have now is the SSCP (sum-of-squares-and-cross-product) matrix between your points wherein the origin is put at geometrical centre of the cloud of N points.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Perform PCA (Principal component analysis) on that matrix and obtain NxN component loading matrix. Some of last columns of it are likely to be all 0, - so cut them off. What you stay with now is actually principal component scores, the coordinates of your N points onto principal components that pass, as axes, through your cloud. This data can be treated as raw data suitable for K-Means input.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. If your distances aren't geometrically correct squared euclidean ones you may encounter problem: the SSCP matrix may be not positive (semi)definite. This problem can be coped with in several ways but with loss of precision.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-06-30T07:20:10.307" Id="12503" LastActivityDate="2011-06-30T07:33:18.087" LastEditDate="2011-06-30T07:33:18.087" LastEditorUserId="3277" OwnerUserId="3277" ParentId="12495" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I see no immediate reason why this should be related to GAM. The fact is that you are using two tests for the same thing. Since there is no absolute certainty in statistics, it is very well possible to have one give a significant result and the other not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps one of the two tests is simply more powerful (but then maybe relies on some more assumptions), or maybe the single significant one is your one-in-twenty type I error.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good example is tests for whether samples come from the same distribution: you have very parametric tests for that (the T-test is one that can be used for this: if the means are different, so should the distributions), and also nonparametric ones: it could happen that the parametric one gives a significant result and the nonparametric one doesn't. This could be because the assumptions of the parametric test are false, because the data is simply extraordinary (type I), or because the sample size is not sufficient for the nonparametric test to pick up the difference, or, finally, because the &lt;em&gt;aspect&lt;/em&gt; of what you really want to test (different distributions) that is checked by the different tests is just different (different means &amp;lt;-&gt; chance of being &quot;higher than&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;If one test result shows significant results, and the other is only slightly non-significant, I wouldn't worry too much.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-30T09:49:25.587" Id="12507" LastActivityDate="2011-06-30T09:49:25.587" OwnerUserId="4257" ParentId="9259" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;This is a supervised learning problem. Ideally would like to work in R due to having an easy way to pre-process the input data, but could work around that as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each sample, input consists of tens of thousands of features. These are genomics data and will likely need to be reduced to a manageable amount, somehow, before being used to train the classifier.&lt;/p&gt;&#10;&#10;&lt;p&gt;Supervisory signal consists of 4 dependent continuous values, representing relative composition of the sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;e.g. continuous between 0 and 1, all 4 summing to 1 for each sample:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sub012  0.5940594   0.26732673  0.07920792  0.059405941&#10;Sub013  0.5102041   0.34693878  0.08163265  0.061224490&#10;Sub014  0.6521739   0.20652174  0.07608696  0.065217391&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Wanted: a regression function capable of predicting the relative composition of a sample in terms of those same 4 dependent continuous values.&lt;/p&gt;&#10;&#10;&lt;p&gt;The constraints on the supervisory signal are what is causing me pause: the dependence of the variables, being constrained between 0-1 and summing to 1. I was hoping someone might have attempted something similar and could point me in the right direction - packages or approaches which may work or definitely won't work - all thoughts welcomed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-06-30T20:00:54.897" Id="12519" LastActivityDate="2011-07-02T17:20:55.793" LastEditDate="2011-07-02T17:20:55.793" LastEditorUserId="5240" OwnerUserId="5240" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;" Title="Supervised learning approaches which can accommodate a supervisory signal composed of multiple dependent continuous variables?" ViewCount="152" />
  <row AcceptedAnswerId="12532" AnswerCount="3" Body="&lt;p&gt;I'm trying to minimize a custom function. It should accept five parameters and the data set and do all sorts of calculations, producing a single number as an output. I want to find a combination of five input parameters which yields smallest output of my function.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-06-30T23:54:42.607" FavoriteCount="5" Id="12525" LastActivityDate="2013-07-10T16:39:39.907" LastEditDate="2011-07-01T04:36:17.623" LastEditorUserId="2970" OwnerUserId="333" PostTypeId="1" Score="15" Tags="&lt;r&gt;&lt;optimization&gt;" Title="Is there a way to maximize/minimize a custom function in R?" ViewCount="6608" />
  <row Body="&lt;p&gt;The intuitive difference between Hausdorff distance and EMD between sets A and B is:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;EMD tells you the &lt;em&gt;total&lt;/em&gt; work required to move &lt;em&gt;all&lt;/em&gt; A's mass onto B, under the optimal scheme for doing so.&lt;/li&gt;&#10;&lt;li&gt;Hausdorff tells you the worst-case distance between an element of A and the nearest element of B. If you consider each point to have unit mass, then you can think of Hausdorff as telling you the &lt;em&gt;worst-case&lt;/em&gt; amount of work required to move a &lt;em&gt;single element&lt;/em&gt; of A onto some element of B, under the optimal scheme for doing so.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Your modification of Hausdorff would have the characterization:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It tells you the &lt;em&gt;average&lt;/em&gt; amount of work required to move &lt;em&gt;each&lt;/em&gt; element of A onto some element of B, under the optimal scheme for doing so.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Of course, which one you want depends on your application...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-01T01:53:31.960" Id="12530" LastActivityDate="2011-07-01T01:53:31.960" OwnerUserId="5179" ParentId="12529" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I wrote a post &lt;a href=&quot;http://jeromyanglim.blogspot.com/2011/02/r-optimisation-tips-using-optim-and.html&quot; rel=&quot;nofollow&quot;&gt;listing a few tutorials using &lt;code&gt;optim&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a quote of the relevant section:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;li&gt;&quot;The combination of the R function &lt;code&gt;optim&lt;/code&gt; and a custom created objective&#10;function, such as a minus log-likelihood function provides a powerful tool for&#10;parameter estimation of custom models. &lt;br /&gt; &#10;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://www.psych.unimelb.edu.au/AMPC2011/rt.zip&quot; rel=&quot;nofollow&quot;&gt;Scott Brown's tutorial&lt;/a&gt; includes an example of&#10;this.&lt;/li&gt; &#10;&lt;li&gt;&lt;a href=&quot;http://www.mayin.org/ajayshah/KB/R/documents/mle/mle.html&quot; rel=&quot;nofollow&quot;&gt;Ajay Shah has an example&lt;/a&gt; &#10;of writing a likelihood function and then getting a maximum likelihood&#10;estimate using &lt;code&gt;optim&lt;/code&gt;.&lt;/li&gt; &#10;&lt;li&gt;Benjamin Bolker has great material available on the web from his book&#10;&lt;a href=&quot;http://www.math.mcmaster.ca/~bolker/emdbook/index.html&quot; rel=&quot;nofollow&quot;&gt;Ecological Models and Data in R&lt;/a&gt;.&#10;PDFs, Rnw, and R code for early versions of the chapters are provided on&#10;the website.&#10;Chapter 6 (&lt;a href=&quot;http://www.math.mcmaster.ca/~bolker/emdbook/chap6A.pdf&quot; rel=&quot;nofollow&quot;&gt;likelihood and all that&lt;/a&gt;)&#10;, 7 (&lt;a href=&quot;http://www.math.mcmaster.ca/~bolker/emdbook/chap7A.pdf&quot; rel=&quot;nofollow&quot;&gt;the gory details of model fitting&lt;/a&gt;),&#10;and 8 (&lt;a href=&quot;http://www.math.mcmaster.ca/~bolker/emdbook/chap8A.pdf&quot; rel=&quot;nofollow&quot;&gt;worked likelihood estimation examples&lt;/a&gt;).&lt;/li&gt; &#10;&lt;li&gt;Brian Ripley has a &lt;a href=&quot;http://portal.stats.ox.ac.uk/userdata/ruth/APTS2012/Rcourse5.pdf&quot; rel=&quot;nofollow&quot;&gt;set of slides on simulation and optimisation in R&lt;/a&gt;. &#10;In particular it provides a useful discussion of the various optimisation&#10;algorithms available using &lt;code&gt;optim&lt;/code&gt;&quot;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt; &#10; &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-07-01T02:37:22.900" Id="12532" LastActivityDate="2013-07-10T16:39:39.907" LastEditDate="2013-07-10T16:39:39.907" LastEditorUserId="442" OwnerUserId="183" ParentId="12525" PostTypeId="2" Score="16" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two dataset that i want to compare.&#10;each dataset contain the weight of 10 different person measured for 3 different day.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested  in measuring the probabily that the two sample originate from the same population.&lt;/p&gt;&#10;&#10;&lt;p&gt;People seem to suggest doing a Kolmogorov-Smirnov test but i need a measurement.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking doing the EMD to compare the distribution for each day &lt;/p&gt;&#10;&#10;&lt;p&gt;EMD(dataset1-day1,dataset2-day1) + EMD(dataset1-day2,dataset2-day2) + EMD(dataset1-day3,dataset2-day3) &#10;But i could probably take each person as a 3d datapoint and do the EMD in 3d.&lt;/p&gt;&#10;&#10;&lt;p&gt;One other possibility was to do the Hausdorff distance but doing the average of the distance for each point instead of taking the maximum distance.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the main difference between the two technique.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-01T01:58:21.050" FavoriteCount="0" Id="12535" LastActivityDate="2011-07-01T04:04:58.323" OwnerDisplayName="skyde" OwnerUserId="5244" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="Measuring probability that 2 sample originate from the same population" ViewCount="159" />
  <row Body="&lt;p&gt;Winsorization replaces extreme data values with less extreme values.&#10;&lt;a href=&quot;http://www.r-bloggers.com/winsorization/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-bloggers.com/winsorization/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-01T06:09:23.313" Id="12536" LastActivityDate="2011-07-01T06:09:23.313" OwnerUserId="1709" ParentId="12498" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Take a look at the &lt;a href=&quot;http://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html&quot; rel=&quot;nofollow&quot;&gt;(HMM) Toolbox for Matlab by Kevin Murphy&lt;/a&gt; and also section &lt;strong&gt;Recommended reading on HMMs&lt;/strong&gt; on this site.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also get &lt;a href=&quot;http://code.google.com/p/pmtk3/&quot; rel=&quot;nofollow&quot;&gt;Probabilistic modeling toolkit for Matlab/Octave&lt;/a&gt; with some examples of using Markov Chains and HMM.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also find lectures and labs on HMM, for example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;ftp://ftp.idiap.ch/pub/OLD/sacha/labs/&quot; rel=&quot;nofollow&quot;&gt;Labs&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.stanford.edu/class/cs229/section/cs229-hmm.pdf&quot; rel=&quot;nofollow&quot;&gt;Lecture1&lt;/a&gt; and &lt;a href=&quot;http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf&quot; rel=&quot;nofollow&quot;&gt;Lecture2&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-07-01T06:27:39.323" CreationDate="2011-07-01T06:27:39.323" Id="12537" LastActivityDate="2011-07-01T06:27:39.323" OwnerUserId="5172" ParentId="3294" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The transform that most evens things out is the rank transform (just replace the data by the ranks).  If there are no ties then the result is uniform.&lt;/p&gt;&#10;&#10;&lt;p&gt;If data is fairly normal (bell shaped) then the inverse of the normal distribution will spread towards uniform.  Actually any s-shaped curve will tend to do this including the arctangent and inverse logit (center and choose an appropriate scale first).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-01T16:33:24.643" Id="12550" LastActivityDate="2011-07-01T16:33:24.643" OwnerUserId="4505" ParentId="12549" PostTypeId="2" Score="2" />
  
&#10; &amp;amp; \y - \X \b \geq - t \ones \&amp;gt;, \\
&#10;\end{array}
&#10;\text{minimize} &amp;amp; \t^T \ones \\
  <row AnswerCount="0" Body="&lt;p&gt;I'm looking for a large dataset (~20k nodes,~100k edges) for a geometric network, like utility networks, etc (&lt;a href=&quot;http://gis.stackexchange.com/questions/6885/difference-between-network-dataset-and-geometric-network&quot;&gt;See&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, at a minimum I would like a  dataset that includes identified 'nodes' with geographic coordinates and a connectivity arrangement between the nodes.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a bonus, some timestamped quantities that are measured between the nodes would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been looking in all the regular places (data.gov, data4nr.net, d8taplex.com, data.gov.uk) but I can't seem to find something that could meet all this criteria.  I know I am asking too much but would like to know if anyone is aware of an intermediate dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-07-02T01:24:02.433" FavoriteCount="1" Id="12570" LastActivityDate="2011-07-08T00:34:12.677" LastEditDate="2011-07-08T00:34:12.677" LastEditorUserId="5257" OwnerUserId="5257" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;spatial&gt;&lt;networks&gt;&lt;spatio-temporal&gt;" Title="Searching for geometric network dataset" ViewCount="113" />
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I conducted a t-test / ANOVA (both repeated measurements) and I want to represent the difference in the mean via a bar graph. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are several different views about the appropriate error bars for repeated-measurements: personal preference (Field, 2000), &lt;a href=&quot;http://www.springerlink.com/content/l2n37n5724024227/&quot; rel=&quot;nofollow&quot;&gt;Root Mean Square Error&lt;/a&gt; (Estes, 1997) or rather &lt;a href=&quot;http://www.lrdc.pitt.edu/schunn/SSB/&quot; rel=&quot;nofollow&quot;&gt;Statistical Significance Bars&lt;/a&gt; (Schunn, 1999).&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best solution?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-03T07:59:00.110" FavoriteCount="3" Id="12588" LastActivityDate="2011-07-04T02:41:08.897" LastEditDate="2011-07-03T16:06:34.250" LastEditorUserId="930" OwnerUserId="5267" PostTypeId="1" Score="4" Tags="&lt;standard-error&gt;" Title="Appropriate error bars for repeated-measurements designs" ViewCount="1087" />
  <row AcceptedAnswerId="17833" AnswerCount="2" Body="&lt;p&gt;This question is in some way similar to &lt;a href=&quot;http://stats.stackexchange.com/questions/3589/correlation-between-two-variables-of-unequal-size&quot;&gt;this one&lt;/a&gt;, but about another nuance.&#10;I have two time series (update: stationary - with both mean and variance equal over time) with missing values in one of them, such as&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;1,2,3&lt;/li&gt;&#10;&lt;li&gt;1,Absent,3&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(in reality I have many more observations). I want to compute their correlation, so I will not use the 2nd time-point in these data. But should I use it for computing the mean of the first series?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think that including this point in computing the mean will lead to a more precise estimate of the population mean for the first variable, but don't know whether it is correct though.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-07-03T17:55:49.413" FavoriteCount="2" Id="12597" LastActivityDate="2011-11-02T16:46:10.450" LastEditDate="2011-11-01T20:43:52.313" LastEditorUserId="5271" OwnerUserId="5271" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;correlation&gt;" Title="Mean when computing correlation between samples of unequal size" ViewCount="2106" />
  <row AcceptedAnswerId="12603" AnswerCount="2" Body="&lt;p&gt;What are some applications of Chinese restaurant processes?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to learn a bit about non-parametric Bayesian methods, starting with Dirichlet processes and CRPs, but all the tutorials I've found are about theory, without describing any applications in depth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Names of papers would be great. I'm not really looking for state-of-the-art applications, but just some &quot;canonical&quot; examples (say, in natural language processing) of why Dirichlet processes and Chinese restaurant processes are useful and why I should care.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-03T20:42:33.997" FavoriteCount="2" Id="12599" LastActivityDate="2011-07-04T17:08:40.403" LastEditDate="2011-07-04T06:41:58.703" LastEditorUserId="88" OwnerUserId="1106" PostTypeId="1" Score="5" Tags="&lt;nonparametric-bayes&gt;" Title="What are some applications of Chinese restaurant processes?" ViewCount="576" />
  
  <row Body="&lt;p&gt;R is good and can be freely downloaded from &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-project.org/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;R takes some time to get used to, but here's a simple example (&quot;#&quot; indicates that a comment follows):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rnorm(20)  # generate a sample of size 20 from N(0,1)&#10;y &amp;lt;- 10^x  # define y_i = 10^(x_i) for each i=1,...,20&#10;plot(x, y)  # plot y vs x&#10;plot(x, y, log=&quot;y&quot;)  # plot y vs x with log scale for y&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Edited after comment from @chl&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-04T11:55:23.640" Id="12616" LastActivityDate="2011-07-04T12:26:11.963" LastEditDate="2011-07-04T12:26:11.963" LastEditorUserId="3835" OwnerUserId="3835" ParentId="12615" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="12626" AnswerCount="1" Body="&lt;p&gt;I used my training dataset to fit cluster using kmenas function&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit &amp;lt;- kmeans(ca.data, 2);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I use fit object to predict cluster membership in a new dataset?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-04T14:32:46.033" FavoriteCount="1" Id="12623" LastActivityDate="2011-07-04T15:16:53.970" LastEditDate="2011-07-04T15:16:53.970" LastEditorUserId="88" OwnerUserId="333" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;" Title="Predicting cluster of a new object with kmeans in R" ViewCount="1492" />
  
  <row Body="&lt;p&gt;For a quick and easy graph, you should give &lt;a href=&quot;http://graphcalc.com&quot; rel=&quot;nofollow&quot;&gt;GraphCalc&lt;/a&gt; a try.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-04T16:51:36.597" Id="12631" LastActivityDate="2011-07-04T16:51:36.597" OwnerUserId="4754" ParentId="12615" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;In Information Retrieval research, an experiment is often repeated several times (in system-oriented IR e.g. about 50 times with slightly varying data). In that case the solution is normally to establish some measure of central tendency (e.g. arithmetic mean) and do the significance test on that. Your population is human beings so you would want to come up with a measure for a single human being, which, in your case, could be the mean answer.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-05T05:12:05.167" Id="12652" LastActivityDate="2011-07-05T05:12:05.167" OwnerUserId="1048" ParentId="12587" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Rerun your clusterization to save the final cluster centers as .SAV data file (check &quot;Write final&quot;). Then you open the data file with new objects to classify (this dataset may contain only new objects or a mix of new and old objects - it will make no difference). Check &quot;Read initial&quot; and choose here that saved file with cluster centers. Check &quot;Classify only&quot; instead of &quot;Iterate and classify&quot;. Order to save cluster memberships under &quot;Save&quot; button. Run.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-05T07:24:35.493" Id="12655" LastActivityDate="2011-07-05T07:24:35.493" OwnerUserId="3277" ParentId="12648" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;How do you do normalization so that when I get the mean/variance, the flat values wont affect the results? For example, in the figure below, Graph 1 &amp;amp; 2 are both considered to be noisy as compared to Graph 3. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BqGcq.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as you can see, each of their flat intensity values vary from each other, so it could be possible that a 'clean' graph that contains very high intensity values can be considered 'noisy' as well as a 'noisy graph that contains low intensity values can be considered as 'clean'. I am using mean/variance and a threshold for determining if it is noisy or not, hence the need to 'normalize' the values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ive tried to do, getting the total and dividing each intensity to the total (e.g. arr[x] = arr[x] / total) but it does not work properly that way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips? Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-05T11:10:28.843" Id="12661" LastActivityDate="2011-07-05T14:18:20.093" OwnerUserId="5290" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;mean&gt;&lt;normalization&gt;" Title="How to do 'normalization'?" ViewCount="838" />
  
&#10;s^2 = \frac{1}{N}\sum_{i=1}^N{(y_i - \bar{y})^2}.
  <row AcceptedAnswerId="12692" AnswerCount="3" Body="&lt;p&gt;EDIT: The &lt;a href=&quot;http://cran.r-project.org/web/views/WebTechnologies.html&quot;&gt;Web Technologies and Services&lt;/a&gt; CRAN &lt;a href=&quot;http://cran.r-project.org/web/views/&quot;&gt;task view&lt;/a&gt; contains a much more comprehensive list of data sources and APIs available in R.  You can &lt;a href=&quot;https://github.com/ropensci/webservices&quot;&gt;submit a pull request on github&lt;/a&gt; if you wish to add a package to the task view.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I'm making a list of the various data feeds that are already hooked into R or that are easy to setup. Here's my initial list of packages, and I was wondering what else I'm missing.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to limit this list to &quot;real time&quot; or &quot;close to real time&quot; data feeds/APIs, where the underlying data might change between downloads.  There's plenty of &lt;a href=&quot;http://archive.ics.uci.edu/ml/&quot;&gt;lists out there&lt;/a&gt; for static datasets, which only require one download.&lt;/p&gt;&#10;&#10;&lt;p&gt;This list is currently biased towards financial/time series data, and I could use some help expanding it into other domains.&lt;/p&gt;&#10;&#10;&lt;p&gt;Free Data:&lt;br&gt;&#10;Data Source -  Package&lt;br&gt;&#10;&lt;a href=&quot;http://www.google.com/finance/historical?q=NASDAQ%3aMSFT&quot;&gt;Google Finance historical data&lt;/a&gt; - &lt;a href=&quot;http://www.quantmod.com/&quot;&gt;quantmod&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.google.com/finance?q=NASDAQ%3aMSFT&amp;amp;fstype=ii&quot;&gt;Google Finance balance sheets&lt;/a&gt; - quantmod&lt;br&gt;&#10;&lt;a href=&quot;http://finance.yahoo.com/q/hp?s=MSFT%20Historical%20Prices&quot;&gt;Yahoo Finance historical data&lt;/a&gt; - quantmod&lt;br&gt;&#10;Yahoo Finance historical data - &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/tseries/html/get.hist.quote.html&quot;&gt;tseries&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://finance.yahoo.com/q/op?s=MSFT&quot;&gt;Yahoo Finance current options chain&lt;/a&gt; - quantmod&lt;br&gt;&#10;&lt;a href=&quot;http://finance.yahoo.com/q/ud?s=MSFT&quot;&gt;Yahoo Finance historical analyst estimates&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/fImport/index.html&quot;&gt;fImport&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://finance.yahoo.com/q/ks?s=MSFT&quot;&gt;Yahoo Finance current key stats&lt;/a&gt; - fImport - seems to be broken&lt;br&gt;&#10;&lt;a href=&quot;http://www.oanda.com/&quot;&gt;OANDA historic exchange rates/metal prices&lt;/a&gt; - quantmod&lt;br&gt;&#10;&lt;a href=&quot;http://research.stlouisfed.org/fred2/&quot;&gt;FRED historic macroeconomic indicators&lt;/a&gt; - quantmod&lt;br&gt;&#10;&lt;a href=&quot;http://data.worldbank.org/indicator&quot;&gt;World Bank historic  macroeconomic indicators&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/WDI/index.html&quot;&gt;WDI&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.google.com/trends?q=microsoft&quot;&gt;Google Trends historic search volume data&lt;/a&gt; - &lt;a href=&quot;http://www.omegahat.org/RGoogleTrends/&quot;&gt;RGoogleTrends&lt;/a&gt;&lt;br&gt;&#10;Google Docs - &lt;a href=&quot;http://www.omegahat.org/RGoogleDocs/&quot;&gt;RGoogleDocs&lt;/a&gt;&lt;br&gt;&#10;Google Storage - &lt;a href=&quot;http://www.omegahat.org/RGoogleStorage/&quot;&gt;RGoogleStorage&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://twitter.com/#!/search/microsoft&quot;&gt;Twitter&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/twitteR/index.html&quot;&gt;twitteR&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.zillow.com/&quot;&gt;Zillow&lt;/a&gt; - &lt;a href=&quot;http://www.omegahat.org/Zillow/&quot;&gt;Zillow&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.nytimes.com/&quot;&gt;New York Times&lt;/a&gt; - &lt;a href=&quot;http://www.omegahat.org/RNYTimes/&quot;&gt;RNYTimes&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.census.gov/main/www/cen2000.html&quot;&gt;US Census 2000&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/UScensus2000/index.html&quot;&gt;UScensus2000&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.infochimps.com/&quot;&gt;infochimps&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/infochimps/index.html&quot;&gt;infochimps&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://datamarket.com/&quot;&gt;datamarket&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/rdatamarket/index.html&quot;&gt;rdatamarket&lt;/a&gt; - requires free account&lt;br&gt;&#10;&lt;a href=&quot;http://www.factual.com/&quot;&gt;Factual.com&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/factualR/index.html&quot;&gt;factualR&lt;/a&gt;&lt;br&gt;&#10;Geocode addresses - &lt;a href=&quot;http://thelogcabin.wordpress.com/2011/05/02/r-and-the-data-science-toolkit/&quot;&gt;RDSTK&lt;/a&gt;&lt;br&gt;&#10;Map coordinates to political boundaries - RDSTK&lt;br&gt;&#10;&lt;a href=&quot;http://www.wunderground.com/&quot;&gt;Weather Underground&lt;/a&gt; - &lt;a href=&quot;http://casoilresource.lawr.ucdavis.edu/drupal/node/991&quot;&gt;Roll your own&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.google.com/finance/company_news?q=NASDAQ%3aMSFT&quot;&gt;Google News&lt;/a&gt; - &lt;a href=&quot;http://stackoverflow.com/questions/5761576/improving-a-function-to-get-stock-news-data-from-google-in-r&quot;&gt;Roll your own&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://opendap.deltares.nl/thredds/catalog/opendap/catalog.html&quot;&gt;Earth Sciences&lt;/a&gt; netCDF &lt;a href=&quot;http://www.unidata.ucar.edu/software/netcdf/examples/files.html&quot;&gt;Data&lt;/a&gt; - &lt;a href=&quot;http://public.deltares.nl/display/OET/KML+overview+of+OPeNDAP+data#KMLoverviewofOPeNDAPdata-AccessingnetCDF/OPeNDAPdatawithR&quot;&gt;Roll your own&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://processtrends.com/RClimate.htm&quot;&gt;Climate Data&lt;/a&gt; - &lt;a href=&quot;http://chartsgraphs.wordpress.com/2011/01/24/using-rclimate-to-retrieve-climate-series-data/&quot;&gt;Roll your own&lt;/a&gt;&lt;br&gt;&#10;Public health data - &lt;a href=&quot;http://www.medepi.net/msamuel/R.Visual.Display.Samuel.10-2008.pdf&quot;&gt;Roll your own&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://cran.r-project.org/web/packages/OAIHarvester/index.html&quot;&gt;OAI Harvester&lt;/a&gt; - Open Archives Initiative harvester&lt;br&gt;&#10;&lt;a href=&quot;http://www.omegahat.org/RAmazonS3/&quot;&gt;RAmazonS3&lt;/a&gt; -  S3 Amazon storage server&lt;br&gt;&#10;&lt;a href=&quot;http://www.omegahat.org/Rflickr/&quot;&gt;Rflikr&lt;/a&gt; - Flikr api    &lt;/p&gt;&#10;&#10;&lt;p&gt;Requires a subscription:&lt;br&gt;&#10;&lt;a href=&quot;http://www.bloomberg.com/professional/software_support/&quot;&gt;Bloomberg&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/RBloomberg/index.html&quot;&gt;RBloomberg&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.lim.com/&quot;&gt;LIM&lt;/a&gt; - &lt;a href=&quot;http://www.lim.com/sites/default/files/R_package_LIMWS.pdf&quot;&gt;LIM&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.nyxdata.com/Data-Products/Daily-TAQ&quot;&gt;Trades and Quotes from NYSE&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/RTAQ/index.html&quot;&gt;RTAQ&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.interactivebrokers.com/en/p.php?f=marketData&quot;&gt;Interactive Brokers&lt;/a&gt; - &lt;a href=&quot;http://cran.r-project.org/web/packages/IBrokers/index.html&quot;&gt;IBrokers&lt;/a&gt;   &lt;/p&gt;&#10;" CommentCount="7" CommunityOwnedDate="2011-07-05T22:16:55.993" CreationDate="2011-07-05T14:31:00.900" FavoriteCount="49" Id="12670" LastActivityDate="2014-10-15T03:27:09.367" LastEditDate="2014-04-14T17:36:37.713" LastEditorUserId="2817" OwnerUserId="2817" PostTypeId="1" Score="47" Tags="&lt;r&gt;&lt;dataset&gt;&lt;references&gt;&lt;big-list&gt;" Title="Data APIs/feeds available as packages in R" ViewCount="9518" />
  <row AcceptedAnswerId="12693" AnswerCount="4" Body="&lt;p&gt;Our lab used to have a program called GS+ that let us make semi variograms on our data and analyse them. Unfortunately the licence has expired.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there another piece of software that lets you do this? If so, how specifically? We prefer a Free and open source licence, but all suggestions are welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2011-07-05T21:24:45.850" CreationDate="2011-07-05T20:25:43.703" FavoriteCount="1" Id="12681" LastActivityDate="2011-07-06T00:37:17.370" LastEditDate="2011-07-05T21:23:06.600" LastEditorUserId="919" OwnerUserId="2830" PostTypeId="1" Score="3" Tags="&lt;data-visualization&gt;&lt;multivariate-analysis&gt;&lt;spatial&gt;&lt;big-list&gt;" Title="Software for making semi variograms and analyses?" ViewCount="1905" />
  <row Body="&lt;p&gt;If Truth/Lie and color are independent and everything is random, then you would have an 18% chance of an honest person seeing blue and a 28% chance of a lier seeing yellow (and therefore claiming to see blue), you also have a 12% chance of lier seeing blue and 42% chance of honest seeing yellow.  So the probability of an honest person seeing blue given that the person claimed to see blue is $\frac{0.18}{0.18+0.28} = 0.39$.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-07-05T20:59:57.837" Id="12684" LastActivityDate="2011-07-05T20:59:57.837" OwnerUserId="4505" ParentId="12678" PostTypeId="2" Score="1" />
&#10;\begin{cases} 
  
  <row AcceptedAnswerId="12718" AnswerCount="2" Body="&lt;p&gt;I have a large dataset and have performed a multilevel regression in Stata, the model is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;xtmixed dependent independen1 independent2 independent3  independent4    || independendt5:&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So there is one grouping factor: &lt;code&gt;independent5&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In R I did the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(dependent ~ independent1 + indepdendent2 + independent3 + independent4 + 1 | independent5, REML=TRUE) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A few questions: is this identical?&lt;/p&gt;&#10;&#10;&lt;p&gt;Stata output gives me number of groups, 100, while R gives number of groups as 99.&#10;Furthermore, the variances and standard deviations are not the same.&#10;Also I would like to know how to obtain p values and coefficients from the R ouput.&#10;I have done &lt;code&gt;fixef(model)&lt;/code&gt; and &lt;code&gt;ranef(model)&lt;/code&gt; but when I do &lt;code&gt;coef(model)&lt;/code&gt; it says: &lt;code&gt;Error in coef(model) : unable to align random and fixed effects&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also Stata only gives me 1 coefficient for each predictor while the R &lt;code&gt;fixef(model)&lt;/code&gt; gives one for each group. &lt;/p&gt;&#10;&#10;&lt;p&gt;So someone familiar with both Stata and R could help me with this.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-07-06T12:38:53.277" FavoriteCount="3" Id="12709" LastActivityDate="2011-07-06T16:01:52.827" LastEditDate="2011-07-06T12:53:53.063" LastEditorUserId="1036" OwnerDisplayName="DBR" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;stata&gt;&lt;multilevel-analysis&gt;&lt;lmer&gt;" Title="Multilevel regression using lmer function in R and Stata" ViewCount="2211" />
  <row Body="&lt;p&gt;There is nothing in the theory behind regression models that requires any distribution for X other than having a minimum number of observations in each range of X for which you want to learn something.  The only problem you usually run into is overly influential observations due to a heavy right tail of the distribution of X.  To deal with that I often fit something like a restricted cubic spline in the cube root or square root of X.  In the R &lt;strong&gt;rms&lt;/strong&gt; package this would look like &lt;code&gt;y ~ rcs(x^(1/3)) + ...&lt;/code&gt; other variables or &lt;code&gt;rcs(sqrt(x),5) + ...&lt;/code&gt; (5=5 knots using default knot placement).  That way you only assume a smooth relationship but you limit the influence of large values, while allowing for zeros (though not negative values).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-06T15:26:07.927" Id="12716" LastActivityDate="2011-07-06T15:38:26.320" LastEditDate="2011-07-06T15:38:26.320" LastEditorUserId="1390" OwnerUserId="4253" ParentId="12715" PostTypeId="2" Score="15" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to know how to estimate a population average model of a hierarchical logistic regression using &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;geepack&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;Stata&lt;/code&gt; code is: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xtlogit dep ind1 ind2 ind3, i(ind4) pa&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to reproduce this in &lt;code&gt;R&lt;/code&gt; using &lt;code&gt;geepack&lt;/code&gt; or any other method.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-06T23:03:44.740" Id="12743" LastActivityDate="2013-09-02T14:36:21.880" LastEditDate="2013-09-02T14:36:21.880" LastEditorDisplayName="DBR" LastEditorUserId="21599" OwnerDisplayName="DBR" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;stata&gt;&lt;lmer&gt;&lt;gee&gt;&lt;population-average&gt;" Title="Estimating population average models in lmer or geepack" ViewCount="543" />
  
  <row AcceptedAnswerId="12757" AnswerCount="2" Body="&lt;p&gt;Last night I started a complex calculation with gamm() and it took me... &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;     user        system       elapsed &#10;    9259.76      326.05     9622.64 (s)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;...meaning it took me 160 minutes or 2.67 hours for that calculation. The problem is that I have to do around 50 or even 100 more of these! So I was wondering if there is any way that could speed up these calculations. I compared the 32bit with the 64bit version (4gb) and R 2.12.2 to calculate a less complex gamm().  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;32bit solution&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; User      System        elapsed &#10; 41.87        0.01       42.01&#10;&lt;/code&gt;&lt;/pre&gt;&#10;  &#10;  &lt;p&gt;64 bit solution&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  User      System      elapsed&#10; 40.06        2.82       43.05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;but it took even longer using 64bit!  &lt;/p&gt;&#10;&#10;&lt;p&gt;My question now: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Would it help to simply buy more ram, for example 8GB DDR3? or would that be a waste of money? Or would the compiler package in R 2.13.0 be able to handle that properly? I do not think that rcpp can handle gamm() functions, or am I wrong?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;any comments welcome!&lt;/p&gt;&#10;&#10;&lt;p&gt;the gamm() model call for the 160min process was: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  g1 &amp;lt;- gamm(CountPP10M    ~ s(tempsurf,bs=&quot;cr&quot;) &#10;                           + s(salsurf,bs=&quot;cr&quot;) &#10;                           + s(speedsurf,bs=&quot;cr&quot;) &#10;                           + s(Usurf,bs=&quot;cr&quot;)&#10;                           + s(Vsurf,bs=&quot;cr&quot;)   &#10;                           + s(Wsurf,bs=&quot;cr&quot;)&#10;                           + s(water_depth,bs=&quot;cr&quot;)&#10;                           + s(distance.to.bridge,bs=&quot;cr&quot;)&#10;                           + s(dist_land2,bs=&quot;cr&quot;)&#10;                           + s(Dist_sventa,bs=&quot;cr&quot;),&#10;                           data=data,&#10;                           random=list(ID_Station=~1),&#10;                           family=poisson,&#10;                           methods=&quot;REML&quot;,&#10;                           control=lmc)&#10;            )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="9" CreationDate="2011-07-07T07:17:32.013" FavoriteCount="1" Id="12756" LastActivityDate="2011-07-07T10:01:43.750" LastEditDate="2011-07-07T08:35:30.640" LastEditorUserId="88" OwnerUserId="5280" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;computing&gt;" Title="How to make R's gamm work faster? " ViewCount="836" />
  
  
  <row Body="&lt;p&gt;If there is the possibility that something may belong to more than one class, another approach is to train N classifiers with the '-b 1' flag (to enable probability estimates). You will then get confidence levels of each data point belonging to each classifier. However, there's still the question of what threshold to use. If you want to get around the problem of picking the 'best' threshold, you can use 11-pt Mean Average Precision. This measures the AP for threshold values [0.0, 0.1, 0.2, ..., 1.0] (thus the 11 pt).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-07T18:24:41.640" Id="12774" LastActivityDate="2011-07-07T18:24:41.640" OwnerUserId="5268" ParentId="12068" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="4" Body="&lt;h3&gt;Context&lt;/h3&gt;&#10;&#10;&lt;p&gt;I have 25 men and 25 women as participants, and they did exactly the same thing: Each of them&#10;heard an attractive dialogue and they had to choose between a photo of a&#10;woman in red and a woman in green. Then each of them heard an unattractive&#10;dialogue and they chose again between the red and the green shirt. &lt;/p&gt;&#10;&#10;&lt;p&gt;My hypothesis is that men are attracted to women in red in contrast to women. So, I 've got gender (2levels:0=male, 1=female), attraction (1=yes, 0=no) and&#10;trials/colour (0=green, 1=red)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in showing that the colour (red) predicts attraction as far as men&#10;are concerned and that there are gender differences! Men are much more&#10;attracted to it. &lt;/p&gt;&#10;&#10;&lt;h3&gt;Questions&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Should I use a test which combine all of them or a test that combine first&#10;the attraction and colour for men, then the attraction and colour for women&#10;and then the gender differences?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I've been told to run a pearson's correlation (one for men and one for women)&#10;but I think that it demands interval data.I was also told to use the chi-&#10;square test but it's not for participants who parrticipated in the same&#10;experimental conditions. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What about repeated measures logistic regression? &#10;If so, can you give me some advice how to process my data? Any suggestions? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;P.S. I really appreciate everyone who answered my previous posts &#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/12587/should-i-use-repeated-measures-anova-or-which-other-spss-test-should-i-use&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/12453/what-test-do-i-use-in-order-to-analyze-a-within-participants-repeated-measure-exp&quot;&gt;here&lt;/a&gt;, they helped me to go one step further!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-08T02:45:35.777" Id="12790" LastActivityDate="2011-07-08T14:51:00.000" LastEditDate="2011-07-08T11:22:34.923" LastEditorUserId="88" OwnerUserId="5218" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;spss&gt;" Title="What's the appropriate spss test?" ViewCount="1017" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like to know if it is useful (or maybe dangerous) to reduce the number of attributes (by selecting the most informative ones among thousands) before seeking for latent variables or not (in an exploratory perspective).&lt;/p&gt;&#10;&#10;&lt;p&gt;A subsidiary question: in the same case, would it be beneficial to select the most important features for each categorie of features (these can be compressed using an entity-attribute-value model which is not really suitable for data mining) before detecting the latent variables?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-08T13:41:58.733" FavoriteCount="1" Id="12806" LastActivityDate="2011-07-08T17:16:18.483" LastEditDate="2011-07-08T14:18:24.740" LastEditorUserId="88" OwnerUserId="5330" PostTypeId="1" Score="3" Tags="&lt;feature-selection&gt;&lt;latent-variable&gt;" Title="Feature selection and latent variables" ViewCount="389" />
  <row AnswerCount="2" Body="&lt;p&gt;I plot something to make a point to myself or someone else.  Usually a question starts this process, and often the person asking hopes for a particular answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I learn interesting things about the data in a less biased way?&lt;/p&gt;&#10;&#10;&lt;p&gt;Right now I'm roughly following this method:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;summary statistics&lt;/li&gt;&#10;&lt;li&gt;stripchart&lt;/li&gt;&#10;&lt;li&gt;scatter plot&lt;/li&gt;&#10;&lt;li&gt;maybe repeat with an interesting subset of data&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;But that doesn't seem methodical or scientific enough.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there guidelines or procedures to follow that reveal things about the data I wouldn't think to ask?  How do I know when I have done an adequate analysis?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-08T22:37:15.973" FavoriteCount="3" Id="12822" LastActivityDate="2013-03-17T06:49:22.983" LastEditDate="2011-07-09T10:06:01.960" LastEditorUserId="88" OwnerUserId="5335" PostTypeId="1" Score="7" Tags="&lt;best-practices&gt;&lt;knowledge-discovery&gt;" Title="Guidelines for discovering new knowledge in data" ViewCount="186" />
  <row AcceptedAnswerId="12989" AnswerCount="2" Body="&lt;p&gt;I'm interested in an estimator of the standard deviation in a Poisson regression. So the variance is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$Var(y)=\phi\cdot V(\mu)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\phi=1$ and $V(\mu)=\mu$. So the variance should be $Var(y)=V(\mu)=\mu$. (I'm just interested in how the variance should be, so if overdispersion occurs ($\widehat{\phi}\neq 1$), I don't care about it). Thus an estimator of the variance should be &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\widehat{Var}(y)=V(\widehat{\mu})=\widehat{\mu}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and an estimator of the standard deviation should be &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt{\widehat{Var}(y)}=\sqrt{V(\widehat{\mu})}=\sqrt{\widehat{\mu}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct? I haven't found a discussion about standard deviation in the context with Poisson regression yet, that's why I'm asking.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So here is an easy example (which makes no sense btw) of what I'm talking about. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data1 &amp;lt;- function(x) {x^(2)}&#10;numberofdrugs &amp;lt;- data1(1:84)&#10;data2 &amp;lt;- function(x) {x}   &#10;healthvalue &amp;lt;- data2(1:84)&#10;plot(healthvalue, numberofdrugs)&#10;test &amp;lt;- glm(numberofdrugs ~ healthvalue, family=poisson)&#10;summary(test) #beta0=5.5 beta1=0.042&#10;mu &amp;lt;- function(x) {exp(5.5+0.042*x)}&#10;plot(healthvalue, numberofdrugs)&#10;curve(mu,  add=TRUE, col=&quot;purple&quot;, lwd=2)&#10;# the purple curve is the estimator for mu and it's also &#10;# the estimator of the variance,but if I'd like to plot &#10;# the (not constant) standard deviation I just take the &#10;# square root of the variance. So it is var(y)=mu=exp(Xb) &#10;# and thus the standard deviation is sqrt(exp(Xb))&#10;sd &amp;lt;- function(x) {sqrt(exp(5.5+0.042*x))}&#10;curve(sd, col=&quot;green&quot;, lwd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is the the green curve the correct estimator of the standard deviation in a Poisson regression? It should be, no?&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2011-07-09T08:51:15.067" FavoriteCount="0" Id="12826" LastActivityDate="2011-07-13T14:53:14.843" LastEditDate="2011-07-13T14:29:03.643" LastEditorUserId="919" OwnerUserId="4496" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;standard-deviation&gt;&lt;poisson&gt;&lt;generalized-linear-model&gt;" Title="Estimating standard deviation in Poisson regression" ViewCount="595" />
&#10;&amp;amp;+&amp;amp; 1 &amp;amp;\cdot &amp;amp;(-1)&amp;amp;\cdot &amp;amp;P(X=1,Y=-1) \\
  
  
  
  
  
  
  <row AcceptedAnswerId="12991" AnswerCount="4" Body="&lt;p&gt;My understanding is that R squared cannot be negative as it is the square of R. However I ran a simple linear regression in SPSS with a single independent variable and a dependent variable. My SPSS output give me a negative value for R-squared. If I was to calculate this by hand from R then R squared would be &#10;positive. What has SPSS done to calculate this as negative? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R=-.395&#10;R squared =-.156&#10;B (un-standardized)=-1261.611&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Code I've used:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DATASET ACTIVATE DataSet1. &#10;REGRESSION /MISSING LISTWISE /STATISTICS COEFF OUTS R ANOVA &#10;           /CRITERIA=PIN(.05) POUT(.10) /NOORIGIN &#10;           /DEPENDENT valueP /METHOD=ENTER ageP&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get a negative value. Can anyone explain what this means?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks. &#10;&lt;img src=&quot;http://i.stack.imgur.com/fCCDi.png&quot; alt=&quot;Negative RSquared&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/h1cBS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2011-07-11T17:07:34.553" FavoriteCount="9" Id="12900" LastActivityDate="2015-01-04T22:22:00.833" LastEditDate="2011-07-16T05:22:39.133" LastEditorUserId="4498" OwnerUserId="4498" PostTypeId="1" Score="21" Tags="&lt;spss&gt;&lt;r-squared&gt;" Title="When is R squared negative?" ViewCount="26883" />
  <row AcceptedAnswerId="12915" AnswerCount="2" Body="&lt;p&gt;I'm trying to visualize a matrix of 3 datasets.&lt;sup&gt;*&lt;/sup&gt; For the sake of example, let's say I have a list of hats, coats and shoes, and I want to display a 2D grid/visualization of each possible combination.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it even possible without turning it into a 5x25 grid?&lt;/p&gt;&#10;&#10;&lt;p&gt;I had a look at this thread:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/11650/visualizing-multi-dimensional-data-lsi-in-2d&quot;&gt;Visualizing multi-dimensional data (LSI) in 2D&lt;/a&gt;, but from what I understand that one is about plotting as opposed to showing the fields in a grid/table of some sorts.&lt;/p&gt;&#10;&#10;&lt;p&gt;(*) I think i'm using the wrong terminology here, what I mean isn't about drawing points in a graph but rather a way to illustrate the data set in a meaningful matter that still allows for editing &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; This might explain what i'm trying to do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    +------+------------+-------------+------------+&#10;    |      | hat        | shirt       | shoe       |&#10;    +------+------------+-------------+------------+&#10;    | red  | red hat    | red shirt   | red shoe   |&#10;    +------+------------+-------------+------------+&#10;    | blue | blue hat   | blue shirt  | blue shoe  |&#10;    +------+------------+-------------+------------+&#10;    |green | green hat  | green shirt | green shoe |&#10;    +------+------------+-------------+------------+&#10;    |black | black hat  | black short | black shoe |&#10;    +------+------------+-------------+------------+&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;... but with 3 data sets instead of just colors and clothes.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-07-11T20:19:34.960" FavoriteCount="1" Id="12913" LastActivityDate="2011-07-11T22:25:34.680" LastEditDate="2011-07-11T21:45:39.263" LastEditorUserId="930" OwnerUserId="5359" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;multivariable&gt;" Title="Visualizing a 5x5x5 data set" ViewCount="167" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot;&gt;Fixed-effects ANOVA&lt;/a&gt; (or its linear regression equivalent) provides a powerful family of methods to analyze these data.  To illustrate, here is a dataset consistent with the plots of mean HC per evening (one plot per color):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       |              Color&#10;   Day |         B          G          R |     Total&#10;-------+---------------------------------+----------&#10;     1 |       117        176         91 |       384 &#10;     2 |       208        193        156 |       557 &#10;     3 |       287        218        257 |       762 &#10;     4 |       256        267        271 |       794 &#10;     5 |       169        143        163 |       475 &#10;     6 |       166        163        163 |       492 &#10;     7 |       237        214        279 |       730 &#10;     8 |       588        455        457 |     1,500 &#10;     9 |       443        428        397 |     1,268 &#10;    10 |       464        408        441 |     1,313 &#10;    11 |       470        473        464 |     1,407 &#10;    12 |       171        185        196 |       552 &#10;-------+---------------------------------+----------&#10; Total |     3,576      3,323      3,335 |    10,234 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;ANOVA of &lt;code&gt;count&lt;/code&gt; against &lt;code&gt;day&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; produces this table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                       Number of obs =      36     R-squared     =  0.9656&#10;                       Root MSE      =  31.301     Adj R-squared =  0.9454&#10;&#10;              Source |  Partial SS    df       MS           F     Prob &amp;gt; F&#10;          -----------+----------------------------------------------------&#10;               Model |  605936.611    13  46610.5085      47.57     0.0000&#10;                     |&#10;                 day |  602541.222    11  54776.4747      55.91     0.0000&#10;           colorcode |  3395.38889     2  1697.69444       1.73     0.2001&#10;                     |&#10;            Residual |  21554.6111    22  979.755051   &#10;          -----------+----------------------------------------------------&#10;               Total |  627491.222    35  17928.3206   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;model&lt;/code&gt; p-value of 0.0000 shows the fit is highly significant.  The &lt;code&gt;day&lt;/code&gt; p-value of 0.0000 is also highly significant: you can detect day to day changes.  However, the &lt;code&gt;color&lt;/code&gt; (semester) p-value of 0.2001 should not be considered significant: you cannot detect a systematic difference among the three semesters, &lt;em&gt;even after controlling for day to day variation.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Tukey%27s_range_test&quot;&gt;Tukey's HSD&lt;/a&gt; (&quot;honest significant difference&quot;) test identifies the following significant changes (among others) in day-to-day means (regardless of semester) at the 0.05 level:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 increases to 2, 3&#10;3 and 4 decrease to 5&#10;5, 6, and 7 increase to 8,9,10,11&#10;8, 9, 10, and 11 decrease to 12.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This confirms what the eye can see in the graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the graphs jump around quite a bit, there's no way to detect day-to-day correlations (serial correlation), which is the whole point of time series analysis.  In other words, don't bother with time series techniques: there's not enough data here for them to provide any greater insight.&lt;/p&gt;&#10;&#10;&lt;p&gt;One should always wonder how much to believe the results of any statistical analysis.  Various diagnostics for heteroscedasticity (such as the &lt;a href=&quot;http://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test&quot;&gt;Breusch-Pagan test&lt;/a&gt;) don't show anything untoward.  The residuals don't look very normal--they clump into some groups--so all the p-values have to be taken with a grain of salt.  Nevertheless, they appear to provide reasonable guidance and help quantify the sense of the data we can get from looking at the graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can carry out a parallel analysis on the daily minima or on the daily maxima.  Make sure to start with a similar plot as a guide and to check the statistical output.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-07-12T02:09:43.260" Id="12921" LastActivityDate="2011-07-12T09:54:06.470" LastEditDate="2011-07-12T09:54:06.470" LastEditorUserId="2116" OwnerUserId="919" ParentId="12902" PostTypeId="2" Score="7" />
&#10;p_A - p_B = -0.36 \\
  <row Body="&lt;p&gt;From the documentation of &lt;code&gt;?gls&lt;/code&gt;:&#10;&lt;code&gt;data: an optional data frame&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In relation, the documentation of &lt;code&gt;?read.spss&lt;/code&gt; shows a parameter &lt;code&gt;to.data.frame&lt;/code&gt;, whose default is &lt;code&gt;FALSE&lt;/code&gt;, so typically &lt;code&gt;reg1&lt;/code&gt; will be a list instead of a &lt;code&gt;data.frame&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code you provided is not useful for testing on my system (as you do not provide the actual dataset), but this will probably work (disregarding library statements):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reg1 &amp;lt;- read.spss(&quot;URL of the spss file&quot;, to.data.frame=TRUE)&#10;class(reg1) #check that it is indeed a data.frame&#10;colnames(reg1) #check that all column names are properly imported&#10;model1 &amp;lt;- gls(v1 ~ x1 + x2 + x3 + x4 + x5 + x6, correlation=corARMA(p=1), data=reg1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-07-12T11:42:42.993" Id="12937" LastActivityDate="2011-07-12T11:42:42.993" OwnerUserId="4257" ParentId="12936" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;In general, what you want is a covariance matrix.&#10;In MatLab, this is relatively simple to implement. The function corrcoef (correlation coefficient) will tell you the correlation between every pair of variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;You will want an nx11 matrix, where n is the number of instances (one per customer) and 11 is your number of variables (6 customer variables, 3 campaign variables, and 2 results)&lt;/p&gt;&#10;&#10;&lt;p&gt;Corrcoef will multiply (and normalize) M'M to get you an 11x11 result. The diagonal of this matrix will just be how the i'th variable correlates with itself, namely, a diagonal of 1's. Any i,j entry in the correlation matrix will represent how variable i correlates to variable j. Answers close to 1 mean very high correlation. Answers close to -1 mean anti-correlation. Values near 0 mean no correlation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-12T14:12:49.993" Id="12941" LastActivityDate="2011-07-12T14:12:49.993" OwnerUserId="5368" ParentId="12924" PostTypeId="2" Score="0" />
&#10;E_p[X] &amp;amp;= (1-p)^2X(\alpha,\alpha) +(1-p)pX(\alpha,\beta) + p(1-p)X(\beta,\alpha)  + p^2X(\beta,\beta) \\
&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This expectation is zero for all $p$.  However, because $X(\alpha,\beta) \ne 0$, $X$ will be nonzero with positive probability whenever $p \notin \{0,1\}$, showing the model of sampling with replacement is incomplete.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The point (intuitively)&lt;/strong&gt; is that when $n \gt 1$, sampling with replacement is a multidimensional situation but you only have a one-dimensional family of probability laws available.  You cannot represent all the possible multidimensional probability measures by means of the one-dimensional family you have.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-07-12T15:06:11.013" Id="12942" LastActivityDate="2011-07-12T18:06:15.477" LastEditDate="2011-07-12T18:06:15.477" LastEditorUserId="919" OwnerUserId="919" ParentId="12935" PostTypeId="2" Score="5" />
&#10; 2 &amp;amp; 1 \\
&#10; 16 &amp;amp; 1
  
  <row Body="&lt;p&gt;An easy approach would be to approximate the situation by a model where every eligible member $n$ (i.e., every member that didn't win in the previous week) has a fixed, unknown chance $p_n$ of winning. So in this approximation, the number of prizes per week would not be fixed. If this is the case, then you can reduce the gaps to drawings of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Geometric_distribution&quot; rel=&quot;nofollow&quot;&gt;geometrically distributed&lt;/a&gt; random variable by subtracting 2 from each of them (or 1 depending on your definition of the geometric distribution). You can then estimate $p_n$; an obvious choice would be the maximum likelihood estimator, which would be&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\hat p}_n = \left( \frac{1}{k_n}\sum_{i=1}^{k_n} (G_{n,i} - 2) \right)^{-1},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $G_{n,i}$ is the $i$th gap between successive winnings of player $n$, for $1 \leq i \leq k_n$; $k_n$ is the number of gaps known for player $n$, which in turn is the number of times $n$ has won minus one. I think this estimator is biased; I'm not sure what an appropriate correction factor would be. But if you have more than a few observations per player, you should be fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, from the data that you give, using the analysis of @whuber in the other answer, I would think that the effect of &quot;luck&quot; is substantially greater than the true differences in $p_n$s. This means that the quality of the information you get out of this or any other estimation method will be rather poor until you have many years of data for constant $p_n$ - if someone has a low $\hat p_n$, that means that the likelihood of having a low actual $p_n$ is only marginally larger than for members with a high $\hat p_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note also that if you get new members, then everybody's $p_n$ changes. It would probably be possible to take this into account by a scaling factor, but I don't immediately see how.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-12T21:56:15.923" Id="12952" LastActivityDate="2011-07-12T21:56:15.923" OwnerUserId="2898" ParentId="12947" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AcceptedAnswerId="12992" AnswerCount="5" Body="&lt;p&gt;I am working with a time series of meteorological data and want to extract just the summer months. The data frame looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;FECHA;H_SOLAR;DIR_M;DIR_S;VEL_M;VEL_S;VEL_X;U;V;TEMP_M;HR;BAT;PRECIP;RAD;UVA;UVB;FOG;GRID;&#10;00/01/01;23:50:00;203.5;6.6;2.0;0.5;-99.9;-99.9;-99.9;6.0;-99.9;9.0;-99.9;-99.9;-99.9;-99.9;-99.9;-99.9&#10;00/01/02;23:50:00;235.5;7.5;1.8;0.5;-99.9;-99.9;-99.9;6.1;-99.9;8.9;-99.9;-99.9;-99.9;-99.9;-99.9;-99.9&#10;00/01/03;23:50:00;217.4;6.1;1.4;0.5;-99.9;-99.9;-99.9;7.0;-99.9;8.9;-99.9;-99.9;-99.9;-99.9;-99.9;-99.9&#10;00/01/04;23:50:00;202.5;8.6;1.8;0.5;-99.9;-99.9;-99.9;6.4;-99.9;8.8;-99.9;-99.9;-99.9;-99.9;-99.9;-99.9&#10;00/01/05;23:50:00;198.5;7.1;1.8;0.5;-99.9;-99.9;-99.9;5.4;-99.9;8.8;-99.9;-99.9;-99.9;-99.9;-99.9;-99.9&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have found some examples of time subsetting in R but only between an starting and end date. What I want is to extract all data from a month for all years to create a new data frame to work with.  I can create a zoo time series from the data but how do I subset? zoo aggregate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-13T11:12:12.447" FavoriteCount="2" Id="12980" LastActivityDate="2014-08-30T03:52:14.300" LastEditDate="2011-11-21T14:10:15.943" LastEditorUserId="919" OwnerUserId="4147" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;aggregation&gt;" Title="Subset data by month in R" ViewCount="5937" />
  <row Body="&lt;p&gt;I'm not particularly familiar with the sem package, but I do know that the lavaan package offers a WLS estimation method for cfa and sem models. It should all be described in the relevant documentation.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the use of bootstrapping, I'm not familiar enough with the theory, but I would tend to avoid such methods with such a tiny sample. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-13T13:45:03.453" Id="12986" LastActivityDate="2011-07-13T13:45:03.453" OwnerUserId="656" ParentId="12978" PostTypeId="2" Score="2" />
&#10;&amp;amp;&amp;amp;\text{of friends}&amp;amp;  &amp;amp;            &amp;amp;           &amp;amp;\text{no of friends}&amp;amp; \\ 
&#10; &amp;amp; All&amp;amp; 5.31&amp;amp; 5.27&amp;amp; 85&amp;amp; 85 &amp;amp;3.273&amp;amp; 2.978
&#10;&amp;amp;all&amp;amp;48.414&amp;amp;85&amp;amp;20.487&amp;amp; \\ 
&#10;&amp;amp;sig=0.009&amp;amp;&amp;amp;&amp;amp;&amp;amp; \\ 
  <row Body="&lt;p&gt;Your example leads to unequal cell sizes, which means that the different &quot;types of sum of squares&quot; matter, and the test for main effects is not as simple as you state it. &lt;code&gt;Anova()&lt;/code&gt; uses type II sum of squares. See &lt;a href=&quot;http://stats.stackexchange.com/questions/11209/the-effect-of-the-number-of-replicates-in-different-cells-on-the-results-of-anova&quot;&gt;this question&lt;/a&gt; for a start.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are different ways to test the contrasts. Note that SS types don't matter as we are ultimately testing in the associated one-factorial design. I suggest using the following steps:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# turn your 2x2 design into the corresponding 4x1 design using interaction()&#10;&amp;gt; d$ab &amp;lt;- interaction(d$a, d$b)       # creates new factor coding the 2*2 conditions&#10;&amp;gt; levels(d$ab)                        # this is the order of the 4 conditions&#10;[1] &quot;a1.b1&quot; &quot;a2.b1&quot; &quot;a1.b2&quot; &quot;a2.b2&quot;&#10;&#10;&amp;gt; aovRes &amp;lt;- aov(y ~ ab, data=d)       # oneway ANOVA using aov() with new factor&#10;&#10;# specify the contrasts you want to test as a matrix (see above for order of cells)&#10;&amp;gt; cntrMat &amp;lt;- rbind(&quot;contr 01&quot;=c(1, -1,  0,  0),  # coefficients for testing a within b1&#10;+                  &quot;contr 02&quot;=c(0,  0,  1, -1),  # coefficients for testing a within b2&#10;+                  &quot;contr 03&quot;=c(1, -1, -1,  1))  # coefficients for interaction&#10;&#10;# test contrasts without adjusting alpha, two-sided hypotheses&#10;&amp;gt; library(multcomp)                   # for glht()&#10;&amp;gt; summary(glht(aovRes, linfct=mcp(ab=cntrMat), alternative=&quot;two.sided&quot;),&#10;+         test=adjusted(&quot;none&quot;))&#10;Simultaneous Tests for General Linear Hypotheses&#10;Multiple Comparisons of Means: User-defined Contrasts&#10;Fit: aov(formula = y ~ ab, data = d)&#10;&#10;Linear Hypotheses:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)&#10;contr 01 == 0  -0.7704     0.7875  -0.978    0.330&#10;contr 02 == 0  -1.0463     0.9067  -1.154    0.251&#10;contr 03 == 0   0.2759     1.2009   0.230    0.819&#10;(Adjusted p values reported -- none method)    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now manually check the result for the first contrast.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; P       &amp;lt;- 2                             # number of levels factor a&#10;&amp;gt; Q       &amp;lt;- 2                             # number of levels factor b&#10;&amp;gt; Njk     &amp;lt;- table(d$ab)                   # cell sizes&#10;&amp;gt; Mjk     &amp;lt;- tapply(d$y, d$ab, mean)       # cell means&#10;&amp;gt; dfSSE   &amp;lt;- sum(Njk) - P*Q                # degrees of freedom error SS&#10;&amp;gt; SSE     &amp;lt;- sum((d$y - ave(d$y, d$ab, FUN=mean))^2)    # error SS&#10;&amp;gt; MSE     &amp;lt;- SSE / dfSSE                   # mean error SS&#10;&amp;gt; (psiHat &amp;lt;- sum(cntrMat[1, ] * Mjk))      # contrast estimate&#10;[1] -0.7703638&#10;&#10;&amp;gt; lenSq &amp;lt;- sum(cntrMat[1, ]^2 / Njk)       # squared length of contrast&#10;&amp;gt; (SE   &amp;lt;- sqrt(lenSq*MSE))                # standard error&#10;[1] 0.7874602&#10;&#10;&amp;gt; (tStat &amp;lt;- psiHat / SE)                   # t-statistic&#10;[1] -0.9782893&#10;&#10;&amp;gt; (pVal &amp;lt;- 2 * (1-pt(abs(tStat), dfSSE)))  # p-value&#10;[1] 0.3303902&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2011-07-13T17:48:58.323" Id="12999" LastActivityDate="2011-07-13T18:13:08.957" LastEditDate="2011-07-13T18:13:08.957" LastEditorUserId="1909" OwnerUserId="1909" ParentId="12993" PostTypeId="2" Score="8" />
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But I don't what to do next. Can anyone give hints? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-13T20:27:00.417" FavoriteCount="2" Id="13003" LastActivityDate="2011-12-07T16:48:19.980" LastEditDate="2011-07-13T20:51:09.387" LastEditorUserId="88" OwnerUserId="4864" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;boosting&gt;" Title="The upper bound of the training error of AdaBoost" ViewCount="411" />
&#10;\end{aligned}
&#10;                &amp;amp; &amp;amp; +\frac{1}{4}(g&amp;#39;&amp;#39;(\mu_X))^2\left(\mathbf{E}[X^4]-4\mu_X\mathbf{E}[X^3]+6\mu_{X}^{2}(\sigma_{X}^{2}+\mu_{X}^{2})-3\mu_{X}^{4}-\sigma_{X}^{4}\right)\\
  
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I would like to understand the topic conceptually rather than through equations&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-14T10:18:51.203" Id="13028" LastActivityDate="2014-12-20T02:22:24.500" OwnerUserId="4128" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;" Title="Where can I find accessible reference to vector autoregression?" ViewCount="83" />
  
  <row Body="&lt;p&gt;To simplify a bit, let's assume that you only have two diagnostic tests. You want to calculate&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  <row AcceptedAnswerId="29225" AnswerCount="1" Body="&lt;p&gt;Suppose you have a data set $Y_{1}, ..., Y_{n}$ from a continuous distribution with density $p(y)$ supported on $[0,1]$ that is not known, but $n$ is pretty large so a kernel density (for example) estimate, $\hat{p}(y)$, is pretty accurate. For a particular application I need to transform the observed data to a finite number of categories to yield a new data set $Z_{1}, ..., Z_{n}$ with an implied mass function $g(z)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;A simple example would be $Z_{i} = 0$ when $Y_{i} \leq 1/2$ and $Z_{i} = 1$ when $Y_{i} &amp;gt; 1/2$. In this case the induced mass function would be &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat{g}(0) = \int_{0}^{1/2} \hat{p}(y) dy, \ \ \ \hat{g}(1) = \int_{1/2}^{1} \hat{p}(y)dy$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The two &quot;tuning parameters&quot; here are the number of groups, $m$, and the $(m-1)$ length vector of thresholds $\lambda$. Denote the induced mass function by $\hat{g}_{m,\lambda}(y)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like a procedure that answers, for example, &quot;What is the best choice of $m, \lambda$ so that increasing the number of groups to $m+1$ (and choosing the optimal $\lambda$ there) would yield a negligible improvement?&quot;. I feel like perhaps a test statistic can be created (maybe with the difference in KL divergence or something similar) whose distribution can be derived. Any ideas or relevant literature? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I have evenly spaced temporal measurements of a continous variable and am using an inhomogenous Markov chain to model the temporal dependence. Frankly, discrete state markov chains are much easier to handle and that is my motivation. The observed data are percentages. I'm currently using an ad hoc discretization that looks very good to me but I think this is an interesting problem where a formal (and general) solution is possible. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; Actually minimizing the KL divergence would be equivalent to not discretizing the data at all, so that idea is totally out. I've edited the body accordingly.  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-07-14T18:11:53.780" FavoriteCount="1" Id="13054" LastActivityDate="2012-05-29T17:32:17.517" LastEditDate="2012-05-27T15:43:13.297" LastEditorUserId="4856" OwnerUserId="4856" PostTypeId="1" Score="6" Tags="&lt;continuous-data&gt;&lt;discrete-data&gt;" Title="Determining an optimal discretization of data from a continuous distribution" ViewCount="727" />
  
  <row Body="&lt;p&gt;It appears that each sequence has a set (equally spaced) of possibly auto-correlated historical values. To answer the question is the sequence expected to increase,decrease or remain the same is at the heart of time series modelling with Intervention Detection. For example each sequence may be described in two possible ways : 1) y(t) = y(t-1) + trend plus etc ARMA structure OR 2 ) y(t)= b0 + b1*t where t=1,2,3,..... . plus ARMA structure. Two possible ways of assessing trend ! Now in general in case (1) there could be multiple differencing operators or in case (2) there could be multiple trend break points . Now just to generalize one step further i.e. make less presumptive specifications about the model sample space, either 1) or 2) could possibly include one or more Level or Step Shifts which are not trend changes but intercept changes. Tons of software confuse i.e. fail to distinguish between trend changes and level changes. Not to make this more complex than it already seems to be, one might have changes in parameters or changes in error variance over time. Thus your problem is solved . Now all you have to is to find out how to implement this in some reasonable time frame. Be careful to challenge a presumed a model-based approach that doesn't verify the Gaussian Assumptions or doesn't adhere to strict tests of necessity and sufficiency of a proposed model. Whew ! This tires me out specifying what you have to do to correctly answer your questions.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-14T21:01:23.673" Id="13060" LastActivityDate="2011-07-14T21:29:01.807" LastEditDate="2011-07-14T21:29:01.807" LastEditorUserId="3382" OwnerUserId="3382" ParentId="13059" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Adaboost works by fitting a weak classifier, such as a stumpy decision tree, reweighing the data to emphasize difficult cases, and repeating. Under some reasonable definitions, the number of possible trees is large but finite (you can only divide the data into two pieces in so many ways). But as your tutorial points out, that isnt an essential feature. Here's a paper that used neural nets as it's weak classifier, for example: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.829&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.829&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The neural nets have continuous parameters, but that doesn't really affect the algorithm, and no discretization is needed. I suppose you could discretize it after the fact if that helped with interpretation, but it works fine with real numbers. &lt;/p&gt;&#10;&#10;&lt;p&gt;You should be able to find other examples with search terms like &quot;boosted X,&quot; where X is a continuous classifier. Hope this helps!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-15T01:36:40.053" Id="13068" LastActivityDate="2011-07-15T01:36:40.053" OwnerUserId="4862" ParentId="13007" PostTypeId="2" Score="1" />
  
  
&#10;         &amp;amp;= \frac{1}{R-K}\sum_{j=1}^K K_j \frac{1}{K_j}\sum_{k=1}^{K_j}(x_k - \mu_j)^2 \\
  <row Body="&lt;p&gt;I use loess (R &lt;code&gt;lowess&lt;/code&gt; function) with outlier detection turned off, when Y is binary.  The R &lt;code&gt;Hmisc&lt;/code&gt; package makes this easier - see its &lt;code&gt;plsmo&lt;/code&gt; function.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-15T18:07:04.013" Id="13109" LastActivityDate="2011-07-15T18:07:04.013" OwnerUserId="4253" ParentId="11580" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Given i.i.d. samples from a gaussian distribution $X_1,...,X_n \sim N(\mu,\sigma) $ and the M-estimator, $\mu_m = \underset{a}{\operatorname{argmin}}  \sum\rho(|X_i-a|)$, what properties on $\rho$ are sufficient to guarantee $\mu_m \rightarrow \mu$ in probability? Is $\rho$ being strictly convex and strictly increasing sufficient?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-15T19:03:08.253" FavoriteCount="1" Id="13114" LastActivityDate="2011-10-05T06:39:01.553" LastEditDate="2011-07-16T21:15:07.850" LastEditorUserId="930" OwnerUserId="5418" PostTypeId="1" Score="9" Tags="&lt;estimation&gt;" Title="Conditions for the M-estimator to converge to the true mean" ViewCount="251" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm a new statistics student :) I have some questions about linear regression, i'm using R to do some tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two simple lists, like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; a &amp;lt;- c(1,2,3,4)&#10;&amp;gt; b &amp;lt;- c(5,6,7,8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then I do&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; model &amp;lt;- lm(a ~ b)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the result (coefficient) is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; coef(model)&#10;(Intercept)           b &#10;         -4           1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the &quot;relationship&quot; is perfect (1). &#10;But i didn't understand the intercept value, why is it -4?&lt;/p&gt;&#10;&#10;&lt;p&gt;Then if i change my R test with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; b &amp;lt;- c(5,6,7,80)&#10;&amp;gt; a &amp;lt;- c(1,2,3,40)&#10;&amp;gt; model &amp;lt;- lm(a ~ b)&#10;&amp;gt; coef(model)&#10;(Intercept)           b &#10; -1.0868825   0.5137503 &#10;&amp;gt; model &amp;lt;- lm(b ~ a)&#10;&amp;gt; coef(model)&#10;(Intercept)           a &#10;   2.125346    1.945622 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;obviously &lt;strong&gt;lm(a ~ b) != lm(b ~ a)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But, what are there criteria for selecting which list to put in the first place?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-16T12:06:56.953" Id="13126" LastActivityDate="2011-07-17T08:51:31.567" LastEditDate="2011-07-17T08:51:31.567" LastEditorUserId="183" OwnerUserId="5405" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;linear-model&gt;" Title="Understanding intercept in simple linear regression and why one variable is a predictor and the other is an outcome variable" ViewCount="836" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;For your first question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If the output of a regression isn't making sense to me (and even if it is), I always look at the plot of the regression line. In your case, this can be accomplished with this code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a &amp;lt;- c(1,2,3,4)&#10;b &amp;lt;- c(5,6,7,8)&#10;mod &amp;lt;- lm(a ~ b)&#10;plot(a ~ b, xlim = c(-1,9), ylim = c(-5,5))&#10;abline(mod, lwd = 3)&#10;abline(v = 0, col = 'blue', lwd = 3, lty = 3)&#10;abline(h = 0, col = 'blue', lwd = 3, lty = 3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This produces the following graph.&#10;&lt;img src=&quot;http://i.stack.imgur.com/EwKBt.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The blue lines are the axes, the circles are the original data, and the black line is the regression line. It seems reasonable that the intercept would be at -4. If this isn't convincing enough, try working through the math by hand.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For your second question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When you've got two sets of meaningless numbers it is hard to decide which should be the independent variable and which should be the dependent variable. In most cases (in fact, probably all cases) there will be some meaning attached to these numbers. In addition, you will have a question that you'd like to answer, such as: &lt;em&gt;What is the effect of hot dog size on the amount of time it takes to finish the hot dog?&lt;/em&gt;. I know, it's a silly example, but it gets the point across. You have two variables: $X$ - hot dog size, $Y$ - time to eat the hot dog. Based on the question you'd like to answer, it only makes sense to regress $Y$ on $X$, and not the other way.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-16T15:23:44.380" Id="13128" LastActivityDate="2011-07-16T15:23:44.380" OwnerUserId="4812" ParentId="13126" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;&quot;I'm wondering what is the best way to combine these numbers in a way that will yield a final score that is (hopefully) more reliable than any single test.&quot;  A very common way is to compute Cronbach's alpha and, more generally, to perform what some would call a &quot;standard&quot; reliability analysis.  This would show to what degree a given score correlates with the mean of the 17 other scores; which tests' scores might be best dropped from the scale; and what the internal consistency reliability is both with all 18 and with a given subset.  Now, some of your comments seem to indicate that many of these 18 are uncorrelated; if that is true, you may end up with a scale that consists of just a few tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT AFTER COMMENT:  Another approach draws on the idea that there is a tradeoff between internal consistency and validity.  The less correlated your tests are, the better their content coverage, which enhances content validity (if not reliability).  So thinking along these lines you would ignore Cronbach's alpha and the related indicators of item-total correlation and instead use a priori reasoning to combine the 18 tests into a scale.  Hopefully such a scale would correlate highly with your gold standard.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-16T15:25:55.310" Id="13129" LastActivityDate="2011-07-16T17:32:14.470" LastEditDate="2011-07-16T17:32:14.470" LastEditorUserId="2669" OwnerUserId="2669" ParentId="13014" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Thanks to the posts and advice here, I've gotten off to a good start with cluster analysis. On a good day I can even get R to compute one without breaking down in tears! But I'm wondering if people have tips and techniques for interpreting what the clusters they get actually mean. By this I mean that when you sit down with a dendrogram or summary statistics from a cluster analysis, how to do you go about identifying what those clusters have in common? How do you apply meaning to the results? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be interested in learning about any statistical or qualitative techniques people have found useful for analyzing clusters&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-16T18:09:22.943" Id="13131" LastActivityDate="2011-07-16T18:09:22.943" OwnerUserId="5425" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;" Title="Techniques and tips for interpreting a cluster analysis" ViewCount="424" />
  <row AcceptedAnswerId="20100" AnswerCount="2" Body="&lt;p&gt;It strikes me that the available corrections for multiple comparisons in the context of a repeated measures ANOVA are excessively conservative.  Is this actually the case?  If so, what are some citations I can use to support this point and learn more?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-07-16T18:37:26.773" FavoriteCount="1" Id="13132" LastActivityDate="2011-12-21T10:21:43.647" LastEditDate="2011-07-19T21:13:26.360" LastEditorUserId="919" OwnerUserId="196" PostTypeId="1" Score="8" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;multiple-comparisons&gt;" Title="Correcting for multiple comparisons in a within subjects / repeated measures ANOVA; excessively conservative?" ViewCount="1157" />
  
  <row Body="&lt;p&gt;Let $p = Pr(\text{sucess})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $N = \text{number of 1000-throw trials}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $k = \text{number of success in the final trial that stops the sequence}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$L=(1-p)^{1000(N-1)} {1000 \choose{k}} p^{k} (1-p)^{1000-k}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Maximizing, $\hat{p}={k \over{1000N}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;As you suggested. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-16T21:53:31.917" Id="13139" LastActivityDate="2011-07-17T19:13:13.210" LastEditDate="2011-07-17T19:13:13.210" LastEditorUserId="812" OwnerUserId="812" ParentId="13136" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;If this is something that you will be doing often you might consider writing a method for   the xtrfm function.  You can use regular expressions to split the strings and order with multiple columns to det the ordering right.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might also want to look at the mixedsort function in the gtools package to see if that works for you (or you could use it as a base and modify the code if it does not work as is).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-17T04:08:40.250" Id="13145" LastActivityDate="2011-07-17T04:08:40.250" OwnerUserId="4505" ParentId="13117" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="13153" AnswerCount="1" Body="&lt;h3&gt;Context&lt;/h3&gt;&#10;&#10;&lt;p&gt;When I teach students about the correlation coefficient, I like to give students a sense of how correlations map on to common associations that might be encountered in daily life and across various topics and disciplines.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some years back I read a psychology journal article  (it might have been American Psychologist) that presented a table of meta analytic correlations across a wide range of topics and disciplines (e.g., 40 or 50 correlations from medical, psychological, economic, and other domains). The table aimed to highlight what are typical correlations in a given domain in order to provide context for the interpretation of a correlation coefficient. However, I can't remember the reference for the particular journal article.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Question&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Does anyone know where a general listing of the magnitude of typical correlations across a wide range of disciplines and topics could be obtained?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2011-07-17T06:00:48.687" Id="13146" LastActivityDate="2011-07-17T11:19:06.973" OwnerUserId="183" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;meta-analysis&gt;&lt;teaching&gt;" Title="Listing of typical correlations across spectrum of topics and disciplines to assist with teaching meaning of correlation coefficient" ViewCount="227" />
  <row Body="&lt;p&gt;Here's a collection of links to a SPSS forum. Hope you find it relevant to you to some degree: &lt;a href=&quot;http://listserv.uga.edu/cgi-bin/wa?A2=ind9708&amp;amp;L=spssx-l&amp;amp;P=R4764&amp;amp;D=1&amp;amp;H=0&amp;amp;O=D&amp;amp;T=1&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;http://listserv.uga.edu/cgi-bin/wa?A2=ind9810&amp;amp;L=spssx-l&amp;amp;P=R23134&amp;amp;D=1&amp;amp;H=0&amp;amp;O=D&amp;amp;T=1&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;http://listserv.uga.edu/cgi-bin/wa?A2=ind9811&amp;amp;L=spssx-l&amp;amp;P=R182&amp;amp;D=1&amp;amp;H=0&amp;amp;O=D&amp;amp;T=1&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;http://listserv.uga.edu/cgi-bin/wa?A2=ind9811&amp;amp;L=spssx-l&amp;amp;P=R4346&amp;amp;D=1&amp;amp;H=0&amp;amp;O=D&amp;amp;T=1&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-17T07:12:06.840" Id="13147" LastActivityDate="2011-07-17T07:12:06.840" OwnerUserId="3277" ParentId="13132" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Lets say I know what is the overall budget for some units and I want to predict share of budget each unit will get. I have historical data and could do regression analysis. Is it better to predict shares directly with logistic regression or to try to predict amount and then get calculate its share in overall predictions? Or something else? One more thing, I would like to spread entire budget at the end.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-17T15:23:15.207" Id="13156" LastActivityDate="2011-07-17T17:09:15.450" OwnerUserId="2341" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;predictive-models&gt;" Title="How to predict shares?" ViewCount="277" />
  
  <row Body="&lt;p&gt;Interesting question...here's a Stata-based solution (that doesnt require OS utilities like awk). My example uses the data example contributed by chl, except that I added in the data problems described by Fr. (missing years, multiple siblings, etc). &lt;/p&gt;&#10;&#10;&lt;p&gt;Run the code below from your do-file editor.  Note that there are some comments explaining alternatives for handling multiple siblings by reshaping the data wide.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;**********************! Begin example&#10;clear&#10;&#10;**input dataset:&#10;input str20(v1)&#10;&quot;&quot;&#10;&quot;N: toto&quot;&#10;&quot;Y: 2000&quot;&#10;&quot;S: tata&quot;&#10;&quot;&quot;&#10;&quot;N: titi&quot;&#10;&quot;Y: 2004&quot;&#10;&quot;S: tutu&quot;&#10;&quot;&quot;&#10;&quot;N: toto&quot;&#10;&quot;Y: 2000&quot;&#10;&quot;S: tata2&quot;&#10;&quot;&quot;&#10;&quot;N: toto&quot;&#10;&quot;Y: 2000&quot;&#10;&quot;S: tata3&quot;&#10;&quot;&quot;&#10;&quot;N: tete&quot;&#10;&quot;Y: 2002&quot;&#10;&quot;S: tyty&quot;&#10;&quot;&quot;&#10;&quot;N: tete&quot;&#10;&quot;Y: 2002&quot;&#10;&quot;S: tyty2&quot;&#10;&quot;S: tyty99&quot;&#10;&quot;&quot;&#10;&quot;N: tete2&quot;&#10;&quot;S: tyty22&quot;&#10;&quot;&quot;&#10;&quot;N: tete3&quot;&#10;&quot;Y: 2004&quot;&#10;&quot;S: tytya&quot;&#10;&quot;S: tytyb&quot;&#10;&quot;S: tytyc&quot;&#10;end&#10;&#10;**parse data/v1&#10;split v1, parse(&quot;: &quot;)&#10;l in 1/7&#10;drop v1&#10;&#10;**create panel id&#10;g id = _n if mi(v12)&#10;replace id = id[_n-1] ///&#10;     if mi(id) &amp;amp; !mi(v12)&#10;drop if mi(v12)&#10;//get max value for later//&#10;bys id: g i = _n&#10;qui sum i&#10;loc max `r(max)'&#10;rename v12 attribute&#10;l in 1/6&#10;&#10;**reshape data&#10;foreach x in N Y S {&#10;g `x' = attribute if v11==&quot;`x'&quot; ///&#10;    &amp;amp; !mi(attribute)&#10;    forval n = 1/`=`max'-1' {&#10;    foreach s in + - {&#10;bys id: replace `x' = `x'[_n`s'`n'] if !mi(`x'[_n`s'`n']) /// &#10;    &amp;amp; mi(`x')&#10;    } //end x.loop&#10;  } //end n.loop&#10;} //end s.loop&#10;&#10;drop v11 attribute i&#10;duplicates drop&#10;&#10;**if you want multiple siblings on one line, reshape wide:&#10;bys id N Y: g j = _n&#10;reshape wide S, i(id) j(j)&#10;order id N Y&#10;li&#10;&#10;**********************! End example&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-07-17T19:32:55.963" Id="13163" LastActivityDate="2011-07-18T02:59:08.477" LastEditDate="2011-07-18T02:59:08.477" LastEditorUserId="1307" OwnerUserId="1033" ParentId="11547" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;To answer your specific question 1, yes you can do planned comparisons with &lt;code&gt;multcomp&lt;/code&gt; even though you are using a generalized linear model. From the package description:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Simultaneous tests and confidence intervals for general linear hypotheses in parametric models, including linear, generalized linear, linear mixed effects, and survival models.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You can easily implement this with the Zelig output (which is an object from the &lt;code&gt;negbin&lt;/code&gt; class since Zelig calls the &lt;code&gt;glm.nb&lt;/code&gt; function from the &lt;code&gt;MASS&lt;/code&gt; package). Here is an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(Zelig)&#10;library(multcomp)&#10;data(sanction)&#10;z.out &amp;lt;- zelig(num ~ target  * coop, model = &quot;negbin&quot;, data = sanction)&#10;&#10;## construct contrast matrices&#10;hypo.mat &amp;lt;- rbind(&quot;coop0:target1 - target0&quot; = c(0, 1, 0, 0),&#10;                  &quot;coop1:target1 - target0&quot; = c(0, 1, 0, 1))&#10;summary(glht(z.out, hypo.mat))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which gives the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Simultaneous Tests for General Linear Hypotheses&#10;&#10;Fit: zelig(formula = num ~ target * coop, model = &quot;negbin&quot;, data = sanction)&#10;&#10;Linear Hypotheses:&#10;                             Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;coop0:target1 - target0 == 0  0.04201    0.38908   0.108    0.971&#10;coop1:target1 - target0 == 0  0.09089    0.24811   0.366    0.786&#10;(Adjusted p values reported -- single-step method)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that I used different contrasts than you gave. You are putting the contrasts in terms of the vector of groups, but &lt;code&gt;multicomp&lt;/code&gt; (and its general form of hypothesis testing) wants contrasts on the model parameters. We can write the model above as&lt;/p&gt;&#10;&#10;&lt;p&gt;$\log \mu_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \beta_3 (x_i \times z_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $E(Y) = \mu_i$ is the expected value of the outcome. Thus, in this model, the hypothesis that the effect of $x_i$ is zero when $z_i$ is 0 is just:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0: \beta_1 = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;This leads to the contrast &lt;code&gt;c(0,1,0,0)&lt;/code&gt;. The hypothesis that the effect of $x_i$ is zero when $z_i$ is 0 is just:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0: \beta_1 + \beta_3 = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;This leads to the contrast &lt;code&gt;c(0,1,0,1)&lt;/code&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-17T23:37:38.083" Id="13168" LastActivityDate="2011-07-18T00:22:11.090" LastEditDate="2011-07-18T00:22:11.090" LastEditorUserId="4160" OwnerUserId="4160" ParentId="13091" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Not a great answer but two places to look for research are:&lt;/p&gt;&#10;&#10;&lt;p&gt;International Society for Music Information Retrieval has tons of published papers about just this topic, amazing how much info there is www.ismir.net&lt;/p&gt;&#10;&#10;&lt;p&gt;&amp;amp; Echo Nest (A Startup with an API to do similar stuff) echonest.com&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: they also released some open source fingerprinting code.&#10;&lt;a href=&quot;http://echoprint.me/&quot; rel=&quot;nofollow&quot;&gt;http://echoprint.me/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-18T12:40:13.290" Id="13191" LastActivityDate="2011-07-22T12:52:33.877" LastEditDate="2011-07-22T12:52:33.877" LastEditorUserId="-1" OwnerUserId="5453" ParentId="13184" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Apparently the answer is &quot;check your data structure&quot;.  To that I would also add check the type of the Sums of Squares you are using.  Further, I would add, ezANOVA is a godsend in that it handles the model formulation for you and gives more traditional (Type-III Ss) results.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-18T14:43:10.987" Id="13195" LastActivityDate="2011-07-18T14:43:10.987" OwnerUserId="196" ParentId="11113" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I prefer a repeated measures model.  Not only is it easier to interpret the results, it is more flexible in that you can specify a covariance structure.&lt;/p&gt;&#10;&#10;&lt;p&gt;This reference may be of use as it works through an example:&#10;&lt;a href=&quot;http://blogs.sas.com/content/sastraining/2011/02/02/the-punchline-manova-or-a-mixed-model/&quot; rel=&quot;nofollow&quot;&gt;Mixed or MANOVA&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-07-18T16:09:58.953" Id="13200" LastActivityDate="2014-06-12T16:05:46.293" LastEditDate="2014-06-12T16:05:46.293" LastEditorUserId="20943" OwnerUserId="2310" ParentId="13197" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="13258" AnswerCount="1" Body="&lt;p&gt;Motivated by the problem of &lt;a href=&quot;http://stats.stackexchange.com/questions/11368/how-to-ensure-properties-of-covariance-matrix-when-fitting-multivariate-normal-mo&quot;&gt;covariance estimation&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\Sigma$ be a positive-definite matrix whose diagonal entries are identically 1.  (i.e. $\Sigma$ is a correlation matrix.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If $L$ is an lower-triangular matrix such that $L^T L = \Sigma$, can the entries of $L$ be bounded? (i.e. do there exist real numbers $l, u$ such that $l \geq (L)_{ij} \leq u$?)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-07-18T16:59:56.393" Id="13204" LastActivityDate="2011-07-19T17:19:32.610" OwnerUserId="3567" PostTypeId="1" Score="2" Tags="&lt;matrix-decomposition&gt;" Title="Bounds on entries of Cholesky factors?" ViewCount="88" />
  <row AcceptedAnswerId="13219" AnswerCount="1" Body="&lt;p&gt;Assume we have a noisy system where data is available via &lt;code&gt;sample()&lt;/code&gt; and in order to filter out the noise someone has implemented the following voting algorithm:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sample_a = sample();&#10;sample_b = sample();&#10;if (sample_a != sample_b) {&#10;    sample_c = sample();&#10;    if (sample_a == sample_c)&#10;        return sample_a;&#10;    else if (sample_b == sample_c)&#10;        return sample_b;&#10;}&#10;return sample_a;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So it's almost a voting algorithm where we select the result appearing at least 2 out of 3 times.  In the case where the first two match there is no third sample, but since the samples are not selected from a set but by adding a new &quot;trial&quot; I don't know if it matters that the third test is sometimes omitted.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is:&lt;/strong&gt;  If I have an estimate of how often the &lt;em&gt;result of the above algorithm&lt;/em&gt; still yields an incorrect sample (based on outside knowledge of what makes a valid sample) can I reason back from that to the failure rate of the base &lt;code&gt;sample()&lt;/code&gt; function?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My thinking:&lt;/strong&gt; Take $P$ = probability of a bad sample.  Consider the 8 combinations of right/wrong results in a 3-way vote: half of those cases have at least 2 failures.  The 3 with two failures each have $P^{2}(1-P)$ chance of being wrong (two wrongs, one right) and the all-wrong case is $P^{3}$.  The probability of the vote being wrong is then $3P^{2}(1-P)+P^{3}$ which simplifies to $-2P^{3}+3P^{2}$ which at least goes to the right limits (0 for $P\rightarrow 0$ and 1 for $P\rightarrow 1$).  Leaning on Wolfram Alpha to solve that for me I get:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gJtjK.gif&quot; alt=&quot;solved for P&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(I'm sure my tag selection is poor -- If I knew the right tags I would probably know the answer!)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-18T18:38:46.767" FavoriteCount="1" Id="13208" LastActivityDate="2011-07-18T22:34:23.117" LastEditDate="2011-07-18T21:02:24.657" LastEditorUserId="919" OwnerUserId="2622" PostTypeId="1" Score="3" Tags="&lt;binomial&gt;&lt;conditional-probability&gt;&lt;inference&gt;" Title="How often is data corrupted given how often a 3-way vote passes incorrect data?" ViewCount="55" />
  
  <row Body="&lt;p&gt;There are a couple ways to interpret seasonality in your questions and the solution varies accordingly.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;You can add a categorical dummy variable to control and capture the effect of seasonality. Let's say you have 4 seasons (Q1, Q2, Q3, and Q4). Then you need to add a dummy variable where the values of the dummy represent the season associated with the measurement period. This is the most straightforward solution and is typical of estimates sales with seasonality. (Note that in a typical linear regression only 3 dummies would be required -- each dummy would represent the incremental sales vs. the baseline with no dummies so all 4 states are captured.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If your time-series exhibits seasonality following an auto-regressive process (as interpreted in the comments above), you can create an autoregressive process not using CART. However, you would miss on the ability of CART to discriminate the population using other predictors. My suggestion here would be to add a new continuous independent variable which is the lag of the prior period's sales. I would normalize your sales growth so you are dealing with stationary series (perhaps log of sales divided by log of sales from the prior period if the series exhibits exponential growth).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A third solution would be to add &quot;time dummies&quot;. This would mean adding date as a variable to the model. You must have a strong understanding of the relationship between time and your dependent variable if you elect this unconventional approach however. If you add a continuous time-variable then CART will identify if there are changes in the functional form of the sales dependent variable over different periods of time.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-07-18T21:18:52.503" Id="13214" LastActivityDate="2011-07-19T18:31:56.813" LastEditDate="2011-07-19T18:31:56.813" LastEditorUserId="8101" OwnerUserId="8101" ParentId="13205" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="13224" AnswerCount="1" Body="&lt;p&gt;I have a dependent variable: rtln (log transformed reaction time) and 3 predictors: &#10;choicenum (ranging from 1 to 6), ifrelevant (0 and 1), and condition (0 and 1), and I have a variable with subject IDs.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to estimate a model where choicenum is nested within subjects.&#10;I use the following notation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(rtln~choicenum + ifrelevant + condition &#10; + (1|subject/choicenum),    data=myData)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for some reason this estimates 5 fixed effects for choicenum.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Here's the output: &lt;a href=&quot;http://dl.dropbox.com/u/22681355/output.tiff&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/22681355/output.tiff&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Here's the dataset: &lt;a href=&quot;http://dl.dropbox.com/u/22681355/data.csv&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/22681355/data.csv&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Here's the stata output of what I'd like to get: &lt;a href=&quot;http://dl.dropbox.com/u/22681355/output.pdf&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/22681355/output.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="5" CreationDate="2011-07-18T22:45:44.303" Id="13220" LastActivityDate="2011-07-19T06:12:11.753" LastEditDate="2011-07-19T06:12:11.753" LastEditorUserId="88" OwnerDisplayName="DBR" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;lmer&gt;" Title="Another 3 level hierarchical regression" ViewCount="237" />
  <row Body="&lt;p&gt;You specified a null hypothesis of trend stationary. That is the data follow a straight line time trend with stationary errors. The p-value is 0.1, so the null hypothesis is not rejected at the usual 5% level.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-07-19T02:35:40.147" Id="13227" LastActivityDate="2011-07-19T02:35:40.147" OwnerUserId="159" ParentId="13213" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;This is not a complete answer. It is just an attempt to give a pointer.  I know nothing about LBA, I just like R code, so your millage may vary.&lt;/p&gt;&#10;&#10;&lt;p&gt;The key to finding the appropriate section of code was knowing that the value Ter was simply added to the end result of the model calculations (and backtracking from the obj function which is in the optim and parameterization wrapper 'fitter').  That lead me to pqlba and lbameans.  In lbameans, Ter is added as at the end of tmp$mean, in turn derived from the n1mean function which accepts as parameters x0max, chi, drift, and sdI which seemed like reasonable matches for your X1:X4 names.  But, nothing calls lbameans, leading me back to pqlba.  Digging through that I can see that pqlba (prior to adding Ter) bounces through a couple functions - and ends up at fptpdf.  At this point I am stymied.   &lt;/p&gt;&#10;&#10;&lt;p&gt;The nice part is that, if I am right, fptpdf has all of the major players present.  The bad part is that, 1) it would take more time to see whether the parameters are doing other things and need to be controlled prior to fptpdf (probably), and 2) Eliminating X1 (aka x0max) is problematic because the function is divided by x0max.  Setting it to 0 then causes obvious problems (dividing by 0 is bad mkay?). Thus a greater understanding of how the model is working is probably needed before you can achieve your aims.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-19T16:19:09.560" Id="13256" LastActivityDate="2011-07-19T16:19:09.560" OwnerUserId="196" ParentId="11070" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="13286" AnswerCount="2" Body="&lt;p&gt;Consider $n\cdot m$ independent draws from cdf $F(x)$, which is defined over 0-1, where $n$ and $m$ are integers.  Arbitrarily group the draws into $n$ groups with m values in each group.  Look at the minimum value in each group.  Take the group that has the greatest of these minima.  Now, what is the distribution that defines the maximum value in that group?&#10;More generally, what is the distribution for the $j$-th order statistic of $m$ draws of $F(x)$, where the kth order of those m draws is also the pth order of the n draws of that kth order statistic?&lt;/p&gt;&#10;&#10;&lt;p&gt;All of that is at the most abstract, so here is a more concrete example.&#10;Consider 8 draws of $F(x)$.  Group them into 4 pairs of 2.  Compare the minimum value in each pair.  Select the pair with the highest  of these 4 minima.  Label that draw &quot;a&quot;.  Label the other value in that same pair as &quot;b&quot;.  What is the distribution $F_b(b)$?  We know $b&amp;gt;a$.  We know a is the maximum of 4 minimums of $F(x)$, of $F_a(a) = (1-(1-F(x))^2)^4$.  What is $F_b(b)$? &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-07-19T18:05:43.903" Id="13259" LastActivityDate="2011-07-28T07:06:56.833" LastEditDate="2011-07-19T19:16:10.990" LastEditorUserId="88" OwnerUserId="5471" PostTypeId="1" Score="7" Tags="&lt;distributions&gt;&lt;probability&gt;&lt;extreme-value&gt;&lt;order-statistics&gt;" Title="What is the distribution of maximum of a pair of iid draws, where the minimum is an order statistic of other minima?" ViewCount="610" />
  <row AnswerCount="0" Body="&lt;p&gt;We know that due to LD, we can get significant p-values for markers near a causal marker (or a marker closes to the causal region) in &lt;a href=&quot;https://secure.wikimedia.org/wikipedia/en/wiki/Genome-wide_association_study&quot; rel=&quot;nofollow&quot;&gt;GWAS&lt;/a&gt; studies. I've seen &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2276357/?tool=pubmed&quot; rel=&quot;nofollow&quot;&gt;attempts&lt;/a&gt; looking at LD to do multiple testing correction, by counting the effective number of tests, but as far as I know, they are simply trying to fix the correct global p-value cutoff for a certain error rate. My question is regarding the actual problem of selecting causal markers. What attempts have their been to remove these false positives that occur due to them being in LD with the causal markers? It seems regression and penalized regression will remove some correlated markers, but are their any techniques other than penalized regression to remove these false positives? Regression is obviously also intractable when we have a large number of markers.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-07-19T19:00:40.587" Id="13265" LastActivityDate="2011-07-20T20:12:36.737" LastEditDate="2011-07-20T20:12:36.737" LastEditorUserId="2728" OwnerUserId="2728" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;genetics&gt;" Title="Countering false positives in GWAS due to linkage disequilibrium" ViewCount="157" />
  <row Body="&lt;p&gt;AFAIK, there is no closed form for the distribution. Using R, the naive implementation of getting the exact distribution works for me up to group sizes of at least 12 - that takes less than 1 minute on a Core i5 using Windows7 64bit and current R. For R's own more clever algorithm in C that's used in &lt;code&gt;pwilcox()&lt;/code&gt;, you can check the source file src/nmath/wilcox.c&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n1 &amp;lt;- 12                                # size group 1&#10;n2 &amp;lt;- 12                                # size group 2&#10;N  &amp;lt;- n1 + n2                           # total number of subjects&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now generate all possible cases for the ranks within group 1. These are all ${N \choose n_{1}}$ different samples from the numbers $1, \ldots, N$ of size $n_{1}$. Then calculate the rank sum (= test statistic) for each of these cases. Tabulate these rank sums to get the probability density function from the relative frequencies, the cumulative sum of these relative frequencies is the cumulative distribution function.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rankMat &amp;lt;- combn(1:N, n1)               # all possible ranks within group 1&#10;LnPl    &amp;lt;- colSums(rankMat)             # all possible rank sums for group 1&#10;dWRS    &amp;lt;- table(LnPl) / choose(N, n1)  # relative frequencies of rank sums: pdf&#10;pWRS    &amp;lt;- cumsum(dWRS)                 # cumulative sums: cdf&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Compare the exact distribution against the asymptotically correct normal distribution.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;muLnPl  &amp;lt;- (n1    * (N+1)) /  2         # expected value&#10;varLnPl &amp;lt;- (n1*n2 * (N+1)) / 12         # variance&#10;&#10;plot(names(pWRS), pWRS, main=&quot;Wilcoxon RS, N=(12, 12): exact vs. asymptotic&quot;,&#10;     type=&quot;n&quot;, xlab=&quot;ln+&quot;, ylab=&quot;P(Ln+ &amp;lt;= ln+)&quot;, cex.lab=1.4)&#10;curve(pnorm(x, mean=muLnPl, sd=sqrt(varLnPl)), lwd=4, n=200, add=TRUE)&#10;points(names(pWRS), pWRS, pch=16, col=&quot;red&quot;, cex=0.7)&#10;abline(h=0.95, col=&quot;blue&quot;)&#10;legend(x=&quot;bottomright&quot;, legend=c(&quot;exact&quot;, &quot;asymptotic&quot;),&#10;       pch=c(16, NA), col=c(&quot;red&quot;, &quot;black&quot;), lty=c(NA, 1), lwd=c(NA, 2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Bf58s.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-19T19:22:42.417" Id="13268" LastActivityDate="2011-07-19T20:07:06.050" LastEditDate="2011-07-19T20:07:06.050" LastEditorUserId="1909" OwnerUserId="1909" ParentId="13264" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;One thing you could use is a distance measure from a central point, ${\bf c}=(c_{1},c_{2})$, such as the sample mean of the points $(\overline{x}, \overline{y})$, or perhaps the centroid of the observed points. Then a measure of dispersion would be the average distance from that central point: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{1}{n} \sum_{i=1}^{n} || {\bf z}_{i} - {\bf c} || $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where ${\bf z}_{i} = \{ x_{i}, y_{i} \}$. There are many potential choices for a distance measure but the $L_{2}$ norm (e.g. euclidean distance) may be a reasonable choice: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ || {\bf z}_{i} - {\bf c} || = \sqrt{ (x_{i}-c_{1})^{2} +  (y_{i}-c_{2})^{2} } $$&lt;/p&gt;&#10;&#10;&lt;p&gt;There are lots of other potential choices, though. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Norm_%28mathematics%29&quot;&gt;http://en.wikipedia.org/wiki/Norm_%28mathematics%29&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-19T19:50:02.047" Id="13274" LastActivityDate="2011-07-19T19:50:02.047" OwnerUserId="4856" ParentId="13272" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="13281" AnswerCount="3" Body="&lt;p&gt;Running a crude model using Bayesian inference, I get some results &gt; 1 (ie, more than 100% &quot;certain&quot;) for some combinations of &quot;evidence&quot;.  For instance, for one bit of evidence the conditional probability of the null hypothesis is 0.85 while the marginal probability is 0.77.  If the prior probability is 0.9, the computed posterior probability is 1.008.  Which maybe could be ascribed to rounding error, except that the next bit of evidence raises the posterior probability to 1.34.&lt;/p&gt;&#10;&#10;&lt;p&gt;It stands to reason that, for a problem with two hypotheses, one would have a conditional probability less than the marginal probability and the other would be greater.  So the resulting P(E|H) / P(E) multiplier would be &gt; 1.  So it's hard to see how such &gt; 1 results can be avoided in the general case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this just the way Bayesian inference works, or do I likely have an error somewhere in my calcs?&lt;/p&gt;&#10;&#10;&lt;h2&gt;Data&lt;/h2&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;One &quot;evidence&quot;:&#10;Total 6134 samples&#10;Total actual positives 2845&#10;Total actual negatives 3289&#10;Test was true 1623 times&#10;Test was true 465 times when the &quot;gold standard&quot; was positive&#10;Test was true 1158 times when the &quot;gold standard&quot; was negative&#10;&#10;conditional probability of positive hypothesis = 465/2845 = 0.1634&#10;conditional probability of negative hypothesis = 1158/3289 = 0.3521&#10;marginal probability of test being true = 1623/6134 = 0.2646&#10;Bayes multiplier for the negative hypothesis = 0.3521 / 0.2646 = 1.3307&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As can be seen, the multiplier is significantly &gt; 1, and with several such tests back-to-back it seems hard to avoid probabilities &gt; 1.  (Of course, I suppose one can argue that the tests aren't truly independent, and that puts the fly in the ointment.)&lt;/p&gt;&#10;&#10;&lt;h2&gt;Fudging&lt;/h2&gt;&#10;&#10;&lt;p&gt;So does anyone have any suggestions as to how to &quot;fudge&quot; non-independent measurements to improve an estimate? For the two most egregious cases I can come up with a fair estimate of how connected the measurements are, but I don't have a feel for how to factor that knowledge in.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-07-19T21:28:44.167" Id="13275" LastActivityDate="2011-09-03T13:01:54.787" LastEditDate="2011-07-20T22:12:30.913" LastEditorUserId="5392" OwnerUserId="5392" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;conditional-probability&gt;" Title="Bayesian probability &gt; 1 -- is it possible?" ViewCount="3386" />
  <row Body="&lt;p&gt;Welcome to the wonderful world of Statistics.  Despite a Masters degree in Medical Statistics, it never ceases to amaze me that the one thing I am learning over and over again is how little I actually understand about this fascinating modality!!&lt;/p&gt;&#10;&#10;&lt;p&gt;For what it is worth from a non-practising and not very experienced statistician, you need to look at a time-series data analysis with a time varying covariate. The Cox Proportional Hazards model can be used for this data with relative ease.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check out this paper on the CRAN website:&#10;&lt;a href=&quot;http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf&quot; rel=&quot;nofollow&quot;&gt;Cox Proportional Hazards Regression (J Fox, 2002)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My two cents - hope it's useful.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-20T01:19:15.733" Id="13289" LastActivityDate="2011-07-20T01:19:15.733" OwnerUserId="5317" ParentId="13230" PostTypeId="2" Score="1" />
&#10;$$&#10;$$
  <row Body="&lt;p&gt;A good reference on metrics for the spatial distribution of point patterns is the &lt;a href=&quot;http://www.icpsr.umich.edu/CrimeStat/download.html&quot; rel=&quot;nofollow&quot;&gt;CrimeStat manual&lt;/a&gt; (in particular for this question, &lt;a href=&quot;http://www.icpsr.umich.edu/CrimeStat/files/CrimeStatChapter.4.pdf&quot; rel=&quot;nofollow&quot;&gt;Chapter 4&lt;/a&gt; will be of interest). Similar to the metric Macro suggested, the Standard Distance Deviation is similar to a 2D standard deviation (the only difference is that you would divide by &quot;n-2&quot; not &quot;n&quot; in the first formula Macro gave).&lt;/p&gt;&#10;&#10;&lt;p&gt;Your example experiment actually reminds me a bit of how studies evaluate &lt;a href=&quot;http://www.nij.gov/maps/gp.htm&quot; rel=&quot;nofollow&quot;&gt;Geographic Offender Profiling&lt;/a&gt;, and hence the metrics used in those works may be of interest. In particular the terms precision and accuracy are used quite a bit and would be pertinent to the study. Guesses could have a small standard deviation (i.e. precise) but still have a very low accuracy.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-20T04:11:06.267" Id="13292" LastActivityDate="2011-07-20T04:11:06.267" OwnerUserId="1036" ParentId="13272" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="13297" AnswerCount="1" Body="&lt;p&gt;I have a text corpus, from which I have computed the unigram probabilities of every word.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, let $p_{i}$ &amp;amp; $p_{j}$ be the unigram probabilities of $i$ and $j$ respectively, what will be the probability with which a pair of words $i$ and $j$ will co-occur(within a maximum gap of 1 word) when independence between $i$ and $j$ is assumed? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&#10;Here $p_{i}$ is the probability of any given word being $i$. &lt;/p&gt;&#10;&#10;&lt;p&gt;By co-occurrence probability, I mean the chance that word $i$ &amp;amp; $j$ co-occur in a random sequence of three words drawn from the corpus.&#10;By co-occurring I mean that $i$ and $j$ occurring anywhere in this 3 word random sequence within a maximum word distance of 1. (For example, $i$*$j$ and $ij$ are both valid co-occurrences of $i$ &amp;amp; $j$ but $i**j$ is not a valid co-occurrence since the distance between $i$ &amp;amp; $j$ is more than 1.) &lt;/p&gt;&#10;&#10;&lt;p&gt;And finally, $ijij$ counts as 3 co-occurrences.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-20T10:56:57.087" FavoriteCount="1" Id="13296" LastActivityDate="2011-07-20T13:47:18.050" LastEditDate="2011-07-20T12:25:50.727" LastEditorUserId="4966" OwnerUserId="4966" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;conditional-probability&gt;" Title="How to compute co-occurrence probability assuming independence from unigram probabilities?" ViewCount="708" />
  
  
  <row Body="&lt;p&gt;I don't know if there is a generally accepted generic term, but I think you might say &quot;classifier performance metrics/measures&quot; (like in the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;ROCR&lt;/code&gt;), or &quot;measures of predictive/classification performance&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The widely cited paper by &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.4088&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;Fawcett&lt;/a&gt;, for example, talks about &quot;common performance metrics&quot; and lists true positive rate (tpr), fpr, sensitivity, speficitiy, precision, and recall.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-07-20T16:24:06.213" Id="13304" LastActivityDate="2011-07-20T16:24:06.213" OwnerUserId="5020" ParentId="13303" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Chapter 5: &lt;a href=&quot;http://press.princeton.edu/titles/8434.html&quot; rel=&quot;nofollow&quot;&gt;http://press.princeton.edu/titles/8434.html&lt;/a&gt;&lt;br&gt;&#10;Chapter 12: (Cameron&amp;amp;Trivedi textbook)&lt;br&gt;&#10;Chapter 15: (Greene's 7th edition)&lt;br&gt;&#10;This one discusses the general issues and is freely available: &lt;a href=&quot;http://elsa.berkeley.edu/books/choice2.html&quot; rel=&quot;nofollow&quot;&gt;http://elsa.berkeley.edu/books/choice2.html&lt;/a&gt;&lt;br&gt;&#10;And I also recall that the handbook of econometrics chapter on simulation (40) is particularly readable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edited to remove extra links as only 2 are allowed.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-07-21T03:39:50.040" Id="13324" LastActivityDate="2011-07-21T03:39:50.040" OwnerUserId="5494" ParentId="13322" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Using the usual notations, the log-likelihood of the ML method is&lt;/p&gt;&#10;&#10;&lt;p&gt;$l(\beta_0, \beta_1 ; y_1, \ldots, y_n) = \sum_{i=1}^n \left\{ -\frac{1}{2} \log (2\pi\sigma^2) - \frac{(y_{i} - (\beta_0 + \beta_1 x_{i}))^{2}}{2 \sigma^2} \right\}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;It has to be maximised with respect to $\beta_0$ and $\beta_1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;But, it is easy to see that this is equivalent to minimising&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^{n}  (y_{i} - (\beta_0 + \beta_1 x_{i}))^{2} $.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, both ML and OLS lead to the same solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;More details are provided in these &lt;a href=&quot;http://data.princeton.edu/wws509/notes/c2s2.html&quot;&gt;nice lecture notes&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-07-21T10:23:57.200" Id="13332" LastActivityDate="2011-07-21T10:36:47.723" LastEditDate="2011-07-21T10:36:47.723" LastEditorUserId="3019" OwnerUserId="3019" ParentId="13331" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;My first advice would be that unless identifying the informative features is a goal of the analysis, don't bother with feature selection and just use a regularised model, such a penalised logistic regression, ridge regression or SVM, and let the regularisation handle the over-fitting.  It is often said that feature selection improves classifier performance, but it isn't always true.&lt;/p&gt;&#10;&#10;&lt;p&gt;To deal with the class imbalance problem, give different weights to the patterns from each class in calculating the loss function used to fit the model.  Choose the ratio of weights by cross-validation (for a probabilistic classifier you can work out the asymptically optimal weights, but it generally won't give optimal results on a finite sample).  If you are using a classifier that can't give different weights to each class, then sub-sample the majority class instead, where again the ratio of positive and negative patterns is determined by cross-validation (make sure the test partition in each fold of the cross-validation procedure has the same relative class frequencies you expect to see in operation).&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, it is often the case in practical application with a class imbalance that false-positives and false-negatives are not of equal seriousness, so incorporate this into the construction of the classifier.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-07-21T13:17:33.747" Id="13337" LastActivityDate="2011-07-21T13:17:33.747" OwnerUserId="887" ParentId="13335" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="13343" AnswerCount="2" Body="&lt;p&gt;I am interested in testing a simple mediation model with one IV, one DV, and one mediator. The indirect effect is significant as tested by the Preacher and Hayes SPSS macro, which suggests the mediator does serve to statistically mediate the relationship. &lt;/p&gt;&#10;&#10;&lt;p&gt;When reading about mediation I have read things such as &quot;Note that a mediational model is a causal model.&quot; - &lt;a href=&quot;http://davidakenny.net/cm/mediate.htm&quot;&gt;David Kenny&lt;/a&gt;. I can certainly appreciate the use of mediation models as causal models, and indeed, if a model is theoretically sound, I can see this as very useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my model, however, the mediator (a trait considered to be a diathesis for anxiety disorders) is not caused by the independent variable (symptoms of an anxiety disorder). Rather, the mediator and independent variables are related, and I believe the association between the independent variable and the dependent variable can be explained largely by variance between the IV-mediator-DV. In essence I am trying to demonstrate that previous reports of the IV-DV relationship can be explained by a related mediator that is not caused by the IV. &lt;/p&gt;&#10;&#10;&lt;p&gt;Mediation is useful in this case because it explains how the IV-DV relationship can be statistically explained by the IV-Mediator-DV relationship. My problem is the question of causation. Could a review come back and tell us that the mediation is not appropriate because the IV does not in fact cause the mediator (which I would have never argued in the first place)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this make sense? Any feedback on this matter would be greatly appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: What I mean to say is that X is correlated with Y not because it causes Y, but because Z causes Y (partially) and because X and Z are highly correlated. A bit confusing, but that is it. The causal relationships in this instance are not really in question and this manuscript is not so much about causation. I simply seek to demonstrate that variance between X and Y can be explained by variance between Z and Y. So basically, that X is correlated indirectly to Y through Z (the &quot;mediator&quot; in this case). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-21T15:00:49.527" FavoriteCount="9" Id="13340" LastActivityDate="2014-05-23T06:01:55.143" LastEditDate="2011-07-22T00:38:18.910" LastEditorUserId="183" OwnerUserId="3262" PostTypeId="1" Score="14" Tags="&lt;causal-inference&gt;&lt;mediation&gt;" Title="Are mediation analyses inherently causal?" ViewCount="2728" />
  <row AnswerCount="3" Body="&lt;p&gt;I will give my examples with R calls. First a simple example of a linear regression with a dependent variable 'lifespan', and two continuous explanatory variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data.frame(height=runif(4000,160,200))-&amp;gt;human.life&#10;human.life$weight=runif(4000,50,120)&#10;human.life$lifespan=sample(45:90,4000,replace=TRUE)&#10;summary(lm(lifespan~1+height+weight,data=human.life))&#10;&#10;Call:&#10;lm(formula = lifespan ~ 1 + height + weight, data = human.life)&#10;&#10;Residuals:&#10;Min       1Q   Median       3Q      Max &#10;-23.0257 -11.9124  -0.0565  11.3755  23.8591 &#10;&#10;Coefficients:&#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 63.635709   3.486426  18.252   &amp;lt;2e-16 ***&#10;height       0.007485   0.018665   0.401   0.6884    &#10;weight       0.024544   0.010428   2.354   0.0186 *  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 13.41 on 3997 degrees of freedom&#10;Multiple R-squared: 0.001425,   Adjusted R-squared: 0.0009257 &#10;F-statistic: 2.853 on 2 and 3997 DF,  p-value: 0.05781&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In order to find the estimate of 'lifespan' when the value of 'weight' is 1, I add (Intercept)+height=63.64319&lt;/p&gt;&#10;&#10;&lt;p&gt;Now what if I have a similar data frame, but one where one of the explanatory variables is categorical?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data.frame(animal=rep(c(&quot;dog&quot;,&quot;fox&quot;,&quot;pig&quot;,&quot;wolf&quot;),1000))-&amp;gt;animal.life&#10;animal.life$weight=runif(4000,8,50)&#10;animal.life$lifespan=sample(1:10,replace=TRUE)&#10;summary(lm(lifespan~1+animal+weight,data=animal.life))&#10;&#10;Call:&#10;lm(formula = lifespan ~ 1 + animal + weight, data = animal.life)&#10;&#10;Residuals:&#10;Min      1Q  Median      3Q     Max &#10;-4.7677 -2.7796 -0.1025  3.1972  4.3691 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 5.565556   0.145851  38.159  &amp;lt; 2e-16 ***&#10;animalfox   0.806634   0.131198   6.148  8.6e-10 ***&#10;animalpig   0.010635   0.131259   0.081   0.9354    &#10;animalwolf  0.806650   0.131198   6.148  8.6e-10 ***&#10;weight      0.007946   0.003815   2.083   0.0373 *  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 2.933 on 3995 degrees of freedom&#10;Multiple R-squared: 0.01933,    Adjusted R-squared: 0.01835 &#10;F-statistic: 19.69 on 4 and 3995 DF,  p-value: 4.625e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this case, to find the estimate of 'lifespan' when the value of 'weight' is 1, should I add each of the coefficients for 'animal' to the intercept: (Intercept)+animalfox+animalpig+animalwolf? Or what is the proper way to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&#10;Sverre&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-07-21T22:12:17.597" FavoriteCount="1" Id="13353" LastActivityDate="2013-11-08T01:30:52.750" LastEditDate="2012-07-14T13:00:27.373" LastEditorUserId="4856" OwnerUserId="5501" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;interpretation&gt;&lt;regression-coefficients&gt;" Title="Interpreting coefficient in a linear regression model with categorical variables" ViewCount="5402" />
  
  <row Body="&lt;p&gt;The maximum is $1/(p-1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To see this, note first that the eigenvalues of the matrix with all off-diagonal entries equal to a constant $x$ are $1-x$ (with multiplicity $p-1$) and $1+(p-1)x$.  When $x \lt -1/(p-1)$, the smallest eigenvalue will therefore be negative implying the matrix is not positive definite.  Because the smallest eigenvalue is a continuous function of the entries, we can find a positive $\epsilon$ such that when all off-diagonal entries are in the interval $[x, x+\epsilon]$ (but no longer all equal to each other), the smallest eigenvalue remains negative.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose $a \gt 1/(p-1)$.  Setting $x=-a$, choose an $\epsilon$ as just described and if necessary make it even smaller, but still positive, to assure that $a - \epsilon \gt 1/(p-1)$.  Assuming the off-diagonal entries are independently generated, the probability that all entries lie in the interval $[-a, -a+\epsilon]$ equals $(\epsilon / (2a))^{p(p-1)/2} \gt 0$, showing that the matrix has a positive probability of not being positive definite.&lt;/p&gt;&#10;&#10;&lt;p&gt;This has established $1/(p-1)$ as an &lt;em&gt;upper bound&lt;/em&gt; for $a$.  We need to show that it suffices.  Consider an arbitrary symmetric $p$ by $p$ matrix $(a_{ij})$ with unit diagonal and all entries in size less than $1/p$.  By a suitable induction on $p$, and by virtue of &lt;a href=&quot;http://en.wikipedia.org/wiki/Positive-definite_matrix#Characterizations&quot; rel=&quot;nofollow&quot;&gt;Sylvester's Criterion&lt;/a&gt;, it suffices to show this matrix has positive determinant.  Row-reduction using the first row reduces this question to considering the sign of a $p-1$ by $p-1$ determinant with entries $(a_{ij} / (1 + a_{1i})$.  Because $-1/p \lt a_{1i} \lt 1/p$, these clearly are less than $1/(p-1)$ in absolute value, so we are done by induction.  (The base case $p=2$ is trivial.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-22T13:36:32.047" Id="13372" LastActivityDate="2011-07-22T13:36:32.047" OwnerUserId="919" ParentId="13368" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;To me the slope graph looks really messy and I think I'd have trouble looking at it, especially across eight time series.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not an expert in graph design, so this may also be a no-go, but have you considered four colors with three types of plot type?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Though, I think there is an even better approach.  I know you say that 4 colors is a no-go, I'm about to ignore that.  It is probably canonically true... but you are describing fruits.  These have canonical colors as well as shapes associated with them. If you use those colors and shapes I think it would be hard to go wrong.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Using colors alone there might be some confusion, e.g. green apple vs honeydew, red apple vs watermelon, etc. But using colors poses an additional problem, color blind individuals. You can test for the extent to which this would be a problem by creating an image of your different colors and looking here: &lt;a href=&quot;http://www.vischeck.com/vischeck/&quot; rel=&quot;nofollow&quot;&gt;http://www.vischeck.com/vischeck/&lt;/a&gt;. Protanopia and Deuteranopia, forms of red-green color blindness, are by far the most common (occurring almost always in males).  Even so, color blindness is a misnomer and if you select your pallet carefully the differences in shades may be sufficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;In conjunction with a color approach, you want to use fruit shaped points.  These are unlikely to be a default in a plotting program and you may have to spend some time in photoshop to make it look right.  Even if you can't take that time, differing geometric plot points AND color should make things reasonable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, you could use the approach I suggest with a slope graph.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a side note, if you have a technically/mathematically astute audience, a Y-axis in log odds might mean more to them that percentages alone.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-07-22T13:40:32.653" Id="13374" LastActivityDate="2011-07-22T13:49:11.357" LastEditDate="2011-07-22T13:49:11.357" LastEditorUserId="196" OwnerUserId="196" ParentId="13327" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Maybe use the laplace distribution, which has density &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{1}{2s} {\rm exp} \left( -\frac{|x-\mu|}{s} \right ) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mu \in \mathbb{R}, s &amp;gt; 0$ are parameters. As Wolfgang pointed out, your distribution appears to be symmetric but has higher kurtosis than the normal distribution; the laplace distribution has these properties and the QQ plot looks pretty similar to what you're observed from your data: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(VGAM)&#10;x=rlaplace(1000)&#10;qqnorm(x)&#10;qqline(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(you will need to download the package VGAM). The logistic distribution could also work but the QQ plots don't agree with what you've observed as well. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-22T14:34:07.277" Id="13375" LastActivityDate="2011-07-22T14:34:07.277" OwnerUserId="4856" ParentId="13362" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="13388" AnswerCount="1" Body="&lt;p&gt;We teach supplementary lessons in nearly two dozen local schools, and have two data sets of approximately four hundred records each from pre-post tests given at these schools. Each record contains pre and post values (correct, incorrect) for questions on 12 topics as well as whether or not there was an intervention (lesson taught) relating to that topic.&#10;Our goal is of course to assess the impact of the lessons taught.&lt;/p&gt;&#10;&#10;&lt;p&gt;The pre and post test responses are pair-matched by student, and clustered by school since&#10;the lessons taught vary by school, in addition to the other factors generally accepted as valid for clustering at this level i.e; similar socioeconomic background, school culture, common instructors, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without clustering, McNemar is the accepted test for analysis of this data, and&#10;several authors have explored various modifications to McNemar to allow for clustering including:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Methods for the Analysis of Pair-Matched Binary Data from School-Based&#10;Intervention Studies. Vaughan &amp;amp; Begg. 1999.&#10;doi: 10.3102/10769986024004367&lt;/li&gt;&#10;&lt;li&gt;Analysis of clustered matched-pair data. Durkalski et al. 2003.&#10;doi: 10.1002/sim.1438&lt;/li&gt;&#10;&lt;li&gt;Methods for the Statistical Analysis of Binary Data in Split-Cluster Designs.&#10;Donner, Klar, Zou. 2004.&#10;doi: 10.1111/j.0006-341X.2004.00247.x&lt;/li&gt;&#10;&lt;li&gt;Adjustment to the McNemar’s Test for the Analysis of Clustered&#10;Matched-Pair Data.&#10;McCarthy. 2007. &lt;a href=&quot;http://biostats.bepress.com/cobra/ps/art29/&quot; rel=&quot;nofollow&quot;&gt;http://biostats.bepress.com/cobra/ps/art29/&lt;/a&gt; (Free to download)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have subsequently experimented with the Durkalski method as documented in McCarthy,&#10;since it seems to be deemed rather robust, as well as being the simplest for me to&#10;understand and code. However, none of the documented methods fit our case&#10;exactly as they use the matched pairs for pre-post or control-treatment only, and treat &#10;the clusters as a single class. We actually have matched pairs of pre-post in multiple &#10;control &amp;amp; treatment clusters, but this latter level of information is not used and &#10;discarding the ~50% of our data points from the control groups seems sub-optimal. Is &#10;anyone aware of a technique designed to analyze this data configuration, or someone whom &#10;might be interested in exploring this area?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-22T16:14:01.627" Id="13383" LastActivityDate="2011-07-22T18:01:59.267" OwnerUserId="5510" PostTypeId="1" Score="3" Tags="&lt;binary-data&gt;" Title="Analyzing (hierarchical?) clustered pair-matched binary data" ViewCount="472" />
  
  <row Body="&lt;p&gt;Hint: &lt;em&gt;quantization&lt;/em&gt; might be a better keyword to search information.&lt;/p&gt;&#10;&#10;&lt;p&gt;Designing an &quot;optimal&quot; quantization requires some criterion. To try to conserve  the first moment of the discretized variable ... sounds interesting, but I don't think it's very usual.&lt;/p&gt;&#10;&#10;&lt;p&gt;More frequently (especially if we assume a probabilistic model, as you do) one tries to minimize some &lt;em&gt;distortion&lt;/em&gt;: we want the discrete variable to be close to the real one, in some sense. If we stipulate minimum average squared error (not always the best error measure, but the most tractable), the problem is well known, and we can easily build a non-uniform quantizer with &lt;a href=&quot;http://en.wikipedia.org/wiki/Quantization_%28signal_processing%29#Rate.E2.80.93distortion_quantizer_design&quot; rel=&quot;nofollow&quot;&gt;minimum rate distortion&lt;/a&gt;, if we know the probability of the source; this is almost a synonym of &quot;Max Lloyd quantizer&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because a non-uniform quantizer (in 1D) is equivalent to pre-applying a non-linear transformation to a uniform quantizer, this kind of transformation (&quot;companding&quot;) (in probabilistic terms, a function that turns our variable into a quasi-uniform) are very related to non uniform quantization (sometimes the concepts are used interchangeably). A pair of venerable examples are the &lt;a href=&quot;http://en.wikipedia.org/wiki/%CE%9C-law_algorithm&quot; rel=&quot;nofollow&quot;&gt;u-Law and A-Law&lt;/a&gt; specifications for telephony.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-24T00:19:56.330" Id="13418" LastActivityDate="2012-03-23T23:04:41.163" LastEditDate="2012-03-23T23:04:41.163" LastEditorUserId="7972" OwnerUserId="2546" ParentId="13342" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="13437" AnswerCount="1" Body="&lt;p&gt;As a non-statistician, I have a real world statistical/probability problem that I'm having trouble framing.&#10;The software I rely on in inventory management interprets the 'movements' (number of times inventory is used) in a strange way. It offers the number of months, out of 24 months, that the item has been used. &#10;For example, a moving code of 20 means that the item was used at least once for 20 out of 24 months.. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I need to be able to do is translate that to find &lt;strong&gt;the most probable number of movements over that 24 month period.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;If movements randomly fall into 20 out of 24 months with no limitations, obviously the number of movements that really occur is likely to be much greater than 20. How much greater?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry, this question is extra challenging because I have no ideas on how to begin tackling this. Any help is much appreciated. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-24T10:42:19.397" Id="13428" LastActivityDate="2011-07-25T00:48:30.593" LastEditDate="2011-07-25T00:48:30.593" LastEditorUserId="183" OwnerUserId="5206" PostTypeId="1" Score="4" Tags="&lt;probability&gt;" Title="Probability of total events occurring given that one or more events occur in specified number of months" ViewCount="317" />
  
  
  <row AcceptedAnswerId="13467" AnswerCount="1" Body="&lt;p&gt;Are there R packages that offer eigenvalue de-noising methods grounded in Random Matrix Theory? Various cleansing methods include the Power Law and Krazanowski filter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-25T16:23:25.453" Id="13466" LastActivityDate="2011-07-25T16:38:54.890" LastEditDate="2011-07-25T16:38:54.890" LastEditorUserId="8101" OwnerUserId="8101" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;variance-covariance&gt;" Title="Eigenvalue cleansing methods in R?" ViewCount="93" />
  <row AcceptedAnswerId="13472" AnswerCount="1" Body="&lt;p&gt;I have used &lt;code&gt;rpart.control&lt;/code&gt; for &lt;code&gt;minsplit=2&lt;/code&gt;, and got the following results from &lt;code&gt;rpart()&lt;/code&gt; function. In order to avoid overfitting the data, do I need to use splits 3 or splits 7? Shouldn't I use splits 7? Please let me know.&lt;/p&gt;&#10;&#10;&lt;p&gt;Variables actually used in tree construction: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[1] ct_a ct_b usr_a&#10;&#10;Root node error: 23205/60 = 386.75&#10;&#10;n= 60        &#10;&#10;    CP nsplit rel error  xerror     xstd&#10;1 0.615208      0  1.000000 1.05013 0.189409&#10;2 0.181446      1  0.384792 0.54650 0.084423&#10;3 0.044878      2  0.203346 0.31439 0.063681&#10;4 0.027653      3  0.158468 0.27281 0.060605&#10;5 0.025035      4  0.130815 0.30120 0.058992&#10;6 0.022685      5  0.105780 0.29649 0.059138&#10;7 0.013603      6  0.083095 0.21761 0.045295&#10;8 0.010607      7  0.069492 0.21076 0.042196&#10;9 0.010000      8  0.058885 0.21076 0.042196&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-07-25T16:56:44.123" Id="13471" LastActivityDate="2011-07-25T21:52:01.743" LastEditDate="2011-07-25T21:52:01.743" LastEditorUserId="930" OwnerUserId="25133" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;cart&gt;&lt;rpart&gt;" Title="How to choose the number of splits in rpart()?" ViewCount="1445" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Using &lt;code&gt;R plot()&lt;/code&gt; and &lt;code&gt;plotcp()&lt;/code&gt; methods, we can visualize linear regression model (&lt;code&gt;lm&lt;/code&gt;) as an equation and decision tree model (&lt;code&gt;rpart&lt;/code&gt;) as a tree. We can develop k-nearest neighbour model using R &lt;code&gt;kknn()&lt;/code&gt; method, but I don't know how to present this model. Please suggest me some R methods that produce nice graphs for knn model visualization.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-26T04:27:33.463" FavoriteCount="1" Id="13485" LastActivityDate="2011-07-26T22:27:26.160" OwnerUserId="25133" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;k-nearest-neighbour&gt;" Title="Visualizing k-nearest neighbour?" ViewCount="741" />
  <row AcceptedAnswerId="13489" AnswerCount="1" Body="&lt;p&gt;I was playing around with ggplot2 using the following commands to fit a line to my data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(data=datNorm, aes(x=Num, y=Val)) + geom_point() + &#10;stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom=&quot;errorbar&quot;, colour=&quot;red&quot;, width=0.8) + &#10;stat_sum_single(median) + &#10;stat_sum_single(mean, colour=&quot;blue&quot;) + &#10;geom_smooth(level = 0.95, aes(group=1), method=&quot;lm&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The red dots are median values, blue are the means and the vertical red lines show the error bars. As a final step, I used &lt;code&gt;geom_smooth&lt;/code&gt; to fit a line using linear smoothing so I used &lt;code&gt;method=&quot;lm&quot;&lt;/code&gt;. Along with the line, a dull shade was generated as well around the line. While I figured out how to remove it from the documentation, the option I used to turn it off is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;se: display confidence interval around smooth? &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can someone please tell me what I am supposed to understand from the shade around the line? Specifically, I am trying to understand how to interpret it. It must be some goodness-of-fit for the line perhaps but any extra information could be very useful to me. Any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7y7Pm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-26T05:10:40.910" Id="13487" LastActivityDate="2011-07-26T07:50:31.570" OwnerUserId="2164" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;modeling&gt;&lt;dataset&gt;&lt;data-mining&gt;&lt;ggplot2&gt;" Title="What does this blur around the line mean in this graph?" ViewCount="1292" />
  <row Body="&lt;p&gt;kNN is just a simple interpolation of feature space, so its visualization would be in fact equivalent to just drawing a train set in some less or more funky manner, and unless the problem is simple this would be rather harsh to decipher. &lt;/p&gt;&#10;&#10;&lt;p&gt;You may do this by counting the distances between train objects the way you did it in kknn, then use &lt;code&gt;cmdscale&lt;/code&gt; to cast this on 2D, finally plot directly or using some smoothed scatterplot using colours to show classes or values (the smoothed regression version would require probably some hacking with hue and intensity). However, as I wrote, this would be probably a totally useless plot.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-26T07:54:30.830" Id="13490" LastActivityDate="2011-07-26T07:54:30.830" OwnerUserId="88" ParentId="13485" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You have to decide to what extent one substitutes for another and whether this measure is constant or are there &quot;decreasing returns&quot; to one of them. Economists typically use min[a*x,y] for perfect complementarity (no substitution), a*x+y for perfect substitution (a units of x for unit of y whatever y is) and a &quot;compromise&quot; (called Cobb-Douglas function) of x^a*y^b with b typically in (0,1) and equal to 1-a so you need less x for unit of y the more you have of y to keep the function at the same level.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-26T22:04:02.997" Id="13525" LastActivityDate="2011-07-26T22:04:02.997" OwnerUserId="5494" ParentId="13518" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Here are my two cents:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;From the graphic, it seems that the main difference is in the beginning. try to remove the begenning (until point 5 ot 6 in the x-axis in the graphic) to see if I'm right.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Why don't you model it with a poisson (o negative binomial if there is too much zeros)?&#10;Your independente variables would be the platafform, conditions and a time effect.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Maybe some hierarchical model could help here.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-07-26T23:58:43.880" Id="13529" LastActivityDate="2011-07-26T23:58:43.880" OwnerUserId="3058" ParentId="13395" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;In a paper I've written I model the random variables $X+Y$ and $X-Y$ rather than $X$ and $Y$ to effectively remove the problems that arise when $X$ and $Y$ are highly correlated and have equal variance (as they are in my application). The referees want me to give a reference. I could easily prove it, but being an application journal they prefer a reference to a simple mathematical derivation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have any suggestions for a suitable reference? I thought there was something in Tukey's EDA book (1977) on sums and differences but I can't find it.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-07-27T01:38:17.490" FavoriteCount="1" Id="13530" LastActivityDate="2011-08-21T02:39:29.123" OwnerUserId="159" PostTypeId="1" Score="9" Tags="&lt;correlation&gt;&lt;multicollinearity&gt;" Title="Reference for the sum and difference of highly correlated variables being almost uncorrelated" ViewCount="244" />
  <row Body="&lt;p&gt;You seem to include the interaction term &lt;code&gt;ub:lb&lt;/code&gt;, but not &lt;code&gt;ub&lt;/code&gt; and &lt;code&gt;lb&lt;/code&gt; themselves as separate predictors. This would violate the so-called &quot;principle of marginality&quot; which states that higher-order terms should only include variables present in lower-order terms (&lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_marginality&quot;&gt;Wikipedia for a start&lt;/a&gt;). Effectively, you are now including a predictor that is just the element-wise product of &lt;code&gt;ub&lt;/code&gt; and &lt;code&gt;lb&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;$VIF_{j}$ is just $1 / (1-R_{j}^{2}$) where $R_{j}^{2}$ is the $R^{2}$ value when you run a regression with your original predictor variable $j$ as criterion predicted by all remaining predictors (it is also the $j$-th diagonal element of $R_{x}^{-1}$, the inverse of the correlation matrix of the predictors). A VIF-value of 50 thus indicates that you get an $R^{2}$ of .98 when predicting &lt;code&gt;ub&lt;/code&gt; with the other predictors, indicating that &lt;code&gt;ub&lt;/code&gt; is almost completely redundant (same for &lt;code&gt;lb&lt;/code&gt;, $R^{2}$ of .97).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would start doing all pairwise correlations between predictors, and run the aforementioned regressions to see which variables predict &lt;code&gt;ub&lt;/code&gt; and &lt;code&gt;lb&lt;/code&gt; to see if the redundancy is easily explained. If so, you can remove the redundant predictors. You can also look into ridge regression (&lt;code&gt;lm.ridge()&lt;/code&gt; from package &lt;code&gt;MASS&lt;/code&gt;in R).&lt;/p&gt;&#10;&#10;&lt;p&gt;More advanced multicollinearity diagnostics use the eigenvalue-structure of $X^{t}X$ where $X$ is the design matrix of the regression (i.e., all predictors as column-vectors). The condition $\kappa$ is $\sqrt(\lambda_{max} / \lambda_{min})$ where $\lambda_{max}$ and $\lambda_{min}$ are the largest and smallest ($\neq 0$) eigenvalues of $X^{t}X$. In R, you can use &lt;code&gt;kappa(lm(&amp;lt;formula&amp;gt;))&lt;/code&gt;, where the &lt;code&gt;lm()&lt;/code&gt; model typically uses the standardized variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Geometrically, $\kappa$ gives you an idea about the shape of the data cloud formed by the predictors. With 2 predictors, the scatterplot might look like an ellipse with 2 main axes. $\kappa$ then tells you how &quot;flat&quot; that ellipse is, i.e., is a measure for the ratio of the length of largest axis to the length of the smallest main axis. With 3 predictors, you might have a cigar-shape, and 3 main axes. The &quot;flatter&quot; your data cloud is in some direction, the more redundant the variables are when taken together.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some rules of thumb for uncritical values of $\kappa$ (I heard less than 20). But be advised that $\kappa$ is not invariant under data transformations that just change the unit of the variables - like standardizing. This is unlike VIF: &lt;code&gt;vif(lm(y ~ x1 + x2))&lt;/code&gt; will give you the same result as &lt;code&gt;vif(lm(scale(y) ~ scale(x1) + scale(x2)))&lt;/code&gt; (as long as there are not multiplicative terms in the model), but &lt;code&gt;kappa(lm(y ~ x1 + x2))&lt;/code&gt; and &lt;code&gt;kappa(lm(scale(y) ~ scale(x1) + scale(x2)))&lt;/code&gt; will almost surely differ.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-07-27T10:14:33.957" Id="13545" LastActivityDate="2011-07-27T10:14:33.957" OwnerUserId="1909" ParentId="13532" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;If you continue to update your prior in the manner that you described, aren't you assuming that the process that is generating your data stationary? &lt;/p&gt;&#10;&#10;&lt;p&gt;If the answer to the question is yes, then all that you should need to do is take a random sample of your data to create a likelihood function and then generate the posterior. In that way you would not have to worry about overflow. &lt;/p&gt;&#10;&#10;&lt;p&gt;On the otherhand, although I do not know what the process is that you are investigating, it seems almost impossible that a process could remain stationary over any long period of time. In fact, you could check to see if your data generating process is serially changing by monitoring independent estimates of the alpha and beta parameters over time. Minimally, you could make a control chart of the two parameters; or better yet there is probably a simple way to implement a likelihood ratio to check for stationarity. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-27T13:16:05.547" Id="13554" LastActivityDate="2011-07-27T13:16:05.547" OwnerUserId="3805" ParentId="13540" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="15292" AnswerCount="3" Body="&lt;p&gt;I'm trying to draw violin plots and wondering if there is an accepted best practice for scaling them across groups. Here are three options I've tried using the R &lt;code&gt;mtcars&lt;/code&gt; data set (Motor Trend Cars from 1973, &lt;a href=&quot;http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=plyr%3asplitter_d&quot; rel=&quot;nofollow&quot;&gt;found here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;h2&gt;Equal Widths&lt;/h2&gt;&#10;&#10;&lt;p&gt;Seems to be what the &lt;a href=&quot;http://www.seqc.es/dl.asp?175.145.205.255.15.30.27.21.118.133.24.113.255.173.41.5.166.146.64.144.249.7.59.174.215.16.239.104.101.102.1.136.118.147.148.2.78.140&quot; rel=&quot;nofollow&quot;&gt;original paper&lt;/a&gt; does and what R &lt;code&gt;vioplot&lt;/code&gt; does (&lt;a href=&quot;http://www.statmethods.net/graphs/boxplot.html&quot; rel=&quot;nofollow&quot;&gt;example&lt;/a&gt;). Good for comparing shape.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/u3Hcz.png&quot; alt=&quot;Equal Area Violin Plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h2&gt;Equal Areas&lt;/h2&gt;&#10;&#10;&lt;p&gt;Feels right since each plot is a probability plot, and so the area of each should equal 1.0 in some coordinate space. Good for comparing density within each group, but seems more appropriate if the plots are overlaid.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hpDMy.png&quot; alt=&quot;Equal Width Violin Plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h2&gt;Weighted Areas&lt;/h2&gt;&#10;&#10;&lt;p&gt;Like equal area, but weighted by number of observations. 6-cyl gets relatively thinner since there are fewer of those cars. Good for comparing density across groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lSEjd.png&quot; alt=&quot;Weighted Area Violin Plots&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-07-27T13:52:41.023" FavoriteCount="2" Id="13555" LastActivityDate="2014-02-02T10:54:24.613" LastEditDate="2011-09-07T21:08:05.997" LastEditorUserId="930" OwnerUserId="1191" PostTypeId="1" Score="12" Tags="&lt;distributions&gt;&lt;data-visualization&gt;&lt;nonparametric&gt;" Title="How to scale violin plots for comparisons?" ViewCount="750" />
  
  
&#10;$$&#10;where $f(x)$ is the black curve and $g(x)$ is the red curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you only appear to know the curves at discrete points, you will probably need to interpolate between the points in order to estimate the integral. It appears that you are working in R, so the &lt;code&gt;approx()&lt;/code&gt; function may be useful for the interpolation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-28T06:55:37.757" Id="13574" LastActivityDate="2011-07-28T06:55:37.757" OwnerUserId="159" ParentId="13572" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;With regards to whether you should compute agreement for each item, this depends somewhat on how you plan to analyse the data. &#10;&lt;ul&gt;&#10;&lt;li&gt;If you plan to compute scale scores (e.g., sum up the binary responses or sum up the likert responses) to form a scale, then you could perform a reliability analysis on the scale scores. In this situation, you may be starting to have enough scale points to use other procedures for inter-rater reliability assessment that assume numeric data, such as looking at ICC. Your overall evaluation of reliability would then focus on the scale score. Reliability analysis of individual items might then just be used as a means of assessing which items to include in the composite scale (e.g., you could drop items with particularly low agreement).&lt;/li&gt;&#10;&lt;li&gt;If you plan to report individual items, then you would want to report kappa for each item. You may still find it useful to summarise these individual kappas, in order to quickly communicate the general reliability of the items (e.g., report range, mean, and sd of kappa across items).&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;If you don't like the Kappa values that you are getting, this is not a reason not to use Kappa (apologies for the triple negative). &#10;&lt;ul&gt;&#10;&lt;li&gt;It may be that your rules of thumb for interpreting Kappa are inappropriate. &lt;/li&gt;&#10;&lt;li&gt;Alternatively, it may be that items are just not that reliable (high percentages of agreement can be obtained when variables are skewed even when the two raters disagree on which cases are in the minority category). In general, individual items are going to be less reliable than composite scales; also some binary evaluations are quite clear (e.g., gender), but in other cases where a judge is being asked whether an object passes over some threshold, ratings might be more reliable if they were asked to rate on a continuum.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;You can use an ordinal kappa on likert items. &lt;a href=&quot;http://stats.stackexchange.com/questions/3539/inter-rater-reliability-for-ordinal-or-interval-data/3546#3546&quot;&gt;@chl has an excellent discussion of the issues and alternatives here&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-07-29T00:57:57.683" Id="13604" LastActivityDate="2011-07-29T01:03:29.370" LastEditDate="2011-07-29T01:03:29.370" LastEditorUserId="183" OwnerUserId="183" ParentId="12415" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;A few quick thoughts:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Your example makes it sound like you have a specific application in mind, such that you could program the cut points as a set of if-then rules. E.g., if max = 20, then cutpoint is 10; if max = 50 then cutpoints are 15, 30, 45; etc...&lt;/li&gt;&#10;&lt;li&gt;You could have a look at the &lt;a href=&quot;http://astrostatistics.psu.edu/datasets/R/html/base/html/cut.html&quot; rel=&quot;nofollow&quot;&gt;cut function in R&lt;/a&gt; to get additional ideas.&lt;/li&gt;&#10;&lt;li&gt;The literature on histograms, bin size, etc (e.g., see &lt;a href=&quot;http://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;) may also be relevant.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You probably want to think about what properties are most desirable:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Importance of rounding to 5s, 10s, 100s on interval borders&lt;/li&gt;&#10;&lt;li&gt;Equality of intervals versus equality of proportion of sample in each interval&lt;/li&gt;&#10;&lt;li&gt;The functional relationship between range and number of intervals that you want to use&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-07-29T06:40:20.463" Id="13611" LastActivityDate="2011-07-29T06:40:20.463" OwnerUserId="183" ParentId="13590" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="13628" AnswerCount="1" Body="&lt;p&gt;I'm working at a plant breeding company. During some spare time I recently did some exploring of our data (yield trials), mostly just out of curiosity and wanting to learn R better.&lt;/p&gt;&#10;&#10;&lt;p&gt;I started by plotting standard deviation of means created with different number of replications (in the actual data each color would represent a particular seed lot grown in a particular environment). Like this (examples are not actual yield data):&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/cF3lz.png&quot; alt=&quot;Figure 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Figure 1. Standard deviation of means created from different number of normally distributed numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;That looks perfectly understandable to me. I then got the idea to plot the standard deviation relative to the 1-rep standard deviation: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ASjm6.png&quot; alt=&quot;Figure 2&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Figure 2. Standard deviation, relative to standard deviation in 1 rep, of means created from different number of normally distributed numbers.&lt;a href=&quot;http://i.stack.imgur.com/ASjm6.png&quot; rel=&quot;nofollow&quot;&gt;2&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously this relative standard deviation has a strong relationship with the number of replications. I have been thinking about it a while, but I'm having trouble to formulate the math behind this. So my question is this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Can someone please explain to me, in mathematical terms, why we see the relationship visualized in the second figure?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(I have a feeling that the answer will leave me feeling utterly stupid for asking. I also realize that this may sound an awful lot like I'm trying to get you to do a homework assignment for me, but I'm really just curious and have access to loads of data but not a lot of training in statistics and mathematics. We have a statistician that I could have tried to ask, but he's on vacation.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-29T14:35:33.847" FavoriteCount="1" Id="13622" LastActivityDate="2011-07-29T19:00:42.340" LastEditDate="2011-07-29T18:54:11.733" LastEditorUserId="2958" OwnerUserId="5592" PostTypeId="1" Score="4" Tags="&lt;variance&gt;&lt;standard-deviation&gt;" Title="Relationship between number of replications and the standard deviation of entry means" ViewCount="119" />
  
  <row Body="&lt;p&gt;The dichotomy between the cases $d &amp;lt; 3$ and $d \geq 3$ for the admissibility of the MLE of the mean of a $d$-dimensional multivariate normal random variable is certainly shocking.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is another very famous example in probability and statistics in which there is a dichotomy between the $d &amp;lt; 3$ and $d \geq 3$ cases. This is the recurrence of a simple random walk on the lattice $\mathbb{Z}^d$. That is, the $d$-dimensional simple random walk is recurrent in 1 or 2 dimensions, but is transient in $d \geq 3$ dimensions. The continuous-time analogue (in the form of Brownian motion) also holds.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;It turns out that the two are closely related.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www-stat.wharton.upenn.edu/~lbrown/&quot;&gt;Larry Brown&lt;/a&gt; proved that the two questions are essentially equivalent. That is, the best invariant estimator $\hat{\mu} \equiv \hat{\mu}(X) = X$ of a $d$-dimensional multivariate normal mean vector is admissible if and only if the $d$-dimensional Brownian motion is recurrent.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, his results go &lt;em&gt;much&lt;/em&gt; further. For &lt;em&gt;any&lt;/em&gt; sensible (i.e., generalized Bayes) estimator $\tilde{\mu} \equiv \tilde{\mu}(X)$ with bounded (generalized) $L_2$ risk, there is an explicit(!) corresponding $d$-dimensional diffusion such that the estimator $\tilde{\mu}$ is admissible if and only if its corresponding diffusion is recurrent.&lt;/p&gt;&#10;&#10;&lt;p&gt;The local mean of this diffusion is essentially the discrepancy between the two estimators, i.e., $\tilde{\mu} - \hat{\mu}$ and the covariance of the diffusion is $2 I$. From this, it is easy to see that for the case of the MLE $\tilde{\mu} = \hat{\mu} = X$, we recover (rescaled) Brownian motion.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, in some sense, we can view the question of admissibility through the lens of stochastic processes and use well-studied properties of diffusions to arrive at the desired conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;L. Brown (1971). &lt;a href=&quot;http://projecteuclid.org/euclid.aoms/1177693318&quot;&gt;Admissible estimators, recurrent diffusions, and insoluble boundary value problems&lt;/a&gt;. &lt;em&gt;Ann. Math. Stat.&lt;/em&gt;, vol. 42, no. 3, pp. 855&amp;ndash;903. &lt;/li&gt;&#10;&lt;li&gt;R. N. Bhattacharya (1978). &lt;a href=&quot;http://projecteuclid.org/euclid.aop/1176994584&quot;&gt;Criteria for recurrence and existence of invariant measures for multidimensional diffusions&lt;/a&gt;. &lt;em&gt;Ann. Prob.&lt;/em&gt;, vol. 6, no. 4, 541&amp;ndash;553.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-07-30T14:41:15.597" Id="13647" LastActivityDate="2011-08-01T14:06:17.193" LastEditDate="2011-08-01T14:06:17.193" LastEditorUserId="2970" OwnerUserId="2970" ParentId="13494" PostTypeId="2" Score="24" />
  
  
  <row Body="&lt;p&gt;It would still have a trend if you have not properly detrended it.  Trends are a complicated thing, and you may want to re-visit the model you are using to describe the trend.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, if you're using a linear model, it is almost certainly wrong.  You can clearly see a unit root, where the shock in September '06 propagates through time.  The series does not revert to the mean.  I would say this series is begging for a first-order differencing.  That might give you a stationary series you can use for more traditional regression analysis.  If that simple fix doesn't work, try an ARIMA model.  If you don't know what that is, ask!&lt;/p&gt;&#10;&#10;&lt;p&gt;If I may ask, what is the goal of this analysis?  Could you provide some example data that we might use to help you find a proper model?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-07-30T21:34:42.623" Id="13657" LastActivityDate="2011-07-30T21:34:42.623" OwnerUserId="2817" ParentId="13656" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You can find a lot of this in &lt;a href=&quot;http://www.math.uah.edu/stat/&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Virtual Laboratories in Probability and Statistics&lt;/em&gt;&lt;/a&gt; from the University of Alabama in Huntsville.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-31T00:46:40.040" Id="13661" LastActivityDate="2011-07-31T00:46:40.040" OwnerUserId="2958" ParentId="13654" PostTypeId="2" Score="4" />
&#10;g_1(y)=y^2&amp;amp; 0\leq y\leq 1  \\  
  
  <row Body="&lt;p&gt;Trending data is nonstationary by definition, so &quot;nonstationary&quot; does not add anything to your description. What linear regression in Excel are you talking about? The one that would use the index of the period (e.g. 1,2,3...) as the regressor would be a natural start. If you have an idea on what goes on in the data, detrending is not inherently statistical, just a trick to get you something like a &quot;sample&quot; - a realization of a stochastic process that is (in some sense) stationary, so at least its mean does not change.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-07-31T07:52:20.357" Id="13667" LastActivityDate="2011-07-31T19:44:38.870" LastEditDate="2011-07-31T19:44:38.870" LastEditorUserId="5494" OwnerUserId="5494" ParentId="13666" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This may be overkill, but &lt;a href=&quot;http://www.cs.toronto.edu/~hinton/&quot; rel=&quot;nofollow&quot;&gt;Geoffrey Hinton&lt;/a&gt; has been doing something like this using &lt;a href=&quot;http://www.cs.toronto.edu/~ilya/pubs/2011/LANG-RNN.pdf&quot; rel=&quot;nofollow&quot;&gt;artificial neural networks and backpropagation in time&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;He trains his model using blocks of text from wikipedia or NYTimes, the network learns the chained statistical relationships you mention in your question.  Then the network can iteratively generate characters from a short prompt.  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The sample below was obtained by running the MRNN less than 10 times&#10;  and selecting the most intriguing sample. The beginning&#10;  of the paragraph and the parentheses near the end are par-&#10;  ticularly interesting. The MRNN was initialized with the&#10;  phrase “&lt;strong&gt;The meaning of life is&lt;/strong&gt;”:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;The meaning of life is&lt;/strong&gt; the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove&#10;  her bigger. In the show’s agreement unanimously resurfaced. The&#10;  wild pasteured with consistent street forests were incorporated&#10;  by the 15th century BE. In 1996 the primary rapford undergoes&#10;  an effort that the reserve conditioning, written into Jewish cities,&#10;  sleepers to incorporate the .St Eurasia that activates the popula-&#10;  tion. Mar??a Nationale, Kelli, Zedlat-Dukastoe, Florendon, Ptu’s&#10;  thought is. To adapt in most parts of North America, the dynamic&#10;  fairy Dan please believes, the free speech are much related to the&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The take-home from this paper is that the model was able to learn the fact that characters are organized into words, to learn many of those words, and to learn some of the rules of how words are strung together grammatically.  That may be much more structure than the ABC character sequence you had in mind, so I think user4581 's answer is more appropriate.  But this one is a lot of fun.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-31T16:40:03.673" Id="13678" LastActivityDate="2011-07-31T16:40:03.673" OwnerUserId="5539" ParentId="13650" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;These are not &quot;products&quot;, they're interaction terms.  You will inevitably get a higher R^2 when you add more terms (which is what the * does).  Take a look at the adjusted R^2 which is printed one line below the R^2 value in the &lt;code&gt;summary&lt;/code&gt; ... You should almost certainly read more about regression; there is a terse introduction to regression model formulae in section 11 of the &lt;a href=&quot;http://cran.r-project.org/doc/manuals/R-intro.pdf&quot; rel=&quot;nofollow&quot;&gt;Introduction to R&lt;/a&gt;, or you may prefer a book like Dalgaard's &lt;em&gt;Introductory Statistics with R&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;And, when in doubt about what is going on, look at the data if possible:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X &amp;lt;- read.table(&quot;binreg.txt&quot;,header=TRUE)&#10;X &amp;lt;- na.omit(X)&#10;library(ggplot2)&#10;## to label facets with variable names&#10;label_parseall &amp;lt;- function(variable, value) {&#10;    plyr::llply(value, function(x) parse(text = paste(variable, &#10;        x, sep = &quot;==&quot;)))&#10;}&#10;&#10;ggplot(X,aes(x=Spiciness,y=AA.Int,colour=factor(undergrad)))+&#10;  geom_point()+facet_grid(multi~HW,labeller=label_parseall)+&#10;  stat_smooth(method=&quot;lm&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/h06i6.png&quot; alt=&quot;data plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This indicates to me that there &lt;em&gt;might&lt;/em&gt; be some interaction between &lt;code&gt;multi&lt;/code&gt;, &lt;code&gt;HW&lt;/code&gt;, and &lt;code&gt;Spiciness&lt;/code&gt;, but that you really don't have enough information about &lt;code&gt;undergrad&lt;/code&gt; to say anything about it or about its interactions with the other variables ...&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-07-29T01:21:54.213" Id="13697" LastActivityDate="2011-07-29T13:14:31.743" OwnerDisplayName="Ben Bolker" OwnerUserId="2126" ParentId="13696" PostTypeId="2" Score="10" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I just found &quot;Robust Fitting of Linear Models&quot; rlm() function in the MASS library.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know what is the difference between this function and the standard lm() (linear regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone give me a short explanation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-07-31T18:18:19.200" Id="13702" LastActivityDate="2011-08-01T08:28:54.080" OwnerDisplayName="Dail" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;regression&gt;" Title="What is the difference between lm() and rlm()?" ViewCount="2348" />
  
  
  <row Body="&lt;p&gt;@johannes gave an excellent answer. If you are a SAS user, then LASSO is available through PROC GLMSELECT and partial least squares through PROC PLS.&lt;/p&gt;&#10;&#10;&lt;p&gt;David Cassell and I made a presentation about LASSO (and Least Angle Regression) at a couple of SAS user groups. It's available &lt;a href=&quot;http://www.nesug.org/proceedings/nesug07/sa/sa07.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-01T12:31:43.920" Id="13710" LastActivityDate="2011-08-01T12:31:43.920" OwnerUserId="686" ParentId="13686" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Assume we have an $I \times J$ table of relative frequencies $f_{ij} \; (1 \leq i \leq I, 1 \leq j \leq J)$, where (without loss of generality) $I &amp;lt; J$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$
  <row Body="&lt;p&gt;There is no need to do recentering.  The linear part of the GLM can simply include an intercept term (e.g. &lt;code&gt;b0 + b1*x1 + b2*x2 + b3*x3&lt;/code&gt;), and this will be estimated by standard statistical software.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-01T21:42:30.790" Id="13734" LastActivityDate="2011-08-01T21:42:30.790" OwnerUserId="5256" ParentId="13726" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to get myself more savvy with literature on False Discovery Rates (FDR).  At its original incarnation, FDR assumes that the tests being compared are independent.  Several authors have published cases where the tests can have &quot;clumpy dependencies&quot; (tests are related to each other in &quot;families&quot;, but not across families of tests), slight dependencies, etc., and still hold true.  Most notably, Storey &amp;amp; Tibshirani (&quot;Estimating the Positive False Discovery Rate Under Dependence...&quot;) give several ways the independence rule can be &quot;bent&quot; and how to do so.&#10;&lt;br&gt;&lt;br&gt;&#10;However, I won't know a priori dependencies between tests, and dependencies may be different on the next set of test that I perform FDR upon.  So, it seems to me that the easiest thing is to perform a principal components analysis (PCA) on all of the test data before calculating p-values and FDR.&#10;&lt;br&gt;&lt;br&gt;&#10;What is the downside to performing PCA on the data set before performing FDR?  The only down side that I can foresee is that I won't know &lt;strong&gt;&lt;em&gt;which&lt;/em&gt;&lt;/strong&gt; tests are found to be significant, since information is lost in performing that co-ordinate shift.  But if my main concern is simply to determine whether 2 different levels across samples, for instance, are significantly different, I shouldn't need this information.&#10;&lt;br&gt;&lt;br&gt;&#10;I haven't found any information combining the two methods, but it seems quite powerful to me.  For one thing, PCA can shrink the data set immensely when looking at far more test parameters than observations (which is always the case in DNA microarrays, an example used often in the literature).  But most importantly, I won't have to worry any more whether tests are overly dependent to apply FDR, since PCA assures orthogonality, obviously.&#10;&lt;br&gt;&lt;br&gt;&#10;Being such a powerful combination (in my view), and seeing no literature on this subject, I &quot;fear&quot; I am missing something in combining these two methodologies.&#10;&lt;br&gt;&lt;br&gt;&#10;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;PS  Dependency obviously matters at some point:  if all the tests were fully dependent, they provide the same information as 1 test, thus proper analysis should follow the standard 1-test methods.  So this doesn't seem, to me, an extreme case or moot question.&#10;&lt;br&gt;&lt;br&gt;&#10;&lt;hr&gt;&#10;&lt;br&gt;&#10;Thanks to comments so far!  Apologies for the imprecise nature of the question.  Following is an example, although I plan to use this method for many different types of problems.&#10;&lt;br&gt;&lt;br&gt;&#10;Hypothesis to test:  using a new process, is there a marked improvement in process quality.  My case is a chemical process, this is NOT gene expression.&#10;&lt;br&gt;&lt;br&gt;&#10;Sample size:  144 different observations, split evenly into the two groups of &quot;new process&quot; and &quot;POR&quot; (process on record).&#10;&lt;br&gt;&lt;br&gt;&#10;test size:  there are potentially around 630 different tests that will be used to compare whether the new process is significantly different from POR.&#10;&lt;br&gt;&#10;Many of these tests are correlated with each other.  Obviously due to the sample size being larger than the test size, even if the tests were not &quot;correlated by nature&quot;, they become correlated due to the sparse data set.&#10;&lt;br&gt;&lt;br&gt;&#10;Proposal:&#10;&lt;br&gt;&#10;&lt;strong&gt;BEFORE&lt;/strong&gt; performing any t-tests or similar tests to generate a p-value, perform PCA upon the [144 x 630] matrix of test measurements.&#10;&lt;br&gt;&lt;br&gt;&#10;Using the newly formed [144 x 144] matrix of PC's of test measurements, split them into &quot;new process&quot; and &quot;POR&quot;.&#10;&lt;br&gt;&lt;br&gt;&#10;Generate 144 test statistic p-values, comparing new process to POR&#10;&lt;br&gt;&lt;br&gt;&#10;Perform FDR analysis on set of 144 statistics.  I know this is far from the ~ 3000+ that are often used for FDR, but it still seems significant enough to use FDR instead of FWER, Bonferroni, or something similar.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-02T00:24:26.460" FavoriteCount="1" Id="13739" LastActivityDate="2014-05-11T19:59:48.193" LastEditDate="2011-08-02T18:18:46.590" LastEditorUserId="919" OwnerUserId="5555" PostTypeId="1" Score="3" Tags="&lt;pca&gt;&lt;multiple-comparisons&gt;&lt;p-value&gt;" Title="Can we combine false discovery rate and principal components analysis?" ViewCount="315" />
  
  
  
  
  
&#10;0.253&amp;amp;0.249&amp;amp;
&#10;\hline
&#10;\hline
  
  <row Body="&lt;p&gt;In theory the confidence intervals are derived from the estimated distribution of the forecast. This means that the model gives estimate $\hat{y}$ and its estimated cdf $\hat{F}$. The 95%-confidence interval is then calculated as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\hat{F}^{-1}(0.025),\hat{F}^{-1}(0.975))$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\hat{F}$ would be available you could assign (with certain assumptions) probability for the target $y$ by simply reporting $1-\hat{F}(y)$ which will be the probability $P(\hat{y}&amp;gt;y)$, i.e. the probability that the forecast will be larger than the target. &lt;/p&gt;&#10;&#10;&lt;p&gt;However you do not have $\hat{F}$, you just have 2 values: $\hat{F}^{-1}(0.025)$ and $\hat{F}^{-1}(0.975)$. In general this is too little information to recover $\hat{F}$. On the other hand it helps to know that for a lot of models $\hat{F}$ is normal distribution. Usually:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{y}\sim N(\mu_{\hat y},\sigma^2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mu_{\hat y}$ is the reported mean or median forecast. Since in this case the 95% confidence interval is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\mu_{\hat y}-1.96\sigma,\mu_{\hat y}+1.96\sigma)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is not hard then to recover $\sigma$ and for the target $y$ we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(\hat{y}&amp;gt;y)=1-\Phi\left(\frac{y-\mu_{\hat y}}{\sigma}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\Phi$ is standard normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;This should be used with caution, since although a lot of models do use normal distribution for $\hat{F}$, this is not always the case. Furthermore it might be that for example $\log y$ was modelled, but confidence interval was reported for $y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So to sum up the answers to your questions would be:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You can say that, but the uniform distribution assumption is almost guaranteed to be false.&lt;/li&gt;&#10;&lt;li&gt;Yes you can suppose that with some confidence, but you need to check the model details to be sure.&lt;/li&gt;&#10;&lt;li&gt;Yes it is model dependent. As I illustrated you can recover something, since a lot of models use the same approach in calculating confidence intervals, but there is a lot of models too where some different approach is used.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-08-03T07:59:26.057" Id="13791" LastActivityDate="2011-08-03T12:25:01.147" LastEditDate="2011-08-03T12:25:01.147" LastEditorUserId="2116" OwnerUserId="2116" ParentId="13779" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="13800" AnswerCount="8" Body="&lt;p&gt;I am having difficulties understanding the underlying logic in setting the &lt;a href=&quot;http://en.wikipedia.org/wiki/Null_hypothesis&quot;&gt;null hypothesis&lt;/a&gt;. In this &lt;a href=&quot;http://stats.stackexchange.com/questions/12461/how-to-specify-the-null-hypothesis-in-hypothesis-testing&quot;&gt;answer&lt;/a&gt; the obviously generally accepted proposition is stated that the null hypothesis is the hypothesis that there will be no effect, everything stays the same, i.e. nothing new under the sun, so to speak.&lt;/p&gt;&#10;&#10;&lt;p&gt;The alternative hypothesis is then what you try to prove, that e.g. a new drug delivers on its promises.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now coming form science theory and general logic we know that we can only falsify propositions, we cannot prove something (no number of white swans can prove that all swans are white but one black swan can disprove it). This is why we try to disprove the null hypothesis, which is not equivalent to proving the alternative hypothesis - and this is where my skepticism starts - I will give an easy example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I want to find out what kind of animal is behind a curtain. Unfortunately I cannot directly observe the animal but I have a test which gives me the number of legs of this animal. Now I have the following logical reasoning:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If the animal is a dog then it will have 4 legs.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If I conduct the test and find out that it has 4 legs this is &lt;em&gt;no&lt;/em&gt; proof that it is a dog (it can be a horse, a rhino or any other 4-legged animal). But if I find out that it has &lt;em&gt;not&lt;/em&gt; 4 legs this is a definite proof that it can &lt;em&gt;not&lt;/em&gt; be a dog (assuming a healthy animal).&lt;/p&gt;&#10;&#10;&lt;p&gt;Translated into drug effectiveness I want to find out if the drug behind the curtain is effective. The only thing I will get is a number that gives me the effect. If the effect is positive, nothing is proved (4 legs). If there is no effect, I disprove the effectiveness of the drug.&lt;/p&gt;&#10;&#10;&lt;p&gt;Saying all this I think - contrary to common wisdom - the only valid null hypothesis must be&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The drug is effective (i.e.: if the drug is effective you will see an effect).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;because this is the only thing that I can disprove - up to the next round where I try to be more specific and so on. So it is the null hypothesis that states the effect and the alternative hypothesis is the default (&lt;em&gt;no effect&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why is it that statistical tests seem to have it backwards?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt;: You cannot even negate the above hypothesis to get a valid equivalent hypothesis, so you &lt;em&gt;cannot&lt;/em&gt; say &quot;The drug is &lt;em&gt;not&lt;/em&gt; effective&quot; as a null hypothesis because the only logically equivalent form would be &quot;if you see &lt;em&gt;no&lt;/em&gt; effect the drug will &lt;em&gt;not&lt;/em&gt; be effective&quot; which brings you nowhere because now the conclusion is what you want to find out!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P.P.S.&lt;/strong&gt;: Just for clarification after reading the answers so far: If you accept scientific theory, that you can only falsify statements but not prove them, the only thing that is logically consistent is choosing the null hypothesis as the new theory - which can then be falsified. Because if you falsify the status quo you are left empty handed (the status quo is disproved but the new theory far from being proved!). And if you fail to falsify it you are in no better position either.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-08-03T10:22:32.813" FavoriteCount="9" Id="13797" LastActivityDate="2013-12-12T16:55:50.067" LastEditDate="2011-11-21T07:36:46.567" LastEditorUserId="6961" OwnerUserId="230" PostTypeId="1" Score="15" Tags="&lt;hypothesis-testing&gt;&lt;philosophical&gt;" Title="Which one is the null hypothesis? Conflict between science theory, logic and statistics?" ViewCount="2761" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Granger_causality&quot; rel=&quot;nofollow&quot;&gt;Granger causality&lt;/a&gt; test was invented to test statistically that variable $x_t$ has a significantly sound information that helps to predict $y_t$. If however $y_t$ is potentially linked with some other variables than $x_t$ (both are a part of a vector, which could be modeled by vector autoregression) the testing scheme becomes a bit more complicated (involves some matrix manipulations). Note also that since testing for Granger causality involves $F$-test in the final stage it is sensitive to deviations from normality assumption (you may then consider some extensions from the wiki link above).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;To make a predictive model you have to choose the horizon (denote it by $h$) up to which you would want to predict. Your complete predictive model is then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_t = \alpha_0 +\alpha_1 y_{t-1} + \dots + \alpha_p y_{t-p} + \beta_1 x_{t-1} + \dots + \beta_p x_{t-p} + \varepsilon_t, $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;for sequential predictions (note that in sequential case you have to make a predictive model for $x_t$), and in direct $h$ step ahead case:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_t = \alpha_0 +\alpha_1 y_{t-h} + \dots + \alpha_p y_{t-h-p+1} + \beta_1 x_{t-h} + \dots + \beta_p x_{t-h-p+1} + \varepsilon_t, $$&lt;/p&gt;&#10;&#10;&lt;p&gt;then the $h$ step prediction after estimation and testing for Granger-(non)causality ($H_0: \beta_1 = ... = \beta_p = 0$) in direct case:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_{T+h}^{(h)} = \hat\alpha_0 +\hat\alpha_1 y_{T} + \dots + \hat\alpha_p y_{T-p+1} + \hat\beta_1 x_{T} + \dots + \hat\beta_p x_{T-p+1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and in sequential:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_{T+1}^{(1)} = \hat\alpha_0 +\hat\alpha_1 y_{T} + \dots + \hat\alpha_p y_{T-p+1} + \hat\beta_1 x_{T} + \dots + \hat\beta_p x_{T-p+1}, $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_{T+2}^{(2)} = \hat\alpha_0 +\hat\alpha_1 y^{(1)}_{T+1} + \dots + \hat\alpha_p y_{T-p} + \hat\beta_1 x^{(1)}_{T+1} + \dots + \hat\beta_p x_{T-p}, \dots $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus &lt;em&gt;estimation/testing&lt;/em&gt; step is separated from the &lt;em&gt;prediction&lt;/em&gt; step which for linear models is straightforward. All this scheme is easily implementable in $R$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-03T10:50:08.733" Id="13798" LastActivityDate="2011-08-03T10:50:08.733" OwnerUserId="2645" ParentId="13756" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In statistics there are tests of equivalence as well as the more common test the Null and decide if sufficient evidence against it. The equivalence test turn this on its head and posits that effects are different as the Null and we determine if their is sufficient evidence against this Null.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not clear on your drug example. If the response is a value/indicator of the effect, then an effect of 0 would indicate not effective. One would set that as the Null and evaluate the evidence against this. If the effect is sufficiently different from zero we would conclude that the no-effectiveness hypothesis is inconsistent with the data. A two-tailed test would count sufficiently negative values of effect as evidence against the Null. A one tailed test, the effect is positive &lt;em&gt;and&lt;/em&gt; sufficiently different from zero, might be a more interesting test.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to test if the effect is 0, then we'd need to flip this around and use an equivalence test where the H0 is the effect is not equal to zero, and the alternative is that H1 = the effect = 0. That would evaluate the evidence against the idea that effect was different from 0.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-03T10:59:58.070" Id="13800" LastActivityDate="2011-08-03T11:35:36.277" LastEditDate="2011-08-03T11:35:36.277" LastEditorUserId="1390" OwnerUserId="1390" ParentId="13797" PostTypeId="2" Score="12" />
&#10;$$ &#10;This gives you the proportional error of&#10;the estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, note that, since all of the involved functions are nonnegative,&#10;by using the bounding properties of $\Su(z)$ and $\Sl(z)$, we get&#10;$$
  
&#10;\begin{array}{cc}
&#10; 0 &amp;amp; 1
&#10;\end{array}
  <row AcceptedAnswerId="58785" AnswerCount="2" Body="&lt;p&gt;A simple example:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;plot(hclust(dist(c(1:3)),method = &quot;ward&quot;))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know which calculations (in R) can reproduce the distance of 3 from {1,2} to be &#10;1.67&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XXMs2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-03T18:34:09.330" FavoriteCount="1" Id="13817" LastActivityDate="2013-05-12T15:55:33.487" OwnerUserId="253" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;clustering&gt;&lt;ward&gt;" Title="How does &quot;ward&quot; clustering (in R's hclust function) work?" ViewCount="2014" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm trying to make sense of these two statements about UMP (uniformly most powerful) tests: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If $g(t\mid\theta)~$ is a UMP then $~g(t\mid\theta_1)&amp;gt;k g(t\mid\theta_0)~\forall~t\in C$   and   $g(t\mid\theta_1)&amp;lt;k g(t\mid\theta_0)~\forall~t\in C^c$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For $\mathbf X~i.i.d.~f(x\mid\theta): \theta\in\Omega\subset\mathcal R$ if $f(x\mid\theta)$ has an MLR in $T(x)$ and any $k$, a test that rejects $H_0 \iff T&amp;gt;k~$ is a UMP test of size $\alpha$ with $\alpha=P_{\theta_0}(T&amp;gt;k)$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Why are these statements not tautological? How are they not the definition of &lt;em&gt;any&lt;/em&gt; one-tailed statistical test? What would be an example of a one-tailed statistical test that isn't a UMP? &lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, what is the relationship between LRT and UMP tests? I'm reviewing old exams where sometimes a question asks for an LRT and sometimes a UMP... Aren't all simple LRT tests UMP?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-03T21:01:41.613" FavoriteCount="1" Id="13820" LastActivityDate="2011-08-05T15:30:21.233" LastEditDate="2011-08-04T03:56:31.490" LastEditorUserId="4829" OwnerUserId="4829" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;&lt;hypothesis-testing&gt;&lt;likelihood-ratio&gt;" Title="Practical definition of a UMP test?" ViewCount="533" />
&#10; 2 &amp;amp; \text{A} &amp;amp; \text{Right}  &amp;amp; 90 &amp;amp; \ldots &amp;amp; 23\\ 
  
  <row AcceptedAnswerId="13864" AnswerCount="2" Body="&lt;p&gt;In many financial models we are interested in measuring the correlation between variables, returns etc. However, research shows that during crises times we observe &quot;Correlation Breaks&quot; where previously un-correlated variables become correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best way to quantify &quot;stability of correlation&quot; by examining the historic time series of the variables on which I am interested?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would some kind of bootstrapping be a good way to start examining the sample distribution of the correlation? Is there any other method that I could apply?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-03T22:51:10.177" FavoriteCount="2" Id="13828" LastActivityDate="2011-08-04T18:32:28.340" LastEditDate="2011-08-04T17:53:33.760" LastEditorUserId="88" OwnerUserId="5672" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;correlation&gt;&lt;bootstrap&gt;" Title="How to quantify correlation stability?" ViewCount="1194" />
  
  
&#10; 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 4 &amp;amp; 2 \\
&#10; 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \\
  
&#10;\text{Morning} &amp;amp; x \, x \, x  &amp;amp;  x \, x \, x   &amp;amp;  x \, x \, x  &amp;amp;  x \, x \, x\\
&#10;\end{array}
&#10;\text{Morning} &amp;amp; x \, x \, x  &amp;amp;  x \, x \, x   &amp;amp;  x \, x \, x  &amp;amp;  x \, x \, x\\
  
  <row Body="&lt;p&gt;It sounds like your F-tests are not the ones you want, the default F-tests returned by most software compare the fitted model to a null (intercept-only) model.  It sounds like you want to do an F-test comparing Model 2 to Model 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do this, you could still find that the F-test comparing these two models is insignificant, while the t-test on one of the interactions is significant.  It is important to keep in mind that these are different tests, answering slightly different questions.  The t-test on the interaction term is a test of how strongly the data reject the hypothesis that the interaction term is zero, &lt;em&gt;holding fixed all the other coefficients in the extended model, including the other added interaction term&lt;/em&gt;.  The F-test is a test of how strongly the data reject the hypothesis that all the added terms are zero.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If the two terms you added in the extended model have one t-stat that is narrowly significant and another that is insignificant, the F-test can be insignificant.  One way to think about why this can happen is that adding multiple terms to a model creates a multiple testing problem. The chances of spuriously finding a large coefficient on one of the interaction terms increases as you add more terms.  The F-test takes this into account, while the individual t-tests for each term do not.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-04T14:07:40.707" Id="13852" LastActivityDate="2011-08-04T14:07:40.707" OwnerUserId="4881" ParentId="13850" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I can't thank the experts enough for their clarifications. One final question following my earlier posts on forming coomposite variables &lt;a href=&quot;http://stats.stackexchange.com/questions/13757/how-do-i-weigh-several-variables-at-two-levels-to-create-an-overall-composite-var&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/13805/is-there-a-difference-between-an-index-score-and-a-composite-score&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;http://stats.stackexchange.com/questions/13833/is-subjective-weighting-acceptable-to-create-composites-for-correlation-analysis&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I have measured some variables as cardinal numbers (e.g. 1, 2, 3, 4, 5 times) and others on a Likert scale, can I then just add the scores together?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How many times do you visit the local hospital per month? Answer: 5&#10;times &lt;/li&gt;&#10;&lt;li&gt;How would you rate the medical service provided to you: Answer&#10;3 on a scale of five going from poor to excellent.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Can I add 5 and 3 and say the composite score is 8?&lt;/p&gt;&#10;&#10;&lt;p&gt;What if the likert scale goes from negative to positive (e.g. strongly disagree = -2, disagree = -1, neither disagree or agree = 0, agree = 1 and strong agree = 2)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Would the same rule as above apply if the score is say 0 or -2?&lt;/p&gt;&#10;&#10;&lt;p&gt;My objective is to keep the aggregation process simple and not get 'confused' in complex statistical formulas.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-04T14:15:17.237" FavoriteCount="2" Id="13853" LastActivityDate="2011-08-05T12:44:40.063" LastEditDate="2011-08-05T11:08:31.473" LastEditorUserId="183" OwnerUserId="5647" PostTypeId="1" Score="5" Tags="&lt;count-data&gt;&lt;likert&gt;&lt;composite&gt;" Title="What is the best way of weighing cardinal scores and Likert scale scores to create a composite score?" ViewCount="1559" />
  <row Body="&lt;p&gt;One option would be simulation or permutation testing.  If you know the distribution that your data comes from you could simulate from that distribution, but with all the observations independent.  If you don't know the distribution then you can permute each of your variables independly of each other and that will give you the same general marginal distribution of each variable, but with any correlation removed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do either of the above (keeping the sample size and matrix dimensions the same) a whole bunch of times (10,000 or so) and look at the maximum absolute correlation, or another high quantile that may be of interest.  This will give you the distribution from the null hypothesis that you can then compare the maximum of your actual observed correlations to (and the other high quantiles of interest).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-04T16:13:42.187" Id="13860" LastActivityDate="2011-08-04T16:13:42.187" OwnerUserId="4505" ParentId="13810" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I concur with @iterator. If a large proportion had surgery on both eyes, I'd do some sort of matched pairs. If only a small proportion had surgery on both eyes, I'd probably just not use either eye for those people, but certainly not both.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-04T18:25:57.130" Id="13865" LastActivityDate="2011-08-04T18:25:57.130" OwnerUserId="686" ParentId="13822" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Summarising data in base R is just a headache. This is one of the areas where SAS works quite well. For R, I recommend the &lt;code&gt;plyr&lt;/code&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;In SAS:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/* tabulate by a and b, with summary stats for x and y in each cell */&#10;proc summary data=dat nway;&#10;  class a b;&#10;  var x y;&#10;  output out=smry mean(x)=xmean mean(y)=ymean var(y)=yvar;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;with &lt;code&gt;plyr&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;smry &amp;lt;- ddply(dat, .(a, b), summarise, xmean=mean(x), ymean=mean(y), yvar=var(y))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-08-05T00:41:51.207" Id="13878" LastActivityDate="2011-08-05T00:49:56.847" LastEditDate="2011-08-05T00:49:56.847" LastEditorUserId="1569" OwnerUserId="1569" ParentId="13855" PostTypeId="2" Score="6" />
  
  
  
  <row Body="&lt;p&gt;You may want to think in terms of practical significance rather than statistical significance (or both).  With enough data you can find things signifcant statistically that will have no real impact on your usage.  I remember analyzing a model one time where the 5-way interactions were statistically significant, but when the predictions from the model including everything up to the 5-way interactions were compared to the predictions from a model including only 2-way interactions and main effects, the biggest difference was less than 1 person (the response was number of people and all interesting values were away from 0).  So the added complexity was not worth it.  So look at the differences in your predictions to see if the differences are enough to justify the extra cost, if not then why bother even looking for the statistical significance?  If the differences are big enough to justify the cost if they are real, then I second the other sugestions of using cross validation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-05T14:34:53.080" Id="13908" LastActivityDate="2011-08-05T14:34:53.080" OwnerUserId="4505" ParentId="13869" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;(This is really a comment, but because it requires an illustration it has to be posted as a reply.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Ed Tufte redesigned the boxplot&lt;/strong&gt; in his &lt;a href=&quot;http://www.edwardtufte.com/tufte/books_vdqi&quot;&gt;Visual Display of Quantitative Information&lt;/a&gt; (p. 125, First Edition 1983) precisely to enable &quot;informal, exploratory data analysis, where the research worker's time should be devoted to matters other than drawing lines.&quot;  I have (in a perfectly natural manner) extended his redesign to accommodate drawing outliers in this example showing 70 parallel boxplots:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aKxzH.png&quot; alt=&quot;Tufte boxplots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I can think of several ways to improve this further, but it's characteristic of what one might produce in the heat of exploring a complex dataset: we are content to make visualizations that let us &lt;em&gt;see&lt;/em&gt; the data; good presentation can come later.&lt;/p&gt;&#10;&#10;&lt;p&gt;Compare this to a conventional rendition of the same data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jFXqs.png&quot; alt=&quot;Conventional boxplots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Tufte presents several other redesigns based on his principle of &quot;maximizing the data ink ratio.&quot;  Their value lies in illustrating how this principle can help us design effective exploratory graphics.  As you can see, the mechanics of plotting them amounts to finding any graphics platform in which you can draw point markers and lines.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-08-05T15:49:25.073" Id="13915" LastActivityDate="2011-08-05T15:49:25.073" OwnerUserId="919" ParentId="13875" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;The zeros of the infinite product will be the union of the zeros of the terms.  Computing out to the 20th term shows the general pattern:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AMv6o.png&quot; alt=&quot;plot of complex zeros&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This plot of the zeros in the complex plane distinguishes the contributions of the individual terms in the product by means of different symbols: at each step, the apparent curves are extended further and a new curve is started even further left.&lt;/p&gt;&#10;&#10;&lt;p&gt;The complexity of this picture demonstrates &lt;strong&gt;there exists no closed-form solution&lt;/strong&gt; in terms of well-known functions of higher analysis (such as gammas, thetas, hypergeometric functions, etc, as well as the elementary functions, as surveyed in a classic text like &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521091896&quot;&gt;Whittaker &amp;amp; Watson&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, &lt;strong&gt;the problem might be more fruitfully posed a little differently&lt;/strong&gt;: what do you need to know about the distributions of the order statistics?  Estimates of their characteristic functions?  Low order moments?  Approximations to quantiles?  Something else?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-05T17:35:06.540" Id="13919" LastActivityDate="2011-08-05T17:35:06.540" OwnerUserId="919" ParentId="13887" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;It seems this set contains only rotated &quot;1&quot;s (plus some noise and rescales), and the decision is whether the digit is rotated right or left from the upright position.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-06T08:16:02.057" Id="13933" LastActivityDate="2011-08-06T08:16:02.057" OwnerUserId="88" ParentId="13931" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="13949" AnswerCount="1" Body="&lt;p&gt;In &lt;a href=&quot;http://stats.stackexchange.com/questions/13902/how-do-i-compare-correlation-coefficients-of-the-same-variables-across-different&quot;&gt;this question&lt;/a&gt; they ask how to compare Pearson r for two independent groups (such as males vs females). Reply and comments suggested two ways: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use Fisher's well-known formula using &quot;z-tranformation&quot; of r; &lt;/li&gt;&#10;&lt;li&gt;Use comparison of slopes (regression coefficients). &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The latter could be easily performed just via a saturated linear model:&#10;$Y = a + bX + cG + dXG$, where $X$ and $Y$ are the correlated variables and $G$ is a dummy (0 vs 1) variable indicating the two groups. The magnitude of $d$ (the interaction term coefficient) is exactly the difference in coefficient $b$ after model $Y = a + bX$ conducted in two groups individually, and its ($d$'s) significance is thus the test of difference in slope between the groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, slope or regression coef. isn't yet a correlation coef. &lt;em&gt;But&lt;/em&gt; if we standardize $X$ and $Y$ - &lt;em&gt;separately&lt;/em&gt; in two groups - then $d$ will be equal to the difference &lt;em&gt;r in group 1 minus r in group 0&lt;/em&gt; and therefore its significance will be testing the difference between the two correlations: we're testing slopes but it appeares [as if - ?] we're testing correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that I've written correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;If yes, there's left the question which is a better test of correlations - this one described or Fisher's one? For they will yield not identical results. What do you think?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Later Edit:&lt;/strong&gt; Thanking &lt;strong&gt;@Wolfgang&lt;/strong&gt; for his reply I nevertheless feel I miss understanding &lt;em&gt;why&lt;/em&gt; Fisher's test is more correct a test for r than comparison-of-slope-under-standardization approach described above. So, more answers are welcome. Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-06T13:12:34.783" FavoriteCount="6" Id="13936" LastActivityDate="2011-08-08T07:42:22.187" LastEditDate="2011-08-08T07:42:22.187" LastEditorUserId="3277" OwnerUserId="3277" PostTypeId="1" Score="9" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;hypothesis-testing&gt;" Title="Can we compare correlations between groups by comparing regression slopes?" ViewCount="2799" />
  
  <row Body="&lt;p&gt;Max, good question.  It sounds like a question of model selection (another tag to add?); you seem to be ultimately asking, &quot;When is Model A (e.g., treating countries separately) better than Model B?&quot;  And you seem to address the issue of misspecification (&quot;if these countries are 'very different'&quot; then Model B would be bad).&lt;/p&gt;&#10;&#10;&lt;p&gt;I like your example; maybe I can write out some equations for others to help discuss.  My understanding is that $Y$ is lifespan (life duration), $X$ is a vector of &quot;features of people,&quot; and we have data from two countries.  Without assuming any further functional form other than independence across people, we could model country 1 with $$Y_i=h_1(X_i,u_i)$$ and country 2 with $$Y_i=h_2(X_i,v_i),$$ where $u_i$ and $v_i$ are error terms.  Alternatively, we can pool the data and write $$Y_i=h_3(X_i,\nu_i)+\gamma c_i,$$ where $\nu_i$ is an error term and $c_i=1$ if individual $i$ is from country 2 and $c_i=0$ otherwise (country 1).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I think part of your question is, in the &quot;real world,&quot; is the actual predicted lifespan difference between the two countries just a constant ($\gamma$), or does it depend on $X_i$?  And the other part is, will a forecast based on a more flexible model actually be better?  I think knowledge of the actual phenomenon being studied (e.g., lifespan) is irreplaceable, but there are also many existing (automated) ways to choose among different models (or combine them by &quot;model averaging&quot;), such as Aikake's information criterion (AIC), the Schwarz/Bayesian information criterion (BIC, or SIC or SBIC), cross-validation (CV), etc.--the &lt;a href=&quot;http://books.google.com/books?id=sjQFwB2XEFAC&amp;amp;dq=model%20selection%20claeskens%20hjort&amp;amp;hl=en&amp;amp;ei=PRA-Tv_sEMrRiAKp28DDBg&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=result&amp;amp;resnum=1&amp;amp;sqi=2&amp;amp;ved=0CCoQ6AEwAA&quot; rel=&quot;nofollow&quot;&gt;text by Claeskens and Hjort&lt;/a&gt; is a good one, but maybe other people have other suggestions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the basic idea of model selection is, we know that (in this case) the model with the dummy variable is misspecified, but if it's &quot;pretty close&quot; then we might prefer it to a more flexible model that has significantly higher variance.  The usual mean squared error (MSE) measure of an estimator's accuracy is the variance plus the square of the bias--it can be better to be a little biased if you can lower the variance a lot.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of the lifespan model, I would guess researchers use something in between, depending on the mix of countries.  (E.g., if you have Norway and Sweden, you should model it differently than Switzerland and Malawi.)  Ideally, we can understand real world phenomenon that work similarly even in different countries, so ideally we would include all countries in the same estimation.  And of course, should always be careful of the difference between forecasting/prediction and understanding causality, but that is another topic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope something in there helps, or can lay some groundwork for other answers!&#10;Dave&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-07T04:23:00.783" Id="13947" LastActivityDate="2011-08-07T04:23:00.783" OwnerUserId="5696" ParentId="13943" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;There's a big database of documents that came out in the tobacco lawsuits at the &lt;a href=&quot;http://legacy.library.ucsf.edu/&quot; rel=&quot;nofollow&quot;&gt;Legacy Tobacco Documents Library&lt;/a&gt;.  Try using the expert search interface, and search on &quot;partially redacted&quot;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-08T04:01:30.833" Id="13970" LastActivityDate="2011-08-08T04:01:30.833" OwnerUserId="5729" ParentId="9148" PostTypeId="2" Score="1" />
&#10;y2&amp;amp;0.6237&amp;amp;0.6906&amp;amp;0.7318&amp;amp;0.7918&amp;amp;1.0\\
&#10;x2&amp;amp;0.6640&amp;amp;0.7477&amp;amp;.&amp;amp;.&amp;amp;.\\
&#10;  \beta = \begin{array} {r|rrr|rr} 
&#10;\hline
&#10;$      &lt;/p&gt;&#10;&#10;&lt;p&gt;We see, that the columns 2 to 5 are free of variance of the dv1, and we can do the regression by postmultiplication with the inverse of the partial matrix $L_{1..3,2..4} $ We get the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \qquad \small 
&#10;  \beta_2 = \begin{array} {r|r|rrr|r} 
&#10;y1&amp;amp;0.9465&amp;amp;0.3227&amp;amp;.&amp;amp;.&amp;amp;.\\
&#10;     \end{array}
&#10;y2&amp;amp;0.9465&amp;amp;-0.0303&amp;amp;-0.7206&amp;amp;0.1906&amp;amp;-0.0603\\
  <row AnswerCount="1" Body="&lt;p&gt;Let's say I have 4 plants and I measure &lt;code&gt;something&lt;/code&gt; under 2 conditions. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;The entries in the table show how many times I &lt;code&gt;saw something&lt;/code&gt; under condition (C1 or C2) for plant (P1, P2, P3 or P4). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;     P1,  P2,  P3,  P4 &#10;C1    0   20    0   19 &#10;C2  100   80  180  150 &#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I would like to answer the question &quot;what is the probability that these 39 times of seeing &lt;code&gt;something&lt;/code&gt; is unevenly distributed between these 4 plants&quot;? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am much interested in observation like this, rather than, say:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;     P1,  P2,  P3,  P4 &#10;C1   25   20   15   19 &#10;C2  100   80  180  150 &#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is it right to use Fisher's exact test for this? I just extended the question from a 2*2 table to 2*4.&lt;/p&gt;&#10;" ClosedDate="2013-07-22T08:03:56.923" CommentCount="3" CreationDate="2011-08-08T08:39:00.107" Id="13977" LastActivityDate="2013-07-21T23:11:05.730" LastEditDate="2013-07-21T23:11:05.730" LastEditorUserId="7290" OwnerUserId="4751" PostTypeId="1" Score="3" Tags="&lt;fishersexact&gt;" Title="Is Fisher's exact test right for this problem?" ViewCount="460" />
  
  <row AnswerCount="1" Body="&lt;p&gt;First I have to tell my experience in development is extremely thin (a few bash scripts that's all) so bear with me if I can't keep up :)&lt;/p&gt;&#10;&#10;&lt;p&gt;A little background story to begin:&lt;/p&gt;&#10;&#10;&lt;p&gt;A few years ago during my sysadmin formation I had the chance to meet a senior security expert which showed me something I found wonderful, phase space visualization of system logs with &lt;a href=&quot;http://www.gnu.org/software/octave/&quot; rel=&quot;nofollow&quot;&gt;Octave&lt;/a&gt;. (By &quot;system logs&quot; I mean logs from routers/switches/firewalls/webservers/database servers/etc...)  &lt;/p&gt;&#10;&#10;&lt;p&gt;That person used this type of visualization to detect anomalies in behavior of computer systems by mapping them &quot;in phases [sic] space&quot; (I don't know if I can employ the term this way.)&lt;br&gt;&#10;The result had generally the aspect of a sphere (when represented in 3D) but protruding from it were &quot;significant events&quot; (anomalies in this case).&#10;The same person showed my once how he could map this differently and it had the aspect of Lorenz attractors (&lt;a href=&quot;http://en.wikipedia.org/wiki/File%3aLorenz_caos2.png&quot; rel=&quot;nofollow&quot;&gt;This page has images which are very similar to what he showed me&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;The resulting visualization may seem to have no meanginful output given that I would plot IP addresses against time against URLs for example, remember my goal is only to detect unusual behaviors, those who depart from the general ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The question:&lt;/strong&gt; The problem I have is I lost the &quot;formula&quot; to plot system logs &quot;in phases space&quot; could you help me to find it ?&lt;br&gt;&#10;I'm not directly interested in working code, the formula in pseudocode to plot those logs would already make me happy :)&lt;/p&gt;&#10;&#10;&lt;p&gt;Given my weak skills in math and development I'm not sure I explained clearly my point so here are some resources I found related to this subject:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://notebook.cowgar.com/access.log&quot; rel=&quot;nofollow&quot;&gt;here is an example webserver access log, that's the type of logs i want to plot&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://rp-www.cs.usyd.edu.au/~comp5318/survey/anomaly.pdf&quot; rel=&quot;nofollow&quot;&gt;a whitepaper&lt;/a&gt; describing anomaly detection, it seems pretty similar to what I want to do&lt;/li&gt;&#10;&lt;li&gt;the abstract of &lt;a href=&quot;http://portal.acm.org/citation.cfm?id=1981210&quot; rel=&quot;nofollow&quot;&gt;this whitepaper&lt;/a&gt; seems to fit the idea too but I don't have access to it&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;PS: an article for math-guys I just stumbled upon, take it like a little present for your kindness to, at least, read my question which I think is a bit blurry due to my lack of skill: &lt;a href=&quot;https://christopherolah.wordpress.com/2011/08/08/the-real-3d-mandelbrot-set/&quot; rel=&quot;nofollow&quot;&gt;https://christopherolah.wordpress.com/2011/08/08/the-real-3d-mandelbrot-set/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-08T13:55:21.353" Id="13990" LastActivityDate="2012-12-02T13:47:59.767" LastEditDate="2012-12-02T13:47:59.767" LastEditorUserId="5735" OwnerUserId="5735" PostTypeId="1" Score="0" Tags="&lt;data-visualization&gt;&lt;matlab&gt;&lt;outliers&gt;" Title="How to visualize system log data in &quot;phase space&quot;?" ViewCount="490" />
  
  <row Body="&lt;p&gt;paul,&lt;/p&gt;&#10;&#10;&lt;p&gt;Your series would appear to have a number of &quot;trends&quot; and possibly some assymptotic ARIMA structure. You can incorporate this series &quot;as is&quot; in a regression model. Detecting i.e. identifying any needed lag structure using this variable would probably need suitable differencing.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-08T22:21:16.100" Id="14015" LastActivityDate="2011-08-08T22:21:16.100" OwnerUserId="3382" ParentId="13979" PostTypeId="2" Score="0" />
  <row AnswerCount="3" Body="&lt;p&gt;I ran two programs &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;, and their corresponding features are &lt;code&gt;a&lt;/code&gt;,&lt;code&gt;b&lt;/code&gt;,&lt;code&gt;c&lt;/code&gt;, and &lt;code&gt;d&lt;/code&gt;. I would like to represent their features data in a better way instead of a table as shown below to explain why A is better than B or vice-versa. First, I thought about barplot representation, but feature &lt;code&gt;a&lt;/code&gt; data is very small and that makes barplot representation as an improper one. Moreover, I also thought about barplot represnetatuon of A/B (normalization), but not satisfied with the resulting barplot graph. It would be great help if you share your ideas. I would like to use R for plotting the data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;prog  a   b    c    d&#10;A    0.8 7900  70  27&#10;B    0.3 1920 393  43&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here, the lower &lt;code&gt;d&lt;/code&gt; values is better. Therefore, I would like to use the data of &lt;code&gt;a&lt;/code&gt;,&lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt; to explain why one program is better than other. In this case why A is better than B.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-09T03:36:30.293" FavoriteCount="1" Id="14019" LastActivityDate="2011-08-09T09:00:00.927" LastEditDate="2011-08-09T05:42:16.330" LastEditorUserId="25133" OwnerUserId="25133" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;" Title="Data representation in a better way?" ViewCount="152" />
&#10;P(\text{Sports}|\text{news}, \text{football}, \text{tennis}) = \frac{P(\text{news}, \text{football}, \text{tennis}|\text{Sports})P(\text{Sports})}{P(\text{news}, \text{football}, \text{tennis})} 
&#10;P(\text{news}, \text{football}, \text{tennis}|\text{Sports})=P(\text{news}|\text{Sports})P( \text{football}|\text{Sports})P(\text{tennis}|\text{Sports})
  <row Body="&lt;p&gt;Here's a letter-based histogram.  Considered sizing the first letters by number, but decided against since that's already encoded in the vertical component.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# &quot;Load&quot; data&#10;nms &amp;lt;- c(&quot;Ad&quot;,&quot;am&quot;,&quot;ar&quot;,&quot;as&quot;,&quot;bc&quot;,&quot;bd&quot;,&quot;bp&quot;,&quot;br&quot;,&quot;BR&quot;,&quot;bs&quot;,&quot;by&quot;,&quot;c&quot;,&quot;C&quot;,&quot;cc&quot;,&quot;cd&quot;,&quot;ch&quot;,&quot;ci&quot;,&quot;CJ&quot;,&quot;ck&quot;,&quot;Cl&quot;,&quot;cm&quot;,&quot;cn&quot;,&quot;cq&quot;,&quot;cs&quot;,&quot;Cs&quot;,&quot;cv&quot;,&quot;d&quot;,&quot;D&quot;,&quot;dc&quot;,&quot;dd&quot;,&quot;de&quot;,&quot;df&quot;,&quot;dg&quot;,&quot;dn&quot;,&quot;do&quot;,&quot;ds&quot;,&quot;dt&quot;,&quot;e&quot;,&quot;E&quot;,&quot;el&quot;,&quot;ES&quot;,&quot;F&quot;,&quot;FF&quot;,&quot;fn&quot;,&quot;gc&quot;,&quot;gl&quot;,&quot;go&quot;,&quot;H&quot;,&quot;Hi&quot;,&quot;hm&quot;,&quot;I&quot;,&quot;ic&quot;,&quot;id&quot;,&quot;ID&quot;,&quot;if&quot;,&quot;IJ&quot;,&quot;Im&quot;,&quot;In&quot;,&quot;ip&quot;,&quot;is&quot;,&quot;J&quot;,&quot;lh&quot;,&quot;ll&quot;,&quot;lm&quot;,&quot;lo&quot;,&quot;Lo&quot;,&quot;ls&quot;,&quot;lu&quot;,&quot;m&quot;,&quot;MH&quot;,&quot;mn&quot;,&quot;ms&quot;,&quot;N&quot;,&quot;nc&quot;,&quot;nd&quot;,&quot;nn&quot;,&quot;ns&quot;,&quot;on&quot;,&quot;Op&quot;,&quot;P&quot;,&quot;pa&quot;,&quot;pf&quot;,&quot;pi&quot;,&quot;Pi&quot;,&quot;pm&quot;,&quot;pp&quot;,&quot;ps&quot;,&quot;pt&quot;,&quot;q&quot;,&quot;qf&quot;,&quot;qq&quot;,&quot;qr&quot;,&quot;qt&quot;,&quot;r&quot;,&quot;Re&quot;,&quot;rf&quot;,&quot;rk&quot;,&quot;rl&quot;,&quot;rm&quot;,&quot;rt&quot;,&quot;s&quot;,&quot;sc&quot;,&quot;sd&quot;,&quot;SJ&quot;,&quot;sn&quot;,&quot;sp&quot;,&quot;ss&quot;,&quot;t&quot;,&quot;T&quot;,&quot;te&quot;,&quot;tr&quot;,&quot;ts&quot;,&quot;tt&quot;,&quot;tz&quot;,&quot;ug&quot;,&quot;UG&quot;,&quot;UN&quot;,&quot;V&quot;,&quot;VA&quot;,&quot;Vd&quot;,&quot;vi&quot;,&quot;Vo&quot;,&quot;w&quot;,&quot;W&quot;,&quot;y&quot;) #all names&#10;two_in_base &amp;lt;- c(&quot;ar&quot;, &quot;as&quot;, &quot;by&quot;, &quot;cm&quot;, &quot;de&quot;, &quot;df&quot;, &quot;dt&quot;, &quot;el&quot;, &quot;gc&quot;, &quot;gl&quot;, &quot;if&quot;, &quot;Im&quot;, &quot;is&quot;, &quot;lh&quot;, &quot;lm&quot;, &quot;ls&quot;, &quot;pf&quot;, &quot;pi&quot;, &quot;pt&quot;, &quot;qf&quot;, &quot;qr&quot;, &quot;qt&quot;, &quot;Re&quot;, &quot;rf&quot;, &quot;rm&quot;, &quot;rt&quot;, &quot;sd&quot;, &quot;ts&quot;, &quot;vi&quot;) # 2-letter names in base R&#10;vowels &amp;lt;- c(&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;)&#10;vowels &amp;lt;- c( vowels, toupper(vowels) )&#10;&#10;# Constants&#10;yoffset.singles &amp;lt;- 3&#10;&#10;# Define a function to give us consistent X coordinates&#10;returnX &amp;lt;- function(vec) {&#10;  sapply(vec, function(x) seq(length(all.letters))[ x == all.letters ] )&#10;}&#10;&#10;# Make df of 2-letter names&#10;combi &amp;lt;- nms[ sapply( nms, function(x) nchar(x)==2 ) ]&#10;combidf &amp;lt;- data.frame( first = substr(combi,1,1), second=substr(combi,2,2) )&#10;library(plyr)&#10;combidf &amp;lt;- arrange(combidf,first,second)&#10;&#10;# Add vowels&#10;combidf$first.vwl &amp;lt;- (combidf$first %in% vowels)&#10;combidf$second.vwl &amp;lt;- (combidf$second %in% vowels)&#10;&#10;# Flag items only in base R&#10;combidf$in_base &amp;lt;- paste(combidf$first,combidf$second,sep=&quot;&quot;) %in% two_in_base&#10;&#10;# Create a data.frame to hold our plotting information for the first letters&#10;combilist &amp;lt;- dlply(combidf,.(first),function(x) x$second)&#10;combi.first &amp;lt;- data.frame( first = names(combilist), n = sapply(combilist,length) ,stringsAsFactors=FALSE )&#10;combi.first$y &amp;lt;- 0&#10;all.letters &amp;lt;-  c(letters,LETTERS) # arrange(combi.first,desc(n))$first to go in order of prevalence (which may break the one-letter name display)&#10;combi.first$x &amp;lt;- returnX( combi.first$first )&#10;&#10;# Create a data.frame to hold plotting information for the second letters&#10;combidf$x &amp;lt;- returnX( combidf$first )&#10;combidf$y &amp;lt;- unlist( by( combidf$second, combidf$first, seq_along ) )&#10;&#10;# Make df of 1-letter names&#10;sngldf &amp;lt;- data.frame( sngl = nms[ sapply( nms, function(x) nchar(x)==1 ) ] )&#10;singles.y &amp;lt;- max(combidf$y) + yoffset.singles&#10;sngldf$y &amp;lt;- singles.y&#10;sngldf$x &amp;lt;- returnX( sngldf$sngl )&#10;&#10;# Plot&#10;library(ggplot2)&#10;ggplot(data=combidf, aes(x=x,y=y) ) +&#10;  geom_text(aes( label=second, size=3, colour=combidf$in_base ), position=position_jitter(w=0,h=.25)) +&#10;  geom_text( data=combi.first, aes( label=first, x=x, y=y, size=4 ) ) +&#10;  geom_text( data=sngldf, aes( label=sngl, x=x, y=y, size=4 ) ) +&#10;  scale_size(name=&quot;Order (2-letter names)&quot;,limits=c(1,4),breaks=c(1,2),labels=c(&quot;Second&quot;,&quot;First&quot;)) +&#10;  scale_x_continuous(&quot;&quot;,breaks=c(13,39),labels=c(&quot;lower&quot;,&quot;UPPER&quot;)) +&#10;  scale_y_continuous(&quot;&quot;,breaks=c(0,5,singles.y),labels=c(&quot;First letter of two-letter names&quot;,&quot;Second letter of two-letter names&quot;,&quot;One-letter names&quot;) ) +&#10;  coord_equal(1.5) +&#10;  labs( colour=&quot;In base R&quot; )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ETyau.png&quot; alt=&quot;version with one- and two-letter names on same plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7621L.png&quot; alt=&quot;letter-based histogram&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-09T07:35:15.020" Id="14028" LastActivityDate="2011-08-09T17:59:22.337" LastEditDate="2011-08-09T17:59:22.337" LastEditorUserId="3488" OwnerUserId="3488" ParentId="13999" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;To elaborate on @John's answer: in R's formulas, you have a few operators you can apply to the terms: &quot;+&quot; simply adds them, &quot;:&quot; means that you add a term (or several terms) that refer to their interaction (see below), &quot;*&quot; means both, that is: the &quot;main effects&quot; are added, and the interaction term(s) are added as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what does this interaction mean? Well, in the case of continuous variables, it is indeed a term that is added that is simply the multiple of the two variables. If you'd have height and weight as predictors, and use &lt;code&gt;out ~ height * weight&lt;/code&gt; as formula, the linear model will thus contain three 'variables', namely weight, height and their product (it also contains the interaction but that is of less interest here).&lt;/p&gt;&#10;&#10;&lt;p&gt;Although I suggest otherwise above: this works exactly the same way for categorical variables, but now the 'product' applies to the (set of) dummy variable(s) for each categorical variable. Suppose your height and weight are now categorical, each with three categories (S(mall), M(edium) and L(arge)). Then in linear models, each of these is represented by a set of two dummy variables that are either 0 or 1 (there are other ways of coding, but this is the default in R and the most commonly used). Let's say we use S as the reference category for both, then we have each time two dummies height.M and height.L (and similar for weight).&lt;/p&gt;&#10;&#10;&lt;p&gt;So now, model &lt;code&gt;out ~ height * weight&lt;/code&gt; now contains the 4 dummies + all the products of all dummy-combinations (I'm not explicitly writing the coefficients here, they are implied):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(intercept) + height.M + height.L + weight.M + weight.L + height.M * weight.M + height.L * weight.M + height.M * weight.L + height.L * weight.L.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the line above, '*' now again refers to a simple product, but this time of the dummies, so each product itself is also either 1 (when all factors are 1) or 0 (when at least one is not).&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case the 8 'variables' enable different (mean) outcomes in all combinations of the two variables: the effect of having large weight is now no longer the same for small people (for them the effect is simply formed by the term &lt;code&gt;weight.L&lt;/code&gt;) as for large people (here, the effect is &lt;code&gt;weight.L  + height.L * weight.L&lt;/code&gt;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-09T08:01:46.313" Id="14030" LastActivityDate="2011-08-09T08:01:46.313" OwnerUserId="4257" ParentId="14005" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;There is no single answer for your question, but you can approximate the six distributions to a varying degree of accuracy. First thing you should do is plot them using either histogram (hist() in R) or a kernel density estimate (density()). It should give you and idea as to what parametric family (exponential, normal, log-normal...) might provide you with a reasonable fit. If there is one, you can proceed with estimating the parameters.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-09T11:40:11.660" Id="14035" LastActivityDate="2011-08-09T11:40:11.660" OwnerUserId="5494" ParentId="14033" PostTypeId="2" Score="3" />
  
  
  
  
  <row Body="&lt;p&gt;I agree with the answer of StasK and would like to add the following. If you are willing to use Stata, have a look at Stephen Jenkins' &lt;a href=&quot;http://ideas.repec.org/c/boc/bocode/s438501.html&quot; rel=&quot;nofollow&quot;&gt;pgmhaz8&lt;/a&gt; routine. It will do the trick. You can also have a look at his &lt;a href=&quot;http://ideas.repec.org/a/bla/obuest/v57y1995i1p129-38.html&quot; rel=&quot;nofollow&quot;&gt;1995 paper&lt;/a&gt; on the subject. &#10;If you are a little comfortable with R, you wil be able to implement these methods without too much pain. You can use the &lt;a href=&quot;http://had.co.nz/reshape/&quot; rel=&quot;nofollow&quot;&gt;reshape&lt;/a&gt; package to structure the data and the glm funtion to estimate the model. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-09T18:08:41.140" Id="14053" LastActivityDate="2011-08-09T18:08:41.140" OwnerDisplayName="user5644" ParentId="13614" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is a strange way to approach it, frankly. Your &lt;code&gt;aa&lt;/code&gt; variable must already be a numeric variable, as trying to run &lt;code&gt;xtile&lt;/code&gt; with a string variable produces an expected error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    . sysuse auto, clear&#10;    . xtile qmake = make , nq(5)&#10;    type mismatch&#10;    r(109);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So if you were able to run &lt;code&gt;xtile bb=aa,nq(5)&lt;/code&gt;, you must have a numeric variable &lt;code&gt;aa&lt;/code&gt;, and now you want 5 quintiles of it. If you have only 10% of very dissatisfied and 10% of dissatisfied customers in the sample (coded in &lt;code&gt;bb&lt;/code&gt; as 1 and 2, respectively), then of course they'll be clumped together as the result of computing quintiles: they are both in the bottom quintile.&lt;/p&gt;&#10;&#10;&lt;p&gt;Typing &lt;code&gt;describe aa&lt;/code&gt; will tell you what the storage type is, and typing&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    tabulate aa&#10;    tabulate aa, nolabel&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;will show you frequencies with and without the labels. (I suspect you ran the first one, freaked out that this is a string variable, and then decided to convert it, even though it was not necessary since it was a numeric variable, to begin with.)&lt;/p&gt;&#10;&#10;&lt;p&gt;To convert strings to numbers, you can use &lt;code&gt;destring&lt;/code&gt;. To convert numbers, you can use &lt;code&gt;recode&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-09T18:43:32.333" Id="14054" LastActivityDate="2011-08-09T18:43:32.333" OwnerUserId="5739" ParentId="13517" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="14083" AnswerCount="4" Body="&lt;h3&gt;Context&lt;/h3&gt;&#10;&#10;&lt;p&gt;A popular question on this site is &quot; &lt;a href=&quot;http://stats.stackexchange.com/questions/4551/what-are-common-statistical-sins&quot;&gt;What are common statistical sins?&lt;/a&gt;&quot;.&#10;One of the sins mentioned is assuming that &quot;correlation implies causation...&quot; &lt;a href=&quot;http://stats.stackexchange.com/questions/4551/what-are-common-statistical-sins/4552#4552&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, in the comments with 5 upvotes it is suggested that: &quot;Google makes $65B a year not caring about the difference.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;At the risk of over-analysing a light quip, I thought this might be a useful discussion point for fleshing out the distinction between correlation and causation and the practical relevance of the distinction; and perhaps it could highlight something about the relationship between machine learning and the distinction between correlation and causation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming the comment is addressing technologies that underlie the generation of search engine results and advertising display related technologies. &lt;/p&gt;&#10;&#10;&lt;h3&gt;Question&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;To what extent is the distinction between correlation and causation relevant to Google's income generation, perhaps focusing particularly on the generation of income through advertising display related technologies and quality search results?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2011-08-10T01:38:44.733" FavoriteCount="1" Id="14067" LastActivityDate="2014-10-12T23:56:11.087" LastEditDate="2011-08-11T06:04:56.713" LastEditorUserId="183" OwnerUserId="183" PostTypeId="1" Score="19" Tags="&lt;machine-learning&gt;&lt;causal-inference&gt;" Title="To what extent is the distinction between correlation and causation relevant to Google?" ViewCount="498" />
  <row Body="&lt;p&gt;A great &lt;strong&gt;advantage&lt;/strong&gt; of &lt;a href=&quot;http://en.wikipedia.org/wiki/Phillips%E2%80%93Perron_test&quot;&gt;Philips-Perron&lt;/a&gt; test is that it is &lt;em&gt;non-parametric&lt;/em&gt;, i.e. it does not require to select the level of serial correlation as in ADF. It rather takes the same estimation scheme as in DF test, but corrects the statistic to conduct for autocorrelations and heteroscedasticity (HAC type corrections).&lt;/p&gt;&#10;&#10;&lt;p&gt;The main &lt;strong&gt;disadvantage&lt;/strong&gt; of the PP test is that it is based on asymptotic theory. Therefore it works well only in large samples that are indeed luxury if not it comes for financial time series data. And it also shares disadvantages of ADF tests: sensitivity to structural breaks, poor small sample power resulting in to often unit root conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is &lt;strong&gt;advisable&lt;/strong&gt; to make several tests and see if the results match, if not check the properties of the time series (PP is more robust to deviations from &quot;gentleman's&quot; set of properties!). You may also consider Zivot-Andrew test if you believe the data has structural breaks.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have found this lecture notes &lt;a href=&quot;http://www.bankofengland.co.uk/education/ccbs/handbooks/pdf/ccbshb22.pdf&quot;&gt;Unit Root Testing To Help Model Building&lt;/a&gt; by L. Mahadeva and P.Robinson useful to answer the questions. May be it will give more information to you.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2011-08-10T11:18:51.497" Id="14077" LastActivityDate="2011-08-10T15:06:08.770" LastEditDate="2011-08-10T15:06:08.770" LastEditorUserId="2645" OwnerUserId="2645" ParentId="14076" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;Lots of metric exist and no one is generally the best to use, it depends of your problem, of your data. Often, many metric can be used. I find usefull, to compute both hypothesis test and different metric (RMSE, MAPE ...), and see if they provide similar result. So your conclusions won't be based only on one metric. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-10T15:04:00.687" Id="14087" LastActivityDate="2011-08-10T15:04:00.687" OwnerUserId="5767" ParentId="13478" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The mean you described (the arithmetic mean) is what people typically intend when they say &quot;mean&quot; and, yes, that is the same as average. The only ambiguity that can occur is when someone is using a different type of mean, such as the geometric mean or the harmonic mean, but I think it is implicit from your question that you were talking about the arithmetic mean&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-10T17:25:59.707" Id="14100" LastActivityDate="2011-08-10T18:51:31.080" LastEditDate="2011-08-10T18:51:31.080" LastEditorUserId="919" OwnerUserId="4856" ParentId="14089" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;You get this error because the model matrix in the linear model is exactly &lt;a href=&quot;http://en.wikipedia.org/wiki/Multicollinearity&quot; rel=&quot;nofollow&quot;&gt;colinear&lt;/a&gt; (i.e., one of the predictor variables can be written as linear combination of the other predictor variables). If you add some noise to the time-series, then your example will run:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pp.test(jitter(1:1000))&#10;&#10;        Phillips-Perron Unit Root Test&#10;&#10;data:  jitter(1:1000)&#10;Dickey-Fuller Z(alpha) = -1083,254, Truncation lag parameter = 7,&#10;p-value = 0.01&#10;alternative hypothesis: stationary&#10;&#10;Warning message:&#10;In pp.test(jitter(1:1000)) : p-value smaller than printed p-value&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-08-10T10:45:59.890" Id="14112" LastActivityDate="2011-08-10T10:45:59.890" OwnerDisplayName="nullglob" OwnerUserId="1027" ParentId="14111" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for a statistical test to compare frequencies in a particular dataset. In a long string, a particular sequence of characters occurs at a frequency of x per 1000 characters. In some portions of the string, the frequency is higher at y per 1000 characters. Can we tell if y and x are significantly different or not?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-10T19:00:11.393" Id="14113" LastActivityDate="2011-11-15T22:29:47.243" OwnerUserId="5776" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;" Title="Best statistic to compare frequencies in two datasets" ViewCount="842" />
  
  <row AcceptedAnswerId="14184" AnswerCount="3" Body="&lt;p&gt;I have the following timeseries&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Q6Qld.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;obtained using the data posted below.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a sliding window size of 10, I am trying to compute the KL-divergence between the PMF of values within the current sliding window and the PMF of the history with the final goal of plotting the value of KL-divergence across time so that I can compare two time series. &lt;/p&gt;&#10;&#10;&lt;p&gt;As of now, there is a conceptual problem I am facing (which I'll explain using Python):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;In [228]: samples = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]&#10;&#10;# In reality this 10 should be 20 because that is the max value I have seen in the timeseries&#10;In [229]: bins = scipy.linspace(0, 10, 21) &#10;In [230]: bins&#10;Out[230]:&#10;array([  0. ,   0.5,   1. ,   1.5,   2. ,   2.5,   3. ,   3.5,   4. ,&#10;         4.5,   5. ,   5.5,   6. ,   6.5,   7. ,   7.5,   8. ,   8.5,&#10;         9. ,   9.5,  10. ])&#10;In [231]: scipy.histogram(samples, bins=bins, density=True)&#10;Out[231]:&#10;(array([ 1.63636364,  0.        ,  0.36363636,  0.        ,  0.        ,&#10;        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,&#10;        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,&#10;        0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),&#10; array([  0. ,   0.5,   1. ,   1.5,   2. ,   2.5,   3. ,   3.5,   4. ,&#10;         4.5,   5. ,   5.5,   6. ,   6.5,   7. ,   7.5,   8. ,   8.5,&#10;         9. ,   9.5,  10. ]))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem is that the resulting &lt;code&gt;PMF&lt;/code&gt; contains &lt;code&gt;0&lt;/code&gt;s so that I cannot really multiple two &lt;code&gt;PMF&lt;/code&gt;s to get the KL-divergence. Can someone tell me how to mitigate this problem?&lt;/p&gt;&#10;&#10;&lt;h3&gt;Data&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 12 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-08-11T00:08:36.817" Id="14127" LastActivityDate="2011-08-12T16:32:12.033" LastEditDate="2011-08-11T17:19:55.250" LastEditorUserId="919" OwnerUserId="2164" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;probability&gt;&lt;multivariate-analysis&gt;&lt;histogram&gt;&lt;kullback-leibler&gt;" Title="How to compute the Kullback-Leibler divergence when the PMF contains 0s?" ViewCount="2221" />
  <row Body="&lt;p&gt;Full disclosure: I haven't used &lt;code&gt;monomvn&lt;/code&gt; before, but glancing at the help files, it doesn't appear that it's really meant to directly impute missing data. Rather it seems intended be a way to estimate the multivariate normal distribution of data with lots of missing values, which is an important difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your goal is data imputation, perhaps you should investigate some other imputation method. I'm most familiar with imputation using random forests, but there's an enormous amount of literature out there on this topic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe one way to get started would be to try running &lt;code&gt;findFn(&quot;impute&quot;)&lt;/code&gt; from the &lt;code&gt;sos&lt;/code&gt; package.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-11T04:01:29.190" Id="14131" LastActivityDate="2011-08-11T04:01:29.190" OwnerUserId="5055" ParentId="14119" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Task view on time series mentions this: &lt;a href=&quot;http://cran.r-project.org/web/packages/tsDyn/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/tsDyn/index.html&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-11T06:25:38.370" Id="14137" LastActivityDate="2011-08-11T06:25:38.370" OwnerUserId="5739" ParentId="14110" PostTypeId="2" Score="2" />
  
  <row Body="&lt;h3&gt;Mean versus average&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The &lt;strong&gt;mean&lt;/strong&gt; most commonly refers to the &lt;strong&gt;arithmetic mean&lt;/strong&gt;, but may refer to some other form of mean, such as harmonic or geometric (see the &lt;a href=&quot;http://en.wikipedia.org/wiki/Mean&quot;&gt;Wikipedia article&lt;/a&gt;). Thus, when used without qualification, I think most people would assume that &quot;mean&quot; refers to the arithmetic mean.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Average&lt;/strong&gt; has many meanings, some of which are much less mathematical than the term &quot;mean&quot;. Even within the context of numerical summaries, &quot;average&quot; can refer to a broad range of measures of central tendency.&lt;/li&gt;&#10;&lt;li&gt;Thus, the &lt;strong&gt;arithmetic mean is one type of average&lt;/strong&gt;.&#10;Arguably, when used without qualification the average of a numeric variable often is meant to refer to the arithmetic mean.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Side point&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It is interesting to observe that Excel uses the sloppier but more accessible name of &lt;code&gt;AVERAGE()&lt;/code&gt; for its arithmetic mean function, where R uses &lt;code&gt;mean()&lt;/code&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-08-11T10:37:41.800" Id="14148" LastActivityDate="2011-08-11T10:37:41.800" OwnerUserId="183" ParentId="14089" PostTypeId="2" Score="17" />
  <row Body="&lt;p&gt;Your initial question was a simple problem of comparing means via one batch sample.  Your revised question is about &lt;a href=&quot;http://en.wikipedia.org/wiki/Sequential_analysis&quot; rel=&quot;nofollow&quot;&gt;sequential analysis&lt;/a&gt;, which is even more judicious about sample size.  I wish more people thought about sequential analysis, as it's much more cost-effective when samples are &lt;strong&gt;&lt;em&gt;expensive&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In deference to the &quot;teach a man to fish&quot; school of thought, I'll recommend looking into a book on this topic.  Wald's 1973 book is very nice, though I've never seen a Dover book list for so much on Amazon.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-11T12:02:23.230" Id="14151" LastActivityDate="2011-08-11T12:02:23.230" OwnerUserId="5256" ParentId="14106" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am having an input data set( 14 x 250 )  which has attached a labels set ( 1 x 250 ).&lt;/p&gt;&#10;&#10;&lt;p&gt;The labels set is discrete( 0 vs. 1). The problem is that the each one of the 14 characteristics are continuous. For example, for characteristic 1 I am having :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;10.18 ; 11.18 ; 9.89 ; 22.45&#10;0     ; 1     ; 0    ; 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to transform the continuous variables to discrete variables, and while I am doing this I want to have a Mutual Information value between the transformed characteristics and the label values, as high as possible. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I want to keep the number of variables in which I split each characteristic as low as possible, because I want to use them for a Bayesian Network model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you recommend me some transform algorithms/techniques for this? Or some matlab build in functions. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that I can apply the mean and standard deviation for each characteristic and then build the labels based on it, but I am not sure whether the mutual information value will be high.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-12T15:37:31.620" FavoriteCount="1" Id="14182" LastActivityDate="2011-08-12T15:37:31.620" OwnerUserId="5228" PostTypeId="1" Score="2" Tags="&lt;clustering&gt;&lt;data-transformation&gt;" Title="Transform continuous variable to discrete variable" ViewCount="313" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm not good at Statistics but I need to use it to summarize my case study.&#10;So I consult with a person who're good at Statistics. He suggested that&#10;My data is discrete because all the values are integer and some are duplicated as shown below. I want to find correlation between x and y of the two data sets below. In data set 1, x and y are discrete. In data set 2, x is discrete but y is continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Data Set 1&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; x   y&#10;-5  -19&#10;-5  -21&#10;-5  -23&#10;-5  -23&#10;-4  -17&#10;-4  -17&#10;-4  -18&#10;-4  -19&#10;-4  -19&#10;-4  -23&#10;-4  -19&#10;-4  -18&#10;-3  -13&#10;-3  -20&#10;-3  -17&#10;-3  -15&#10;-3  -17&#10;-3  -18&#10;-3  -18&#10;-3  -18&#10;-3  -18&#10;-3  -18&#10;-3  -18&#10;-3  -15&#10;-3  -15&#10;-2  -15&#10;-2  -15&#10;-2  -14&#10;-2  -14&#10;-2  -11&#10;-2  -12&#10;-1  1&#10;0   0&#10;0   0&#10;0   0&#10;0   0&#10;0   0&#10;0   0&#10;0   0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Data Set 2&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1   287.92&#10;1   340&#10;1   219.62&#10;1   30&#10;1   30&#10;1   292.56&#10;-2  -70.32&#10;-2  -82.04&#10;-2  -68.53&#10;-2  -68.53&#10;-2  -51.89&#10;-2  -57.36&#10;-3  -94.01&#10;-3  -66.61&#10;-3  -95.91&#10;-3  -95.91&#10;-3  -95.91&#10;-3  -95.91&#10;-3  -95.91&#10;-3  -95.91&#10;-3  -78.14&#10;-3  -125.34&#10;-3  -78.14&#10;-4  -110.45&#10;-4  -94.01&#10;-4  -94.01&#10;-4  -162.85&#10;-4  -162.85&#10;-4  -162.85&#10;-4  -120.93&#10;-5  -307.67&#10;-5  -123.19&#10;-5  -371.38&#10;-5  -166.91&#10;-5  -162.85&#10;-5  -302.21&#10;-5  -176.42&#10;-5  -307.67&#10;-5  -371.38&#10;-6  -390.71&#10;-6  -387.74&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so he suggested me to find covariance and then find the correlation from covariance&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;question 1&lt;/strong&gt;: Should I use &lt;em&gt;sample&lt;/em&gt; covariance or &lt;em&gt;population&lt;/em&gt; covariance&lt;/p&gt;&#10;&#10;&lt;p&gt;Its difference is that in sample covariance, the average of X and Y in the sample are used; whereas in population covariance, ux and uy are used.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;question 2&lt;/strong&gt;: some books mentioned the equation of population covariance that&lt;/p&gt;&#10;&#10;&lt;p&gt;pop_cov = sigma(xi-ux)(yi-uy)  /   N&lt;/p&gt;&#10;&#10;&lt;p&gt;others said &lt;/p&gt;&#10;&#10;&lt;p&gt;pop_cov = sigma(xi-ux)(yi-uy) = sigma(XY)- (uxuy)&lt;/p&gt;&#10;&#10;&lt;p&gt;Which one is correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;question 3&lt;/strong&gt;: if the answer of question 1 is population covariance, how can I get the probability of occuring each value of X (p(x)), each value of Y(p(y)), and occuring a particular X and Y (p(x,y)). Can I calculate these 3 probabilities from the sample I have, for example p(5) is 4/39 (4 is a number of 5 and 39 is total number of X).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;question 4&lt;/strong&gt;: the formula of correlation need standard deviation (corr = covariance/(sdx sdy)), will we use standard deviation of population or sample ( is it depending on the type of covariance we use, of pop or sample)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much. I 'm sorry that I am so novice about Statictiscs&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-12T15:41:41.517" Id="14183" LastActivityDate="2012-03-02T07:44:29.763" OwnerUserId="3584" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;correlation&gt;&lt;discrete-data&gt;&lt;variance-covariance&gt;&lt;covariance&gt;" Title="Finding correlation of discrete distribution data" ViewCount="2619" />
  
  <row AcceptedAnswerId="14252" AnswerCount="1" Body="&lt;p&gt;I am working on an ANN-based forecasting model for a financial time series. I'm using 5-fold cross-validation and the average performance is so so. Performance on the last fold (the iteration where the last segment is omitted from training and used for validation) is better than average.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a coincidence / data-dependent, or is validation performance on the last fold usually better? (presumably because training with all preceding data is more related to the subsequent data in time series)&lt;/p&gt;&#10;&#10;&lt;p&gt;This feels a bit like a weird question, but I'm hoping for some responses anyway. Thanks in advance :)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-12T20:24:27.103" FavoriteCount="1" Id="14197" LastActivityDate="2011-08-14T23:39:50.477" LastEditDate="2011-08-12T22:23:20.633" LastEditorUserId="88" OwnerUserId="5802" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;cross-validation&gt;&lt;neural-networks&gt;&lt;finance&gt;" Title="k-fold CV of forecasting financial time series -- is performance on last fold more relevant?" ViewCount="428" />
  <row Body="&lt;p&gt;A good place to start might be the &lt;a href=&quot;http://tm.r-forge.r-project.org/publications.html&quot;&gt;list of publications&lt;/a&gt; at the website for &lt;code&gt;tm&lt;/code&gt;, such as this one: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Text Mining Infrastructure in R. &lt;a href=&quot;http://www.jstatsoft.org/v25/i05&quot;&gt;http://www.jstatsoft.org/v25/i05&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The references list at the end of each of these publications includes successful applications of &lt;code&gt;tm&lt;/code&gt;, which is what you seem to be looking for. There are many -- especially if you then follow the references of the references. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, Here's one that might be of relevance: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Feinerer I, Hornik K (2007). \Text Mining of Supreme Administrative Court Jurisdictions.&quot;&#10;  In C Preisach, H Burkhardt, L Schmidt-Thieme, R Decker (eds.), \Data Analysis, Machine&#10;  Learning, and Applications (Proceedings of the 31st Annual Conference of the Gesellschaft&#10;  f ur Klassikation e.V., March 7{9, 2007, Freiburg, Germany),&quot; Studies in Classication,&#10;  Data Analysis, and Knowledge Organization. Springer-Verlag.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Good luck. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-12T20:48:05.787" Id="14200" LastActivityDate="2011-08-12T20:48:05.787" OwnerUserId="5387" ParentId="12466" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;There are a lot of answers to this question.  Here's one that you probably won't see elsewhere so I'm including it here because I believe it's pertinent to the topic.  People often believe that because the median is considered a robust measure with respect to outliers that it's also robust to most everything.  In fact, it's also considered robust to bias in skewed distributions.  These two robust properties of the median are often taught together.  One might note that underlying skewed distributions also tend to generate small samples that look like they have outliers and conventional wisdom is that one use medians in such situations.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#function to generate random values from a skewed distribution&#10;rexg &amp;lt;- function (n, m, sig, tau) {&#10;    rexp(n, rate = 1/tau) + rnorm(n, mean = m, sd = sig)&#10;    }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(just a demonstration that this is skewed and the basic shape)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hist(rexg(1e4, 0, 1, 1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GvZyG.png&quot; alt=&quot;plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, let's see what happens if we sample from this distribution various sample sizes and calculate median and mean to see what the differences between them are.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#generate values with various n's&#10;N &amp;lt;- 1e4&#10;ns &amp;lt;- 2:30&#10;y &amp;lt;- sapply(ns, function(x) mean(apply(matrix(rexg(x*N, 0, 1, 1), ncol = N), 2, median)))&#10;plot(ns,y, type = 'l', ylim = c(0.85, 1.03), col = 'red') &#10;y &amp;lt;- sapply(ns, function(x) mean(colMeans(matrix(rexg(x*N, 0, 1, 1), ncol = N))))&#10;lines(ns,y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cyIIw.png&quot; alt=&quot;plot2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As can be seen from the above plot the median (in red) is much more sensitive to the n than the mean.  This is contrary to some conventional wisdom regarding using medians with low ns, especially if the distribution might be skewed.  And, it reinforces the point that the mean is a known value while the median is sensitive to other properties, one if which being the n. &lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis is similar to &#10;Miller, J. (1988). A warning about median reaction time. &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(3):539–543.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;REVISION&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Upon thinking about the skew issue I considered that the impact on the median might just be because in small samples you have a greater probability that the median is in the tail of the distribution, whereas the mean will almost always be weighted by values closer to the mode.  Therefore, perhaps if one was just sampling with a probability of outliers then maybe the same results would occur.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I thought about situations where outliers may occur and experimenters may attempt to eliminate them.&lt;/p&gt;&#10;&#10;&lt;p&gt;If outliers happened consistently, such as one in every single sampling of data, then medians are robust against the effect of this outlier and the conventional story about the use of medians holds.&lt;/p&gt;&#10;&#10;&lt;p&gt;But that's not usually how things go.  &lt;/p&gt;&#10;&#10;&lt;p&gt;One might find an outlier in very few cells of an experiment and decide to use median instead of mean in this case.  Again, the median is more robust but it's actual impact is relatively small because there are very few outliers.  This would definitely be a more common case then the one above but the effect of using a median would probably be so small that it wouldn't matter much.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps more commonly outliers might be a random component of the data.  For example, the true mean and standard deviation of the population may be about 0 but there's a percentage of the time we sample from an outlier population where the mean is 3.  Consider the following simulation, where just such a population is sampled varying the sample size.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#generate n samples N times with an outp probability of an outlier.&#10;rout &amp;lt;- function (n, N, outp) {&#10;    outPos &amp;lt;- sample(0:1,n*N, replace = TRUE, prob = c(1-outp,outp))&#10;    numOutliers &amp;lt;- sum(outPos)&#10;    y &amp;lt;- matrix( rnorm(N*n), ncol = N )&#10;    y[which(outPos==1)] &amp;lt;- rnorm(numOutliers, 4)&#10;    return(y)&#10;    }&#10;&#10;outp &amp;lt;- 0.1&#10;N &amp;lt;- 1e4&#10;ns &amp;lt;- 3:30&#10;yMed &amp;lt;- sapply(ns, function(x) mean(apply(rout(x,N,outp), 2, median)))&#10;var(yMed)&#10;yM &amp;lt;- sapply(ns, function(x) mean(colMeans(rout(x,N,outp))))&#10;var(yM)&#10;plot(ns,yMed, type = 'l', ylim = range(c(yMed,yM)), ylab = 'Y', xlab = 'n', col = 'red') &#10;lines(ns,yM)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aRVl7.png&quot; alt=&quot;results&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The median is in red and mean in black. This is a similar finding to that of a skewed distribution.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In a relatively practical example of the use of medians to avoid the effects of outliers one can come up with situations where the estimate is affected by n much more when the median is used than when the mean is used.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2011-08-13T09:00:34.560" Id="14212" LastActivityDate="2011-08-15T14:24:16.570" LastEditDate="2011-08-15T14:24:16.570" LastEditorUserId="601" OwnerUserId="601" ParentId="14210" PostTypeId="2" Score="11" />
  
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;From the mean it's easy to calculate the sum over all items, e.g. if you know the average income of the population and the size of the population, you can immediately calculate the total income of the entire population.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The mean is straightforward to calculate in &lt;code&gt;O(n)&lt;/code&gt; time complexity. Calculating the median in linear time &lt;a href=&quot;http://en.wikipedia.org/wiki/Selection_algorithm#Proof_of_O.28n.29_running_time&quot;&gt;is possible&lt;/a&gt; but requires more thought. The obvious solution requiring sorting has worse (&lt;code&gt;O(n log n)&lt;/code&gt;) time complexity.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;And I speculate that there is another reason for the mean being more popular than the median:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The mean is taught to more persons at school and it's probably taught before teaching the median&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-08-13T11:31:51.517" Id="14214" LastActivityDate="2011-08-13T11:31:51.517" OwnerUserId="961" ParentId="14210" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="14216" AnswerCount="2" Body="&lt;p&gt;I am trying to develop a logic to identify association between different time series for association mining. I have a lot of series and need to find whether or not the association exists. I figured two ways to do this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Constructive: try to find relation in given two series.&lt;/li&gt;&#10;&lt;li&gt;Destructive: try to prove that relation does not exist.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Are there any existing mathematical parameters to identify such relations? if not, any suggestions?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-08-13T14:22:30.093" Id="14215" LastActivityDate="2011-09-08T11:49:55.870" LastEditDate="2011-09-08T11:49:55.870" LastEditorUserId="88" OwnerDisplayName="mihsathe" OwnerUserId="5808" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;data-mining&gt;" Title="Ways of finding associations in time series" ViewCount="208" />
  
  
  <row AcceptedAnswerId="14233" AnswerCount="2" Body="&lt;p&gt;Let's say I have 1 success in 4 bernoulli trials, and I wish to plot the distribution of the parameter $p$ of the corresponding binomial distribution. I'm using R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability of seeing 1 sucess and 3 failures in 4 tests for $p=0.25$ is, for these parameters:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; n &amp;lt;- 4&#10;&amp;gt; p &amp;lt;- 0.25&#10;&amp;gt; dbinom(1, n, p)&#10;[1] 0.421875&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To get the distribution for the parameter, I use a beta distribution $Beta(k+1, n-k+1)$. But when I try to calculate the value for $p=0.25$, I get a different result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; k &amp;lt;- 1&#10;&amp;gt; dbeta(p, k+1, n-k+1)&#10;&amp;gt; [1] 2.109375&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I tried to divide that by the function beta $B(k+1, n-k+1)$, but that did not work either:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dbeta(p, k+1, n-k+1)/beta(k+1, n-k+1)&#10;&amp;gt; [1] 42.1875&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That's exactly 100 times the result I expected. I am certainly missing something - maybe something basic, I'm no expert.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another thing that intrigues me: &lt;code&gt;dbeta&lt;/code&gt; region sums to 1 without dividing by the beta function.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-14T01:36:47.767" Id="14231" LastActivityDate="2011-08-14T13:05:20.413" OwnerUserId="3678" PostTypeId="1" Score="1" Tags="&lt;binomial&gt;&lt;prior&gt;&lt;beta&gt;" Title="When using the beta distribution as a prior distribution for binomial, why won't the distribution results match with the calculated probability?" ViewCount="888" />
  
  <row Body="&lt;p&gt;Yes, that is possible. Take these data for example&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      x      y       xy&#10;  .2217  .5000    .1108&#10;  .3048 -.9787   -.2983&#10;-1.6445  .3512   -.5775&#10; -.2461 -.4866    .1197&#10; -.3170 -.0954    .0302&#10;-1.1603 1.8352  -2.1294&#10; -.8720  .1372   -.1196&#10;-1.7852 -.2160    .3856&#10; 1.0100  .0165    .0166&#10;  .3000 -.3251   -.0975&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;$XY$ is a product of $X$ and $Y$. Multiple regression of $X$ on $Y$ and $XY$ yields $b$ for $XY$ as 0 and $b$ for $Y$ as -.444. Constant is -.386.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note the theoretical prerequisite for this: $bXY$ will be 0 if and only if $rX.XY$ (i.e. correlation bw $X$ and $XY$; &quot;.&quot; here means &quot;with&quot;) $= rX.Y * rY.XY$. Here, .280 = (-.361) * (-.776).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-14T08:02:49.593" Id="14235" LastActivityDate="2011-08-14T08:14:28.647" LastEditDate="2011-08-14T08:14:28.647" LastEditorUserId="3277" OwnerUserId="3277" ParentId="14234" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;You can get rid of some by looking for pairs that are very highly correlated and randomly deleting one of the pair.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you can look at partial least squares, and pick variables that are important in the PLS solution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I did this with a similar problem and it worked pretty well (that is, the resulting discriminant function did pretty well)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-14T14:38:09.527" Id="14242" LastActivityDate="2011-08-14T14:38:09.527" OwnerUserId="686" ParentId="14209" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If the concern is over the presence of outliers, there are some straight-forward ways to check your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Outliers, almost by definition, come into our data when something changes either in the process generating the data or in the process collecting the data. i.e. the data ceases to be homogeneous. If your data is not homogeneous then neither the mean nor the median make much sense, since you are trying to estimate the central tendency of two separate data sets that have been mixed together.&lt;/p&gt;&#10;&#10;&lt;p&gt;The best method to ensure homogeneity is to examine the data-generating and -collection processes to ensure that all of your data is coming from a single set of processes. Nothing beats a little brain-power, here.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a secondary check, you can turn to one of several statistical tests: chi-squared, Dixon's Q-test, Grubb's test or the control chart / process behavior chart (typically X-bar R or XmR). My experience is that, when your data can be ordered as it was collected, the process behavior charts are better at detecting outliers than the outlier tests. This use for the charts may be somewhat controversial, but I believe it is entirely consistent with Shewhart's original intent and it is a use that is &lt;a href=&quot;http://www.qualitydigest.com/magazine/2009/may/department/all-outliers-are-evidence.html&quot; rel=&quot;nofollow&quot;&gt;explicitly advocated&lt;/a&gt; by Donald Wheeler. Whether you use the outliers tests or the process behavior charts, remember that a detected &quot;outlier&quot; is merely signalling &lt;em&gt;potential&lt;/em&gt; non-homogeneity that needs to be further examined. It rarely makes sense to throw out data points if you don't have some explanation for why they were outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are using R, the &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/outliers/html/00Index.html&quot; rel=&quot;nofollow&quot;&gt;outliers package&lt;/a&gt; provides the outliers tests, and for process behavior charts there is the &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/qcc/html/00Index.html&quot; rel=&quot;nofollow&quot;&gt;qcc&lt;/a&gt;, IQCC and qAnalyst. I have a personal preference for the usage and output of the qcc package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-14T15:24:46.127" Id="14244" LastActivityDate="2011-08-14T15:24:46.127" OwnerUserId="145" ParentId="14210" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;As requested in a comment, here are some pointers for processing steps.  A number of tools may be found at the &lt;a href=&quot;http://cran.r-project.org/web/views/NaturalLanguageProcessing.html&quot;&gt;CRAN Task View for Natural Language Processing&lt;/a&gt;.  You may also want to look at &lt;a href=&quot;http://www.jstatsoft.org/v25/i05/paper&quot;&gt;this paper on the &lt;code&gt;tm&lt;/code&gt; (text mining) package for R&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Prior to processing, consider normalization of the word tokens.  &lt;code&gt;openNLP&lt;/code&gt; (for which there is an R package) is one route.&lt;/li&gt;&#10;&lt;li&gt;For text processing, a common pre-processing step is to normalize the data via &lt;code&gt;tf.idf&lt;/code&gt; -- term frequency * inverse document frequency - &lt;a href=&quot;http://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;see the Wikipedia entry&lt;/a&gt; for more details.  There are other more recent normalizations, but this is a bread and butter method, so it's important to know it.  You can easily implement it in R: just store (docID, wordID, freq1, freq2) where freq1 is the count of times the word indexed by wordID has appeared in the given document and freq2 is the # of documents in which it appears.  No need to store this vector for words that don't appear in a given document.  Then, just take freq1 / freq2 and you have your tf.idf value.&lt;/li&gt;&#10;&lt;li&gt;After calculating the tf.idf values, you can work with the full dimensionality of your data or filter out those words that are essentially uninformative.  For instance, any word that appears in only 1 document is not going to give much insight.  This may reduce your dimensionality substantially.  Given the small # of documents being examined, you may find that reducing to just 1K dimensions is appropriate.&lt;/li&gt;&#10;&lt;li&gt;I wouldn't both recentering the data (e.g. for PCA), but you can store the data now in a term matrix (where entries are now tf.idf values) with ease, using the sparse matrices, as supported by the &lt;code&gt;Matrix&lt;/code&gt; package.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;At this point, you have a nicely pre-processed dataset.  I would recommend proceeding with the tools cited in the CRAN task view or the text mining package.  Clustering the data, for instance by projecting onto the first 4 or 6 principal components, could be very interesting to your group when the data is plotted.&lt;/p&gt;&#10;&#10;&lt;p&gt;One other thing: you may find that dimensionality reduction along the lines of PCA (*) can be helpful when using various classification methods, as you are essentially aggregating the related words.  The first 10-50 principal components may be all that you need for document classification, given your sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;(*) Note: PCA is just a first step.  It can be very interesting for someone just starting out with text mining and PCA, but you may eventually find that it is a bit of a nuisance for sparse data sets.  As a first step, though, take a look at it, especially via the &lt;code&gt;prcomp&lt;/code&gt; and &lt;code&gt;princomp&lt;/code&gt; functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: I didn't state a preference in this answer - I recommend &lt;code&gt;prcomp&lt;/code&gt; rather than &lt;code&gt;princomp&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-08-14T19:07:26.593" Id="14250" LastActivityDate="2011-08-15T14:47:27.127" LastEditDate="2011-08-15T14:47:27.127" LastEditorUserId="5256" OwnerUserId="5256" ParentId="14219" PostTypeId="2" Score="13" />
  
  
  <row Body="&lt;p&gt;This is a statistics question, not a programming question, and would better be asked on &lt;a href=&quot;http://stats.stackexchange.com&quot;&gt;CrossValidated&lt;/a&gt;. At least, the LaTeX code is getting parsed there automatically :). Also, this is more complicated than what is readily available on that webpage. I'll give some guidance, but as long as you want to learn how to do things, this won't be the complete answer. (If you don't want to do that, we can locate the cooked answers on the web, too.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Each sampling of betas relies on the complete data set. If you are doing this with individual y_i's and x_i's, you are not doing this right. Before you start working with the code, you need to sit down with a piece of paper (letter size or A4, depending on your geography) and derive the posterior distributions of betas:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;This is given: y|beta is normal with mean x'beta and precision tau&lt;/li&gt;&#10;&lt;li&gt;This is given: prior for beta is normal with mean mu and precision gamma&lt;/li&gt;&#10;&lt;li&gt;Obtain this: the marginal distribution of y, by integrating the betas out (which is easy to do, since the joint distribution of y and beta is multivariate normal, and you can do this by kernel matching: the part that depends on beta is going to be exp[ a quadratic form in beta], so you recognize this to be a relevant part of a normal distribution distribution to integrate over; whatever's left after integration should be a normal density in y and the prior parameters)&lt;/li&gt;&#10;&lt;li&gt;Obtain this: the posterior distribution of beta given y, by Bayes theorem (the likelihood times the prior divided by the posterior; again this should be a moderately complicated combination of exp[ jointly quadratic in y and beta ])&lt;/li&gt;&#10;&lt;li&gt;Obtain this: the conditional distribution of beta_1 given beta_2 and y, one of the margins of the multivariate normal distribution obtained at the previous step.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;You need to know how to manipulate the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multivariate_normal_distribution&quot; rel=&quot;nofollow&quot;&gt;multivariate normal distribution&lt;/a&gt; and get conditional and marginal distributions out of it. Again, if this is over your head, we can find the ready solutions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that you also need a sampler for the variance of regression errors, unless you treat it as known (which is hardly a practical situation). This will be slightly more complicated, as you would need to incorporate another dimension into your integration procedures.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-12T18:53:54.183" Id="14261" LastActivityDate="2011-08-12T18:53:54.183" OwnerDisplayName="StasK" OwnerUserId="5739" ParentId="14260" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm running AB tests on my website homepage, with 8 different variations. You can read about the purpose of the test here if you need to (not essential) - &lt;a href=&quot;http://westiseast.co.uk/blog/ab-split-testing-a-promise-ogilvy/&quot; rel=&quot;nofollow&quot;&gt;http://westiseast.co.uk/blog/ab-split-testing-a-promise-ogilvy/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is this - at what point can I remove the worst performing variants from the test? &lt;/p&gt;&#10;&#10;&lt;p&gt;Can I remove the worst performing variant when it achieves statistical significance when compared to the BEST performing variant? Or when it achieves statistical significance compared to the 2nd worst performing variant?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to continually refine the test, and removing the worst performing examples improves the traffic going to the 'better' ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks for your help!&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-08-15T01:12:09.197" FavoriteCount="1" Id="14270" LastActivityDate="2011-08-16T06:27:58.373" LastEditDate="2011-08-16T06:27:58.373" LastEditorUserId="264" OwnerDisplayName="Chris" PostTypeId="1" Score="4" Tags="&lt;experiment-design&gt;&lt;ab-test&gt;&lt;sequential-analysis&gt;" Title="AB Multi-variate testing - when can I eliminate worst performing variants?" ViewCount="276" />
  
  <row AcceptedAnswerId="14279" AnswerCount="1" Body="&lt;p&gt;I have a dataset that includes individual responses to a series of questions.&#10;Participants played a game with one of two roles (hider, seeker) and indicated their response with a binary variable (0:BLUE, 1:RED).&#10;There were 4 within subject conditions and 3 between subject conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would somehow like to graphically summarize aggregate patterns in individual responses; i.e. I would like to summarize common patterns in each condition.  For example, to see how many participants gave the same answer in each condition, across conditions, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'm thinking of some kind of frequency distribution, but for each individual question. So probably some kind of multidimensional grid would be best.&#10;Any suggestions?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-15T12:54:37.383" Id="14272" LastActivityDate="2011-08-16T08:34:23.433" LastEditDate="2011-08-15T14:02:46.050" LastEditorUserId="919" OwnerUserId="5837" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;" Title="Seeking suggestions for summarizing and visualizing data" ViewCount="401" />
  
  
  <row Body="&lt;p&gt;You could do a &lt;a href=&quot;https://secure.wikimedia.org/wikipedia/en/wiki/Exact_test&quot; rel=&quot;nofollow&quot;&gt;permutation test&lt;/a&gt;: re-sample the predictions to take the same number of predictions you originally had in your results for each algorithm, but randomly assigning the scores to algorithms while keeping the same number of positive and negative labelled sets. Calculate the difference in squared errors for each randomization. If you do this N times and k times you get a difference you observed or higher, the significance p-value for the difference in accuracy is $k/N$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To simplify, suppose you were running the algorithms on the same data. For example, for data point $i$: the true label is $t_i$, algorithm 1 gives output $y^1_i$, and algorithm 2 gives $y^2_i$. The error cost for algorithm 1 is $e^1_i = (t_i - y^1_i)^2$, and $e^2_i = (t_i-y^2_i)^2$ for algorithm 2. Thus we have two vectors $\mathbf{e_1}=[e_1^1, e_2^1,...e_n^1]$, and $\mathbf{e_2}=[e_1^2, e_2^2,...e_n^2]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference in accuracy is $s=\|\mathbf{e_1}\|^2-\|\mathbf{e_2}\|^2$. Now let us just randomize byrandomly switching $e_i^1$ and $e_i^2$ for each $i$ with a probability of $0.5$. For each randomization $r$, you will get a difference in accuracy $s_r$. If you do this $N$ times, your p-value is $\frac{\sum_{r=0}^{N-1}\mathbf{1}_{|s_r|\ge s}}{N}$. This is the fraction of times you get an absolute value of the difference equal to or greater than that you observed in your experiment.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-15T14:24:47.003" Id="14291" LastActivityDate="2011-08-15T17:19:00.177" LastEditDate="2011-08-15T17:19:00.177" LastEditorUserId="2728" OwnerUserId="2728" ParentId="14280" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="14297" AnswerCount="1" Body="&lt;p&gt;Let's say I want to generate 100 numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I don't allow for any duplications it would go from 1 to 100. But I want to allow for 20% of duplications i.e. so to 20% of the sample would be non-unique -- how can I do that?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-15T15:24:43.987" Id="14294" LastActivityDate="2011-08-15T16:28:31.513" LastEditDate="2011-08-15T16:28:31.513" LastEditorUserId="88" OwnerUserId="333" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;random-variable&gt;" Title="How to generate random sample with controlled repetition?" ViewCount="159" />
  
  
  <row Body="&lt;p&gt;This question concerns a lecture on EM with an &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_family&quot; rel=&quot;nofollow&quot;&gt;exponential family&lt;/a&gt; of distributions.  The logarithm of each density $f$ in this family can be expressed (up to a normalization constant) as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log(f_X(x|\mathbf{\theta})) = \eta(x) + \sum_i s_i(x) \theta_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathbf{\theta} = (\theta_i)$ is a vector of parameters.  Apart from influencing the normalization constant (which doesn't matter for inference), parameter $\theta_i$ enters the formula &lt;em&gt;solely&lt;/em&gt; through its product with $s_i(x)$, a function of the observation $x$.  Thus, everything that can be known about $f$ depends on $x$ only through the vector of values $\mathbf(s) = (s_i(x))$.  It is a short step to conclude that the functions $s_i$ generate a set of &lt;a href=&quot;http://en.wikipedia.org/wiki/Sufficient_statistic&quot; rel=&quot;nofollow&quot;&gt;sufficient statistics&lt;/a&gt; for this family.  The lecture uses the term &quot;ith sufficient statistic&quot; to refer to $s_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This form of distribution function is one of the simplest possible in any theory or algorithm, like EM, that relies on the log likelihood function, because the log likelihood is a $\theta$-linear combination of the sufficient statistics: the linearity implies the log-likelihood is a differentiable function of the parameters and the partial derivatives can be instantly recognized (they are obtained by summing each $s_i(x)$ over the dataset).  Thus it leads to relatively simple formulas and a mathematically tractable analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-15T18:03:56.823" Id="14300" LastActivityDate="2011-08-15T18:03:56.823" OwnerUserId="919" ParentId="13959" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Frankly, in my field, I never see measures of model fit reported. They tend to get used as internal diagnostics, with the assumption that by the time you've reached the point where you're trying to report model results, you've properly tuned your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;In essence, we're interested in estimates of effect - if your model fits terribly, why are you (hypothetical you, not @author) trying to publish it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-15T21:24:06.320" Id="14307" LastActivityDate="2011-08-15T21:24:06.320" OwnerUserId="5836" ParentId="11680" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="14329" AnswerCount="2" Body="&lt;p&gt;I'm looking at a few logistic regression issues.  (&quot;regular&quot; and &quot;conditional&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, I'd like to weight each of the input cases so that the glm will focus more on predicting the higher weighted cases correctly at the expense of possibly misclassifying the lower weighted cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Surely this has been done before.  Can anyone point me toward some relevant literature (Or possibly suggest a modified likelihood function.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-16T05:34:20.237" FavoriteCount="2" Id="14324" LastActivityDate="2011-08-16T07:15:10.303" OwnerUserId="2566" PostTypeId="1" Score="4" Tags="&lt;logistic&gt;" Title="Case weighted logistic regression" ViewCount="3750" />
  
  <row AcceptedAnswerId="14370" AnswerCount="5" Body="&lt;p&gt;I am building a big data frame by merging the content of a few files together. These files share the same columns layout.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;c = read.delim('bigfile1.txt')&#10;c1 = read.delim('bigfile2.txt')&#10;c2 = read.delim('bigfile3.txt')&#10;&#10;ctmp1 = merge(c, c1, all=TRUE)&#10;ctmp2 = merge(ctmp1, c2, all=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is the above code efficient?&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I reuse the same variable name instead, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tmp = merge(c, c1, all=TRUE)&#10;tmp = merge(tmp, c2, all=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="9" CreationDate="2011-08-16T07:49:37.040" Id="14332" LastActivityDate="2014-01-20T11:38:23.787" LastEditDate="2014-01-20T11:38:23.787" LastEditorUserId="35842" OwnerUserId="825" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;dataframe&gt;" Title="Efficient way to merge multiple dataframes in R" ViewCount="5038" />
  <row Body="&lt;p&gt;When you have a finite population and the sample size is more than 5-10% of the population then you should use the Finite Population Correction, that is you multiply your standard error times $\sqrt{\frac{N-n}{N-1}}$, see &lt;a href=&quot;http://en.wikipedia.org/wiki/Finite_population_correction#Correction_for_finite_population&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should still use the t-distribution, but the standard error will be smaller to account for the large portion of the population in the sample.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-16T12:10:35.663" Id="14340" LastActivityDate="2011-08-18T08:37:26.840" LastEditDate="2011-08-18T08:37:26.840" LastEditorUserId="2116" OwnerUserId="4505" ParentId="14323" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="14346" AnswerCount="1" Body="&lt;p&gt;Let's say I've collected a &lt;em&gt;small&lt;/em&gt; number (N) of observations about a hypothesis that I'd like to test. I could use the bootstrap method to produce a sample distribution for the mean result of N observations, but I'm concerned that this model could break down when N gets very small, introducing error into the sample distribution itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, how can I determine what the minimum N is that I need for reasonable results; or more quantitatively, how is N tied to the sampling error as N-&gt;0?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; I am coming to understand that the minimum value for N will vary based on the nature of the underlying data. So, in this case what meta-observations can I make to help me determine this? I don't know the true underlying distribution, or else I wouldn't need the bootstrapping...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-16T13:38:16.863" Id="14343" LastActivityDate="2011-08-16T15:05:37.070" LastEditDate="2011-08-16T15:05:37.070" LastEditorUserId="5265" OwnerUserId="5265" PostTypeId="1" Score="7" Tags="&lt;bootstrap&gt;" Title="Bootstrapping with a small number of observations" ViewCount="360" />
  <row AcceptedAnswerId="14407" AnswerCount="3" Body="&lt;p&gt;I have a dataset with responses coded as 0 and 1. I am trying to specify 3 models of responses and compare it to the observed results. I would therefore like to make single comparisons between two binary distributions. Is there a way to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically what I want is to compare observed results for 16 question per participant with a model that has predictions for each 16 questions. I would like to see how much these predictions match the actual observed responses.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-08-16T15:25:48.793" Id="14355" LastActivityDate="2011-08-17T14:42:27.493" LastEditDate="2011-08-16T16:16:38.180" LastEditorUserId="5837" OwnerUserId="5837" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;" Title="Comparing two binary distributions" ViewCount="636" />
  
  
  <row Body="&lt;p&gt;You can learn a lot by exploring the temporal relationships among the data before setting off on more sophisticated methods of time series analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the image does not provide a horizontal scale, I have used an artificial one (pixel coordinates of the image columns).  I have freely rescaled the two plots and placed the activity at zero, which is likely what its minimum is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7JWkd.png&quot; alt=&quot;Data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(There's a tiny problem where the two graphs originally overlapped, but that won't affect the analysis.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Because you're looking for a potential causal effect of activity (red) on volume (blue), &lt;strong&gt;plot the forward cross-correlation function&lt;/strong&gt;.  This simply computes the correlation of the activity[1..n+1-k] and volume[k..n] as a function of k (the &quot;lag&quot;).  My first pass at this found--as one would expect--that the extreme activity peak has a lot of leverage, so I went back and used the logarithm of 1+activity instead.  Here's the cross-correlation:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vpf6c.png&quot; alt=&quot;Cross-correlation plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It reaches a definite peak at a lag of k=20.  (The peak using the untransformed activities was at k=18, essentially the same.)  To better see what's going on, look at the scatterplot of activity[1..n+1-20] against volume[20..n]:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7f4ow.png&quot; alt=&quot;Scatterplot at lag 20&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, each log activity value (x) is plotted against the volume after 20 units of time have elapsed (y).  (The points are colored according to time.  This is really a 3D plot with time as the z coordinate, looked at from high above.  3D manipulations make the patterns clearer.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This scatterplot makes it evident that the relatively high correlation at this lag (about 0.40 according to the preceding plot) is due &lt;em&gt;entirely&lt;/em&gt; to the activity peak at time 220 being followed by the volume peak at time 220+20 (as followed by a plunge shortly thereafter).  Everything else looks like random wandering.  This tells us that &lt;strong&gt;there may be less than meets the eye&lt;/strong&gt;: the appearance of a temporal relationship between these two series is caused by a single event and its aftermath; it does not appear to be a general association.&lt;/p&gt;&#10;&#10;&lt;p&gt;In effect, you may have &lt;em&gt;one&lt;/em&gt; observation of a high activity followed after a certain short time by a spike in volume.  Was this a coincidence or (hopefully) evidence of a pattern?  It's impossible to tell.  Additional data might bear out this generalization or might falsify it.  Regardless, cross-correlation plots and lagged scatterplots are powerful and useful tools to evaluate the relationships.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-16T17:25:44.423" Id="14366" LastActivityDate="2011-08-16T17:25:44.423" OwnerUserId="919" ParentId="14351" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;The gradient should be (by chain rule)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;%the gradient&#10;%helper function&#10;expt =  @(w)(exp(-t .* (phis * w')));&#10;%precompute -t * phis&#10;tphis = -diag(t) * phis;  %or bsxfun(@times,t,phis);&#10;%the gradient&#10;gradf = @(w)((sum(bsxfun(@times,expt(w) ./ (1 + expt(w)), tphis),1)'/size(phis,1)) + 2*coef * w');&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;probably would be faster not to compute &lt;code&gt;expt(w)&lt;/code&gt; twice per evaluation, so you can rewrite this in terms of another anonymous function which takes &lt;code&gt;exptw&lt;/code&gt; as input.&lt;/p&gt;&#10;&#10;&lt;p&gt;also I may have goofed up the dimensions on the sum--it seems like you are using &lt;code&gt;w&lt;/code&gt; as a row vector, which is somewhat nonstandard.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;: as @whuber noted, this kind of thing is easy to screw up. I didn't actually try the code I had previously. the above should be correct now. To test it, I estimated the gradient numerically and compared to the 'exact' value, as below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;%set up the problem&#10;N = 9;&#10;phis = rand(N,N+1);&#10;t = rand(N,1);&#10;coef = rand(1);&#10;&#10;%the objective&#10;f = @(w)((sum(log(1 + exp(-t .* (phis * w'))),1) / size(phis, 1)) + coef * w*w');&#10;&#10;%helper function&#10;expt =  @(w)(exp(-t .* (phis * w')));&#10;%precompute -t * phis&#10;tphis = -diag(t) * phis;  %or bsxfun(@times,t,phis);&#10;%the gradient&#10;gradf = @(w)((sum(bsxfun(@times,expt(w) ./ (1 + expt(w)), tphis),1)'/size(phis,1)) + 2*coef * w');&#10;&#10;%test the code now:&#10;%compute the approximate gradient numerically&#10;w0 = randn(1,N+1);&#10;fw = f(w0);&#10;&#10;%%the numerical:&#10;delta = 1e-6;&#10;eyeN = eye(N+1);&#10;&#10;gfw = nan(size(w0));&#10;for iii=1:numel(w0)&#10;    gfw(iii) = (f(w0 + delta * eyeN(iii,:)) - fw) ./ delta;&#10;end&#10;&#10;%the 'exact':&#10;truegfw = gradf(w0);&#10;&#10;%report&#10;fprintf('max difference between exact and numerical is %g\n',max(abs(truegfw' - gfw)));&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;when I run this (sorry, should have set the rand seed), I get:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;max difference between exact and numerical is 4.80006e-07&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;YMMV, depending on the rand seed and the value of &lt;code&gt;delta&lt;/code&gt; used.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-16T19:26:50.367" Id="14371" LastActivityDate="2011-08-16T21:08:14.010" LastEditDate="2011-08-16T21:08:14.010" LastEditorUserId="795" OwnerUserId="795" ParentId="14368" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;John Fox has a great appendix available on-line using nlme to look at longitudinal data. It may be useful for you:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There's a lot of great stuff there (and Fox' books are generally quite good!).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-16T21:58:30.640" Id="14376" LastActivityDate="2011-08-16T21:58:30.640" OwnerUserId="4514" ParentId="2777" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;In my opinion, Frank Harrell's &quot;Regression Modeling Strategies&quot; is a good reference. In fact, it is probably my favourite statistics book.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've only studied less than half of the book so far, but have got lots of good stuff out of it, for example, representing predictors as splines to avoid assuming linearity, multiple imputation for missing data, and bootstrap model validation. Perhaps my favourite thing about the book is the general theme that an important goal is to get results which will replicate on new data, not results that only hold on the current data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional benefits are Frank Harrell's R package rms which makes it easy to do many of the things described in the book, and his willingness to answer questions here and on R-help.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-17T01:53:57.760" Id="14382" LastActivityDate="2011-08-17T10:29:35.783" LastEditDate="2011-08-17T10:29:35.783" LastEditorUserId="3835" OwnerUserId="3835" ParentId="14380" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Use the &lt;em&gt;TEST&lt;/em&gt; statement.  So for your example you would have:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;proc reg data = data;&#10;model a = b; &#10;test b=1;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-08-17T06:52:33.443" Id="14389" LastActivityDate="2011-08-17T06:52:33.443" OwnerUserId="2392" ParentId="14383" PostTypeId="2" Score="3" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The estimate using the gradients will be then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{n}\sum_{i=1}^n\frac{\partial \log f(X_i)}{\partial \theta}\left(\frac{\partial \log f(X_i)}{\partial \theta}\right)&amp;#39;.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But for this you will need somehow to estimate the gradient at each point $X_i$. This can be done either numerically or analytically. Note that since the differentiation is done with respect to $\theta$, with $X_i$ as constant it might not be that hard.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-17T11:01:24.133" Id="14394" LastActivityDate="2011-08-17T11:01:24.133" OwnerUserId="2116" ParentId="12041" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="14530" AnswerCount="3" Body="&lt;p&gt;Could you suggest more recent reviews or book on the methods for handling missing data (i.e. multiple imputation)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know the challenges in this field.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-17T13:31:15.363" FavoriteCount="1" Id="14401" LastActivityDate="2012-08-14T22:04:16.767" LastEditDate="2011-08-17T15:10:51.190" LastEditorUserId="5864" OwnerUserId="5864" PostTypeId="1" Score="4" Tags="&lt;books&gt;&lt;missing-data&gt;" Title="What are latest statistical challenges regarding missing data?" ViewCount="226" />
  
  
  
  <row AcceptedAnswerId="14459" AnswerCount="1" Body="&lt;p&gt;How can one easily and quickly grasp the idea of Bayesian statistics and modelling? I can understand the Bayesian theorem on conditional probabilities, I understand how frequentist statistics works (test statistics, p-values, hypothesis testing...), I understand &lt;a href=&quot;http://stats.stackexchange.com/questions/2356/are-there-any-examples-where-bayesian-credible-intervals-are-obviously-inferior-t/6431#6431&quot;&gt;some ideas of Bayesian statistics&lt;/a&gt;, but how does Bayesian inference work (technically) - that's still a little mystery to me.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-18T07:29:10.853" FavoriteCount="3" Id="14444" LastActivityDate="2011-08-18T11:39:48.057" LastEditDate="2011-08-18T11:39:48.057" LastEditorUserId="930" OwnerUserId="5509" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;" Title="How to understand Bayesian statistics?" ViewCount="694" />
&#10;     $ FX.Fixed     : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ...&#10; $ Branch       : Factor w/ 1 level &amp;quot;nil&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
  <row Body="&lt;p&gt;Answer to Goal 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aggregate(. ~ Currency, &#10;          data=subset(c,select=c(&quot;Currency&quot;,&#10;                                 names(sapply(c,is.numeric)))),sum)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-08-18T10:11:52.757" Id="14455" LastActivityDate="2011-08-18T10:11:52.757" OwnerUserId="419" ParentId="14448" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="14476" AnswerCount="3" Body="&lt;p&gt;I'm wondering if anybody knows of a compendium of cross-validation techniques with a discussion of the differences between them and a guide on when to use each of them. Wikipedia &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#Common_types_of_cross-validation&quot;&gt;has a list&lt;/a&gt; of the most common techniques, but I'm curious if there are other techniques, and if there are any taxonomies for them.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I just run into a library that allows me to choose one of the following strategies:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Hold out&lt;/li&gt;&#10;&lt;li&gt;Bootstrap&lt;/li&gt;&#10;&lt;li&gt;K Cross-validation&lt;/li&gt;&#10;&lt;li&gt;Leave one out&lt;/li&gt;&#10;&lt;li&gt;Stratified Cross Validation&lt;/li&gt;&#10;&lt;li&gt;Balanced Stratified Cross Validation&lt;/li&gt;&#10;&lt;li&gt;Stratified Hold out&lt;/li&gt;&#10;&lt;li&gt;Stratified Bootstrap&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;and I am trying to understand what stratified and balanced mean in bootstrapping, hold out or CV.&lt;/p&gt;&#10;&#10;&lt;p&gt;We can also turn this post into a community wiki if people want ,and collect a discussion of techniques or taxonomies here. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-08-18T16:24:48.727" CreationDate="2011-08-18T15:53:08.557" FavoriteCount="17" Id="14474" LastActivityDate="2011-08-19T06:50:53.660" LastEditDate="2011-08-18T15:58:42.713" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="27" Tags="&lt;cross-validation&gt;" Title="Compendium of cross-validation techniques" ViewCount="2210" />
  <row Body="&lt;p&gt;A flexible framework for this is provided by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Power_transform&quot;&gt;Box-Cox transformation&lt;/a&gt;, i.e., a power transformation $y \mapsto C(y,\lambda) y^\lambda$ where the parameter $\lambda$ of transformation $\lambda$ is driven by the data (essentially, towards reducing the skewness); $C(y,\lambda)$ is a scaling parameter to make comparison of the goodness of fit provided by the different powers meaningful.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-18T16:01:16.177" Id="14475" LastActivityDate="2011-08-18T16:01:16.177" OwnerUserId="5739" ParentId="14469" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I found this function to be useful, but the learning curve was steep. If you read the documentation within SPSS for every step in the automated process you can find out exactly what was done and possibly duplicate it through other means.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-18T17:59:30.783" Id="14479" LastActivityDate="2011-08-18T17:59:30.783" OwnerUserId="5885" ParentId="7432" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If your dates are in string or factor form, convert them to Dates with &lt;code&gt;as.Date(DateVariable, format=&quot;%Y-%m-%d&quot;)&lt;/code&gt; (potentially with an &lt;code&gt;as.character&lt;/code&gt; around &lt;code&gt;DateVariable&lt;/code&gt; if it is a factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sort the data.frame with the date and outcome by surgery date.  To get the y variable you describe, the easiest way is to take the cumulative number of outcomes and divide by the number of cases to date (which is just a running sequence from 1 to the number of cases when sorted).  In code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Make some random data to play with&#10;DF &amp;lt;- data.frame(DateVariable = as.Date(runif(100, 0, 800), origin=&quot;2005-01-01&quot;),&#10;    outcome = rbinom(100, 1, 0.1))&#10;#Sort by date&#10;DF &amp;lt;- DF[order(DF$DateVariable),]&#10;DF &amp;lt;- rbind.fill(data.frame(outcome=1),DF)&#10;&#10;#Add case numbers (in order, since sorted)&#10;DF$x &amp;lt;- seq(length=nrow(DF))&#10;&#10;#Create your definition for y (average to date, which is sum to date divided by number to date)&#10;DF$y &amp;lt;- cumsum(DF$outcome) / DF$x&#10;&#10;#Plot it&#10;library(ggplot2)&#10;ggplot(DF, aes(x,y)) + &#10;geom_point(shape=4) +&#10;geom_point(aes(x,outcome),shape=3) +&#10;stat_smooth(method=&quot;loess&quot;, se=FALSE, color=&quot;darkgreen&quot;, size=1) +&#10;scale_y_continuous(name= &quot;Failure rate&quot;, limits=c(0, 1)) +&#10;scale_x_continuous(name= &quot;Operations performed&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Result:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZsFra.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(plus marks success and error cases. X - cumulative sum, line - loess curve)&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think, however, this is a good metric.  Check out some work on CUSUM curves and risk adjusted CUSUM curves.  CUSUM is just plotting number of (negative) outcomes versus case number; risk adjusted CUSUM assumes you can determine a probability of negative outcome (based on pre-operative variables) and use that to determine if performance is exceeding or lagging expectations.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-08-18T19:45:58.960" Id="14488" LastActivityDate="2011-08-22T07:13:38.973" LastEditDate="2011-08-22T07:13:38.973" LastEditorUserId="3376" OwnerUserId="5880" ParentId="14480" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Python with &lt;a href=&quot;http://matplotlib.sourceforge.net/&quot;&gt;MatplotLib&lt;/a&gt;. Python is pretty good at manipulating csv files.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;but haven't seen any simple examples for plotting multiple columns of&#10;  data onto the same X and Y axes as a spreadsheet scatterplot will do.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Have you explored &lt;a href=&quot;http://had.co.nz/ggplot2/&quot;&gt;ggplot2&lt;/a&gt;? You can keep adding series to a plot using ggplot2. It also has a very good facet plotting feature.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-19T01:26:50.897" Id="14498" LastActivityDate="2011-08-19T01:26:50.897" OwnerUserId="2635" ParentId="14497" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;An informative &quot;learning curve&quot; will indicate the &lt;em&gt;current performance&lt;/em&gt; of success.  A cumulative average, however, doesn't do that, because it includes old results along with the new.  When learning truly improves, the cumulative average will be biased low.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A good solution is to use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Local_regression&quot; rel=&quot;nofollow&quot;&gt;Lowess smooth&lt;/a&gt;&lt;/strong&gt; of the response plotted over time.  This robust nonparametric smoother is implemented in many &lt;code&gt;R&lt;/code&gt; packages: search for the current choices on the &lt;a href=&quot;http://www.rseek.org&quot; rel=&quot;nofollow&quot;&gt;RSeek page&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example, here is a scatterplot of simulated data, their cumulative average, and a Lowess smooth of the data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7Nna9.png&quot; alt=&quot;Cumulative average&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The green (upper) curve is the Lowess smooth.  (This coding uses 1 for success, 0 for failure, so that an improving curve slopes &lt;em&gt;upward&lt;/em&gt;.)  &lt;strong&gt;The two curves differ substantially&lt;/strong&gt;: the cumulative average barely has a positive slope (about 0.0015) and ends up around 0.5 at the most recent time, whereas the Lowess smooth has a slope around .005 (over three times as great) and ends up around 0.85.  To get a sense of which is correct, consider the most recent responses, say those after time=80: there are 26 of them averaging 0.77.  The Lowess curve is consistent with that level during this period but the cumulative average curve is clearly too low.&lt;/p&gt;&#10;&#10;&lt;p&gt;(This is a standard exploratory technique to prepare for logistic regression.  If, &lt;em&gt;on a &lt;a href=&quot;http://en.wikipedia.org/wiki/Logit&quot; rel=&quot;nofollow&quot;&gt;logit&lt;/a&gt; scale&lt;/em&gt;, the Lowess smooth is approximately linear, then &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt; of the response versus time should work well to model the learning curve.  If the smooth is not linear, you could consider logistic regression with &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/splines/html/00Index.html&quot; rel=&quot;nofollow&quot;&gt;cubic splines&lt;/a&gt;, allowing for changes in slope over time.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-19T05:21:11.340" Id="14501" LastActivityDate="2011-08-19T05:21:11.340" OwnerUserId="919" ParentId="14480" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;One other thing to keep in mind is that the tests on the individual coefficients each assume that all of the other predictors are in the model. In other words each predictor is not significant as long as all of the other predictors are in the model. There must be some interaction or interdependence between two or more of your predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;As someone else asked above - how did you diagnose a lack of multicollinearity?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-19T21:02:58.443" Id="14525" LastActivityDate="2011-08-19T21:02:58.443" OwnerUserId="118" ParentId="14500" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let $X$ be an $n \times l$ matrix, and $F$ an $n \times p$ matrix, with the rows of $X$ and $F$ drawn &lt;em&gt;i.i.d.&lt;/em&gt; from multivariate Gaussians. (The independence applies to rows: the $X$ and $F$ may be correlated.) Let $\Sigma_X, \Sigma_F, \Sigma_{XF}$ be the covariances of the two Gaussians and the covariance of the two Gaussians with each other. (It may be simpler to think of a single multivariate Gaussian which I have partitioned into two parts.) Given $p$-variate vector $v$, I would like to compute the distribution of&#10;$$v^{\top}(F^{\top}F)^{-1}F^{\top}X\left(X^{\top}X - X^{\top}F(F^{\top}F)^{-1}F^{\top}X\right)^{-1}X^{\top}F(F^{\top}F)^{-1}v$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am usually looking at the case where one of the columns of $F$ is all ones (which we can model as a Gaussian with a very small variance, if need be). In the case where $p = 1$ and $F$ is all ones, then we recover Hotelling's $T^2$ (up to scaling of $v$, which I ignore here). In fact I arrived at this monster by asking the question: given $l$-variate vector $w$, letting $y=Xw$, then regressing $y$ against $F$ to get $\hat{\beta}$, and computing the t-statistics associated with $v$ (&lt;em&gt;i.e.&lt;/em&gt; $(\hat{\beta}^{\top}v)/(\hat{\sigma}\sqrt{v^{\top}(F^{\top}F)^{-1}v})$), what is the maximal value of the t-statistic over all $w$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-20T03:43:14.757" Id="14536" LastActivityDate="2011-08-20T03:43:14.757" OwnerUserId="795" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;distributions&gt;&lt;t-distribution&gt;&lt;hotelling&gt;" Title="What is the distribution of this nearly-Hotelling statistic?" ViewCount="65" />
  
  
  
  
  <row AnswerCount="4" Body="&lt;p&gt;I'm currently on a project where I basically need, like we all do, to understand how output $y$ is related to input $x$. The particularity here is that data $(y,x)$ is given to me one piece at a time, so I want to update my analysis every time I receive a new $(y,x)$. I believe this is called &quot;on-line&quot; processing, as opposed to &quot;batch&quot; processing, where you have all the data needed and do your calculations using all data at the same time.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I've looked around for ideas, and I've finally came with the conclusion that the world is divided in three:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The first part is the land of statistics and econometrics. People there do OLS, GLS, instrument variables, ARIMA, tests, difference of differences, PCA and whatnot. This land is mostly dominated by linearity and does only &quot;batch&quot; processing.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The second part is the island of machine learning and other words like artificial intelligence, supervised and unsupervised learning, neural networks and SVMs. Both &quot;batch&quot; and &quot;on-line&quot; processing are done here.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The third part is a whole continent I've just discovered, mostly populated by electrical engineers, so it seems. There, people often add the word &quot;filter&quot; to their tools, and they invented great stuffs like the Widrow-Hoff algorithm, &lt;a href=&quot;http://en.wikipedia.org/wiki/Recursive_least_squares_filter&quot;&gt;Recursive least squares&lt;/a&gt;, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Wiener_filter&quot;&gt;Wiener filter&lt;/a&gt;, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kalman_filter&quot;&gt;Kalman filter&lt;/a&gt;, and probably other things I haven't discovered yet. Apparently they do mostly &quot;on-line&quot; processing as it better fits their needs.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So my question is, do you have a global vision on all this? I'm under the impression that these three parts of the world don't talk too much to each other. Am I wrong? Is there a Grand Unified Theory of Understanding How $Y$ Relates to $X$? Do you know any resources where the bases of that theory might be laid down?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure if this question really makes sense, but I'm a little lost between all those theories. I imagine the answer to the question &quot;should I use this or that?&quot; would be &quot;it depends on what you want to do (and on your data)&quot;. However I feel like those three worlds try to answer to the same question ($y=f(x)$?) and so it should be possible to have a higher view on all this, and deeply understand what makes each technique particular.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2011-08-22T17:22:44.733" CreationDate="2011-08-21T02:56:54.377" FavoriteCount="9" Id="14571" LastActivityDate="2013-09-16T08:31:10.763" LastEditDate="2013-09-16T08:31:10.763" LastEditorUserId="3277" OwnerUserId="4398" PostTypeId="1" Score="23" Tags="&lt;modeling&gt;&lt;model-selection&gt;" Title="Do you have a global vision on those analysis techniques?" ViewCount="437" />
  <row Body="&lt;p&gt;No.&lt;/p&gt;&#10;&#10;&lt;p&gt;However it seems you are using &lt;code&gt;nls&lt;/code&gt; in a wrong way -- it will never work with non-differentiable functions. Can't you just fit two linear models to both parts of the data (i.e. &lt;code&gt;data.frame(x,y)[g,]&lt;/code&gt; and &lt;code&gt;data.frame(x,y)[!g,]&lt;/code&gt;)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-21T08:14:54.420" Id="14575" LastActivityDate="2011-08-21T08:14:54.420" OwnerUserId="88" ParentId="14561" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The sm.density function picks a smoothing parameter for you, unless you specify one. It seems like it chose different smoothing parameters for the different variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Try setting h to some fixed value. e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sm.density(sim$v2, h = 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for each graph&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-21T11:49:59.157" Id="14579" LastActivityDate="2011-08-21T11:49:59.157" OwnerUserId="686" ParentId="14566" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="14597" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/124/statistical-classification-of-text/143#143&quot;&gt;Based on an answer on Cross Validated&lt;/a&gt; I've been looking into implementing a &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_forest&quot;&gt;random forest&lt;/a&gt; in .NET/C# to classify documents of text.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking around the web to see if there are existing implementations, I came across an algorithm for a &lt;a href=&quot;http://www.alglib.net/dataanalysis/decisionforest.php&quot;&gt;decision forest on Alglib&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The thing is, I can't seem to find anything specific to &quot;decision forest&quot; anywhere (it's ambiguous &lt;a href=&quot;http://stats.stackexchange.com/search?q=decision%20forest&quot;&gt;even here&lt;/a&gt;), it's typically mixed in with random forest.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, are the two the same, just being referenced differently, or are there inherent differences between the two?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-21T20:04:44.080" FavoriteCount="1" Id="14595" LastActivityDate="2011-08-21T21:45:39.363" OwnerUserId="740" PostTypeId="1" Score="6" Tags="&lt;random-forest&gt;" Title="Are decision forests and random forests the same thing?" ViewCount="1032" />
  
  
  <row Body="&lt;p&gt;Orthogonal contrasts have non-overlapping, complementary effects (in balanced design): whether the groups differ significantly or not in one contrast doesn't impact whether the groups differ significantly or not in another contrast, orthogonal to it. This is convenient to interpret.&lt;/p&gt;&#10;&#10;&lt;p&gt;Contrasts need not be orthogonal and often they are not. Actually, &quot;contrasts&quot; is a set of &quot;contrast variables&quot; to which a categorical predictor is being recoded (internally) by statistical procedure (such as GLM). Famous &lt;em&gt;dummy&lt;/em&gt; variables are an example of contast variables, and they don't make orthogonal contrasts: if you recode your categorical predictor consisting of equally sized k groups into k-1 set of dummy variables, they will correlate with each other. But if you recode to &lt;em&gt;Helmert&lt;/em&gt; variables (they constitute orthogonal contrasts) they won't correlate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-22T08:32:52.397" Id="14614" LastActivityDate="2011-08-22T08:51:16.767" LastEditDate="2011-08-22T08:51:16.767" LastEditorUserId="3277" OwnerUserId="3277" ParentId="14600" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="14630" AnswerCount="2" Body="&lt;p&gt;I have a graph that I did in SPSS &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FGTir.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to replicate this in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;The data file is here: &lt;a href=&quot;http://dl.dropbox.com/u/22681355/sendergraphR.csv&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/22681355/sendergraphR.csv&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE:&lt;/p&gt;&#10;&#10;&lt;p&gt;I've figured out how to to it for the other graph: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;graph2&amp;lt;-read.csv(file=&quot;SendergraphR.csv&quot;)&#10;ddr &amp;lt;- recast(graph2,no.GREEN+no.RED+Senderidentity~variable,&#10;    fun.aggregate=mean,id.var=c(&quot;no.GREEN&quot;,&quot;no.RED&quot;,&quot;Senderidentity&quot;))&#10;&#10;qplot(x=no.GREEN,y=Mean_Message,data=ddr,colour=Senderidentity, &#10;    group=Senderidentity,geom=&quot;line&quot;)+facet_wrap(~no.RED,ncol=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now what I would like to do is to separate this second graph into two columns one looking at the cases where the variable urn1 is blue and the other where its red: here's a graph of how it should look: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nX4vd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking of doing it the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ddr &amp;lt;- recast(graph2,no.GREEN+no.RED+Senderidentity+urn1~variable,&#10;    fun.aggregate=mean,id.var=c(&quot;no.GREEN&quot;,&quot;no.RED&quot;,&quot;Senderidentity&quot;, &quot;urn1&quot;))&#10;qplot(x=no.GREEN,y=Mean_Message,data=ddr,colour=Senderidentity, group=Senderidentity,geom=&quot;line&quot;)+facet_wrap(~no.RED,ncol=1) +&#10;    facet_wrap(~urn1, ncol=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But this doesn't seem to work. What am I doing wrong?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-08-22T10:29:27.160" Id="14625" LastActivityDate="2011-08-23T09:34:16.210" LastEditDate="2011-08-23T09:34:16.210" LastEditorUserId="88" OwnerUserId="5837" PostTypeId="1" Score="-2" Tags="&lt;r&gt;&lt;data-visualization&gt;" Title="How to replicate a plot of means for a 2 by 3 by 4 design in R?" ViewCount="283" />
  
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/14643/r-language-and-statistics-hypergeometric-test&quot;&gt;R language and statistics hypergeometric test&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;This is in reference to &lt;a href=&quot;http://stats.stackexchange.com/questions/14643/r-language-and-statistics-hypergeometric-test&quot;&gt;my first question&lt;/a&gt;, although I have got an answer for it from Spacedman. But I am not entirely satisfied with the answer as it does not give me any sort of value (p or z value). So I am re-framing my question and posting it again. No offences to Mr.Spacedman.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dictionary of say 61000 elements and out of this dictionary, I have two sets. Set A contains 23000 elements and Set B contains 15000 elements and an overlap of Set A and Set B gives 10000 elements. How can I estimate a p-value or z value to show that this overlap is significant and is not occuring by chance or vice versa. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" ClosedDate="2011-08-23T09:26:16.317" CommentCount="0" CreationDate="2011-08-23T08:21:18.813" FavoriteCount="0" Id="14686" LastActivityDate="2011-08-23T09:01:41.143" LastEditDate="2011-08-23T09:01:41.143" LastEditorUserId="183" OwnerUserId="5931" PostTypeId="1" Score="-1" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;hypothesis-testing&gt;" Title="R language-statistics-significance testing" ViewCount="83" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to compute inter-rater reliability for ranking multiple objects across multiple scenarios.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example suppose for scenario A we had multiple objects and multiple raters like in the table below.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;Object   V   W   X   Y   Z&#10;Rater1   2   1   3   4   5&#10;Rater2   1   3   5   2   4&#10;Rater3   4   5   2   1   3&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I only had this one table, I would just compute Kendall's W and be on my way.  But what happens if we have multiple scenarios (say B and C in addition to A)?  Suppose in each scenario the objects differ and the raters might differ (think Mechanical Turk data).  How might I compute reliability in ranking across all scenarios?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-23T09:15:16.340" Id="14691" LastActivityDate="2011-08-23T09:15:16.340" OwnerUserId="1503" PostTypeId="1" Score="1" Tags="&lt;reliability&gt;&lt;inter-rater&gt;" Title="Kendall's W for multiple objects and multiple surveys" ViewCount="253" />
  <row AnswerCount="4" Body="&lt;p&gt;I have some real data from the past. The data show application demand (for example cpu demand) at a certain time slot. The data looks like this for example: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;3,2,1,5,7,8,9,1,3,12,4,5&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;These 12 values show the application demand of cpu in first 12 hours of a day respectively: 3 was the demand between 00:00 and 01:00m, 2 the demand between 01:00 - 02:00 etc...&lt;/p&gt;&#10;&#10;&lt;p&gt;So what I have is a bunch of values and what I would like to do with this is a forecasting or estimation of application demand in the future. Say I have 120 values showing the demand of 10 days of 12 hours each. Based on these data, I want to estimate with which probability the demand values will be more or less than the previous ones. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I achieve this with R? I think this question is not directly related to R, but thought there are people with great knowledge in general who can give me some concrete ideas. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your precious time!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-23T13:11:08.733" FavoriteCount="2" Id="14705" LastActivityDate="2011-08-28T16:33:49.507" LastEditDate="2011-08-26T21:59:38.437" LastEditorUserId="919" OwnerDisplayName="Bob" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;time-series&gt;" Title="How to forecast CPU demand from a time series?" ViewCount="558" />
  
  
  <row Body="&lt;p&gt;It's not usually possible to identify a distribution from looking at a histogram like this.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a start, &lt;strong&gt;plot the density on a log scale&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yy3Bb.png&quot; alt=&quot;Log density plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The tail of this density (from around 40 onward) is close to linear, showing it is close to exponential.  That's part of the characterization.  To go further, &lt;em&gt;compare&lt;/em&gt; the density to this characterization by forming the residuals (on a log scale, effectively taking the &lt;em&gt;ratio&lt;/em&gt; of the density to an exponential curve):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3rAyO.png&quot; alt=&quot;Residuals&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly this density is &lt;em&gt;not&lt;/em&gt; exponential: for small values it is almost four times greater than the exponential fit to the tail would indicate.  We must go further with the characterization.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;We seek to characterize the residuals as simply as possible:&lt;/strong&gt; this means in terms of longish straight segments or parabolic sections.  (On this log scale, a straight segment is an exponential trend, whereas a parabolic section looks like a piece of a &lt;em&gt;Normal&lt;/em&gt; distribution.)  Evidently there are two parabolic-like sections: a sharp peaked one centered near 1 and a shallow, broad one centered near 25-30.  The first would correspond to a healthy part of a truncated Normal distribution with small standard deviation (around 5-6) whereas the second would correspond to most of a Normal distribution with a larger standard deviation (around 10 perhaps).  This indicates the density is not going to be adequately described by a simple mathematical formula, such as a Gamma or Weibull, but perhaps it can be decomposed into a &lt;em&gt;mixture&lt;/em&gt; of two or three components.  Look for each of those components to have some meaning: could these data indeed involve some combination of phenomena tending to occur near 1, near 25, and out beyond 40?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-23T18:46:15.443" Id="14718" LastActivityDate="2011-08-23T18:46:15.443" OwnerUserId="919" ParentId="13497" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I have typically seen the uniform prior used in either &quot;instructive&quot; type examples, or in cases in which truly nothing is known about a particular hyperparameter. Typically, I see uninformed priors that provide little information about what the solution will be, but which encode mathematically what a good solution probably looks like. For example, one typically sees a Gaussian prior ($\mu=0$) placed over a regression coefficient, encoding the knowledge that all things being equal, we prefer solutions in which the coefficients have lower magnitudes. This is to avoid overfitting a data set, by finding solutions that do maximize the objective function but which don't make sense in the particular context of our problem. In a sense, they provide a way to give the statistical model some &quot;clues&quot; about a particular domain.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, this isn't (in my opinion) the most important aspect of Bayesian methodologies. Bayesian methods are generative, in that they provide a complete &quot;story&quot; for how the data came into existence. Thus, they aren't simply pattern finders, but rather they are able to take into account the full reality of the situation at hand. For example, consider LDA (latent Dirichlet allocation), which provides a full generative story for how a text document comes to be, that goes something like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Select some mix of topics based on the likelihood of particular topics co-occurring; and &lt;/li&gt;&#10;&lt;li&gt;Select some set of words from the vocabulary, conditioned based on the selected topics.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thus, the model is fit based on a very specific understanding of the objects in the domain (here, text documents) and how they got created; therefore, the information we get back is tailored directly to our problem domain (likelihoods of words given topics, likelihoods of topics being mentioned together, likelihoods of documents containing topics and to what extent, etc.). The fact that Bayes Theorem is required to do this is almost secondary, hence the little joke, &quot;Bayes wouldn't be a Bayesian, and Christ wouldn't be a Christian.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, Bayesian models are all about rigorously modeling the domain objects using probability distributions; therefore, we are able to encode knowledge that wouldn't otherwise be available with a simple discriminative technique.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-23T21:26:34.607" Id="14725" LastActivityDate="2011-08-23T21:26:34.607" OwnerUserId="5832" ParentId="14721" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="14751" AnswerCount="1" Body="&lt;p&gt;attached is a (German...) bar chart which shows the five possible values of the one variable &quot;Culture&quot;: Nix, BV, Candida, Mix, Unspecific.&lt;br&gt;&#10;The variable is measured at three different times, 1 (blue), 2 (green), 3 (something).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for some kind of test to determine that the number of measured individual values increase, or decrease. For example, I'd like to show that over time (i.e. from time=1 to time=3), the value &quot;Nix&quot; is measured more often, whereas &quot;Candida&quot; and possibly &quot;Mix&quot; go down.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am a bit confused because we have three time points and five possible values, this makes it a bit unclear for me.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pjeBq.jpg&quot; alt=&quot;Bar chart&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-24T10:28:48.993" Id="14739" LastActivityDate="2011-08-24T12:46:25.183" OwnerUserId="2091" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;categorical-data&gt;" Title="How to test for increases in categorical variable values measured over time?" ViewCount="443" />
  <row AcceptedAnswerId="14769" AnswerCount="1" Body="&lt;p&gt;At the moment I'm trying to interpret the green and red dashed lines in a contour plot when visualizing a generalized additive model (GAM) with R. These two lines seem to be something like confidence bands, but I'm not sure how to interpret these dashed lines in a contour plot. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody have experiences with contour plots using R, specifically when fitting GAM?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-24T12:22:22.587" Id="14746" LastActivityDate="2011-08-24T18:55:26.800" LastEditDate="2011-08-24T18:55:26.800" LastEditorUserId="930" OwnerUserId="4496" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;nonparametric&gt;" Title="What does the dashed bounds mean when plotting a contour plot with R GAM?" ViewCount="359" />
  <row AnswerCount="2" Body="&lt;p&gt;This question is motivated &lt;a href=&quot;http://stats.stackexchange.com/questions/9859/what-are-the-disadvantages-of-the-profile-likelihood&quot;&gt;by this one&lt;/a&gt;. I looked up two sources and this is what I found. &lt;/p&gt;&#10;&#10;&lt;p&gt;A. van der Vaart, Assymptotic Statistics:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It is rarely possible to compute a profile likelihood explicitly, but&#10;  its numerical evaluation is often feasible. Then the profile&#10;  likelihood may serve to reduce the dimension of the likelihood&#10;  function. Profile likelihood functions are often used in the same way&#10;  as (ordinary) likelihood functions of parametric models. Apart from&#10;  taking their points of maximum as estimators $\hat\theta$, the second&#10;  derivative at $\hat\theta$ is used as an estimate of minus the inverse&#10;  of the asymptotic covariance matrix of e. Recent research appears to&#10;  validate this practice.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;J. Wooldridge, Econometric Analysis of Cross Section and Panel Data (the same in both editions):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;As a device for studying asymptotic properties, the concentrated&#10;  objective function is of limited value because $g(W,\beta)$ generally&#10;  depends on all of $W$, in which case the objective function cannot be&#10;  written as the sum of independent, identically distributed summands.&#10;  One setting where equation (12.89) is a sum of i.i.d. functions occurs&#10;  when we concentrate out individual-specific effects from certain&#10;  nonlinear panel data models. In addition, the concentrated objective&#10;  function can be useful for establishing the equivalence of seemingly&#10;  different estimation approaches.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Wooldridge discusses the problem in wider context of M-estimators, so it applies to maximum likelihood estimators also.&lt;/p&gt;&#10;&#10;&lt;p&gt;So we get two different answers for the same question. The devil in my opinion is in the details. For some models we can use hessian of profile likelihood safely for some models not. Are there any general results which give conditions when can we do that (or cannot)?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-08-24T12:59:41.197" FavoriteCount="2" Id="14752" LastActivityDate="2013-06-14T01:08:33.723" OwnerUserId="2116" PostTypeId="1" Score="13" Tags="&lt;maximum-likelihood&gt;&lt;profile-likelihood&gt;" Title="Hessian of profile likelihood used for standard error estimation" ViewCount="646" />
  
  
  
  <row Body="&lt;p&gt;Using the terminology of &lt;a href=&quot;http://en.wikipedia.org/wiki/Likelihood_function#Profile_likelihood&quot; rel=&quot;nofollow&quot;&gt;Profile Likelihood&lt;/a&gt; from the maximum likelihood estimation theory, your way of finding the $f$ may be called &lt;strong&gt;profile posterior distribution&lt;/strong&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-24T18:06:56.847" Id="14766" LastActivityDate="2011-08-24T18:06:56.847" OwnerUserId="1307" ParentId="14713" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I'll use the example in the linked help page with dependent variable &lt;code&gt;y&lt;/code&gt; and factors &lt;code&gt;g1&lt;/code&gt;, &lt;code&gt;g2&lt;/code&gt;, &lt;code&gt;g3&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the &lt;code&gt;model&lt;/code&gt; option is set to &lt;code&gt;linear&lt;/code&gt;, no interaction terms are considered in the full model. For type III sum of squares (default in &lt;code&gt;anovan&lt;/code&gt;), the full model is always &lt;code&gt;y ~ g1+g2+g3&lt;/code&gt;. The restricted models for the model comparisons for the 3 tests of main effects are as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;g1&lt;/code&gt;: restricted model &lt;code&gt;y ~ g2+g3&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;g2&lt;/code&gt;: restricted model &lt;code&gt;y ~ g1+g3&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;g3&lt;/code&gt;: restricted model &lt;code&gt;y ~ g1+g2&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;When the &lt;code&gt;model&lt;/code&gt; option is set to &lt;code&gt;interaction&lt;/code&gt;, pairwise interactions are considered for the full model. For type III sum of squares, the full model is now always &lt;code&gt;y ~ g1+g2+g3 + g1:g2 + g1:g3 + g2:g3&lt;/code&gt;. The restricted models for the model comparisons for the 3 tests of main effects now are as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;g1&lt;/code&gt;: restricted model &lt;code&gt;y ~ g2 + g3 + g1:g2 + g1:g3 + g2:g3&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;g2&lt;/code&gt;: restricted model &lt;code&gt;y ~ g1 + g3 + g1:g2 + g1:g3 + g2:g3&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;g3&lt;/code&gt;: restricted model &lt;code&gt;y ~ g1 + g2 + g1:g2 + g1:g3 + g2:g3&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The ANOVA F-test is based on the difference in error sum of squares of the full and restricted models, relative to the error sum of squares of the full model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you have different model comparisons for the main effects in the two cases, their p-values differ. This should not be the case when you use type II or type I sum of squares, since the inclusion of interactions does not change their choices for full and restricted models for the tests of main effects. So which set of comparisons should you choose? I'm afraid this has to follow from theoretical considerations, i.e., which comparisons actually test the hypotheses you have?&lt;/p&gt;&#10;&#10;&lt;p&gt;The help page example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y  &amp;lt;- c(52.7, 57.5, 45.9, 44.5, 53.0, 57.0, 45.9, 44.0)&#10;g1 &amp;lt;- factor(c(1, 2, 1, 2, 1, 2, 1, 2))&#10;g2 &amp;lt;- factor(c('hi', 'hi', 'lo', 'lo', 'hi', 'hi', 'lo', 'lo'))&#10;g3 &amp;lt;- factor(c('may', 'may', 'may', 'may', 'june', 'june', 'june', 'june'))&#10;&#10;# don't consider interactions to replicate MATLAB's model='linear' option&#10;# model comparisons for tests of main effects with SS type III&#10;anova(lm(y ~      g2 + g3), lm(y ~ g1 + g2 + g3))&#10;anova(lm(y ~ g1      + g3), lm(y ~ g1 + g2 + g3))&#10;anova(lm(y ~ g1 + g2),      lm(y ~ g1 + g2 + g3))&#10;&#10;# consider pairwise interactions, test main effects and interactions with SS type III&#10;# replicate MATLAB's model='interaction' option, switch to effect-coding first&#10;options(contrasts=c(unordered=&quot;contr.sum&quot;, ordered=&quot;contr.poly&quot;))&#10;drop1(lm(y ~ g1 + g2 + g3 + g1:g2 + g1:g3 + g2:g3), ~ ., test=&quot;F&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-08-24T18:32:11.517" Id="14768" LastActivityDate="2011-08-24T19:16:39.860" LastEditDate="2011-08-24T19:16:39.860" LastEditorUserId="1909" OwnerUserId="1909" ParentId="14765" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a data set of 11,000+ distinct items, each of which was classified on a nominal scale by at least 3 different raters on Amazon's &lt;a href=&quot;http://mturk.com&quot; rel=&quot;nofollow&quot;&gt;Mechanical Turk&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;88 different raters provided judgments for the task, and no one rater completed more about 800 judgments. Most provided significantly fewer than that.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is this:&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to calculate some measure of inter-rater reliability for the ratings, something better than a simply looking at consensus. I believe, however, that Fleiss Kappa, which is the measure I know best, would require a consistent group of raters for the entire set of items, and so I cannot use Fleiss Kappa to check IRR with my data. Is this correct? Is there another method I could use?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice would be much appreciated!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-24T22:23:37.280" FavoriteCount="1" Id="14781" LastActivityDate="2015-01-13T22:06:47.343" LastEditDate="2011-08-25T09:05:40.060" LastEditorUserId="88" OwnerUserId="5972" PostTypeId="1" Score="5" Tags="&lt;reliability&gt;&lt;inter-rater&gt;&lt;kappa&gt;" Title="Inter-rater reliability with many non-overlapping raters" ViewCount="318" />
&#10;    = &amp;amp;\frac{1/(n+1)(3(2n-2)/(2n+1)^2  + 2(2n-1)/(2n+1)^2)}{1/(2n+1)(2(n-1)/(n+1)^2 + n/(n+1)^2)} \\
  <row Body="&lt;p&gt;This was done some 13 years ago by &lt;a href=&quot;http://www.jstor.org/stable/2669832&quot;&gt;Chapman, George and McCulloch (1998, JASA)&lt;/a&gt;. Of course there's been huge literature on Bayesian regression trees that grew out of this idea.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-24T22:40:17.643" Id="14783" LastActivityDate="2011-08-24T22:40:17.643" OwnerUserId="5739" ParentId="14770" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;FWIW after 2 months, the survival method you describe does seem like a productive approach.  Not sure I'm qualified to give it the mantle of being &quot;correct.&quot;  And, just as with skewed predictors we often try transforming via a square root, a log, and so on in order to find the strongest relationship with an outcome, it makes sense that you would try each of your ideas about overdrafts.  It's interesting:  on the one hand, a variable like &quot;...last 7 days&quot; might be too narrowly distributed to predict well (might involve overly rare events), but on the other, perhaps that narrowness will be overriden by its &quot;a propos-ness.&quot;  Just make sure you test the different versions separately so that they don't cannibalize one another.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-24T23:34:16.883" Id="14787" LastActivityDate="2011-08-24T23:34:16.883" OwnerUserId="2669" ParentId="11906" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I've got a supervised learning problem where I've got about 15 features, mostly numeric, and I'm mapping these to a set of 5-7classes. Using decision trees or forests, I'm able to get confusion matrices that I'm quite satisfied with, but I'm interested in seeing what the &quot;top&quot; rules RIPPER would find would be.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been attempting to train on a small subset of the data, about 2 million rows (Again 15 features), but I find myself unable to actually generate a model via JRip as I always run into memory constraints after 6-7 hours of Weka's JRip churning. (Even on large EC2 instances)&lt;/p&gt;&#10;&#10;&lt;p&gt;Has anyone had success using RIPPER on large-ish data sets? Would I find more success 'bucketing' my numeric attributes first? Are there any important preprocessing steps I'm missing?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-25T01:41:50.997" Id="14792" LastActivityDate="2011-08-25T09:02:05.993" LastEditDate="2011-08-25T09:02:05.993" LastEditorUserId="88" OwnerUserId="5974" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;weka&gt;" Title="Ripper algorithm on large data sets" ViewCount="774" />
  <row AcceptedAnswerId="14811" AnswerCount="2" Body="&lt;p&gt;I have run 3 separate logistic regressions and would somehow like to summarize how well the model fits the data graphically.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-08-25T14:16:20.770" FavoriteCount="5" Id="14803" LastActivityDate="2011-09-05T20:21:43.877" LastEditDate="2011-09-05T20:21:43.877" LastEditorUserId="88" OwnerUserId="5837" PostTypeId="1" Score="6" Tags="&lt;data-visualization&gt;&lt;logistic&gt;" Title="Presenting logistic model fit graphically " ViewCount="1215" />
&#10;X^2 = \sum_{i=1}^k \frac{(S_i - n \pi_i)^2}{n\pi_i} \&amp;gt; .
&#10;(U_1,\ldots,U_k)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $S_i$ is $\mathrm{Bin}(n,\pi_i)$, then by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem#Classical_CLT&quot;&gt;Central Limit Theorem&lt;/a&gt;, &#10;$$
&#10;\newcommand{\sqpi}{\sqrt{\boldsymbol{\pi}}}
  <row AnswerCount="1" Body="&lt;p&gt;I have two variables both are binary. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to see how well one predicts the other. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to do this in logistic regression? variables are coded as 0 and 1 but 1 does not indicate that something is  'correct'. can I still use logistic regression to see how well it is predicted?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-25T20:18:07.113" Id="14820" LastActivityDate="2011-08-25T21:35:10.933" OwnerUserId="5837" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;" Title="Predicting binary values with binary independent variables in logistic regression" ViewCount="243" />
&#10;\frac{\partial \left[h(1-h)\right]^2W^2}{\partial W}
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And I'm having difficulty in interpreting this. I am a bit confused about whether h(1-h) is then an dot product. If so, does that just give me a scalar multiplied by W? If not, and it's an element-wise multiplication, then I think the dimensionality would be all wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or perhaps I did all of this incorrectly. Any help would be greatly appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-25T22:33:12.010" FavoriteCount="3" Id="14827" LastActivityDate="2011-08-30T08:52:14.687" LastEditDate="2011-08-26T00:18:57.297" LastEditorUserId="2970" OwnerUserId="5691" PostTypeId="1" Score="2" Tags="&lt;neural-networks&gt;&lt;regularization&gt;&lt;mathematics&gt;&lt;checking&gt;" Title="How to calculate derivative of the contractive auto-encoder regularization term?" ViewCount="1150" />
  
  <row AcceptedAnswerId="14840" AnswerCount="3" Body="&lt;p&gt;Which is more correct? &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&quot;Accept the null hypothesis&quot;; or&lt;/li&gt;&#10;&lt;li&gt;&quot;Do not reject the null hypothesis&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is there truly a separate meaning that I'm missing here?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-08-26T00:01:58.947" FavoriteCount="1" Id="14830" LastActivityDate="2011-08-27T07:13:01.467" LastEditDate="2011-08-26T09:39:51.223" LastEditorUserId="88" OwnerUserId="776" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;" Title="Hypothesis testing terminology surrounding null" ViewCount="251" />
  <row Body="&lt;p&gt;You could use Cohen's &lt;strong&gt;kappa&lt;/strong&gt; test for the 3 x 3 classification table (actual membership by clustering membership).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-26T08:05:44.657" Id="14841" LastActivityDate="2011-08-26T08:05:44.657" OwnerUserId="3277" ParentId="14831" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="15071" AnswerCount="1" Body="&lt;p&gt;I am looking at using the lasso as a method for selecting features and fitting a predictive model with a binary target.  Below is some code I was playing with to try out the method with regularized logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is I get a group of &quot;significant&quot; variables but am I able to rank order these to estimate relative importance of each? Can the coefficients be standardized for this purpose of rank by absolute value (I understand that they are shown on the original variable scale through the &lt;code&gt;coef&lt;/code&gt; function)? If so, how to do so (using the standard deviation of x and y) &lt;a href=&quot;http://www.nd.edu/~rwilliam/stats1/x92.pdf&quot;&gt;Standardize Regression Coefficients&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;SAMPLE CODE:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    library(glmnet)&#10;&#10;    #data comes from&#10;&#10;#http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)&#10;&#10;    datasetTest &amp;lt;- read.csv('C:/Documents and Settings/E997608/Desktop/wdbc.data.txt',head=FALSE)&#10;&#10;&#10;#appears to use the first level as the target success&#10;   datasetTest$V2&amp;lt;-as.factor(ifelse(as.character(datasetTest$V2)==&quot;M&quot;,&quot;0&quot;,&quot;1&quot;))&#10;&#10;&#10;#cross validation to find optimal lambda&#10;#using the lasso because alpha=1&#10;&#10;    cv.result&amp;lt;-cv.glmnet(       &#10;              x=as.matrix(dataset[,3:ncol(datasetTest)]),&#10;              y=datasetTest[,2],        &#10;              family=&quot;binomial&quot;,        &#10;              nfolds=10,        &#10;              type.measure=&quot;deviance&quot;,       &#10;              alpha=1      &#10;              )&#10;&#10;#values of lambda used&#10;&#10;    histogram(cv.result$lambda)&#10;&#10;#plot of the error measure (here was deviance)&#10;#as a CI from each of the 10 folds&#10;#for each value of lambda (log actually)&#10;&#10;    plot(cv.result) &#10;&#10;#the mean cross validation error (one for each of the&#10;#100 values of lambda&#10;&#10;    cv.result$cvm&#10;&#10;#the value of lambda that minimzes the error measure&#10;#result: 0.001909601&#10;&#10;    cv.result$lambda.min&#10;    log(cv.result$lambda.min)&#10;&#10;#the value of lambda that minimzes the error measure&#10;#within 1 SE of the minimum&#10;#result: 0.007024236&#10;&#10;    cv.result$lambda.1se&#10;&#10;#the full sequence was fit in the object called cv.result$glmnet.fit&#10;#this is same as a call to it directly.&#10;#here are the coefficients from the min lambda&#10;&#10;    coef(cv.result$glmnet.fit,s=cv.result$lambda.1se)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-08-26T13:16:29.653" FavoriteCount="8" Id="14853" LastActivityDate="2011-09-01T19:13:11.293" LastEditDate="2011-08-26T13:33:45.913" LastEditorUserId="88" OwnerUserId="2040" PostTypeId="1" Score="8" Tags="&lt;logistic&gt;&lt;importance&gt;&lt;glmnet&gt;" Title="Variable importance from GLMNET " ViewCount="3920" />
  <row AcceptedAnswerId="14860" AnswerCount="5" Body="&lt;p&gt;I am looking to do classification on my text data. I have &lt;code&gt;300 classes&lt;/code&gt;, 200 training documents per class (so &lt;code&gt;60000 documents in total&lt;/code&gt;) and this is likely to result in &lt;strong&gt;very&lt;/strong&gt; high dimensional data (we may be looking in excess of &lt;strong&gt;1million dimensions&lt;/strong&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to perform the following steps in the pipeline (just to give you a sense of what my requirements are):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Converting each document to feature vector (&lt;code&gt;tf-idf&lt;/code&gt; or &lt;code&gt;vector space model&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Feature selection&lt;/code&gt; (&lt;code&gt;Mutual Information&lt;/code&gt; based preferably, or any other standard ones) &lt;/li&gt;&#10;&lt;li&gt;Training the classifier (&lt;code&gt;SVM&lt;/code&gt;, &lt;code&gt;Naive Bayes&lt;/code&gt;, &lt;code&gt;Logistic Regression&lt;/code&gt; or &lt;code&gt;Random Forest&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;Predicting unseen data based on the classifier model trained. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So the question is what tools/framework do I use for handling such high dimensional data? I am aware of the usual suspects (R, WEKA...) but as far as my knowledge goes (I may be wrong) possibly none of them can handle data this large. Is there any other off the shelf tool that I could look at?&lt;/p&gt;&#10;&#10;&lt;p&gt;If I have to parallelize it, should I be looking at &lt;a href=&quot;http://mahout.apache.org/&quot;&gt;Apache Mahout&lt;/a&gt;? Looks like it may not quite yet provide the functionality I require.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks to all in advance.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Update: I looked around &lt;a href=&quot;http://stats.stackexchange.com/questions/14219/how-well-does-r-scale-to-text-classification-tasks&quot;&gt;this website&lt;/a&gt;, R mailing list and the internet in general. It appears to me that the following problems could emerge in my situation:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Preprocessing of my data using R (&lt;a href=&quot;http://cran.r-project.org/package=tm&quot;&gt;tm&lt;/a&gt; package in particular) could be &lt;a href=&quot;http://www.michaelbommarito.com/blog/2011/02/16/pre-processing-text-rtm-vs-pythonnltk/&quot;&gt;impractical&lt;/a&gt;, since &lt;code&gt;tm&lt;/code&gt; will be prohibitively slow. &lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Since I will need to use an ensemble of R packages (pre-processing, sparse matrices, classifiers etc.) interoperability between the packages could become a problem, and I may incur an additional overhead in converting data from one format to another. For example, if I do my pre-processing using &lt;code&gt;tm&lt;/code&gt; (or an external tool like WEKA)  I will need to figure out a way to convert this data into a form that the HPC libraries in R can read. And again it is not clear to me if the classifier packages would directly take in the data as provided by the HPC libraries. &lt;/p&gt;&#10;&#10;&lt;p&gt;Am I on the right track? And more importantly, am I making sense :-) ?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;" CommentCount="9" CreationDate="2011-08-26T16:08:13.640" FavoriteCount="11" Id="14856" LastActivityDate="2014-10-03T21:24:04.167" LastEditDate="2011-08-28T18:29:43.467" LastEditorUserId="6003" OwnerUserId="6003" PostTypeId="1" Score="14" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" Title="Large scale text classification" ViewCount="4537" />
  <row Body="&lt;p&gt;&lt;strong&gt;A simple method&lt;/strong&gt;, useful both for exploration and hypothesis testing, takes the &lt;a href=&quot;http://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic&quot;&gt;Durbin Watson statistic&lt;/a&gt; and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variogram&quot;&gt;semivariogram&lt;/a&gt; as its point of departure: denoting the sequence of residuals by $e_t$ (with $t$ the &quot;index&quot; of the plot), &lt;strong&gt;compute&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\gamma(t) = \frac{1}{2}(e_{t+1}-e_t)^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The data (as presented in the question) result in this plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4a0Fh.png&quot; alt=&quot;Residual volatility plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(The residual values have been uniformly rescaled compared to what is shown on the vertical axis in the original plot.  This does not affect the subsequent analysis.)  Typically there will be scatter, as shown here.  To visualize this better, &lt;strong&gt;smooth it&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/TbYJU.png&quot; alt=&quot;Smoothed residual volatility&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This plot is on a square root scale (let's call this the &quot;dispersion&quot;) to dampen the extreme oscillations.  The wiggly blue lines are a modest smooth (medians of 3 followed by a lag-13 moving average).  The solid red line is an aggressive smooth of that. (Here, it's a Gaussian convolution; in general, though, I would recommend a Lowess smooth of $\sqrt{\gamma(t)}$.) Together, these lines trace out detailed fluctuations in dispersion and intermediate-range trends, respectively.  The yellow and green thresholds, shown for reference, delimit (a) the lowest smoothed dispersion in the first 225 indexes and (b) the mean smoothed dispersion from index 226 onwards.  I chose the changepoint of 225 by inspection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly almost all the smoothed dispersion values above 6 occurred during the first 225 indexes.  From index=225 to index=350 or so, the smoothed dispersions decrease and then stay constant around 2.7.&lt;/p&gt;&#10;&#10;&lt;p&gt;The change in dispersion is now visually obvious.  For most purposes--where the change is so clear and strong--that's all one needs.  But &lt;strong&gt;this approach lends itself to formal testing, too&lt;/strong&gt;.  To identify a point where the dispersion changed, and to estimate the uncertainty of that point, apply a sequential online &lt;a href=&quot;http://stats.stackexchange.com/search?q=changepoint&quot;&gt;changepoint procedure&lt;/a&gt; to the sequence $(\gamma(t))$.  An even more straightforward check, when you suspect there has been just one major change in volatility, is to use regression to check the dispersion for a trend.  If one exists, you can reject the null hypothesis of homogeneous volatility.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2011-08-26T19:42:26.600" Id="14867" LastActivityDate="2011-08-26T19:59:54.573" LastEditDate="2011-08-26T19:59:54.573" LastEditorUserId="919" OwnerUserId="919" ParentId="14842" PostTypeId="2" Score="7" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am reading Ruppert's &lt;a href=&quot;http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4419-7786-1&quot; rel=&quot;nofollow&quot;&gt;Statistics and Data Analysis for Financial Engineering&lt;/a&gt;, which contains the following theorem:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $Y_1$, $...$ , $Y_n$ be an i.i.d. sample with a CDF $F$. Suppose&#10;  that $F\;$ has a density $f\;$ that is continuous and positive at&#10;  $F^{-1}(q)$, 0 &amp;lt; $q$ &amp;lt; 1. Then for large $n\;$, the $q^{th}$ sample&#10;  quantile is approximately normally distributed with mean equal to the&#10;  population quantile $F^{-1}(q)$ and variance equal to:&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;$$
&#10;\frac{q(1-q)}{n \left[f\;\left(F^{-1}(q)\right)\right]^2}
&#10;\frac{\pi\sigma^2}{2n}
  <row Body="&lt;p&gt;:Dail To test for non-constant variance one must understand the hypothesis behind the popular statistical tests. you need to follow the recipe i,e, the tests that I outlined in &lt;a href=&quot;http://stats.stackexchange.com/questions/14842/how-to-check-if-the-volatility-is-stationary&quot;&gt;How to check if the volatility is stationary?&lt;/a&gt;&#10;to fully verify that a series can't be proven to have non-constant variance. All six of the tests that I outlined must yield acceptance of the null hypothesis of non-constant variance. Rejection by any one of the 6 tests suggests that the error variance is indeed non-constant.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-28T01:29:00.887" Id="14897" LastActivityDate="2011-08-28T01:29:00.887" OwnerUserId="3382" ParentId="14872" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to predict the future performance of customers on one of the websites I run. We provide practice tests leading up to a final exam, taken elsewhere. Based on the user's answer to multiple questions (only counting the first time they see the question), I am hoping to predict if they will pass their final exam.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is part of my dataset for past customer performance&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Set A       Set B&#10;Q   Cor Tot Cor Tot&#10;1   16  34  6   12&#10;3   33  37  11  12&#10;4   41  43  9   11&#10;6   23  31  7   10&#10;7   18  29  5   7&#10;9   32  34  10  10&#10;10  19  33  6   8&#10;11  14  29  5   10&#10;12  34  38  9   10&#10;13  15  25  5   7&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Q&lt;/code&gt; is a question identifier, Set A contains the customers who passed the final exam on their first try, Set B contains the customers who did not pass the final exam on their first try.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Cor&lt;/code&gt; indicates how many customers answered that question correctly on their first try. &lt;code&gt;Tot&lt;/code&gt; indicates how many customers have tried answering that question.&lt;/p&gt;&#10;&#10;&lt;p&gt;In total, set A contains 620 customers, set B contains 162 customers.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Now, assume that our customer Alice whose performance we want to predict answers the questions as follows (&lt;code&gt;1&lt;/code&gt; is correct, &lt;code&gt;0&lt;/code&gt; is wrong):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Q   A&#10;1   1&#10;3   1&#10;4   0&#10;6   1&#10;7   1&#10;9   1&#10;10  0&#10;11  1&#10;12  1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I am assuming this fits well into a naive Bayes classifier:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(Pass) = 620 / (162 + 620) \approx 0.7928388747
  
  <row Body="&lt;p&gt;Partial transparency (&quot;alpha&quot;) might help you here, e.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; temp11=runif(100)&#10;&amp;gt; temp22=runif(100)&#10;&amp;gt; plot(temp11, type=&quot;p&quot;, col=rgb(0.2, 0.2, 1, 0.6), pch=19, xlab=&quot;Time&quot;, ylab=&quot;price&quot;)&#10;&amp;gt; lines(temp11, lwd=3, col=rgb(0.2, 0.2, 1, 0.3))&#10;&amp;gt; points(temp22, pch=19, col=rgb(1, 0.2, 0.2, 0.6))&#10;&amp;gt; lines(temp22, lwd=3, col=rgb(1, 0.2, 0.2, 0.3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is an example of the image:&#10;&lt;img src=&quot;http://i.stack.imgur.com/DjRnc.png&quot; alt=&quot;http://i.stack.imgur.com/PUdkm.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It doesn't look that great with runif data, but on your data I think it would work a bit better.  Alpha support is device-dependent, but PDF does support it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-08-28T20:17:55.340" Id="14918" LastActivityDate="2011-08-29T00:27:03.437" LastEditDate="2011-08-29T00:27:03.437" LastEditorUserId="6028" OwnerUserId="6028" ParentId="14898" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;There are probably &lt;strong&gt;many&lt;/strong&gt; ways to do this but the first one that comes to mind is based on linear regression. You can regress the consecutive residuals against each other and test for a significant slope. If there is auto-correlation, then there should be a linear relationship between consecutive residuals. To finish the code you've written, you could do: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod = lm(prices[,1] ~ prices[,2])&#10;res = mod$res &#10;n = length(res) &#10;mod2 = lm(res[-n] ~ res[-1]) &#10;summary(mod2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;mod2 is a linear regression of the time $t$ error, $\varepsilon_{t}$, against the time $t-1$ error, $\varepsilon_{t-1}$. if the coefficient for res[-1] is significant, you have evidence of autocorrelation in the residuals. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This implicitly assumes that the residuals are autoregressive in the sense that only $\varepsilon_{t-1}$ is important when predicting $\varepsilon_{t}$. In reality there could be longer range dependencies. In that case, this method I've described should be interpreted as the one-lag autoregressive approximation to the true autocorrelation structure in $\varepsilon$. &lt;/p&gt;&#10;" CommentCount="12" CreationDate="2011-08-28T21:22:44.593" Id="14920" LastActivityDate="2011-08-29T10:28:15.783" LastEditDate="2011-08-29T10:28:15.783" LastEditorUserId="4856" OwnerUserId="4856" ParentId="14914" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;Use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic&quot;&gt;Durbin-Watson test&lt;/a&gt;, implemented in the &lt;a href=&quot;http://cran.r-project.org/web/packages/lmtest/&quot;&gt;lmtest&lt;/a&gt; package.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dwtest(prices[,1] ~ prices[,2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="9" CreationDate="2011-08-29T00:18:43.377" Id="14923" LastActivityDate="2011-08-29T00:18:43.377" OwnerUserId="159" ParentId="14914" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;The DW Test or the Linear Regression test are not robust to anomalies in the data. If you have Pulses, Seasonal Pulses , Level Shifts or Local Time Trends these tests are useless as these untreated components inflate the variance of the errors thus downward biasing the tests causing you ( as you have found out ) to incorrectly accept the null hypothesis of no auto-correlation. Before these two tests or any other parametric test that I am aware of can be used one has to &quot;prove&quot; that the mean of the residuals is not statistically significantly different from 0.0 EVERYWHERE otherwise the underlying assumptions are invalid. It is well known that one of the constraints of the DW test is its assumption that the regression errors are normally distributed. Note normally distributed means among other things : No anomalies ( see &lt;a href=&quot;http://homepage.newschool.edu/~canjels/permdw12.pdf&quot;&gt;http://homepage.newschool.edu/~canjels/permdw12.pdf&lt;/a&gt; ). Additionally the DW test only test for auto-correlation of lag 1. Your data might have a weekly/seasonal effect and this would go undiagnosed and furthermore , untreated , would downward bias the DW test.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-08-29T00:32:31.903" Id="14924" LastActivityDate="2011-08-29T00:59:30.347" LastEditDate="2011-08-29T00:59:30.347" LastEditorUserId="3382" OwnerUserId="3382" ParentId="14914" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;wrt your first question: this depends on your software of choice. There are really two types of p-values that are used frequently in these scenarios, both typically based upon likelihood ratio tests (there are others but these are typically equivalent or at least differ little in their results).&lt;/p&gt;&#10;&#10;&lt;p&gt;It is important to realize that all of these p-values are &lt;em&gt;conditional&lt;/em&gt; on (part of) the rest of the parameters. That means: &lt;em&gt;Assuming&lt;/em&gt; (some of) the other parameter estimates are correct,  you test whether or not the coefficient for a parameter is zero. Typically, the null hypothesis for these tests is that the coefficient is zero, so if you have a small p-value, it means (conditionally on the value of the other coefficients) that the coefficient itself is unlikely to be zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;Type I tests test for the zeroness of each coefficient conditionally on the value of the coefficients that come before it in the model (left to right). Type III tests (marginal tests), test for the zeroness of each coefficient conditional on the value of all other coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;Different tools present different p-values as the default, although typically you have ways of obtaining both. If you don't have a reason outside of statistics to include the parameters in some order, you will generally be interested in the type III test results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally (relating more to your last question), with a likelihood ratio test you can always create a test for any set of coefficients conditional on the rest. This is the way to go if you want to test for multiple coefficients being zero at the same time (otherwise you run into some nasty multiple testing issues).&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-08-29T09:13:51.233" Id="14935" LastActivityDate="2011-08-29T09:13:51.233" OwnerUserId="4257" ParentId="14928" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I've seen liblinear runtimes very sensitive to tol; try tol=.1,&#10;and if possible linear not rbf. How many classes do you have ?&#10;How much memory do you have ? Monitor real / virtual with &quot;top&quot; or the like.&lt;/p&gt;&#10;&#10;&lt;p&gt;Stochastic gradient descent,&#10;&lt;a href=&quot;http://scikit-learn.sourceforge.net/modules/sgd.html&quot;&gt;SGDClassifier&lt;/a&gt;&#10;in scikits.learn is &lt;em&gt;fast&lt;/em&gt;.&#10;For example, on Mnist handwritten digit data, 10k rows x 768 features,&#10;80 % of the raw data 0, -= mean and /= std: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 12 sec  sgd        mnist28 (10000, 784)  tol 0.1  C 1  penalty l2  correct 89.6 %&#10;321 sec  LinearSVC  mnist28 (10000, 784)  tol 0.1  C 1  penalty l2  correct 86.6 %&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is with no tuning nor cross-validation; your mileage will vary.&lt;/p&gt;&#10;&#10;&lt;p&gt;Added: see also &lt;a href=&quot;http://code.google.com/p/sofia-ml&quot;&gt;Sofia-ml&lt;/a&gt; -- comments anyone ?&lt;/p&gt;&#10;&#10;&lt;p&gt;And please post what worked / what didn't.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-29T09:35:13.957" Id="14936" LastActivityDate="2011-08-29T15:06:15.220" LastEditDate="2011-08-29T15:06:15.220" LastEditorUserId="557" OwnerUserId="557" ParentId="14893" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="14971" AnswerCount="1" Body="&lt;p&gt;I need to calculate matrix inverse and have been using &lt;code&gt;solve&lt;/code&gt; function. While it works well on small matrices, &lt;code&gt;solve&lt;/code&gt; tends to be very slow on large matrices. I was wondering if there is any other function or combination of functions (through SVD, QR, LU, or other decomposition functions) that can give me faster results.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-08-29T18:15:05.933" FavoriteCount="5" Id="14951" LastActivityDate="2011-08-30T12:37:13.170" LastEditDate="2011-08-29T19:19:39.337" LastEditorUserId="930" OwnerUserId="1261" PostTypeId="1" Score="9" Tags="&lt;r&gt;&lt;matrix-decomposition&gt;&lt;matrix-inverse&gt;" Title="Efficient calculation of matrix inverse in R" ViewCount="7985" />
  
&#10;              &amp;amp; \sigma^2_2                  &amp;amp; \rho_{23} \sigma_2 \sigma_3 &amp;amp;                      &amp;amp; \phi_{22} \sigma^2_2        &amp;amp; \phi_{23} \sigma_2 \sigma_3 \\
  
  
  
&#10;\frac{1}{P}\sum_{w=1}^W\sum_{f=1}^{F_w}\sum_{c=1}^{C_{wf}}\sum_{p=1}^{P_{wfc}} x_{wfcp}
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, these two expressions will not be equal.  Plus there are several variations in-between.  By my count, there are 8 ways of doing this (including the two above); I've listed all in their full glory at the end of this post.  For example, one could compute this (number 6 in the list below):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;&amp;amp;&amp;amp; \mathrm{where} &amp;amp;&amp;amp;
&#10;F = \sum_{w=1}^W \sum_{f=1}^{F_w} \; 1 \, , \,
&#10;4. \;\; \frac{1}{C}\sum_{w=1}^W
&#10;\left[\frac{1}{P_{wfc}} \sum_{p=1}^{P_{wfc}} x_{wfcp}\right]
&#10;
&#10;\left[\frac{1}{C_w} \sum_{f=1}^{F_w} \sum_{c=1}^{C_{wf}}
&#10;&amp;amp;&amp;amp; \mathrm{where} &amp;amp;&amp;amp; C_w = \sum_{f=1}^{F_w} \sum_{c=1}^{C_{wf}} \; 1 \\
&#10;\left[\frac{1}{P_{wfc}} \sum_{p=1}^{P_{wfc}} x_{wfcp}\right]\right] &amp;amp;&amp;amp; &amp;amp;&amp;amp; \\
&#10;8. \;\; \frac{1}{W}\sum_{w=1}^W
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm a novice in statistics and I have some confusion about the assumption of independence for statistical tests. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I searched the Internet and some information says that for the t-test, the observations in the two groups should be independent (that is, measurements in sample 1 and measurements in sample 2 should be different). Some other information says that all observations (even in the same group) should be independent. Which one is correct? &lt;/li&gt;&#10;&lt;li&gt;Is the independence assumption for ANOVA and the independence assumption for the t-test the same? &lt;/li&gt;&#10;&lt;li&gt;Do non-parametric tests, such as the Wilcoxon signed rank test, also need to satisfy the independence assumption? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-08-31T08:24:08.037" Id="15008" LastActivityDate="2012-10-28T23:16:03.037" LastEditDate="2012-10-28T23:12:35.803" LastEditorUserId="7290" OwnerUserId="6081" PostTypeId="1" Score="5" Tags="&lt;independence&gt;&lt;assumptions&gt;&lt;basic-concepts&gt;" Title="Question about independence assumption for ANOVA, t-test, and non-parametric tests" ViewCount="3190" />
  
  <row AcceptedAnswerId="15037" AnswerCount="2" Body="&lt;p&gt;I want to simulate a user reading a book.  The amount of time spent on a page will be random, but subject to a particular distribution, I just don't know which one.  A normal distribution has the problem that it can produce negative values, which is clearly not right.&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the correct distribution for this kind of random variable?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-31T17:23:31.673" Id="15033" LastActivityDate="2011-08-31T20:37:10.593" LastEditDate="2011-08-31T18:41:56.820" LastEditorUserId="919" OwnerUserId="6106" PostTypeId="1" Score="6" Tags="&lt;modeling&gt;&lt;random-variable&gt;&lt;simulation&gt;" Title="What's the correct distribution for page reading time?" ViewCount="431" />
  <row Body="&lt;p&gt;&lt;strong&gt;It helps to have data.&lt;/strong&gt;  Jakob Nielsen has measured reading times for web pages (&lt;a href=&quot;http://www.useit.com/alertbox/percent-text-read.html&quot; rel=&quot;nofollow&quot;&gt;&quot;How Little Do Users Read&quot;&lt;/a&gt;, 2008), which gives some strong hints:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The data show that the variation in reading time is directly proportional to the number of words on a page.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The variation therefore should be expressed as a time relative to the page length, not as a fixed amount.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The log of this ratio has an approximately normal distribution with a standard deviation of around 12%.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/EExLv.gif&quot; alt=&quot;Nielsen plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;This scatterplot from Nielsen's report is valuable for revealing the amount of &lt;strong&gt;relative variation&lt;/strong&gt; in reading times, even though only a small portion (about 18%) of each page is actually read.  Notice how the absolute variation increases with word length (page size); this complication is handled by using the log of the relative variation.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Don't forget that book pages have varying amounts of words, too.  The variation will depend on accumulated tiny differences related to word length, page width, paragraph length, amount of dialog, and so on.  For a book with uniform looking pages we can therefore expect that variation to be normal, &lt;em&gt;except&lt;/em&gt; for the ends and beginnings of chapters.  The end pages will have approximately a &lt;em&gt;uniform&lt;/em&gt; distribution of word lengths.  The beginning pages will have approximately a normal distribution with a smaller mean than the typical (full) page, depending on the page design.&lt;/p&gt;&#10;&#10;&lt;p&gt;This gives a complex distribution but it's relatively easy to simulate.  The parameters should include&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The mean number of words per (full) page, $w$.  You can easily measure this for actual books you are trying to simulate.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The standard deviation in words per (full) page, $s$.  Again, this is readily measured.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The mean number of words per start page, $u$, also easily measured.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The log variation in reading times, $\sigma$.  Use a value around 12% to start, based on Nielsen's study; consider looking at other studies for other realistic values.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The user's reading speed, $v$, as words per minute, say.  Values around 200-250 wpm are often used, depending on the kind of reader.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The mean number of pages per chapter, $n$, also easily measured.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The mean page turning time, $t$.  You could do your own little study of readers, perhaps by spending an hour with a stopwatch in a library :-).  Don't be too fussy about this number--it will depend on the book size, page material, and the reader--but it could contribute enough time to be of interest in the simulation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The simulation should comprise an entire chapter, simulated as a sequence consisting of one start page, $n-2$ normal pages, and one end page.  Simulate the number of words, $m$, as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$m = (n-2)w + u + zw + r$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $z$ has a uniform $(0,1)$ distribution and $r$ has a normal distribution with mean $0$ and standard deviation $s \sqrt{n}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Draw a value $x$ from a normal distribution with mean $0$ and standard deviation $\sigma$.  Multiply $m$ by $w \exp(x) / v$ to simulate the reading time.  Add $n t$ minutes for the page turns.&lt;/p&gt;&#10;&#10;&lt;p&gt;This process will capture the most important influences on reading time for a book with homogeneous text, uniform reading difficulty, and no illustrations.  For more complex books, such as collections of readings, math or science, books with lots of dialog, illustrated books, and so on, the model may need to be more complex in order to be realistic.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Edit&lt;/h3&gt;&#10;&#10;&lt;p&gt;It turns out we may be able to justify and flesh out the suggestion offered by @Jason, because it happens that this complex but realistic simulation can be extremely well approximated by a &lt;em&gt;version&lt;/em&gt; of a Gamma distribution in most cases.  We have to rescale and shift the Gamma, in addition to selecting its shape parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a (typical) example based on $100,000$ iterations with $w=300$ words per page, $s=15$ words (SD per page), $u=100$ words per start page, $\sigma=0.12$, $v=250$ words per minute, $n=8$ pages per chapter, and $t = 0.04$ minutes per page turn.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/E9OlY.png&quot; alt=&quot;Simulation results&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The histogram gives the distribution of results while the solid red curve is the PDF for a Gamma distribution with shape parameter $27.416$, scale parameter $0.2043$, offset by $2.98$ minutes.&lt;/p&gt;&#10;&#10;&lt;p&gt;This approximation breaks down only for extremely short chapter lengths, but is still decent even when $n=3$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9QGpp.png&quot; alt=&quot;Simulation results for n=3&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The potential advantage of this observation is that you can avoid estimating many of the parameters needed to model reading a simple, homogeneous book &lt;em&gt;if&lt;/em&gt; you are willing to specify three independent parameters of the distribution, such as its mean, standard deviation, and skewness.  For instance, if you have actual data about chapter reading times you could use the first three sample moments to fit a three-parameter Gamma distribution to the data, then perform the simulation via draws from that Gamma.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, if you assume the times to read the book chapters are independent, it is easy to add these Gammas (one per chapter) to obtain a distribution for the length of time to read the entire book (because the shape parameter for the sum of Gamma distributions having a common scale factor is the sum of their shape parameters).  Even with minimal data (such as used here) you could run some simulations for a single chapter, fit a Gamma to those simulation results, and proceed to &lt;em&gt;deduce&lt;/em&gt; (rather than simulate) the total book reading times. &lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, in this case the reading times for a book of $16$ chapters should follow a Gamma distribution with shape parameter $16 \times 27.4164$, scale parameter $0.2043$, offset by $16 \times 2.98$ minutes.  For many books (having many chapters), the resulting distribution will be Normal for all practical purposes.  The chance assigned to negative values by this distribution would be so astronomically small that it doesn't matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hhbZl.png&quot; alt=&quot;Distribution of book reading times&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The blue curve shows the distribution of book reading times.  The dashed red curve superimposed on it is a Normal approximation.  Neither distribution assigns any appreciable probability to times less than 240 minutes.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-08-31T18:39:04.623" Id="15037" LastActivityDate="2011-08-31T20:37:10.593" LastEditDate="2011-08-31T20:37:10.593" LastEditorUserId="919" OwnerUserId="919" ParentId="15033" PostTypeId="2" Score="13" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have the following results from a biological experiment. I measure the size of the sample (2,4,8,16,32 or 64) and count its distribution in a population of 50 (sum of counts=50). I do those measurements in the wildtype (wt) and the experimental sample (KO) in three independent experiments (replicate 1,2,3).&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I in a setup for a two way ANOVA with replicates? How can I proceed rigorously, with the maximum power in R as I am not sure (1) my data is normally distributed and (2) how to code that in R anyway? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;strain  Replicate   size    count&#10;wt  1   2   2&#10;wt  1   4   6&#10;wt  1   8   11&#10;wt  1   16  12&#10;wt  1   32  9&#10;wt  1   64  10&#10;wt  2   2   2&#10;wt  2   4   7&#10;wt  2   8   15&#10;wt  2   16  11&#10;wt  2   32  4&#10;wt  2   64  11&#10;wt  3   2   0&#10;wt  3   4   5&#10;wt  3   8   17&#10;wt  3   16  9&#10;wt  3   32  9&#10;wt  3   64  10&#10;KO  1   2   0&#10;KO  1   4   0&#10;KO  1   8   2&#10;KO  1   16  15&#10;KO  1   32  23&#10;KO  1   64  10&#10;KO  2   2   0&#10;KO  2   4   2&#10;KO  2   8   2&#10;KO  2   16  20&#10;KO  2   32  22&#10;KO  2   64  4&#10;KO  3   2   0&#10;KO  3   4   0&#10;KO  3   8   3&#10;KO  3   16  20&#10;KO  3   32  21&#10;KO  3   64  6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2011-09-01T14:24:15.480" Id="15067" LastActivityDate="2011-09-01T16:37:01.747" LastEditDate="2011-09-01T16:37:01.747" LastEditorUserId="88" OwnerUserId="6027" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;" Title="Is this a two way ANOVA with replicates?" ViewCount="429" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Given the following experimental setup:&lt;/p&gt;&#10;&#10;&lt;p&gt;Multiple samples are taken from a subject and each sample is treated multiple ways (including a control treatment). What is mainly interesting is the difference between the control and each treatment.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can think of two simple models for this data. With sample $i$, treatment $j$, treatment 0 being the control, let $Y_{ij}$ be the data, $\gamma_i$ be the baseline for sample $i$, $\delta_j$ be the difference for treatment $j$. The first model looks at both the control and difference:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  <row Body="&lt;p&gt;Unfortunately, I think the only way to do this is to edit the underlying Stat.KDE.Graphics.Contour template, which is not for the faint of heart. See the PROC TEMPLATE example at &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_ods_sect016.htm&quot; rel=&quot;nofollow&quot;&gt;http://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_ods_sect016.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-02T19:16:39.583" Id="15123" LastActivityDate="2011-09-02T19:16:39.583" OwnerUserId="2773" ParentId="15000" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;If you're just analyzing the one subject it's not a paired test.  There's nothing to pair across.  You also need to be careful in how you describe it because you can only make inferences about the performance of the individual subject and not subjects in general.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're analyzing multiple subjects then you need to actually have paired data, which means aggregating across subjects to comparable paired measures, such as how many cookies/day or mean cookies/hr.  You're not allowed to have more than one measure per predictor/level in a paired test for each subject.  You have two levels, stamina0 and stamina+.  Therefore, you can only have two measures / subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you could use mixed effects modelling that will allow you to use the number of cookies and hours and generate a much more precise model of what is going on.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-02T23:22:13.307" Id="15132" LastActivityDate="2011-09-02T23:22:13.307" OwnerUserId="601" ParentId="15130" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Your question is fairly complex, because the answer is: as many as a simulation needs. It depends entirely on the size of your data set, and how you are coding things. I have, for example, simulations that can be run on computers with &lt;em&gt;megabytes&lt;/em&gt; of available RAM - and one which crushed a cluster node with 96 GB of memory.&lt;/p&gt;&#10;&#10;&lt;p&gt;R keeps all its data in memory, which means as you start having huge data sets - or large collections of data sets - you're going to top out your RAM. This system has the benefit of being extremely fast, but limited by RAM. If you're running out of memory resources, recode your project to save these data sets to a file, clear them from R -using rm() - and then open them back up when you need them. This will slow down your code somewhat, but is way cheaper than buying new RAM, especially as a Macbook Pro is going to top out at 16 GB anyway.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-03T02:12:08.410" Id="15136" LastActivityDate="2011-09-03T02:12:08.410" OwnerUserId="5836" ParentId="12769" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Here's a paper from from NESUG that goes through the use of both GENMOD and PHREG for matched cohort data: &lt;a href=&quot;http://www.nesug.org/proceedings/nesug07/sa/sa01.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.nesug.org/proceedings/nesug07/sa/sa01.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It appears in this paper that continuous covariates are included in their standard form in PHREG, and somewhat suggests that including them in GENMOD will be difficult, if not impossible. But that should provide a decent starting point. If you &lt;em&gt;cannot&lt;/em&gt; use a Cox proportional hazard model and need a poisson estimate, you might be able to use something like a propensity-score or inverse-probability-of-treatment weights to address your confounding before the data ever hits a model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-03T02:38:55.970" Id="15137" LastActivityDate="2011-09-03T02:38:55.970" OwnerUserId="5836" ParentId="10621" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You can certainly apply different transformations to different variables, but it will change the meaning of the results.  A key assumption in factor analysis is that the observed measurements are linear combinations of the underlying factors (plus noise).  Transforming the observed measurements could have a big influence on the results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-03T14:43:43.653" Id="15153" LastActivityDate="2011-09-04T11:40:07.420" LastEditDate="2011-09-04T11:40:07.420" LastEditorUserId="930" OwnerUserId="5862" ParentId="15147" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="15162" AnswerCount="1" Body="&lt;p&gt;I have a large dataset with patients and I'm studying a rare outcome (~ 2%) and death is a competing risk (mean age ~69 years). I've used the R &quot;cmprsk&quot; package for my statistics and it seems that competing risks and the Cox regression are performing similarly although the competing risk analysis is more conservative giving hazard ratios closer to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been suggested to do a Poisson regression on the data but the results don't make any sense and I would be really grateful to get some input on the benefits of doing this kind of analysis on survival data. I've created this simulation for creating a dataset with similar risk factors:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;cmprsk&quot;)&#10;# The time for the study&#10;accrual_time &amp;lt;- 10&#10;followup_time &amp;lt;- 1&#10;&#10;base_risk &amp;lt;- list(&quot;event&quot; = .015, &quot;cmprsk&quot; = .1)&#10;&#10;risk_factors &amp;lt;- list(list(&quot;frequency&quot;=.1, &#10;                &quot;event&quot; = base_risk$event*.5, &#10;                &quot;cmprsk&quot; = base_risk$cmprsk*2),&#10;        list(&quot;frequency&quot;=.05, &#10;                &quot;event&quot; = base_risk$event*1, &#10;                &quot;cmprsk&quot; = base_risk$cmprsk*1),&#10;        list(&quot;frequency&quot;=.05, &#10;                &quot;event&quot; = base_risk$event*-.5, &#10;                &quot;cmprsk&quot; = base_risk$cmprsk*0))&#10;&#10;# Number of subjects&#10;n &amp;lt;- 5000&#10;&#10;# Create base time, sequential inclusion&#10;time_in_study &amp;lt;- rep(c(1:n)/n*accrual_time + followup_time, 1)&#10;&#10;set.seed(100)&#10;&#10;# Create empty sets&#10;x &amp;lt;- matrix(0, ncol=length(risk_factors), nrow=n)&#10;time_2_event &amp;lt;- rep(0, n)&#10;time_2_comprsk &amp;lt;- rep(0, n)&#10;&#10;# Create each studied observation and outcome&#10;for(i in 1:n){&#10;    # Set base risk&#10;    event_risk &amp;lt;- base_risk$event &#10;    comp_risk &amp;lt;- base_risk$cmprsk&#10;&#10;    for(j in 1:length(risk_factors)){&#10;        x[i, j] &amp;lt;- rbinom(1, 1, risk_factors[[j]]$frequency)[1]&#10;&#10;        # If there is a risk factor defined&#10;        if (x[i, j] &amp;gt; 0){&#10;            event_risk &amp;lt;- event_risk +&#10;                    risk_factors[[j]]$event&#10;            comp_risk &amp;lt;- comp_risk + &#10;                    risk_factors[[j]]$cmprsk&#10;        }&#10;    }&#10;&#10;    # Time 2 event/risk is 1/rate meaning that higher number -&amp;gt; shorter time&#10;    time_2_event[i] &amp;lt;- rexp(1, rate=event_risk)[1]&#10;    time_2_comprsk[i] &amp;lt;- rexp(1, rate=comp_risk)[1]&#10;}&#10;&#10;cn &amp;lt;- c()&#10;for(i in 1:length(risk_factors)){&#10;    ev_rsk &amp;lt;- risk_factors[[i]]$event/base_risk$event+1&#10;    cmp_rsk &amp;lt;- risk_factors[[i]]$cmprsk/base_risk$cmprsk+1&#10;    name &amp;lt;- paste(&quot;Risk factor no: &quot;, i, &quot;\n * ev=&quot;, ev_rsk, &quot; cr=&quot;, cmp_rsk, &quot; *&quot;, sep=&quot;&quot;)&#10;    cn &amp;lt;- c(cn, name)&#10;}&#10;colnames(x) &amp;lt;- cn&#10;&#10;# Select the event that happens first: study ends, evenent occurs, a competing event occurs&#10;time &amp;lt;- apply(cbind(time_in_study, time_2_event, time_2_comprsk), 1, min)&#10;&#10;# Outcome identifiers&#10;event &amp;lt;- (time_2_event == time) + 0&#10;comprsk &amp;lt;- (time_2_comprsk == time) + 0&#10;cens &amp;lt;- event+2*(event==0 &amp;amp; comprsk==1)&#10;&#10;out.cox_ev &amp;lt;- coxph(Surv(time, event)~x)&#10;summary(out.cox_ev)&#10;&#10;out.crr_ev &amp;lt;- crr(time, cens, x, failcode=1)&#10;summary(out.crr_ev)&#10;&#10;out.cox_cmprsk &amp;lt;- coxph(Surv(time, comprsk)~x)&#10;summary(out.cox_cmprsk)&#10;&#10;out.crr_cmprsk &amp;lt;- crr(time, cens, x, failcode=2)&#10;summary(out.crr_cmprsk)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output makes sense but when I do a:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;out.glm_pr &amp;lt;- glm(event ~ x, family=&quot;poisson&quot;)&#10;summary(out.glm_pr)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It gives estimates of:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;RF 1 ~ .14 &lt;/li&gt;&#10;&lt;li&gt;RF 2 ~ .41 &lt;/li&gt;&#10;&lt;li&gt;RF 3 ~ -.23&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My questions: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is the glm() code correct or should I somehow transform my data?&lt;/li&gt;&#10;&lt;li&gt;Does the Poisson output make any sense and how should if so interpret it?&lt;/li&gt;&#10;&lt;li&gt;What are the benefits/pitfalls in using Poisson regression for survival data?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;UPDATE&lt;/h2&gt;&#10;&#10;&lt;p&gt;After adding exp(out.glm_pr$coefficients) the results are almost identical to the competing risk regression, here's a forest plot that compares the three:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/14Zt0.png&quot; alt=&quot;A forestplot comparing the different methods - Poisson: 1.152  1.509  0.794, CRR: 1.151 1.524 0.812, Cox PH: 1.897 1.931 0.798&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The x-axis is perhaps not entirely valid (should be &quot;incident rate ratios&quot; for the Poisson regression) but why are the outcomes for CRR &amp;amp; poisson almost identical?&lt;/p&gt;&#10;&#10;&lt;p&gt;As for testing over-dispersion I've found these two methods:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(qcc)&#10;&amp;gt; qcc.overdispersion.test(event)&#10;&#10;Overdispersion test Obs.Var/Theor.Var Statistic p-value&#10;       poisson data         0.9391878      4695 0.99902&#10;&amp;gt; &#10;&amp;gt; library(pscl)&#10;&amp;gt; out.glm_nb &amp;lt;- glm.nb(event ~ x)&#10;Warning messages:&#10;1: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &amp;gt;  :&#10;  iteration limit reached&#10;2: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &amp;gt;  :&#10;  iteration limit reached&#10;&amp;gt; odTest(out.glm_nb)&#10;Likelihood ratio test of H0: Poisson, as restricted NB model:&#10;n.b., the distribution of the test-statistic under H0 is non-standard&#10;e.g., see help(odTest) for details/references&#10;&#10;Critical value of test statistic at the alpha= 0.05 level: 2.7055 &#10;Chi-Square Test Statistic =  -0.0139 p-value = 0.5 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I conclude that there isn't any evidence of over-dispersion or are there other methods better suited for testing over-dispersion in this kind of survival data?&lt;/p&gt;&#10;&#10;&lt;p&gt;The quasipoisson analysis gives similar values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; out.glm_quasi_pr &amp;lt;- glm(event ~ x, family=quasipoisson(link=&quot;log&quot;))&#10;&amp;gt; round(exp(out.glm_quasi_pr$coefficients), 3)&#10;(Intercept)       xRF 1       xRF 2       xRF 3 &#10;      0.059       1.152       1.509       0.794 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-09-03T17:49:29.403" Id="15160" LastActivityDate="2011-09-05T01:40:49.790" LastEditDate="2011-09-04T17:18:28.343" LastEditorUserId="5429" OwnerUserId="5429" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;poisson&gt;&lt;survival&gt;&lt;simulation&gt;" Title="Poisson regression in a survival setting on a simulated set" ViewCount="1440" />
  
  <row Body="&lt;p&gt;I thinks that, EDA helps you to build a model, make some assumptions and (if required) update the model and its assumptions. I select a pragmatics approach to use for model fitting and assessment.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-04T06:55:00.887" Id="15174" LastActivityDate="2011-09-04T06:55:00.887" OwnerUserId="6138" ParentId="15131" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have short daily time series (less than 4 years) representing sales and exhibiting two seasonalities (weekly and yearly) and I am seeking to identify outliers (not only data reporting errors but also particular events impacting the sales). Because of the presence of a trend and seasonalities, I think detrending and deseasonalising data beforehand is necessary; so I was thinking of using decomposition models based on moving averages to get the irregular component and then use statistical tests (Z score methods, use of interquartile range...) or distance/density based algorithms but the results will be closely related to the quality of the decomposition (that depends itself on the number of outliers) and I am not even sure it will work. Or I was thinking of using the method exposed by Tsay to detect the different types of outliers (additive, innovational, level shifts...) using regARIMA models with dummy variables for festivals, trends and seasonalities (I hope I will have enough data for that). I have only free software (R) at my disposal. Which would be the best approach to adopt ? I would be vey grateful for any help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-04T15:19:24.403" Id="15182" LastActivityDate="2011-09-04T22:55:14.097" OwnerUserId="6170" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;outliers&gt;&lt;seasonality&gt;" Title="Outlier detection in short time series with two seasonalities" ViewCount="271" />
  
&#10;$$&#10;$$
&#10;$$    &lt;/p&gt;&#10;&#10;&lt;p&gt;This implies that group $3$ is represented by $X_1=0, X_2=0$; represent your response -- shopping habit as $Y$ and the quantitative explanatory variable weight loss as $W$. You are now fit this linear model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;E[Y]= \beta_0 + \beta_3W \text{ -- for third group},
&#10;$$&#10;$$
&#10;E[Y]= (\beta_0 + \beta_2)+\beta_3W \text{ -- for second group},
&#10;\text{Group 2 vs Group 1: } \beta_2 + \beta_0 - (\beta_0+\beta_1) = 0.
  
&#10;\Cov(U_1, U_2) = \Cov(X_1,X_1) - \Cov(X_1,X_3) - \Cov(X_1,X_2) + \Cov(X_2,X_3) \&amp;gt;.
&#10;$$&#10;and, of course, by symmetry again, $\Cov(X_1,X_2) = \Cov(X_1,X_3) = \Cov(X_2,X_3)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, $\Cov(U_1, U_2) = n p (1-p) + n p^2 = np = n/3$ and by a similar calculation, $\Cov(U_1, U_1) = \Cov(U_2,U_2) = 2 n p = 2 n / 3$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Observe that $U_1$ and $U_2$ are each the sums of independent and identically distributed random variables. For example, if $\xi_i \in \{1,2,3\}$ is the outcome of the $i$th draw, then $U_1 = \sum_{i=1}^n 1_{(\xi_i = 1)} - 1_{(\xi_i = 2)}$ where $1_{(\cdot)}$ is the indicator function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, by the multivariate central limit theorem, we conclude that&#10;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is some very brief $R$ code that compares a simulation of the true process against a simulation using the normal approximation assuming $n = 100$ underlying trinomial trials. &lt;BR&gt;&#10;First, the picture.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/w7y5y.png&quot; alt=&quot;Comparison of actual process and normal approximation via simulation&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the code.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(.Random.seed[1])&#10;n &amp;lt;- 100&#10;N &amp;lt;- 10000&#10;&#10;X   &amp;lt;- matrix( sample(1:3, n*N, replace=T), nc=n )&#10;xt  &amp;lt;- apply(X,1,table)&#10;dxt &amp;lt;- cbind( xt[1,]-xt[2,], xt[1,]-xt[3,] )&#10;xtt &amp;lt;- table(apply(dxt,1,min))&#10;&#10;L   &amp;lt;- matrix( c(sqrt(2), 1/sqrt(2), 0, sqrt(3/2)), 2 )&#10;Y   &amp;lt;- L %*% matrix( rnorm( 2*N ), nr=2 ) * sqrt(n) / sqrt(3)&#10;ytt &amp;lt;- table( floor(apply(Y,2,min)) )&#10;&#10;plot( names(xtt), xtt/N, type=&quot;h&quot;, xlab=&quot;a&quot;, ylab=&quot;Density of T(a)&quot; )&#10;lines( names(ytt), ytt/N, col=&quot;red&quot;, type=&quot;h&quot; )&#10;legend( &quot;topright&quot;, legend=c(&quot;actual&quot;, &quot;normal approx.&quot;), lty=&quot;solid&quot;,&#10;        col=c(&quot;black&quot;, &quot;red&quot;), bg=&quot;white&quot;, inset=0.02 )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are also R packages to calculate bivariate normal densities and probabilities. Both &lt;a href=&quot;http://cran.r-project.org/web/packages/mnormt/index.html&quot;&gt;mnormt&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/fMultivar/index.html&quot;&gt;fMultivar&lt;/a&gt; are examples, but I don't know enough to recommend any one over another.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-05T01:36:38.427" Id="15196" LastActivityDate="2011-09-05T01:57:53.743" LastEditDate="2011-09-05T01:57:53.743" LastEditorUserId="2970" OwnerUserId="2970" ParentId="14400" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="15263" AnswerCount="1" Body="&lt;p&gt;Sort of like the archipelago map in Age of Empires II&lt;/p&gt;&#10;&#10;&lt;p&gt;(But this is actually a real question that I do need for my exoplanet research)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-09-05T06:14:54.600" Id="15201" LastActivityDate="2011-09-06T18:32:45.347" OwnerUserId="5288" PostTypeId="1" Score="4" Tags="&lt;monte-carlo&gt;" Title="What is a good Monte Carlo algorithm for generating an archipelago in a particular map?" ViewCount="175" />
  
  <row Body="&lt;p&gt;In Stata, you can achieve this as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;clear&#10;set seed 99&#10;set obs 1000&#10;gen inc = 10000 * exp(invnorm(uniform()))&#10;gen lb = floor(inc/10000) * 10000&#10;tabulate lb&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This code creates left-bounded intervals, i.e. including the lower bound, but excluding the upper bound. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you prefer the &lt;code&gt;count&lt;/code&gt; approach, you can use the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;forvalues n = 0(10000)280000 {&#10;    count if inrange(inc, `n', `n' + 10000)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the &lt;code&gt;inrange&lt;/code&gt; function refers to left-bounded intervals too.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you need a table to be inserted in a document, you can add the following lines:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;collapse (count) N = inc, by(lb)&#10;gen interval = &quot;[&quot; + string(lb) + &quot;;&quot; + string(lb + 10000) + &quot;)&quot; &#10;list interval N, sep(0) noobs&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-09-05T08:55:05.467" Id="15203" LastActivityDate="2011-09-09T06:44:40.480" LastEditDate="2011-09-09T06:44:40.480" LastEditorDisplayName="user5644" OwnerDisplayName="user5644" ParentId="15124" PostTypeId="2" Score="2" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Your dependent variables should be normal in each cell of between-subject design. You have 2 such cells: 2 groups, so normality should be in both groups. Also, variance-covariance matrix between your 3 DV should be same in the 2 groups. You could check normality by Shapiro-Wilk test or Kolmogorov-Smirnov (with Lilliefors correction) test in EXPLORE procedure. Variance-covariance homogeneity could be tested by Box's M test (found in Discriminant analysis). Note however that ANOVA is quite robust to violations to both assumptions.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Mauchly's test checks the so called sphericity assumption which is necessary for univariate approach to repeated measures ANOVA. This assumption requires that, roughly speaking, differences between your repeated measure DVs don't intercorrelate. If the assumption is violated you should disregard &quot;Spericity assumed&quot; in Tests of Within-Subjects Effects table - there found some corrections (such as Greenhouse-Geisser) instead.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;While Tests of Within-Subjects Effects table reflects &quot;univariate approach&quot; in RM-ANOVA, Multivariate Tests table reflects &quot;multivariate approach&quot;. These two are both useful and there's a little debate which is &quot;better&quot;. Read a little &lt;a href=&quot;http://stats.stackexchange.com/questions/13197/differences-between-manova-and-repeated-measures-anova/13201#13201&quot;&gt;here&lt;/a&gt; about them, a bit more &lt;a href=&quot;http://www.statsoft.com/textbook/anova-manova/#sphericity&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Usually one won't check pairwise tests if overall effect is non-significant, it has little sense.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-09-05T12:34:32.883" Id="15212" LastActivityDate="2011-09-05T13:03:26.033" LastEditDate="2011-09-05T13:03:26.033" LastEditorUserId="3277" OwnerUserId="3277" ParentId="15207" PostTypeId="2" Score="8" />
  <row AnswerCount="2" Body="&lt;p&gt;I have trouble proving the following fact in my econometrics homework. The lecturer said that I should merely look at my statistics books, but I cannot seem to find it anywhere! Thus, sorry if it is (too) ignorant a question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that random variables $\varepsilon_{1t}$, $\varepsilon_{2t} \sim IIN(0,\Sigma)$ (i.e. identically independently normally distributed with a vector of means equal to $0$ and a variance-covariance matrix $\Sigma$). &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I then show that $\varepsilon_{1t}=\lambda\varepsilon_{2t}+u_t$, where $\lambda = \frac {\sigma_{12}} {\sigma_{22}}$ and $Var(u_t)=\sigma_{11}-\frac{\sigma_{12}^2}{\sigma_{22}}$ and $u_t$ is a disturbance term? ($\sigma_{ij}$ denotes the corresponding element of the variance-covariance matrix).   &lt;/p&gt;&#10;&#10;&lt;p&gt;All help is greatly appreciated. :)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Clarification: I just wanted to add that this question comes from a time-series context. Thus, IIN means that the $\varepsilon$'s are independent over time (i.e. no autocorrelation) and that the distribution does not change. However, there is contemporaneous correlation between the $\varepsilon$'s as they come from a bivariate distribution.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-05T18:43:38.497" Id="15231" LastActivityDate="2011-09-06T15:36:00.590" LastEditDate="2011-09-06T07:35:26.147" LastEditorUserId="-1" OwnerUserId="6187" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;self-study&gt;&lt;econometrics&gt;&lt;bivariate&gt;" Title="What is the relation between two IIN mean zero random variables?" ViewCount="186" />
  <row Body="&lt;p&gt;If it actually worked, and was something you could implement statistical code on in one way or another? Absolutely. There are undoubtedly new techniques that could emerge from throwing yet more computational firepower at something. Or, as importantly, making currently bleeding edge, computationally intensive techniques accessible. Just think about current computers - Bayesian estimation isn't exactly new. But being able to run MCMC-based analysis on massively complex data sets where that's not the focus of the paper, but just something that happened along the way, is profoundly powerful thing.&lt;/p&gt;&#10;&#10;&lt;p&gt;So even if they don't bring about new techniques (which they will) being able to go &quot;yeah, sure we can do that&quot; to computationally intensive techniques on huge data sets is a big deal.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-09-05T22:16:59.137" Id="15237" LastActivityDate="2011-09-06T16:16:18.280" LastEditDate="2011-09-06T16:16:18.280" LastEditorUserId="5836" OwnerUserId="5836" ParentId="15206" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;That requirement is...nonsensical. You should probably considering emailing the editor for guidance, or note your issue with reporting something like R-squared&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-05T22:20:47.027" Id="15238" LastActivityDate="2011-09-05T22:20:47.027" OwnerUserId="5836" ParentId="11643" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Apparently not, if you consider that this still hasn't been answered.&lt;/p&gt;&#10;&#10;&lt;p&gt;More seriously though: There actually there were some insights in the question and the comments. The main insight seems to be that you need a control if you want to demonstrate that something is unusual.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-05T23:31:36.870" Id="15242" LastActivityDate="2011-09-05T23:31:36.870" OwnerUserId="3874" ParentId="6306" PostTypeId="2" Score="2" />
  <row Body="&lt;h3&gt;General Resource on interpreting repeated measures ANOVA with SPSS&lt;/h3&gt;&#10;&#10;&lt;p&gt;It sounds like you need a better general resource on repeated measures ANOVA. Here are a few web resources, but in general a search for &lt;a href=&quot;http://www.google.com.au/search?sourceid=chrome&amp;amp;ie=UTF-8&amp;amp;q=SPSS+repeated+measures+ANOVA&quot; rel=&quot;nofollow&quot;&gt;&quot;SPSS repeated measures ANOVA&quot;&lt;/a&gt; will yield many useful options.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/seminars/Repeated_Measures/default.htm&quot; rel=&quot;nofollow&quot;&gt;UCLA&lt;/a&gt; has an example of SPSS output with interpretation by a 2 by 3 mixed ANOVA along with several other examples.&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.statisticshell.com/docs/repeatedmeasures.pdf&quot; rel=&quot;nofollow&quot;&gt;Andy Field on repeated measures ANOVA&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;1. Checking normality&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;From a practical perspective, tests of normality are often used to justify transformations. If you do apply a transformation, then you need to apply the same transformation to all cells of the design. &lt;/li&gt;&#10;&lt;li&gt;A common way to assess normality using SPSS is to set up your model and save the residuals and then examine the distribution of residuals.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;2. Value of Mauchly's test&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A common strategy is to look at Mauchly's test and if it is statistically significant, interpret either the univariate corrected tests or the multivariate tests.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;3. Multivariate&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I think @ttnphns has summed this up well.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;4. Pairwise comparisons&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I think @ttnphns has summed this up well. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-09-06T04:05:58.980" Id="15245" LastActivityDate="2011-09-06T04:05:58.980" OwnerUserId="183" ParentId="15207" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I will denote by $K$ the number of classes and by $p$ the dimension of the feature space. &lt;/p&gt;&#10;&#10;&lt;p&gt;I assume you use a procedure that can be rephrased as (it is the case of LDA, see conclusion): &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;1- For all $k=1,\dots,K$, estimate $\hat{P}_k:\mathbb{R}^p\rightarrow \mathbb{R}$ from a learning set. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;2- For all $k=1,\dots,K$ set $$V_k=\{x\in \mathbb{R}^p:\; \forall j=1,\dots,K\;\; \widehat{\mathcal{L}}_{kj}(x)=\log(\hat{P}_k(x)/\hat{P}_{j}(x))\geq 0 \}$$ &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The almost non trivial point here is that $(V_k)_{k=1,\dots,K}$ forms a partition of $\mathbb{R}^p$, up to a set null measure (under suitable regularity condition satisfyed in the case of LDA). (i.e. $\bigcup_{k=1,\dots,K} V_k=\mathbb{R}^p$ and $mes (V_i\cap V_j) =0$ for any $i\neq j$). In other words: &#10;it is not possible (expept in pathologic cases such as $\hat{P}_i=\hat{P}_j$) that for two different classes $i,j$,  $x\in V_i$, and $x\in V_j$,  i.e. $x$ is classified in class $i$ and in class $j$. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Proof: &#10;The result is true because you can rewrite: &#10;$$V_k=\{x\in \mathbb{R}^p:\; \forall j=1,\dots,K\;\; \log(\hat{P}_k(x))\geq \log (\hat{P}_{j}(x)) \}$$&#10;$$V_k=\{x\in \mathbb{R}^p:\; k=arg\max_{j=1,\dots,K}\;\; \log(\hat{P}_j(x))\}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence $V_i\cap V_i \subset \{x:\;\hat{P}_j(x)=\hat{P}_i(x)\}$ which should be of null measure&lt;/p&gt;&#10;&#10;&lt;p&gt;The intuition behind this result is that you certainly have two much separating functions  ($K(K-1)/2$as you noticed in your question) but these are build from $K$ densities only... &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Usual &quot;LDA&quot; belongs to that type of algorithm with $\forall k=1,\dots,K$,  $\hat{P}_k$ gaussian with mean $\hat{\mu}_k$ and covariance $\hat{C}$ estimated in step 1-.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a conclusion, &lt;strong&gt;the answer is no&lt;/strong&gt; (there can't be problem such as the one you state) whatever the number of classes $k$ and the feature space dimension $p$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A problem that can occur with dimensionality reduction:&lt;/strong&gt; If you are in a high dimensional space it might be a good idea to reduce the dimension of your problem. If you choose &quot;one subspace&quot; to work with then there is no problem. However, if you choose to reduce the dimension per pairs of class (which is often natural, there is one good space to work with per pairs) this will result in a &quot;separation function&quot; $\widehat{\mathcal{L}}_{kj}(x)$ that cannot be decomposed into $f_k-f_j$... &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-06T13:29:40.933" Id="15248" LastActivityDate="2011-09-06T20:22:54.613" LastEditDate="2011-09-06T20:22:54.613" LastEditorUserId="223" OwnerUserId="223" ParentId="11689" PostTypeId="2" Score="1" />
&#10;       &amp;amp;= \sigma_{11} - 2\frac{\sigma_{12}^2}{\sigma_{22}} + \frac{\sigma_{12}^2}{\sigma_{22}} \\
  <row AnswerCount="0" Body="&lt;p&gt;I have this series:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;r &amp;lt;- c(14.1649994,17.3997831,32.8877030,6.1547551,6.3594748,101.9584077,196.8411517,23.8292043,11.7583992,2.2915478,5.5088687,16.8188390,2.3077979,2.6666377,2.0955362,0.9319688,0.6688078,1.0523184,6.8158264,13.1813211,17.8467944,1.9876636,7.0219306,3.9272217,4.3325448,7.8778125,1.1878058,1.7207762,2.9655665,5.8571428)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm using the &lt;code&gt;strucchange&lt;/code&gt; package to detect the structural changes. The result is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sctest(r~1)&#10;&#10;    Recursive CUSUM test&#10;&#10; data:  r ~ 1 &#10; S = 0.7785, p-value = 0.1579&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How is it possible that the test didn't detect changes? There is a very big move rising around 200.&lt;/p&gt;&#10;&#10;&lt;p&gt;Take a look at the plot of the series below.&#10;&lt;img src=&quot;http://i.stack.imgur.com/iwvB2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-09-06T17:23:17.927" FavoriteCount="1" Id="15261" LastActivityDate="2011-09-06T22:27:51.337" LastEditDate="2011-09-06T22:27:51.337" LastEditorUserId="776" OwnerUserId="5405" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;variance&gt;&lt;structural-change&gt;" Title="How can this series pass the structural change test?" ViewCount="879" />
  
  <row AcceptedAnswerId="15273" AnswerCount="1" Body="&lt;p&gt;Let $X_1$ and $X_2$ be independent, normal distributed random variables with equal mean $\mu$ but non-equal standard deviations $\sigma_1$ and $\sigma_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I know $\sigma_1$ and $\sigma_2$ and I have $n$ samples $x_{11},\ldots,x_{1n}$ from $X_1$ and m samples $x_{21}, \ldots, x_{2m}$ from $X_2$, what is the best estimator for $\mu$? What is its distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;(edit: I'm mainly interested in the n=1, m=1 case)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-09-06T18:51:41.573" FavoriteCount="1" Id="15266" LastActivityDate="2011-09-06T21:20:53.590" LastEditDate="2011-09-06T20:08:48.060" LastEditorUserId="2848" OwnerUserId="2848" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;unbiased-estimator&gt;" Title="Combining two unequal normal distributions" ViewCount="520" />
  <row Body="&lt;p&gt;&lt;em&gt;Ad 1.&lt;/em&gt; Your formula vaguely represents the variance of residuals, and related indicators (such as R²) are commonly used as quality criteria.&lt;/p&gt;&#10;&#10;&lt;p&gt;R², which is commonly used to describe the goodness-of-fit of a model, compares the ratio of the variance of the score that is explained by the ELO-ranking to the variance that is not explained by it. The formula is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R&amp;#178; =\frac{ESS}{TSS}$&lt;/p&gt;&#10;&#10;&lt;p&gt;while&lt;/p&gt;&#10;&#10;&lt;p&gt;$ESS = \sum ($predicted score$ − $true score$)²$&lt;/p&gt;&#10;&#10;&lt;p&gt;$TSS = \sum ($true score$ − $average true score$)²$&lt;/p&gt;&#10;&#10;&lt;p&gt;This R² should be corrected to account for the number of parameters and observations:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\bar{R&amp;#178;} = 1-(1-R&amp;#178;)\frac{n-1}{n-p-1}$&#10;where $n$ is the number of observations and $p$ is the number of parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;One option is to use the model with the highest R², but others may know more about model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Ad 2.&lt;/em&gt; Why would you apply a &lt;em&gt;visual&lt;/em&gt; method for parameter optimization? Why don’t you apply some sort of regression analysis to determine those parameter values that minimize the standard error of regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me you already put quite a lot of effort in testing your rank (75 runs and a number of csv files and plots). If you specify how your new ranking scheme is calculated, and how the data you base it on is structured, I bet someone will walk you through the necessary steps to perform a statistical analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to stick with visualisation, I would plot the values of your “residuals”, i.e. the predicted scores minus the actual ones, or distributions thereof. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I wonder whether chess scores can be objectively assessed at all. If one player wins against another one, does this always mean he should have had better rating? I would be interested to know what your rating is based on.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-07T08:31:04.640" Id="15282" LastActivityDate="2011-09-07T10:42:52.097" LastEditDate="2011-09-07T10:42:52.097" LastEditorUserId="5191" OwnerUserId="5191" ParentId="15281" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;@Karl Broman's reply suggested one might create a &lt;a href=&quot;http://itl.nist.gov/div898/handbook/eda/section3/eda33qb.htm&quot; rel=&quot;nofollow&quot;&gt;scatterplot matrix&lt;/a&gt; to good effect.  As an example, here are the famous &lt;a href=&quot;http://ideas.repec.org/p/boc/bocins/iris.html&quot; rel=&quot;nofollow&quot;&gt;iris data&lt;/a&gt;, ridiculously scaled down to press the point that you don't need lots of ink or space on the page to be able to present &lt;em&gt;far&lt;/em&gt; more information than a mere table of numbers or simple bar chart or correlation plot would reveal.  For instance, none of those would indicate the separation into distinct populations that is so clear here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2zsk6.png&quot; width=&quot;320&quot; height=&quot;200&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(The variables, left to right and top to bottom, are sepal length, sepal width, petal length, and petal width, all expressed in centimeters.  Colors indicate species: &lt;em&gt;i. setosa&lt;/em&gt; is blue, &lt;em&gt;i. versicolor&lt;/em&gt; is red, and &lt;em&gt;i. virginica&lt;/em&gt; is green.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Some statistical software (such as Systat), as an option, will superimpose ellipses on the scatterplots to approximate contours of the fitted bivariate Normal distribution.  This visual aid is a nice graphical way to indicate correlation coefficients.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-07T13:57:55.367" Id="15284" LastActivityDate="2011-09-07T13:57:55.367" OwnerUserId="919" ParentId="15168" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Why not just take the loss function used in robust linear regression, and plug in your forecasts with the corresponding actuals for a hold-out sample? (where &quot;forecasts&quot; are calculated using &lt;a href=&quot;http://en.wikipedia.org/wiki/Backtesting&quot; rel=&quot;nofollow&quot;&gt;backtesting&lt;/a&gt; for that hold-out sample).&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if &lt;a href=&quot;http://en.wikipedia.org/wiki/Huber_loss_function&quot; rel=&quot;nofollow&quot;&gt;Huber loss function&lt;/a&gt; is used in the regression then just use that functional form to calculate average value of the loss function on a hold-out sample.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-07T16:36:37.737" Id="15288" LastActivityDate="2011-09-07T16:55:07.720" LastEditDate="2011-09-07T16:55:07.720" LastEditorUserId="6129" OwnerUserId="6129" ParentId="15030" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;I think a bit more information is necessary to understand why the model isn't converging.  First, a clarification: is the variable &quot;x&quot; a factor, representing the red v. the black curves in the image?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, use of the &quot;*&quot; to indicate an interaction implies (in R syntax) that you want main effects for each term as well as the interaction term.  So this would expand to&lt;/p&gt;&#10;&#10;&lt;p&gt;x + days_ind + x:days_ind&lt;/p&gt;&#10;&#10;&lt;p&gt;It is possible that the main effect for days_ind is causing the convergence problem.&#10;I would suggest trying x+x:days_ind instead of x*days_ind. So, your line of R would look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;coxph(Surv(time=days,event=event2)~x+x:days_ind)&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that works.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your other specification: x*days is not a good (or valid?) cox model formula for the reason above, in that the main effect for &quot;days&quot; implies a constantly changing hazard.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-09-07T19:23:04.360" Id="15297" LastActivityDate="2011-09-07T19:23:04.360" OwnerUserId="6209" ParentId="15279" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row Body="The probability of receiving a treatment given a set of observed covariates. " CommentCount="0" CreationDate="2011-09-08T14:55:01.190" Id="15332" LastActivityDate="2011-09-08T15:01:36.303" LastEditDate="2011-09-08T15:01:36.303" LastEditorDisplayName="user5644" OwnerUserId="-1" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Monte Carlo methods are used to mimic the random behavior of real random phenomena by drawing sequences of random or pseudo-random numbers. These numbers are incorporated as inputs to the model describing the phenomena of interest. They require a (pseudo-)random number generator; the most common implementations provide a number from 0 to 1; let's call this $U$. Thus, to simulate a fair coin toss, draw a random number $U$ and associate the values from $[0,0.5)$ with heads, and the values from $[0.5,1)$, with tails. Of course, more complicated systems require heavier use of the random number generators, as well as smart ways of transforming the basic $U[0,1]$ variates to the random variables with the necessary distribution, e.g., using acceptance-rejection methods or Metropolis algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;In very serious Monte Carlo, an efficient use of variance reduction techniques is often warranted. These include antithetic samples, importance sampling, control variates, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides the direct use of Monte Carlo methods to model random phenomena, they are also used in mathematical physics to evaluate multidimensional integrals. A point is drawn randomly from the domain of the integrand; the value of the integrand is added to the running sum; the process is repeated until the desired accuracy is achieved. The advantage of the method is that the error decreases at the rate $O( (N \log(\log N))^{-1/2})$, independent of the dimension of the space. Low discrepancy, or quasi-Monte Carlo, sequences are arguably better suited for this purpose.&lt;/p&gt;&#10;&#10;&lt;p&gt;References: &lt;/p&gt;&#10;&#10;&lt;p&gt;Lemieux, C. (2009) Monte Carlo and Quasi-Monte Carlo Sampling. Springer. &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387781641&quot; rel=&quot;nofollow&quot;&gt;Amazon link&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Liu, J. S. (2001) Monte Carlo Strategies in Scientific Computing. Springer. &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387952306&quot; rel=&quot;nofollow&quot;&gt;Amazon link&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also: tags &lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/sampling&quot;&gt;sampling&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/simulation&quot;&gt;simulation&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-08T20:58:29.393" Id="15344" LastActivityDate="2011-09-08T22:36:24.070" LastEditDate="2011-09-08T22:36:24.070" LastEditorUserId="5739" OwnerUserId="5739" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;LDS is deterministic, but Orthogonal is still based on pseudo-random. The quadrant defined in Orthogonal looks similar to LDS though.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-09T08:52:30.820" Id="15358" LastActivityDate="2011-09-09T08:52:30.820" OwnerUserId="6236" ParentId="15315" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You have a very good point: the estimates of the percentages in different categories are correlated. In fact, if categories $i$ and $j$ have true probabilities of $p_i$ and $p_j$, then the estimated probabilities will have a correlation of $-\sqrt{\frac{p_i p_j}{(1-p_i)(1-p_j)}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This, however, does not affect the marginal variability of each particular estimate. So one usually presents regular standard errors. You can show regular confidence intervals as well as long as you are clear that those are not simultaneous confidence intervals. As you go deeper into the analysis, then depending on the questions you want answered, the presence of correlation might have to be taken into account.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-09T12:56:09.967" Id="15365" LastActivityDate="2011-09-09T12:56:09.967" OwnerUserId="279" ParentId="15354" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking at developing a mixed effect model for repeated measures of blood pressure using two different techniques.  I have been discussing the benefits of mixed models over repeated measures ANOVA on SE previously.  Only problem is that I'm finding the literature on mixed models in R is based on a starting point quite a bit higher than my feeble understanding.  Does anyone know of a good introduction to mixed modelling in R or can they suggest a good sequence of topics that will lead to a thorough understanding of how to construct and (more importantly) interpret the output of a mixed model in R? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-09T14:07:42.483" Id="15368" LastActivityDate="2011-11-15T14:07:17.323" LastEditDate="2011-09-09T15:16:44.280" LastEditorUserId="88" OwnerUserId="5317" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;books&gt;" Title="Tutorial suggestions leading to mixed-effect modelling" ViewCount="191" />
  <row AcceptedAnswerId="15373" AnswerCount="5" Body="&lt;p&gt;Would like to know how confident I can be in my $\lambda$. Anyone know of a way to set upper and lower confidence levels for a Poisson distribution? &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Observations ($n$) = 88 &lt;/li&gt;&#10;&lt;li&gt;Sample mean ($\lambda$) = 47.18182&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;what would the 95% confidence look like for this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-09T12:25:40.150" FavoriteCount="4" Id="15371" LastActivityDate="2014-08-08T21:23:16.137" LastEditDate="2011-09-09T17:24:38.680" LastEditorUserId="88" OwnerDisplayName="Travis" OwnerUserId="9363" PostTypeId="1" Score="15" Tags="&lt;poisson&gt;&lt;confidence-interval&gt;" Title="How to calculate a confidence level for a Poisson distribution?" ViewCount="20569" />
  
  <row Body="&lt;p&gt;not an expert, but maybe the ex-gaussian (gaussian plus exponential distribution)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Gaussian_minus_exponential_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Gaussian_minus_exponential_distribution&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/gamlss.dist/html/exGAUS.html&quot; rel=&quot;nofollow&quot;&gt;http://rss.acs.unt.edu/Rdoc/library/gamlss.dist/html/exGAUS.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.tqmp.org/doc/vol4-1/p35-45_Lacouture.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.tqmp.org/doc/vol4-1/p35-45_Lacouture.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;From the last link:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the framework of cognitive&#10;  processes, this convolution can be&#10;  seen as representing the overall&#10;  distribution of RT [Response Time] resulting from two&#10;  additive or sequential processes. As&#10;  proposed by Luce (1986, chap. 6), the&#10;  exponential process can be seen as the&#10;  decision component, i.e., the time&#10;  required to decide which response to&#10;  make, while the Gaussian component can&#10;  be conceptualized as the transduction&#10;  component, i.e., the sum of the time&#10;  required by the sensory process and&#10;  the time required to physically make&#10;  the response.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-08-15T04:54:49.673" Id="15376" LastActivityDate="2010-08-15T04:54:49.673" OwnerDisplayName="apeescape" OwnerUserId="291" ParentId="15375" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;For the basic Bayes Formula one common example to use is disease screening.  Assume that you have a test for a disease that if used on someone who has the disease will show positive with 95% probability and if used with someone without the disease will show negative with 90% probability; further we know that 1 in 1,000 in the population have the disease.  We randomly choose a person from the population (don't know ahead of time if they have the disease) and do the test which turns out positive: what is the probability that they have the disease?  This example is often eye-opening to a lot of people.  One way to demonstrate this (and quickly show the effect of changes) is using the &lt;code&gt;SensSpec.demo&lt;/code&gt; function in the TeachingDemos function for R (also see &lt;code&gt;tkexamp&lt;/code&gt; in the same package for a GUI interface to this in the examples).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to expand to Bayesian statistics then one fun approach is to start by showing the students a simple success/fail game like throwing a dart at a target, tossing a wadded up piece of paper into a basket, etc., and choosing a student that will play the game.  Ask the students how many times out of 4 they predict the student will succeed, and use their prediction as parameters for a Beta distribution as the prior distribution (plot this to show where they think the true probability could be).  Now have the student do the game 10 times and count the successes, use this as the data for a binomial likelihood, and combine with the prior to get a posterior distribution for the student's proportion of successes.  Show how you moved from a prior to a posterior using data and fairly simple calculations.  If you have time you can let the student play the game more times and use the first posterior as a new prior, then get an updated posterior, and show how the distribution changes with additional information.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-09T17:20:23.440" Id="15386" LastActivityDate="2013-07-05T08:57:15.877" LastEditDate="2013-07-05T08:57:15.877" LastEditorUserId="17230" OwnerUserId="4505" ParentId="15379" PostTypeId="2" Score="6" />
&#10;{\rm exp} \Big( -\frac{(y-x^{2})^2}{2}  \Big) dy $$&lt;/p&gt;&#10;&#10;&lt;p&gt;The integrand is a $N(x^{2},1)$ density and therefore it integrates to 1. Therefore, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x) = \frac{1}{\sqrt{2 \pi}} {\rm exp} ({-x^{2}/2})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, the marginal distribution of $X$ is standard normal.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-09T17:58:13.030" Id="15388" LastActivityDate="2011-09-09T17:58:13.030" OwnerUserId="4856" ParentId="15387" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="15402" AnswerCount="1" Body="&lt;p&gt;For an experiment I'm analyzing, I took many measurements of many different values. (To make it concrete, let's pretend I measured the height of people from many different age groups -- I have the height of ten 20-year-olds, ten 21-year-olds, ten 22-year-olds, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to demonstrate that the variance of all my readings, per variable, is pretty low. (In my example, I'd like to show that people of any given age are all about the same height. That is, say 20-year-olds are all close to 5'4&quot; with low variance and 21-year-olds are all close to 5'6&quot;. The particular heights don't matter; just that people of any given age are similar in height to other people of that age. That's obviously not true, but let's pretend it is.)&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the best metric to report? I could, for example, take the arithmetic mean of the variances -- or maybe the harmonic mean or geometric mean would be better? Or perhaps I should be averaging the standard deviations or the standard errors?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-09T18:18:14.830" FavoriteCount="1" Id="15391" LastActivityDate="2011-09-09T22:34:43.060" LastEditDate="2011-09-09T20:58:24.323" LastEditorUserId="6242" OwnerUserId="6242" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;standard-deviation&gt;&lt;standard-error&gt;&lt;average&gt;" Title="How should I characterize the &quot;typical&quot; variance of a large collection of variables?" ViewCount="138" />
  
  <row Body="&lt;p&gt;In functional data analysis, people often use penalties of the form &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \int_{D} [f^{(m)}(x)]^{2} dx $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;to ensure that an estimate of $f$ is smooth. Here $D$ is the domain of the function and $f^{(m)}(x)$ is the $m$'th derivative of $f$. In my own research I've found $m = 2$ to be useful. In your case the log-likelihood is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ -\frac{1}{2} \left( 2 \sum_{i=1}^{N} \log \big( \sigma(t_{i}) \big) + \sum_{i=1}^{N} \frac{x_{i}^{2}}{\sigma^{2}(t_{i})} \right) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;An estimator that minimizes&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ 
  
  
  
  <row Body="&lt;p&gt;It sounds like you want reinforcement learning. I'm having a little trouble parsing the exact details of your specific problem, but perhaps it could be cast in the framework of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Multi-armed_bandit&quot; rel=&quot;nofollow&quot;&gt;Multi-Armed Bandit&lt;/a&gt; problem?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-11T15:45:49.293" Id="15434" LastActivityDate="2011-09-11T16:19:39.110" LastEditDate="2011-09-11T16:19:39.110" LastEditorUserId="6248" OwnerUserId="6248" ParentId="15433" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;The side of the square is one, so its surface $= 1 * 1 = 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The side of the upper white square-cornered triangle is $1 - w_1$, so its surface is $(1-w_1)*(1-w_1)/2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly with the bottom triangle.&lt;/p&gt;&#10;&#10;&lt;p&gt;So: the surface of the gray area = square - two white triangles = formula $P2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally note that each 'point' in the square is equally likely to occur, so that the area of the 'valid' points (the grey area) divided by the area of 'all' points (the white square) is the probability of a 'valid' event (i.e. the two persons meeting). Since the surface of the square is 1, the result is still formula $P2$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-12T07:26:58.073" Id="15459" LastActivityDate="2011-09-12T07:26:58.073" OwnerUserId="4257" ParentId="15448" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Excel gets a bad rap (I suspect because of those horrible 3d graphs and inappropriate pie charts), but this is fairly easily accomplished in Excel charts. It simply takes knowing how to arrange elements on the chart.&lt;/p&gt;&#10;&#10;&lt;p&gt;This example by Jon Peltier on how to &lt;a href=&quot;http://peltiertech.com/WordPress/excel-box-and-whisker-diagrams-box-plots/&quot;&gt;make box plots in Excel&lt;/a&gt; can be very easily adapted to forest plots. In fact he has a &lt;a href=&quot;http://peltiertech.com/WordPress/error-bars-in-excel-2007/&quot;&gt;free utility to make error bars&lt;/a&gt;, although doing it yourself is not all that difficult.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you plan on doing work in Excel and that involves making charts, it would do everyone good to peruse the work on Jon's blog. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-12T12:19:20.230" Id="15462" LastActivityDate="2011-09-12T12:19:20.230" OwnerUserId="1036" ParentId="15456" PostTypeId="2" Score="6" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to predict a factor and also get an upper bound for my prediction. It seems that what I want is prediction interval. but prediction interval only specifies the bound for one next prediction not all. Recently I found tolerance interval definition. Can I use it like prediction interval for future samples?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-12T13:56:39.450" Id="15467" LastActivityDate="2012-10-20T16:03:05.287" LastEditDate="2012-04-19T15:08:52.327" LastEditorUserId="919" OwnerUserId="5496" PostTypeId="1" Score="1" Tags="&lt;prediction-limit&gt;&lt;tolerance-interval&gt;" Title="Prediction vs. tolerance interval" ViewCount="323" />
  
  
  
  <row Body="&lt;p&gt;You should not exclude outliers just because they cause problems, nor should you use a subset of your data because the full data causes problems. Neither of these solved the &quot;problem&quot; in your case, but even if they did, it wouldn't be right.&lt;/p&gt;&#10;&#10;&lt;p&gt;You haven't given a lot of detail about what you are trying to do or how you are doing it, but can you add reaction time as a covariate? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-13T09:55:54.550" Id="15503" LastActivityDate="2011-09-13T09:55:54.550" OwnerUserId="686" ParentId="15497" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="15512" AnswerCount="1" Body="&lt;p&gt;I have two datasets, A and B, of weighted (x,y) pairs. I computed the best fit lines, L_A and L_B, respectively, of these datasets, and then computed the intersection of these two lines, (x*,y*).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, A and B are drawn from random distributions (of the form y=mx+b+e, where e is a normally distributed error term). I'd like to compute a confidence interval around x*. How can I do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;A naive approach I tried involved computing confidence intervals for the slope/intercept estimates of L_A and L_B, generating approximate distributions for those 4 parameters, and repeatedly randomly sampling from these distributions. But a given line's slope and intercept estimates are not independent of each other, and I wasn't sure how to salvage this approach to account for that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-13T17:07:32.497" FavoriteCount="1" Id="15511" LastActivityDate="2011-09-16T16:26:22.813" OwnerUserId="2221" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;confidence-interval&gt;" Title="Estimating the intersection of two lines" ViewCount="1875" />
  <row AcceptedAnswerId="15521" AnswerCount="1" Body="&lt;p&gt;I have data on tariff rates and a proxy for tariff evasion (that is common in the literature). The data spans a couple of years before the country I'm studying implements a tariff reform and lowers its tariffs. The data also spans many years after the implementation of this tariff reform. Now I'm looking for a suitable way of analyzing the effect this tariff reduction has had on evasion (given it's a good proxy). The hypothesis, perhaps obviously, is that a lowering of the tariff rate will lead to a decrease in evasion. I've read about the difference-in-difference method but here I don't have a &quot;control group&quot;. I'm quite a new practitioner to econometrics so any suggestions will do. I'm thinking that even though the simple set up (sorry for the sloppy notation):  evasion(i) = intercept + B1(tariff(i)) + error(i) were to be run on some year(s) before and after the tariff reform and a reduced effect on tariff is found, there are possibly a number of things that could have influenced this change in evasion other than the tariff reduction. What are the the problems and possibilities with this data-set?  Any suggestions would be much appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks! /Oscar&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-13T21:10:20.790" Id="15518" LastActivityDate="2011-09-13T23:29:04.763" OwnerUserId="5905" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;least-squares&gt;&lt;case-control-study&gt;" Title="Measuring tariff evasion before and after tariff cut" ViewCount="63" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Here is a hint&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider carefully the term $\mathbb P( X \leq z Y \mid z Y &amp;lt; 1 )$. In particular, for concreteness, choose $z = 2$, so that we are considering the event $\mathbb P( X \leq 2 Y \mid  Y &amp;lt; 1/2 )$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, look at this picture (which is very closely related to the above probability).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yM6qg.png&quot; alt=&quot;Conditional probability plot for ratio of uniforms&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Now, does that conditional probability depend on our particular choice of $z$?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-14T01:12:14.827" Id="15523" LastActivityDate="2011-09-14T01:29:20.977" LastEditDate="2011-09-14T01:29:20.977" LastEditorUserId="2970" OwnerUserId="2970" ParentId="15522" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Plots let you tell a story, to spin the data in the way that you want the reader to interpret your results. What's the takeaway message? What do you want to stick in their minds? Determine that message, then think about how to make it into a figure.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your plots, I don't know what message I should learn and you give me too much of the raw data back---I want efficient summaries, not the data themselves.&lt;/p&gt;&#10;&#10;&lt;p&gt;For plot 1, I'd ask, what comparisons do you want to make? The charts that you have illustrate the run times across program for a given computer. It sounds like you want to do the comparisons across computers for a given program. If this is the case, then you want the stats for program a on computer x to be in the same plot as the stats for program a on computer y. I'd put all 8 boxes in your two boxplots in the same figure, ordered ax, ay, bx, by, ... to facilitate the comparison that you are really making.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same goes for plot 2, but I find this plot strange. You are basically showing every data point that you have---a box for each run and a run only has 4 observations. Why not just give me a box plot of total run times for computer x and one for computer y?&lt;/p&gt;&#10;&#10;&lt;p&gt;The same &quot;too much data&quot; critique applies to your last plot as well. Plot 3 doesn't add any new information to plot 2. I can get the overall time if I just multiply the mean time by 4 in plot 2. Here, too, you could plot a box each for computer x and y, but these will literally be multiples of the plot that I proposed to replace plot 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree with @Andy W that computer y isn't that interesting and maybe you want to just state that and exclude it from the plots for brevity (though I think the suggestions that I made can help you trim these plots down). I don't think that tables are very good ways to go, however.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-14T14:24:59.453" Id="15542" LastActivityDate="2011-09-14T14:24:59.453" OwnerUserId="401" ParentId="15529" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Your results certainly suffer from sample selection bias. You do not have a random sample of kids who made friends out of school; you only have a random sample of children (at best; the original sample may have had its own biases). By selecting a subsample for analysis that is based on what the subjects have chosen to do (or were forced to by the parents choice), you confound your &quot;significant correlations&quot; with whatever factors were relevant for the original choice. E.g., in a family with both working parents, the child has to attend an after-school program for pretty much a fixed amount of time, and may be making friends there. Generally, restricting your sample based on the response variable is a very poor idea. Consider this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    x &amp;lt;- runif(1000)&#10;    y &amp;lt;- x + rnorm(1000,mean=0,sd=0.15)&#10;    lm(y~x)&#10;    lm(y[y&amp;gt;0.5]~x[y&amp;gt;0.5])&#10;    lm(y[x&amp;gt;0.5]~x[x&amp;gt;0.5])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The regression coefficient is 0.65-ish instead of 1 in the second regression; there aren't any problems in the second regression, except for lower power, of course.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would analyze the whole data set as a Poisson regression with hours of play as an offset variable, and settings as additional explanatory variables (i.e., they increase proportionaly the rate of &quot;friend-making&quot;). May be as a zero-inflated Poisson regression, if there are any characteristic of the child or the location that preclude them from making friends (e.g., walkable vs. car-only neighborhoods).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-14T18:12:06.587" Id="15554" LastActivityDate="2011-09-14T18:12:06.587" OwnerUserId="5739" ParentId="15541" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;Can somebody clarify how hidden Markov models are related to expectation maximization? I have gone through many links but couldn't come up with a clear view.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-14T18:34:40.560" FavoriteCount="4" Id="15556" LastActivityDate="2011-09-15T00:07:29.687" LastEditDate="2011-09-14T23:51:34.057" LastEditorUserId="88" OwnerUserId="6311" PostTypeId="1" Score="6" Tags="&lt;markov-process&gt;&lt;em-algorithm&gt;&lt;hidden-markov-model&gt;" Title="Hidden Markov models and expectation maximization algorithm" ViewCount="1248" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a set of 50 million text snippets and I would like to create some clusters out of them. The dimensionality might be somewhere between 60k-100k. The average text snippet length would be 16 words. As you can imagine, the frequency matrix would be pretty sparse. I am looking for a software package / libray / sdk that would allow me to find those clusters. I had tried CLUTO in the past but this seems a very heavy task for CLUTO. From my research online I found that BIRCH is an algorithm that can handle such problems, but, unfortunately, I couldn't find any BIRCH implementation software online (I only found a couple of ad-hoc implementations, like assignment projects, that lacked any sort of documentation whatsoever).&#10;Any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Here is some explanation on what &quot;dimensionality&quot; and &quot;similarity&quot; is in the context of my problem:&#10;Each text-snippet is a sentence taken from a huge text corpus (news articles). A vector is created for each sentence by projecting each text snippet onto a N-dimensional space where each &lt;strong&gt;dimension&lt;/strong&gt; corresponds to a word. &#10;So, if a sentence is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;I like red apples more than green apples&quot; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then the corresponding vector would have dimensions &quot;I&quot;:1, &quot;like&quot;:1, &quot;apples&quot;:2 &quot;more&quot;:1 etc...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;I will decide which of the millions of distinct words will make up the space upon which the projection will be made. Typically the top-N most frequent words will be selected for this.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, the &lt;strong&gt;similarity&lt;/strong&gt; will simply be the distance between the vectors of two sentences. (usually I normalize the vectors first and then take the dot product between two vectors - this is known as &quot;cosine similarity&quot;)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-14T22:23:39.840" FavoriteCount="2" Id="15564" LastActivityDate="2011-11-10T07:09:21.713" LastEditDate="2011-11-10T07:09:21.713" LastEditorUserId="7306" OwnerUserId="6317" PostTypeId="1" Score="3" Tags="&lt;clustering&gt;&lt;text-mining&gt;&lt;large-data&gt;" Title="Clustering of 10's of millions of high dimensional data" ViewCount="542" />
  <row AcceptedAnswerId="15709" AnswerCount="1" Body="&lt;p&gt;I have a simple regression with price returns:&lt;/p&gt;&#10;&#10;&lt;p&gt;$r_{t+1} = \alpha + \beta r_t + \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: do I need to do anything if $r_{t+1}$ and $r_t$ are over different horizons?  Suppose the &lt;em&gt;x-variable&lt;/em&gt; is a one month return but the &lt;em&gt;y-variable&lt;/em&gt; is a 1-day return: do I need to adjust the variables to account for the different volatilities?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In response to the current set of questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine that we have two time series $r_{monthly}$ with rolling monthly returns (sampled daily) and $r_{daily}$ with daily returns.  And then we have the simple regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;$r_{daily, t+1} = \alpha + \beta r_{monthly, t} + \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;So we are predicting the next day return with the preceding monthly return.  These returns are of different volatilities (assuming returns are gaussian, by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Volatility_%28finance%29#Volatility_terminology&quot; rel=&quot;nofollow&quot;&gt;square-root of time rule&lt;/a&gt;).  Should I do a transformation to normalize the volatilities before running the regression?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-09-15T02:37:54.603" FavoriteCount="1" Id="15572" LastActivityDate="2011-09-17T23:01:02.777" LastEditDate="2011-09-17T17:09:19.747" LastEditorUserId="988" OwnerUserId="988" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;data-transformation&gt;" Title="Volatility of x and y variables in linear regression?" ViewCount="622" />
  
  
  <row Body="&lt;p&gt;If you have an a priori classification into groups, you should not rely on labels being identical between the a priori classification and the one you obtained. I would start  by computing the distances between the two clusterings (treating the classification  as a clustering) using a metric distance between clusterings. All such metrics can typically be derived from the confusion matrix only, and hence do not depend on labels beyond their indicating commonality of grouping within a single clustering. I usually recommend &lt;em&gt;Comparing clusterings by the variation of information&lt;/em&gt; by Marina Meila. It discusses three metrics: the main contribution of the paper, the variation of information (which is very good), the Mirkin distance (related to the Jaccard index, well known, but not so good as it is affected in a quadratical manner by cluster sizes), and the split/join distance (Meila calls it 'van Dongen' distance). Disclaimer: the last one was developed by me. It has the advantage that it is interpretable as the number of nodes that need reallocation to change one clustering or classification into the other. There are many other clustering (dis)similarity measures, but I would only recommend these three, and although popular, I would not recommend the Jaccard/Mirkin measures.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-15T14:05:42.893" Id="15600" LastActivityDate="2011-09-15T14:05:42.893" OwnerUserId="4495" ParentId="15548" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="15610" AnswerCount="2" Body="&lt;p&gt;I'm reviewing a paper where the authors compare the survival of an insect fed on three different diets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following the survival analysis (using &lt;code&gt;Surv&lt;/code&gt;) they perform multiple comparisons of the mean survival among the treatments using the contrasts procedure from Crawley's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470510242&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;The R book&lt;/em&gt;&lt;/a&gt;. They present their results &lt;em&gt;a la&lt;/em&gt; a Tukey test (i.e., means with significantly different values have different letters).&lt;/p&gt;&#10;&#10;&lt;p&gt;Comparisons between 2 treatment levels in R can be done with &lt;code&gt;survdiff&lt;/code&gt;, but that generates a chi-square statistic which doesn't jive with what I'm seeing in the paper&lt;/p&gt;&#10;&#10;&lt;p&gt;The more general question of post-hoc tests in survival analysis when there are &gt;2 treatment levels (akin to Tukey's test) has been asked on R-help before but never answered.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is twofold: I'm not familiar with survival analysis to know if the contrast procedure is appropriate to the question, and I don't have access to Crawley's book. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone provide an example of Crawey's method or point to a web resource where his method is explained? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-15T14:40:56.727" Id="15603" LastActivityDate="2011-09-17T14:06:17.097" LastEditDate="2011-09-15T18:38:40.953" LastEditorUserId="1475" OwnerDisplayName="Chris" OwnerUserId="1475" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;survival&gt;" Title="Performing contrasts among treatment levels in survival analysis" ViewCount="1205" />
  <row Body="&lt;p&gt;The answer to your question is &quot;Yes, why not?&quot;  @Nick answered this for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first part of your interpretation is correct. The second part is not correct. You have no indication, from the above, of how much ice cream or yogurt men and women buy. The strong correlations could be over different ranges. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-15T15:07:01.360" Id="15604" LastActivityDate="2011-09-15T15:07:01.360" OwnerUserId="686" ParentId="15598" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Suppose you have ID numbers in column A, from row 2 through 604, and a blank column B.  In cell B2 type &quot;=countif(\$a\$2:\$a\$604,a2)&quot;.  Copy that formula all the way down.  The number in each cell in column B will now be the number of times that its row's ID appears.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-15T17:44:04.870" Id="15609" LastActivityDate="2011-09-15T22:18:23.670" LastEditDate="2011-09-15T22:18:23.670" LastEditorUserId="919" OwnerUserId="2669" ParentId="15581" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;whuber indicates that this is a frequency, which is the mean number of events at an instant. It is also a proportion, which is another perspective on a mean. The math is the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps the someone was referring to the median frequency, which could also be termed the median proportion or, more confusingly, the median mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example: You measure each day the proportion of restaurant patrons who are female. This gives you a proportion/mean for each day. Perhaps you are interested in what the proportion is for a typical day. Maybe you do the median of this proportion rather than the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the example above, you arbitrarily chose median. If you are doing non-parametric statistics, then you would need to use the median.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-15T23:17:47.323" Id="15617" LastActivityDate="2011-09-15T23:17:47.323" OwnerUserId="3874" ParentId="15616" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Multiply $X^\prime X$:&#10;$$\begin{align*} \left[ \begin{array}{cc} 1^\prime 1 &amp;amp; 1^\prime x \\ x^\prime 1 &amp;amp; x^\prime x \end{array} \right] \end{align*}$$&#10;Take the inverse (&lt;a href=&quot;http://en.wikipedia.org/wiki/Invertible_matrix#Inversion_of_2.C3.972_matrices&quot; rel=&quot;nofollow&quot;&gt;see here&lt;/a&gt;):&#10;$$\begin{align*} \frac{1}{Nx^\prime x - (1^\prime x)^2}\left[ \begin{array}{cc} x^\prime x &amp;amp; -1^\prime x \\ -1^\prime x &amp;amp; N \end{array} \right] \end{align*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that&#10;$$\begin{align*}\sum{(x_i - \bar{x})^2} = \sum{x_i^2} - N \bar{x}^2.\end{align*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the lower right corner of the matrix, we have&#10;$$\begin{align*}\frac{N}{N\sum{x_i^2} - \left(\sum{x_i}\right)^2}=
&#10;\frac{1}{\sum{x_i^2} - \frac{1}{N}\left(\sum{x_i}\right)^2} =       \frac{1}{\sum{x_i^2} - N \bar{x}^2} = \frac{1}{\sum{(x_i - \bar{x})^2}}.\end{align*}$$&#10;Multiplying by $\sigma^2$ gives the variance of the slope coefficient.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-16T06:27:24.790" Id="15632" LastActivityDate="2011-09-16T14:44:29.640" LastEditDate="2011-09-16T14:44:29.640" LastEditorUserId="401" OwnerUserId="401" ParentId="15627" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;In case of large sample sizes, the significance test is the same for both indexes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$t = \sqrt{\frac{(n-2)}{(1-r^2)}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So you can generally treat both indexes the same way and test their difference for significance. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, things might be a bit complicated, as they are non-independent correlations (i.e., both are derived from the same sample). There have been articles by Steiger (1980, [1]) and Meng et al. (1992, [2]) which treat this issue. In the cases covered there, however, it is always a correlation between one variable $x$ and two other variables $y$ and $z$ (i.e., comparing $r_{xy}$ with $r_{xz}$), which is not exactly your case.&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Steiger, J. H. (1980). &lt;a href=&quot;http://dionysus.psych.wisc.edu/coursewebsites/Psy710/Readings/SteigerJ1980a.pdf&quot; rel=&quot;nofollow&quot;&gt;Tests for comparing elements of a correlation matrix.&lt;/a&gt; Psychological Bulletin, 87, 245-251.&lt;/p&gt;&#10;&#10;&lt;p&gt;[2] Meng, X. L., Rosenthal, R., &amp;amp; Rubin, D. B. (1992). Comparing correlated correlation coefficients. Psychological Bulletin, 111, 172-175.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-16T06:43:20.613" Id="15634" LastActivityDate="2011-09-16T07:21:01.113" LastEditDate="2011-09-16T07:21:01.113" LastEditorUserId="6082" OwnerUserId="6082" ParentId="15631" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="15637" AnswerCount="1" Body="&lt;p&gt;I'm looking to cluster a small data set (64 observations of 4 interval variables and a single three-factor categorical variable).  Now, I'm quite new to cluster analysis, but I am aware that there has been considerable progress since the days when hierarchical clustering or k-means were the only available options.  In particular, it seems that new methods of model based clustering are available that, as &lt;a href=&quot;http://stats.stackexchange.com/questions/8148/assumptions-of-cluster-analysis/11475#11475&quot;&gt;pointed out by chl&lt;/a&gt;, enable the use of &quot;goodness-of-fit indices to decide about the number of clusters or classes&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the standard R package for model based clustering &lt;code&gt;mclust&lt;/code&gt; apparently will not fit models with mixed data types.  The &lt;code&gt;fpc&lt;/code&gt; model will, but has trouble fitting a model, I suspect because of the non-gaussian nature of the continuous variables.  Should I continue with the model-based approach?  I'd like to continue to use R if possible.  As I see it, I have a few options:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Convert the three-level categorical variable into two dummy variables and use &lt;code&gt;mclust&lt;/code&gt;.  I'm unsure if this will bias the results, but if not this is my preferred option.&lt;/li&gt;&#10;&lt;li&gt;Transform the continuous variables somehow and use the &lt;code&gt;fpc&lt;/code&gt; package.&lt;/li&gt;&#10;&lt;li&gt;Use some other R package I haven't yet encountered.&lt;/li&gt;&#10;&lt;li&gt;Create a dissimilarity matrix using Gower's measure and use traditional hierarchical or relocation cluster techniques.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Does the stats.se hivemind have any suggestions here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-16T07:15:21.377" FavoriteCount="2" Id="15635" LastActivityDate="2011-09-16T07:48:53.440" OwnerUserId="179" PostTypeId="1" Score="5" Tags="&lt;clustering&gt;&lt;mixture&gt;&lt;model-based-clustering&gt;" Title="Robust cluster method for mixed data in R" ViewCount="1806" />
  <row Body="&lt;p&gt;AIC and BIC are information criteria for comparing models. Each tries to balance model fit and parsimony and each penalizes differently for number of parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;AIC is Akaike Information Criterion the formula is AIC = 2k - 2ln(L) where k is number of parameters and L is likelihood; with this formula, smaller is better. (I recall that some programs output the opposite 2Ln(L) - 2k, but I don't remember the details)&lt;/p&gt;&#10;&#10;&lt;p&gt;BIC is Bayesian Information Criterion, the formula is BIC = k ln(k) - 2ln(L).  It favors more parsimonious models than AIC&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't heard of KIC.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-09-16T09:48:21.247" Id="15643" LastActivityDate="2012-07-14T11:56:23.773" LastEditDate="2012-07-14T11:56:23.773" LastEditorDisplayName="user10525" OwnerUserId="686" ParentId="577" PostTypeId="2" Score="3" />
&#10; R^2 = \frac{\sum{\left(\hat{y}_i - \bar{y}\right)^2}}{\sum{\left(y_i - \bar{y}\right)^2}} = 1 - \frac{\sum{e_i^2}}{\sum{\left(y_i - \bar{y}\right)^2}}.
  <row Body="&lt;p&gt;For interactively exploring higher dimensional cluster solutions I have used &lt;a href=&quot;http://www.ggobi.org/&quot; rel=&quot;nofollow&quot;&gt;GGobi&lt;/a&gt; in the past, it may also be appropriate to your case.&lt;/p&gt;&#10;&#10;&lt;p&gt;It takes some time getting used to, but it interfaces nicely with R and runs on GNU/Linux.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-16T15:09:58.550" Id="15660" LastActivityDate="2011-09-16T15:09:58.550" OwnerUserId="4978" ParentId="15655" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The idea that the t-test is only for small samples is a historical hold over.  Yes it was originally developed for small samples, but there is nothing in the theory that distinguishes small from large.  In the days before computers were common for doing statistics the t-tables often only went up to around 30 degrees of freedom and the normal was used beyond that as a close approximation of the t distribution.  This was for convenience to keep the t-table's size reasonable.  Now with computers we can do t-tests for any sample size (though for very large samples the difference between the results of a z-test and a t-test are very small).  The main idea is to use a t-test when using the sample to estimate the standard deviations and the z-test if the population standard deviations are known (very rare).&lt;/p&gt;&#10;&#10;&lt;p&gt;The Central Limit Theorem lets us use the normal theory inference (t-tests in this case) even if the population is not normally distributed as long as the sample sizes are large enough.  This does mean that your test is approximate (but with your sample sizes, the appromition should be very good).&lt;/p&gt;&#10;&#10;&lt;p&gt;The Wilcoxon test is not a test of means (unless you know that the populations are perfectly symmetric and other unlikely assumptions hold).  If the means are the main point of interest then the t-test is probably the better one to quote.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that your standard deviations are so different, and the shapes are non-normal and possibly different from each other, the difference in the means may not be the most interesting thing going on here.  Think about the science and what you want to do with your results.  Are decisions being made at the population level or the individual level?  Think of this example: you are comparing 2 drugs for a given disease, on drug A half the sample died immediatly the other half recovered in about a week; on drug B all survived and recovered, but the time to recovery was longer than a week.  In this case would you really care about which mean recovery time was shorter?  Or replace the half dying in A with just taking a really long time to recover (longer than anyone in the B group).  When deciding which drug I would want to take I would want the full information, not just which was quicker on average.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-16T16:38:32.107" Id="15667" LastActivityDate="2011-09-16T16:38:32.107" OwnerUserId="4505" ParentId="15664" PostTypeId="2" Score="10" />
  
  
  
  <row Body="&lt;p&gt;In the R &lt;code&gt;rms&lt;/code&gt; package there is are wrapper function for the survival package's &lt;code&gt;coxph&lt;/code&gt; and &lt;code&gt;survreg&lt;/code&gt; functions.  When you use one of these two functions you can use &lt;code&gt;contrast.rms&lt;/code&gt; to easily obtain single d.f. or multiple d.f. contrasts.  Type &lt;code&gt;?contrast.rms&lt;/code&gt; for guidance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-17T14:06:17.097" Id="15700" LastActivityDate="2011-09-17T14:06:17.097" OwnerUserId="4253" ParentId="15603" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;No - you cannot say &quot;the sample has a normal distribution&quot; or &quot;the sample comes from a population which has a normal distribution&quot;, but only &quot;you cannot reject the hypothesis that the sample comes from a population which has a normal distribution&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact the sample does not have a normal distribution (see the &lt;em&gt;qqplot&lt;/em&gt; below), but you would not expect it to as it is only a sample.  The question as to the distribution of the underlying population remains open.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qqnorm( c(0.269, 0.357, 0.2, 0.221, 0.275, 0.277, 0.253, 0.127, 0.246) )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Nk6Lk.png&quot; alt=&quot;qqplot&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-17T16:01:14.307" Id="15702" LastActivityDate="2011-09-17T16:01:14.307" OwnerUserId="2958" ParentId="15696" PostTypeId="2" Score="11" />
  <row Body="&lt;h2&gt;Sampling Method&lt;/h2&gt;&#10;&#10;&lt;p&gt;I would like to expand the problem mentioned by Henry. Let's just point out a few problems that may arise:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;People tend to use the supermarket that is close to their homes. As you know, there are different types of people in different areas - and I would expect that the goods bought highly depend on personal education and financial background. &lt;/li&gt;&#10;&lt;li&gt;There are supermarkets with higher prices - it is likely that those supermarkets are visited by different people than the discount supermarkets.&lt;/li&gt;&#10;&lt;li&gt;Mothers with children probably buy at different times than people that do work full-time. But do they buy the same?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In statistical terminology, you will have to take care of the sampling method you use. With every random sample you can make a guess at the distribution it was taken from. However, you have to take care that you take the sample from the &lt;em&gt;correct&lt;/em&gt; distribution and not a special subset.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Sample sizes&lt;/h2&gt;&#10;&#10;&lt;p&gt;Firstly, on your wording: The population we are talking about is the set of all items that can be bought, not the people living in austria. The population always denotes the possible outcomes of one random sample - and you are observing items bought. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is hard to tell if you will get enough samples - this will depend on the number of customers you will be able to observe, as well as it will depend on the amount of different goods you record. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's have a look at two (very constructed) examples. Say, you record 1000 people each buying exactly one item. In extreme cases, the following might happen:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;All people buy the same product, let's say it is milk. In this case, your sample size should certainly be big enough to conclude that milk is one of the top sold products.&lt;/li&gt;&#10;&lt;li&gt;Everybody buys something different. Then, with this sample size, it will be impossible to determine the most sold product. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This shows that the sample size you have to take on a huge amount depends on the &lt;em&gt;variance&lt;/em&gt; you encounter in your data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, the sample sizes depend on the statistical method you will use. Usually, the more you can &lt;em&gt;assume&lt;/em&gt; on your distribution, the stronger the method you can use. For example, if you can assume a normal distribution, you may use parametric tests that usually do not need a lot of samples. This is not surprising, as you put a lot of information (normality) as a guess in your data, which leaves only a little bit of information to be determined by the data. However, if you have no information on the distribution, the test will have to guess &lt;em&gt;everything&lt;/em&gt;. This naturally means more information will be needed beforehand.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is why often small sample sizes are taken as a pre-study. Afterwards, you will have a feeling on the variance and will be able to determine the statistical methods that will be used as well as their requirements in terms of sample sizes. &lt;/p&gt;&#10;&#10;&lt;h2&gt;Further Additions&lt;/h2&gt;&#10;&#10;&lt;p&gt;Finally, you should define the groups you are looking for. Is the manufacturer of something important to you? Will you just group 'Cheese', or will there be different groups of cheese? &lt;/p&gt;&#10;&#10;&lt;p&gt;How would I do it? This really depends on my intention. Do I have a budget? Do I have multiple people taking samples? Maybe there are supermarkets that offer me their product statistics. Maybe it would be an idea to interview the people you recorded to identify differences in personal background. Then you could check whether this differences influence the output. Furthermore, it might be worth doing a small study first to identify further problems that may arise with sampling and data recording.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you are looking at Austria, I assume you speak German. Which means I can point you to a book that I do not yet have read in total but that might bring up a lot of questions relevant to your problem. It is called &quot;Stichproben&quot; by Kauermann and Küchenhoff. Sorry for all the english readers around here, I do not know an english book about that topic...&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-09-17T18:31:00.537" Id="15704" LastActivityDate="2011-09-18T07:37:14.130" LastEditDate="2011-09-18T07:37:14.130" LastEditorUserId="6246" OwnerUserId="6246" ParentId="15701" PostTypeId="2" Score="5" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I often see in various tutorials and webpages that explains fuzzy c-means clustering and soft k-means clustering. But i was not able to find any material which differentiates them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it that both fuzzy c-means and soft k-means clustering are same or different?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-19T06:51:32.293" Id="15738" LastActivityDate="2011-11-19T17:29:31.717" LastEditDate="2011-09-19T08:41:05.863" LastEditorUserId="88" OwnerUserId="4290" PostTypeId="1" Score="3" Tags="&lt;clustering&gt;" Title="Difference between soft k-means and fuzzy c-means" ViewCount="1191" />
  
  
  <row Body="&lt;p&gt;Big things I tend to check:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Variable type - to see that a number is numeric, and not factor/character (might indicate some problem with the data that was entered)&lt;/li&gt;&#10;&lt;li&gt;Consistent value levels - to see that a variable with the name &quot;t1&quot; didn't find it self again with the name &quot;t1 &quot; or &quot;t 1&quot;&lt;/li&gt;&#10;&lt;li&gt;Outliers - see that the ranges of value make sense. (did you get a blood pressure value of 0? or minus?).  Here we sometimes find out that someone encoded -5 as missing value, or something like that.&lt;/li&gt;&#10;&lt;li&gt;Linear restrictions.  I don't use that, but some find that they wish to have restructions on the dependencies of some columns (columns A, B must add to C, or something like that).  For this you can have a look at the &lt;a href=&quot;http://www.warwick.ac.uk/statsdept/useR-2011/abstracts/080311-vanderloomark.pdf&quot;&gt;deducorrect package&lt;/a&gt; (I met the speaker, Mark van der Loo, in the last useR conference - and was very impressed with his package)&lt;/li&gt;&#10;&lt;li&gt;too little randomness.  Sometimes values got to be rounded to some values, or truncated at some point.  These type of things are often more clear in scatter plots.&lt;/li&gt;&#10;&lt;li&gt;Missing values - making sure that the missing is not related to some other variable (missing at random).  But I don't have a rule of thumb to give here.&lt;/li&gt;&#10;&lt;li&gt;Empty rows or rows with mostly no-values.  These should be (usually) found and omitted.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Great question BTW - I hope to read other people's experience on the matter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-19T08:44:43.440" Id="15743" LastActivityDate="2011-09-19T08:44:43.440" OwnerUserId="253" ParentId="11659" PostTypeId="2" Score="9" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for a test for circular data that is equivalent to linear repeated measures ANOVA (I have an experiment using human participants where the same sample of participants perform multiple experimental conditions and the dependent variable is an angle).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed 3 tests for performing ANOVA on circular data:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Watson &amp;amp; Williams (1956), &lt;/li&gt;&#10;&lt;li&gt;Harrison &amp;amp; Kanji (1986), &lt;/li&gt;&#10;&lt;li&gt;Anderson &amp;amp; Wu (1995)&#10;&lt;a href=&quot;http://fmatoolbox.sourceforge.net/API/FMAToolbox/General/CircularANOVA.html&quot;&gt;http://fmatoolbox.sourceforge.net/API/FMAToolbox/General/CircularANOVA.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Do these tests require independent samples (as I suspect is the case for the Watson-Williams test) or can they be used on repeated measures data? If the former, is there another test that does what I desire?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-20T11:17:46.510" FavoriteCount="1" Id="15791" LastActivityDate="2012-05-12T19:16:59.507" LastEditDate="2011-09-20T11:52:20.807" LastEditorUserId="930" OwnerUserId="6389" PostTypeId="1" Score="5" Tags="&lt;repeated-measures&gt;&lt;directional-statistics&gt;" Title="Repeated measures ANOVA for circular / angular / directional data" ViewCount="1017" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;I can be wrong but it seems to me there's no sense in updating model and scores for the subsample that you will be &lt;em&gt;not&lt;/em&gt; taking in your future panel. First, it is unlikely that changing behaviours of those 1000 you select for each next wave to continue with may influence behaviours of the rest, whom you don't select, since every one of your sample lives their own lives: you don't mean to model &lt;em&gt;society dynamics&lt;/em&gt; (with its mass-media feedbacks etc.). Second, even if you mean it, future reactions of the 1000 should update those past behaviours of the rest of the sample - the behaviours that were predictors of scores in your initial model; but they can't: the rest are all &quot;dead&quot; by the time of follow-up with those 1000 and their &quot;past&quot; is frozen forever. So, I believe you should just go on with selected 1000 and do future re-modelling/re-scoring only with them. Of course, important variables such as &quot;recency&quot; should be properly taken into account.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-21T10:00:52.803" Id="15835" LastActivityDate="2011-09-21T10:15:57.667" LastEditDate="2011-09-21T10:15:57.667" LastEditorUserId="3277" OwnerUserId="3277" ParentId="15826" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="15852" AnswerCount="5" Body="&lt;p&gt;This is a bit more &quot;think about it&quot; question - but I see it as an important one to ask.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been struggling for the past few days with having a more reproducible-research-like workflow.  I am confused with the two different strategies for writing a report.&lt;/p&gt;&#10;&#10;&lt;p&gt;The two strategies are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Sweave or brew. Where there is a report.Rnw or report.brew file that has a mixture of some markup language (either HTML or LaTeX) and R code between special braces (say &amp;lt;&amp;lt;&gt;&gt;=  @).  This file needs to be run through Sweave or brew in order to create the report file (report.html or report.tex).&lt;/li&gt;&#10;&lt;li&gt;R2HTML (for HTML) and Hmisc (for LaTeX). Where the .r file uses R functions to construct report.html or report.tex; running the R commands generates the report directly.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What is clear to me is that most people online seem to be using option 1.  But I do not understand why it is so common, when option 2 seems to me (without too much experimenting) to be less work.&lt;/p&gt;&#10;&#10;&lt;p&gt;When is each of the two strategies better?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas/feedback/thoughts would be welcome,  Thanks.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-21T12:30:57.870" FavoriteCount="2" Id="15840" LastActivityDate="2011-09-23T15:09:53.907" LastEditDate="2011-09-23T15:09:53.907" LastEditorUserId="3601" OwnerUserId="253" PostTypeId="1" Score="5" Tags="&lt;reproducible-research&gt;&lt;sweave&gt;&lt;latex&gt;" Title="Comparing reproducible research strategies: brew or Sweave vs. R2HTML" ViewCount="710" />
  <row AnswerCount="3" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/3038/testing-hypothesis-of-no-group-differences&quot;&gt;Testing hypothesis of no group differences&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Suppose I have $k$ samples from 2 independent experiments (service times by 2 methods) and their means are similar. How do I statistically show that both methods have similar service times?&lt;/p&gt;&#10;" ClosedDate="2011-09-23T15:11:02.610" CommentCount="5" CreationDate="2011-09-21T12:42:48.653" FavoriteCount="2" Id="15841" LastActivityDate="2011-09-22T04:06:20.480" OwnerUserId="562" PostTypeId="1" Score="4" Tags="&lt;anova&gt;&lt;mean&gt;" Title="How do you show that two populations are statistically similar?" ViewCount="6101" />
  
  <row Body="&lt;p&gt;When it comes to sample size, bigger is better, but we often have to take what we get.  With the smaller sample sizes, your estimates of the correlation are going to become extremely noisy, and comparisons between different estimates (which I expect is your primary goal in the subsets analyses) are going to be particularly noisy.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.miislita.com/information-retrieval-tutorial/a-tutorial-on-standard-errors.pdf&quot; rel=&quot;nofollow&quot;&gt;This online tutorial on standard errors (as a pdf)&lt;/a&gt; contains formulas for the SE of the correlation coefficient (as well as of the Fisher transformation of the correlation, which is a better scale to be measuring the SE). You'll see that the scales by approximately $1/\sqrt{n}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a correlation of about 0.5, the SE with a sample size of 200 will be about 0.06; with a sample size of 50 it will be about double that.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-21T14:55:26.907" Id="15848" LastActivityDate="2011-09-21T14:55:26.907" OwnerUserId="5862" ParentId="15842" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;One approach would be to think in terms of pairwise comparisons and apply the &lt;a href=&quot;http://en.wikipedia.org/wiki/Pairwise_comparison&quot; rel=&quot;nofollow&quot;&gt;Bradley Terry model&lt;/a&gt;.  A person who ranks the items as D,B,C,A would be equivalent of saying D beats B, D beats C, D beats, A, B beats C, B beats A, and C beats A.  Then another person with a different order would result in a different set of pairwise comparisons.  You would then combine all the pairwise comparisons and get a set of rankings of the 4 choices. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-21T18:39:44.823" Id="15859" LastActivityDate="2011-09-21T18:39:44.823" OwnerUserId="4505" ParentId="15849" PostTypeId="2" Score="4" />
  
  <row AnswerCount="4" Body="&lt;p&gt;I have a question about confidence intervals.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In general, are confidence intervals open or closed?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2011-09-21T22:15:21.867" Id="15872" LastActivityDate="2012-08-09T01:17:48.550" OwnerUserId="6424" PostTypeId="1" Score="9" Tags="&lt;mathematical-statistics&gt;" Title="Are confidence intervals open or closed intervals?" ViewCount="906" />
&#10;&amp;amp;= \text{E}[y_i \mid x_i = 0] + \big[\text{E}[y_i \mid x_i=1] - \text{E}[y_i \mid x_i=0]\big] x_i \\
  <row AcceptedAnswerId="15907" AnswerCount="2" Body="&lt;p&gt;I'm using an arcane free program off the internet called &quot;stable.exe&quot; trying to fit a stable distribution curve to a dataset, but I'm having trouble entering the dataset file into the program.  When the program asks me to input the file, I enter &quot;stable test 1&quot;, which is the name of the dataset (a notepad document consisting of a column of 20 numbers, in the same working directory as the program).  It then spits out an &quot;error reading file&quot; message.  I've tried specifying the directory as &lt;code&gt;C:\...etc...\stable test 1&lt;/code&gt;, and so on, to no avail.  If someone has had experience with this nifty yet somewhat enigmatic program, or has fortran knowledge in general, I would appreciate your advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, the read me file for the program says that the program will not read the last number in the dataset unless I include a &quot;carriage return&quot; after it.  Can someone explain what a &quot;carriage return&quot; is.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-21T23:19:10.297" Id="15879" LastActivityDate="2011-09-22T08:19:36.173" LastEditDate="2011-09-22T08:09:55.863" LastEditorUserId="2116" OwnerUserId="6146" PostTypeId="1" Score="0" Tags="&lt;software&gt;&lt;cdf&gt;&lt;pareto-distribution&gt;" Title="Trouble using pareto levy stable distribution software" ViewCount="205" />
  <row Body="&lt;p&gt;This is not a cartoon, but a joke worth mentioning:&lt;/p&gt;&#10;&#10;&lt;p&gt;A statistic professor travels to a conference by plane. When he passes the security check, they discover a bomb in his carry-on-baggage. Of course, he is hauled off immediately for interrogation. &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;I don't understand it!&quot; the interrogating officer exclaims. &quot;You're an accomplished professional, a caring family man, a pillar of your parish - and now you want to destroy that all by blowing up an airplane!&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Sorry&quot;, the professor interrupts him. &quot;I had never intended to blow up the plane.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;So, for what reason else did you try to bring a bomb on board?!&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Let me explain. Statistics shows that the probability of a bomb being on an airplane is 1/1000. That's quite high if you think about it - so high that I wouldn't have any peace of mind on a flight.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;And what does this have to do with you bringing a bomb on board of a plane?&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;You see, since the probability of one bomb being on my plane is 1/1000, the chance that there are two bombs is 1/1000000. This way I am much safer...&quot;&lt;/p&gt;&#10;" CommentCount="8" CommunityOwnedDate="2011-09-22T01:48:05.797" CreationDate="2011-09-22T01:48:05.797" Id="15886" LastActivityDate="2013-01-17T10:41:19.890" LastEditDate="2013-01-17T10:41:19.890" LastEditorUserId="5509" OwnerUserId="5509" ParentId="423" PostTypeId="2" Score="14" />
  
  
  <row Body="&lt;p&gt;The class of the output of &lt;code&gt;lme&lt;/code&gt; is, not surprisingly, &lt;code&gt;lme&lt;/code&gt;. You can find this by running &lt;code&gt;class(fm2orth.lm)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a consequence, when you call &lt;code&gt;summary&lt;/code&gt; on it, what is really called is &lt;code&gt;summary.lme&lt;/code&gt;. This function is not exported from the nlme package (you can discover this when you type &lt;code&gt;summary.lme&lt;/code&gt; at the prompt: you get a message that the object is not found), but you can still get to its source like this: &lt;code&gt;getAnywhere(&quot;summary.lme&quot;)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You will see there some of the objects of the summary are built up and what name they are given (e.g. &lt;code&gt;object$tTable &amp;lt;- ...&lt;/code&gt;) You will also see that the result of &lt;code&gt;summary.lme&lt;/code&gt; is an object of class &lt;code&gt;summary.lme&lt;/code&gt; (again, no surprises there).&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you can check &lt;code&gt;print.summary.lme&lt;/code&gt;, because &lt;code&gt;print&lt;/code&gt; is what is automatically called by R to display the return value of the last statement, but like &lt;code&gt;summary&lt;/code&gt;, it is dispatched to a version that is class-specific. Note, once again, this function is not exported from the package, so you'll have to use &lt;code&gt;getAnywhere&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, in this function's source, you'll see a call &lt;code&gt;cat(&quot;Fixed effects: &quot;)&lt;/code&gt; that obviously marks the start of printing the information you're interested in. There you can see that what is really printed (&lt;code&gt;cat(deparse)&lt;/code&gt;), is based upon the summary's member &lt;code&gt;call$fixed&lt;/code&gt; but may depend on the way the original model was built.&lt;/p&gt;&#10;&#10;&lt;p&gt;This should give you enough info to find out how to get to it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: I just noticed that the object returned from &lt;code&gt;summary.lme&lt;/code&gt; is really the orginal fit (so the return value of &lt;code&gt;lme&lt;/code&gt; itself, with some extra list items added to it. As such, this &lt;code&gt;call&lt;/code&gt; member used in &lt;code&gt;print.summary.lme&lt;/code&gt; is already part of the original fit (, since it is not added by &lt;code&gt;summary.lme&lt;/code&gt;). Result: you probably don't really need the summary to get to the fixed effects.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-22T07:26:07.527" Id="15901" LastActivityDate="2011-09-22T07:53:24.377" LastEditDate="2011-09-22T07:53:24.377" LastEditorUserId="4257" OwnerUserId="4257" ParentId="15867" PostTypeId="2" Score="2" />
  
&#10;3 &amp;amp; 32 &amp;amp; 33 &amp;amp; 34 \\ 
  <row AcceptedAnswerId="15935" AnswerCount="2" Body="&lt;p&gt;There are 13 subjects in this study, each subject was &quot;repeatedly measured&quot; right and left legs on five walking conditions. The variables of this data set is: ID, Y, Leg, Conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The research questions are to find the difference between legs and conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;At beginning, I plan to use RM-ANOVA: $Y = \text{Leg} + \text{Conditions} + \text{Leg}\times\text{Conditions}$. But the RM-ANOVA might not be appropriate because right and left legs are also correlated (from same subject). &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have an idea how to analyze this type of data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-22T18:49:40.140" Id="15934" LastActivityDate="2011-09-22T22:31:24.910" LastEditDate="2011-09-22T22:31:24.910" LastEditorUserId="183" OwnerUserId="4559" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;repeated-measures&gt;" Title="How to analyse a study where measures are taken repeatedly from right and left legs in five walking conditions?" ViewCount="113" />
  
  <row AcceptedAnswerId="15946" AnswerCount="1" Body="&lt;p&gt;I have data from an experiment where I applied two different treatments in identical initial conditions, producing an integer between 0 and 500 in each case as the outcome. I want to use a paired t-test to determine whether the effects produced by the two treatments are significantly different. The outcomes for each treatment group are normally distributed, but the &lt;em&gt;difference&lt;/em&gt; between each pair is &lt;em&gt;not&lt;/em&gt; normally distributed (asymmetric + one long tail). &lt;/p&gt;&#10;&#10;&lt;p&gt;Can I use a paired t-test in this case, or is the assumption of normality violated, meaning I should use a non-parametric test of some kind?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-09-22T19:56:34.800" FavoriteCount="1" Id="15940" LastActivityDate="2011-09-23T12:27:45.320" LastEditDate="2011-09-23T12:27:45.320" LastEditorUserId="449" OwnerUserId="6446" PostTypeId="1" Score="8" Tags="&lt;t-test&gt;&lt;normality&gt;" Title="Can I use a paired t-test when the samples are normally distributed but their difference is not?" ViewCount="2912" />
  
  <row Body="&lt;p&gt;A paired t test only analyzes the list of paired differences, and assumes that sample of values is randomly sampled from a Gaussian population. If that assumption is grossly violated, the paired t test is not valid. The distribution from which the before and after values are samples is irrelevant -- only the population the differences are sampled from matters.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-22T20:40:46.613" Id="15946" LastActivityDate="2011-09-22T20:40:46.613" OwnerUserId="25" ParentId="15940" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;If you have a fair coin: $(1/2)^{13}$. $1/2$ that you have the first correct, multiplied by $1/2$ that you have the second correct, ...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-23T08:39:37.033" Id="15962" LastActivityDate="2011-09-23T08:39:37.033" OwnerUserId="2732" ParentId="15936" PostTypeId="2" Score="2" />
  
  
  
&#10;$$&#10;except that, instead of minimizing the sum of squared residuals,&#10;$$
&#10;$$&#10;I want to minimize&#10;$$
  <row AcceptedAnswerId="16016" AnswerCount="1" Body="&lt;p&gt;Suppose I have the conditional probabilities of $B$ given $A$ (both categorical): $$\Pr(B|A).$$ The probabilities are given as a contingency table.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using these conditional probabilities, I can fudge a $B$ column into a dataset that has only an $A$ column. For each row in the dataset, I choose a value for $B$ at random based on $P(B|A)$ for the value of $A$ of the current row.&lt;/p&gt;&#10;&#10;&lt;p&gt;From my understanding, this can be seen as a model, although a quite simple one. Adding the $B$ column could be thought of as &quot;simulating&quot; the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question: Is the term &quot;model&quot; applicable in this case? If so, how would such a model be called?&lt;/p&gt;&#10;&#10;&lt;p&gt;Somebody mentioned the term &quot;Poisson distribution&quot; in this context, but I was unable to align these two concepts with my limited knowledge of statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also &lt;a href=&quot;http://stats.stackexchange.com/questions/15913/add-a-column-to-a-dataframe-based-on-a-probability-distribution&quot;&gt;this question&lt;/a&gt; on how to implement this in R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-23T22:59:48.697" FavoriteCount="1" Id="15995" LastActivityDate="2011-09-25T06:38:01.803" LastEditDate="2011-09-25T06:38:01.803" LastEditorUserId="6432" OwnerUserId="6432" PostTypeId="1" Score="1" Tags="&lt;modeling&gt;&lt;terminology&gt;&lt;conditional-probability&gt;" Title="Is there a name for models that are based on a given conditional distribution only?" ViewCount="92" />
  <row Body="&lt;p&gt;The procedure you describe is only partially correct. It only gives you the correct parameter estimates, i.e. the slopes. That is what happened in your case. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the covariance matrix obtained by this approach is not correct. This is why your &quot;betas&quot; (standardized coefficients) and your t statistics diverge with respect to the correct two-stage approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem arises in the second stage where you regress $y$ on the predicted $x$. You implicitly assume that $x$ is known, but in reality it is estimated $x$. The two-stage least squares estimator takes this into account, whereas the manual procedure you have chosen does not do that. &lt;/p&gt;&#10;&#10;&lt;p&gt;For this reason, you are generally advised to avoid the procedure you have described and to use the correct two-stage least squares procedures. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-24T07:33:53.550" Id="16006" LastActivityDate="2011-09-24T07:33:53.550" OwnerDisplayName="user5644" ParentId="16001" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;2SLS speaking generally is a sequential OLS regression (I --&gt; X'; X' --&gt; Y'), but in contrast with sequential regression performed by hand it computes standard errors from initial X (predictor), not from X' (X as predicted by I, i.e. its direct image), because we are interested in the model X --&gt; Y, not model I --&gt; Y. Thus and so only &lt;strong&gt;b&lt;/strong&gt; coincides between 2SLS and explicit sequential regression; &lt;strong&gt;beta&lt;/strong&gt;, &lt;strong&gt;p-value&lt;/strong&gt;, &lt;strong&gt;R-square&lt;/strong&gt; - they are different.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-24T08:58:32.243" Id="16007" LastActivityDate="2011-09-24T15:08:43.023" LastEditDate="2011-09-24T15:08:43.023" LastEditorUserId="3277" OwnerUserId="3277" ParentId="16001" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="16009" AnswerCount="3" Body="&lt;ul&gt;&#10;&lt;li&gt;What does it mean to say that &quot;the variance is a biased estimator&quot;.&lt;/li&gt;&#10;&lt;li&gt;What does it mean to convert a biased estimate to an unbiased estimate through a simple formula. What does this conversion do exactly? &lt;/li&gt;&#10;&lt;li&gt;Also, What is the practical use of this conversion? Do you convert these scores when using certain kind of statistics? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-09-24T09:19:53.300" FavoriteCount="4" Id="16008" LastActivityDate="2012-11-15T16:33:29.807" LastEditDate="2011-09-26T00:38:58.360" LastEditorUserId="183" OwnerUserId="5837" PostTypeId="1" Score="12" Tags="&lt;theory&gt;&lt;unbiased-estimator&gt;&lt;descriptive-statistics&gt;" Title="What does &quot;unbiasedness&quot; mean?" ViewCount="3679" />
  
  <row Body="&lt;p&gt;I don't think there will be a simple or general form for the distribution of the sum of independent hypergeometric distributions.  You'd need to work with convolutions.  It would be relatively easy to do the calculations numerically if there aren't too many urns.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the specific case in (2), of the probability that they are all 0, is rather simple.  You just take $\prod_i \Pr(k_i = 0).$&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your postscript, on the sum of binomials with different proportions: this is &lt;em&gt;not&lt;/em&gt; binomial.  Consider the extreme case with $p_1 = 0$ and $p_2 = 1$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-09-25T00:44:24.707" Id="16014" LastActivityDate="2011-09-25T00:44:24.707" OwnerUserId="5862" ParentId="16010" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="16023" AnswerCount="3" Body="&lt;p&gt;There are several outputs of a numeric computations, I'm seeking a method to model them with a &lt;i&gt;general statistical model&lt;/i&gt; (e.g., distribution function). &lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;...&#10;0.044423&#10;0.127713&#10;0.010692&#10;0.019980&#10;0.013242&#10;0.071450&#10;0.054766&#10;0.013618&#10;0.067680&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Each series has different length. Intuitively, &lt;a href=&quot;http://www.statsoft.com&quot; rel=&quot;nofollow&quot;&gt;Statistica&lt;/a&gt; has an option such as &quot;Distribution Fitting&quot; which works well but only on a series of data at a time. It provides a graph and model as follows for each series:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Exponential, Chi-Square test = 72.94, df = 12 (adjusted) , p = 0.00&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ljy0e.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are at least 30 such a series. How to fit a general (best) model to each of them but finally select the best overall fitted distribution model? How to avoid doing the job manually?&#10;A short comment about the above output is also appreciated. How can the information be interpreted?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here is what I recently did. I just put together all histograms of 30 series (black dots) and then used a kernel-based smoothing algorithm to find a curve (red curve). Now it should be easy to fit a known distribution to that curve (because it is only one).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Any comments about this solution?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Bo5FD.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;** please note the updated comments put on answers.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-24T15:37:38.873" Id="16022" LastActivityDate="2011-09-30T06:12:46.850" LastEditDate="2011-09-30T06:12:46.850" LastEditorUserId="6489" OwnerDisplayName="Developer" OwnerUserId="6489" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;" Title="How to find a fitted statistical model to a series of data?" ViewCount="572" />
  <row AnswerCount="4" Body="&lt;p&gt;Let's suppose I have a set of measurements on a continuous variable which follows a power law distribution, and I’m interested in deriving the exponent of such a distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;So far I've used the binning method, so I was dividing the interval in n bins, plotting the number of observation for each bin in log-log scale and fitting a regression line trough the data. However this way of estimating the exponent is not enough accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I’d like to move on and use the maximum likelihood method. But I’m a bit confused, maybe because I keep referring to the binning method. I know how to write the function of the expected theoretical distribution but I can’t visualize how to derive the empirical distribution.&#10;Can anyone point me in the right direction?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-25T19:48:42.777" FavoriteCount="1" Id="16037" LastActivityDate="2012-12-22T13:02:39.670" LastEditDate="2011-09-26T08:21:04.460" LastEditorUserId="930" OwnerDisplayName="matteo" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;distributions&gt;" Title="How to fit a continuous empirical variable to a theoretical distribution?" ViewCount="1124" />
  <row AcceptedAnswerId="16049" AnswerCount="6" Body="&lt;p&gt;I have data with continuous class and I'm searching for good methods to reduce number of attributes. Now I'm using correlation based filters, random forests and Gram–Schmidt  algorithm. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to achieve is answer which attributes are more important/relevant to class attribute than others. &lt;/p&gt;&#10;&#10;&lt;p&gt;By using methods that I mentioned before I can reach this goal, but is there any other good algorithms worth noticing?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-07-10T18:22:09.473" FavoriteCount="4" Id="16046" LastActivityDate="2012-09-03T10:02:22.890" OwnerDisplayName="pixel" OwnerUserId="1260" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;data-mining&gt;&lt;feature-selection&gt;" Title="Algorithms and methods for attribute/feature selection?" ViewCount="2093" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm trying to reduce dataset dimensionality. PCA is a good metric but that gives me new dataset. My goal is to determine from number of events (e.g. 60) and number of trials (e.g. 6) which events are more relevant.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;1st, 3rd, 21st, 45th ... (N total) events are good enough to approximate behavior of dataset.&lt;br&gt;&#10;That will allow me to discard  60-N events, and to deal with only N.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For now, I'm calculating covariance matrix, and take events for which correlation is smaller than some threshold.&lt;br&gt;&#10;Is there some official metric or math function for this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-16T07:54:09.360" Id="16051" LastActivityDate="2011-09-27T00:26:54.313" LastEditDate="2011-09-27T00:26:54.313" LastEditorUserId="183" OwnerDisplayName="user1313" OwnerUserId="1313" PostTypeId="1" Score="1" Tags="&lt;feature-selection&gt;&lt;dimensionality-reduction&gt;" Title="How best to reduce dimensionality of a dataset composed of events and trials?" ViewCount="213" />
  <row AnswerCount="3" Body="&lt;p&gt;I am working on a machine control project.  We can measure the motor's current during operation.  Sample data from two motors performing an operation successfully is below.  The red trace shows the current from one motor, the blue trace the current from another.  I'd like to try and come up with an algorithm for identifying problems with machine behavior.  Problems could be excessively high motor current, near zero motor current, current increasing at the end of the operation, a shorter time series than normal, anything in general which doesn't look like a typical operation below.  Can anyone suggest a good algorithm for achieving this?  The only one I'm familiar with is a neural network.  I have put an Excel file of actual data at &lt;a href=&quot;http://resplendid.com/motorcurrents.xlsx&quot;&gt;motor currents&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RQka3.png&quot; alt=&quot;Motor currents - good operation&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/tOIpI.png&quot; alt=&quot;Motor currents - jam at end of operation&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-09-25T01:49:55.263" FavoriteCount="1" Id="16054" LastActivityDate="2011-09-29T06:35:37.837" LastEditDate="2011-09-28T22:31:16.680" LastEditorUserId="6544" OwnerDisplayName="fred basset" OwnerUserId="6544" PostTypeId="1" Score="7" Tags="&lt;time-series&gt;" Title="Best algorithm for classifying time series motor data" ViewCount="686" />
  <row AnswerCount="1" Body="&lt;p&gt;How to keep statistical lessons that we catch growing? Is there any model to learn statistics in a way to get the best use of them in engineering applications? I don't mean recommendations on basic statistics via books etc. Practical ideas are inquired in the level of intermediate-high statistics.&lt;/p&gt;&#10;" ClosedDate="2011-09-26T16:00:33.943" CommentCount="1" CreationDate="2011-09-26T14:51:49.013" Id="16062" LastActivityDate="2011-09-26T15:07:50.573" OwnerUserId="6489" PostTypeId="1" Score="0" Tags="&lt;learning&gt;" Title="How to keep statistical knowledge growing?" ViewCount="121" />
  
  
  <row Body="&lt;p&gt;First, I encourage you to say more about why &quot;just comparing the grades is not enough.&quot;  But more to your point, although the use of correlation is not out of the question, you and the people to whom you are reporting would no doubt be better off viewing the problem as one of group differences.  Then your question becomes &quot;Is Device A, on average, viewed more or less favorably than Device B?&quot; For this, &lt;em&gt;T&lt;/em&gt;-tests are the usual starting place. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or perhaps you have highly non-normal distributions, in which case means are not the best way to represent the typical score.  In such a situation, you would want to test for differences using medians, or ranks.  So you would want to get familiar with the techniques of Median tests and/or Mann-Whitney tests involving ranks.  &lt;/p&gt;&#10;&#10;&lt;p&gt;(Since the same group of 30 people have supplied each set of ratings, you have what are called &quot;dependent sets of scores&quot; and may need to look for a more specialized test that takes this into account.  Others on this site will probably have ideas.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach would be to average the 12 ratings, creating a single summary score for each person with regard to each device.  This average would probably be normally distributed, and so you could use a single Dependent-Means &lt;em&gt;T&lt;/em&gt;-test of differences.  Just to make things confusing, this is also called a Correlated Means test.  But if you are going to create such a summary score, you might want to check and see that it is an effective one, i.e., that all 12 component scores are correlated enough to &quot;play their part&quot; in forming the summary score.  this is a reliability problem, so you could look into that as well.  and one measure of reliability that would help you would be Cronbach's alpha.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-26T22:45:32.190" Id="16082" LastActivityDate="2011-09-26T22:53:06.813" LastEditDate="2011-09-26T22:53:06.813" LastEditorUserId="2669" OwnerUserId="2669" ParentId="16080" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="16091" AnswerCount="2" Body="&lt;p&gt;I wish to simulate from a normal density (say mean=1, sd=1) but only want positive values.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way is to simulate from a normal and take the absolute value.  I think of this as a folded normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;I see in R there are functions for truncated random variable generation.  If I simulate from a truncated normal (truncation at 0) is this equivalent to the folded approach?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-27T03:07:06.853" Id="16089" LastActivityDate="2011-09-27T04:24:40.050" LastEditDate="2011-09-27T04:14:13.680" LastEditorUserId="183" OwnerUserId="2310" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;&lt;simulation&gt;&lt;truncation&gt;" Title="Is sampling from a folded normal distribution equivalent to sampling from a normal distribution truncated at 0?" ViewCount="666" />
  <row Body="&lt;p&gt;Yes, the approaches give the same results for a &lt;strong&gt;zero-mean&lt;/strong&gt; Normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;It suffices to check that probabilities agree on intervals, because these generate the sigma algebra of all (Lebesgue) measurable sets.  Let $\Phi$ be the standard Normal density: $\Phi((a,b])$ gives the probability that a standard Normal variate lies in the interval $(a,b]$.  Then, for $0 \le a \le b$, the truncated probability is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Phi_{\text{truncated}}((a,b]) = \Phi((a,b]) / \Phi([0, \infty]) = 2\Phi((a,b])$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(because $\Phi([0, \infty]) = 1/2$) and the folded probability is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Phi_{\text{folded}}((a,b]) = \Phi((a,b]) + \Phi([-b,-a)) = 2\Phi((a,b])$$&lt;/p&gt;&#10;&#10;&lt;p&gt;due to the symmetry of $\Phi$ about $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis holds for &lt;em&gt;any&lt;/em&gt; distribution that is symmetric about $0$ and has zero probability of being $0$.  &lt;strong&gt;If the mean is nonzero&lt;/strong&gt;, however, the distribution is &lt;em&gt;not&lt;/em&gt; symmetric and the two approaches do &lt;em&gt;not&lt;/em&gt; give the same result, as the same calculations show.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4r0MC.png&quot; alt=&quot;Three distributions&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This graph shows the probability density functions for a Normal(1,1) distribution (yellow), a folded Normal(1,1) distribution (red), and a truncated Normal(1,1) distribution (blue).  Note how the folded distribution does not share the characteristic bell-curve shape with the other two.  The blue curve (truncated distribution) is the positive part of the yellow curve, scaled up to have unit area, whereas the red curve (folded distribution) is the sum of the positive part of the yellow curve and its negative tail (as reflected around the y-axis).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-27T04:13:37.240" Id="16091" LastActivityDate="2011-09-27T04:24:40.050" LastEditDate="2011-09-27T04:24:40.050" LastEditorUserId="919" OwnerUserId="919" ParentId="16089" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="16109" AnswerCount="3" Body="&lt;p&gt;In geostatistical context it is common practice that for simulations of a variable of interest (e.g., grade of concentrations of metal in rock samples) the &lt;strong&gt;number of simulations&lt;/strong&gt; at least needs to be &lt;strong&gt;30&lt;/strong&gt;. I would ask what &lt;em&gt;the criteria&lt;/em&gt; is to choose a right trial number considering that each trial has an expense of large computation.&#10;Is that criteria generic enough to be applied on all simulations?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-27T12:45:53.133" FavoriteCount="1" Id="16106" LastActivityDate="2011-09-28T05:29:46.940" OwnerUserId="6489" PostTypeId="1" Score="4" Tags="&lt;simulation&gt;" Title="How to set the optimal number of simulations" ViewCount="469" />
  <row AcceptedAnswerId="16111" AnswerCount="1" Body="&lt;p&gt;I recently wrote a program that graphed data points so that a user could scroll through them and find &quot;interesting&quot; parts of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I am looking at ways to make it even simpler by making a table of values deemed to be interesting. These are usually either the value switching to a new value, or spiking and returning to the old one. The main problem is that I have to take into account a lot of noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I'm already saving hours of data trawling, making it conservative (i.e. is trigger happy) is fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;Algorithms and theories I'm looking at are k-nearest neighbour and Local Outlier factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking at using both, and then collecting them by saying errors will be unique within +-100 samples, giving errors detected by both a higher rating... Or something along those lines.&lt;/p&gt;&#10;&#10;&lt;p&gt;What algorithms are there I should be looking at? And what would you recommend as the best algorithm?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit: More specific information:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The values are read in ~20ms intervals (but to say time is constant is ok), and are between +-10 (as single precision floats). The amount of noise is unknown, but is pretty consistent (i.e. same magnitude of noise throughout), and usually jumps between two values.&lt;/p&gt;&#10;&#10;&lt;p&gt;A typical set of data is 120000+ records long (so records are read 1 at a time, and the whole set, or even it's size, is not known), changes can occur anywhere in that set, and at any frequency, though the change will usually be &gt;2 (though that fact is not reliable) and aren't usually within 1000 records of each other (again, not reliable). Typically the change or spike is &amp;lt;2 records long, and there are few gradual changes (though the possibility of them cannot be ruled out. Changes and spikes are always obvious to a human, and since this is to point a human in the right direction it only needs to be accurate to +- 100 records.&lt;/p&gt;&#10;&#10;&lt;p&gt;At the moment I am looking at something along the lines of:&#10;1 - Get the following from 100 records the mean and standard deviation&#10;2 - For each value test if it is within 2 standard deviations of the mean.&#10;3 - if not then print the change and start from 1 again (ignoring 20 records)&lt;/p&gt;&#10;&#10;&lt;p&gt;That would be simple to implement, and would show gradual changes as being changes. I could add more complex checks to find the type of exception (spike or change) and the 20 doesn't need to be a constant&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-09-26T15:19:27.087" FavoriteCount="1" Id="16108" LastActivityDate="2011-09-27T21:23:00.770" LastEditDate="2011-09-27T18:40:09.950" LastEditorUserId="6529" OwnerDisplayName="Mat" OwnerUserId="6529" PostTypeId="1" Score="3" Tags="&lt;data-mining&gt;" Title="Change and anomaly detection" ViewCount="230" />
  
  
&#10;\frac{p_{1m}}{p_{0m}} \leq \frac{\beta}{1-\alpha}. \label{lbound}
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;which is the best example from the real world to show the advantages&#10;  of graphing?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Any big table.  For examples, google images of &quot;official census table&quot;.  You'll see things like &lt;a href=&quot;http://www.empirecenter.org/pb/2011/08/migration1080311.cfm&quot;&gt;the one below&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also look at &lt;a href=&quot;http://www.jstor.org/stable/3087382&quot;&gt;Gelman et al. (2002) Let's Practice What We Preach: Turning Tables into Graphs.  American Statistician 56:121-130&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vkwWH.png&quot; alt=&quot;huge complicated table&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-27T18:19:26.967" Id="16133" LastActivityDate="2011-09-27T18:19:26.967" OwnerUserId="5862" ParentId="16130" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I'd say calling this a Monte Carlo method is a bit misleading.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think that what you do is usually called Simple Random Sampling, see here:&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_random_sample&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Simple_random_sample&lt;/a&gt;&#10;and is particularly used in survey analysis, polls, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Monte Carlo methods usually refer to sampling methods used in integral approximations, where you choose the number N of draws, and the larger N, the more precise your results are. They are usually only justified asymptotically, when N goes to infinity (in which case they are expected to give the &quot;right solution&quot;, usually to compute an integral of interest). In this respect your method seems different from classical MC methods, as introduced e.g. here:&#10;&lt;a href=&quot;http://books.google.com/books/about/Monte_Carlo_statistical_methods.html?id=HfhGAxn5GugC&quot; rel=&quot;nofollow&quot;&gt;http://books.google.com/books/about/Monte_Carlo_statistical_methods.html?id=HfhGAxn5GugC&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-27T19:36:54.560" Id="16139" LastActivityDate="2011-09-27T19:36:54.560" OwnerUserId="6532" ParentId="16063" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Your charts will depend on what story you are interested in.  Are you interested in amounts spent (on average the rich usually spend more on most items than the poor do)? Or are you interested in the shares of expenditure (or income) going on different items?   Can you avoid your charts getting too cluttered? Do you intend to adjust your numbers for household size?  Or household type, especially whether there are children?  &lt;/p&gt;&#10;&#10;&lt;p&gt;You might get some ideas from the UK publication &lt;a href=&quot;http://www.ons.gov.uk/ons/rel/family-spending/family-spending/2010-edition/family-spending-2010--living-costs-and-food-survey-2009-.pdf&quot; rel=&quot;nofollow&quot;&gt;Living Costs and Food Survey&lt;/a&gt;.  You will find that linear regression of expenditure on income is often not particularly appropriate.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-28T00:43:00.583" Id="16144" LastActivityDate="2011-09-28T00:43:00.583" OwnerUserId="2958" ParentId="16142" PostTypeId="2" Score="1" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, we have an &quot;affinity&quot; parameter for each player that affects how much that player improves the chance of his team winning. Define the player's &quot;strength&quot; by $s_i = e^{\alpha_i}$. Then, this model asserts that&#10;$$
  <row Body="&lt;p&gt;Why no use &lt;a href=&quot;http://en.wikipedia.org/wiki/Polynomial_interpolation&quot; rel=&quot;nofollow&quot;&gt;Polynom Interpolation&lt;/a&gt; [create an approximation polynom for your data, the x axis will be 1,2,... and the y axis will be the numbers you have]. &lt;a href=&quot;http://en.wikipedia.org/wiki/Polynomial_interpolation#Constructing_the_interpolation_polynomial&quot; rel=&quot;nofollow&quot;&gt;Constructing&lt;/a&gt; the polynoms is fairly easy to code, and have well based theory behind it.&lt;/p&gt;&#10;&#10;&lt;p&gt;After you have this polynom, you can easily analyze it using the well known tools for analyzing functions and polynoms [i.e. calculus to find Derivative and using it to define  where the functio is rising, local min/max, ...].&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-26T21:37:43.973" Id="16152" LastActivityDate="2011-09-26T21:37:43.973" OwnerDisplayName="amit" OwnerUserId="18240" ParentId="16151" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You could describe your process as a Kalman process, that is, construct a Kalman filter e.g. consisting of current value, first and second derivative and trying to track the position. The Kalman filter would give you a current velocity estimate, which is an estimator for the change.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-27T08:52:08.400" Id="16156" LastActivityDate="2011-09-27T08:52:08.400" OwnerDisplayName="thiton" OwnerUserId="5545" ParentId="16151" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The answer to your question is as follows. Given that you are trying to detect the presence of local time trends this can be seen as an extension of detecting local level shifts, The literature of Intervention Detection  (Not Intervention Modelling which requires a user specification) but rather the search for the points in time where the trend has changed and the magnitude of that change. Now if there are Level shifts and/or ARIMA structure required you will have to integrate all three components in your search.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-28T12:31:33.313" Id="16159" LastActivityDate="2011-09-28T12:31:33.313" OwnerUserId="3382" ParentId="16151" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Imagine I have measured heights of humans (in cm) and show the data in a histogram:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Generate sample data&#10;sampleSize = 100000&#10;sampleData = round(rnorm(n = sampleSize, mean = 175, sd = 14))&#10;&#10;# Draw histogram of sample&#10;h = hist(sampleData, breaks = max(sampleData) - min(sampleData), plot=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I want to know how realistic each size in the distribution is. For example I found a human with the size of 100cm and I want to know if this individual is overrepresented in my sample. &lt;strong&gt;I want to give a measure for the uncertainty for each height class.&lt;/strong&gt; How can I do that?&lt;/p&gt;&#10;&#10;&lt;p&gt;My first idea was to bootstrap the measured data, calculate the standard deviation for each class among the bootstrapped samples and give a relative standard-deviation (scaled by 1000) for each class.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;######################################################################&#10;# Calculate the standard Deviation for each sampled size of a bootstrap measurement&#10;cycles = 100000&#10;&#10;# initialize results-matrix&#10;result = matrix(0, nrow = length(h$counts), ncol = cycles)&#10;&#10;# sample original data&#10;for (i in 1:cycles) {&#10;    tmp = hist(sample(sampleData, replace = T, size = sampleSize), breaks = h$breaks, plot=F)&#10;    result[,i] = tmp$counts&#10;    print(paste(&quot;cycle:&quot;, i))&#10;}&#10;&#10;# calculate standard deviations&#10;for (i in 1:length(h$counts)) {&#10;    h$sdboot[i] = sd(result[i,])&#10;}&#10;&#10;######################################################################&#10;pdf(&quot;boot.pdf&quot;)&#10;    # Plotting&#10;    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++&#10;    plot(h)&#10;    # Bootstrap: standarddeviations&#10;    polygon(x=c(rev(h$mids), h$mids), y= c(rev(h$counts + h$sdboot), h$counts - h$sdboot), col=&quot;#ff000022&quot;)&#10;&#10;    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++&#10;    plot(h)&#10;    # Bootstrap: standarddeviations&#10;    polygon(&#10;        x=c(rev(h$mids), h$mids),&#10;        y= c(rev(h$counts + 1000 * (h$sdboot / h$counts)), h$counts - 1000 * (h$sdboot / h$counts)),&#10;        col=&quot;#00ff0022&quot;&#10;    )&#10;&#10;    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++&#10;    plot(x=h$mids, y=h$sdboot/h$counts)&#10;&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The resulting graphs (2 and 3) show the increasing uncertainty with lower numbers of counts in the class, but I'm not sure how informative that is. It does not really answer the question &quot;&lt;strong&gt;How likely is it, that you miss members of a certain height class in a measurement with the same sampleSize?&lt;/strong&gt;&quot; or &quot;&lt;strong&gt;How likely is it, that the value is of the measured size just by chance&lt;/strong&gt;&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;One problem might be, that I do not have a model for my real data (For human height one could assume a normal distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it somehow possible to give a measure for the uncertainty of my real data?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-28T13:07:42.587" FavoriteCount="1" Id="16160" LastActivityDate="2011-09-28T13:07:42.587" OwnerUserId="6543" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;distributions&gt;&lt;quality-control&gt;" Title="Calculate quality estimates for distribution of real data" ViewCount="81" />
  <row Body="&lt;p&gt;This not a full answer, but too much for a comment.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Are the treatments ordered? By converting them to a numeric value in line 2 of your code, you are assuming that they have values 1 through 10 in a meaningful way (eg. dose).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The model you wrote has a fixed &lt;code&gt;site&lt;/code&gt; effect, fixed &lt;code&gt;year&lt;/code&gt; effect, and a linear treatment effect within each year. On the other hand, you don't have a fixed treatment effect. That is probably not what you intended. I would certainly make &lt;code&gt;site&lt;/code&gt; a random effect, a if you care about the specific treatments (and they are not just random representatives of possible treatments), then make it a fixed effect.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you want to treat both &lt;code&gt;site&lt;/code&gt; and &lt;code&gt;year&lt;/code&gt; as random effect, you might find the &lt;code&gt;lme4&lt;/code&gt; package easier to use, because it handles crossed random effects much easier.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-09-28T13:51:00.097" Id="16162" LastActivityDate="2011-09-28T13:51:00.097" OwnerUserId="279" ParentId="16088" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As @rolando2 mentioned, this depends very much on what your trying to accomplish or what question(s) you are trying to answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are trying to find a good model for prediction then rather than just deciding on whethere to include a term or not, it is better to use some type of shrinkage method such as penalized regression, ridge regression, lasso/lars, or model averaging.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should also take into account outside knowledge about the variables.  If my doctor had a choice of 2 predictive models to help in diagnosing me I would prefer that he use the one that uses blood pressure as a predictor rather than the one that uses the results from an exploratory surgery, even if it has a slightly smaller $R^2$ value.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-28T15:25:08.910" Id="16166" LastActivityDate="2011-09-28T15:25:08.910" OwnerUserId="4505" ParentId="16146" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;My initial thought was that, for ordinary linear regression, we just plug in our estimate of the residual variance, $\sigma^2$, as if it were the truth.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, take a look at &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470073713&quot; rel=&quot;nofollow&quot;&gt;McCulloch and Searle (2001)  Generalized, linear and mixed models, 1st edition&lt;/a&gt;, Section 6.4b, &quot;Sampling variance&quot;. They indicate that &lt;i&gt;you can't just plug in the estimates of the variance components&lt;/i&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Instead of dealing with the variance (matrix) of a vector $X \hat{\beta}$ we &#10;  consider the simpler case of the scalar $l' \hat{\beta}$ for estimable $l'\beta$ (i.e., $l' = t'X$ for some $t'$).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;For known $V$,  we have from (6.21) that $\text{var}(l' \beta^0) = l'(X'V{-1}X)^- l$.  A replacement for this when $V$ is not known is to use $l'(X'\hat{V}^{-1}X)^- l$, which is an estimate of $\text{var}(l'\beta^0) = \text{var}[l' (X' V^{-1} X)^- X' V^{-1} y]$.  But it is &lt;i&gt;not&lt;/i&gt; an estimate of $\text{var}(l'\hat{\beta}) = \text{var}[l' (X' \hat{V}^{-1} X)^- X' \hat{V}^{-1} y]$.  The latter requires taking account of the variability of $\hat{V}$ as well as that in $y$.  To deal with this, Kackar and Harville (1984, p. 854) observe that (in our notation) $l' \hat{\beta} - l' \beta$ can be expressed as the sum of two independent parts, $l' \hat{\beta} - l' \beta^0$ and $l'\beta^0 - l' \beta$.  This leads to $\text{var}(l' \hat{\beta})$ being expressed as a sum of two variances which we write as&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$\text{var}(l'\hat{\beta}) = ... \approx l'(X' V^{-1} X)l + l' T \; l$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;They go on to explain $T$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So this answers the first part of your question and indicates that your intuition was correct (and mine was wrong).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-28T18:32:25.940" Id="16171" LastActivityDate="2014-09-07T02:26:05.373" LastEditDate="2014-09-07T02:26:05.373" LastEditorUserId="7290" OwnerUserId="5862" ParentId="13454" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I would suggest covariance because it will tell you direction with variance. e.g.,&lt;/p&gt;&#10;&#10;&lt;p&gt;cov(a,b)= -9.16667&lt;/p&gt;&#10;&#10;&lt;p&gt;you can also do correlation test in R. e.g., cor.test(a,b)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-09-28T19:21:36.587" Id="16173" LastActivityDate="2011-09-28T19:21:36.587" OwnerUserId="5864" ParentId="16170" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="16240" AnswerCount="3" Body="&lt;p&gt;I've been trying to code an algorithm to suggest bets in 1X2 (weighted) games.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, each game has a set of matches (home vs away teams):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;1&lt;/code&gt;: home wins&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: draw&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;2&lt;/code&gt;: away wins&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mD1w8.jpg&quot; alt=&quot;BWin 1X2 Betting Game&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For each match and symbol (&lt;code&gt;1&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;), I will assign a percentage that represents the chances / likelihood of that symbol being the correct match outcome. Here is an array representing the structure:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$game = array&#10;(&#10;    'match #1' =&amp;gt; array // stdev = 0.0471&#10;    (&#10;        '1' =&amp;gt; 0.3,     // 30%     home wins&#10;        'X' =&amp;gt; 0.4,     // 40%     draw&#10;        '2' =&amp;gt; 0.3,     // 30%     away wins&#10;    ),&#10;&#10;    'match #2' =&amp;gt; array // stdev = 0.4714&#10;    (&#10;        '1' =&amp;gt; 0.0,     //   0%    home wins&#10;        'X' =&amp;gt; 0.0,     //   0%    draw&#10;        '2' =&amp;gt; 1.0,     // 100%    away wins&#10;    ),&#10;&#10;    'match #3' =&amp;gt; array // stdev = 0.4027&#10;    (&#10;        '1' =&amp;gt; 0.1,     //  10%    home wins&#10;        'X' =&amp;gt; 0.0,     //   0%    draw&#10;        '2' =&amp;gt; 0.9,     //  90%    away wins&#10;    ),&#10;);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I also calculate the standard deviation for each bet (commented in the above snippet); higher standard deviations represent a higher certainty, while the matches with the lowest standard deviations translate to a higher level of uncertainty, and, ideally, should be covered with a double or triple bet, if possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following pseudo-algorithm should describe the overall workflow:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for each match, sorted by std. dev         // &quot;uncertain&quot; matches first&#10;    if still can make triple bets&#10;        mark top 3 symbols of match        // mark 3 (all) symbols&#10;    else if still can make double bets&#10;        mark top 2 symbols of match        // mark 2 (highest) symbols&#10;    else if can only make single bets      // always does&#10;        mark top symbol of match           // mark 1 (highest) symbol&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So far so good, but I need to tell the algorithm how much I want to spend. Lets say a single bet costs &lt;code&gt;1&lt;/code&gt; in whatever currency, the formula to calculate how much a multiple bet costs is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2^double_bets * 3^triple_bets * cost_per_bet (= 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Obviously, the algorithm should try to allocate as much money available as possible into the bet suggestion (it wouldn't make much sense otherwise), and now is where this gets trickier...&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say I wanna pay a maximum of &lt;code&gt;4&lt;/code&gt;, listing all possible multiples in PHP (&lt;a href=&quot;http://www.ideone.com/A2uOU&quot; rel=&quot;nofollow&quot;&gt;@ IDEOne&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$cost = 1; // cost per single bet&#10;$result = array();&#10;$max_cost = 4; // maximum amount to bet&#10;&#10;foreach (range(0, 3) as $double)&#10;{&#10;    foreach (range(0, 3) as $triple)&#10;    {&#10;        if (($double + $triple) &amp;lt;= 3) // game only has 3 matches&#10;        {&#10;            $bets = pow(2, $double) * pow(3, $triple); // # of bets&#10;&#10;            $result[$bets] = array&#10;            (&#10;                'cost'      =&amp;gt; $bets * $cost, // total cost of this bet&#10;                'double'    =&amp;gt; $double,&#10;                'triple'    =&amp;gt; $triple,&#10;            );&#10;&#10;            if ($result[$bets]['cost'] &amp;gt; $max_cost)&#10;            {&#10;                unset($result[$bets]);&#10;            }&#10;        }&#10;    }&#10;}&#10;&#10;ksort($result);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Yields the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Array&#10;(&#10;    [1] =&amp;gt; Array&#10;        (&#10;            [cost] =&amp;gt; 1&#10;            [double] =&amp;gt; 0&#10;            [triple] =&amp;gt; 0&#10;        )&#10;&#10;    [2] =&amp;gt; Array&#10;        (&#10;            [cost] =&amp;gt; 2&#10;            [double] =&amp;gt; 1&#10;            [triple] =&amp;gt; 0&#10;        )&#10;&#10;    [3] =&amp;gt; Array&#10;        (&#10;            [cost] =&amp;gt; 3&#10;            [double] =&amp;gt; 0&#10;            [triple] =&amp;gt; 1&#10;        )&#10;&#10;    [4] =&amp;gt; Array&#10;        (&#10;            [cost] =&amp;gt; 4&#10;            [double] =&amp;gt; 2&#10;            [triple] =&amp;gt; 0&#10;        )&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;The Problem&lt;/h2&gt;&#10;&#10;&lt;p&gt;If I choose to play the maximum amount of money available (&lt;code&gt;4&lt;/code&gt;) I would have to bet with two doubles, if I use the pseudo-algorithm I described above I would end up with the following bet suggestion:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;match #1&lt;/code&gt; =&gt; &lt;code&gt;X1&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;match #2&lt;/code&gt; =&gt; &lt;code&gt;2&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;match #3&lt;/code&gt; =&gt; &lt;code&gt;12&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Which seems sub-optimal when compared to a triple bet that costs &lt;code&gt;3&lt;/code&gt; and covers more uncertainty:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;match #1&lt;/code&gt; =&gt; &lt;code&gt;X12&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;match #2&lt;/code&gt; =&gt; &lt;code&gt;2&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;match #3&lt;/code&gt; =&gt; &lt;code&gt;2&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The above example gains even more relevance if you consider that &lt;code&gt;match #3&lt;/code&gt; odds could be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$game['match #3'] = array // stdev = 0.4714&#10;(&#10;    '1' =&amp;gt; 0.0,           //   0%    home wins&#10;    'X' =&amp;gt; 0.0,           //   0%    draw&#10;    '2' =&amp;gt; 1.0,           // 100%    away wins&#10;);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this case I would be wasting a double for no good reason.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I can only choose the &lt;strong&gt;biggest (possibly stupid) bet&lt;/strong&gt; and not the &lt;strong&gt;smartest, biggest bet&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been banging my head against the wall for some days now, hoping I get some kind of epiphany but so far I've only been able to come up with two half-[bad-]solutions:&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;1) Draw a &quot;Line&quot;&lt;/h3&gt;&#10;&#10;&lt;p&gt;Basically I would say that matches with a stdev lower than a specific value would be triple, matches with a stdev a big higher would be double bets and the rest single bets.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with this, of course, is finding out the appropriate specific boundaries - and even if I do find the perfect values for the &quot;smartest&quot; bet, I still don't know if I have enough money to play the suggested bet or if I could make a even bigger (also smart) bet...&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;2) Bruteforce&lt;/h3&gt;&#10;&#10;&lt;p&gt;I came up with this idea while writing this question and I know it won't make perfect sense in the context I described but I think I could make it work using somewhat different metrics. Basically, I could make the program suggest bets (# of triple and double bets) for every possible amount of money I could play (from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;4&lt;/code&gt; in my example), applying the pseudo-algorithm I described above and calculating a global ranking value (something like &lt;code&gt;% of symbols * match stdev&lt;/code&gt; - I know, it doesn't make sense).&lt;/p&gt;&#10;&#10;&lt;p&gt;The bet with the highest ranking (covering uncertainty) would be the suggested bet. The problem with this approach (besides the fact that it doesn't make any sense yet) is that the games my program is going to work with are not limited to 3 matches and the number of double and triple bet combinations for those matches would be substantially higher.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I feel like there is an elegant solution, but I just can't grasp it...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Any help cracking this problem is greatly appreciated, thanks.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;There seems to be some confusion regarding my problem, I've already addressed this in this question and also in the comments but the misinterpretation still seems to prevail, at least to some.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I need to know &lt;strong&gt;how many triple, double and single bets I will play&#10;  for a specific game&lt;/strong&gt; (all matches). I already know what&#10;  symbols I want to play by looking at each match individually.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="16" CreationDate="2011-09-26T23:55:16.040" FavoriteCount="2" Id="16174" LastActivityDate="2011-09-29T20:00:08.503" LastEditDate="2011-09-28T22:19:45.250" LastEditorUserId="88" OwnerDisplayName="Alix Axel" OwnerUserId="2801" PostTypeId="1" Score="15" Tags="&lt;optimization&gt;" Title="Making big, smart(er) bets" ViewCount="485" />
  <row Body="&lt;p&gt;How about making a solution based on the Simplex Method. Since the premise for using the Simplex method isn't fulfilled we need to modify the method slightly. I call the modified version &quot;Walk the line&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Method:&lt;/p&gt;&#10;&#10;&lt;p&gt;You are able to measure the uncertainty of each match. Do it! Calculate the uncertainty of each match with a single or double bet (for a triple bet there is no uncertainty).&#10;When adding a double or triple bet, always choose the one that reduces uncertainty the most.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Start at maximum number of triple bets. Calculate total uncertainty.&lt;/li&gt;&#10;&lt;li&gt;Remove one triple bet. Add one or two double bets, keeping under maximum cost. Calculate total uncertainty.&lt;/li&gt;&#10;&lt;li&gt;Repeat step 2 until you have the maximum number of double bets.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Pick the bet with the lowest total uncertainty.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-28T07:26:03.333" Id="16176" LastActivityDate="2011-09-28T07:26:03.333" OwnerDisplayName="Klas Lindbäck" OwnerUserId="55226" ParentId="16174" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You'll notice that in your ANOVA's (deviance tables) of the models there is no difference in the main effects with, or without the interaction.  You don't have to know how to interpret the deviance table, just recognize that there's no difference!  &lt;/p&gt;&#10;&#10;&lt;p&gt;Keep in mind that your &quot;Estimate&quot; column in the regression is about the magnitude of the slope and the associated tests are of that magnitude.  When you add interactions you can change how the slope is calculated and change it's significance.  That doesn't mean it went away, it just means it's qualified by an interaction.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, a short answer is that, if you had a main effect without the interaction then you have a main effect.  It's very common to do the additive and then additive+interaction models separately so you can see where your main effects are and then look at your interactions.  The fact that it went away gives you some clues about the kind of interaction that you have but it's hard for someone to answer the whole thing with just what you've reported.  Your next step is to start making some graphs.  For example, make one with obdobinehn at different levels of kraj.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should really look at a paper on interpreting these interaction &lt;a href=&quot;http://www.mendeley.com/research/the-detection-and-interpretation-of-interaction-effects-between-continuous-variables-in-multiple-regression/&quot; rel=&quot;nofollow&quot;&gt;effects&lt;/a&gt;.  A complete answer for your query is far too difficult to do based on what you've provided, and even guidance about where to go next is very involved.  Read the linked paper, see how far you get, and get back to the SE with more questions at that time.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-29T02:42:34.987" Id="16182" LastActivityDate="2011-09-29T03:27:52.573" LastEditDate="2011-09-29T03:27:52.573" LastEditorUserId="601" OwnerUserId="601" ParentId="16178" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="16244" AnswerCount="1" Body="&lt;p&gt;The &lt;a href=&quot;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html&quot; rel=&quot;nofollow&quot;&gt;actor-critic model&lt;/a&gt; is used within temporal difference learning, which is a method within reinforcement learning, to optimize a process on a state-by-state basis by using the difference between performance and expected performance for each respective state. In other words, there is a set of possible input patterns to the process, each of which constitutes a state (the state in which the agent finds itself). For each state, the actor computes an output, which is evaluated by a performance metric (which can theoretically define a different target output for each state) that returns the reward (performance) signal. Meanwhile, for each state, the critic tracks the expected reward, which is a function of the past reward signals for that state. At each iteration, the difference between the reward signal and the expected reward signal is taken and used to update both the actor's policy (hopefully up a performance gradient) and the critic's performance expectation for that state.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hypothetically, for some problem there exists two or more states (input patterns) that are similar and where the information that differs between the two is irrelevant. In other words, the target output for the states is the same and the input is similar. It would then be potentially useful for the critic to have a method for recognizing that all states that share a certain similarity are effectively the same state, so that the set of those states can share the same predicted outcome. In particular, if you are dealing with a system that features a massive amount of inputs, and if most of those are non-pertinent at any given point in time, then you don't want to relearn the expected performance for each possible combination of non-pertinent inputs.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question, then, is how you do this. How do you get a critic to organize different states into state classes, and in a way that improves performance over the long run?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-29T08:34:40.157" Id="16194" LastActivityDate="2011-09-29T22:46:17.553" LastEditDate="2011-09-29T22:46:17.553" LastEditorUserId="3443" OwnerUserId="3443" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;reinforcement-learning&gt;" Title="Getting critics to recognize that two similar input patterns refer to the same output-performance relationship" ViewCount="74" />
  <row AcceptedAnswerId="16205" AnswerCount="11" Body="&lt;p&gt;What do you call an average that does not include outliers?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example if you have a set:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{90,89,92,91,5} avg = 73.4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but excluding the outlier (5) we have&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{90,89,92,91(,5)} avg = 90.5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do you describe this average in statistics?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2009-02-02T14:21:12.103" FavoriteCount="3" Id="16198" LastActivityDate="2015-02-20T14:33:53.160" LastEditDate="2011-09-29T11:57:02.423" LastEditorUserId="183" OwnerDisplayName="Tawani" PostTypeId="1" Score="31" Tags="&lt;mean&gt;&lt;outliers&gt;&lt;average&gt;" Title="What do you call an average that does not include outliers?" ViewCount="22416" />
  
  <row Body="&lt;p&gt;For a very specific name, you'll need to specify the mechanism for outlier rejection.  One general term is &quot;robust&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;dsimcha mentions one approach: trimming.  Another is clipping: all values outside a known-good range are discarded.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2009-02-02T14:24:58.430" Id="16200" LastActivityDate="2009-02-02T14:24:58.430" OwnerDisplayName="Mr Fooz" ParentId="16198" PostTypeId="2" Score="14" />
  
  <row AnswerCount="1" Body="&lt;p&gt;In some sense the multivariate normal  is the &quot;nicest&quot; distribution that we can describe using only a vector (rank one tensor) and a symmetric positive definite matrix (rank two tensor). &#10;$$\mathcal{N}(\mu_i, \Sigma_{ij}) = |2\pi\Sigma|^{-\frac{1}{2}}e^{-(x-\mu)^T\Sigma^{-1}(x-\mu)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a generalization to higher order tensors? I.e., is there a &quot;nicest&quot; distribution described by a vector, symmetric matrix, and symmetric third order tensor? $\mathcal{M}(\mu_i, \Sigma_{ij}, \Pi_{ijk})$ ? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-09-29T16:29:00.817" Id="16233" LastActivityDate="2011-09-30T14:09:46.157" LastEditDate="2011-09-29T21:28:25.577" LastEditorUserId="930" OwnerUserId="3830" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;" Title="Higher order generalization of the multivariate normal distribution" ViewCount="468" />
  <row Body="&lt;p&gt;The micromap example that @karl suggested is very nice; I think it would, though, be similar to a dot plot, which are easy to implement in R or SAS (and probably other software). Something like &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;corr &amp;lt;- c(.4, .4, .3, .2)&#10;group = c(&quot;Education&quot;, &quot;gender&quot;, &quot;age&quot;, &quot;more&quot;)&#10;dotchart(corr, labels = group)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;works in R&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-29T23:26:59.713" Id="16248" LastActivityDate="2011-09-29T23:26:59.713" OwnerUserId="686" ParentId="16235" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You are close, with your use of &lt;code&gt;dhyper&lt;/code&gt; and &lt;code&gt;phyper&lt;/code&gt;, but I don't understand where &lt;code&gt;0:2&lt;/code&gt; and &lt;code&gt;-1:2&lt;/code&gt; are coming from.&lt;/p&gt;&#10;&#10;&lt;p&gt;The p-value you want is the probability of getting 100 &lt;em&gt;or more&lt;/em&gt; white balls in a sample of size 400 from an urn with 3000 white balls and 12000 black balls.  Here are four ways to calculate it.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sum(dhyper(100:400, 3000, 12000, 400))&#10;1 - sum(dhyper(0:99, 3000, 12000, 400))&#10;phyper(99, 3000, 12000, 400, lower.tail=FALSE)&#10;1-phyper(99, 3000, 12000, 400)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These give 0.0078.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;dhyper(x, m, n, k)&lt;/code&gt; gives the probability of drawing exactly &lt;code&gt;x&lt;/code&gt;.  In the first line, we sum up the probabilities for 100 &amp;ndash; 400; in the second line, we take 1 minus the sum of the probabilities of 0 &amp;ndash; 99.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;phyper(x, m, n, k)&lt;/code&gt; gives the probability of getting &lt;code&gt;x&lt;/code&gt; or fewer, so &lt;code&gt;phyper(x, m, n, k)&lt;/code&gt; is the same as &lt;code&gt;sum(dhyper(0:x, m, n, k))&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;lower.tail=FALSE&lt;/code&gt; is a bit confusing.  &lt;code&gt;phyper(x, m, n, k, lower.tail=FALSE)&lt;/code&gt; is the same as &lt;code&gt;1-phyper(x, m, n, k)&lt;/code&gt;, and so is the probability of &lt;code&gt;x+1&lt;/code&gt; or more.  [I never remember this and so always have to double check.]&lt;/p&gt;&#10;&#10;&lt;p&gt;At that &lt;a href=&quot;http://stattrek.com/Tables/Hypergeometric.aspx&quot;&gt;stattrek.com site&lt;/a&gt;, you want to look at the last row, &quot;Cumulative Probability: P(X $\ge$ 100),&quot; rather than the first row &quot;Hypergeometric Probability: P(X = 100).&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Any &lt;em&gt;particular&lt;/em&gt; number that you draw is going to have small probability (in fact, &lt;code&gt;max(dhyper(0:400, 3000, 12000, 400))&lt;/code&gt; gives $\sim$0.050), and getting 101 or 102 or any larger number is even more interesting that 100, and the p-value is the probability, if the null hypothesis were true, of getting a result as interesting or more so than what was observed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a picture of the hypergeometric distribution in this case.  You can see that it's centered at 80 (20% of 400) and that 100 is pretty far out in the right tail.&#10;&lt;img src=&quot;http://i.stack.imgur.com/IXHjr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-09-30T03:11:32.837" Id="16259" LastActivityDate="2011-10-02T03:23:28.150" LastEditDate="2011-10-02T03:23:28.150" LastEditorUserId="5862" OwnerUserId="5862" ParentId="16247" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;Plotting you data should never be overlooked. In this example, the correlation between X and Y is 0, but surely the two variable are related.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10; |  x                         x&#10; |   x                      x &#10; |    x                    x   &#10; |     x                  x &#10;Y|      x               x&#10; |        x           x&#10; |          x       x&#10; |            x x x&#10; |_______________________________&#10;               X&#10;&#10;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-09-30T13:22:26.533" Id="16284" LastActivityDate="2011-09-30T13:38:03.323" LastEditDate="2011-09-30T13:38:03.323" LastEditorUserId="6581" OwnerUserId="6581" ParentId="16264" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;From what you've written above, it sounds like you want to use a Poisson Distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Poisson_distribution&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;with lambda equal to:&#10;f*p  =# times gc will run per hour (assuming that &quot;p&quot; is just a draw from a uniform distribution)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-30T19:41:31.700" Id="16300" LastActivityDate="2011-09-30T19:41:31.700" OwnerUserId="6446" ParentId="16229" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;If I'm using a machine learning model (e.g. boosted regression trees like gbm in R) on a dataset, what does it mean if there's a significant difference between the OOB estimated optimal # of iterations and the optimal # of iterations on the test set (I'm holding out 20% of the data)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I ask because I'm trying to model a time series response variable, and when I take the training set and sample from it without replacement, then run gbm, I get pretty different OOB and test set optimal iterations. However, when I leave the training set in its original form (oldest date to newest date), the OOB and test set optimal iterations are much closer. I'm beginning to wonder if the more recent data I have is quite a bit different from the old data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Again general question so let me know if you need more info and I will clarify as I can.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-09-30T21:10:27.993" Id="16304" LastActivityDate="2011-11-25T03:56:05.830" LastEditDate="2011-09-30T21:51:24.223" LastEditorUserId="88" OwnerUserId="6434" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;boosting&gt;" Title="Different optimal number of boosting iterations obtained from OOB and on test" ViewCount="213" />
  <row AcceptedAnswerId="16311" AnswerCount="5" Body="&lt;p&gt;So let's say I have a bunch of data points in R^n, where n is pretty big (like, 50). I know this data falls into 3 clusters, and I know which cluster each data point is a part of. All I want to do is visualize these clusters in 2D in such a way as to maximize the visual between-clusters separation that I see, with the goal being to prove that the clusters are easily separable based on the location of the data point in R^n alone.&lt;/p&gt;&#10;&#10;&lt;p&gt;The way I've been going about this up until now involves doing a PCA transform on the data points and then visualizing pairs of PCs at random until I find one where the clusters appear to be pretty cleanly separated. This approach seems pretty ad hoc though, and it seems like there should be an easy way to find a PCA-style rotation of the data that, instead of maximizing overall variance, maximizes inter-cluster separation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a standard technique out there that does this? If not, any ideas about how to create such a transformation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-09-30T23:46:15.787" FavoriteCount="2" Id="16305" LastActivityDate="2011-10-03T20:16:21.067" OwnerUserId="6595" PostTypeId="1" Score="7" Tags="&lt;clustering&gt;&lt;pca&gt;&lt;dimensionality-reduction&gt;&lt;data-visualization&gt;" Title="Dimensionality reduction technique to maximize separation of known clusters?" ViewCount="575" />
  <row Body="&lt;p&gt;Hidden Markov models would apply if the data were random emissions from some underlying unobserved Markov model; I wouldn't rule that out, but it doesn't seem a very natural model.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would think about &lt;a href=&quot;http://en.wikipedia.org/wiki/Point_process&quot; rel=&quot;nofollow&quot;&gt;point processes&lt;/a&gt;, which match your particular data well.  There is a great deal of work on predicting earthquakes (though I don't know much about it) and even &lt;a href=&quot;http://www.nytimes.com/2011/08/16/us/16police.html&quot; rel=&quot;nofollow&quot;&gt;crime&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If there are many different people printing, and you're just seeing the times but not the individual identities, a Poisson process might work well (the superposition of multiple independent point processes is approximately Poisson), though it would have to be inhomogeneous (the chance of a point varies over time): people are less likely to be printing at 3am than at 3pm.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the &lt;a href=&quot;http://en.wikipedia.org/wiki/Non-homogeneous_Poisson_process&quot; rel=&quot;nofollow&quot;&gt;inhomogeneous Poisson process&lt;/a&gt; model, the key would be getting a good estimate of the chance of a print job at a particular time on a particular day.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If these print times are for students in a classroom, though, it could be quite tricky, as they're not likely to be independent and so the Poisson process wouldn't work well.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.math.ucla.edu/~mbshort/papers/crime2.pdf&quot; rel=&quot;nofollow&quot;&gt;Here's a link to a paper&lt;/a&gt; on the crime application.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-09-30T23:46:49.690" Id="16306" LastActivityDate="2011-10-01T02:20:15.603" LastEditDate="2011-10-01T02:20:15.603" LastEditorUserId="5862" OwnerUserId="5862" ParentId="16302" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;Everything I have read has indicated that when performing ANOVA, the null and alternative hypothesis are always the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$H_0$: There is no difference in the means,&lt;/li&gt;&#10;&lt;li&gt;$H_a$: There is a difference in the means.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;But if I have a treatment and I expect, based on past experience or literature, that this treatment would perform better than the other, can the hypothesis with ANOVA ever be:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$H_0$: A would equal or worse than B,&lt;/li&gt;&#10;&lt;li&gt;$H_a$: A would be better than B?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I realize the above can be tested with a one-tailed $t$-test, but is the same hypothesis correct/possible for ANOVA, or is it a statistical no no?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-01T01:01:14.563" Id="16308" LastActivityDate="2011-10-02T07:18:57.960" LastEditDate="2011-10-02T07:18:57.960" LastEditorUserId="183" OwnerUserId="6596" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;" Title="How to test a directional hypothesis using ANOVA?" ViewCount="1108" />
  
  <row Body="&lt;p&gt;It's correct and reasonable, but ANOVA looks at square of the effect, so you have to go back to the one-sided t-test.  But with two treatments, the t-test and ANOVA &lt;em&gt;are&lt;/em&gt; the same thing; the ANOVA F statistic is just $t^2$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-10-01T01:32:35.540" Id="16309" LastActivityDate="2011-10-01T02:09:24.293" LastEditDate="2011-10-01T02:09:24.293" LastEditorUserId="5862" OwnerUserId="5862" ParentId="16308" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The particular table helps a lot.  The Fisher's exact test assigns probabilities to tables with these particular marginals using the hypergeometric distribution.  In this case, we're thinking of drawing 9 balls from an urn (the cases) with 2852 white balls (exposed) and 2861 black balls (not exposed).  The number of white balls drawn is the count for exposed cases.&#10;The distribution is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0     1     2     3     4     5     6     7     8     9 &#10;0.002 0.018 0.071 0.165 0.247 0.246 0.163 0.070 0.017 0.002&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The one-sided test in your output is giving the probability of 2 or fewer: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.002 + 0.018 + 0.071 = 0.0904&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The one-sided test in the other direction would give the probability of 2 or more, which is 1 minus the probability of 0 or 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 - (0.002 + 0.018) = 0.98&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the two-sided test is the probability of 0, 1, 2, 7, 8, or 9, which does come to &lt;code&gt;0.179&lt;/code&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So the p-values for the two one-tailed tests don't add to one, because they each include the particular observed value and the distribution is discrete.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-01T13:59:37.917" Id="16322" LastActivityDate="2011-10-02T04:27:10.760" LastEditDate="2011-10-02T04:27:10.760" LastEditorUserId="5862" OwnerUserId="5862" ParentId="16319" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Rescaling the variable as if it were continuous is easy enough. If it's centered on zero and the end points represent real extremes you can just just scale something like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_{SCALED} = X_{ORIG} \times MAX_{SCALED}/MAX_{ORIG}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, as was suggested in the answer to the other question linked in the comment above, scaling Likert scales is, for substantive reasons, not a great idea. Simple scaling with you result in non-integer answers which are impossible on either scale and might distort the substantive meaning that comes from scales where numbers represent real statements (e.g., 2 means &quot;somewhat satisfied&quot; while 0 means &quot;neutral&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm generally hesitant to treat Likert scales as continuous in the first place. Look carefully at the distribution of answers on your two different scales to get a sense of any distortion that you have. It may make more substantive sense to do something other than scaling (i.e., collapsing categories in one or more of the scales and then comparing the variables as categorical).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-01T14:42:31.167" Id="16324" LastActivityDate="2011-10-01T14:42:31.167" OwnerUserId="6575" ParentId="16320" PostTypeId="2" Score="1" />
  
  
  <row Body="" CommentCount="0" CreationDate="2011-10-02T18:27:56.873" Id="16366" LastActivityDate="2011-10-02T18:27:56.873" LastEditDate="2011-10-02T18:27:56.873" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="igraph is a collection of software packages for graph theory and network analysis. Its core is a software library written in C/C++, and it has interfaces to GNU R and Python." CommentCount="0" CreationDate="2011-10-02T18:27:56.873" Id="16367" LastActivityDate="2011-10-02T20:37:43.657" LastEditDate="2011-10-02T20:37:43.657" LastEditorUserId="3748" OwnerUserId="3748" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Section 4.4 in the SAS course notes on &lt;em&gt;&lt;a href=&quot;http://books.google.com/books?id=zR1UPwAACAAJ&amp;amp;source=gbs_book_other_versions&quot; rel=&quot;nofollow&quot;&gt;Multivariate Statistical Methods: Practical Research Applications&lt;/a&gt;&lt;/em&gt; there are examples for empirical validation and scoring.&lt;/p&gt;&#10;&#10;&lt;p&gt;You'd want to consider using the testlist or testout options for example using proc discrim&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;proc discrim data = old-data testdata = new-data testlist;&lt;br /&gt;&#10;  class variabls;&lt;br /&gt;&#10;  priors priors;&lt;br /&gt;&#10;  var variables;&lt;br /&gt;&#10;  run;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;proc discrim data = old-data testdata = new-data testout = scored-data;&lt;br /&gt;&#10;  class variable;&lt;br /&gt;&#10;  priors priors;&lt;br /&gt;&#10;  var variables;&lt;br /&gt;&#10;  run;&lt;br /&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2011-10-02T19:20:45.533" Id="16368" LastActivityDate="2011-10-02T19:20:45.533" OwnerUserId="4325" ParentId="16333" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Assuming that both studies are testing the same null hypothesis, your question exposes an important problem with Neyman-Pearson type hypothesis testing. If we don't take the first result as final then we are not 'playing by the rules' and we don't control the rate of false positive results. That is why Fisher repeatedly complained that the N-P method was good for industrial quality control and acceptance procedures but all but useless for scientific enquiry.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really want a composite result from the two studies then you need to use the exact P values rather than the N-P binary division between significant and not significant. Google 'combining p values' to get a start (I'm on leave playing with a iPad so I don't have access to my resources, so that's the best I can do for you at the moment.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-02T22:40:17.173" Id="16371" LastActivityDate="2011-10-02T22:40:17.173" OwnerUserId="1679" ParentId="16326" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;A contingency table analysis will suffice.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example, in Stata,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; tabi 13 24 \ 6 37, exact chi2&#10;&#10;           |          col&#10;       row |         1          2 |     Total&#10;-----------+----------------------+----------&#10;         1 |        13         24 |        37 &#10;         2 |         6         37 |        43 &#10;-----------+----------------------+----------&#10;     Total |        19         61 |        80 &#10;&#10;&#10;&#10;  Pearson chi2(1) =   4.9272   Pr = 0.026&#10;           Fisher's exact =                 0.036&#10;   1-sided Fisher's exact =                 0.025&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The two groups are likely to really be different, and this difference is statistically significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fisher's exact test P value = 0.036&#10;Chi-squared P value = 0.026&lt;/p&gt;&#10;&#10;&lt;p&gt;95% confidence intervals of the difference can also be obtained easily:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;csi 13 6 24 37, exact&#10;&#10;                 |   Exposed   Unexposed  |      Total&#10;-----------------+------------------------+------------&#10;           Cases |        13           6  |         19&#10;        Noncases |        24          37  |         61&#10;-----------------+------------------------+------------&#10;           Total |        37          43  |         80&#10;                 |                        |&#10;            Risk |  .3513514    .1395349  |      .2375&#10;                 |                        |&#10;                 |      Point estimate    |    [95% Conf. Interval]&#10;                 |------------------------+------------------------&#10; Risk difference |         .2118165       |    .0263769     .397256 &#10;      Risk ratio |         2.518018       |    1.063687    5.960791 &#10; Attr. frac. ex. |         .6028623       |    .0598736     .832237 &#10; Attr. frac. pop |         .4124847       |&#10;                 +-------------------------------------------------&#10;                                  1-sided Fisher's exact P = 0.0250&#10;                                  2-sided Fisher's exact P = 0.0356&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-10-03T00:09:49.117" Id="16374" LastActivityDate="2011-10-03T00:16:11.743" LastEditDate="2011-10-03T00:16:11.743" LastEditorUserId="561" OwnerUserId="561" ParentId="16373" PostTypeId="2" Score="7" />
  <row Body="&lt;h3&gt;How can I mathematically determine the probability that Y is greater than zero?&lt;/h3&gt;&#10;&#10;&lt;p&gt;$P(x_1&amp;gt;0) + P(x_2&amp;gt;0) -P(x_1&amp;gt;0)\times P(x_2&amp;gt;0)$  or use the complement : $1-P(x_1&amp;lt;0)\times P(x_2&amp;lt;0)$ ... I think.. LOL&lt;/p&gt;&#10;&#10;&lt;h3&gt;How can I use a simulation to determine the probability that Y is greater than zero?&lt;/h3&gt;&#10;&#10;&lt;p&gt;Have you tried?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-03T06:52:08.073" Id="16384" LastActivityDate="2011-10-03T22:03:01.463" LastEditDate="2011-10-03T22:03:01.463" LastEditorUserId="183" OwnerUserId="6369" ParentId="16380" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Hi I am trying to check the existence of change points in the mean and in the variance using R. My data is not normally distributed, so I want to use the cusum test. Also I want the significance level to be 0.05.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to use the package &quot;changepoints&quot;, but for change points in the mean I couldn't find a way of choosing the significance level because it says the asymptotic penalty is not available with cusum!&lt;/p&gt;&#10;&#10;&lt;p&gt;For the variance it lets me choose the significance level using:&#10;&lt;code&gt;single.var.css(x,penalty=&quot;Asymptotic&quot;,value=0.05,class=TRUE,param.estimates=TRUE)&lt;/code&gt;&#10;however it always detects a change point. Sometimes it looks like a real changepoint, but when the series doesn't seem to have one it just detects a change point in the end of the series.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone knows a way of calculating change points in R using the cusum test with a significance level of 0.05?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for helping!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-03T12:20:47.057" FavoriteCount="1" Id="16398" LastActivityDate="2011-10-03T12:20:47.057" OwnerUserId="5034" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;change-point&gt;" Title="Change points in the mean and in the variance using R" ViewCount="444" />
  
  <row AcceptedAnswerId="16425" AnswerCount="1" Body="&lt;p&gt;I am learning logistic regression from The elements of statistical learning: data mining, inference, and prediction, by Trevor Hastie, Robert Tibshirani, Jerome H. Friedman.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $G$ is a random variable representing response taking class label in $\{1,\ldots,K \}$, and $X$ is a random vector representing predictor taking values in $\mathbb{R}^n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;After modeling $P(G = k|X = x; \theta), k=1,\ldots,K, x \in \mathbb{R}^n$ by logistic model in &lt;a href=&quot;http://books.google.com/books?id=tVIjmNS3Ob8C&amp;amp;printsec=frontcover&amp;amp;dq=elements%20of%20statistical%20learning&amp;amp;hl=en&amp;amp;ei=styJTv_WIab40gGT7PjjDw&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=book-thumbnail&amp;amp;resnum=1&amp;amp;ved=0CDEQ6wEwAA#v=snippet&amp;amp;q=%22The%20logistic%20regression%20model%20arises%20from%20the%20desire%20to%20model%20the%20posterior%22&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;section 4.4 Logistic Regression&lt;/a&gt;, the parameter $\theta$ is estimated from samples $\{(x_i,k_i), i=1,\ldots,N \}$ in section &lt;a href=&quot;http://books.google.com/books?id=tVIjmNS3Ob8C&amp;amp;printsec=frontcover&amp;amp;dq=elements%20of%20statistical%20learning&amp;amp;hl=en&amp;amp;ei=styJTv_WIab40gGT7PjjDw&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=book-thumbnail&amp;amp;resnum=1&amp;amp;ved=0CDEQ6wEwAA#v=snippet&amp;amp;q=Logistic%20regression%20models%20are%20usually%20%EF%AC%81t%20by%20maximum%20likelihood,&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;4.4.1 Fitting Logistic Regression Models&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Logistic regression models are usually ﬁt by maximum likelihood, using&#10;  the conditional likelihood of $G$ given $X$. Since $P(G|X)$ completely&#10;  specifies the conditional distribution, the multinomial distribution&#10;  is appropriate. The log likelihood for $N$ observations is  $$ℓ(θ) =
&#10;\log P((x_1,k_1),\ldots,(x_N,k_N)) 
  <row AnswerCount="1" Body="&lt;p&gt;Using the sde package in R, I would like to simulate the following model for stock prices $p_t$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{d}\sigma^2_t = (\theta_1 - \theta_2\sigma^2_t)\mathrm{d}t + \theta_3\sigma_t\mathrm{d}W_{\sigma,t}$  (CIR model used for stochastic volatility)&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{d}\log{p_t} = \sigma_t\mathrm{d}W_{p,t}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where correlation between $W_{p,t}$ and $W_{\sigma,t}$ is possibly non-zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;Q: Is that possible? Is it even possible to simulate multivariate SDEs in the &quot;sde&quot; package the way one can in S+FinMetrics using the &lt;code&gt;gensim&lt;/code&gt; functions? Is there another package in R that might do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm able to simulate the variance process simply enough with the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(sde)&#10;sig2 &amp;lt;- sde.sim(X0=0.04,  theta=c(0.3141, 8.0369, 0.43),  model=&quot;CIR&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can then simulate the price series (starting with initial price = 100) using: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logr &amp;lt;- rnorm(n=length(sig2),sd=sqrt(sig2))&#10;logp &amp;lt;- cumsum(c(log(100),logr))&#10;p &amp;lt;- exp(logp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But this approach seems unnecessarily clunky and can't capture non-zero correlation between the Brownian Motions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-03T18:20:14.517" FavoriteCount="2" Id="16416" LastActivityDate="2012-09-20T12:40:43.397" OwnerUserId="3577" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;simulation&gt;&lt;stochastic-processes&gt;" Title="Using the sde package in R to simulate a SV model with leverage" ViewCount="734" />
  <row Body="&lt;p&gt;I won't try to deliver my own answer, but I would refer you to the &lt;a href=&quot;http://whatisasurvey.info/&quot; rel=&quot;nofollow&quot;&gt;&quot;What Is a Survey?&quot;&lt;/a&gt; booklet compiled by the Survey Research Methods Section of the American Statistical Association. (Fritz Scheuren endorsing it on the title page is a former President of ASA from about five years ago. He used to be a high profile statistician in federal agencies such as the Social Security Administration and Internal Revenue Service, and now semi-retired from government to continue working as a VP of the National Opinion Research Center at University of Chicago.) The booklet delivers a clear and concise explanation of when and why you can, or can not, extrapolate the survey findings to the target population.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-03T19:15:20.337" Id="16423" LastActivityDate="2011-10-03T19:15:20.337" OwnerUserId="5739" ParentId="16413" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Whether there's a problem or not is dependent upon the degree of correlation, not whether it's perfect (and dividing by 100 doesn't affect the correlation one bit).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, what each coefficient will be reduced to, (depending on the regression method) is the unique variance contributed by each of these factors over and above what is contributed by the other factors.  Therefore, they will change.  Let's say I wanted to use height and weight as predictors of self esteem.  These are two correlated variables but let's assume they're not correlated too much and they're allowed in my regression together.  Perhaps I checked the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multicollinearity&quot; rel=&quot;nofollow&quot;&gt;tolerance&lt;/a&gt; and it's OK.  When they're both part of the regression then the coefficient I find for height is not a simple effect of height but instead the effect of height that's extra over and above the effect of weight that is correlated with it.  If I had left weight out then the effect of height would include part of the effect of weight (the part correlated with height).&lt;/p&gt;&#10;&#10;&lt;p&gt;So, not only will the coefficients change in value but the interpretation of them will change as well.  Consider how you would interpret the unique contribution of the amount of broadband connections over and above the amount of broadband connections/country.  Think about that, because that's what your regression would be about with both included.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to consider how you would interpret an alternate regression using the same information.  What if you made raw population and number of connections the two predictors?  They too will be correlated, but far less so.  Now think about what it would mean to have an effect of the number of connections that's over and above the effect of the population.  To me, that's a more sensible idea.  But, it's your study and I have no idea your research questions.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-10-04T00:46:22.577" Id="16443" LastActivityDate="2011-10-04T03:10:04.520" LastEditDate="2011-10-04T03:10:04.520" LastEditorUserId="601" OwnerUserId="601" ParentId="16439" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I'm thinking you need to consider a Bonferroni type of approach. &lt;/p&gt;&#10;&#10;&lt;p&gt;You want the probability that there's no error overall&lt;/p&gt;&#10;&#10;&lt;p&gt;P(error)= 1- P(No Error)&lt;/p&gt;&#10;&#10;&lt;p&gt;Alpha = 1-(1-Alpha)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, we're looking at an overall case.. So I'll call that alphaoverall and alphacell is the chances of making an error in any particular cell..&lt;/p&gt;&#10;&#10;&lt;p&gt;alphaoverall = 1-(1-alphacell)^n&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you want to get z score in terms of an overall alpha..&lt;/p&gt;&#10;&#10;&lt;p&gt;alphaoverall = 1-(1-alphacell)^n&lt;/p&gt;&#10;&#10;&lt;p&gt;alphaoverall -1= -(1-alphacell)^n&lt;/p&gt;&#10;&#10;&lt;p&gt;1-alphaoverall = (1-alphacell)^n&lt;/p&gt;&#10;&#10;&lt;p&gt;take the nth root&lt;/p&gt;&#10;&#10;&lt;p&gt;(1-alphaoverall)^(1/n)= 1- alphacell&lt;/p&gt;&#10;&#10;&lt;p&gt;(1-alphaoverall)^(1/n)-1= - alphacell&lt;/p&gt;&#10;&#10;&lt;p&gt;1-(1-alphaoverall)^(1/n)=  alphacell&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what this means, is say you want a 20% error rate for the whole table, plug in .2 for alpha over all, with your number of cell entries as n.. Say you have a 2*3 table, then that's n= 6.&lt;/p&gt;&#10;&#10;&lt;p&gt;1-(1-.2)^(1/6) =  0.03650752&#10; ..&lt;/p&gt;&#10;&#10;&lt;p&gt;You'll have a 2 sided test, so..&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, cut that .0365 in 2 ,  0.01825376  now find the score that has that much area in the upper tail, and find the appropriate z is 2.09.&lt;/p&gt;&#10;&#10;&lt;p&gt;My hunch though, is that the errors in each cell might be correlated. Which I think will matter.. If you don't have an error in one cell, it makes it less likely to have an error in the next cell. Doesn't it?  Come to think of it, if you have a 2x2 table, and one point estimate is very close to the true parameter, the others would have to be very close as well..&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-10-04T02:08:43.077" Id="16447" LastActivityDate="2011-11-07T01:59:30.747" LastEditDate="2011-11-07T01:59:30.747" LastEditorUserId="6369" OwnerUserId="6369" ParentId="16193" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;What you require is a truncated geometric distribution.  The usual geometric distribution has a support of $0,1,\dots$ (i.e. to infinity) whereas your one has support only up to $12$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you need to renormalise the pdf.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Pr(X=x|p)=Ap(1-p)^x\implies A^{-1}=\sum_{x=0}^{12}p(1-p)^x=1-(1-p)^{13}$$&#10;$$\implies Pr(X=x|p)=\frac{p(1-p)^x}{1-(1-p)^{13}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we take expectations over this pdf (letting $q=1-p=\frac{12}{13}$):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(X|p)=\frac{p}{1-q^{13}}\sum_{x=0}^{12}xq^x$$&#10;$$=\frac{pq}{1-q^{13}}\sum_{x=0}^{12}\frac{\partial}{\partial q}q^{x}
  
&#10;  a_{T-1} &amp;amp; a_{T} \\
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which has a unique solution if $a_{T-1}b_{T}-b_{T-1}a_{T}\neq 0$.  You can choose your categories for $T,T-1$ so that this condition is satisfied, because of the permutational symmetry in the problem.  If you can't, then I think (but not sure) this means you really only have one constraint, and one of them can be dropped (as all of the b's are essentially functions of the a's in a similar manner to an autoregressive equation).  The solution, if it exists, is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;  -b_{T-1} &amp;amp; a_{T-1} 
&#10; \begin{matrix}p_{T-1} &amp;amp; = &amp;amp; \frac{(1-s_a)b_T-(1-s_b)a_{T}}{a_{T-1}b_{T}-b_{T-1}a_{T}}\\ p_{T} &amp;amp; = &amp;amp; \frac{(1-s_a)b_{T-1}-(1-s_b)a_{T-1}}{a_{T-1}b_{T}-b_{T-1}a_{T}}\end{matrix}
  
  <row Body="&lt;p&gt;I'd go with YAML. Straight forward to edit and has plenty parsers in different languages:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;---&#10;- &#10;  question: 1 + 1&#10;  incorrect:&#10;    - 1&#10;    - 3&#10;    - 4&#10;  correct: 2&#10;-&#10; question: What is the capital city of the country renowned for koalas, emus, and kangaroos?&#10; incorrect:&#10;   - Melbourne&#10;   - Sydney&#10;   - Australia&#10; correct: Canberra&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You could then write a little script to randomly mix the incorrect with correct answers and output the LaTeX suggested in DQdlM's answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: This ruby script:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require 'yaml'&#10;&#10;questions = YAML.load(File.read(ARGV.first))&#10;questions.each_with_index do |question,index|&#10;  answers = question['incorrect'].map{|i| '    \choice ' + i.to_s }&#10;  answers &amp;lt;&amp;lt; '    \CorrectChoice ' + question['correct'].to_s&#10;&#10;  output = [&quot;\\question{#{index + 1}}&quot;]&#10;  output &amp;lt;&amp;lt; question['question']&#10;  output &amp;lt;&amp;lt; &quot;  \\begin{choices}&quot;&#10;  output &amp;lt;&amp;lt; answers.sort_by{rand}&#10;  output &amp;lt;&amp;lt; &quot;  \\end{choices}&quot;&#10;  output &amp;lt;&amp;lt; &quot;\n&quot;&#10;&#10;  puts output.flatten.join(&quot;\n&quot;)&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Will produce the following output&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;\question{1}&#10;1 + 1&#10;  \begin{choices}&#10;    \choice 4&#10;    \choice 1&#10;    \choice 3&#10;    \CorrectChoice 2&#10;  \end{choices}&#10;&#10;\question{2}&#10;What is the capital city of the country renowned for koalas, emus, and kangaroos?&#10;  \begin{choices}&#10;    \choice Melbourne&#10;    \choice Sydney&#10;    \CorrectChoice Canberra&#10;    \choice Australia&#10;  \end{choices}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-10-04T14:33:26.350" Id="16476" LastActivityDate="2011-10-04T14:52:24.307" LastEditDate="2011-10-04T14:52:24.307" LastEditorUserId="5659" OwnerUserId="5659" ParentId="16454" PostTypeId="2" Score="6" />
&#10;f(\phi) &amp;amp;= 3\sin(\phi), &amp;amp;0 \le \phi \le \pi/4; \\
  <row Body="&lt;p&gt;You could do a logistic regression/chi-square test of association for each variable and only retain those that have a p-value less than some value, say .2.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-04T16:54:11.390" Id="16484" LastActivityDate="2011-10-04T16:54:11.390" OwnerUserId="2310" ParentId="16480" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="16512" AnswerCount="1" Body="&lt;p&gt;If $g=f(x,y)$ is a function of independent random variables $x$ and $y$ then how do we arrive at the expression for the probability density function of $g$, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_G(g) = \iint f_X(x)f_Y(y)\delta(g-f(x,y))\mbox{d}x\mbox{d}y\ ?$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been looking at some statistics books but cannot find it. Neither am I able to derive it myself. Any help would be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-04T23:55:50.623" FavoriteCount="1" Id="16509" LastActivityDate="2011-10-05T01:57:36.250" LastEditDate="2011-10-05T01:55:44.333" LastEditorUserId="919" OwnerUserId="4714" PostTypeId="1" Score="4" Tags="&lt;pdf&gt;&lt;intuition&gt;" Title="PDF for a function of random variables " ViewCount="147" />
&#10;\int_{-\infty}^{t}\delta(g - f(x,y))dg =\int_{-\infty}^{t-f(x,y)}\delta(u)du = I_{t - f(x,y) \ge 0} = I_{f(x,y)\le t}
&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $I$ is an indicator function).  This is the &lt;em&gt;definition&lt;/em&gt; of $\delta$.  Now write the CDF for $g$ as an integral; namely,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
&#10;\int_{-\infty}^{t} f_G(g)dg = &amp;amp;G(t) \\
&#10;     = &amp;amp;\mathbb{P}[f(x,y)\le t] \\
  <row AcceptedAnswerId="16521" AnswerCount="2" Body="&lt;p&gt;How can I calculate the confidence interval of a mean in a non-normally distributed sample?&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand bootstrap methods are commonly used here, but I am open to other options. While I am looking for a non-parametric option, if someone can convince me that a parametric solution is valid that would be fine. The sample size is &gt; 400.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone could give a sample in R it would be much appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-05T03:14:34.283" FavoriteCount="4" Id="16516" LastActivityDate="2011-10-05T17:47:28.333" OwnerUserId="179" PostTypeId="1" Score="12" Tags="&lt;confidence-interval&gt;&lt;nonparametric&gt;&lt;bootstrap&gt;&lt;descriptive-statistics&gt;" Title="How can I calculate the confidence interval of a mean in a non-normally distributed sample?" ViewCount="6180" />
  <row Body="&lt;p&gt;The answer to your second question is yes: The sample mean is a minimum contrast estimator when your function $l_0$ is $(x-u)^2$, when x and u are real numbers, or $(x-u)&amp;#39;(x-u)$, when x and u are column vectors. This follows from least-squares theory or differential calculus.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A minimum contrast estimator is, under certain technical conditions, both consistent and asymptotically normal.  For the sample mean, this already follows from the LLN and the central limit theorem.  I don't know that minimum contrast estimators are &quot;optimal&quot; in any way. What's nice about minimum contrast estimators is that many robust estimators (e.g. the median, Huber estimators, sample quantiles) fall into this family, and we can conclude that they are consistent and asymptotically normal just by applying the general theorem for minimum contrast estimators, so long as we check some technical conditions (though often this is much difficult than it sounds).&lt;/p&gt;&#10;&#10;&lt;p&gt;One notion of optimality that you don't mention in your question is efficiency which, roughly speaking, is about how large a sample you need to get an estimate of a certain quality.  See &lt;a href=&quot;http://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency&lt;/a&gt; for a comparison of the efficiency of mean and median (mean is more efficient, but the median is more robust to outliers). &lt;/p&gt;&#10;&#10;&lt;p&gt;For the third question, without some restriction on the set of functions f over which you are finding the argmin, I don't think the sample mean will be optimal. For any distribution P, you can fix f to be a constant that ignores the $x_i$'s and minimizes the loss for the particular P.  Sample mean can't beat that. &lt;/p&gt;&#10;&#10;&lt;p&gt;Minimax optimality is a weaker condition than the one you give: instead of asking that $f^*$ be the best function for any $P$ in a class, you can ask that $f^*$ have the best worst-case performance.  That is, between the argmin and the expectation, put in a $\max_{P\in F}$.  Bayesian optimality is another approach: put a prior distribution on $P\in F$, and take the expectation over $P$ as well as the sample from $P$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-10-05T03:57:28.783" Id="16519" LastActivityDate="2011-10-11T15:56:22.570" LastEditDate="2011-10-11T15:56:22.570" LastEditorUserId="6640" OwnerUserId="6640" ParentId="16507" PostTypeId="2" Score="3" />
&#10;$$&#10;for a $D(y)$ with mean zero under $F$ and&#10;$$
&#10;\sqrt{n}\left(\hat{\theta}_{n}-\theta_{0}\right)\stackrel{d}{\rightarrow}\mathcal{N}_{p}(0,J^{-1} K J^{-1}).
  
  <row Body="&lt;p&gt;Cronbach alpha and its versions are measures of &lt;em&gt;inter-item homogeneity&lt;/em&gt;. By definition, a scale should be homogeneous enough to be able to measure one construct. Reliability approach poses that items in a scale are all the &quot;same&quot; except for some biases that are treated as &quot;random error&quot;, unrelated to the measured construct, and that cancel each other in the end. So, alpha is not suitable for battery of constructs: it is unidimentional. Another pair of shoes, though, is that truly unidimensional construct is somewhat unrealistic concept. In factor analysis of a scale it often splits into main factor which is the construct of interest and few minor factors which interfere. Still, reliability approach treat these systematic factors as noise.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-10-05T06:37:36.473" Id="16525" LastActivityDate="2011-10-05T06:37:36.473" OwnerUserId="3277" ParentId="16524" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="16550" AnswerCount="1" Body="&lt;p&gt;I'm usually a UseR.&#10;However, for didactic purposes I have to use SPSS today.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to specify a General linear model with ordinal structure because what I'm examining is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Change in Likert scale scores (1,2,3,4,5) WITHIN people, BETWEEN intervention groups over 2 timepoints.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can specify a full factorial model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;* Generalized Linear Models. &#10; GENLIN Question1 (ORDER=ASCENDING) BY Group PrePost Participant (ORDER=ASCENDING) &#10;  /MODEL Group PrePost Group*PrePost &#10;  DISTRIBUTION=MULTINOMIAL LINK=CUMLOGIT &#10;   /CRITERIA METHOD=FISHER(1) SCALE=1 COVB=MODEL MAXITERATIONS=100 MAXSTEPHALVING=5     &#10;   PCONVERGE=1E-006(ABSOLUTE) SINGULAR=1E-012 ANALYSISTYPE=3(LR)      CILEVEL=95 CITYPE=WALD LIKELIHOOD=FULL &#10;   /MISSING CLASSMISSING=EXCLUDE &#10;   /PRINT CPS DESCRIPTIVES MODELINFO FIT SUMMARY SOLUTION.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But that model doesn't take account of the non-independence of the participant's measures over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be massively grateful if someone could help me (help someone else) speify a model that would have a random effects component, taking into account the &quot;clustering within patients&quot;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-05T16:41:56.110" Id="16546" LastActivityDate="2011-10-06T00:51:13.797" LastEditDate="2011-10-06T00:51:13.797" LastEditorUserId="183" OwnerUserId="6666" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;spss&gt;&lt;ordinal&gt;&lt;logistic&gt;" Title="How to perform repeated measures ordinal logistic regression using SPSS?" ViewCount="2142" />
  
  <row Body="&lt;p&gt;For this I would expect a wide variation, as well as an increasing trend with age.  If there is a lot of data then you could use &lt;a href=&quot;http://en.wikipedia.org/wiki/Box_plot&quot; rel=&quot;nofollow&quot;&gt;boxplots&lt;/a&gt; (or some of the modern variants such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Violin_plot&quot; rel=&quot;nofollow&quot;&gt;violin plots&lt;/a&gt;) segmented by age.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-05T21:48:15.870" Id="16573" LastActivityDate="2011-10-05T21:48:15.870" OwnerUserId="2958" ParentId="16566" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="16581" AnswerCount="2" Body="&lt;p&gt;How do I compare d-prime scores from 2 different conditions for the same individual, using the d-primes and standard errors?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-05T22:07:12.547" FavoriteCount="2" Id="16576" LastActivityDate="2011-10-05T23:36:53.340" OwnerUserId="6674" PostTypeId="1" Score="1" Tags="&lt;signal-detection&gt;" Title="How do I compare d-prime scores for 2 different conditions for the same individual?" ViewCount="2121" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;&quot;Ordinal&quot; versus &quot;interval&quot; versus &quot;ratio&quot; (versus &quot;nominal&quot;) is a statistical straitjacket.&lt;/strong&gt;  This classification can be useful for &lt;em&gt;suggesting&lt;/em&gt; appropriate analyses, but in no circumstances should it be used to constrain your options.  At this stage, it's almost a meaningless question.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, angular measurements arguably fit &lt;em&gt;none&lt;/em&gt; of the classical characterizations of data.  But this doesn't matter.  One thing you should be thinking about at this stage (in addition to experimental design) is the nature of the measuring process: how accurate is it?  How precise?  How repeatable?  What are the identifiable sources of variation (due, perhaps, to differences in measuring instrument, skill of the measurer, physical characteristics of the person being measured, and so on) and how might each source be contributing to overall measurement variation?  If you haven't identified these sources, do so before designing the experiment, because one of the experiment's secondary objectives will be to quantify the important sources of variation.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have databases of previous measurements, or can take preliminary measurements, then you can go further to characterize the likely (univariate) distribution of the measurement errors and to explore how that distribution might vary with other factors.  In standard ways this characterization can suggest appropriate ways to re-express the angles, if necessary, so that the comparisons are performed in meaningful and powerful ways.  However, it's likely that such a detailed study would not be necessary, provided reasonable care is taken to establish a reproducible, accurate measurement protocol and that measurements are recorded with full precision.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-06T15:38:54.543" Id="16603" LastActivityDate="2011-10-06T15:38:54.543" OwnerUserId="919" ParentId="16596" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;There are a few clear options for doing this in Python, and yes, it can be done:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There is always the &lt;a href=&quot;http://www.omegahat.org/RSPython/index.html&quot; rel=&quot;nofollow&quot;&gt;R/S+ - Python Interface&lt;/a&gt; which is essentially cheating and letting you use R from Python.&lt;/li&gt;&#10;&lt;li&gt;The probably next best option is &lt;a href=&quot;http://statsmodels.sourceforge.net/regression.html&quot; rel=&quot;nofollow&quot;&gt;StatsModels&lt;/a&gt;, which is a promising beginning development in terms of making Python a functional statistics workbench.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'll echo what other people have said however. This kind of work is really best handled by a statistics package. There's a reason R (and nearly every other statistics language) can act more like a command-line interface than a &quot;proper&quot; programming language. That's because a great deal of work is pretty back-and-forth, adding or subtracting a variable from a model, etc. In my mind, Python programmers would be well served also learning R if they're doing data-analysis. For that matter, I'm pretty sure R programmers would be well served learning Python if they ever have to deal with the actual data.&lt;/p&gt;&#10;&#10;&lt;p&gt;SAS programmers can look on smugly from the sidelines, then wonder where there several thousand dollars ran off to.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-06T18:00:06.983" Id="16617" LastActivityDate="2011-10-06T18:00:06.983" OwnerUserId="5836" ParentId="16605" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;In addition to the other comments/answers, in any study like this you need to be very aware of both order effects and placebo effects, see &lt;a href=&quot;http://revision3.com/scamschool/placebobands&quot; rel=&quot;nofollow&quot;&gt;this video&lt;/a&gt; for an example of what people can be convinced of.  Without double blinding and randomization of the order your study will have no more validity than what is shown in the video.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-06T18:14:38.720" Id="16621" LastActivityDate="2011-10-06T18:14:38.720" OwnerUserId="4505" ParentId="16596" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Not quite clear on your notation, so I'll set up the problem from&#10;scratch:&lt;/p&gt;&#10;&#10;&lt;p&gt;We want to map vectors $x\in\mathcal{R}^{m}$ to $y\in\mathcal{R}$. Let $A$&#10;be the design matrix -- that is, our sample of $x$ vectors are rows&#10;of $A$. We want to find $\alpha\in\mathcal{R}^{m}$ such that $y\approx x&amp;#39;\alpha$.&#10;In our objective function, rather than penalizing with $\lambda\alpha^{T}\alpha$ (the standard ridge penalty),&#10;we will penalize with $\lambda(\alpha-\alpha_{0})^{T}(\alpha-\alpha_{0})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we get&#10;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Expanding out the quadratic forms and dropping terms not involving $\alpha$, we find that this is equivalent to &#10;$$
&#10;$$&#10; If $K$ is symmetric and [strictly] positive definite (such as the identity matrix in the standard ridge regression case), then $\alpha=(A&amp;#39;A+\lambda K)^{-1}\left(A&amp;#39;y+\lambda K\alpha_{0}\right)$,&#10;otherwise, we replace the inverse by a generalized inverse, such as&#10;the pseudoinverse (pinv in matlab).&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-10-06T19:00:31.123" Id="16627" LastActivityDate="2011-10-06T22:20:36.677" LastEditDate="2011-10-06T22:20:36.677" LastEditorUserId="6640" OwnerUserId="6640" ParentId="16604" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to test some results I have for significance. It has been recommended that I use R and I am completely new to this.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Set-up:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Groups: two groups of 8 subjects (16 total)&lt;/li&gt;&#10;&lt;li&gt;Two conditions: alert and passive&lt;/li&gt;&#10;&lt;li&gt;Measurements: responses for three different stimuli (A, B, and C) measured in each condition&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Experiment: Testing the order of conditions&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Group one: Alert A, B and C followed by Passive A, B, and C &lt;/li&gt;&#10;&lt;li&gt;Group two: Passive A, B, and C followed by Alert A, B, and C&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Stimuli A, B and C are randomly interleaved in the experiment, does this matter in my ANOVA?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in making a between and within group comparison of responses to A, B, and C&lt;/p&gt;&#10;&#10;&lt;h3&gt;Here is what I am doing:&lt;/h3&gt;&#10;&#10;&lt;p&gt;My data is arranged in the following way&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Group Subject Condition Stimulus  Response&#10;One      S1    Alert          A    _Value_&#10;One      S1    Alert          B    _Value_&#10;One      S1    Alert          C    _Value_&#10;One      S1    Passive        A    _Value_&#10;One      S1    Passive        B    _Value_&#10;One      S1    Passive        C    _Value_&#10;One      S2    Alert          A    _Value_&#10;...&#10;&#10;Two      S9    Alert          A     _Value_&#10;Two      S9    Alert          B     _Value_&#10;Two      S9    Alert          C     _Value_&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is the code I used:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;My_anova = aov(Response ~ Condition * Stimulus * Group + &#10;           Error(Subject/Condition * Stimulus ), data=My_Data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Question&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is this model correctly specified in R?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-10-07T03:51:47.387" FavoriteCount="1" Id="16641" LastActivityDate="2011-10-07T07:26:19.950" LastEditDate="2011-10-07T04:24:56.393" LastEditorUserId="183" OwnerUserId="6695" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="Is this mixed ANOVA correctly specified in R?" ViewCount="459" />
  
  
  <row Body="&lt;p&gt;In probability theory, the expected value (or expectation, or mathematical expectation, or mean, or the first moment) of a random variable is the weighted average of all possible values that this random variable can take on. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example of Die&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let X represent the outcome of a roll of a six-sided die. More specifically, X will be the number of pips showing on the top face of the die after the toss. The possible values for X are 1, 2, 3, 4, 5, 6, all equally likely (each having the probability of  1/6). The expectation of X is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(X)=1\cdot(1/6)+2\cdot(1/6)+3\cdot(1/6)+4\cdot(1/6)+5\cdot(1/6)+6\cdot(1/6)=21/6$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you roll the die n times and compute the average (mean) of the results, then as n grows, the average will almost surely converge to the expected value, a fact known as the strong law of large numbers. One example sequence of ten rolls of the die is 2, 3, 1, 2, 5, 6, 2, 2, 2, 6, which has the average of 3.1, with the distance of 0.4 from the expected value of 3.5. The convergence is relatively slow: the probability that the average falls within the range 3.5 ± 0.1 is 21.6% for ten rolls, 46.1% for a hundred rolls and 93.7% for a thousand rolls. See the &lt;a href=&quot;http://en.wikipedia.org/wiki/Expected_value&quot; rel=&quot;nofollow&quot;&gt;figure&lt;/a&gt; for an illustration of the averages of longer sequences of rolls of the die and how they converge to the expected value of 3.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, the weights used in computing this average correspond to the probabilities in case of a discrete random variable, or densities in case of a continuous random variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-07T12:03:02.997" Id="16659" LastActivityDate="2012-04-01T00:32:08.393" LastEditDate="2012-04-01T00:32:08.393" LastEditorUserId="4856" OwnerUserId="5864" ParentId="16643" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Possibly you already know of these, but here they are anyway:&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://archive.ics.uci.edu/ml/&quot; rel=&quot;nofollow&quot;&gt;UCI Machine Learning Repositor&lt;/a&gt;y has many publicly accessible, real world data sets. &lt;/p&gt;&#10;&#10;&lt;p&gt;The US Government makes many of its datasets public at &lt;a href=&quot;http://www.data.gov/&quot; rel=&quot;nofollow&quot;&gt;data.gov&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want some tricky visualization data, I'd suggest looking at a classification task. Seems to me that the Bag of Words set on the UCI MLR has some nice properties, but I could be mistaken (been a while since I used it).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-07T15:38:08.287" Id="16675" LastActivityDate="2011-10-07T15:38:08.287" OwnerUserId="6446" ParentId="16130" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am looking at the text, &lt;em&gt;A Practitioner's Guide to Resampling for Data Analysis, Data Mining, and Modeling&lt;/em&gt;, by Phillip Good (&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1439855501&quot; rel=&quot;nofollow&quot;&gt;Amazon&lt;/a&gt;).  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I would like to see if anyone has read this book (or seen a review) and can offer objective advice on it (rating and level of material).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(I have a suspicion this question - being subjective - should be placed in a community location. Problem is, not sure how to do that. So, I will beg forgiveness in advance and look to a moderator to help direct me.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-07T16:48:40.107" Id="16680" LastActivityDate="2011-10-08T00:43:44.547" LastEditDate="2011-10-08T00:43:44.547" LastEditorUserId="183" OwnerUserId="2040" PostTypeId="1" Score="1" Tags="&lt;books&gt;" Title="Book review of &quot;A Practitioner's Guide to Resampling for Data Analysis, Data Mining and Modeling&quot;" ViewCount="176" />
  <row AcceptedAnswerId="16725" AnswerCount="2" Body="&lt;p&gt;A colleague of mine with little statistics experience is trying to perform an experimental evaluation of a computer program. He created a between subjects design and solicited test subjects.&lt;/p&gt;&#10;&#10;&lt;p&gt;11 people were given his &quot;new and improved&quot; computer program to use. 10 others got the &quot;old and boring&quot; computer program to use, for the same task. &lt;/p&gt;&#10;&#10;&lt;p&gt;He asked me, and several other people around the lab how to analyze his data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I told him he should examine the data for normality. If it was normally distributed, he should use a t-test. If it was not, he should use a Wilcoxon rank sum test.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of my colleagues told him he should use ANOVA, even though he only has 2 groups. Apparently using ANOVA on non-normal data in R produces a new degree of freedom measure which can be put into a t-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've never heard of such a thing. Is this true? Is it statistically valid? Why would anyone use it instead of just doing a rank-sum test?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-07T19:39:53.933" Id="16688" LastActivityDate="2011-10-08T21:45:57.870" LastEditDate="2011-10-07T20:26:04.020" LastEditorUserId="6446" OwnerUserId="6446" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;nonparametric&gt;&lt;t-test&gt;" Title="Why would I use ANOVA instead of a Rank-Sum test?" ViewCount="867" />
  
  <row Body="&lt;h3&gt;Reasons to prefer zero-one coding of binary variables:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The mean of a zero-one variable represents the proportion in the category represented by the value one (e.g., the percentage of males).&lt;/li&gt;&#10;&lt;li&gt;In a simple regression $y = a + bx$ where $x$ is the zero-one variable, the constant has a straightforward interpretation (e.g., $a$ is the mean of $y$ for females). &lt;/li&gt;&#10;&lt;li&gt;Any coding of a binary variable where the difference between the two values is one (i.e., zero-one, but also one-two) gives a straightforward interpretation to the regression coefficient (e.g., $b$ is the effect of going from female to male on y).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Assorted points about coding binary variables:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Any coding of a binary variable that preserves the order of the categories (e.g., female = 0, male = 1; female = 1, male = 2; female = 1007, male =2000; etc.) will not affect the correlation of the binary variable with other variables.&lt;/li&gt;&#10;&lt;li&gt;Any tables that report a binary variable in this way should make it clear how the variable was coded. It can also be useful to label the variable by the category that represent the value of one: e.g., &lt;code&gt;y = a + b * Male&lt;/code&gt; rather than &lt;code&gt;y = a + b * Gender&lt;/code&gt;.&lt;/li&gt;&#10;&lt;li&gt;For some binary variables, one category more naturally should be coded as one. For example, when looking at the difference between treatment and control, control should be zero, and treatment should be one, because the regression coefficient is best thought of as the effect of the treatment.&lt;/li&gt;&#10;&lt;li&gt;Flipping the categories (e.g., making female = 1 and male = 0, rather than female = 0 and male = 1) will flip the sign of correlations and regression coefficients.&lt;/li&gt;&#10;&lt;li&gt;In the case of gender, there is typically no natural reason to code the variable female = 0, male = 1, versus male = 0, female = 1. However, convention may suggest one coding is more familiar to a reader; or choosing a coding that makes the regression coefficient positive may ease interpretation. Also, in some contexts, one gender may be thought of as the reference category; for example, if you were studying the effect of being female in a male dominated profession on income, it might make sense to code male = 0, and female = 1, in order to speak of the effect of being female.&lt;/li&gt;&#10;&lt;li&gt;Scaling regression coefficients in thoughtful ways can have a powerful effect on the interpretability of regression coefficients. Andrew Gelman discusses this quite a bit; see for example his 2008 paper &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf&quot;&gt;Scaling regression inputs by dividing by two standard deviations (PDF)&lt;/a&gt; in &lt;em&gt;Statistics in Medicine&lt;/em&gt;, 27, 2865-2873.&lt;/li&gt;&#10;&lt;li&gt;Coding male and female as -1 and +1 is another option that can provide meaningful coefficients (see &lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/effect.htm&quot;&gt;&quot;what is effect coding&quot;&lt;/a&gt;).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2011-10-07T23:22:18.017" Id="16695" LastActivityDate="2011-10-12T22:15:01.603" LastEditDate="2011-10-12T22:15:01.603" LastEditorUserId="183" OwnerUserId="183" ParentId="16689" PostTypeId="2" Score="21" />
  
  
  <row Body="&lt;p&gt;I had assumed that this was because the field type often used to store gender is a bit field, and bit fields in SQL can only have the values 0 or 1. When you dump out the data, it comes out as 0 or 1, and so that's why you get those particular values.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you wanted to use 1 and 2, you'd have to use a bigger field type, which would take up more space, and thus make the whole database slightly bigger.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-08T12:49:53.430" Id="16706" LastActivityDate="2011-10-08T12:49:53.430" OwnerUserId="6713" ParentId="16689" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It's not the difference that's the problem, it's the small number in the smaller group. If you had 700 and 70, then much smaller effect sizes would be significant. If you had 38 and 38, smaller effect sizes. I tried this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(38483810)&#10;IV1 &amp;lt;- c(rep('A', 7), rep('B', 70))&#10;IV2 &amp;lt;- c(rep('A', 38), rep('B', 38))&#10;IV3 &amp;lt;- c(rep('A', 70), rep('B', 700))&#10;&#10;DV1 &amp;lt;- c(rnorm(7, 0, 2), rnorm(70, 1, 2))&#10;DV2 &amp;lt;- c(rnorm(38, 0, 2), rnorm(38, 1, 2))&#10;DV3 &amp;lt;- c(rnorm(70, 0, 2), rnorm(700, 1, 2))&#10;&#10;&#10;lm1 &amp;lt;- lm(DV1~IV1)&#10;lm2 &amp;lt;- lm(DV2~IV2)&#10;lm3 &amp;lt;- lm(DV3~IV3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;lm1 shows p = .47, lm2 p = .04, lm3 p = .001&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-08T22:13:45.527" Id="16728" LastActivityDate="2011-10-08T22:13:45.527" OwnerUserId="686" ParentId="16720" PostTypeId="2" Score="3" />
  
&#10;\sim N\left(\theta_1,\frac{1}{9}\sigma^2\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\theta_1$ is as you have defined it in your question.  This implies that $\frac{M-\theta_1}{\frac{1}{3}\sigma}\sim N(0,1)$.  We also have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{(n-I)s^2}{\sigma^2}=\frac{\sum_{i=1}^{I}\sum_{j=1}^{n_i}(Y_{ij}-\overline{Y}_i)^2}{\sigma^2}\sim\chi^2_{n-I}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We also have the standard result that $Z\sqrt{\frac{v}{\chi^2_v}}\sim T_v$ (i.e. a &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution&quot; rel=&quot;nofollow&quot;&gt;T-distribution&lt;/a&gt; with v degrees of freedom),  so we get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{M-\theta_1}{\frac{1}{3}\sigma}\sqrt{\frac{n-I}{\frac{(n-I)s^2}{\sigma^2}}}=\frac{M-\theta_1}{\frac{1}{3}s}\sim T_{n-I}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So a confidence interval would be (noting that you have $48-4=44$ degrees of freedom)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\overline{Y}_{1}-\frac{1}{3}(\overline{Y}_{2}+\overline{Y}_{3}+\overline{Y}_{4})\pm 
  <row Body="&lt;p&gt;Here is a possible solution that avoids loops.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; tab1 &amp;lt;- rbind(c(0, 2, 5, 1, 9),&#10;+               c(3, 0, 1, 0, 0),&#10;+               c(0, 0, 0, 3, 4),&#10;+               c(0, 4, 5, 0, 2),&#10;+               c(0, 1, 2, 3, 0))&#10;&amp;gt; tab2 &amp;lt;- matrix(as.numeric(tab1!=0), &#10;+                nrow=nrow(tab1), ncol=ncol(tab1))&#10;&amp;gt; &#10;&amp;gt; tab1&#10;     [,1] [,2] [,3] [,4] [,5]&#10;[1,]    0    2    5    1    9&#10;[2,]    3    0    1    0    0&#10;[3,]    0    0    0    3    4&#10;[4,]    0    4    5    0    2&#10;[5,]    0    1    2    3    0&#10;&amp;gt; tab2&#10;    [,1] [,2] [,3] [,4] [,5]&#10;[1,]    0    1    1    1    1&#10;[2,]    1    0    1    0    0&#10;[3,]    0    0    0    1    1&#10;[4,]    0    1    1    0    1&#10;[5,]    0    1    1    1    0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-10-09T12:27:20.367" Id="16744" LastActivityDate="2011-10-09T12:27:20.367" OwnerUserId="3019" ParentId="16743" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Not quite. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your interpretation for the first model is correct, but your explanation isn't quite right. Note that&#10;$$ \begin{equation*} \beta_1 = \frac{\partial \log y}{\partial x}, \end{equation*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;but that isn't very easy to interpret. So, we recall the calculus result that&#10;$$ \begin{equation*} \frac{\partial \log y}{\partial y} = \frac{1}{y} \end{equation*}$$&#10;or&#10;$$ \begin{equation*} \partial \log y = \frac{\partial y}{y}. \end{equation*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Plugging this into the equation for $\beta_1$, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{equation*} \beta_1 = \frac{\partial y / y}{\partial x}. \end{equation*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we multiply both sides by 100, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{equation*} 100\beta_1 = \frac{100 \times \partial y / y}{\partial x}. \end{equation*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We realize that $100 \times \partial y/y$ is just the percentage change in $y$, giving the interpretation that $100 \beta_1$ is the percentage change in the outcome for a one unit increase in $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The correct interpretation for your second model would be that a 1 unit increase in GDP leads to a 10 &lt;strong&gt;percentage point&lt;/strong&gt; increase in sales. It's easiest to understand this by thinking of your outcome as being measured in percentage points, rather than percent. Then, a 1 unit change in $x$ leads to a $\beta_1$ unit change in $y$, just as we normally get.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is an important distinction. An increase in sales from 1% to 5% is a $5 - 1 = 4$ percentage point increase, but a $(5 - 1)/1 \times 100 =400$ percent change.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-09T15:09:26.293" Id="16748" LastActivityDate="2011-10-09T15:34:35.217" LastEditDate="2011-10-09T15:34:35.217" LastEditorUserId="401" OwnerUserId="401" ParentId="16747" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="16799" AnswerCount="1" Body="&lt;p&gt;I'm reading the example of the book: &quot;S-PLUS (and R) Manual to Accompany Agresti’s Categorical Data Analysis (2002) 2nd edition &quot; (page 55) and when I try to reproduce the example I get this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;snoring&amp;lt;-c(0,2,4,5) &#10;&#10;logit.irls&amp;lt;-glm(cbind(yes=c(24,35,21,30), no=c(1355,603,192,224))~snoring, &#10;family=binomial(link=logit)) &#10;&#10;summary(logit.irls)&#10;&#10;Null deviance: 65.9045  on 3  degrees of freedom&#10;Residual deviance:  2.8089  on 2  degrees of freedom&#10;**AIC: 27.061**&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Ok, I get a Akaike information criterion of 27.061.&lt;/p&gt;&#10;&#10;&lt;p&gt;But when I use the multinorm function from the &quot;nnet&quot; library I get this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;modLogit = multinom(cbind(c(24,35,21,30),c(1355,603,192,224))~snoring)&#10;&#10;summary(modLogit)&#10;&#10;Coefficients:&#10;  (Intercept)    snoring&#10;2    3.866248 -0.3973366&#10;&#10;Std. Errors:&#10;  (Intercept)    snoring&#10;2   0.1662144 0.05001066&#10;&#10;Residual Deviance: 837.7316 &#10;**AIC: 841.7316** &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why the AIC (and the Residual Deviance) are too big?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-10T04:47:19.757" FavoriteCount="1" Id="16770" LastActivityDate="2011-10-10T19:32:50.447" LastEditDate="2011-10-10T19:32:50.447" LastEditorUserId="4376" OwnerUserId="4570" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;aic&gt;" Title="Why is AIC from multinom and corresponding glm different?" ViewCount="657" />
  <row AcceptedAnswerId="16781" AnswerCount="5" Body="&lt;p&gt;I have a sort of philosophical question about when multiple comparison correction is necessary.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am measuring a continuous time varying signal (at discrete time points). Seperate events take place from time to time and I would like to establish if these events have a significant effect on the measured signal.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I can take the mean signal that follows an event, and usually I can see some effect there with a certain peak. If I choose the time of that peak and do say a t-test to determine if it is significant vs when the event doesn't occur do I need to do multiple comparison correction?&lt;/p&gt;&#10;&#10;&lt;p&gt;Although I only ever performed one t-test (calculated 1 value), in my initial visual inspection I selected for the one with the largest potential effect from the (say) 15 different post delay time points I plotted. So do I need to do multiple comparison correction for those 15 tests I never performed?&lt;/p&gt;&#10;&#10;&lt;p&gt;If I didn't use visual inspection, but just did the test at each event lag and choose the highest one, I surely would need to correct. I am just a little confused as to whether I do need to or not if the 'best delay' selection is made by some other criterion than the test itself (eg visual selection, highest mean etc.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-10T08:56:35.410" Id="16779" LastActivityDate="2012-06-28T03:28:38.520" LastEditDate="2011-10-10T10:39:55.110" LastEditorUserId="2116" OwnerUserId="5407" PostTypeId="1" Score="5" Tags="&lt;multiple-comparisons&gt;" Title="When is multiple comparison correction necessary?" ViewCount="3911" />
  
  
  <row Body="&lt;p&gt;One thing you could do is look at correlations among all the questions and see if any are very high (say, over .9 or some such). For those pairs, you could eliminate one question at random. That would shorten things, at least. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could also look at Cronbach's alpha for the items you think are under each factor, and eliminate questions that have very low correlations with the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;Neither of these is going to make the questionnaire much better.... There's always GIGO, and you say the questionnaire is very poor. But they should make it a bit shorter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-10T11:21:06.463" Id="16787" LastActivityDate="2011-10-10T11:21:06.463" OwnerUserId="686" ParentId="16755" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Try doing only&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; tab2 &amp;lt;- (tab1!=0)+0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It will work.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; tab1 &amp;lt;- rbind(c(0, 2, 5, 1, 9),&#10;+               c(3, 0, 1, 0, 0),&#10;+               c(0, 0, 0, 3, 4),&#10;+               c(0, 4, 5, 0, 2),&#10;+               c(0, 1, 2, 3, 0))&#10;&amp;gt; tab2 &amp;lt;- (tab1!=0)+0&#10;&amp;gt; &#10;&amp;gt; tab1&#10;     [,1] [,2] [,3] [,4] [,5]&#10;[1,]    0    2    5    1    9&#10;[2,]    3    0    1    0    0&#10;[3,]    0    0    0    3    4&#10;[4,]    0    4    5    0    2&#10;[5,]    0    1    2    3    0&#10;&amp;gt; tab2&#10;    [,1] [,2] [,3] [,4] [,5]&#10;[1,]    0    1    1    1    1&#10;[2,]    1    0    1    0    0&#10;[3,]    0    0    0    1    1&#10;[4,]    0    1    1    0    1&#10;[5,]    0    1    1    1    0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-10-10T11:49:08.580" Id="16789" LastActivityDate="2011-10-10T11:49:08.580" OwnerUserId="6371" ParentId="16743" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you discretize your field into a grid, count the number of observations in each grid cell (seperately for both spatial point patterns, cows and horses), and then treat each grid cell as a seperate observation you could calculate the correlation between the two spatial point patterns. (Sometimes this may be called &quot;quadrat&quot; counting, and all this is doing is calculating the correlation between the quadrats.)&lt;/p&gt;&#10;&#10;&lt;p&gt;That has some undesirable ad hoc steps though (how small of grid cells?), but is a useful exploratory tool. Likely a next analytical step would be to evaluate Ripley's K statistic between the two spatial point patterns. I asked a similar and relevant question on the GIS forum, &lt;a href=&quot;http://gis.stackexchange.com/q/4484/751&quot;&gt;How to compare two spatial point patterns?&lt;/a&gt;, and that has some references for the Ripley's K statistic (plus software and other references on sampling) from @whuber.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Given the update in the comment, Ripley's K is probably not appropriate (Ripley's K is calculated by examining the pair-wise distances of spatial points, you don't have spatial points though!) Still treating each &quot;element&quot; of your matrix as one observation you can calculate the correlations between the intensities the same as the quadrat counting I suggested above.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, that isn't all that enlightening of whether there is spatial &quot;co-localization&quot; between the two variables. One potential way to identify if the variables have some type of spatial auto-correlation between the two processes is to estimate the bivariate Moran's I (Wikipedia has an intro as to what &lt;a href=&quot;http://en.wikipedia.org/wiki/Moran%27s_I&quot; rel=&quot;nofollow&quot;&gt;univariate Moran's I&lt;/a&gt; is and its calculation, and the &lt;a href=&quot;http://geodacenter.asu.edu/system/files/geodaworkbook.pdf&quot; rel=&quot;nofollow&quot;&gt;Geoda workbook&lt;/a&gt; goes into more detail about bivariate Moran's I as well as more general spatial regression modeling). Another potential methodology you could use is to calculate the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variogram&quot; rel=&quot;nofollow&quot;&gt;empirical variogram&lt;/a&gt; between the two processes.&lt;/p&gt;&#10;&#10;&lt;p&gt;The more specific details you give about the the data and how the two processes are related will likely allow for more focused feedback (but I suspect this would be a good start).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-10T12:53:58.027" Id="16791" LastActivityDate="2011-10-10T16:57:26.083" LastEditDate="2011-10-10T16:57:26.083" LastEditorUserId="1036" OwnerUserId="1036" ParentId="16778" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;Partly as a learning exercise, I’m attempting to reproduce a published SAS PROC MIXED analysis using R’s lme4 package.  A full description of both the application and the statistical method is available in &lt;a href=&quot;http://www.biomedcentral.com/1472-6750/11/15&quot; rel=&quot;nofollow&quot;&gt;http://www.biomedcentral.com/1472-6750/11/15&lt;/a&gt; .  Briefly, the broad purpose is to assess the food safety of a specific genetically-modified crop (GMO).  In the experiment, the GMO crop is grown along with a genetically similar but unmodified comparator and with a diverse set of commercially-available reference varieties.  The reference varieties are presumed safe and thus provide estimates of the range of acceptable nutrient levels.  The crop is harvested and subjected to chemical analyses to measure various constituents. Nutrient levels are then compared between the GMO and the comparator and between the GMO and the set of reference varieties.&lt;/p&gt;&#10;&#10;&lt;p&gt;The published method defines a variety group variable with 3 levels: GMO, comparator (comp), and reference (ref).  An integer index is also defined that singles out the reference varieties.  To clarify, assume that we have 4 reference varieties.  The variety-related variables are then:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;variety vgroup  isref&#10;comp    comp    0&#10;GMO     GMO     0&#10;refA    ref     1&#10;refB    ref     1&#10;refC    ref     1&#10;refD    ref     1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Focusing only on this part of the statistical model and ignoring the other (important but straightforward) model factors (i.e. locations &amp;amp; blocks), the published SAS PROC MIXED formulation is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    CLASS variety vgroup …;&#10;    MODEL y=vgroup …;&#10;    RANDOM isref*variety …;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Modeling vgroup as a fixed factor enables formal tests of (GMO – comp) and (GMO – ref).  Note that isref is not designated as a CLASS variable and thus remains a numeric quantity.&#10;The SAS model runs successfully.  A single variance is estimated for the isref*variety term, indicating that this quantity is a single factor rather than an interaction.   The random effect predictions for the isref*variety factor are identically zero for the comp and GMO varieties and are non-zero for the ref varieties.&#10;I have yet to replicate these results in R lmer.  The following model statement &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    y ~ vgroup + (0 + factor(isref)|variety) + …&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is close, but doesn’t annihilate the isref == 0 variance estimate and the associated random effect predictions.  Perhaps as a direct consequence, the vgroup standard errors for the comp and GMO levels are considerably larger than the corresponding SAS results.&lt;/p&gt;&#10;&#10;&lt;p&gt;So finally, is there a way to specify this exact model in R lmer?  If so, how?  If not, why? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-10T17:40:37.123" Id="16803" LastActivityDate="2011-10-10T17:40:37.123" OwnerUserId="6684" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;" Title="Specifying selectively-nested random factors in R lmer" ViewCount="263" />
  
  
&#10;\end{align*}
  
  <row Body="&lt;p&gt;I would go for &lt;strong&gt;Item Response Theory&lt;/strong&gt; (IRT). IRT not only lets you check for unidimensionality but also things like Item Information Curves (or Item Response Functions), which might come in handy if you want to reduce the number of items but still want to precisely measure participants broadly on the whole latent scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a lot of packages in R that fit these kind of models. The ones I have most often used are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;eRm&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Mokken&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would suggest that you have a look at the Mokken package first, because it includes an &lt;strong&gt;automated item selection procedure&lt;/strong&gt; (&lt;em&gt;aisp&lt;/em&gt; function), which automatically builds good scales in an exploratory way (note that &quot;good&quot; is defined here in terms of the so called scalability coefficient H). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-11T07:57:12.633" Id="16828" LastActivityDate="2011-10-11T07:57:12.633" OwnerUserId="5544" ParentId="16755" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Since the plot of &lt;code&gt;confint(lmList)&lt;/code&gt; is a trellis object, additions to the plot can be accomplished via the &lt;code&gt;latticeExtra&lt;/code&gt; package, but you have to assign the plot to an object first. The vertical line is then added to the plot object with &lt;code&gt;+ layer(panel.refline...)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the &lt;code&gt;lme4&lt;/code&gt; example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;library(lattice)&#10;library(latticeExtra)&#10;&#10;Rxplot &amp;lt;- plot(confint(lmList(Reaction ~ Days | Subject, sleepstudy), pooled = FALSE))&#10;Rxplot + layer(panel.refline(v = v, col.line = &quot;grey&quot;), data = list(v = 0))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-10-11T08:37:53.567" Id="16829" LastActivityDate="2011-10-11T08:37:53.567" OwnerUserId="3774" ParentId="16814" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="16848" AnswerCount="5" Body="&lt;p&gt;When reading passages like the following: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;blockquote&gt;&#10;    &lt;p&gt;Based on a representative sample of 88 recent raids, we show that the Turkana sustain costly cooperation in combat at a remarkably large scale, at least in part, through punishment of free-riders. &lt;/p&gt;&#10;  &lt;/blockquote&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I wonder what 'representative sample' might refer to. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it related to power calculations (for example) in statistical inference or is there some way to assess the number of samples required from the total population for it to be considered representative?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-11T16:42:25.680" FavoriteCount="1" Id="16847" LastActivityDate="2012-11-07T14:06:48.673" OwnerUserId="5837" PostTypeId="1" Score="6" Tags="&lt;sampling&gt;&lt;theory&gt;" Title="What exactly does 'representative sample' refer to?" ViewCount="4935" />
  <row Body="&lt;p&gt;Basic idea behind this outlier score is how far an object is from a centroid of its predicted class, thus it makes little sense for regression.&lt;br&gt;&#10;Yet you can still produce proximity from regression RF by putting &lt;code&gt;proximity=TRUE&lt;/code&gt; argument to the call to &lt;code&gt;randomForest&lt;/code&gt; and then force outlier detection on it by calling &lt;code&gt;outlier(&amp;lt;rf object&amp;gt;$proximity)&lt;/code&gt;, but this will only make assumption that all objects are from one class. &lt;/p&gt;&#10;&#10;&lt;p&gt;A bit less hopeless idea is to just convert regression problem to classification by splitting continuous decision into several intervals with &lt;code&gt;cut&lt;/code&gt; or &lt;code&gt;Hmisc::cut2&lt;/code&gt; and doing outlier detection on this -- but I have never tried it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-11T16:52:34.793" Id="16850" LastActivityDate="2011-10-12T20:16:36.503" LastEditDate="2011-10-12T20:16:36.503" LastEditorUserId="930" OwnerUserId="88" ParentId="16837" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a panel cost data with some zero values (not missing values but rounded to 0). How should I handle zeros when I use SAS Proc Glimmix with Gamma distribution? Maybe change zero to very small values? Or is there an option for it (which I couldn't find) in PROC GLIMMIX?&lt;/p&gt;&#10;&#10;&lt;p&gt;Janne&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-11T18:05:01.907" FavoriteCount="2" Id="16857" LastActivityDate="2011-10-12T13:43:14.100" OwnerUserId="6670" PostTypeId="1" Score="3" Tags="&lt;mixed-model&gt;&lt;gamma-distribution&gt;" Title="A mixed model with gamma distribution: handling zeros" ViewCount="994" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have found no tool to estimate an &quot;oprobit&quot;-like ordinal model using the local regression (local likelihood) framework. It's not built into R's locfit. What is a good way to proceed?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&#10;I have a predictor $x\in [-1,1]$, and outcome $y\in\{1,2,3,4,5\}$. I would like to non parametrically predict (trace out, smooth) $Pr(y=n \mid x)$ for all $n \in \{1,2,3,4,5\}$ and x from $(x,y)$ observations. A latent index (with latent cutoffs) could drive the result, as there is a monotonic ranking in levels of y in $1&amp;lt;2&amp;lt;3&amp;lt;4&amp;lt;5$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-11T20:34:50.823" Id="16874" LastActivityDate="2011-10-11T20:34:50.823" OwnerUserId="6534" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;nonparametric&gt;&lt;multinomial&gt;&lt;smoothing&gt;&lt;ordered-variables&gt;" Title="local ordinal regressions" ViewCount="72" />
  <row Body="&lt;p&gt;Using &lt;a href=&quot;http://www.jstatsoft.org/v09/i02&quot; rel=&quot;nofollow&quot; title=&quot;Marsaglia &amp;amp; Marsaglia's code&quot;&gt;Marsaglia &amp;amp; Marsaglia's code&lt;/a&gt;, and a bisection search, one can find that the 0.001 critical value is around 5.9694. This would be for 'Case 1' in the wikipedia article quoted. I am not sure how to convert to 'Case 4'.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-10-11T21:15:54.500" Id="16875" LastActivityDate="2011-10-11T21:15:54.500" OwnerUserId="795" ParentId="11310" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I have done many 1-sample T-tests before, but I can't figure out if I am able to use one in this situation. In our experiment, we took 12 individual insects and placed them in a chamber where they could choose to be on whatever side they pleased (One side had sugar in it and the other did not). We recorded the number of insects on each side &lt;em&gt;at 30 second intervals&lt;/em&gt; for 20 minutes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to use a 1-sample T test here? It seems the data wouldn't be random, violating one of the fundamental assumptions for the test. The number of insects on one side at any time strongly influences the number that will be there at the time of the next reading.&lt;/p&gt;&#10;&#10;&lt;p&gt;What exactly is data like this called? How can I analyze it and potentially reject the null hypothesis that the insects have no preference to the side with sugar vs. the side without?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-10-11T23:27:15.033" Id="16882" LastActivityDate="2011-10-12T19:30:08.250" LastEditDate="2011-10-12T16:07:15.757" LastEditorUserId="919" OwnerUserId="1895" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;hypothesis-testing&gt;&lt;t-test&gt;&lt;markov-process&gt;" Title="What methods of statistical analysis can be used for time series data?" ViewCount="392" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm learning through doing here guys, so I hope this question is considered OK (I'll edit the question down as I go - I'll remove the intro etc).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to plot the empirical cumulative distribution Frequency of a data-set with 781 observations.  The data-set looks like this: &lt;/p&gt;&#10;&#10;&lt;p&gt;(1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;3&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;4&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;5&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;6&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;7&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;8&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;9&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;10&#10;11&#10;11&#10;11&#10;11&#10;11&#10;12&#10;12&#10;12&#10;12&#10;12&#10;12&#10;12&#10;12&#10;12&#10;12&#10;13&#10;13&#10;13&#10;13&#10;13&#10;13&#10;13&#10;14&#10;14&#10;14&#10;14&#10;14&#10;15&#10;15&#10;15&#10;15&#10;16&#10;16&#10;17&#10;17&#10;17&#10;19&#10;19&#10;20&#10;21&#10;21&#10;21&#10;21&#10;22&#10;22&#10;23&#10;23&#10;).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I use the following function in R (which I pulled from &lt;a href=&quot;http://www.r-bloggers.com/example-7-8-plot-two-empirical-cumulative-density-functions-using-available-tools/&quot; rel=&quot;nofollow&quot;&gt;r-bloggers&lt;/a&gt;): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(ecdf(V1), verticals=TRUE, pch=46)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which produces the following graph:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dy4Gp.png&quot; alt=&quot;ECDF plot&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT ===== &lt;/p&gt;&#10;&#10;&lt;p&gt;The graph plots the the actual observation on the x-axis and the percentage of observations on the y-axis.      &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your help,&lt;/p&gt;&#10;&#10;&lt;p&gt;slotishtype&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-12T09:08:25.493" Id="16894" LastActivityDate="2011-10-12T10:23:27.430" LastEditDate="2011-10-12T10:23:27.430" LastEditorUserId="2405" OwnerUserId="2405" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;data-visualization&gt;" Title="Plotting ECDF (empirical cumulative distribution frequencies) with R" ViewCount="5584" />
  <row Body="&lt;p&gt;These are called Electronic Lab Notebooks (ELN).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some of the open source options I've looked at:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://nb.sagemath.org/&quot; rel=&quot;nofollow&quot;&gt;The Sage Notebook&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The new &lt;a href=&quot;http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html&quot; rel=&quot;nofollow&quot;&gt;IPython Notebook&lt;/a&gt;, which can now be run as a webapp on EC2 and Azure.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Leo, which can be used with IPython and in many other ways.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Various wiki, blogging, and CMS solutions.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-10-12T11:05:53.760" CreationDate="2011-10-12T11:05:53.760" Id="16898" LastActivityDate="2012-07-05T19:33:01.270" LastEditDate="2012-07-05T19:33:01.270" LastEditorUserId="12429" OwnerUserId="6773" ParentId="16889" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;You might want to check out the latest &lt;a href=&quot;http://www.zotero.org/blog/announcing-zotero-3-0-beta-release/&quot; rel=&quot;nofollow&quot;&gt;Zotero beta&lt;/a&gt;, which is now standalone and doesn't require Firefox.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2011-10-12T12:52:12.120" CreationDate="2011-10-12T12:52:12.120" Id="16900" LastActivityDate="2011-10-12T12:52:12.120" OwnerUserId="1764" ParentId="16889" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Consider these two simple situations:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The joint distribution of $(X,Y)$ is uniform on $H^2 = [0,1] \times [0,1]$; $Z=X-Y$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The joint distribution of $(X, \eta)$ is uniform on $H^2$.  When $X \le \eta$, $Y=\eta$; otherwise, $Y=1$.  Again, $Z=X-Y$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What is common to both situations are (a) the distribution of $X$ (which is uniform) and (b) the distribution of $Z$ (given that $X \le Y\ $).  Yet--obviously--the distributions of $Y$ differ radically: in the first case $Y$ is uniform, implying $\Pr[Y=1]=0$, whereas in the second case $\Pr[Y=1]=1/2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The point is that we are free to modify $Y$ on the set $Y\gt X$ without changing any of the information in the problem.  The information places a (crude) lower bound on the marginal CDF of $Y$, but that's all.&lt;/p&gt;&#10;&#10;&lt;p&gt;That answers all but the first question.  But given that the solution is so indeterminate, I doubt that problems like this (with such limited information) have been investigated, which addresses the first question.  To make progress, you have to make (parametric) assumptions about the distribution of $Y$ or the joint distribution of $(X,Y)$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-10-12T14:02:41.800" Id="16904" LastActivityDate="2011-10-12T14:02:41.800" OwnerUserId="919" ParentId="16901" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The model needs a constant.  In its current form,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y =\beta x +\varepsilon,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $x$ constrained to be $0$ or $1$, there are two possibilities.  First, let group &quot;A&quot; be coded as $x=0$ (the &quot;reference&quot; group) and group &quot;B&quot; be coded as $x=1$.  For group A the model is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = \beta 0 + \varepsilon = \varepsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;while for group B it is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = \beta + \varepsilon$$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, &lt;em&gt;it assumes the mean value of $y$ in group A is zero,&lt;/em&gt; but it estimates the mean of group B (as $\beta$).  Second, let group A be coded as $x=1$ and group be be coded as $x=0$.  Now the model assumes the mean value of $y$ in group B is zero but estimates the mean of group A.&lt;/p&gt;&#10;&#10;&lt;p&gt;Neither situation is satisfactory unless one of the group means happens to be close to $0$ (relative to the standard error of the residuals): it has no ability to estimate the mean of the reference group.  The F-tests will differ; whichever reference group has a mean closer to zero ought to yield the smallest p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution is to fit&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = \alpha + \beta x + \varepsilon;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;that is, to include a constant in the model.  Now, in group A (with the original coding of $x=0$) the model is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = \alpha + \beta 0 + \varepsilon = \alpha + \varepsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and in group B it is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = \alpha + \beta 1 + \varepsilon = \alpha + \beta.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Whence $\alpha$ estimates the mean of A and $\beta$ estimates the difference in group means.&lt;/p&gt;&#10;&#10;&lt;p&gt;A pooled t-test will be equivalent to the F-test here: it should yield the same p-value.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-10-12T21:00:37.243" Id="16925" LastActivityDate="2011-10-12T21:00:37.243" OwnerUserId="919" ParentId="16916" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Link functions convert the expected value of Y (given X) to something that is unbounded.  While in logistic regression, Y takes values 0 or 1, the logit isn't applied to Y but to Pr(Y=1|X).  (The logit of 0 and 1 are each undefined.)  So it's perfectly reasonable to use the logit or the probit in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other thing to think about is the residual variance: is there a particular transformation that would best stabilize the variance for your case?  For proportions, the arcsine square-root transformation is often used, as it is variance-stabilizing for binomial proportions.  Consider the discussion &lt;a href=&quot;http://rfd.uoregon.edu/files/rfd/StatisticalResources/arcsin.txt&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-13T00:10:34.687" Id="16934" LastActivityDate="2011-10-13T00:10:34.687" OwnerUserId="5862" ParentId="16928" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am at a loss about what the BUGS/JAGS specification of the following should look like.  The background is that one person has four measurements taken by four different instruments.  Each instrument is expected to have a different associated error.  The R code for generating simulated data (not very elegant) is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 63                    # Number of patients&#10;&#10;## (Hyper)parameters&#10;var_a &amp;lt;- 2                  # Variance around population mean&#10;mu_a &amp;lt;- 80                  # population mean&#10;mu_m &amp;lt;- c(0, -5, 1, -6)     # Mean diff for each measurement instrument from the population mean&#10;var_m &amp;lt;- c(1, 1.5, 2, 2.5)  # Variance of each measurement instrument&#10;m_i &amp;lt;- 0                    # Individual measurement error (mean)&#10;m_e &amp;lt;- 1                    # Individual measurement error (variance)&#10;&#10;# Counter to keep track of total number of measurements taken&#10;measures.taken &amp;lt;- 0&#10;&#10;&#10;## Generate the data&#10;for (i in 1:n){                                                       # loop around the number of patients&#10;  a &amp;lt;- mu_a + rnorm(1, 0, sqrt(var_a))                                # Random intercept&#10;  for (j in 1:length(mu_m)){                                          # Loop to generate 4 measures for each patient&#10;    measures.taken &amp;lt;- measures.taken + 1                              # Just keeping track of the number of measurements in total&#10;    individual.error &amp;lt;- rnorm(1, m_i, sqrt(m_e))   &#10;    bp &amp;lt;- rnorm(1, mu_m[j], sqrt(var_m[j])) + a + individual.error   # This sets up the random variability for the measures&#10;    print(c(bp, i, j))                                               # Print data as generated to the standard output&#10;  } &#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any insights gratefully accepted.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-13T02:01:13.733" FavoriteCount="1" Id="16938" LastActivityDate="2011-10-14T13:56:02.467" LastEditDate="2011-10-13T03:48:19.663" LastEditorUserId="1381" OwnerUserId="6808" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;mixed-model&gt;&lt;bugs&gt;&lt;jags&gt;" Title="How do I transform a data generation specification in R into a BUGS/JAGS specification" ViewCount="126" />
  
  <row Body="&lt;p&gt;The general linear model lets us write an ANOVA model as a regression model. Let's assume we have two groups with two observations each, i.e., four observations in a vector $y$. Then the original, overparametrized model is $E(y) = X^{\star} \beta^{\star}$, where $X^{\star}$ is the matrix of predictors, i.e., dummy-coded indicator variables:&#10;$$&#10;\left(\begin{array}{c}\mu_{1} \\ \mu_{1} \\ \mu_{2} \\ \mu_{2}\end{array}\right) = \left(\begin{array}{ccc}1 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 1 \\ 1 &amp;amp; 0 &amp;amp; 1\end{array}\right) \left(\begin{array}{c}\beta_{0}^{\star} \\ \beta_{1}^{\star} \\ \beta_{2}^{\star}\end{array}\right)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The parameters are not identifiable as $((X^{\star})' X^{\star})^{-1} (X^{\star})' E(y)$ because $X^{\star}$ has rank 2 ($(X^{\star})'X^{\star}$ is not invertible). To change that, we introduce the constraint $\beta_{1}^{\star} = 0$ (treatment contrasts), which gives us the new model $E(y) = X \beta$:&#10;$$&#10;\left(\begin{array}{c}\mu_{1} \\ \mu_{1} \\ \mu_{2} \\ \mu_{2}\end{array}\right) = \left(\begin{array}{cc}1 &amp;amp; 0 \\ 1 &amp;amp; 0 \\ 1 &amp;amp; 1 \\ 1 &amp;amp; 1\end{array}\right) \left(\begin{array}{c}\beta_{0} \\ \beta_{2}\end{array}\right)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So $\mu_{1} = \beta_{0}$, i.e., $\beta_{0}$ takes on the meaning of the expected value from our reference category (group 1). $\mu_{2} = \beta_{0} + \beta_{2}$, i.e., $\beta_{2}$ takes on the meaning of the difference $\mu_{2} - \mu_{1}$ to the reference category. Since with two groups, there is just one parameter associated with the group effect, the ANOVA null hypothesis (all group effect parameters are 0) is the same as the regression weight null hypothesis (the slope parameter is 0).&lt;/p&gt;&#10;&#10;&lt;p&gt;A $t$-test in the general linear model tests a linear combination $\psi = \sum c_{j} \beta_{j}$ of the parameters against a hypothesized value $\psi_{0}$ under the null hypothesis. Choosing $c = (0, 1)'$, we can thus test the hypothesis that $\beta_{2} = 0$ (the usual test for the slope parameter), i.e. here, $\mu_{2} - \mu_{1} = 0$. The estimator is $\hat{\psi} = \sum c_{j} \hat{\beta}_{j}$, where $\hat{\beta} = (X'X)^{-1} X' y$ are the OLS estimates for the parameters. The general test statistic for such $\psi$ is:&#10;$$&#10;t = \frac{\hat{\psi} - \psi_{0}}{\hat{\sigma} \sqrt{c' (X'X)^{-1} c}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\sigma}^{2} = \|e\|^{2} / (n-\mathrm{Rank}(X))$ is an unbiased estimator for the error variance, where $\|e\|^{2}$ is the sum of the squared residuals. In the case of two groups $\mathrm{Rank}(X) = 2$, $(X'X)^{-1} X' = \left(\begin{smallmatrix}.5 &amp;amp; .5 &amp;amp; 0 &amp;amp; 0 \\-.5 &amp;amp; -.5 &amp;amp; .5 &amp;amp; .5\end{smallmatrix}\right)$, and the estimators thus are $\hat{\beta}_{0} = 0.5 y_{1} + 0.5 y_{2} = M_{1}$ and $\hat{\beta}_{2} = -0.5 y_{1} - 0.5 y_{2} + 0.5 y_{3} + 0.5 y_{4} = M_{2} - M_{1}$. With $c' (X'X)^{-1} c$ being 1 in our case, the test statistic becomes:&#10;$$&#10;t = \frac{M_{2} - M_{1} - 0}{\hat{\sigma}} = \frac{M_{2} - M_{1}}{\sqrt{\|e\|^{2} / (n-2)}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$t$ is $t$-distributed with $n - \mathrm{Rank}(X)$ df (here $n-2$). When you square $t$, you get $\frac{(M_{2} - M_{1})^{2} / 1}{\|e\|^{2} / (n-2)} = \frac{SS_{b} / df_{b}}{SS_{w} / df_{w}} = F$, the test statistic from the ANOVA $F$-test for two groups ($b$ for between, $w$ for within groups) which follows an $F$-distribution with 1 and $n - \mathrm{Rank}(X)$ df.&lt;/p&gt;&#10;&#10;&lt;p&gt;With more than two groups, the ANOVA hypothesis (all $\beta_{j}$ are simultaneously 0, with $1 \leq j$) refers to more than one parameter and cannot be expressed as a linear combination $\psi$, so then the tests are not equivalent.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-13T13:17:48.403" Id="16956" LastActivityDate="2013-05-29T03:15:40.867" LastEditDate="2013-05-29T03:15:40.867" LastEditorUserId="5176" OwnerUserId="1909" ParentId="16947" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;I can't see any reason to log transform AFTER making z-scores; it COULD be right to log transform and then take z-scores of the logged data. I'm not as sure on the other questions, but my intuition would be to log transform everything, then take z-scores, then do the analysis. That way, a change of 1 in a variable is the same amount in every group.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-13T17:13:09.463" Id="16964" LastActivityDate="2011-10-13T17:13:09.463" OwnerUserId="686" ParentId="16958" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you want a bell curve, you need to use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Gaussian_function&quot; rel=&quot;nofollow&quot;&gt;Gaussian&lt;/a&gt;. The equation for Gaussian is: $f(x) = ae^{\frac{-(x-b)^2}{2c^2}) }$&lt;/p&gt;&#10;&#10;&lt;p&gt;a is the maximum height of the curve. If you want 100% correct to be the maximum value, set a=100.&lt;/p&gt;&#10;&#10;&lt;p&gt;b is the middle of the curve. If you want the maximum value (100%) to be at 50, set b to 50. &lt;/p&gt;&#10;&#10;&lt;p&gt;c relates to the width of the curve. Higher values of c correspond to wider, more gradually sloping curves. Lower values correspond to sharper, steeply sloping curves. This is a value you'd have to pick depending on how much error you're willing to tolerate. The Wikipedia page has some good example graphs. &lt;/p&gt;&#10;&#10;&lt;p&gt;If x is the given number, and b is the actual number, this function returns a when x=b. As we move away from b, we return some fraction of 100. If we assume any value less than 0.5% correct is just &quot;wrong&quot;, and we want a value  which is off by more than $3b$ to be considered &quot;wrong&quot;, then we can set a value of c so that $f(4b)=f(-2b) = 0.05$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the values presented above, &lt;a href=&quot;http://www.wolframalpha.com/input/?i=100%2ae%5E%7B%28-%20%28x-50%29%5E2%29/%282%2846.07945%5E2%29%29%7D&quot; rel=&quot;nofollow&quot;&gt;here's&lt;/a&gt; a function with these properties.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-10-13T20:51:02.453" Id="16970" LastActivityDate="2011-10-14T20:13:59.900" LastEditDate="2011-10-14T20:13:59.900" LastEditorUserId="6446" OwnerUserId="6446" ParentId="16968" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="16976" AnswerCount="1" Body="&lt;p&gt;Can I suppress the intercept if I know the treatment will be zero if the independent variables are zero.  Also, can I suppress the intercept if I know the right hand side of the primary regression equation CANNOT equal zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;treatreg y1 x1 x2 x3, treat(y*=x4 x5, noconstant) noconstant&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can I do this if I know for sure &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt; and &lt;code&gt;x3&lt;/code&gt; cannot combine to equal zero?  Also if &lt;code&gt;x4&lt;/code&gt; and &lt;code&gt;x5&lt;/code&gt; combine to zero then &lt;code&gt;y*&lt;/code&gt; MUST be zero.  Is this an appropriate use of noconstant?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-13T23:03:18.183" Id="16974" LastActivityDate="2011-10-14T11:21:21.357" LastEditDate="2011-10-14T11:21:21.357" LastEditorUserId="2116" OwnerUserId="6834" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;stata&gt;&lt;basic-concepts&gt;&lt;treatment-effect&gt;" Title="When can I suppress the intercept using treatreg?" ViewCount="1057" />
  <row Body="&lt;p&gt;The sample variance, $s^2$, is the variance of the sample, an estimate of the variance of the population from which the sample was drawn.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Sampling variance&quot; I would interpret as &quot;the variance that is due to sampling&quot;, for example of an estimator (like the mean).  And so I would consider these two terms to be quite different.&lt;/p&gt;&#10;&#10;&lt;p&gt;But &quot;sampling variance&quot; is a bit vague, and I would need to see some context to be sure.  And I'd prefer to say &quot;sampling variation&quot; for the general idea.  &lt;/p&gt;&#10;&#10;&lt;p&gt;[Many people (particularly in quantitative genetics) use the term &quot;variance&quot; in place of &quot;variation&quot;, whereas I would reserve &quot;variance&quot; solely for the particular measure of variation.]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-14T03:45:57.610" Id="16984" LastActivityDate="2011-10-14T03:45:57.610" OwnerUserId="5862" ParentId="16982" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The seasonality is set in your data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- ts(rnorm(100),frequency=7)&#10;ets(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This way you tell that your data is seasonal, and function implementing particular method either uses this information or ignores it. That is how &lt;code&gt;auto.arima&lt;/code&gt; works, so it should be the same for &lt;code&gt;ets&lt;/code&gt;, since they both are from the same package &lt;strong&gt;forecast&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-14T11:13:29.687" Id="16992" LastActivityDate="2011-10-14T11:13:29.687" OwnerUserId="2116" ParentId="16990" PostTypeId="2" Score="2" />
  
  
  
  
  <row AcceptedAnswerId="17031" AnswerCount="2" Body="&lt;p&gt;I am trying to understand how, using only the standard deviation and mean, you are able to determine the first and third quartiles on a normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I get that the area under the curve equals one; I understand that Q1 accounts for 25% of the area, and Q3 75%; but I do not understand how to leverage this information to calculate (with my TI-89) where Q1 and Q3 are...&lt;/p&gt;&#10;&#10;&lt;p&gt;I was able to reasonably estimate these values by looking at a z-score table for areas representing 25% and 75%, and I got z-scores of ±0.68. However, I need a greater degree of accuracy than manually estimating from a table, but I do not know how to go about this formulaically or through my TI-89.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-15T03:36:34.207" Id="17028" LastActivityDate="2015-02-26T06:21:52.957" LastEditDate="2011-10-16T02:08:30.830" LastEditorUserId="183" OwnerUserId="6858" PostTypeId="1" Score="2" Tags="&lt;software&gt;&lt;quantiles&gt;" Title="How to calculate quartiles with only standard deviation and mean assuming normal distribution?" ViewCount="7069" />
  
  <row Body="&lt;p&gt;I work as a statistician for a business intelligence team at an online retailer.  At my job, I'm frequently building models to predict various things (like response rates to catalogs, open rates of emails, etc.).  I also help marketing set up A/B tests (like, does email A do better than email B)?  Sounds like a simple problem, but this can be pretty complex when you start to do some power calculations for sample sizes.  Additionally, decision-makers in businesses always want to know if profile A differs from profile B.  For example, did we sell more black shoes than brown shoes this year?  Did we convert more lapsed customers with this catalog?  Did the marketing channels people converted on differ this year from the ones used last year?  These questions are (probably) simple to answer on the surface, but it requires statistics to understand if these answers are significant.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-10-15T12:53:56.063" CreationDate="2011-10-15T12:53:56.063" Id="17036" LastActivityDate="2011-10-15T12:53:56.063" OwnerUserId="6861" ParentId="5457" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;I'm not sure there &lt;em&gt;are&lt;/em&gt; standard practices for generating synthetic data - it's used so heavily in so many different aspects of research that purpose-built data seems to be a more common and arguably more reasonable approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;For me, my best standard practice is &lt;em&gt;not&lt;/em&gt; to make the data set so it will work well with the model. That's part of the research stage, not part of the data generation stage. Instead, the data should be designed to reflect the &lt;em&gt;data generation process&lt;/em&gt;. For example, for simulation studies in Epidemiology, I always start from a large hypothetical population with a known distribution, and then simulate study sampling from that population, rather than generating &quot;the study population&quot; directly.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, based on our discussion below, two examples of simulated data I've made:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Somewhat similar to your SIR-model example below, I once used a mathematical model of the spread of disease over a network to show myself via simulation that a particular constant parameter didn't necessarily imply a constant hazard if you treated the results as the outcome of a cohort study. It was a useful proof of concept while I went digging for an analytical solution.&lt;/li&gt;&#10;&lt;li&gt;I wanted to explore the impact of a certain sampling scheme for a case-control study. Rather than trying to generate the study outright, I walked through each step of the process. A population of 1,000,000 people, with a given known prevalence of disease and a known covariate pattern. Then from that simulating the sampling process - in this case, how cases and controls were drawn from the population. Only then did I throw an actual statistical model at the collected &quot;simulated studies&quot;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Simulations like the latter are very common when examining the impact of study recruitment methods, statistical approaches to controlling for covariates, etc.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-10-16T00:37:20.747" Id="17059" LastActivityDate="2011-10-16T01:53:27.263" LastEditDate="2011-10-16T01:53:27.263" LastEditorUserId="5836" OwnerUserId="5836" ParentId="17039" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The key idea to detect OOS is that you need the most &lt;strong&gt;accurate demand forecast&lt;/strong&gt;, typically at the SKU level &lt;em&gt;per store&lt;/em&gt; and &lt;em&gt;per day&lt;/em&gt;. Indeed, the distribution of the &lt;strong&gt;demand is not stable over time&lt;/strong&gt;: you have to take into account many patterns such as &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;day of the week,&lt;/li&gt;&#10;&lt;li&gt;yearly seasonality,&lt;/li&gt;&#10;&lt;li&gt;promotions, &lt;/li&gt;&#10;&lt;li&gt;...&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Then, once you have your high-quality forecasting model, you will typically focus on the &lt;strong&gt;distribution of error&lt;/strong&gt;. It's through the analyse &lt;em&gt;this&lt;/em&gt; distribution that you will be able to figure the thresholds for your alerts.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Shameless plug:&lt;/em&gt; our product &lt;a href=&quot;http://www.lokad.com/shelfcheck-on-shelf-availability-optimization.ashx&quot; rel=&quot;nofollow&quot;&gt;Shelfcheck&lt;/a&gt; is precisely designed to detect out-of-shelf in Retail.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-16T11:19:48.770" Id="17067" LastActivityDate="2011-10-16T11:19:48.770" OwnerUserId="6872" ParentId="5372" PostTypeId="2" Score="0" />
  <row AnswerCount="3" Body="&lt;p&gt;Imagine that you are planning a study about risk behaviours among HIV positive injecting drug users.&lt;/p&gt;&#10;&#10;&lt;p&gt;All the individuals included in the sample are injecting drugs and all are HIV positive. The main “exposure” in the study is the person’s awareness about their HIV serostatus (some of the drug users do know that they are HIV positive and some don’t know). The outcome is having had unprotected intercourse during the past 4 weeks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your imaginary study has not been performed yet and you are planning for data collection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which variables (age, place of living, relationship status, education etc, you can choose whatever variables you think are relevant) would you like to add in the data in order to assess confounding, mediation and effect modification of the association of interest in the study? &lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-10-16T11:37:24.853" Id="17068" LastActivityDate="2011-11-20T20:23:27.090" LastEditDate="2011-10-19T02:47:13.510" LastEditorUserId="183" OwnerUserId="6673" PostTypeId="1" Score="1" Tags="&lt;experiment-design&gt;&lt;epidemiology&gt;&lt;confounding&gt;&lt;feature-selection&gt;" Title="How to identify suitable variables to assess confounding, mediation and effect modification?" ViewCount="754" />
  <row Body="&lt;p&gt;Check the samples in RapidMiner software first.&lt;/p&gt;&#10;&#10;&lt;p&gt;And the &lt;a href=&quot;http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-loading.html&quot; rel=&quot;nofollow&quot;&gt;videos&lt;/a&gt; - Text Analytics with RapidMiner by Neil McGuigan is quite useful for text mining.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-16T17:33:17.400" Id="17078" LastActivityDate="2011-10-16T17:33:17.400" OwnerUserId="6169" ParentId="17077" PostTypeId="2" Score="4" />
&#10;    \theta_0 $$ $$ l(\theta,1)&amp;lt;l(\theta,0) \text{ for } \theta &amp;gt;
&#10;\begin{array}{r|r|r}
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This table does explain the communalities values ($(0.902)^2 + (0.328)^2 + (-0.085)^2 = 0.929$).&lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding was that the unrotated component matrix when doing FA was the same as the matrix of eigenvectors obtained for the PCA.&#10;From these examples, this doesn't seem to be the case.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should the &quot;unrotated component matrix&quot; be obtained and how does it differ from the PCA eigenvectors?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-17T02:07:32.667" FavoriteCount="2" Id="17090" LastActivityDate="2015-03-05T11:15:14.290" LastEditDate="2011-10-17T06:25:21.860" LastEditorUserId="4607" OwnerUserId="4607" PostTypeId="1" Score="2" Tags="&lt;pca&gt;&lt;factor-analysis&gt;" Title="PCA and FA example - calculation of communalities" ViewCount="3534" />
  <row AnswerCount="1" Body="&lt;p&gt;How to create three dimensional matrix? For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;A = matrix 4 * 5 (4 rows, 5 columns)&lt;/p&gt;&#10;&#10;&lt;p&gt;B = matrix 4 * 10 (4 rows, 10 columns)&lt;/p&gt;&#10;&#10;&lt;p&gt;How to turn these two matrices a three-dimensional matrix?&lt;/p&gt;&#10;" ClosedDate="2013-08-03T19:17:05.850" CommentCount="1" CreationDate="2011-10-17T02:33:27.690" Id="17091" LastActivityDate="2013-08-03T09:06:32.883" LastEditDate="2013-08-03T09:06:32.883" LastEditorUserId="6029" OwnerUserId="6879" PostTypeId="1" Score="-1" Tags="&lt;matlab&gt;&lt;matrix&gt;" Title="How to create a three dimensional matrix in matlab?" ViewCount="4281" />
  
  
  
&#10;  4 &amp;amp; 6 &amp;amp; 66 \\
  <row AcceptedAnswerId="17122" AnswerCount="1" Body="&lt;p&gt;I am running an OLS model with a continuous asset index variable as the DV.  My data is aggregated from three similar communities in close geographic proximity to one another.  Despite this, I thought it important to use community as a controlling variable.  As it turns out, community is significant at the 1% level (t-score of -4.52).  Community is a nominal/categorical variable coded as 1,2,3 for 1 of 3 different communities.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is if this high degree of significance means I should be doing regressions on the communities individually rather than as an aggregation.  Otherwise, is using community as a controlling variable essentially doing that?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-17T12:46:54.120" FavoriteCount="6" Id="17110" LastActivityDate="2011-12-19T00:32:43.197" LastEditDate="2011-12-19T00:32:43.197" LastEditorUserId="919" OwnerUserId="6834" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;categorical-data&gt;&lt;stata&gt;&lt;multiple-regression&gt;&lt;aggregation&gt;" Title="Should I run separate regressions for every community, or can community simply be a controlling variable in an aggregated model?" ViewCount="1077" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Often as a result of several simulations which are themselves intensive a huge amount of information i.e., points/line/plane depending to the subject of investigation are available. Although there are extensive list of multivariate analysis and data-mining techniques to summarize the results however I am often amazed by a simple visualization giving the most of the information in one shot.&lt;br&gt;&#10;Being familiar with some techniques in visualization e.g., application of OpenGL etc I would like to ask:  1- Is there any &lt;strong&gt;software/technique/framework&lt;/strong&gt;/etc providing ability to visualize several million particles (points/lines/...)?&lt;br&gt;&#10;2- What is the most favorite application in the community of statistics? I'm not fan of &lt;strong&gt;R&lt;/strong&gt;! What else?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br&gt;&#10;Let consider 3D data to be visualized &lt;strong&gt;interactively&lt;/strong&gt;, so rotation and selection (slicing) being available &lt;strong&gt;realtime&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-17T12:57:48.573" FavoriteCount="4" Id="17112" LastActivityDate="2012-10-19T14:20:02.870" LastEditDate="2011-10-17T13:48:23.263" LastEditorUserId="6489" OwnerUserId="6489" PostTypeId="1" Score="1" Tags="&lt;interactive-visualization&gt;&lt;data-visualization&gt;" Title="How to visualize large amount of particles?" ViewCount="225" />
  
  
  <row Body="&lt;p&gt;I would generally use Fisher's exact test in this context.  I would focus on the genes for which you have phenotype information.&lt;/p&gt;&#10;&#10;&lt;p&gt;First you turn the information into a 2$\times$2 table, as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                       Phenotype 1&#10;                       asso   not asso&#10;Phenotype 2  asso      9      196&#10;             not asso  13     3817&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In R, you'd then use the function &lt;code&gt;fisher.test&lt;/code&gt;, as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tab &amp;lt;- rbind(c(9, 205-9), c(22-9, 4035-22-205+9))&#10;fisher.test(tab)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It gives a p-value of $5 \times 10^{-7}$.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-17T17:19:01.597" Id="17135" LastActivityDate="2011-10-17T17:19:01.597" OwnerUserId="5862" ParentId="17132" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="17140" AnswerCount="1" Body="&lt;p&gt;Can someone recommend a textbook(s) that covers the same topics as &lt;a href=&quot;http://www.whfreeman.com/Catalog/product/probabilityandstatistics-secondedition-evans/tableofcontents&quot; rel=&quot;nofollow&quot;&gt;Probability and Statistics&lt;/a&gt; by by Michael J. Evans and Jeffrey S. Rosenthal?&lt;/p&gt;&#10;&#10;&lt;p&gt;I find this particular textbook to be very poorly written and terse.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-17T17:26:18.163" Id="17137" LastActivityDate="2011-10-17T21:30:58.400" LastEditDate="2011-10-17T21:30:58.400" LastEditorUserId="183" OwnerUserId="6893" PostTypeId="1" Score="0" Tags="&lt;books&gt;&lt;mathematical-statistics&gt;" Title="What is an alternative to the book &quot;Probability and Statistics&quot; and covers the same material?" ViewCount="302" />
  <row AcceptedAnswerId="17142" AnswerCount="1" Body="&lt;p&gt;How would you explain to an only moderately statistically literate scientific reviewer that adjustment for multiple comparisons is not necessary when sampling from a Bayesian posterior?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-17T18:11:19.893" FavoriteCount="4" Id="17141" LastActivityDate="2011-10-17T19:55:44.997" LastEditDate="2011-10-17T19:55:44.997" LastEditorUserId="88" OwnerUserId="6740" PostTypeId="1" Score="11" Tags="&lt;bayesian&gt;&lt;multiple-comparisons&gt;" Title="Why there is usually no correction for multiple comparisons in Bayesian statistics" ViewCount="760" />
  <row Body="&lt;p&gt;I would follow Gelman's example:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://andrewgelman.com/2008/03/why_i_dont_usua_1/&quot; rel=&quot;nofollow&quot;&gt;http://andrewgelman.com/2008/03/why_i_dont_usua_1/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/multiple2.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.columbia.edu/~gelman/research/unpublished/multiple2.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-10-17T18:29:22.087" Id="17142" LastActivityDate="2011-10-17T18:29:22.087" OwnerUserId="1893" ParentId="17141" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There's no way to prove an error rate &amp;lt; 1/1,000,000 with only 4,000 trials.  You need to somehow select for errors (running more trials in parallel and only watching cases that result in an error) or apply some sort of stress that would increase the chance of an error, and then extrapolating from stressed conditions to normal conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's what geneticists would do, anyway....&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-17T20:59:45.297" Id="17154" LastActivityDate="2011-10-17T20:59:45.297" OwnerUserId="5862" ParentId="17150" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="17171" AnswerCount="4" Body="&lt;p&gt;Im trying to understand how people use R packages and was wondering if there are documented cases where R packages have produced different answers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Clarification: The motivation behind this question comes from an effort I'm involved in where the goal to understand the importance of provenance in the analytical methods and how it facilitates reproducible research. While R is big in the science community at present, and R packages are versioned in CRAN, without detailed information [especially version numbers], someone trying to reproduce a body of work in the future might come to a different conclusion that the original work (even with the original data). &lt;/p&gt;&#10;&#10;&lt;p&gt;Example: Paper by John Doe says &quot;we used R 2.3.1 and package glmulti to fit our models&quot;. 10 years from now, someone might use a new version of glmulti (no one knows what version was used in the original) which might produce a much different conclusion. My question: Are there examples of such a thing happening already? Version 2 or R package produces a much different result that version 1. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-17T22:31:30.097" FavoriteCount="2" Id="17156" LastActivityDate="2011-10-19T08:30:54.897" LastEditDate="2011-10-18T14:33:25.297" LastEditorUserId="1451" OwnerUserId="1451" PostTypeId="1" Score="6" Tags="&lt;r&gt;" Title="Are there examples of R packages changing dramatically between version such that the results of a statistical function were significantly different?" ViewCount="352" />
  <row AnswerCount="3" Body="&lt;p&gt;I have applied the one sample Kolmogorov Smirnov test of normality to two variables and one has a larger p value but both are greater than .05.&#10;e.g., &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$x_1$ (p-value) = 0.09 &lt;/li&gt;&#10;&lt;li&gt;$x_2$ (p-value) = 0.06 &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Does this mean that $x_1$ is better or more normal than $x_2$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-18T08:04:03.963" Id="17173" LastActivityDate="2012-07-11T21:27:55.757" LastEditDate="2012-06-29T20:12:58.053" LastEditorUserId="919" OwnerUserId="6225" PostTypeId="1" Score="4" Tags="&lt;statistical-significance&gt;&lt;p-value&gt;&lt;kolmogorov-smirnov&gt;&lt;model-comparison&gt;" Title="Can you compare p-values of Kolmogorov Smirnov tests of normality of two variables to say which is more normal?" ViewCount="2265" />
  <row AcceptedAnswerId="17180" AnswerCount="1" Body="&lt;p&gt;Following my earlier posts &lt;a href=&quot;http://stats.stackexchange.com/questions/15984/why-is-it-a-statistical-sin-to-run-a-large-number-of-correlations-and-only-rep&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/15584/how-to-test-50-correlations-for-statistical-significance&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/16235/how-to-succinctly-report-the-correlation-between-two-variables-for-20-different&quot;&gt;here&lt;/a&gt;, I understand the dangers of 'fishing' for a significant correlation. I also understand the 'problem' of running several correlations on the same data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am testing the relationship between variables A and B by several demographic characteristics, so I cannot help but run several correlation tests. For example, on gender I have 2 correlation tests (one for male and one for female), on age I have 3 correlation tests (one for less than 18 years, one for 18 to 44 years and one for over 45 years), on marital status, I have 3 correlation tests (one for married, one for never married and one for other).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;With all the concerns expressed in the earlier posts, is there anything I am missing on the use of Pearson r?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Remember, I am just focusing on the relationship between A and B by demographic characteristics of the respondents. &lt;strong&gt;Is there an alternative way to test this relationship?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-18T08:36:48.687" FavoriteCount="1" Id="17175" LastActivityDate="2011-10-18T18:49:18.393" LastEditDate="2011-10-18T18:49:18.393" LastEditorUserId="919" OwnerUserId="6096" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;" Title="How many correlations are too many?" ViewCount="369" />
  <row AcceptedAnswerId="17563" AnswerCount="1" Body="&lt;p&gt;I am currently working on some FX rate modeling. I decided to use the dataset from UBC: &lt;a href=&quot;http://fx.sauder.ubc.ca/data.html&quot; rel=&quot;nofollow&quot;&gt;http://fx.sauder.ubc.ca/data.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But I find that I have a problem about the timezone. The dataset is daily basis, with a Julian Day Number, does this number handle timezone shift? Or does it use GMT+0 or something else?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-18T09:20:14.817" Id="17176" LastActivityDate="2011-10-26T10:52:15.020" LastEditDate="2011-10-18T09:59:23.583" LastEditorUserId="88" OwnerUserId="6908" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;econometrics&gt;" Title="Do Julian day number handle timezone offset?" ViewCount="154" />
  <row Body="&lt;p&gt;If you know that this is the exact pattern to expect, then you can look for this exact pattern, but then you will miss other patterns.  So. If you know that the peak will be 150, then you could look for 2 or 3 or 4 or (however many) consecutive values of 150. But you say &quot;or so&quot; - how big is the &quot;or so&quot;? Perhaps the peak is defined as &quot;3 consecutive values over 130&quot; or maybe it's &quot;3 out of 5 consecutive values over 140&quot;. That's for you to decide. &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, if you are just looking for some general program to detect peaks - well, that's been looked at. There are a bunch of smoothing methods (e.g. loess, splines of various sorts, moving averages etc.). Not a field I'm expert in, but there's lots of literature on this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-18T10:18:24.133" Id="17181" LastActivityDate="2011-10-18T10:18:24.133" OwnerUserId="686" ParentId="17164" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I would recommend analyzing this in the following way:&lt;/p&gt;&#10;&#10;&lt;p&gt;Count each role in which George successfully predicts the outcome as a success, and every other one as a failure.  Then, you easily calculate a probability of success for George, and a 95% or 99% confidence interval.  Does he claim he can predict the outcome &quot;twice as well&quot; as randomly rolling the dice?  Then:&lt;/p&gt;&#10;&#10;&lt;p&gt;H0: p &gt;= 1/3&lt;/p&gt;&#10;&#10;&lt;p&gt;H1: p &amp;lt; 1/3&lt;/p&gt;&#10;&#10;&lt;p&gt;(assuming a 6-sided die).&lt;/p&gt;&#10;&#10;&lt;p&gt;From there, it's pretty straightforward to do the hypothesis test.  Also, you can calculate the power a priori pretty easily (even in something like Excel).  Pick a number of rolls (like 10), and then make a table with the possible successes as rows (0-10).  Then, for each success, calculate the probability he'll have that many successes (if he were to be just guessing, which is what we're assuming he is doing).  Also, for each value, determine if it would lead to a rejection or acceptance of the null.  Then, to find the power, you can simply add up all the probabilities where the null would be rejected.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-18T11:55:50.023" Id="17188" LastActivityDate="2011-10-18T11:55:50.023" OwnerUserId="6861" ParentId="14301" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I would recommend following two books:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387400826&quot; rel=&quot;nofollow&quot;&gt;Statistical methods for bioinformatics&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387848576&quot; rel=&quot;nofollow&quot;&gt;The elements of statistical learning&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="6" CommunityOwnedDate="2011-10-18T16:14:31.897" CreationDate="2011-10-18T13:11:34.533" Id="17193" LastActivityDate="2011-10-18T13:11:34.533" OwnerUserId="6723" ParentId="17189" PostTypeId="2" Score="2" />
&#10;$$&#10;we have that&#10;$$
&#10;P\{A ~\text{occurs before}~ B\} = \frac{k}{\binom{n+1}{2} - \binom{k+1}{2}}
&#10;&amp;amp; = \frac{\rho(\cdots, X_m+1)}{\rho(\cdots, X_m)}.
  
&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;And so,&lt;/p&gt;&#10;&#10;&lt;p&gt;$
  
  
&#10;$$&#10;So $Y\sim N(\mu,\rho^{2}+\sigma^{2})$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\bar{Y}=(Y_{1}+\cdots+Y_{n})/n$ be the our estimator for $\mu$. Then $\bar{Y}\sim\mathcal{N}(\mu,[\rho^{2}+\sigma^{2}]/n)$.&#10;Since we don't know $\sigma^{2}$, I'd recommend plugging in your&#10;favorite sample estimate for the variance of $Y$, call it $\hat{\sigma}_{Y}^{2}$,&#10;and plugging that in for $\rho^{2}+\sigma^{2}$. So you end up with&#10;$\bar{Y}\pm\hat{\sigma}_{Y}/\sqrt{n}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-18T18:57:12.953" Id="17222" LastActivityDate="2011-10-18T18:57:12.953" OwnerUserId="6640" ParentId="17215" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="17240" AnswerCount="1" Body="&lt;p&gt;First, the expected survival time when time is time to event is treated as a continuous r.v. is given as the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_0^{\infty} S(t)\, dt$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula for the mean residual survival time $\text{mrl}(u)  = E(T -u | T&amp;gt;u)$ is given by &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{S(u)}\int_u^{\infty} S(t)\, dt.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain the logic of dividing by $S(u)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;For discrete time, this step does not appear needed (cf &lt;a href=&quot;http://stats.stackexchange.com/questions/17216/restricted-mean-residual-life-survival-analysis-discrete&quot;&gt;related question&lt;/a&gt;). Why?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;ADD after Henry's explanation:&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say I have discrete time (12 months with the following hazards and Survival).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;month&amp;lt;-seq(0,12)&#10;h_x&amp;lt;-c(0,0.115706673,0.110186514,0.115769107,0.108296623,0.08908868,0.082548228,0.060146699,0.048112058,0.042197452,0.036919831,0.024691358,0.012787724)&#10;S_x&amp;lt;-c(1,0.8843,0.7869,0.6958,0.6204,0.5651,0.5185,0.4873,0.4639,0.4443,0.4279,0.4173,0.4120)&#10;&#10;plot(month,S_x,type=&quot;b&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;1) Is the mean (truncated) lifetime &lt;code&gt;sum(S_x[2:12]) #6.3117&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Lets say that we are know that a person survives up to 5th month (start of 5) i.e. they get through 4 complete months and we now want to estimated their mean (truncated) residual life. So, $\gamma$ =7 Is this computed as &lt;code&gt;sum(S_x[5:12])/S_x[5] #6.358317&lt;/code&gt; since 6.358 &amp;lt; $\gamma$+1 ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-18T21:32:43.103" FavoriteCount="1" Id="17229" LastActivityDate="2011-10-19T01:06:31.123" LastEditDate="2011-10-19T01:06:31.123" LastEditorUserId="2040" OwnerUserId="2040" PostTypeId="1" Score="1" Tags="&lt;survival&gt;" Title="Mean Survival Time: Comparison of Continuous and Discrete Time" ViewCount="249" />
  
  
  
  
  
  <row Body="&lt;p&gt;You can demean a variable as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summarize x&#10;generate x_demeaned = x - r(mean)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you have a batch of variables, you can use a loop. Suppose that you have three variables called peter, paul and mary:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;foreach v of varlist peter paul mary {&#10;    summarize `v'&#10;    generate `v'_demeaned = `v' - r(mean)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you have missing values and you want to use the variables in a regression, you might want to do listwise deletion. This means that you don't take into account obsevrations that have missings for any of the variables. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;egen anymiss = rowmiss(peter paul mary) &#10;foreach v of varlist peter paul mary {&#10;    summarize `v' if anymiss == 0&#10;    generate `v'_demeaned = `v' - r(mean) if anymiss == 0&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For detrending you have several possibilities. If you want to remove a linear trend from a variable y, you could do the following, supposing that t is the time index:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;regress y t&#10;predict y_detrended , resid&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can also have a look at&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;help tssmooth&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;as well as the following presentation:&#10;&lt;a href=&quot;http://ideas.repec.org/p/boc/asug06/2.html&quot; rel=&quot;nofollow&quot;&gt;http://ideas.repec.org/p/boc/asug06/2.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-19T12:30:28.167" Id="17267" LastActivityDate="2011-10-19T12:30:28.167" OwnerDisplayName="user5644" ParentId="17264" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;You need to be creative, because these data are consistent with &lt;em&gt;any&lt;/em&gt; mean exceeding $0\times .05 + 1\times .07 + \cdots + 5\times .18$ = $2.89$ and &lt;em&gt;any&lt;/em&gt; standard deviation exceeding $1.38$ (which are attained by assuming nobody visited any more than five times per month).&lt;/p&gt;&#10;&#10;&lt;p&gt;For reporting purposes, simply tabulate or graph the &lt;em&gt;raw&lt;/em&gt; data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jd2fj.png&quot; alt=&quot;Bar chart&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you &lt;em&gt;must&lt;/em&gt; have a summary of location and spread, use alternative measures that &lt;em&gt;can&lt;/em&gt; uniquely be found from these data.  The &lt;em&gt;median&lt;/em&gt; is between 2 and 3, because 45% visited 2 times or fewer and 67% visited 3 times or fewer.  You might simply interpolate linearly and report a median of 2.3 visits per month.  For the spread, use (say) an interquartile range, also computed with linear interpolation.  I find Q1 is 1.4 and Q3 is 3.3, for an IQR of 1.9.&lt;/p&gt;&#10;&#10;&lt;p&gt;To go beyond that, you need to fit the data with a distribution, which requires assumptions and therefore is not just reporting.  But it can be useful.  However, these data are elusive: they will not fit standard models like Binomial or Poisson.  (I recommend against trying to fit discretized versions of continuous distributions, such as Lognormal, because it's hard to find any reason &lt;em&gt;why&lt;/em&gt; they should fit: they don't form informative bases for comparison.  Moreover, since there are only six values here, it would be almost worthless to use more than one parameter in the modeling: two or more parameters give too much flexibility.)&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example of the insight that might be afforded by a simple distributional fit, suppose the visits are made randomly over time by individuals and each individual has the same probability (per unit time) of visiting.  This is potentially a useful and interesting framework against which these data can be compared.  It leads to a Poisson distribution.  The best fit (in a chi-squared sense) is achieved with an intensity of 3.185 per month; this also is the variance (whence the standard deviation is $\sqrt{3.185}$ = $1.8$).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/P7yCV.png&quot; alt=&quot;Data and Poisson fit&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is not a good fit (as a chi-squared test will show, but the eye plainly sees): there are too many people reporting 2 visits and too few reporting 1 visit. &lt;em&gt;That&lt;/em&gt; perhaps is the most interesting thing about this analysis.  You could announce these results like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The median number of monthly visits among the respondents is 2.3 (with an IQR of 1.9).  The data depart significantly from a (best fit) Poisson distribution with a mean of 3.18 visits per month in that 19 fewer people than expected report one visit and 37 more people than expected report two visits.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Incidentally, a Poisson fit suggestively fills in the upper tail of &quot;5 or more visits,&quot; providing &lt;em&gt;quantitative&lt;/em&gt; hypotheses that could be tested in follow-on surveys:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mSAGk.png&quot; alt=&quot;Poisson fit&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Other distributions would give different extrapolations into this upper range.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-10-19T16:03:10.613" Id="17277" LastActivityDate="2011-10-19T16:03:10.613" OwnerUserId="919" ParentId="17256" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose I want to see whether $z$ is a confounder for a model with $y$ the outcome variable and $x$ the predictor. If I adjust for $z$, and the adjusted coefficient of $x$ changes versus the unadjusted coefficient of $x$, does it matter by how much it changes? If the difference between the unadjusted and adjusted coefficients for $x$ is very small, should I still take this into account?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-19T16:50:25.267" Id="17280" LastActivityDate="2011-10-19T17:12:58.780" LastEditDate="2011-10-19T17:12:58.780" LastEditorUserId="2669" OwnerUserId="6934" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;confounding&gt;&lt;suppressor&gt;&lt;partial&gt;" Title="Differences in coefficients" ViewCount="127" />
  
  <row Body="&lt;p&gt;It is a time series data that you are using for unit root test. Therefore, order is usually important.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some stochastic processes though for which the property of &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/Time_reversibility&quot; rel=&quot;nofollow&quot;&gt;time reversibility&lt;/a&gt; &quot;  also holds. &lt;/p&gt;&#10;&#10;&lt;p&gt;According to Wikipedia on &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_walk&quot; rel=&quot;nofollow&quot;&gt;Random Walk&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;, &quot;A random walk on a graph is a very special case of a Markov chain. Unlike a general Markov chain, random walk on a graph enjoys a property called time symmetry or reversibility. Roughly speaking, this property, also called the principle of detailed balance, means that the probabilities to traverse a given path in one direction or in the other have a very simple connection between them (if the graph is regular, they are just equal). This property has important consequences.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;While testing for unit root though, there can be a drift term or a deterministic trend as well. Then it seems that reversibility may or may not hold. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general though, it seems to be the case that you need to know whether the nature of the series is suited to allow for the reversibility to hold. There are some statistical tests for that. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-19T20:08:40.270" Id="17291" LastActivityDate="2011-10-19T20:43:14.507" LastEditDate="2011-10-19T20:43:14.507" LastEditorUserId="4116" OwnerUserId="4116" ParentId="14565" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;With just two variables $X$ and $Y$, there are two sample variances $s^2$ and $t^2$, respectively, and the sample correlation coefficient, $r$.  If you standardize the variables in the usual way to have unit variances, so that $\xi = X/s$ and $\eta = Y/t$, then the two principal components are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$PC_1 = \xi+\eta = X/s + Y/t, \quad PC_2 = \xi-\eta = X/s - Y/t.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;As a check, note that Covar($PC_1, PC_2$) = Var($X/s$) - Var($Y/t$) = $1-1=0$, proving the components are orthogonal (uncorrelated).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Visually:&lt;/strong&gt; when you plot a scatterplot of $X$ and $Y$ in which the coordinate axes are expressed in standard units and have an aspect ratio of 1:1, then the axes of the point cloud fall along diagonal lines parallel to $X=Y$ and $X=-Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3j4az.png&quot; alt=&quot;Scatterplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this example the variances are $s^2 = 0.98$, $t^2 = 7.90$ and the correlation is $r=-0.67$.  Because $X$ and $Y$ are plotted on standardized scales with unit aspect ratio, the major axis of the cloud is diagonal (downward, due to negative correlation).  This is the first principal component, $X/s-Y/t$.  The minor axis of the cloud is also diagonal (upward) and forms the second principal component, $X/s+Y/t$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-19T20:16:22.123" Id="17292" LastActivityDate="2011-10-19T20:25:02.127" LastEditDate="2011-10-19T20:25:02.127" LastEditorUserId="919" OwnerUserId="919" ParentId="17285" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Firstly, I don't think it makes sense to say $x_{max}=f^{-1}(y_{max})$ here, that's like implying that it's a one-to-one function although $x_{max}$ is explained by other unobserved variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, it really depends on the context for which one to treat as an independent or dependent variable. From my experience, unless theory strongly suggests one way; either way is ok. From your comments on Oct 7, it seems like $x$ is the dependent while $y$ is the independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;If possible, look at the residuals and see if you can squeeze anything out of it. There could be another variable that you forgot; or it may help to transform your variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-20T09:05:02.593" Id="17310" LastActivityDate="2011-10-20T09:05:02.593" OwnerUserId="6845" ParentId="16628" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;An intuitive explanation for why&#10;$$  
  
  
  
  
  <row Body="&lt;p&gt;Initially there are three possibilities with equal probabilities:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A will be freed (prob $1/3$)&lt;/li&gt;&#10;&lt;li&gt;B will be freed (prob $1/3$)&lt;/li&gt;&#10;&lt;li&gt;C will be freed (prob $1/3$)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;With the promise of the message, there are four possibilities with different probabilities:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A will be freed and A is told B will be executed (prob $1/6$)&lt;/li&gt;&#10;&lt;li&gt;A will be freed and A is told C will be executed (prob $1/6$)&lt;/li&gt;&#10;&lt;li&gt;B will be freed and A is told C will be executed (prob $1/3$)&lt;/li&gt;&#10;&lt;li&gt;C will be freed and A is told B will be executed (prob $1/3$)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Conditional on &quot;A is told C will be executed&quot; this becomes &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A will be freed and A is told C will be executed (prob $1/3$)&lt;/li&gt;&#10;&lt;li&gt;B will be freed and A is told C will be executed (prob $2/3$)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So after the message A would like to swap with B (the Monty Hall problem) but cannot and so keeps the original $2/3$ probability of being executed.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-20T23:48:59.537" Id="17352" LastActivityDate="2011-10-20T23:48:59.537" OwnerUserId="2958" ParentId="17334" PostTypeId="2" Score="6" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Is a &quot;split plot&quot; ANOVA with two factors identical to two-way ANOVA with repeated measures in one factor? if not, what is the distinction?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-21T14:58:17.950" FavoriteCount="3" Id="17378" LastActivityDate="2011-10-22T11:40:57.710" OwnerUserId="25" PostTypeId="1" Score="5" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;split-plot&gt;" Title="Is a &quot;split plot&quot; ANOVA with two factors the same as two-way ANOVA with repeated measures in one factor?" ViewCount="3413" />
  <row Body="&lt;p&gt;The propagation will depend on the diagonalization algorithm--which might be a black box--as well as the multivariate distribution of the errors.  Pursuing an analytical solution therefore looks unpromising.  Why not just compute an empirical distribution?  That is, draw a large number of variants of the original matrix from the hypothesized error distribution and diagonalize them.  Study the output distribution of the eigenvectors and eigenvalues.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some subtleties, because there will not be a definite matching among the lists of eigenvalues.  For instance, in one iteration the sorted eigenvalues might be $(1.0, 0.99, 0.17)$ and in the next they might be $(1.01, 0.98, 0.17)$.  Is the $1.01$ in the latter a slight variation of the $1.0$ in the former, or perhaps has the $0.99$ been perturbed into $1.01$ and the $1.0$ into $0.98$?  It is impossible to know.  Thus, you need to characterize the multivariate distribution of &lt;em&gt;multisets&lt;/em&gt; of eigenvalues rather than $n$-tuples of eigenvalues.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same problem attaches to the eigenvectors, but it gets worse, because there is no unique normalization of the eigenvectors.  (They are determined only up to sign.)  However, these problems are no different in nature than the ambiguities present in other geometric problems such as characterizing the directions of linear features in a plane (which can be given only up to a multiple of 180 degrees) and so should not present any additional conceptual challenge; they are just going to be a nuisance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example of the empirical distributions of the sorted eigenvalues of a 4 by 4 matrix, using 2500 draws from the error distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/e560T.png&quot; alt=&quot;scatterplot matrix&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This scatterplot matrix also shows the lines y=x on each plot to emphasize the constraints imposed by sorting the four eigenvalues.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-21T16:09:12.543" Id="17384" LastActivityDate="2011-10-21T17:21:03.593" LastEditDate="2011-10-21T17:21:03.593" LastEditorUserId="919" OwnerUserId="919" ParentId="17383" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You might consider the added-variable plot or the component-plus-residual plot (of which the CERES plot is a better variant).  If you're using R, the &lt;code&gt;car&lt;/code&gt; package has these tools built in for both linear models and generalized linear models.  The accompanying reference, &lt;a href=&quot;http://socserv.mcmaster.ca/jfox/Books/Companion/index.html&quot; rel=&quot;nofollow&quot;&gt;An R Companion to Applied Regression&lt;/a&gt;, has more details, as do (I think) the authors' two regression texts that this reference accompanies.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-21T16:11:20.903" Id="17385" LastActivityDate="2011-10-21T16:11:20.903" OwnerUserId="3601" ParentId="17350" PostTypeId="2" Score="1" />
&#10;\mathbb P\left(\frac{r^2}{1-r^2} (n-2) \geq q_{1-\alpha} \right) \approx \alpha \&amp;gt;,
  
  <row AcceptedAnswerId="17392" AnswerCount="1" Body="&lt;p&gt;How would you go about explaining i.i.d to non techncial people?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-07T13:59:06.877" FavoriteCount="2" Id="17391" LastActivityDate="2013-01-27T20:48:53.913" OwnerDisplayName="user333" OwnerUserId="333" PostTypeId="1" Score="7" Tags="&lt;random-variable&gt;" Title="What are i.i.d. random variables?" ViewCount="11356" />
  
  <row AnswerCount="4" Body="&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/17407/how-to-digest-statistical-context&quot;&gt;This question&lt;/a&gt; illustrates the difficulty of a person mastering statistics and probability on their own, in the face of weakly developed resources like Wikipedia.&lt;/p&gt;&#10;&#10;&lt;p&gt;It occurred to me that consulting statisticians, and there are a few here, may routinely face the challenge of explaining certain concepts and methods to a client.  This is the flip side of the pedagogical coin.  When one has mastered the concept, it may make sense to conduct a particular avenue of analyses, but one's references may either be inappropriate or difficult to share with a client.  So, are there common resources that consulting statisticians like to suggest to their clients?  (See update #1 regarding more advanced or specialized topics.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I can think of a few books that may be useful, but I suspect that a lot of clients will go about searching the web, as Developer did, and will come across rather inane material on Wikipedia.  In my answer to Developer, I suggested the &lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/&quot;&gt;NIST Handbook&lt;/a&gt; as one such reference that could be used.  What else?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Update 1: As &lt;a href=&quot;http://stats.stackexchange.com/questions/17413/references-for-consulting-statisticians-to-offer-their-clients/17443#17443&quot;&gt;Peter Flom has pointed out&lt;/a&gt;, for more advanced material or narrower pursuits, it may not be easy to offer a single point of reference.  This is correct and I should have worded the question differently for those cases.  In such cases, how do consultants find and share &lt;strong&gt;accessible&lt;/strong&gt; references?  I believe that many consultants will take the time to write something new in order to explain things to their client, but those aren't references that are found and shared.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some ideas:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Tutorials written by the consultant or others&lt;/li&gt;&#10;&lt;li&gt;Case studies or analyses from projects that demonstrate the same concepts&lt;/li&gt;&#10;&lt;li&gt;Excerpts of books (as I'd suggested in my answer to Developer), which describe the concept&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What else might be a source or how else do you actually go about finding such references?  I realize this is an open ended question, but &lt;a href=&quot;http://stats.stackexchange.com/questions/17407/how-to-digest-statistical-context/17412#17412&quot;&gt;my answer to Developer&lt;/a&gt; shows some of the ways I'd approach this problem.  I don't mean to ask of &lt;em&gt;all&lt;/em&gt; the ways that one could address this, but in one's own experience, how have you typically provided such explanatory resources?&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2011-10-24T18:52:35.213" CreationDate="2011-10-22T13:40:46.540" FavoriteCount="2" Id="17413" LastActivityDate="2011-10-24T23:26:47.693" LastEditDate="2011-10-23T13:40:52.833" LastEditorUserId="5256" OwnerUserId="5256" PostTypeId="1" Score="11" Tags="&lt;teaching&gt;&lt;references&gt;" Title="References for consulting statisticians to offer their clients" ViewCount="299" />
  <row AnswerCount="1" Body="&lt;p&gt;What are the modeling approaches depicted here? Can you name them and their prominent proponents or a landmark model? Is there an accepted superior approach? Who prefers which approach?&#10;&lt;img src=&quot;http://i.stack.imgur.com/64Pe4.jpg&quot; alt=&quot;alt text&quot;&gt;&#10;(From: &lt;a href=&quot;http://www.stat.duke.edu/~mw/fineart.html&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.duke.edu/~mw/fineart.html&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-22T15:02:08.420" Id="17417" LastActivityDate="2011-10-22T17:12:41.007" LastEditDate="2011-10-22T17:12:41.007" LastEditorUserId="6725" OwnerUserId="6725" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;modeling&gt;&lt;prior&gt;&lt;frequentist&gt;" Title="What are the modeling approaches in this cartoon?" ViewCount="344" />
  <row Body="&lt;p&gt;The two tests are not absolutely independent.  They are conditionally independent on C.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thinking about it intuitively, if you test positive for the first test, we can predict that it's more likely you'll test positive for the second test, hence the two are not absolutely independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, &lt;em&gt;given&lt;/em&gt; that you &lt;em&gt;know&lt;/em&gt; that you have cancer, the two tests are independent i.e. knowing the outcome of the first will not affect our prediction of the outcome of the second.  Hence, &quot;conditional independence&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-22T17:39:07.907" Id="17422" LastActivityDate="2011-10-22T17:39:07.907" OwnerUserId="5987" ParentId="17421" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have some measurements of a biological analyte that exhibits a log-normal distribution. There was both a level shift and some long-term drift in the values that I believe is due to the measurement process rather than to biology. I would like to use recent values as the reference distribution and transform within months or quarters so that the mean(log(Y)) and sd(log(Y)) will match. Experimentation makes me think I can do the transformation with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;exp( sd(log(X_ref))/sd(log(Y) *( log(Y) -mean(log(X_ref) ) ) -&amp;gt; Y_transformed&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can this be supported in theory as opposed to experiment?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-23T03:24:06.187" Id="17435" LastActivityDate="2011-10-23T03:24:06.187" OwnerUserId="2129" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;lognormal&gt;" Title="Transform LN variates to match reference mean and SD" ViewCount="130" />
  
  
  
  <row AcceptedAnswerId="22333" AnswerCount="2" Body="&lt;p&gt;Let's say I am analyzing behavioral patterns over the course of an hour.  I have recorded three different behaviors and the time stamps (start end) they occurred at.  Something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;yawning       stretching    whispering&#10;2:21-2:22     3:31-3:33     1:21-1:30&#10;3:42-3:45     8:23-8:59     9:27-9:33&#10;9:20-925      9:34-9:44     14:04-14:07&#10;14:45-14:32   15:01-15:06   18:00-18:22&#10;.&#10;.&#10;.&#10;45:40-45-43   45:23-45:30   44:19-44:44&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there a statistical method for determining if certain behaviors correlate or cluster around certain time periods/to each other?  For instance maybe I want to know if these three (or just 2) behaviors are found in close proximity to one another or maybe I want to know if these which behaviors are not in close proximity to each other.  Which of the three behaviors tend to cluster together?&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't even know what field of stats I'm looking at with this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-23T01:35:33.280" FavoriteCount="1" Id="17462" LastActivityDate="2012-04-26T21:39:57.440" LastEditDate="2011-10-24T10:38:37.917" LastEditorUserId="930" OwnerDisplayName="Tyler Rinker" OwnerUserId="7482" PostTypeId="1" Score="5" Tags="&lt;clustering&gt;" Title="Correlating time stamps" ViewCount="139" />
  
  <row Body="&lt;p&gt;I highly recommend the book &lt;a href=&quot;http://scholar.google.com/scholar?q=statistics%20as%20principled%20argument&amp;amp;hl=en&amp;amp;as_sdt=0&amp;amp;as_vis=1&amp;amp;oi=scholart&quot; rel=&quot;nofollow&quot;&gt;&quot;Statistics as Principled Argument&quot;&lt;/a&gt; by Robert Abelson. It's a small book, with almost no formulas, suitable for anyone who's had any introduction to statistics at all, and for many people who have had a lot of courses. I reviewed it on my blog &lt;a href=&quot;http://www.statisticalanalysisconsulting.com/book-review-statistics-as-principled-argument-by-robert-abelson/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-10-24T23:26:47.693" CreationDate="2011-10-24T23:26:47.693" Id="17499" LastActivityDate="2011-10-24T23:26:47.693" OwnerUserId="686" ParentId="17413" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Any solution will do in principle what you are doing, though with various optimization.  If you are going to be looking for many different cutoffs, it may be worth computing all the percent completes and then testing for many thresholds.  If you are doing it only once, it may be worth just doing the computation on the fly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Make your sample data readable&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat &amp;lt;- read.table(textConnection(&#10;&quot;Time Counts&#10;1 2&#10;2 0&#10;3 4&#10;4 3&#10;5 1&quot;), header=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can easily compute the other two columns:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat$Total.counts &amp;lt;- cumsum(dat$Counts)&#10;dat$Percent.complete &amp;lt;- dat$Total.counts / dat$Total.counts[length(dat$Total.counts)]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then the row that first meets or exceeds a threshold X can be gotten, either by row index or the row itself.  This example uses a threshold of 34%&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- 0.34&#10;# row number&#10;min(which(dat$Percent.complete &amp;gt;= x))&#10;# row itself&#10;dat[min(which(dat$Percent.complete &amp;gt;= x)),]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you are doing it as a one-off and don't want to precompute the third and fourth columns, you can do it all at once:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat[min(which((cumsum(dat$Counts)/sum(dat$Counts)) &amp;gt;= x)),]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-10-25T18:32:49.383" Id="17525" LastActivityDate="2011-10-25T18:32:49.383" OwnerUserId="5880" ParentId="17522" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am currently calculating reliability estimates for test-retest data.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is regarding the difference between standard error of measurement (SEM) versus minimum detectable change (MDC) when seeking to determine if there is a 'real' difference between two measurements.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my thinking thus far:&lt;/p&gt;&#10;&#10;&lt;p&gt;Each measurement has an error band about it. For two measurements, if error bands overlap then there is no 'real' difference between the measurements.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;For example, at 95% confidence, each measurement has an error band of $\pm 1.96 \times SEM$. So, two measurements would need to be more than $2 \times 1.96 \times SEM =3.92 \times SEM$ apart to avoid each measurement's confidence interval overlapping and for their to be a real difference between the two measurements.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Another method for determining if two measurements are 'different' is to use MDC where &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$$MDC = 1.96 \times \sqrt{2} \times SEM =2.77 \times SEM$$&lt;/p&gt;&#10;&#10;&lt;p&gt;[EDIT: for second formula see e.g. p. 238 of Weir, J. P. (2005). Quantifying test-retest reliability using the intraclass correlation coefficient and the SEM. Journal of strength and conditioning research / National Strength &amp;amp; Conditioning Association, 19(1), 231–240. doi:10.1519/15184.1]&lt;/p&gt;&#10;&#10;&lt;p&gt;If the difference between the two measurements is greater than MDC then there is a real difference between the measurements.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously the two formulas are different and would produce different results. So which formula is correct?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-26T00:57:59.640" FavoriteCount="1" Id="17549" LastActivityDate="2013-05-02T14:33:04.490" LastEditDate="2013-05-02T08:43:43.783" LastEditorUserId="16974" OwnerUserId="7056" PostTypeId="1" Score="4" Tags="&lt;reliability&gt;" Title="Standard error of measurement versus minimum detectable change" ViewCount="3242" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to classify two types of objects, which unfortunately have high-dimensional features with few samples. (230 features 12 samples from each group).&lt;/p&gt;&#10;&#10;&lt;p&gt;As a first step:&#10;To reduce the dimension, I have tested three different approaches of PCA (each with slightly different  parameters)  and use the scores of the first 3 PCs as the features instead of the original 230 features.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a second step:&#10;in order to classify those objects, I have tested three different classifiers (SVM, 3-NN, Naive Bayes) where I use the leave one out method, i.e.: training the classifier on 23 objects and tested it on the one leave out, for testing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sum up: I have use 3 different dimensional reduction methods and three use different classifiers (nine combinations overall).&lt;/p&gt;&#10;&#10;&lt;p&gt;In one combination (out of 9), I got excellent classification performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question: Is there some statistic approach I can use here in order to prove that this option was indeed robust and the correct one, and didn't happen by chance?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-10-26T08:51:11.910" FavoriteCount="2" Id="17559" LastActivityDate="2011-10-26T12:42:51.177" LastEditDate="2011-10-26T09:47:00.747" LastEditorUserId="88" OwnerUserId="6637" PostTypeId="1" Score="2" Tags="&lt;model-selection&gt;&lt;dimensionality-reduction&gt;" Title="I have 3 dimension reduction methods and 3 classifiers, how to select the best combination?" ViewCount="156" />
  
  <row Body="&lt;p&gt;Have you considered using &lt;a href=&quot;http://en.wikipedia.org/wiki/Rejection_sampling&quot; rel=&quot;nofollow&quot;&gt;rejection sampling&lt;/a&gt;? Given your estimated density $f(x)$ you will need to find another density $g(x)$ which satisfies $f(x)&amp;lt;Mg(x)$. Then proceed with the algorithm described in a link. If your density $f(x)$ for example has bounded support, i.e. is zero outside and some interval $[a,b]$, you can simply use uniform distribution in $[a,b]$ as your density $g$. This method has its drawbacks, which again are listed in the wikipedia link. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another possible solution is to calculate the cdf $F(x)=\int_{-\infty}^xf(t)dt$ and find its inverse $F^{-1}$. Then sample uniform random variables $U_i$ from interval $[0,1]$ and your sample from $f$ will be $X_i=F^{-1}(U_i)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second approach should be computationally intensive, since you can find the inverse only by numerical methods. But then again it is not that hard to code. &lt;/p&gt;&#10;&#10;&lt;p&gt;These two approaches are general methods which apply to any density $f$. I would search the literature whether there are specific methods. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-26T10:43:07.840" Id="17562" LastActivityDate="2011-10-26T10:43:07.840" OwnerUserId="2116" ParentId="17531" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you would have clicked the links on that page you would have ended up &lt;a href=&quot;http://aa.usno.navy.mil/data/docs/JulianDate.php&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Here is the relevant quote&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Julian dates (abbreviated JD) are simply a continuous count of days&#10;  and fractions since noon Universal Time on January 1, 4713 BCE (on the&#10;  Julian calendar)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Another &lt;a href=&quot;http://aa.usno.navy.mil/faq/docs/UT.php&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt; in that page defines what Universal Time is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The times of various events, particularly astronomical and weather&#10;  phenomena, are often given in &quot;Universal Time&quot; (abbreviated UT) which&#10;  is sometimes referred to, now colloquially, as &quot;Greenwich Mean Time&quot;&#10;  (abbreviated GMT). The two terms are often used loosely to refer to&#10;  time kept on the Greenwich meridian (longitude zero), five hours ahead&#10;  of Eastern Standard Time. Times given in UT are almost always given in&#10;  terms of a 24-hour clock. Thus, 14:42 (often written simply 1442) is&#10;  2:42 p.m., and 21:17 (2117) is 9:17 p.m. Sometimes a Z is appended to&#10;  a time to indicate UT, as in 0935Z.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So the answer is yes. Julian Day uses GMT+0.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-26T10:52:15.020" Id="17563" LastActivityDate="2011-10-26T10:52:15.020" OwnerUserId="2116" ParentId="17176" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I am not familiar with the coding language you are using, but it seems, that you are fitting the linear regression model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$m_1=A+Bm_2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Or vice versa&lt;/p&gt;&#10;&#10;&lt;p&gt;$$m_2=A+B m_1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you measure both $m_1$ and $m_2$ in $cm^2/s$, then $A$ has the dimension $cm^2/s$ and $B$ is dimensionless. Hence any associated measures (such as variance and standard error) with $B$ are dimensionless too.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-26T11:37:26.003" Id="17566" LastActivityDate="2011-10-26T11:37:26.003" OwnerUserId="2116" ParentId="17564" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Informal explanation&lt;/strong&gt;:  In the course of my research I've run into the following problem: I am observing a machine that outputs random numbers.  Most (if not all) of these random numbers come from the Gaussian distribution $\mathcal{N}(0,\sigma_0^2)$.  I know the value of the variance $\sigma_0^2$.  However, some proportion of the numbers I observe may come from another distribution $A$.  I know that the mean of $A$ is zero, and that $A$ is symmetric (not sure if symmetry helps us).  I also know that its variance is $\sigma_a^2&amp;gt;\sigma_0^2$, but I do not know the exact value of $\sigma_a^2$ (unlike $\sigma_0^2$.)  The presence of random numbers from $A$ indicates a problem with the machine I am watching.  Note that even in that case, my machine will &lt;em&gt;mostly&lt;/em&gt; output random values from $\mathcal{N}(0,\sigma_0^2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to construct a hypothesis test on the set $\mathbf{X}$ of $n$ observations I have collected from my machine to determine whether my machine is malfunctioning.  Assume that $n$ is large.  Null hypothesis is that machine operates normally (i.e. $x_i\sim \mathcal{N}(0,\sigma_0^2)$ where $x_i\in\mathbf{X}$). Alternate hypothesis is that machine is malfunctioning (that is, $x_i \sim \mathcal{N}(0,\sigma_0^2)$ with $x_i\in \mathbf{X}_N$, and $y_i\sim A$ with $y_i\in \mathbf{X}_A$, where $\mathbf{X}_N\cup \mathbf{X}_A=\mathbf{X}$ and $\mathbf{X}_N\cap \mathbf{X}_A=\emptyset$.) &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: what is the proportion $\pi$ of observations generated by $A$ in my set of observations $\mathbf{X}$ so that big given enough size of that set $n$, I achieve arbitrarily small type I and type II errors?  I.e. how many out of $n$ observations in $\mathbf{X}$ have to come from $A$ for me to determine with negligible error that my machine is broken?&lt;/p&gt;&#10;&#10;&lt;p&gt;My intuition, based on the CLT, suggests that that if more than $\sqrt{n}$ observations come from $A$, then null hypothesis will be rejected with vanishingly small sum of errors for large $n$. Is my intuition correct?  If so, how can I back it up?  If not, why?  Does it depend on the form of $A$ (what if I told you that $A$ is the sum of $\mathcal{N}(0,\sigma_0^2)$ and another, possibly Gaussian, distribution?)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Attempt at a formal statement of the problem and conjecture&lt;/strong&gt;: For a zero-mean symmetric mixture distribution $(1-\pi)\mathcal{N}(0,\sigma_0^2)+\pi A(\sigma_a^2)$, I construct a test between two hypotheses:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{align}
&#10;H_1:&amp;amp;\pi\neq 0
&#10;p_2 = relatives \times {2 \over n_2} + non.relatives \times {4 \over n_2}
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am fitting a 3-level model in Bugs. The data are in long format. The data consists of 885 Treatments in 1:3 COURSEs of treatment in 108 Patients (MRNs).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model{&#10;&#10;for(DMRN in 1:NMRN) {&#10;&#10;  RMRN[DMRN] ~ dnorm(meanMRN[DMRN], TMRN)&#10;  meanMRN[DMRN] &amp;lt;- intercept&#10;&#10;  for(DCOURSE in SMRN[DMRN]:(SMRN[DMRN+1]-1)){&#10;&#10;    RCOURSE[DCOURSE] ~ dnorm(meanCOURSE[DCOURSE], TCOURSE)&#10;    meanCOURSE[DCOURSE] &amp;lt;- RMRN[DMRN]&#10;&#10;    for(Dobservations in SCOURSE[DCOURSE]:(SCOURSE[DCOURSE+1]-1)){&#10;&#10;      totalEEG[Dobservations] ~ dpois(meanobservations[Dobservations])&#10;      log(meanobservations[Dobservations]) &amp;lt;- RCOURSE[DCOURSE] +inprod(betaobservations[] , Xobservations[Dobservations,])&#10;&#10;    }#observations&#10;  }#COURSE&#10;}#MRN&#10;&#10;&#10;# priors&#10;&#10;intercept ~ dflat()&#10;betaobservations[1] ~ dflat()&#10;betaobservations[2] ~ dflat()&#10;betaobservations[3] ~ dflat()&#10;&#10;TMRN &amp;lt;- pow(SDMRN, -2)&#10;SDMRN ~ dunif(0, 100)&#10;TCOURSE &amp;lt;- pow(SDCOURSE, -2)&#10;SDCOURSE ~ dunif(0, 100)&#10;&#10;} # model&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am worried that my covariates in the model - one of which (Age) varies only at the top level - are replicated in long format and therefore &quot;counted more&quot; in BUGS and giving me spuriously precise posterior distributions. However, obviously it is not contributing any variation at the lowest level (within patient).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;bgs.toteeg&amp;lt;-glmmBUGS(formula = totalDuration ~ AnaDose+ AgeYrs + log.workDose, effects=c(&quot;MRN&quot;, &quot;COURSE&quot;), family=&quot;poisson&quot;, data=book,  modelFile=&quot;model.bug&quot;)&#10;source(&quot;getInits.R&quot;)&#10;startingValues=bgs.toteeg$startingValues&#10;myResult = bugs(bgs.toteeg$ragged, getInits ,model.file = &quot;model.bug&quot;, n.chain = 3, n.iter = 10000,  n.burnin = 1000, parameters.to.save = names(getInits()),n.thin = 10, debug=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;AgeYrs is included as a covariate here but obviously doesn't vary across patient-treatment, but only between patients. All of the other covariates are different values for each treatment.&lt;/p&gt;&#10;&#10;&lt;p&gt;The results back in R are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mysummary$scalars[, c(&quot;mean&quot;, &quot;2.5%&quot;,&quot;97.5%&quot;)]&#10;                mean        2.5%    97.5%&#10;intercept 3.36308519 3.220175000 3.473550&#10;SDCOURSE  0.04677978 0.001696725 0.094267&#10;SDMRN     0.31222407 0.265697500 0.357700&#10;&amp;gt; signif(mysummary$betas[, c(&quot;mean&quot;,&quot;2.5%&quot;, &quot;97.5%&quot;)], 3)&#10;               mean     2.5%   97.5%&#10;MHXKG        0.0605 -0.01360 0.14200&#10;AgeYrs       0.0027  0.00162 0.00407&#10;log.workDose 0.0457  0.03170 0.06000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How does the model prevent spurious replication of repeated measures which in this case, are exactly the same across patient treatments, but only differ between patients, therefore at the COURSE&quot; level there will be a low or no variability, at the Patient level there will be, variability according to :&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4qMdT.png&quot; alt=&quot;Histogram of Age (with replication)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also here is the same model fit with ML in lmer:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Linear mixed model fit by REML &#10;    Formula: totalEEG ~ AgeYrs + MHXKG + log.workDose + (1 | MRN/COURSE) &#10;       Data: book &#10;      AIC  BIC logLik deviance REMLdev&#10;     7015 7048  -3500     7006    7001&#10;    Random effects:&#10;     Groups     Name        Variance   Std.Dev.  &#10;     COURSE:MRN (Intercept) 2.7923e-11 5.2842e-06&#10;     MRN        (Intercept) 1.8464e+02 1.3588e+01&#10;     Residual               2.4688e+02 1.5713e+01&#10;    Number of obs: 817, groups: COURSE:MRN, 114; MRN, 103&#10;&#10;    Fixed effects:&#10;                 Estimate Std. Error t value&#10;    (Intercept)  31.12003    7.35545   4.231&#10;    AgeYrs        0.10522    0.05946   1.769&#10;    MHXKG         4.57064    3.71167   1.231&#10;    log.workDose  1.14654    0.82651   1.387&#10;&#10;    Correlation of Fixed Effects:&#10;                (Intr) AgeYrs MHXKG &#10;    AgeYrs      -0.504              &#10;    MHXKG       -0.562  0.002       &#10;    log.workDos -0.671 -0.006  0.093&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here is a model that has -as far as I can see Age varying only at the Patient level. Fit with Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. xtmixed totalEEG MHXKG workDose, || MRN: AgeYrs, covariance(exchangeable)&#10;&#10;Performing EM optimization: &#10;&#10;Computing standard errors:&#10;&#10;Mixed-effects REML regression                   Number of obs      =       817&#10;Group variable: MRN                             Number of groups   =       103&#10;&#10;                                                Obs per group: min =         1&#10;                                                               avg =       7.9&#10;                                                               max =        31&#10;&#10;&#10;                                                Wald chi2(2)       =     10.60&#10;Log restricted-likelihood = -3504.9961          Prob &amp;gt; chi2        =    0.0050&#10;&#10;------------------------------------------------------------------------------&#10;    totalEEG |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       MHXKG |   2.434717   3.560351     0.68   0.494    -4.543442    9.412877&#10;    workDose |  -.0102969   .0033165    -3.10   0.002    -.0167971   -.0037966&#10;       _cons |   48.81418   4.001097    12.20   0.000     40.97217    56.65618&#10;------------------------------------------------------------------------------&#10;&#10;------------------------------------------------------------------------------&#10;  Random-effects Parameters  |   Estimate   Std. Err.     [95% Conf. Interval]&#10;-----------------------------+------------------------------------------------&#10;MRN: Exchangeable            |&#10;            sd(AgeYrs _cons) |   .1967401   .0184842      .1636517    .2365186&#10;          corr(AgeYrs,_cons) |   .9999981   .0007988            -1           1&#10;-----------------------------+------------------------------------------------&#10;                sd(Residual) |   15.86585   .4234722       15.0572    16.71794&#10;------------------------------------------------------------------------------&#10;LR test vs. linear regression:       chi2(2) =   160.00   Prob &amp;gt; chi2 = 0.0000&#10;&#10;Note: LR test is conservative and provided only for reference.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd be delighted if somebody could clarify how this works .. or whether I'm getting it wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;edit( +2hrs) Having looked back over PinHeiro &amp;amp; Bates I wonder whether: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;check.fit&amp;lt;-lmer(totalEEG ~ MHXKG + workDose +(AgeYrs|MRN), data=book, family=gaussian(link = &quot;identity&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is more correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;It produces the same parameter values as above STATA output. However, how would one specify this model in BUGS - or in glmmBUGS to get into BUGS??&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-27T10:36:21.797" FavoriteCount="1" Id="17617" LastActivityDate="2011-10-27T16:02:34.603" LastEditDate="2011-10-27T16:02:34.603" LastEditorUserId="6666" OwnerUserId="6666" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;multilevel-analysis&gt;&lt;bugs&gt;" Title="Does my multilevel modelling create false replication and spurious precision at the highest level?" ViewCount="115" />
  <row Body="&lt;p&gt;A general approach, which also leads to a fix, is to expand predictors as cubic spline functions.  You can either test all the nonlinear terms together to test linearity, or just use the fits as-is (which preserves type I error).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-27T11:34:32.613" Id="17619" LastActivityDate="2011-10-27T11:34:32.613" OwnerUserId="4253" ParentId="17600" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Assuming there are no biases in the sampling technique, this should be fine. Some questions to ask might be:&lt;/p&gt;&#10;&#10;&lt;p&gt;-&gt; Was the survey conducted in Spanish if requested? (Language bias)&#10;-&gt; Was the survey conducted over the phone or in person? If over the phone, and cell phones were excluded, are Spanish players more or less likely to own cell phones than players in the rest of Europe, and for what reasons? &#10;-&gt; Was the rate at which Spanish players refused to answer survey questions different from the rate for players as a whole?&#10;-&gt; Overall, what proportion of Spanish players were sampled?&lt;/p&gt;&#10;&#10;&lt;p&gt;Without knowing the exact composition of the data it's hard to say more. Are there any specific issues you're concerned about?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-27T17:42:32.560" Id="17636" LastActivityDate="2011-10-27T17:42:32.560" OwnerUserId="6446" ParentId="17626" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You can do it in Excel with &lt;a href=&quot;http://tukhi.com&quot; rel=&quot;nofollow&quot;&gt;Tukhi&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Enter &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;=tukhi.average(abs(2*rand()-1 - (2*rand()-1))) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and hit the run button.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-27T20:54:29.310" Id="17644" LastActivityDate="2011-10-27T22:04:37.580" LastEditDate="2011-10-27T22:04:37.580" LastEditorUserId="582" OwnerUserId="7094" ParentId="17455" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to conduct a meta analysis for dose response studies where I am using fractional polynomial transformation from predefined family of powers. Now data fitting using all possible combinations will yield large number of possible models where some of them are clearly not good while there are few models that produce low heterogeneity. Now to choose the best model there is the problem of the dimensionality similarity. All models have the same number of parameters so using AIC or BIC is just a reflection for the likelihood. Any suggestions to proper model selection method in this case? Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-28T15:52:20.367" Id="17683" LastActivityDate="2011-10-28T15:52:20.367" OwnerUserId="7104" PostTypeId="1" Score="2" Tags="&lt;model-selection&gt;&lt;aic&gt;&lt;bic&gt;" Title="Random coefficient models with fractional polynomial transformation selection method, which criterion to use?" ViewCount="113" />
  
  
  
  <row Body="&lt;p&gt;I have never understood the wish for parsimony.  Seeking parsimony destroys all aspects of statistical inference (bias of regression coefficients, standard errors, confidence intervals, P-values).  A good reason to keep variables is that this preserves the accuracy of confidence intervals and other quantities.  Think of it this way: there have only been developed two unbiased estimators of residual variance in ordinary multiple regression: (1) the estimate from the pre-specified (big) model, and (2) the estimate from a reduced model substituting generalized degrees of freedom (GDF) for apparent (reduced) regression degrees of freedom.  GDF will be much closer to the number of candidate parameters than to the number of final &quot;significant&quot; parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's another way to think of it.  Suppose you were doing an ANOVA to compare 5 treatments, getting a 4 d.f. F-test.  Then for some reason you look at pairwise differences between treatments using t-tests and decided to combine or remove some of the treatments (this is the same as doing stepwise selection using P, AIC, BIC, Cp on the 4 dummy variables).  The resulting F-test with 1, 2, or 3 d.f. will have inflated type I error.  The original F-test with 4 d.f. contained a perfect multiplicity adjustment.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-10-29T13:57:11.290" Id="17704" LastActivityDate="2011-10-29T13:57:11.290" OwnerUserId="4253" ParentId="17624" PostTypeId="2" Score="15" />
  <row AnswerCount="1" Body="&lt;p&gt;Is it to show that MSE = 0 as $n\rightarrow\infty$? I also read in my notes something about plim. How do I find plim and use it to show that the estimator is consistent?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-29T14:15:41.797" FavoriteCount="1" Id="17706" LastActivityDate="2011-10-29T19:50:08.660" LastEditDate="2011-10-29T15:56:23.777" LastEditorUserId="5594" OwnerUserId="3062" PostTypeId="1" Score="6" Tags="&lt;estimation&gt;&lt;convergence&gt;&lt;consistency&gt;" Title="How to show that an estimator is consistent?" ViewCount="9112" />
  <row Body="&lt;p&gt;You can decompose this operation into a set of smaller operations that are easy to parallelize.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you wish to solve $\mathbf{m}\mathbf{v}=\mathbf{u}$ for an $N$ by $N$ matrix $\mathbf{m}$ and $N$-vector $\mathbf{u}$.  Writing $N=n+m$ (intending $n\approx m$), decompose $\mathbf{m}$ into four blocks $\mathbf{a}_{n \times n}$, $\mathbf{b}_{n \times m}$, $\mathbf{c}_{m\times n}$, and $\mathbf{d}_{m \times m}$, and also decompose $\mathbf{u}$ into its first $n$ components $\mathbf{e}_n$ and its last $m$ components $\mathbf{f}_m$ while similarly expressing $\mathbf{v}$ as the concatenation of the $n$-vector $\mathbf{x}$ and the $m$-vector $\mathbf{y}$.  The original system is readily seen to be equivalent to the sequence&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  
  <row AnswerCount="2" Body="&lt;p&gt;Let's say I have some data, i.e. a collection of real numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any good general methods to determine whether this data conforms to a given probability distribution, e.g. the normal distribution, the log-normal distribution, or any other distribution? If so, what are some of the best methods out there?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-10-29T18:58:53.533" Id="17720" LastActivityDate="2012-03-19T17:54:39.037" LastEditDate="2011-10-29T21:38:55.153" LastEditorUserId="919" OwnerDisplayName="Two Cents" OwnerUserId="7115" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;goodness-of-fit&gt;" Title="Determining how well given real-life data fits to a given probability distribution" ViewCount="416" />
  
  <row AcceptedAnswerId="17726" AnswerCount="1" Body="&lt;p&gt;Andrew Gelman, in the book he wrote with Jennifer Hill, states in Chapter 9, (section 9.3), on page 177:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It is only appropriate to control for pre-treatment predictors, or,  more generally, predictors that would not be affected by the treatment (such as race or age). This point willl be illustrated more concretely in Section 9.7...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;And there (9.7 is entitled &quot;do not control for post treatment variables&quot;) he discusses the problem of measuring mediating variables, rather than the pre-post change problem directly.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is important to state here that I think Gelman/Hill is a brilliant text... And  I'm thoroughly enjoying understanding it. However, this bit piqued my interest, as it  brings to mind Everitt &amp;amp; Pickles's approach to the same problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Everitt is of the opinion that using a change score (Score B - Score A) will tend to bias your findings in favour of the treatment, whereas including baseline scores in the model is more conservative. They back this up with a simulation - it's pretty persuasive.&lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding up to here has been that what you are controlling for is group differences in baseline scores that might cause the apparent treatment effect to be greater than it is, or to exist, when it does not. It is also my understanding that this is because regression to the mean is at work, so that higher baseline scores will be associated with greater decreases and vice versa, independent of the treatment effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Everitt is strenuously against &quot;change scores&quot;, and Gelman seems to be advising against Including baseline scores in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, Gelman demonstrates this over the next 2-3 pages, including pre-test scores as a predictor. He gives the caveat that you then get a range of plausible treatment effects that are conditional on the pre-test score, not a range of treatment effects representing merely uncertainty in the effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;My opinion is that using &quot;change scores&quot; seems not to really be doing anything &lt;em&gt;about&lt;/em&gt; regression to the mean, whereas including the baseline score as a predictor allows baseline group differences to &lt;em&gt;cancel out&lt;/em&gt;, essentially introducing a covariance structure.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm a doctor and I have to make real decisions about which treatments work. So what should I do? Include each person's baseline scores or use &quot;change scores&quot;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-10-29T21:44:46.737" FavoriteCount="4" Id="17724" LastActivityDate="2011-10-30T00:11:14.230" LastEditDate="2011-10-29T22:27:08.993" LastEditorUserId="88" OwnerUserId="6666" PostTypeId="1" Score="7" Tags="&lt;clinical-trials&gt;" Title="How should one control for group and individual differences in pre-treatment scores in a randomised controlled trial?" ViewCount="250" />
  <row Body="&lt;p&gt;If the real numbers add up to 100%, and the difference is just round-off error, then it's okay, but I would include a note in the legend to explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the numbers are not proportions of some whole, then you shouldn't use a pie chart.  (And I'm not alone in thinking that you should find an alternative to a pie chart in any case.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-30T15:14:09.347" Id="17739" LastActivityDate="2011-10-30T17:57:09.457" LastEditDate="2011-10-30T17:57:09.457" LastEditorUserId="919" OwnerUserId="5862" ParentId="17737" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;I think you may be interested in circular displays for tabular data (in your case, a two-way table denoting the co-occurence of every binary features), as proposed through &lt;a href=&quot;http://circos.ca/&quot; rel=&quot;nofollow&quot;&gt;Circos&lt;/a&gt;; see example and on-line demo &lt;a href=&quot;http://mkweb.bcgsc.ca/tableviewer/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Sidenote:&lt;/strong&gt; As an alternative, you can also take a look at &lt;a href=&quot;http://eagereyes.org/parallel-sets&quot; rel=&quot;nofollow&quot;&gt;Parallel Sets&lt;/a&gt; that were developed by Robert Kosara. See also, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Robert Kosara, &lt;a href=&quot;http://kosara.net/papers/2010/Kosara_BeautifulVis_2010.pdf&quot; rel=&quot;nofollow&quot;&gt;Turning a Table into a Tree: Growing Parallel Sets&#10;  into a Purposeful Project&lt;/a&gt;, in Steele, Iliinsky (eds), Beautiful&#10;  Visualization, pp. 193–204, O'Reilly Media, 2010.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2011-10-30T16:39:51.597" Id="17743" LastActivityDate="2011-10-30T16:46:45.050" LastEditDate="2011-10-30T16:46:45.050" LastEditorUserId="930" OwnerUserId="930" ParentId="17734" PostTypeId="2" Score="3" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Each integral is straightforward to perform as an iterated integral; only polynomial integrations are involved.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The desired probability therefore equals $1 - (1/4 + 7/12)$ = $1/6$.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Edit&lt;/h3&gt;&#10;&#10;&lt;p&gt;A cleverer solution (which simplifies the work) derives from the recognition that when $y_j$ have iid Exponential distributions, $1\le j\le n+1$, then (writing $y_1+y_2+\cdots+y_{n+1} = Y\ $), the scaled partial sums &lt;/p&gt;&#10;&#10;&lt;p&gt;$$x_i = \sum_{j=1}^{i}y_j/Y,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$1\le i\le n$, are distributed like the uniform order statistics. Because $Y$ is almost surely positive, it follows easily that for &lt;em&gt;any&lt;/em&gt; $n\ge 3$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  <row AcceptedAnswerId="17748" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/3949226/calculating-pearson-correlation-and-significance-in-python/7939259#7939259&quot;&gt;This stackoverflow post&lt;/a&gt; describes computing a Pearson correlation of [1,2,3] and [1,5,7] in several different ways in Python. The most straightforward implementation from &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Definition&quot; rel=&quot;nofollow&quot;&gt;the definition in Wikipedia&lt;/a&gt; comes up with&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.973328526785&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;while Excel, R, NumPy, an online calculator, and a different Python implementation (involving what looks like a more numerically unstable calculation to me) come up with&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.981980506&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am just curious to know what you think.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-10-30T18:45:03.343" Id="17745" LastActivityDate="2011-10-30T20:16:28.813" OwnerUserId="2849" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="What is the correlation of [1,2,3] and [1,5,7] to 8 decimal digits?" ViewCount="154" />
  
  <row Body="&lt;p&gt;Yes, that is absolutely fine, and to be expected.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, when testing the correlation between vectors of length two, the correlation can only take values of 0, 1, or -1  (and will only have value of 0 when one of the vectors has no variance).&lt;/p&gt;&#10;&#10;&lt;p&gt;Try a few simulations to see this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; set.seed(44)&#10;&amp;gt; cor(rnorm(2), rnorm(2))&#10;[1] -1&#10;&amp;gt; cor(rnorm(2), rnorm(2))&#10;[1] 1&#10;&amp;gt; cor(rnorm(2), rnorm(2))&#10;[1] -1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A straight line can perfectly fit any two points in the plane: the points thus exhibit a perfect linear dependence --- the definition of a Pearson correlation coefficient of 1 (or -1). Furthermore, it's not too unlikely that three random points on a plane will fall approximately along a line, resulting in a large correlation, but a million points (or even 10 or 20) rarely will. The expected absolute correlation coefficient for random points thus declines with increasing sample size.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-10-30T20:47:58.353" Id="17758" LastActivityDate="2011-10-30T21:18:49.703" OwnerDisplayName="Josh O'Brien" OwnerUserId="7135" ParentId="17757" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As already noted, minor rounding errors can lead to values that are slightly greater than 100%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Have you considered a bar chart instead of a pie chart? In my experience, bar charts make it easier for the reader to compare the sizes of your categories. This is especially so if you rank them in descending order. Alternately, you might also want to place similar categories adjacent to each other if those categories will make intuitive sense to your readers. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-10-31T19:44:17.060" Id="17771" LastActivityDate="2011-10-31T19:44:17.060" OwnerUserId="7139" ParentId="17737" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;We recently studied a model with a likelihood function of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(d_i|\omega,t_i)=(1-d_i)+(2d_i-1)\cos^2(\omega t_i),$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $d_i\in\{0,1\}$, $\omega$ is an estimation parameter, and $t_i$ are determined by the experimenter.  This is a set of independent but non-identically distributed Bernoulli trials (think: trying to estimate the time-varying bias of a coin).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this was a simple one estimation parameter, one design parameter model, we could essentially employ a first principles approach to the theory and computations (the paper is here: &lt;a href=&quot;http://arxiv.org/abs/1110.3067&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/abs/1110.3067&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now that we plan to generalize this to multiple estimation and design parameters, I really want to make sure we are doing the most efficient thing possible.  I did a bit of searching but couldn't find anything in the literature on statistical models that depend on a periodically varying design parameter $t$.  Has anyone seen a model like this studied before?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-11-01T00:04:56.763" Id="17782" LastActivityDate="2011-11-01T15:26:47.200" LastEditDate="2011-11-01T15:26:47.200" LastEditorUserId="919" OwnerUserId="7146" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;estimation&gt;&lt;experiment-design&gt;" Title="Estimating parameters in a model with a periodic design parameter" ViewCount="130" />
  <row Body="&lt;p&gt;This can be attacked in a number of ways, including fairly economical approaches via the &lt;a href=&quot;http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&quot;&gt;Karush&amp;ndash;Kuhn&amp;ndash;Tucker conditions&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is a quite elementary alternative argument.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The least squares solution for an orthogonal design&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $X$ is composed of orthogonal columns. Then, the least-squares solution is &#10;$$
&#10;\newcommand{\bls}{\hat{\beta}^{{\small \text{LS}}}}\newcommand{\blasso}{\hat{\beta}^{{\text{lasso}}}} \bls = (X^T X)^{-1} X^T y = X^T y \&amp;gt;.
&#10;\blasso_i = \mathrm{sgn}(\bls_i)(|\bls_i| - \gamma)^+ \&amp;gt;.
  <row AcceptedAnswerId="17797" AnswerCount="2" Body="&lt;p&gt;&lt;em&gt;(Apologies for the ASCII tables, Stackexchange doesn't allow HTML tables and since I'm not supposed to link to an image, this is the only way I know of showing the data)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm learning about ANOVA F-testing and stumbled upon this problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;There exists the following set of data regarding four teaching methods and the scores of students who were subject to each teaching method:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;Method 1  Method 2  Method 3  Method 4&#10;----------------------------------------&#10;  65        75        59        94&#10;  87        69        78        89&#10;  73        83        78        80&#10;  79        81        62        88&#10;  81        72        83&#10;  69        79        76&#10;            90&#10;------------------------------------&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;$\bar{x} = 75.67$&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;$78.43$&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $70.83$&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $87.75$&lt;br&gt;&#10;$s = 8.17$&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;$7.11$&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $9.58$&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $5.80$&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\bar{x}$ is the mean of all scores for each teaching method and $s$ is the standard deviation for each method.&lt;/p&gt;&#10;&#10;&lt;p&gt;After conducting an ANOVA, I derive the following ANOVA table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;Source     | Deg. Freedom |    SS     |    MS     |   F  |&#10;-------------------------------------------------------------&#10;Treatment  |     3        | 712.59    |  237.53   | 3.77 |&#10;Error      |     19       | 1196.63   |  62.98    |  -   |&#10;Total      |     22       | 1909.22   |    -      |  -   |&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now the question is: Test a level $\alpha = 0.05$ The null hypothesis is that there is no difference in mean achievement for the four teaching techniques.&lt;/p&gt;&#10;&#10;&lt;p&gt;So to restate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0$: Teaching technique &lt;em&gt;does not&lt;/em&gt; have an influence on mean achievement of students &lt;br&gt;&#10;$H_1$: Teaching technique &lt;em&gt;does&lt;/em&gt; have an influence&lt;/p&gt;&#10;&#10;&lt;p&gt;I work out the critical f value to be $f_{3,19;0.95}$ from an F-distribution table to be $3.1274$&lt;/p&gt;&#10;&#10;&lt;p&gt;The next step is what I don't understand.&lt;/p&gt;&#10;&#10;&lt;p&gt;We can claim that the teaching technique does have an influence on the mean achievement of the students (with less than 5% chance of being wrong)&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The associated p-value is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p = P(X&amp;gt;3.77) = 0.0281$&lt;/p&gt;&#10;&#10;&lt;p&gt;and indeed, p &amp;lt; $0.05$ (hence reject of $H_0$)&lt;/p&gt;&#10;&#10;&lt;p&gt;But now where is this $0.0281$ from? It looks to be the probability that X &gt; 3.77. If I'm not wrong, in this case $X ~ F_{3,19} = 3.1274$ (as calculated before) and so the p value should be the probability that 3.1274 is greater than 3.77. Now how can 3.1274 ever be greater than 3.77? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-11-01T02:42:37.010" Id="17788" LastActivityDate="2011-11-01T10:43:02.257" LastEditDate="2011-11-01T08:49:12.103" LastEditorUserId="930" OwnerDisplayName="Arvin" OwnerUserId="7151" PostTypeId="1" Score="5" Tags="&lt;probability&gt;&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;self-study&gt;&lt;p-value&gt;" Title="How to find P(X &gt; F) in ANOVA F-test?" ViewCount="14640" />
  
  <row Body="&lt;p&gt;I'm not sure if this is a full answer to your question, but median absolute deviation (MAD) is often used as a robust alternative to standard deviation (SD). MAD isn't dimension free, but you could divide MAD by the median.  &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-01T11:15:48.213" Id="17799" LastActivityDate="2011-11-01T11:15:48.213" OwnerUserId="686" ParentId="17792" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'm not sure if I understand your question correctly, but it sounds somewhat self-contradictory. Dimensionless or independent of units means to me that $f(a \cdot x_1, \dotsc, a \cdot x_n) = f(x_1, \dotsc, x_n)$ for (most?) nonzero $a$, but a measure of variation would for me mean that you want $f(a \cdot x_1, \dotsc, a \cdot x_n) = a \cdot f(x_1, \dotsc, x_n)$. And you can't have both.&lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds like you would like to know something like, what fraction of your measurements is far away from the mean, expressed in standard deviations. This sounds more like &lt;a href=&quot;http://en.wikipedia.org/wiki/Kurtosis&quot; rel=&quot;nofollow&quot;&gt;kurtosis&lt;/a&gt; than variation. So are we looking for a robust version of kurtosis?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edited to add: if we are indeed looking for that, you could check to see if the &lt;a href=&quot;http://en.wikipedia.org/wiki/L-moment&quot; rel=&quot;nofollow&quot;&gt;L-kurtosis&lt;/a&gt; satisfies your needs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-01T15:40:36.667" Id="17812" LastActivityDate="2011-11-01T18:20:38.823" LastEditDate="2011-11-01T18:20:38.823" LastEditorUserId="2898" OwnerUserId="2898" ParentId="17792" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;By default, the documentation indicates that &lt;code&gt;rlm&lt;/code&gt; uses &lt;code&gt;psi=psi.huber&lt;/code&gt; weights. Thus, if you want to use Tukey's bisquare, you need to specify &lt;code&gt;psi=psi.bisquare&lt;/code&gt;. The default settings are &lt;code&gt;psi.bisquare(u, c = 4.685, deriv = 0)&lt;/code&gt;, which you can change as desired. For instance, possibly something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rlm(x ~ y, method=&quot;MM&quot;, psi=psi.bisquare, maxit=50)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You may also want to investigate whether you should use least-trimmed squares (&lt;code&gt;init=&quot;lts&quot;&lt;/code&gt;) to initialize your starting values. The default is to use least squares.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-01T15:53:23.627" Id="17813" LastActivityDate="2011-11-01T16:34:23.933" LastEditDate="2011-11-01T16:34:23.933" LastEditorUserId="3265" OwnerUserId="3265" ParentId="17811" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="17817" AnswerCount="2" Body="&lt;p&gt;I was wondering if it is possible to use the caret package with non numerical data.&#10;I know, for example, if I want to use a simple linear regression &lt;code&gt;lm&lt;/code&gt; I could have a factor variable for classification.&#10;However, caret blows up if I attempt this. I'm also following the step outlined here &lt;a href=&quot;http://www.r-project.org/conferences/useR-2010/slides/Kuhn.pdf&quot; rel=&quot;nofollow&quot;&gt;The caret Package: A Unifed Interface for Predictive&#10;Models&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;for illustration I'm attempting to run &lt;code&gt;stepDurationlm &amp;lt;- train (x= trainDescr, y=trainClass, method=&quot;lm&quot;)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;on&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;str(trainDescr)&#10;'data.frame':   589235 obs. of  2 variables:&#10; $ Anon.Student.Id    : Factor w/ 574 levels &amp;quot;02i5jCrfQK&amp;quot;,&amp;quot;02ZjVTxC34&amp;quot;,..: 7 7 7 7 7 7 7 7 7 7 ...
  <row Body="&lt;p&gt;First, you are not seeing genuine zeros in expression data.  Your biologist is saying that, like all biologists do, but when a biologist says &quot;it's zero&quot; it actually means &quot;it's below my detection threshold, so it doesn't exist.&quot;  It's a language issue due to the lack of mathematical sophistication in the field.  I speak from personal experience here.&lt;/p&gt;&#10;&#10;&lt;p&gt;The explanation of the zero inflated Gamma in the link you provide is excellent.  The physical process leading to your data is, if I understand it, a donor is selected, then treated with a certain peptide, and the response is measured from that donor's cells.  There are a couple layers here.  One is the overall strength of the donor's response, which feeds into the expression level of each particular cell being measured.  If you interpret your Bernoulli variable in the zero inflated Gamma as &quot;donor's response is strong enough to measure&quot;, then it might be fine.  Just note that in that case you're lumping the noise of the individual cell's expression with the variation between strongly responding donors.  Since the noise in expression in a single cell is roughly gamma distributed, that may end up causing too much dispersion in your distribution -- something to check for.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the additional variation from donors vs cells doesn't screw up your Gamma fit, and you're just trying to get expression vs applied peptide, then there's no reason why this shouldn't be alright.&lt;/p&gt;&#10;&#10;&lt;p&gt;If more detailed analysis is in order, then I would recommend constructing a custom hierarchical model to match the process leading to your measurements.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-01T19:31:25.007" Id="17822" LastActivityDate="2011-11-01T19:31:25.007" OwnerUserId="873" ParentId="17523" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;In the context of model selection, hypothesis tests can be used either for the (a) selection of the model, or (b) the verification of selected predictors. &#10;Case (a) includes forward/backward/step-wise procedures, which are very common. These procedures however, do not try to account for the massive amount of hypotheses being tested along the way, so they do not really provide the type I error rate you might expect. A &quot;model growth&quot; approach that does try to control for the massive amount of hypotheses being tested, can be found (for instance) in  &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aoas/1239888367&quot; rel=&quot;nofollow&quot;&gt;Benjamini, Gavrilov- A simple forward selection procedure based on false discovery rate control&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case (b), you have already selected a subset of predictors and then wish to verify if they are indeed significant. Note however, that these procedures, answer different questions; It is possible that you will get a smaller prediction error by omitting a significant predictor. It is also possible that adding a non significant predictor will improve your generalization error (if you have multicollinearity for instance). For these reasons, if you are only interested in prediction (and not in &quot;explanation&quot;), there is indeed no need for hypothesis testing.  To the best of my knowledge (I might easily be wrong), properly controlling the type I error, after variable screening using cross-validation is an open question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Concluding- if you decided you do need hypothesis testing for the purpose of interpretation, consider doing it from the start and not after a cross-validation screening. Otherwise, there are many heuristics, but no guarantee on the type I error rate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-01T21:53:54.113" Id="17826" LastActivityDate="2011-11-09T09:43:01.063" LastEditDate="2011-11-09T09:43:01.063" LastEditorUserId="6961" OwnerUserId="6961" ParentId="17825" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="17863" AnswerCount="3" Body="&lt;p&gt;Let's say I want to implement an algorithm based on a paper or book and publish it under a non-proprietary but not necessarily non-commercial-friendly license (e.g. on a blog).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it legal to do this? &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that a general all covering answer cannot be either yes or no, so I want to know additionally how to find out quickly whether this is legal or not. Some examples would be great. Here are some examples for discussion&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Apache Commons Math stated in their &lt;a href=&quot;http://commons.apache.org/math/developers.html&quot; rel=&quot;nofollow&quot;&gt;developer guide&lt;/a&gt;, that all developers should check for license issues before committing and link to &lt;a href=&quot;http://www.nr.com/&quot; rel=&quot;nofollow&quot;&gt;Numerical Recipes&lt;/a&gt; as an example. This is the first time ever  I saw such a warning. I do not have access to this book (neither online nor in dead-tree-format). What does the legal warning looks like ?&lt;/li&gt;&#10;&lt;li&gt;Papers linked for download on the page of the author normally do not contain a legal warning. Does it mean that the algorithms are ... free?&lt;/li&gt;&#10;&lt;li&gt;According to wikipedia, Random Forest is a trademarked term. Does that mean, that noone is allowed to implement this algorithm ? One can give it another name (like Good-Luck-Forests), since reproducing the exact algorithm given that not all details are published in paper (normally) is nearly impossible. &lt;/li&gt;&#10;&lt;li&gt;What about papers where the access is both restricted and not. See for example the paper &lt;em&gt;&quot;Alternatives to the Median Absolute Deviation&quot;&lt;/em&gt;, which has been published in the Journal of the American Statistical Association, but which can be &lt;a href=&quot;http://www.jstor.org/pss/2291267&quot; rel=&quot;nofollow&quot;&gt;bought now via JSTOR&lt;/a&gt; or be downloaded from &lt;a href=&quot;http://web.ipac.caltech.edu/staff/fmasci/home/statistics_refs/BetterThanMAD.pdf&quot; rel=&quot;nofollow&quot;&gt;this page&lt;/a&gt;. Did Frank Masci, the uploader of this paper, break the law?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A lot of papers with or without restricted access can be found online. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; No answer should be treated as legal advice one can refer to before court.&lt;/p&gt;&#10;" ClosedDate="2013-06-20T14:09:08.397" CommentCount="11" CreationDate="2011-11-02T13:24:54.390" FavoriteCount="1" Id="17846" LastActivityDate="2011-11-08T21:14:16.997" LastEditDate="2011-11-02T21:33:17.407" LastEditorUserId="4376" OwnerUserId="264" PostTypeId="1" Score="5" Tags="&lt;untagged&gt;" Title="Is it legal to publish the code of a published algorithm?" ViewCount="561" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you could do an ordinary &lt;a href=&quot;http://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma&quot;&gt;Neyman-Pearson&lt;/a&gt; likelihood ratio test of $H_{0}$ versus $H_{1}$.  Note that $H_{1}$ is &lt;a href=&quot;http://en.wikipedia.org/wiki/Cauchy_distribution&quot;&gt;Cauchy&lt;/a&gt; (infinite variance) and $H_{0}$ is the usual &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution&quot;&gt;Student's&lt;/a&gt; $t$ with 3 degrees of freedom (finite variance) which has PDF:&#10;$$
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, again, we get a simple random sample, calculate $\Lambda(\mathbf{x})$, and reject $H_{0}$ if $\Lambda(\mathbf{x})$ is too big.  How big?  That's the fun part!  It's going to be hard (impossible?) to get a closed form for the critical value, but we could approximate it as close as we like, for sure.  Here's one way to do it, with R.  Suppose $\alpha = 0.05$, and for laughs, let's say $n = 13$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We generate a bunch of samples under $H_{0}$, calculate $\Lambda$ for each sample, and then find the 95th quantile.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;x &amp;lt;- matrix(rt(1000000*13, df = 3), ncol = 13)&#10;y &amp;lt;- apply(x, 1, function(z) prod((1 + z^2/3)^2)/prod(1 + z^2))&#10;quantile(y, probs = 0.95)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This turns out to be (after some seconds) on my machine to be $\approx 12.8842$, which after multiplied by $(\sqrt{3}/2)^{13}$ is $k \approx 1.9859$.  Surely there are other, better, ways to approximate this, but we're just playing around.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In summary,&lt;/strong&gt; when the problem is parametrizable you can set up a hypothesis test just like you would in other problems, and it's pretty straightforward, except in this case for some tap dancing near the end.  Note that we know from our theory the test above is a &lt;em&gt;most powerful test&lt;/em&gt; of $H_{0}$ versus $H_{1}$ (at level $\alpha$), so it doesn't get any better than this (as measured by power).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Disclaimers:&lt;/strong&gt; this is a toy example. I do not have any real-world situation in which I was curious to know whether my data came from Cauchy as opposed to Student's t with 3 df. And the original question didn't say anything about parametrized problems, it seemed to be looking for more of a nonparametric approach, which I think was addressed well by the others.  The purpose of this answer is for future readers who stumble across the title of the question and are looking for the classical dusty textbook approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; it might be fun to play a little more with the test for testing $H_{1}:\nu \leq 1$, or something else, but I haven't done that.  My guess is that it'd get pretty ugly pretty fast.  I also thought about testing different types of &lt;a href=&quot;http://en.wikipedia.org/wiki/Stable_distribution&quot;&gt;stable distributions&lt;/a&gt;, but again, it was just a thought.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-02T16:27:38.757" Id="17851" LastActivityDate="2011-11-02T16:27:38.757" OwnerUserId="1108" ParentId="2504" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;One possibility for those in academe is the use of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Institutional_repository&quot; rel=&quot;nofollow&quot;&gt;campus digital repository&lt;/a&gt; often hosted by campus libraries (to me a logical locus for datasets that accompany publications).&lt;/p&gt;&#10;&#10;&lt;p&gt;A popular (free) digital repository is &lt;a href=&quot;http://www.dspace.org/&quot; rel=&quot;nofollow&quot;&gt;DSpace&lt;/a&gt; which, to my understanding, can host data sets. But this is a service that someone in your institution must host. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-02T16:37:50.193" Id="17852" LastActivityDate="2011-11-02T16:37:50.193" OwnerUserId="6204" ParentId="17850" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;What is the R code to estimate the parameter lambda of the Poisson distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# of occurrence 0 1 2 3 4 5 6 7 &#10;occ  &amp;lt;- 0:7 &#10;# of sequence 150, 200, 220, 230, 240, 250, 180, 260&#10;freq &amp;lt;- c(150,200,220,230,240,250,180,260)&#10;mean_per_seq &amp;lt;- occ %*% freq&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is this correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the R code to find 95 % confidence interval of the parameter lambda?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-02T16:52:14.373" FavoriteCount="1" Id="17853" LastActivityDate="2012-01-19T06:04:34.403" LastEditDate="2012-01-18T11:21:37.960" LastEditorUserId="930" OwnerUserId="7186" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;estimation&gt;&lt;poisson&gt;" Title="R code for estimating a Poisson parameter and its CI?" ViewCount="3309" />
  <row Body="&lt;p&gt;You can do a paired t-test. &lt;/p&gt;&#10;&#10;&lt;p&gt;In R, &lt;code&gt;t.test(surfOM,meanOM,paired=TRUE)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That will give you a p-value and a confidence interval for the mean of the differences. These only really makes sense if the lakes are a sample of a larger population of lakes, not if you have data on all the lakes in your population (e.g. all the lakes in some geographical area, or all the lakes of a certain type of interest).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-02T17:37:00.060" Id="17855" LastActivityDate="2011-11-02T17:37:00.060" OwnerUserId="449" ParentId="17854" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;As mentioned by the OP, this is probably not the right place for expert advice on legal issues, but we all have to live with such things as software licenses and try not to get into trouble, so here are a few things that I have learned. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;On the NR homepage you can find the license information and information on &lt;a href=&quot;http://www.nr.com/licenses/redistribute.html&quot; rel=&quot;nofollow&quot;&gt;redistribution&lt;/a&gt;. This is solely a &lt;em&gt;copyright&lt;/em&gt; issue regarding the source code provided in the books. The algorithms themselves are not copyrighted, and to my understanding you can't get into trouble by implementing and sharing an algorithm that happens to be in NR unless your implementation is &lt;em&gt;derived&lt;/em&gt; from the source code distributed with NR. &lt;/li&gt;&#10;&lt;li&gt;Hmm, nothing is free ... There is almost always a copyright holder. If the paper is published, the copyright may be transferred to the journal, but in some cases the author retain the right to distribute the paper via his or her homepage, say. Patent law is a completely different ball game, but I actually don't know of any examples related to statistical and machine learning algorithms where a patent protection of the algorithm was a problem. Hence, to me, the most important aspect regarding algorithms in papers, whether published in journals or on the authors homepage, is not to violate the copyright.&lt;/li&gt;&#10;&lt;li&gt;A trademark is a third thing. To my understanding it protects only the name or symbol. So you just can't implement a program and call it &quot;Random Forests&quot;, but you can implement and distribute the random forest algorithm. &lt;/li&gt;&#10;&lt;li&gt;I have no idea if Frank Masci broke the law, but it is a copyright issue as I see it. Who holds the copyright and which rights did the copyright holder give to others?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;From my point of view, though violations of patent rights might be serious, the more important issue for the average statistician is that of copyright. If you implement an algorithm from a paper from scratch and cite the paper I don't see any obvious problem with distributing the implementation regardless of the media. But you might think about what is the best way for you to use your copyright on the implementation. I don't know anything about how blogs are generally copyrighted, but if the copyright is transferred to the blog owner automatically, it might, in principle, be a bad idea to post hours of valuable implementations on a blog and thereby effectively give up the copyright of the work to somebody else.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you, on the other hand, modify an existing implementation, you could run into problems with the copyright license. If the license for the original implementation is &lt;a href=&quot;http://www.gnu.org/copyleft/gpl.html&quot; rel=&quot;nofollow&quot;&gt;GPL&lt;/a&gt;, the license for the redistribution has to be GPL. Hence, you have to distribute in a way so that the distribution can be under GPL. This works the other way too. If you want to distribute an implementation under GPL but have to rely on a library that is not distributed under the GNU license, then you might not be able to include the library in your distribution $-$ even if the library is open source and &quot;free&quot;. The library might be distributed under a copyright license that is incompatible with GPL.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-02T21:14:47.693" Id="17863" LastActivityDate="2011-11-03T07:33:06.933" LastEditDate="2011-11-03T07:33:06.933" LastEditorUserId="4376" OwnerUserId="4376" ParentId="17846" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You asked &quot;If other methodologies are more appropriate I’m happy to here about them.&quot; A generalized ARIMA can be easily expressed as a lagged auto-regression ADL or PDL . THis model easily adpats to changes in levels, trends , parameters , seasonal pulses , variability. You are assuming a structure rather than allowing the data to suggest the structure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-02T22:19:52.147" Id="17869" LastActivityDate="2011-11-02T22:19:52.147" OwnerUserId="3382" ParentId="17843" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="17913" AnswerCount="1" Body="&lt;p&gt;SPSS Help provides the following help for testing group differences in a correlation, using the GLM approach. See &lt;a href=&quot;http://www-01.ibm.com/support/docview.wss?uid=swg21478950&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It first requires that X and Y are transformed into z scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; When is this transformation necessary?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-03T05:42:01.093" Id="17876" LastActivityDate="2014-10-31T17:56:03.460" LastEditDate="2011-11-03T10:40:47.280" LastEditorDisplayName="user5644" OwnerUserId="6096" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;spss&gt;&lt;generalized-linear-model&gt;" Title="Z score transformation for testing group differences in correlations" ViewCount="1423" />
  <row Body="&lt;p&gt;You could use the following procedure to compare lists of strings:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;First, define a distance metric that measures how close any two strings are. See the wiki for several &lt;a href=&quot;http://en.wikipedia.org/wiki/String_metric&quot;&gt;string metrics&lt;/a&gt; that you can use.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Second, compute the distance of each element of your candidate list with that of the truth list. So, for A you would compute the distance between the first pair &quot;abc&quot; and its corresponding truth value; the second pair &quot;decf&quot; and its corresponding truth value and so on. The result is a distance vector that gives the distance between each string in A and its corresponding true values.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Third, you need some measure to compute which list is closer to the true list. A useful metric by analogy to ordinary least squares would be the sum of squared distances for each vector.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;You then chose the candidate list that has the lowest overall sum from point 3 above as the 'closest' list to your truth value.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-03T15:28:11.337" Id="17891" LastActivityDate="2011-11-03T15:28:11.337" OwnerUserId="7199" ParentId="17882" PostTypeId="2" Score="5" />
  <row AnswerCount="2" Body="&lt;p&gt;I am looking for a freely available, mathematical description of the standard analysis of variance (several factors, one dependent variable). It should be self-contained and be readable by a person without any knowledge of statistics but a good background in mathematics and probability theory.&lt;/p&gt;&#10;&#10;&lt;p&gt;Almost all texts I find are either step-by-step instructions on example data-sets, heuristic outlines written for scientists from applied areas or they use a lot of statistical lingo to explain the concept (with only crippled math).&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, the text would include a derivation of the F-distribution (why does it emerge in this setting?) and the F-test as well as the relation to regression.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-03T19:15:39.773" FavoriteCount="1" Id="17899" LastActivityDate="2011-11-04T21:21:50.857" OwnerUserId="6577" PostTypeId="1" Score="6" Tags="&lt;anova&gt;&lt;books&gt;" Title="Looking for mathematical account of ANOVA" ViewCount="333" />
  <row AcceptedAnswerId="18012" AnswerCount="1" Body="&lt;p&gt;I have some over-dispersed data and am trying to decide which model would best suit the data. The data are usually counts of symptoms or number of correct items on some cognitive tasks. As an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(69)&#10;g1&amp;lt;-rnorm(700,30,9); g2&amp;lt;-rnorm(100,25,7); g3&amp;lt;-rnorm(100,20,5)&#10;gt&amp;lt;-data.frame(score=c(g1, g2, g3), fac1=factor(rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), c(700, 100, 100))), fac2=ordered(rep(c(0,1,2), c(3,13,4))))&#10;gt$score&amp;lt;-with(gt, ifelse(fac2 == 0, score, score-rnorm(1, 0.5, 2)))&#10;gt$score&amp;lt;-with(gt, ifelse(fac2 == 2, score-rnorm(1, 0.5, 2), score))&#10;gt$score&amp;lt;-round(with(gt, ifelse(score&amp;gt;=30, 30, score))) &#10;gt$cov1&amp;lt;-with(gt, score + rnorm(900, sd=40))/40&#10;gt$score.30&amp;lt;-with(gt, 30-score)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The models I'm thinking about using are:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glmnb1&amp;lt;-glm.nb(score.30~cov1 + fac1*fac2, data=gt)    &#10;hur1&amp;lt;-hurdle(score.30~cov1 + fac1*fac2, dist=&quot;negbin&quot;, data=gt)&#10;quasi1&amp;lt;-glm(cbind(score, score.30)~cov1+fac1*fac2, family=&quot;quasibinomial&quot;, data=gt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How to decide between the negative binomial and the quasibinomial?&lt;/li&gt;&#10;&lt;li&gt;In this example, the hurdle model is a better fit compared to the negative binomial. However, if the quasibinomial was better compared to the negative binomial  (hypothetically or otherwise), how do you compare the hurdle and quasibinomial? Is there a hurdle quasibinomial?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-11-04T04:32:21.030" Id="17918" LastActivityDate="2011-11-06T22:45:51.277" OwnerUserId="966" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;negative-binomial&gt;&lt;quasi-binomial&gt;" Title="Quasibinomial vs negative binomial and hurdles" ViewCount="971" />
  <row Body="&lt;p&gt;The idea is that you can compare this &quot;in sample error&quot; for different models.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you fit a number of models (with more or less main effects and/or interactions) that you think might be of interest. Next you calculate the in sample error for each of these (fitted) models, and you find the model with the smallest in sample error: this is supposedly the 'best' model (wrt this criterion).&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, many people use crossvalidation to achieve model comparison instead of this in sample error, but different criteria may give different results (which may or may not be wanted).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-04T11:00:12.050" Id="17925" LastActivityDate="2011-11-04T11:00:12.050" OwnerUserId="4257" ParentId="17923" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can use bootstrap methods to assess whether the group means are different. Bootstrap  methods do not have any specific distributional assumptions and may be an appropriate tool to use.&lt;/p&gt;&#10;&#10;&lt;p&gt;See the wiki for an explanation of &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Case_resampling&quot; rel=&quot;nofollow&quot;&gt;case resampling&lt;/a&gt; which is one of the simplest bootstrap methods. You can extend the idea to the case of multiple groups and estimate p-values.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-04T18:43:47.833" Id="17939" LastActivityDate="2011-11-04T18:43:47.833" OwnerUserId="7199" ParentId="17938" PostTypeId="2" Score="1" />
  
&#10;$$&#10;your data $\bar x$ and a first moment estimator would imply $p=1$...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-05T07:56:15.700" Id="17959" LastActivityDate="2011-11-05T07:56:15.700" OwnerUserId="7224" ParentId="17958" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Just from reading your question, it seems to me you are mixing computational performances and statistical inference. The posterior distribution on the parameters does not have to be symmetric, so this is not an indicator of poor MCMC convergence. My advice is to try your code on simulated data (meaning simulated from the very semiparametric Cox model you are estimating) where you know the values of the parameters, in order to check for convergence: posteriors should cover the true values to some extent. And take comfort in the fact that one hour of simulation at worst wastes one hour of the computer life, not of yours!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edited Answer&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for providing the tables. They show that the spread is much wider for the corresponding coefficient when using the variable history than when using the variable test. However, I cannot tell from those tables whether or not this is due to more uncertainty in the posterior: are both variables normalised in the same way? If they are, then indeed the posterior distribution is less precise about the coefficient. Which does not mean you should opt for the model involving test rather than history. This requires model comparison. (I also find curious that the 2.5% posterior quantiles on all coefficients are the sames for Model 1. And the 97.5% posterior quantiles on all coefficients are the sames for Model 2. This hints at some high correlation between the covariates or even lack of identifiability...)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-05T08:02:51.370" Id="17960" LastActivityDate="2011-11-06T17:46:15.440" LastEditDate="2011-11-06T17:46:15.440" LastEditorUserId="7224" OwnerUserId="7224" ParentId="17946" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to create a predictive model from a set of categorical observations:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;task   user&#10;----   ----&#10;S      Alice&#10;M      Bob&#10;M      Alice&#10;S      Charlie&#10;L      Bob&#10;M      Charlie&#10;S      Alice&#10;...    ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm not interested in predicting what type of task will be next for a given user.  Rather, I'm interested in predicting a user's most likely distribution of tasks.  For example, if a user has a set of 20 tasks, how many of them will be S, M, and L?&lt;/p&gt;&#10;&#10;&lt;p&gt;The actual dataset has several thousand distinct users that can be grouped hierarchically.  Each user belongs to a workgroup, and each workgroup belongs to a department.  It's pretty easy to come up with a contingency table and empirical distribution, and I can see that the distribution varies among departments, and among workgroups within each department.  I believe that variations among individuals within the same workgroup are insignificant, but this is a statement from intuition, and I don't know how to justify it statistically.&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently, my predictive model just calculates the distribution of tasks across the entire dataset and uses the same prior distribution for all users: if S/M/L are 0.2/0.5/0.3, then the prediction for every individual is based on this distribution.  But in practice, this model doesn't work as well as I would like for several workgroups (and one entire department) because their task distribution appears to be different from that of the whole company.  I'm thinking that in these situations, my model needs to use a workgroup-specific or department-specific distribution for the individuals within it.  But I would like to avoid over-fitting the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, a few questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How can I determine if the distribution in a given department or workgroup differs from the total distribution sufficiently to justify using a different set of parameters for that department or workgroup?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Let's say I generate a model in which each department has its own distribution.  How do I determine whether this model is &quot;better&quot; than the model that uses the same distribution for all departments, or whether the department-specific model is overfitted?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Let's say I build a model in which the predictive distribution for an individual is a weighted mixture of the total, the department and the workgroup distributions.  How can I figure out the appropriate weights for this mixture?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My dataset is huge (&gt;1M rows), and I have access to Python+SciPy and R.  My programming knowledge is much better than my statistical knowledge, so if you describe the statistical technique I can figure out how to implement it in code.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-05T13:55:34.157" Id="17964" LastActivityDate="2011-11-05T16:46:43.650" LastEditDate="2011-11-05T14:19:21.040" LastEditorUserId="88" OwnerUserId="7230" PostTypeId="1" Score="2" Tags="&lt;model-selection&gt;&lt;predictive-models&gt;&lt;multinomial&gt;" Title="Selecting a model for predicting a multinomial response from multinomial data" ViewCount="166" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to use multiple class logistic regression to learn the decision boundaries separating the different classes (denoted by color) in the image below. Kernel logistic regression with a RBF kernel seems like a good choice, but I would like the decision boundary, when projected back to the 2-d space, to fall along the white grid lines. One way of proceeding would be to introduce a penalty term in the objective function, but I'm not sure how to proceed. Does anyone have any references for using a grid as a contraint in this kind of problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;A perhaps different way of asking this is how can I combine, on the one hand, a prior belief of the form $\Pr(\int^\mathbf{x}\pi=.5 \mid \mathbf{x})$ about the distribution over $\mathbf{x}$ of when the cumulative probability distribution of $\pi$ takes on a particular value with, on the other hand, a likelihood $\Pr(\mathbf{x} \mid \pi)$? I &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BnFVP.png&quot; alt=&quot;Different classes of points on a grid&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-05T16:22:15.100" FavoriteCount="1" Id="17967" LastActivityDate="2011-11-06T16:25:52.403" LastEditDate="2011-11-06T16:25:52.403" LastEditorUserId="82" OwnerUserId="82" PostTypeId="1" Score="4" Tags="&lt;logistic&gt;&lt;classification&gt;&lt;kernel&gt;" Title="Constrain decision boundary to fall on grid lines in multiple class logistic regression" ViewCount="131" />
  <row Body="&lt;p&gt;It sounds like you're asking that if you're given an arbitrary random vector ${\bf X} = \{ X_{1}, ..., X_{p} \}$ with associated weights ${\bf w} = \{w_{1}, ..., w_{p} \}$, then is the distribution of &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ {\rm composite \ score} = \sum_{k=1}^{p} w_{k} X_{k} $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;normal. The answer is not necessarily. If $p=2$ and $X_{1}, X_{2}$ are independent ${\rm Bernoulli}(1/2)$ then one composite score would be $X_{1}+X_{2}$, which certainly is not normal. &lt;/p&gt;&#10;&#10;&lt;p&gt;If each component of the composite score is normal, then the composite score will certainly be normal, but I don't know of any less general sufficient conditions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-05T20:49:33.497" Id="17977" LastActivityDate="2011-11-05T20:49:33.497" OwnerUserId="4856" ParentId="17976" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;No, this does not produce a reasonable answer. For two reasons: (1) there is no such thing as an &quot;optimal&quot; or a &quot;true&quot; prior. Priors reflect your prior beliefs or your prior information about the problem, not a truth about the parameter $\theta_0$ that is behind your data $x$, (2) your simulation is dependent on the simulation parameter $\theta_0$ which is arbitrary, so your &quot;choice&quot; of prior will also depend on $\theta_0$. In addition, your method ends up picking a prior for frequentist properties, rather than conditioning on the observation $x$, another departure from the Bayesian perspective...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-05T21:17:19.680" Id="17982" LastActivityDate="2011-11-05T21:17:19.680" OwnerUserId="7224" ParentId="17963" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Let's think about regular linear regression, and to make it concrete, let's say we are trying to predict height of people. When you regress heights against just an intercept term and no predictors, the intercept term will be be the height averaged over all the people in your sample. Lets call this term $\beta_0^{\text{no predictor}}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, we want to add a predictor for sex, so we create and indicator variable that takes a 0 when the sampled person is male and 1 when the person is a female. When we regress against this model, we will get an estimates for an intercept term, $\beta_0^{\text{male reference}}$ and coefficent of the sex variable $\beta_1^{\text{male reference}}$. The estimated intercept is no longer the average height of everybody, but the average height of males, the coefficient of the sex variable is the difference in the average height between males and females. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider if we decided to code our indicator variable differently, so that the sex variable took the value 0 if the person was a female and 1 if the person was a male, in this specification of the model we get the estimates of the intercept and coefficient $\beta_0^{\text{female reference}}, \beta_1^{\text{female reference}}$. Now $\beta_0^{\text{female reference}}$, the intercept term, is the average height of females, and the coefficient is the difference in average height between females and males. So&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  <row AnswerCount="1" Body="&lt;p&gt;How do you determine the degrees of freedom for Kuiper's Test of Uniformity for circular data?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been using various R packages for circular data (circular, CircStats) and I can't seem to find out how to get the degrees of freedom for the Kuiper's test. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-06T02:09:31.580" Id="17987" LastActivityDate="2012-07-17T11:40:20.520" OwnerUserId="7240" PostTypeId="1" Score="1" Tags="&lt;degrees-of-freedom&gt;" Title="Degrees of freedom for Kuiper's Test of uniformity" ViewCount="170" />
  
  <row Body="&lt;p&gt;The Chi-Squared test is a straightforward one and it sounds like exactly what you need for your situation, unless there are complications that you haven't described.  You'll find explanations in any introductory statistics text, on wikipedia, and elsewhere.  You may even find an online calculator you can use; it'll need to accomodate the &quot;5x2&quot; format of your data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-06T16:20:19.690" Id="18000" LastActivityDate="2011-11-06T16:20:19.690" OwnerUserId="2669" ParentId="17999" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="18005" AnswerCount="1" Body="&lt;p&gt;I am curious about the practical implementation of a binary split in a decision tree - as it relates to levels of a categorical predictor $X{j}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, I often will utilize some sort of sampling scheme (e.g. bagging, oversampling etc) when building a predictive model using a decision tree - in order to improve its predictive accuracy and stability. During these sampling routines, it is possible for a categorical variable to be presented to a tree fitting algorithm with less than the complete level set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say a variable X takes on levels &lt;code&gt;{A,B,C,D,E}&lt;/code&gt;. In a sample, maybe only levels &lt;code&gt;{A,B,C,D}&lt;/code&gt; are present. Then, when the resulting tree is used for prediction, the full set may be present.&lt;/p&gt;&#10;&#10;&lt;p&gt;Continuing from this example, say a tree splits on X and sends &lt;code&gt;{A,B}&lt;/code&gt; to the left and &lt;code&gt;{C,D}&lt;/code&gt; to the right. I would expect the logic of the binary split to then say, when faced with new data: &quot;If X has value A or B, send to the left, otherwise, send this case to the right&quot;. What seems to happen in some implementations is &quot;if X has value A or B, send to the left, if X has value C or D send to the right&quot;. When this case takes on value E, the algorithm breaks down. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is the &quot;right&quot; way for a binary split to be handled? It seems the much more robust way is implemented often, but not always (see Rpart below).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here are a couple examples:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Rpart fails, the others are ok.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#test trees and missing values&#10;&#10;summary(solder)&#10;table(solder$PadType)&#10;&#10;# create train and validation&#10;set.seed(12345)&#10;t_rows&amp;lt;-sample(1:nrow(solder),size=360, replace=FALSE)&#10;train_solder&amp;lt;-solder[t_rows,]&#10;val_solder&amp;lt;-solder[-t_rows,]&#10;&#10;#look at PadType&#10;table(train_solder$PadType)&#10;table(val_solder$PadType)&#10;#set a bunch to missing&#10;levels(train_solder$PadType)[train_solder$PadType %in% c('L8','L9','W4','W9')] &amp;lt;- 'MISSING'&#10;&#10;&#10;#Fit several trees, may have to play with the parameters to get them to split on the variable&#10;&#10;####RPART&#10;mod_rpart&amp;lt;-rpart(Solder~PadType,data=train_solder)&#10;predict(mod_rpart,val_solder)&#10;#Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = attr(object,  : &#10;#factor 'PadType' has new level(s) D6, L6, L7, L8, L9, W4&#10;&#10;####TREE&#10;mod_tree&amp;lt;-tree(Solder~PadType,data=train_solder,split=&quot;gini&quot;)&#10;predict(mod_tree,val_solder) #works fine&#10;&#10;####ctree&#10;mod_ctree&amp;lt;-ctree(Solder~PadType,data=train_solder,control = ctree_control(mincriterion = 0.05))&#10;predict(mod_ctree,val_solder) #works fine&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-11-06T18:20:26.723" Id="18004" LastActivityDate="2011-11-06T19:33:09.190" OwnerUserId="2040" PostTypeId="1" Score="4" Tags="&lt;cart&gt;&lt;rpart&gt;&lt;partitioning&gt;" Title="Difference in implementation of binary splits in decision trees" ViewCount="939" />
  
  
  
  <row AcceptedAnswerId="18035" AnswerCount="4" Body="&lt;p&gt;When using SVM, we need to select a kernel.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder how to select a kernel. Any criteria on kernel selection?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-07T11:12:21.673" FavoriteCount="18" Id="18030" LastActivityDate="2014-11-22T23:53:26.773" LastEditDate="2012-04-23T01:45:48.913" LastEditorUserId="10322" OwnerUserId="7259" PostTypeId="1" Score="32" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;kernel-trick&gt;" Title="How to select kernel for SVM?" ViewCount="9172" />
  
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;thus the odds ratio between levels $j$ and $k$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;\frac{e^{b_0+b_j}}{e^{b_0+b_k}} = e^{b_j-b_k}
  
  <row AcceptedAnswerId="18057" AnswerCount="1" Body="&lt;p&gt;I am taking random samples from one distribution, $f(x)$, but trying to get information about another distribution, $g(x)$.  I have a weighting function, $w(x)=Cg(x)/f(x)$, to correct for this. The result is that I have $N$ independent samples, with different weights, $w_i$, attached to them.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is then, what is a good estimate for the number of independent samples I really have.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if my weights are {0.49, 0.48, 0.01, 0.01, 0.01} then I have pretty close to 2 independent samples.  If they are {0.3, 0.3, 0.4} then I have about 3.  Presumably there is a quantitative way to do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, how could I determine, given $f(x)$ and $w(x)$, what the efficiency of sampling is (i.e. How many independent samples of $g(x)$ do I, on average, get for $N$ samples of $f(x)$)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-07T19:10:19.507" FavoriteCount="1" Id="18056" LastActivityDate="2011-11-07T19:30:01.337" OwnerUserId="7268" PostTypeId="1" Score="5" Tags="&lt;independence&gt;&lt;weighted-sampling&gt;" Title="Number of independent samples for weighted samples?" ViewCount="88" />
&#10;\hat N_\text{ess} = 1 \big/ \sum_{j=1}^N \bar w_j^2\,.
&#10;$$&#10;Third, I do not understand your simulation experiment: if you generate from the distribution of $Y_1$ conditional on $X$, this means you generate from a &#10;$$
&#10;$$&#10;distribution (assuming $\mu_X=\mu_Y=0$). What am I missing?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-07T21:25:04.027" Id="18061" LastActivityDate="2011-11-07T21:25:04.027" OwnerUserId="7224" ParentId="18053" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;I &lt;em&gt;am&lt;/em&gt; answering my own question, but I thought It'd be great for the people coming across this post to check out some of the explanations &lt;a href=&quot;http://askville.amazon.com/covariance-apply-statistics/AnswerViewer.do?requestId=6460933&quot; rel=&quot;nofollow&quot;&gt;on this page&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm paraphrasing one of the very well articulated answers (by a user'Zhop'). I'm doing so in case if that site shuts down or the page gets taken down when someone eons from now accesses this post ;)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Covariance is a measure of how much two variables change together.&#10;  Compare this to Variance, which is just the range over which one&#10;  measure (or variable) varies.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;In studying social patterns, you might hypothesize that wealthier&#10;  people are likely to be more educated, so you'd try to see how closely&#10;  measures of wealth and education stay together.  You would use a&#10;  measure of covariance to determine this.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;...&lt;/p&gt;&#10;  &#10;  &lt;p&gt;I'm not sure what you mean when you ask how does it apply to&#10;  statistics. It is one measure taught in many stats classes.  Did you&#10;  mean, when should you use it?&lt;/p&gt;&#10;  &#10;  &lt;p&gt;You use it when you want to see how much two or more variables change&#10;  in relation to each other.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Think of people on a team.  Look at how they vary in geographic&#10;  location compared to each other.  When the team is playing or&#10;  practicing, the distance between individual members is very small and&#10;  we would say they are in the same location.  And when their location&#10;  changes, it changes for all individuals together (say, travelling on a&#10;  bus to a game). In this situation, we would say they have a high level&#10;  of covariance.  But when they aren't playing, then the covariance rate&#10;  is likely to be pretty low, because they are all going to different&#10;  places at different rates of speed.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;So you can predict one team member's location, based on another team&#10;  member's location when they are practicing or playing a game with a&#10;  high degree of accuracy.  The covariance measurement would be close to&#10;  1, I believe.  But when they are not practicing or playing, you would&#10;  have a much smaller chance of predicting one person's location, based&#10;  on a team member's location.   It would be close to zero, probably,&#10;  although not zero, since sometimes team members will be friends, and&#10;  might go places together on their own time.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;However, if you randomly selected individuals in the United States,&#10;  and tried to use one of them to predict the other's locations, you'd&#10;  probably find the covariance was zero.  In other words, there is&#10;  absolutely no relation between one randomly selected person's location&#10;  in the US, and another's.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Adding another one (by 'CatofGrey') that helps augment the intuition:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In probability theory and statistics, covariance is the measure of how&#10;  much two random variables vary together (as distinct from variance,&#10;  which measures how much a single variable varies).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;If two variables tend to vary together (that is, when one of them is&#10;  above its expected value, then the other variable tends to be above&#10;  its expected value too), then the covariance between the two variables&#10;  will be positive. On the other hand, if one of them is above its&#10;  expected value and the other variable tends to be below its expected&#10;  value, then the covariance between the two variables will be negative.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;These two together have made me understand covariance as I've never understood it before! Simply amazing!!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-08T02:23:19.667" Id="18068" LastActivityDate="2011-11-08T02:23:19.667" OwnerUserId="4426" ParentId="18058" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Based on real data (e.g. spot and futures prices of an index) if two series are correlated in the long run (e.g. strong positive significant correlation) it does not mean that they are cointegrated.&lt;/p&gt;&#10;&#10;&lt;p&gt;What if two series are cointegrated: can we infer that they are also correlated in the long run? Can we find a case with real data that two series are cointegrated but they are not correlated in the long run?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-08T13:16:33.443" FavoriteCount="1" Id="18076" LastActivityDate="2011-11-14T14:11:11.913" LastEditDate="2011-11-12T14:23:08.070" LastEditorUserId="2116" OwnerUserId="6281" PostTypeId="1" Score="2" Tags="&lt;econometrics&gt;&lt;cointegration&gt;" Title="Cointegration and correlation" ViewCount="392" />
  
  <row Body="&lt;p&gt;The problem with covariances is that they are hard to compare: when you calculate the covariance of a set of heights and weights, as expressed in (respectively) meters and kilograms, you will get a different covariance from when you do it in other units (which already gives a problem for people doing the same thing with or without the metric system!), but also, it will be hard to tell if (e.g.) height and weight 'covariate better' than, e.g. the length of your toes and fingers, simply because the 'scale' you calculate the covariance on is different.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution to this is to 'normalize' the covariance: you divide the covariance by something that represents the diversity and scale in both the covariates, and end up with a value that is assured to be between -1 and 1: the correlation. Whatever unit your original variables were in, you will always get the same result, and this will also ensure that you can, to a certain degree, compare whether two variables 'correlate' more than two others, simply by comparing their correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: the above starts from the idea that the listener has already been explained what a covariance is.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-08T19:20:52.617" Id="18089" LastActivityDate="2011-11-08T19:20:52.617" OwnerUserId="4257" ParentId="18082" PostTypeId="2" Score="17" />
  <row Body="&lt;p&gt;This sounds like another strident paper by a confused individual.  Fisher didn't fall into any such trap, though many students of statistics do.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hypothesis testing is a decision theoretic problem.  Generally, you end up with a test with a given threshold between the two decisions (hypothesis true or hypothesis false).  If you have a hypothesis which corresponds to a single point, such as $\theta=0$, then you can calculate the probability of your data resulting when it's true.  But what do you do if it's not a single point?  You get a function of $\theta$.  The hypothesis $\theta\not= 0$ is such a hypothesis, and you get such a function for the probability of producing your observed data given that it's true.  That function is the power function.  It's very classical.  Fisher knew all about it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The expected loss is a part of the basic machinery of decision theory.  You have various states of nature, and various possible data resulting from them, and some possible decisions you can make, and you want to find a good function from data to decision.  How do you define good?  Given a particular state of nature underlying the data you have obtained, and the decision made by that procedure, what is your expected loss?  This is most simply understood in business problems (if I do this based on the sales I observed in the past three quarters, what is the expected monetary loss?).&lt;/p&gt;&#10;&#10;&lt;p&gt;Bayesian procedures are a subset of decision theoretic procedures.  The expected loss is insufficient to specify uniquely best procedures in all but trivial cases.  If one procedure is better than another in both state A and B, obviously you'll prefer it, but if one is better in state A and one is better in state B, which do you choose?  This is where ancillary ideas like Bayes procedures, minimaxity, and unbiasedness enter.&lt;/p&gt;&#10;&#10;&lt;p&gt;The t-test is actually a perfectly good solution to a decision theoretic problem.  The question is how you choose the cutoff on the $t$ you calculate.  A given value of $t$ corresponds to a given value of $\alpha$, the probability of type I error, and to a given set of powers $\beta$, depending on the size of the underlying parameter you are estimating.  Is it an approximation to use a point null hypothesis?  Yes.  Is it usually a problem in practice?  No, just like using Bernoulli's approximate theory for beam deflection is usually just fine in structural engineering.  Is having the $p$-value useless?  No.  Another person looking at your data may want to use a different $\alpha$ than you, and the $p$-value accommodates that use.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm also a little confused on why he names Student and Jeffreys together, considering that Fisher was responsible for the wide dissemination of Student's work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, the blind use of p-values is a bad idea, and they are a rather subtle concept, but that doesn't make them useless.  Should we object to their misuse by researchers with poor mathematical backgrounds?  Absolutely, but let's remember what it looked like &lt;em&gt;before&lt;/em&gt; Fisher tried to distill something down for the man in the field to use.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-08T21:08:44.803" Id="18095" LastActivityDate="2011-11-08T21:24:21.340" LastEditDate="2011-11-08T21:24:21.340" LastEditorUserId="919" OwnerUserId="873" ParentId="17897" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;I am familiar with using regression with ARIMA errors to model interrupted time-series, in order to estimate the change in magnitude caused by a policy intervention. These models seem to be designed for a single time series, and thus if multiple time-series are analysed a model must be fit separately for each time-series.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in analysing the national impact of a policy intervention, the implementation of which was staggered in time across all (eight Australian) states. I can see three possible analysis approaches here:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fit a separate ARIMA model for each state&lt;/li&gt;&#10;&lt;li&gt;Attempt to fit an aggregate national model, perhaps with one dummy variable indicating partial implementation and another indicating complete implementation&lt;/li&gt;&#10;&lt;li&gt;Find a different model that works explictly on panel data. This would hopefully bring some kind of compromise between the no-pooling approach 1 and the complete pooling approach 2.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What sort of approach would you recommend here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-08T23:17:42.957" Id="18105" LastActivityDate="2011-11-20T22:40:12.337" OwnerUserId="179" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;panel-data&gt;&lt;intervention-analysis&gt;" Title="Interrupted time-series analysis for panel data" ViewCount="516" />
  
  <row Body="&lt;p&gt;Concordance is defined by checking the mean score, not the maximum score.&lt;/p&gt;&#10;&#10;&lt;p&gt;So for your examples, mean scores for 1 is 0*55.1% + 1*35.8% + 2*9.1% = 0.54, and 2 is (through similar calculations) 0.826.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is this value that you should compare to get the concordance or any other association statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ref -&#10;&lt;a href=&quot;http://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_logistic_sect042.htm&quot; rel=&quot;nofollow&quot;&gt;http://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_logistic_sect042.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-09T06:38:04.127" Id="18122" LastActivityDate="2011-11-09T06:38:04.127" OwnerUserId="994" ParentId="18119" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I just read &lt;a href=&quot;http://quanttrader.info/public/findingSeasonalSpreads.html&quot; rel=&quot;nofollow&quot;&gt;Finding Seasonal Spreads&lt;/a&gt;, by Paul Teetor.&#10;As far as I understand how ANOVA works, it should be accurate for NORMAL distributed series. Financial series are &lt;em&gt;not&lt;/em&gt; normally distributed, or maybe am I wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone explain why he used ANOVA in this case?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-11-09T08:02:49.797" Id="18123" LastActivityDate="2012-01-08T12:06:37.033" LastEditDate="2012-01-08T12:06:37.033" LastEditorUserId="930" OwnerUserId="5405" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;anova&gt;" Title="Is ANOVA good for non-normally distributed series?" ViewCount="474" />
  <row Body="&lt;p&gt;Two variables that would have a high positive covariance (correlation) would be the number of people in a room, and the number of fingers that are in the room.  (As the number of people increases, we expect the number of fingers to increase as well.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;Something that might have a negative covariance (correlation) would be a person's age, and the number of hair follicles on their head. Or, the number of zits on a person's face (in a certain age group), and how many dates they have in a week.  We expect people with more years to have less hair, and people with more acne to have less dates.. These are negatively correlated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-09T09:28:56.813" Id="18127" LastActivityDate="2011-11-09T09:28:56.813" OwnerUserId="6369" ParentId="18058" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://snap.stanford.edu/data/&quot; rel=&quot;nofollow&quot;&gt;Stanford Large Network Dataset Collection&lt;/a&gt;. Which is part of the Stanford Network Analysis Project, SNAP.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-09T09:55:34.043" Id="18128" LastActivityDate="2011-11-13T13:25:46.887" LastEditDate="2011-11-13T13:25:46.887" LastEditorUserId="1036" OwnerUserId="6961" ParentId="7412" PostTypeId="2" Score="0" />
  
&#10;$$&#10;Hence the conditional distribution of $X$ given $Z$ is a binomial distribution with parameters $n$ and $λ_1/(λ_1+λ_2)$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-09T13:47:52.480" Id="18140" LastActivityDate="2011-11-09T17:50:24.780" LastEditDate="2011-11-09T17:50:24.780" LastEditorUserId="7234" OwnerUserId="7234" ParentId="17966" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Fligner-Killeen's and Levene's tests are two ways to test the ANOVA assumption of &quot;equal variances in the population&quot; before conducting the ANOVA test.  Levene's is widely used and is typically the default in programs like SPSS, but either test (or even Brown-Forsythe) is acceptable.  ANOVA is the omnibus test of mean differences among groups. While, in name, ANOVA analyzes the variance (between, within, and overall) among three or more groups, its hypotheses actually make statements about the equality of means versus there being &quot;at least two means different.&quot;  &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-09T14:08:55.290" Id="18142" LastActivityDate="2011-11-09T14:08:55.290" OwnerUserId="7289" ParentId="18137" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Regression sounds like a good starting place. You could also look at non-parametric methods like &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping&quot; rel=&quot;nofollow&quot;&gt;bootstrapping&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;It sounds like you might also be interested in data mining techniques if you're working with this kind of data. For example, if you have information about which products were bought together by the same customer, you can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Apriori_algorithm&quot; rel=&quot;nofollow&quot;&gt;&quot;apriori association mining&quot;&lt;/a&gt; algorithm to find patterns. This method is often used to select the placement of pairs of items in stores, with a classic example being to put the diapers next to the beer (when new fathers are sent shopping alone, turns out they like to get a treat for themselves). &lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-11-09T15:11:56.883" Id="18147" LastActivityDate="2011-11-09T15:11:56.883" OwnerUserId="6446" ParentId="18125" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Some comments first:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You need a testable hypothesis before getting p-values. That means that you need to describe what the data would look like without and with the effect of interest. You can do parameter estimation without hypothesis testing, but then you need to specify what parameter is of interest, and there are no p-values.&lt;/li&gt;&#10;&lt;li&gt;While your values are counts, they represent the number of reads falling into each category with a fixed total per experimental run. Or more precisely, the total is random, but you want to condition on it (you keep saying things like &quot;out of a sample of &lt;code&gt;b&lt;/code&gt; events&quot;). If so, your counts do not have a negative binomial distribution, but rather a binomial distribution. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The following assumes such a conditioning on the total number of reads per experimental run. The right analysis approach still depends on whether your goal is to estimate hte proportion of exon-skipping reads for each junction (with a confidence interval) or do you want to be able to estimate the entire distribution of the number of exon-skipping reads given the total number of reads. You seem to be asking for the latter, but I am not sure whether that's what you really need, as it is not clear what are you planning to do with the results of the analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;You have very little information to inform the model, so you have to make major assumptions. Based on a preliminary analysis using quasi-binomial regression it appears that there might be overdispersion: the probability of an exon-skipping read varies somewhat between the replicates. If that's true, binomial regression cannot be used, but you could consider beta-binomial regression. Here I will show the binomial regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, you have to set up the data so that counts from the same replicate are in the same row.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;d1 &amp;lt;- data.frame(Rep=1:3, Skip=c(8,0,0), Normal=c(12,6,8))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And then we can use a binomial glm:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- glm(cbind(Skip,Normal) ~ 1, data=d1, family=binomial)&#10;summary(m1)&#10;&#10;&#10;&#10;Call:&#10;glm(formula = cbind(Skip, Normal) ~ 1, family = binomial, data = d1)&#10;&#10;Deviance Residuals: &#10;     1       2       3  &#10; 1.634  -1.794  -2.072  &#10;&#10;Coefficients:&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)   &#10;(Intercept)  -1.1787     0.4043  -2.915  0.00355 **&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 10.18  on 2  degrees of freedom&#10;Residual deviance: 10.18  on 2  degrees of freedom&#10;AIC: 15.613&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the p-value shown is probably meaningless to you. It tests whether the parameter is 0, and that corresponds to testing whether the probability of exon-skipping is 0.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;predict&lt;/code&gt; function can be used to get the predicted probability of exon-skipping for each replicate, and the &lt;code&gt;dbinom&lt;/code&gt; function can can be used to get the individual response probabilities:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p1 &amp;lt;- predict(m1, type=&quot;response&quot;)&#10;&amp;gt; p1&#10;        1         2         3 &#10;0.2352941 0.2352941 0.2352941 &#10;&amp;gt; newtotal &amp;lt;- 10&#10;&amp;gt; dbinom(0:newtotal, size=newtotal, p=p1[1])&#10; [1] 6.838240e-02 2.104074e-01 2.913333e-01 2.390427e-01 1.287153e-01 4.752565e-02&#10; [7] 1.218606e-02 2.142605e-03 2.472236e-04 1.690418e-05 5.201286e-07&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, for example, the probability of having 0 exon-skipping reads out of 10 reads at junciton 1 is estimated to be 0.06828.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-09T21:57:35.453" Id="18159" LastActivityDate="2011-11-09T21:57:35.453" OwnerUserId="279" ParentId="18131" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You need to check for correlations amongst your dependent variables (&lt;em&gt;edit: @BilalBarakat's answer is right, the residuals are what's important here&lt;/em&gt;).  If all or some are independent, you can run separate analyses on each.  If they are not independent, or whichever ones aren't, you could run a multivariate analysis.  This will maximize your power while holding the type I error rate at your alpha level.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You should know, however, that this will not make your analysis more accurate/robust.  This is a different issue than simply whether or not your model predicts the data better than the null model.  In fact, with so much going on, unless you have a lot of data, it is likely that you could get very different parameter estimates with a new sample.  It is even possible that the sign on a beta will flip.  Much depends on the size of p and q and the nature of their correlation matrices, but the volume of data required for robustness can be massive.  Remember that, although many people use 'significant' and 'reliable' as synonyms, they actually aren't.  It is one thing to know that a variable is not independent of another variable, but another thing entirely to specify the nature of that relationship in your sample as it is in the population.  It can be easy to run a study twice and find a predictor significant both times, but with the parameter estimate sufficiently different to be theoretically meaningful.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, unless you are doing structural equation modeling, you can't very well incorporate your theoretical knowledge regarding the variables.  That is, techniques like MANOVA tend to be rawly empirical.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach is to utilize what you know about the issue at hand.  For example, if you have several different measures of the same construct (you could check this with a factor analysis), you can combine them.  This can be done by turning them into z scores and averaging them.  Knowledge of other sources of correlation (e.g., common cause or mediation) could also be utilized.  Some people are uncomfortable with putting so much weight on domain knowledge, and I acknowledge that this is a philosophical issue, but I think it can be a mistake to require the analyses to do all of the work and assume that this is the best answer.  &lt;/p&gt;&#10;&#10;&lt;p&gt;As for a reference, any good multivariate textbook should discuss these issues.  Tabachnick and Fidell is well regarded.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-10T04:08:06.260" Id="18169" LastActivityDate="2013-05-21T21:54:30.477" LastEditDate="2013-05-21T21:54:30.477" LastEditorUserId="7290" OwnerUserId="7290" ParentId="18151" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I think your calculated standard deviation for the proportions is $0.0064$ (note extra 0).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Yes,  to calculate the expected standard deviation, you are probably expected to use a binomial model and the formula $\sqrt{\dfrac{p(1-p)}{n}}.$ You can take the mean of your sample data to give you an estimate of $p$, though taking $0.5$ will give much the same result in practice. $n$ comes from &quot;out of an average of 3900 births per month&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-10T10:09:59.493" Id="18186" LastActivityDate="2011-11-10T10:09:59.493" OwnerUserId="2958" ParentId="18185" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;To say something about the math of it rather than the programming: it would be useful to think about whether you can actually avoid the repeated simulations altogether in this case by using elementary properties of the Gaussian normal distribution. In this toy example it doesn't matter, but in general it's vastly more efficient to exploit an analytic solution if it exists. In this case here, the code for the solution becomes much shorter than even chl's example.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hint 1:&#10;What is the distribution of the average of 100 normal random variables with a mean of 69.5 and SD 2.9?&lt;/p&gt;&#10;&#10;&lt;p&gt;Hint 2:&#10;Apply hint 1 to the average of 1000 instantiations of random variable X - Y, for which the closed form is also elementary (if X and Y are normal).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-10T13:39:23.757" Id="18192" LastActivityDate="2011-11-10T13:39:23.757" OwnerUserId="7099" ParentId="18187" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Sometimes we can &quot;augment knowledge&quot; with an unusual or different approach.  I would like this reply to be accessible to kindergartners and also have some fun, so &lt;strong&gt;everybody get out your crayons!&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Given paired $(x,y)$ data, draw their scatterplot.  (The younger students may need a teacher to produce this for them. :-)  Each pair of points $(x_i,y_i)$, $(x_j,y_j)$ in that plot determines a rectangle: it's the smallest rectangle, whose sides are parallel to the axes, containing those points.  Thus the points are either at the upper right and lower left corners (a &quot;positive&quot; relationship) or they are at the upper left and lower right corners (a &quot;negative&quot; relationship).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Draw all possible such rectangles.&lt;/strong&gt; Color them transparently, making the positive rectangles red (say) and the negative rectangles &quot;anti-red&quot; (blue).  In this fashion, wherever rectangles overlap, their colors are either enhanced when they are the same (blue and blue or red and red) or cancel out when they are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XPGjN.png&quot; alt=&quot;Positive and negative rectangles&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(&lt;em&gt;In this illustration of a positive (red) and negative (blue) rectangle, the overlap ought to be white; unfortunately, this software does not have a true &quot;anti-red&quot; color.   The overlap is gray, so it will darken the plot, but on the whole the&lt;/em&gt; net &lt;em&gt;amount of red is correct.&lt;/em&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we're ready for the explanation of covariance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The covariance is the net amount of red in the plot&lt;/strong&gt; (treating blue as negative values).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some examples with 32 binormal points drawn from distributions with the given covariances, ordered from most negative (bluest) to most positive (reddest).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mlcoy.png&quot; alt=&quot;Covariance plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;(The original version of this post has led to the creation of a simplified graphical rendition of the underlying idea. It is accompanied by an admirably clear, step-by-step explanation.  Please check it out at &lt;a href=&quot;http://creatorguides.com/guides/covariance/&quot;&gt;http://creatorguides.com/guides/covariance/&lt;/a&gt;.  For additional explanation also see the answer posted here by &lt;a href=&quot;http://stats.stackexchange.com/a/111296&quot;&gt;arthur.00&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Let's deduce some properties of covariance.&lt;/strong&gt;  Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles. :-)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bilinearity.&lt;/strong&gt; Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Correlation.&lt;/strong&gt; Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Relationship to linear associations.&lt;/strong&gt; Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sensitivity to outliers.&lt;/strong&gt; A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Incidentally, this definition of covariance differs from the usual one only by a universal constant of proportionality (independent of the data set size).  The mathematically inclined will have no trouble performing the algebraic demonstration of the equivalence.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2011-11-10T17:14:03.910" Id="18200" LastActivityDate="2014-08-11T02:21:33.163" LastEditDate="2014-08-11T02:21:33.163" LastEditorUserId="919" OwnerUserId="919" ParentId="18058" PostTypeId="2" Score="97" />
  <row AcceptedAnswerId="18229" AnswerCount="1" Body="&lt;p&gt;H0 is commonly understood to signify the absence of a treatment effect or difference between two groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;Doesn't this understanding ignore the fact that sample data (being a sample) can never fully accurately reflect the nature of the phenomena being studied? &lt;/p&gt;&#10;&#10;&lt;p&gt;Doesn't the existence of things like Type II errors require acceptance that statistics don't always reflect reality?&lt;/p&gt;&#10;&#10;&lt;p&gt;In short: Can H0 be considered a satement about numbers and data, not necessarily about the real world?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-10T18:53:56.477" Id="18205" LastActivityDate="2011-11-11T06:46:34.570" LastEditDate="2011-11-11T00:08:58.973" LastEditorUserId="7316" OwnerUserId="7316" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;" Title="Is it accurate to say that all the Null Hypothesis states is the absence of a significant difference between sets of data?" ViewCount="172" />
  
  <row Body="&lt;p&gt;Variable selection is necessarily because most models don't deal well with a large number of irrelevant variables.  These variables will only introduce noise into your model, or worse, cause you to over-fit. It's a good idea to exclude these variables from analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, you can't include all the variables that exist in every analysis, because there's an infinite number of them out there.  At some point you have to draw the line, and it's good to do so in a rigorous manner.  Hence all the discussion on variable selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the issues with variables selection can be dealt with by cross-validation, or by using a model with built-in penalization and feature selection (such as the elastic net for linear models).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're interested in some empirical results related to multiple variables causing over-fitting, check out the results of the &lt;a href=&quot;http://www.kaggle.com/c/overfitting&quot;&gt;Don't Overfit&lt;/a&gt; competition on Kaggle.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-10T22:29:04.620" Id="18218" LastActivityDate="2011-11-10T22:29:04.620" OwnerUserId="2817" ParentId="18214" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;The $df$ should be taken as approaching zero. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Differential_%28infinitesimal%29&quot; rel=&quot;nofollow&quot;&gt;differential (infinitesimal)&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-11T00:40:39.817" Id="18223" LastActivityDate="2011-11-11T00:40:39.817" OwnerUserId="1569" ParentId="18222" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Classification accuracy (error rate) is an improper scoring rule (optimized by a bogus model), arbitrary, discontinuous, and easy to manipulate.  It's not needed in this context.&lt;/p&gt;&#10;&#10;&lt;p&gt;You didn't state how many predictors there were.  Instead of assessing model fit I would be tempted to just make the model fit.  A compromise approach is to assume that interactions aren't important and to allow continuous predictors to be nonlinear using regression splines.  Plot the estimated relationships.  The &lt;code&gt;rms&lt;/code&gt; package in R makes all this relatively easy.  See &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/rms&quot;&gt;http://biostat.mc.vanderbilt.edu/rms&lt;/a&gt; for more information.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might elaborate on &quot;pairs&quot; and whether your observations are independent.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-11T18:06:34.170" Id="18253" LastActivityDate="2011-11-11T18:06:34.170" OwnerUserId="4253" ParentId="18248" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;My approach is to follow theory to develop composite scales. I would make the choices listed below for the stated reasons:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Sum the individual components if the individual components are part of a whole.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider measuring intelligence using measures for verbal and quantitative intelligence. The overall measure of intelligence should be the sum of the verbal and quantitative components as the individual components are part of a whole.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Average the individual components if they measure the same underlying construct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose, that we measure verbal intelligence by a battery of 10 questions. Each question measures the same aspect of intelligence (namely our ability to parse language). Thus, averaging these scores will give us a better measure of verbal intelligence as random errors in the measurements tend to even out when you average the scores.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Multiply them if the individual components have synergistic effect on the underlying construct.&lt;/p&gt;&#10;&#10;&lt;p&gt;If for some reason you believe that verbal intelligence and quantitative intelligence have synergistic effects then I would multiply them. By synergistic effects I mean that a person who is verbally intelligent can leverage his language abilities to go even further in quantitative fields thus enhancing their quantitative intelligence.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-11-11T19:06:27.957" Id="18258" LastActivityDate="2011-11-11T19:06:27.957" OwnerUserId="7199" ParentId="18249" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here's some example code that might prove helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using the fit command from Curve Fitting Toolbox to perform the regression, but you could use regress, or regstats, or even backslash for the regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The display is using the &quot;Dataset Array&quot; from Statistics Toolbox.  This is a data container designed to store heterogeneous data.  In this case, I'm using it to store a combination of cell strings and doubles.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;clear all&#10;clc&#10;&#10;% Create a dataset array&#10;Coeffs = {'B0'; 'B1'; 'B2'};&#10;foo = dataset(Coeffs);&#10;&#10;% Generate a dataset&#10;X = 1:10;&#10;X = X';&#10;Y = 5*X.^2 + 3*X + 1;&#10;&#10;% Add some noise&#10;Noisy = Y + randn(10,1);&#10;&#10;% Generate a fit&#10;bar = fit(X,Noisy, 'poly2')&#10;&#10;Model_One = [bar.p3;bar.p1; bar.p1];&#10;foo.Model_One = Model_One&#10;&#10;% Repeat&#10;Noisy = Y + randn(10,1);&#10;bar = fit(X,Noisy,'poly2');&#10;&#10;Model_Two = [bar.p3;bar.p1; bar.p1];&#10;foo.Model_Two = Model_Two;&#10;&#10;disp(foo)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-11-11T20:03:01.467" Id="18261" LastActivityDate="2011-11-11T20:03:01.467" OwnerUserId="7336" ParentId="6883" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Not exactly the most accessible explanation can be found &lt;a href=&quot;http://en.wikipedia.org/wiki/Mixture_model&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, but I'm looking for something more intuitive, examples of applications and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Help is much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-11T20:42:04.397" Id="18263" LastActivityDate="2011-11-11T23:06:59.683" LastEditDate="2011-11-11T22:46:45.340" LastEditorUserId="5594" OwnerUserId="333" PostTypeId="1" Score="2" Tags="&lt;conditional-probability&gt;&lt;intuition&gt;&lt;probability&gt;" Title="Plain English explanation of Bernoulli mixture models?" ViewCount="178" />
  <row Body="&lt;p&gt;I believe that the answer is that glmnet automatically fits an intercept term.  So my betas as reported by glmnet are actually the &quot;true&quot; betas + intercept.  This is rather unfortunate, because my model is conceptually clearer without an intercept term.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-11T20:45:42.757" Id="18264" LastActivityDate="2011-11-11T20:45:42.757" OwnerUserId="5902" ParentId="18166" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;It sounds like you're carrying out a cluster analysis on a dataset that has a pretty large number of variables; having difficulty obtaining good results because of the large number of variables (the curse of dimensionality, as you mention); and you're considering using an optimization technique such as simulated annealing to carry out a search through your variables to discover whether you might be able to use just a subset - is that right?&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, that activity is typically called feature selection (sometimes feature extraction), and there's plenty of literature out there that describes how you might approach it. Feature selection involves selecting a subset of the original variables, and is not quite the same as dimensionality reduction, which typically involves creating a small number of linear combinations of the original variables that summarise them (this is what a technique such as PCA or SVD does).&lt;/p&gt;&#10;&#10;&lt;p&gt;A suggestion I might give is to note that you're trying to search through what is a discrete space (the power set of your variables). Simulated annealing, as an optimization technique, is in my experience more easily applied to searching through continuous spaces. This is particularly true of the &lt;a href=&quot;http://www.mathworks.com/help/toolbox/gads/simulannealbnd.html&quot;&gt;implementation&lt;/a&gt; in MATLAB &lt;a href=&quot;http://www.mathworks.com/products/global-optimization/&quot;&gt;Global Optimization Toolbox&lt;/a&gt; (since I note you added the MATLAB tag). If you're using MATLAB for this, I'd suggest that a genetic algorithm might be easier to adapt to searching through discrete spaces.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wrote an &lt;a href=&quot;http://www.mathworks.com/company/newsletters/digest/2005/nov/genalgo.html&quot;&gt;article&lt;/a&gt; for MATLAB Digest a while ago that applies genetic algorithms to a related problem (classification rather than cluster analysis), which comes with example code. You might find that it's possible to adapt that code to your needs. The article carries out feature selection on a classification problem though, so it's maximizing classification accuracy - you'd need to provide a clustering metric for the algorithm to optimize, such as separation, heterogeneity, or a gap statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that helps!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-07T12:33:41.253" Id="18272" LastActivityDate="2011-11-07T12:33:41.253" OwnerDisplayName="Sam Roberts" OwnerUserId="6976" ParentId="18271" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I've struggled with this question for several months and found this website, which is amazing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine a hypothetical league composed of players competing in a simple contest where the object of each contest is to score more than an opponent. Player strategies are simple, existing on a range from focusing on scoring, to focusing on preventing the opponent from scoring, to anywhere in between.  Assume that players stick with a certain strategy over time. For each player in each contest, we then know the number of points scored, and the number of points allowed.&lt;/p&gt;&#10;&#10;&lt;p&gt;The challenge is to evaluate how a player performed in a certain contest. Obviously the number of points scored by the player will not be enough since it does not speak to the strategy of the opponent (for instance, an opponent who has a strategy focused on scoring will allow more points to every player).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have thought up an approach, and am looking for feedback from everyone here, especially if this approach has a name or (as usually happens) has been otherwise thought up. It has three inputs:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The number of points the player scored in the contest&lt;/li&gt;&#10;&lt;li&gt;The average number of points all players scored in all the league’s contests&lt;/li&gt;&#10;&lt;li&gt;The average number of points the opponent allowed in all his previous&#10;contests, but not this one.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The simple formula looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;AdjPointsScored = NumPointsScored – (MeanOppPointsAllowedToDate – MeanLeaguePtsScored)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That basically expresses the player’s performance as +/-  what a typical player playing against this opponent might be expected to score.&lt;/p&gt;&#10;&#10;&lt;p&gt;I’m also wondering if it might be more accurate to express NumPointsScored and MeanOppPointsAllowedToDate as Z-scores based on the distribution of MeanLeaguePtsScored, subtract the z-scores, and then turn the result back into units of MeanLeaguePtsScored, but I’m unsure if that will improve the analysis or just needlessly complicate things.&lt;/p&gt;&#10;&#10;&lt;p&gt;Much Ablidged.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-12T00:37:13.890" FavoriteCount="1" Id="18274" LastActivityDate="2011-11-12T00:37:13.890" OwnerUserId="7308" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;normalization&gt;&lt;games&gt;" Title="Adjusting performance by opponent skill level in a simple contest" ViewCount="56" />
  <row AcceptedAnswerId="18292" AnswerCount="2" Body="&lt;p&gt;Using a two stage RJMCMC process I obtain a vector of log posteriors pertaining to each model.  My goal is to convert these to probabilities after normalization (to use in the second stage of the RJMCMC process).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is the log posteriors are large (ex. 1100).  To convert to probabilities (not on the log scale) I would like to do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;exp(log.post)/sum(exp(log.post))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However this is not possible due to the large values of the log posterior.  Any suggestions on how I can convert?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like the probabilities on the original scale, not on the log scale.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-12T05:46:28.290" Id="18279" LastActivityDate="2011-11-13T11:45:42.353" LastEditDate="2011-11-12T12:04:40.660" LastEditorUserId="88" OwnerUserId="2310" PostTypeId="1" Score="5" Tags="&lt;model-selection&gt;&lt;posterior&gt;" Title="Transform log posteriors to the original posteriors" ViewCount="138" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have data from an experiment where students were tested on multiple words for the correct pronunciation of a specific linguistic phenomena.&lt;/p&gt;&#10;&#10;&lt;p&gt;The experiment was done with a control and experimental group, and all students were tested prior to a specific instruction and at the&#10;end of the experiments (roughly 8 weeks later).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; head(act1)&#10;  studentid groupid itemid test type result&#10;1        1B       B      1    0    1      0&#10;2        5B       B      1    0    1      1&#10;3        6B       B      1    0    1      1&#10;4        8B       B      1    0    1      0&#10;5       11B       B      1    0    1      1&#10;6       15B       B      1    0    1      1&#10;&#10;&amp;gt; levels(act1$groupid)&#10;[1] &quot;B&quot; &quot;D&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;D is the experimental group while B is the control group.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, I want to test the hypothesis that the experimental group has had a significant improvement in the production of the studied linguistic phenomena.&lt;/p&gt;&#10;&#10;&lt;p&gt;I ran McNemar's test on each group individually:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; act1wide &amp;lt;- reshape(act1, idvar=c(&quot;studentid&quot;,&quot;groupid&quot;,&quot;itemid&quot;,&quot;type&quot;), timevar=c(&quot;test&quot;), v.names=c(&quot;result&quot;), direction=&quot;wide&quot;)&#10;&amp;gt; act1wideb &amp;lt;- subset(act1wide, groupid=='B')&#10;&amp;gt; act1wided &amp;lt;- subset(act1wide, groupid=='D')&#10;&amp;gt; mcnemar.test(act1wideb$result.0, act1wideb$result.1)&#10;&#10;    McNemar's Chi-squared test with continuity correction&#10;&#10;data:  act1wideb$result.0 and act1wideb$result.1 &#10;McNemar's chi-squared = 0.0556, df = 1, p-value = 0.8137&#10;&#10;&amp;gt; mcnemar.test(act1wided$result.0, act1wided$result.1)&#10;&#10;    McNemar's Chi-squared test with continuity correction&#10;&#10;data:  act1wided$result.0 and act1wided$result.1 &#10;McNemar's chi-squared = 9.0312, df = 1, p-value = 0.002654&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This seems to show that group 'B' did not improve while group 'D' did.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is this a valid test?&lt;/li&gt;&#10;&lt;li&gt;How can I have a more meaningful test that would compare group data at the same time?&lt;/li&gt;&#10;&lt;li&gt;The factor &lt;em&gt;type&lt;/em&gt; has three levels. How can I break down the analysis to get specific information by type?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I am a bit of a stats noob so I greatly appreciate detailed explanations on how to make sense of my data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-12T20:53:06.830" Id="18299" LastActivityDate="2011-11-12T22:28:54.573" LastEditDate="2011-11-12T22:28:54.573" LastEditorUserId="930" OwnerUserId="7341" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;repeated-measures&gt;" Title="Controlled experiment with binary outcome variable" ViewCount="151" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The answer below responded to the original version of the OP's question, which was quite different in nature and less specific than the current version.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$p(x∣w_1)$ and $p(x∣w2)$ are equal.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;OK, this is going to take a lot longer to answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some statistical applications, a statistician (or a machine, since&#10;you included machine learning as a tag) needs to decide which of two&#10;hypotheses is true:  $H_1 \colon w = w_1$ and $H_2 \colon w = w_2$.&#10;It is known that $$P(w = w_1) = P(w = w_2) = \frac{1}{2}.$$&#10;&lt;em&gt;This&lt;/em&gt; is what the &lt;em&gt;equal a priori probabilities&lt;/em&gt; that you keep referring &#10;to means.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a simple method:  Always &lt;em&gt;decide&lt;/em&gt; that $w = w_1$, and so hypothesis&#10;$H_1$ is always the true hypothesis.  When &lt;em&gt;in fact&lt;/em&gt; $H_1$ is true, your&#10;decision is&#10;perfectly correct; when &lt;em&gt;in fact&lt;/em&gt; $H_2$ is true your decision is&#10;perfectly wrong, and&#10;thus you have a $50\%$ chance of making an error.  More sophisticated methods&#10;use a coin toss or a call to a random number generator to decide, but unfortunately still have a $50\%$ chance of making an an error; the same&#10;as the simpler mulish insistence that $H_1$ is always true.&lt;/p&gt;&#10;&#10;&lt;p&gt;To get better performance, i.e., smaller error probability), the &#10;statistician might observe a random variable whose distribution depends&#10;on the value of $w$.  If $w = w_1$, the distribution is $p(x\mid w_1)$;&#10;if $w = w_2$, the distribution is $p(x\mid w_2)$.  For example, if&#10;$w = w_1$, $x$ is a normal random variable with mean $100$ and variance $1$,&#10;while if $w = w_2$, $x$ is a standard normal random variable with mean $0$ and &#10;variance $1$.  So if the statistician observes that $x$ has value $101.2$,&#10;it is highly likely that $w = w_1$ and thus very likely that&#10;$H_1$ is true because a standard&#10;normal random variable is quite unlikely to have large value.  On the other&#10;hand, if $x$ has small value (say between $-4$ and $+4$), then it is quite&#10;likely that $H_2$ is true and $w = w_2$.  But notice that all this depends&#10;critically on the distributions $p(x\mid w_1)$ and $p(x\mid w_2)$ being&#10;&lt;em&gt;different&lt;/em&gt;.  If the distributions are the same, then observing $x$ is of &#10;no help in deciding between $H_1$ and $H_2$.  Thus when you claim that&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$p(x∣w_1)$ and $p(x∣w_2)$ are equal&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;you are effectively insisting that observing $x$ is useless as &#10;far as deciding between $H_1$ and $H_2$ is concerned.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, how are these distributions known in the first place?  The client might provide&#10;them to the statistician based on the knowledge of how the client's&#10;apparatus works.  Your professor, like Professor Indiana Jones in the&#10;movie &lt;em&gt;Raiders of the Lost Ark&lt;/em&gt;, might&#10;be making them up as he goes along  (Remember that $99\frac{44}{100}\%$&#10;of all statistics are made up!).  In the context of machine &#10;learning, there may be &lt;em&gt;training samples&lt;/em&gt; provided:  Here are&#10;$200$ observations of $x$ when $H_1$ is true, and here are&#10;$200$ more when $H_2$ is true.  (In your particular problem,&#10;$x$ is a &lt;em&gt;bivariate&lt;/em&gt; normal random variable with independent&#10;(standard normal) components when $H_1$ is true and correlated &#10;normal components when&#10;$H_2$ is true, and so each sample would be a a pair of numbers).&#10;The machine &lt;em&gt;estimates&lt;/em&gt;&#10;$p(x\mid w_1)$ from the first set of observations&#10;and $p(x\mid w_2)$ from the second set, and uses these&#10;estimates when making decisions when the real work comes&#10;along.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, your claim that $p(x\mid w_1) = p(x\mid w_2)$&#10;means that $x$ is totally useless in distinguishing the two&#10;cases.  For your &lt;em&gt;particular&lt;/em&gt; distribution, equality holds&#10;(if you nevertheless contiunue to insist on equality)&#10;exactly when $a=b=1$ and $c=d=e=0$ (in which case $ab-c^2 = 1$&#10;as desired).  There is no way of &lt;em&gt;solving&lt;/em&gt; for $a,b,c,d,e$,&#10;or saying  what values of $a,b,c,d,e$ make sense in your &#10;problem based on the information that you have provided.&#10;You need to be given these by your professor,&#10;or you need to be given training data so that you can &lt;em&gt;estimate&lt;/em&gt;&#10;these parameters, or you should emulate Professor Jones&#10;and make up some numbers (subject to the constraints that $ab - c^2 = 1$&#10;and $a, b &gt; 0$) and solve the problem using these.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-11-13T02:04:37.833" Id="18304" LastActivityDate="2011-11-13T16:15:47.743" LastEditDate="2011-11-13T16:15:47.743" LastEditorUserId="2970" OwnerUserId="6633" ParentId="18300" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Retailer Y uses (for client operation purposes) a network of about 3000 devices. The whole network has 10 different models (from distinct manufacturers), aged from 0 years old &#10;(purchased in the current year) to 12 years old.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each device may suffer from malfunctions for which immediate repair is requested. The total time of unavailability for each device is accounted at the end of month and we call it downtime.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same device may have no malfunction during that month period or may have one or more unavailability periods during that month period.&lt;/p&gt;&#10;&#10;&lt;p&gt;Common sense says that older equipment should have higher downtimes.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The Question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(a)  How can I check whether the Age variable has effective impact over the &#10;Downtime variable? Which statistical model should I use?&lt;/p&gt;&#10;&#10;&lt;p&gt;(b)  How can I account for the fact that each type of equipment should behave&#10;differently to aging regarding downtime?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;Eduardo&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-13T14:49:44.987" Id="18313" LastActivityDate="2011-11-14T23:44:18.677" LastEditDate="2011-11-14T23:44:18.677" LastEditorUserId="7347" OwnerUserId="7347" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;" Title="Device age vs. downtime relationship" ViewCount="111" />
  
  <row Body="&lt;p&gt;It appears that you want to model 1 of them given the other 3. This is called a Transfer Function and also sometimes an ARMAX model. You will be interested in capturing not only contemporaneous effects but lag effects.The unexplained component ( the current error term) might be further partitioned into some autoregressive structure (ARIMA) and/or sOme deterministic structure (  Pulses, Level Shifts , Seasonal Pulses, Local Time Trends. I am not an R expert but I don't believe that functionality currently exists. You might want to use the internet &lt;a href=&quot;http://www.google.com/search?sourceid=navclient&amp;amp;ie=UTF-8&amp;amp;rlz=1T4SUNA_enUS287US288&amp;amp;q=multivariate+box-jenkins&quot; rel=&quot;nofollow&quot;&gt;http://www.google.com/search?sourceid=navclient&amp;amp;ie=UTF-8&amp;amp;rlz=1T4SUNA_enUS287US288&amp;amp;q=multivariate+box-jenkins&lt;/a&gt; and to search for &quot;MULTIVARIATE BOX-JENKINS&quot;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-13T15:06:44.590" Id="18315" LastActivityDate="2011-11-13T15:19:19.527" LastEditDate="2011-11-13T15:19:19.527" LastEditorUserId="3382" OwnerUserId="3382" ParentId="18314" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Most of the answers given so far refer to &quot;Supervised Learning&quot; (i.e. where you have labels for a portion of your dataset, that you can use to train algorithms). The question specifically mentioned clustering, which is an &quot;Unsupervised&quot; approach (i.e. no labels are known beforehand). In this scenario I'd suggest looking at:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;k-means and kernel k-means&lt;/li&gt;&#10;&lt;li&gt;Agglomerative Clustering&lt;/li&gt;&#10;&lt;li&gt;Non-negative Matrix Factorisation&lt;/li&gt;&#10;&lt;li&gt;Latent Dirichlet Allocation&lt;/li&gt;&#10;&lt;li&gt;Dirichlet Processes and Hierarchical Dirichlet Processes&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;But actually you'll probably find that your similarity/distance measure is more important than the specific algorithm you use.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have some labelled data, then &quot;Semi-Supervised Learning&quot; approaches are gaining popularity and can be very powerful. A good starting point for SSL is the LapSVM (Laplacian Support Vector Machine).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-14T10:49:21.433" Id="18338" LastActivityDate="2011-11-14T10:49:21.433" OwnerUserId="7365" ParentId="17227" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="18340" AnswerCount="1" Body="&lt;p&gt;How could I use GARCH model to detect if the volatility is constant during all the series(time series)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't do a visual check, I need to detect if the volatility is &lt;strong&gt;constant&lt;/strong&gt; using R and GARCH function of &lt;code&gt;tseries&lt;/code&gt; package&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-14T11:25:40.000" Id="18339" LastActivityDate="2011-11-14T15:21:17.670" LastEditDate="2011-11-14T11:47:30.800" LastEditorUserId="2116" OwnerUserId="5405" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;garch&gt;" Title="How to analyze the volatility with GARCH?" ViewCount="786" />
&#10;(\hat{\beta}_j-\beta_j)(v_j)^{-1}(\hat{\beta}_j-\beta_j)\sim \sigma\chi_1^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that those $\beta_j$ that satisfy the condition&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(\frac{\hat{\beta}_j-\beta_j}{\sqrt{v_j}}\right)^2\le \sigma^2\chi_{1,1-\alpha}^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;fall in the confidence interval described in the equation 3.14. Hence the confidence interval is the set in real line. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now similarly we get&#10;$$(X^TX)^{1/2}(\hat\beta-\beta)\sim N(0, \sigma^2 I),$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;so &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\hat\beta-\beta)X^TX(\hat\beta-\beta)\sim \sigma^2\chi_{p+1}^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p$ is the number of the regressors. Using the same analogy we can look for vector points $\beta\in\mathbb{R}^{p+1}$ which satisfy the condition&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\hat\beta-\beta)X^TX(\hat\beta-\beta)\le \sigma^2\chi_{p+1,1-\alpha}^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For $p=1$ this set will be the interior of the ellipsis. &lt;/p&gt;&#10;&#10;&lt;p&gt;The confidence set is used since it accounts for interactions between $\beta_i$ and $\beta_j$. Look at the scatter plot of two independent normal variables (which would be the case for orthogonal regressors with the same variance):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Fkybf.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The circular shape is evident. Using the univariate confidence intervals the confidence set would be square, and this graph illustrates that it will actualy estimate the confidence incorrectly.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-14T13:44:15.337" Id="18345" LastActivityDate="2011-11-14T13:44:15.337" OwnerUserId="2116" ParentId="18322" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;@Frank Harrell has done a lot of work on this question. I don't know of specific references.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I rather see the two techniques as being for different purposes.  Cross validation is a good tool when deciding on the model -- it helps you avoid fooling yourself into thinking that you have a good model when in fact you are overfitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;When your model is fixed, then using the bootstrap makes more sense (to me at least).&lt;/p&gt;&#10;&#10;&lt;p&gt;There is an introduction to these concepts (plus permutation tests) using R at &lt;a href=&quot;http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html&quot;&gt;http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html&lt;/a&gt;  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-14T15:55:41.853" Id="18351" LastActivityDate="2011-11-14T21:04:53.317" LastEditDate="2011-11-14T21:04:53.317" LastEditorUserId="919" OwnerUserId="6993" ParentId="18348" PostTypeId="2" Score="20" />
  <row AnswerCount="2" Body="&lt;p&gt;Suppose one wants to compute the odds ratio of a disease for a person in group $A$ versus that of a person in group $B$. Suppose we consider the following: (i) include age in the odds ratio calculation, (ii) include age and height in the odds ratio calculation. Is it possible for the odds ratio for (ii) to be much greater than the odds ratio of (i)? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-14T16:10:12.340" Id="18353" LastActivityDate="2011-11-14T17:51:49.757" OwnerUserId="7297" PostTypeId="1" Score="1" Tags="&lt;odds-ratio&gt;" Title="Can odds ratios increase if you include more variables?" ViewCount="231" />
  
  <row Body="&lt;p&gt;It comes down to variance and bias (as usual). CV tends to be less biased but K-fold CV has fairly large variance. On the other hand, bootstrapping tends to drastically reduce the variance but gives more biased results (they tend to be pessimistic). Other bootstrapping methods have been adapted to deal with the bootstrap bias (such as the 632 and 632+ rules).&lt;/p&gt;&#10;&#10;&lt;p&gt;Two other approaches would be &quot;Monte Carlo CV&quot; aka &quot;leave-group-out CV&quot; which does many random splits of the data (sort of like mini-training and test splits). Variance is very low for this method and the bias isn't too bad if the percentage of data in the hold-out is low. Also, repeated CV does K-fold  several times and averages the results similar to regular K-fold. I'm most partial to this since it keeps the low bias and reduces the variance. &lt;/p&gt;&#10;&#10;&lt;h3&gt;Edit&lt;/h3&gt;&#10;&#10;&lt;p&gt;For large sample sizes, the variance issues become less important and the computational part is more of an issues. I still would stick by repeated CV for small and large sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some relevant research is below (esp Kim and Molinaro). &lt;/p&gt;&#10;&#10;&lt;h3&gt;References&lt;/h3&gt;&#10;&#10;&lt;p&gt;Bengio, Y., &amp;amp; Grandvalet, Y. (2005). Bias in estimating the variance of k-fold cross-validation. Statistical modeling and analysis for complex data problems, 75–95.&lt;/p&gt;&#10;&#10;&lt;p&gt;Braga-Neto, U. M. (2004). Is cross-validation valid for small-sample microarray classification Bioinformatics, 20(3), 374–380. doi:10.1093/bioinformatics/btg419&lt;/p&gt;&#10;&#10;&lt;p&gt;Efron, B. (1983). Estimating the error rate of a prediction rule: improvement on cross-validation. Journal of the American Statistical Association, 316–331.&lt;/p&gt;&#10;&#10;&lt;p&gt;Efron, B., &amp;amp; Tibshirani, R. (1997). Improvements on cross-validation: The. 632+ bootstrap method. Journal of the American Statistical Association, 548–560.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furlanello, C., Merler, S., Chemini, C., &amp;amp; Rizzoli, A. (1997). An application of the bootstrap 632+ rule to ecological data. WIRN 97.&lt;/p&gt;&#10;&#10;&lt;p&gt;Jiang, W., &amp;amp; Simon, R. (2007). A comparison of bootstrap methods and an adjusted bootstrap approach for estimating the prediction error in microarray classification. Statistics in &#10;Medicine, 26(29), 5320–5334.&lt;/p&gt;&#10;&#10;&lt;p&gt;Jonathan, P., Krzanowski, W., &amp;amp; McCarthy, W. (2000). On the use of cross-validation to assess performance in multivariate prediction. Statistics and Computing, 10(3), 209–229.&lt;/p&gt;&#10;&#10;&lt;p&gt;Kim, J.-H. (2009). Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap. Computational Statistics and Data Analysis, 53(11), 3735–3745. doi:10.1016/j.csda.2009.04.009&lt;/p&gt;&#10;&#10;&lt;p&gt;Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. International Joint Conference on Artificial Intelligence, 14, 1137–1145.&lt;/p&gt;&#10;&#10;&lt;p&gt;Martin, J., &amp;amp; Hirschberg, D. (1996). Small sample statistics for classification error rates I: Error rate measurements.&lt;/p&gt;&#10;&#10;&lt;p&gt;Molinaro, A. M. (2005). Prediction error estimation: a comparison of resampling methods. Bioinformatics, 21(15), 3301–3307. doi:10.1093/bioinformatics/bti499&lt;/p&gt;&#10;&#10;&lt;p&gt;Sauerbrei, W., &amp;amp; Schumacher1, M. (2000). Bootstrap and Cross-Validation to Assess Complexity of Data-Driven Regression Models. Medical Data Analysis, 26–28.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tibshirani, RJ, &amp;amp; Tibshirani, R. (2009). A bias correction for the minimum error rate in cross-validation. Arxiv preprint arXiv:0908.2904.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-14T16:25:08.960" Id="18355" LastActivityDate="2011-11-14T21:03:52.133" LastEditDate="2011-11-14T21:03:52.133" LastEditorUserId="919" OwnerUserId="3468" ParentId="18348" PostTypeId="2" Score="33" />
  
  <row Body="&lt;p&gt;Yes, it is possible for group (ii) to be greater. (whether it's MUCH greater would depend on your definition)&lt;/p&gt;&#10;&#10;&lt;p&gt;For an intuitive point of view, (i) averages out the effect of height among the people in the groups, but (ii) allows the effect to vary. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are to look at the calculation (taken from wikipedia on logistic regression), the model is $1/(1+e^{-z})$; so as $z$ gets larger, the odds would increase as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-14T16:37:17.777" Id="18356" LastActivityDate="2011-11-14T16:37:17.777" OwnerUserId="6845" ParentId="18353" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider survey data from surgeries. $Y$ represents observed surgical &#10;quality and is measured post-surgery; $X$ represents perceived surgical &#10;difficulty level and is measured pre and post surgery.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is desired to assess the relationship between $Y$ and $X_{pre}$ and also $Y$ &#10;on $X_{\delta} = X_{post} - X_{pre}$. However, since $X_\delta$ is derived from &#10;$X_{pre}$, we know that $X_{pre}$ and $X_{\delta}$ will be highly correlated.    One &#10;option is to attempt to reduce such multicollinearity via centering.&lt;br&gt;&#10;Any thoughts on alternative strategies and pros/cons?   This scenario &#10;sounds similar to the commonly discussed scenario of how to handle &#10;change scores via ANCOVA (e.g., Senn 2009, Stat Med) when we have change &#10;scores with respect to $Y$, but it is different since here we have no &#10;baseline $Y$ and a change score for $X$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-14T17:27:23.170" Id="18359" LastActivityDate="2014-04-27T17:47:03.997" LastEditDate="2014-04-27T17:47:03.997" LastEditorUserId="26338" OwnerUserId="7368" PostTypeId="1" Score="2" Tags="&lt;multicollinearity&gt;" Title="multicollinearity question, X and X change score" ViewCount="92" />
  <row AcceptedAnswerId="18523" AnswerCount="3" Body="&lt;p&gt;I have four different time series of hourly measurements:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The heat consumption inside a house&lt;/li&gt;&#10;&lt;li&gt;The temperature outside the house&lt;/li&gt;&#10;&lt;li&gt;The solar radiation&lt;/li&gt;&#10;&lt;li&gt;The wind speed&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I want to be able to predict the heat consumption inside the house. There is a clear seasonal trend, both on a yearly basis, and on a daily basis. Since there is a clear correlation between the different series, I want to fit them using an ARIMAX-model. This can be done in R, using the function arimax from the package TSA.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to read the documentation on this function, and to read up on transfer functions, but so far, my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;regParams = ts.union(ts(dayy))&#10;transferParams = ts.union(ts(temp))&#10;model10 = arimax(heat,order=c(2,1,1),seasonal=list(order=c(0,1,1),period=24),xreg=regParams,xtransf=transferParams,transfer=list(c(1,1))&#10;pred10 = predict(model10, newxreg=regParams)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives me:&#10;&lt;img src=&quot;http://i.stack.imgur.com/UuJGj.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where the black line is the actual measured data, and the green line is my fitted model in comparison. Not only is it not a good model, but clearly something is wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will admit that my knowledge of ARIMAX-models and transfer functions is limited. In the function arimax(), (as far as I have understood), xtransf is the exogenous time series which I want to use (using transfer functions) to predict my main time series. But what is the difference between xreg and xtransf really?&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, what have I done wrong? I would like to be able to get a better fit than the one achieved from lm(heat ~ temp*radi*wind*time).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&#10;Based on some of the comments, I removed transfer, and added xreg instead:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;regParams = ts.union(ts(dayy), ts(temp), ts(time))&#10;model10 = arimax(heat,order=c(2,1,1),seasonal=list(order=c(0,1,1),period=24),xreg=regParams)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where dayy is the &quot;number day of the year&quot;, and time is the hour of the day. Temp is again the temperature outside. This gives me the following result:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GDuzQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;which is better, but not nearly what I expected to see.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-14T20:41:02.867" FavoriteCount="19" Id="18375" LastActivityDate="2012-04-03T21:53:52.767" LastEditDate="2012-04-03T21:53:52.767" LastEditorUserId="930" OwnerUserId="7339" PostTypeId="1" Score="14" Tags="&lt;time-series&gt;&lt;modeling&gt;&lt;arima&gt;" Title="How to fit an ARIMAX-model with R?" ViewCount="10168" />
  
  
  
  
  
  
  
&#10;\begin{align*}
  
  <row Body="&lt;p&gt;I suppose that depends on what they mean by &quot;parametric and non-parametric&quot;? At the same time exactly both, or a blend of the two?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many consider the Cox proportional hazards model to be semi-parametric, as it doesn't parametrically estimate the baseline hazard.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or you might choose to view many non-parametric statistics as actually massively parametric.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-16T03:08:36.360" Id="18466" LastActivityDate="2011-11-16T03:08:36.360" OwnerUserId="5836" ParentId="18450" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="18498" AnswerCount="3" Body="&lt;p&gt;Suppose you have a population and some measurement which you could do on each member of the population (e.g. the population could be all the people in the world, and the measurement could be height).  So one can regard this measurement as a random variable $X$ on the population, with some mean $\mu$ and variance $\sigma^2$; $\mu$ is known, $\sigma^2$ may or may not be known.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose you have a subset of the population, a sample of size $N$, and you wish to know whether these people are significantly different than the overall population with respect to this measurement.  You can measure them and find the mean $\bar{x}$ and variance $s^2 = \frac 1N\sum_1^N(x_i-\bar{x})^2$ where the $x_i$ are the individual measurements of the people in your sample.  One way to determine the significance of your measurements is to do the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X_i \sim_{\mathrm{iid}} X$ for $i = 1, 2, \dots, N$ and let $Y = \frac 1N\sum_1^NX_i$.  Estimate a distribution for $Y$.  Based on this estimate, determine the probability $P(|Y - E[Y]| &amp;gt; |\bar{x} - E[Y]|)$ and if this probability is larger than some predetermined threshold, then you reject the null hypothesis (which in this case would've roughly captured the hypothesis that your sample population is not different from the overall population).&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are about the estimated distribution for $Y$.  The Central Limit Theorem says that if $N$ is large, then we may assume $Y$ is normally distributed.  But if $N$ is small, we're supposed to use Student's t-distribution [Disclaimer: I'm sure it's more complicated than that, but this is what I'm supposed to teach my students so I need to know the reason that this might be a reasonable thing to teach them].  So my first (multi-part) question is: What is the conventional cutoff for small $N$/large $N$, why is that cutoff conventionally accepted, and why wouldn't we just always use Student's t-distribution even for large $N$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Once we know what kind of distribution to use, we still need to know the parameters.  It's not hard to see that $Y$ will have mean $\mu$ and variance $\frac{\sigma^2}{N}$.  Now if $\sigma^2$ isn't known, then we estimate it by $\hat{s}^2 = \frac{N}{N-1}s^2$.  So my next (multi-part) question is: Why precisely the $\frac{N}{N-1}$ factor, and is there ever a case where we would use $\hat{s}^2$ even if $\sigma^2$ were known?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-16T09:23:28.323" FavoriteCount="1" Id="18475" LastActivityDate="2011-11-16T23:39:28.003" LastEditDate="2011-11-16T15:33:16.197" LastEditorUserId="7405" OwnerUserId="7405" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;standard-deviation&gt;&lt;unbiased-estimator&gt;&lt;students-t&gt;" Title="Two questions on significance testing" ViewCount="241" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I want to perform  a test of independence for a contingency table where the Y characteristic codes for whether an individual go to school or not, and X stands for its gender (MA, male; FE, female).&#10;A typical contingency table would be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;+-----+-----------+-----------------+&#10;| X/Y | Go School | No Go to school |&#10;+-----+-----------+-----------------+&#10;| MA  |           |                 |&#10;+-----+-----------+-----------------+&#10;| FE  |           |                 |&#10;+-----+-----------+-----------------+&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This kind of test is clear to me. Now suppose that my contingency is like the following one&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;+-----+-----------+-----------------+&#10;| X/Y | Go School | No Go to school |&#10;+-----+-----------+-----------------+&#10;| MA  |           |                 |&#10;+-----+-----------+-----------------+&#10;| MA  |           |                 |&#10;+-----+-----------+-----------------+&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In other words the X variable refers to sample of &quot;the same type&quot;; that is, they are both extracted from the same population and have the same gender. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: If I discover that there is a significant difference between the proportions of these two samples, I can't conclude that these two samples are &quot;similar&quot;; rather, there must be another factor or characteristic (that I haven't considered) which causes this difference although the samples come from the same population. Is this correct? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-16T09:38:56.733" Id="18477" LastActivityDate="2011-11-16T22:54:45.520" LastEditDate="2011-11-16T21:08:47.810" LastEditorUserId="930" OwnerUserId="7406" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;contingency-tables&gt;" Title="Tests of independence in contingency tables" ViewCount="201" />
  <row AcceptedAnswerId="18479" AnswerCount="1" Body="&lt;p&gt;I'm trying to solve a couple of exercises in Gelman and Hill 2007.&lt;/p&gt;&#10;&#10;&lt;p&gt;The datasets they use are all on a website, however, every time I try to import the files I get an error:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;pollution=read.table(&quot;pollution.dta&quot;)&#10;  Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  : &#10;    line 1 did not have 2 elements&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here's the dataset: &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/arm/examples/pollution/&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.columbia.edu/~gelman/arm/examples/pollution/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess there's a problem with the datasets, but I'm wondering if there's a modification that could enable me to import it?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-16T09:42:00.173" Id="18478" LastActivityDate="2011-11-16T10:52:08.760" LastEditDate="2011-11-16T10:52:08.760" LastEditorUserId="930" OwnerUserId="5837" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;dataset&gt;&lt;stata&gt;" Title="Interpreting error when trying to import dta file in R" ViewCount="81" />
  <row AcceptedAnswerId="18495" AnswerCount="2" Body="&lt;p&gt;I asked this question on Stackoverflow:&#10;&lt;a href=&quot;http://stackoverflow.com/questions/8142118/incidence-rate-ratios-in-r-poisson-regression&quot;&gt;http://stackoverflow.com/questions/8142118/incidence-rate-ratios-in-r-poisson-regression&lt;/a&gt;&#10; and was advised to post here instead.&#10;I have data that looks like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   sex agecat  cases population&#10;&#10;1 male    0-4  12     126526&#10;2 male    5-9  12     128375&#10;3 female  0-4  11     129280&#10;4 male    10-14 4     127910&#10;5 female  0-4  13     127158&#10;6 male    0-4  8      125125&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to duplicate the output I get in stata with this command&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;poisson cases i.agecat, exp(pop) irr&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives output such as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   cases |        IRR   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;---------+----------------------------------------------------------------&#10;  agecat |&#10;      2  |   .5125755   .0530442    -6.18   0.000     .4578054    .6669639&#10;      3  |    .323456   .0381304    -9.60   0.000     .2665044    .4172274&#10;population | (exposure)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;in R with a command such as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(cases~agecat, family = poisson(link = &quot;log&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I know I need to exponentiate the coefficients and confidence intervals, but I think I also need some kind of offset so so that the intercept is zero; and adjust for per unit population vs baseline. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help/advise ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&#10;EDIT: The question on SO has been answered, but I posted more detail here. In particular, I think the issue has to do with adjusting for population size in stata with exp(pop) - and how to replicate this in R. &lt;/p&gt;&#10;" CommentCount="11" CreationDate="2011-11-16T17:24:14.883" FavoriteCount="1" Id="18490" LastActivityDate="2011-11-16T19:18:11.560" LastEditDate="2011-11-16T18:13:32.270" LastEditorUserId="6884" OwnerUserId="6884" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;stata&gt;&lt;epidemiology&gt;" Title="Incidence rate ratios (Stata vs R)" ViewCount="1783" />
  <row Body="&lt;p&gt;Think of the N-1 as a special case of the number of independent observations minus the number of linear constraints you assert about them.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In the familiar situation where we estimate both a mean and a variance of a univariate distribution, the independent measurements are the N sample data points, and the linear constraint is your estimate of the unknown mean. If that is not intuitive, note that asserting the mean of a N numbers to be $m$ is the same as forcing them to add up to $m \times N$.  At most N-1 of the original numbers can vary while fulfilling this sum constraint, so there are really only N-1 bits of information left to tell you about the standard deviation.  Equivalently, but perhaps less intuitively, as &lt;a href=&quot;http://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;this page&lt;/a&gt; puts it, the sum of the squared deviations from the estimated mean must be equal to 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;With this schema in mind it should be clear why the correction is not always N-1.  Slightly more generally, it will be N-K where K is the number of other 'free parameters' in the model. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-16T18:07:16.350" Id="18492" LastActivityDate="2011-11-16T18:07:16.350" OwnerUserId="1739" ParentId="18475" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Gradient Boosted Trees.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;At least as accurate as RF on a lot of applications&lt;/li&gt;&#10;&lt;li&gt;Incorporates missing values seamlessly&lt;/li&gt;&#10;&lt;li&gt;Var importance (like RF probably biased in favor of continuous and many level nominal)&lt;/li&gt;&#10;&lt;li&gt;Partial dependency plots&lt;/li&gt;&#10;&lt;li&gt;GBM versus randomForest in R : handles MUCH larger datasets&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-11-17T00:00:16.620" CreationDate="2011-11-17T00:00:16.620" Id="18507" LastActivityDate="2011-11-17T00:00:16.620" OwnerUserId="2040" ParentId="258" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="18517" AnswerCount="2" Body="&lt;p&gt;I am comparing three relatively simple GLMs having a Gamma distribution with AIC and BIC. The aim is to identify the effects of fertilizers (fdung), year and site on biomass of a specific grass species. Hence, the aim is not to predict new values but merely to identify the effects of the three factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the models used:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;res1 &amp;lt;-  glm((Biomass..g.m².) ~  fdung * fyear * fblock, family=Gamma(link=&quot;identity&quot;))&#10;res2 &amp;lt;-  glm((Biomass..g.m².) ~  fdung * fyear + fblock, family=Gamma(link=&quot;identity&quot;))&#10;res3 &amp;lt;-  glm((Biomass..g.m².) ~  fdung + fyear + fblock, family=Gamma(link=&quot;identity&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I expect the third model to be the most simplistic one and want to confirm this by an information criterion. However, when looking at AIC and BIC I get this output.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;AIC(res1,res2,res3)     BIC(result1,res2,res3)&#10;        df      AIC            df      BIC&#10;res1    49 5271.617     res1   49 5465.198&#10;res2    16 5334.234     res2   16 5397.44&#10;res3    10 5331.253     res3   10 5370.760&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For AIC, the most complex model is &quot;best&quot; and for BIC the one with fewest df is best. I am thinking that with regard to my aim (identify effects on biomass) I should trust BIC.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I wrong here with my conclusion?&lt;/p&gt;&#10;&#10;&lt;p&gt;I already tried mixed effect models with the fblock as random factor but then the model with the Gamma distribution did not work any more and also I could not use fblock as fixed effect any more (leading to NAs for fblock), but this is not part of my question.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-17T08:26:53.817" FavoriteCount="1" Id="18511" LastActivityDate="2011-11-17T20:12:58.300" OwnerUserId="5280" PostTypeId="1" Score="5" Tags="&lt;generalized-linear-model&gt;&lt;interpretation&gt;&lt;aic&gt;&lt;bic&gt;" Title="Why do AIC and BIC show inversed outputs?" ViewCount="724" />
  
  <row Body="&lt;p&gt;It is called the &lt;em&gt;LSD Threshold Matrix&lt;/em&gt;, according to the &lt;em&gt;JMP Statistics and Graphics Guide&lt;/em&gt; (it is headed as &lt;code&gt;Abs(Dif)-LSD&lt;/code&gt;), and those numbers reflect absolute difference between group means minus Fisher's least significant difference (LSD). They are displayed as off diagonal entries with the greatest difference in the upper-right or lower-left corner. The diagonal entries represent the comparison of each group mean with itself and are just the opposite of the LSD value.&#10;(Note that the matrix is symmetric so you just need to look at the upper or lower diagonal entries.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, as stated in the documentation, a high positive value would indicate a large departure from the LSD, whereas negative values would mean that the observed difference of means is less than it. &#10;See also Ramirez and Ramirez, &lt;a href=&quot;http://www.sascommunity.org/wiki/Analyzing_and_Interpreting_Continuous_Data_Using_JMP%3a_A_Step-by-Step_Guide&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Analyzing and Interpreting Continuous Data Using JMP: A Step-by-Step Guide&lt;/em&gt;&lt;/a&gt;, SAS Publishing 2009 (pp. 314-316), for more information on how JMP handles the computation of LSD and HSD. Gerard E. Dallal also offers a good overview of &lt;a href=&quot;http://www.jerrydallal.com/LHSP/mc.htm&quot; rel=&quot;nofollow&quot;&gt;Multiple Comparison Procedures&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-17T11:17:53.753" Id="18516" LastActivityDate="2011-12-01T15:21:41.720" LastEditDate="2011-12-01T15:21:41.720" LastEditorUserId="930" OwnerUserId="930" ParentId="18487" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="18530" AnswerCount="1" Body="&lt;p&gt;First of all, I am very new to this subject. I have been using weka some days to familiarize myself and reading about data mining to understand it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataset (from UCI, &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29&quot; rel=&quot;nofollow&quot; title=&quot;this one&quot;&gt;german credit data&lt;/a&gt;) and I need to classify it. I have been trying different tree algorithms using different attributes instead of all and I wish to find which one gives me the best tree. (splitting my data 66/34 for training and testing) &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to automate this, that is make weka try all combinations of attributes to find which ones give the best tree? I would prefer to use weka since is the tool I have been using, but this is not compulsory.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-17T13:21:08.983" Id="18519" LastActivityDate="2011-11-17T17:03:04.813" LastEditDate="2011-11-17T14:17:26.350" LastEditorUserId="930" OwnerUserId="5196" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;weka&gt;" Title="Automating tree-based classification by using different subset of attributes" ViewCount="437" />
  <row AcceptedAnswerId="18537" AnswerCount="4" Body="&lt;p&gt;I have the vector &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x&amp;lt;-c(1,2,3,4,5,5,5,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,9,9,9,10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(my actual vector has a length of &gt;10.000)&lt;/p&gt;&#10;&#10;&lt;p&gt;and i would like to find the intervals where the 90% of the density lies. Is the &lt;code&gt;quantile(x, probs=c(0.05,0.95), type=5)&lt;/code&gt; the most appropriate or is there any other way?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much&#10;E.C&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-16T11:32:28.953" Id="18533" LastActivityDate="2011-11-17T16:31:42.290" OwnerDisplayName="ECII" OwnerUserId="6015" PostTypeId="1" Score="2" Tags="&lt;r&gt;" Title="Find probability density intervals" ViewCount="1245" />
  <row Body="&lt;p&gt;It certainly seems like the most straightforward approach. The function is quite fast. I use it all the time on samples that are hundreds of times larger that the one you are using, and the stability of the estimates should be good at your sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are functions in other packages that provide more complete sets of descriptive statistics. The one I use is &lt;code&gt;Hmisc::describe&lt;/code&gt;, but there are several other packages with &lt;code&gt;describe&lt;/code&gt; functions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-16T12:40:06.437" Id="18535" LastActivityDate="2011-11-16T12:40:06.437" OwnerDisplayName="DWin" OwnerUserId="2129" ParentId="18533" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="18594" AnswerCount="1" Body="&lt;p&gt;In MATLAB, I've written two snippets of code that compute the PDF of a multivariate normal distribution. However there's a difference in the values these two methods produce and I can't figure out why. I've narrowed the problem down to something having to do with computing the inverse of the covariance matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;Inaccurate code&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;function p = mvnpdf_inacc(X, mu, sigma)&#10;    xc = bsxfun(@minus, X, mu);&#10;    [n, k] = size(xc);&#10;    twopic = (2 * pi) ^ (-k / 2);&#10;    sqrtdetsig = sqrt(det(sigma)) ^ -1;&#10;    c = twopic * sqrtdetsig;&#10;    p = zeros(n, 1);&#10;    for i = 1:n&#10;        xci = xc(i, :);&#10;        p(i) = c * exp(-0.5 * (xci / sigma * xci'));&#10;    end&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Accurate code&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;function p = mvnpdf_acc(X, mu, sigma)&#10;    [R, err] = cholcov(sigma, 0);&#10;&#10;    if err&#10;        error('%s', 'sigma is not both symmetric and positive definite');&#10;    end&#10;&#10;    X0 = bsxfun(@minus, X, mu) / R;&#10;    d = min(size(X));&#10;    slogdet = sum(log(diag(R)));&#10;    p = exp(-0.5 * sum(X0 .^ 2, 2) - slogdet - 0.5 * d * log(2 * pi));&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Testing code&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;function iseq_func = test_mvnpdf(n)&#10;    x = linspace(-2, 2, n);&#10;    y = x;&#10;    [X, Y] = meshgrid(x, y);&#10;    XY = [X(:), Y(:)];&#10;    mu = [0, 0];&#10;    sigma = [1.0, 0.5; 0.5, 0.4];&#10;    p_inacc = mvnpdf_inacc(XY, mu, sigma);&#10;    p_acc = mvnpdf_acc(XY, mu, sigma);&#10;    p_diff = abs(p_inacc - p_acc);&#10;    iseq_func = nnz(p_diff) == 0;&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get a value of false from running &lt;code&gt;iseq_func(25)&lt;/code&gt;. What the heck is going on here? Thanks!&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2011-11-17T18:45:25.267" Id="18542" LastActivityDate="2011-11-20T23:54:13.853" LastEditDate="2011-11-20T23:54:13.853" LastEditorUserId="2660" OwnerUserId="2660" PostTypeId="1" Score="5" Tags="&lt;matlab&gt;&lt;covariance&gt;" Title="Numerical accuracy of multivariate normal distribution" ViewCount="1232" />
  
  <row Body="&lt;p&gt;Welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;You are correct in that many books look at a single variable. I might suggest searching this site for Multivariate analysis. There is an answer &lt;a href=&quot;http://stats.stackexchange.com/q/2181/2591&quot;&gt;dealing with books&lt;/a&gt; that you may find helpful. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-17T20:30:28.560" Id="18549" LastActivityDate="2011-11-17T20:30:28.560" OwnerUserId="2591" ParentId="18544" PostTypeId="2" Score="1" />
  
  <row Body="An interval of random variables, depending on observed data, which, with a fixed probability, contain an unknown parameter of interest." CommentCount="0" CreationDate="2011-11-18T06:05:27.680" Id="18573" LastActivityDate="2011-11-18T07:39:08.757" LastEditDate="2011-11-18T07:39:08.757" LastEditorUserId="795" OwnerUserId="795" PostTypeId="4" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;We analyze blood vessel samples of patients in an organ bath which gives us force values. Each subject (patient) donates two types of blood vessels (pulmonary vein and pulmonary artery). The organ bath measurements include dose-response curves with a pharmaceutical, i.e. different concentrations are applied. From what I understand, this should be analyzed using a two-way RM ANOVA test. There are two variables (vessel type and concentration), and all levels of both are applied to each subject. The data table looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subject  vessel  conc  force&#10;20110818 PA      -12   0&#10;20110818 PV      -12   0&#10;20110818 PA      -11   0.09&#10;20110818 PV      -11   0.15&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are no missing values or anything like that to consider.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perusing several R tutorials, I came up with the following query:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov.ex2=aov(force~(vessel*conc)+Error(subject/(vessel*conc)),data=data.ex2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, the results are somewhat unclear to me:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(aov.ex2)&#10;&#10;Error: subject&#10;          Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;Residuals  1 15.370  15.370               &#10;&#10;Error: subject:vessel&#10;       Df Sum Sq Mean Sq&#10;vessel  1 166.72  166.72&#10;&#10;Error: subject:concentration&#10;              Df Sum Sq Mean Sq&#10;concentration  1 3134.3  3134.3&#10;&#10;Error: subject:vessel:concentration&#10;                     Df Sum Sq Mean Sq&#10;vessel:concentration  1 148.32  148.32&#10;&#10;Error: Within&#10;           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;Residuals 219   3031  13.840               &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is quite a lot of information, but in contrast to the nice examples on the web I can't seem to find the useful part of the information: p values and whether or not there are significant differences. Is there something wrong with my input, or does the result simply mean there is nothing interesting to see in these data?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-18T11:29:53.280" Id="18579" LastActivityDate="2014-10-09T18:14:38.623" LastEditDate="2011-11-18T13:31:23.877" LastEditorUserId="930" OwnerUserId="7443" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="Using R's aov() for two-way repeated-measurement ANOVA" ViewCount="2739" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am doing a salary comparison depending on 2 cities. Very similar to this &lt;a href=&quot;http://cgi.money.cnn.com/tools/costofliving/costofliving.html&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So you select a source city, then a destination city, your current salary in the source city and it will output the &quot;comparable&quot; salary in the destination city. I want to confirm that what I am doing is correct, just bare with me for a minute. I will give you an example. All the data is fictional.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Source city: Cleveland, OH &#10;Destination city: Seattle, WA &#10;Cleveland Salary: $120,000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The way I compare is I take the housing prices, transportation prices and see where it's higher/lower. So, let's assume I will take into consideration only these two pieces of spending: housing and transportation.&lt;/p&gt;&#10;&#10;&lt;p&gt;On average housing represents 30% of spending, while transportation is 20%.&#10;Let's assume that housing is 10% more expensive in Seattle, while transportation is 5% cheaper.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, here is where I want your help. Am I doing this right?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;30% + 20% = 50% in salary will fluctuate (since I take into consideration only these two spendures)&lt;/li&gt;&#10;&lt;li&gt;Total amount that will fluctuate from source salary is: $120,000 - 50% = 60K&lt;/li&gt;&#10;&lt;li&gt;Housing is 30% (=$36,000) from 60K&lt;/li&gt;&#10;&lt;li&gt;Transportation is 20% (=$24,000) from 60K&lt;/li&gt;&#10;&lt;li&gt;Housing is 10% higher, so 36K + 10% (3,6K) = $39,600&lt;/li&gt;&#10;&lt;li&gt;Transportation is 5% lower, so 24K - 5% (1,2K) = $22,800&lt;/li&gt;&#10;&lt;li&gt;Comparable Salary in Seattle would be (the untouched) 60K + 39,600 + 22,800 = $122,400&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Do these calculations seem right to you?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-18T15:04:26.040" Id="18587" LastActivityDate="2013-05-13T06:04:04.073" LastEditDate="2011-12-19T17:22:47.780" LastEditorUserId="1036" OwnerUserId="7446" PostTypeId="1" Score="2" Tags="&lt;predictive-models&gt;" Title="Comparable salary in a different city calculations" ViewCount="281" />
  <row Body="&lt;p&gt;Two things - &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Faster convergence by avoiding curse of dimensionality. Because most points in a grid lie on the same hyper plane without contributing significantly extra information. Random points fill the N-dimensional space evenly. LDS is even better.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Sometimes for Monte carlo methods we need statistically random points in no particular order. An ordered sequence of grid points will result in poor statistical properties.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2011-11-18T15:19:50.580" Id="18588" LastActivityDate="2011-11-18T15:19:50.580" OwnerUserId="7449" ParentId="13536" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;As pointed out in the other answers, the mean $\bar{x}$ and standard deviation&#10;$\sigma_x$ are essentially unrelated in that it is not necessary for the standard deviation to be smaller than the mean.  However, if the data are nonnegative, taking on values in $[0,c]$, say, then, for large data sets (where the distinction between dividing by $n$ or by $n-1$ does not matter very much), the following inequality &#10;&lt;a href=&quot;http://math.stackexchange.com/questions/83046/maximum-of-the-variance-function-for-given-set-of-bounded-numbers/83144#83144&quot;&gt;holds&lt;/a&gt;:&#10;$$\sigma_x \leq \sqrt{\bar{x}(c-\bar{x})} \leq \frac{c}{2}$$&#10;and so if $\bar{x} &amp;gt; c/2$,  we can be sure that $\sigma_x$ will be smaller.&#10;Indeed, since $\sigma_x = c/2$ only for an extremal distribution (half the&#10;data have value $0$ and the other half value $c$), $\sigma_x &amp;lt; \bar{x}$ can&#10;hold in some cases when $\bar{x} &amp;lt; c/2$ as well.&#10;If the data are measurements of some physical quantity that is nonnegative&#10;(e.g. area) and have an empirical distribution that is a good fit to a &#10;normal distribution, then $\sigma_x$ will be considerably smaller &#10;than $\min\{\bar{x}, c - \bar{x}\}$ since the fitted normal distribution &#10;should assign negligibly small probability to the events $\{X &amp;lt; 0\}$&#10;and $\{X &amp;gt; c\}$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-18T17:03:52.650" Id="18593" LastActivityDate="2011-11-18T18:25:53.280" LastEditDate="2011-11-18T18:25:53.280" LastEditorUserId="6633" OwnerUserId="6633" ParentId="18590" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="18653" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Brief Summary&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Why is it more common for logistic regression (with odds ratios) to be used in cohort studies with binary outcomes, as opposed to poisson regression (with relative risks)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Undergraduate and graduate statistics and epidemiology courses, in my experience, generally teach that logistic regression should be used for modelling data with binary outcomes, with risk estimates reported as odds ratios.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, poisson regression (and related: quasipoisson, negative binomial, etc.) can also be used to model data with binary outcomes and, with appropriate methods (e.g. robust sandwich variance estimator), provides valid risk estimates and confidence levels. E.g.,&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Greenland S., &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/15286014&quot;&gt;Model-based estimation of relative risks and other epidemiologic measures in studies of common outcomes and in case-control studies&lt;/a&gt;, Am J Epidemiol. 2004 Aug 15;160(4):301-5.&lt;/li&gt;&#10;&lt;li&gt;Zou G., &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/15033648&quot;&gt;A modified poisson regression approach to prospective studies with binary data&lt;/a&gt;, Am J Epidemiol. 2004 Apr 1;159(7):702-6. &lt;/li&gt;&#10;&lt;li&gt;Zou G.Y. and Donner A., &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22072596&quot;&gt;Extension of the modified Poisson regression model to prospective studies with correlated binary data&lt;/a&gt;, Stat Methods Med Res. 2011 Nov 8.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;From poisson regression, relative risks can be reported, which some have argued are easier to interpret compared to odds ratios, especially for frequent outcomes, and especially by individuals without a strong background in statistics, see Zhang J. and Yu K.F., &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/9832001&quot;&gt;What's the relative risk? A method of correcting the odds ratio in cohort studies of common outcomes&lt;/a&gt;, JAMA. 1998 Nov 18;280(19):1690-1.&lt;/p&gt;&#10;&#10;&lt;p&gt;From reading the medical litterature, among cohort studies with binary outcomes it seems that it is still far more common to report odds ratios from logistic regressions rather than relative risks from poisson regressions. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For cohort studies with binary outcomes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is there good reason to report odds ratios from logistic regressions rather than relative risks from poisson regressions?&lt;/li&gt;&#10;&lt;li&gt;If not, can the infrequency of poisson regressions with relative risks in the medical literature be attributed mostly to a lag between methodological theory and practice among scientists, clinicians, statisticians, and epidemiologists?&lt;/li&gt;&#10;&lt;li&gt;Should intermediate statistics and epidemiology courses include more discussion of poisson regression for binary outcomes?&lt;/li&gt;&#10;&lt;li&gt;Should I be encouraging students and colleagues to consider poisson regression over logistic regression when appropriate?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2011-11-18T18:10:20.473" FavoriteCount="6" Id="18595" LastActivityDate="2011-11-19T23:24:39.687" LastEditDate="2011-11-19T21:17:59.960" LastEditorUserId="930" OwnerUserId="2981" PostTypeId="1" Score="17" Tags="&lt;logistic&gt;&lt;poisson&gt;&lt;epidemiology&gt;&lt;odds-ratio&gt;&lt;relative-risk&gt;" Title="Poisson regression to estimate relative risk for binary outcomes" ViewCount="6169" />
  <row AnswerCount="3" Body="&lt;p&gt;This question is similar to another question I had recently posted but I have a follow on.&lt;/p&gt;&#10;&#10;&lt;p&gt;In classical linear regression, we have &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\beta } \sim N(\beta,(X^{T}X)^{-1}\sigma^2).$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Using this, one builds individual hypotheses of the significance of the coefficients, as done in the book by Tibshirani et al. My questions are two fold:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The book talks about a combined hypothesis built by proving that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\hat{\beta }-\beta)^T(X^{T}X)^{-1}(\hat{\beta }-\beta) \sim \sigma^2\chi_{p+1}^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't see how this formula can be derived from the equation I wrote above. I do see that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\beta }-\beta \sim N(0,(X^{T}X)^{-1}\sigma^2).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;How do we take the $X^TX$ matrix out and prove the above?  I would be grateful if someone could outline the steps. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) My second question is, how do we think about building the hypothesis?  Do we think about building individual coefficient hypotheses or is it a good idea to view everything together? In other words, what is the difference/pros and cons of using the two different styles of hypotheses, the individual coefficient one or viewing everything together as per the above equation? Can we have an example of building a combined hypothesis?  I am guessing that most statistical packages don't really take into account the correlation between different $\beta$ which is encoded in the matrix $X^TX$. Please clarify, any help will be much appreciated.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-11-18T18:21:22.640" Id="18596" LastActivityDate="2014-06-29T03:21:07.803" LastEditDate="2011-11-18T21:05:54.470" LastEditorUserId="449" OwnerUserId="7455" PostTypeId="1" Score="2" Tags="&lt;regression&gt;" Title="Statistical significance of betas in linear regression" ViewCount="287" />
  <row Body="&lt;p&gt;As you said &quot;The PACF values are the coefficients of an autoregression of the series of interest on lagged values of the series&quot; and I add where the PACF(K) is the coefficient of the last (kth) lag. Thus to compute the PACF of lag 3 for example compute &#10;\begin{equation}&#10;Y_{t}= a_{0}+ a_{1}Y_{t−1}+ a_{2}Y_{t−2}+  a_{3}Y_{t−3}&#10;\end{equation} &lt;/p&gt;&#10;&#10;&lt;p&gt;and $a_{3}$ is the PACF(3). &lt;/p&gt;&#10;&#10;&lt;p&gt;Another example. To compute the PACF(5), estimate &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;Y_{t}= a_{0}+ a_{1}Y_{t−1}+ a_{2}Y_{t−2}+ a_{3}Y_{t−3}+ a_{4}Y_{t-4}+ a_{5}Y_{t-5} &#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;and $a_{5}$ is the PACF(5). &lt;/p&gt;&#10;&#10;&lt;p&gt;In general the PACF(K) is the KTH order coefficient of a model terminating with lag K. By the way SAS and other software vendors use the Yule-Walker approximation to compute the PACF which will provide slightly different estimates of the PACF. They do this for computational efficiency and in my opinion to duplicate the results in standard textbooks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-18T22:34:00.010" Id="18612" LastActivityDate="2013-05-10T13:10:36.477" LastEditDate="2013-05-10T13:10:36.477" LastEditorUserId="24617" OwnerUserId="3382" ParentId="18599" PostTypeId="2" Score="7" />
  <row AnswerCount="2" Body="&lt;p&gt;I am going to be using R for text analysis (mostly clustering, classification and some visualization) and was wondering what mechanisms R provides for handling high dimensional, sparse data sets. If I understand correctly, R does provide some packages (e.g., &lt;a href=&quot;http://cran.r-project.org/web/packages/Matrix/&quot; rel=&quot;nofollow&quot;&gt;matrix library&lt;/a&gt;) for handling large and sparse matrices - which brings me to my question. &lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, I would like to know:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Which R libraries are most appropriate for storing and processing high dimensional sparse data? Just FYI, my data will fit into memory. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Do such libraries inter-operate with existing text analysis (clustering/classification) packages? Would I need to convert these sparse data structures to and from data frames if I need to text analysis? Wouldn't that add additional time overhead to the computations?  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am fairly new to R, so please excuse me if this sounds vague (or too general). &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-19T01:04:53.007" FavoriteCount="0" Id="18616" LastActivityDate="2011-12-22T14:46:01.503" LastEditDate="2011-11-19T09:56:54.467" LastEditorUserId="930" OwnerUserId="3301" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;text-mining&gt;&lt;large-data&gt;" Title="Data structures and libraries for high dimensional text analysis with R" ViewCount="264" />
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;2) again, I do not have the book so cannot guess what the author mean by &quot;building an hypothesis&quot;. The natural approach is to have an exogenous question about the significance of one group of variables and to test it by the corresponding chi-square test, using the corresponding submatrix of $(X^TX)^{-1}$. For instance, testing for $\beta_1=\beta_2=0$ leads to&#10;$$
  
  
  <row Body="&lt;p&gt;I interpret your question ( and others may have their own interpretation ) as asking for different ways to detrend data. In answering your question I am clearing up some unfinished discussions regarding data recently analyzed (how does CohortB relate to CohortA ). There is free software available on the net to fit local splines to a time series thus enabling a powerful way of detrending &lt;a href=&quot;http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=7305899&quot; rel=&quot;nofollow&quot;&gt;http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=7305899&lt;/a&gt; is one source for STL2. It is defined as &quot;An algorithm is described which generates a piecewise linear approximation to tabulated data which is within specified tolerances of the data points&quot; The only problem is that you have to pre-define how many individual break points i.e the number of individual trends you require and you have to manually cleanse your series of aberrant data points. Too many splines (local trends) and you connect two dots . Too few splines (local trends) and you connect none. An alternative approach is to use Intervention Detection schemes promoted by many statisticians including myself. You can review my comments and references for the statistical theory of Intervention Detection at &lt;a href=&quot;http://stats.stackexchange.com/questions/12651/box-jenkins-model-selection&quot;&gt;Box-Jenkins model selection&lt;/a&gt; . This body of work forms valid statistical tests to incorporate a minimally sufficient number of breakpoints not simply the result of eye-balling. Now to give you an example of this approach to &quot;detrending&quot; which necessarily includes the isolation of one-time unusual values I will use the CohortB series from &lt;a href=&quot;http://stats.stackexchange.com/questions/18538/how-to-compare-2-non-stationary-time-series-to-determine-a-correlation/18547#18547&quot;&gt;How to compare 2 non-stationary time series to determine a correlation?&lt;/a&gt; which by itself suggests as many as 5 local time trends. When one accounts for the influence/impact of the supporting series there are no additional trends required as the predictor variable has “explained the growth”.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example the original series clearly suggests the need for one or more local time trends and even the possibility that there is curvature or a quadratic component&lt;img src=&quot;http://i.stack.imgur.com/6xjB5.jpg&quot; alt=&quot;enter image description here&quot;&gt; . The search for the number of local lines requires the simultaneous isolation of &quot;unusual points&quot; as we don't want to be incorrectly influenced by them. &lt;img src=&quot;http://i.stack.imgur.com/Zq3ld.jpg&quot; alt=&quot;a useful equation &quot;&gt; details the five local trends separated by 4 breakpoints (1,9,19,29) for the 38 data points. Graphically we have &lt;img src=&quot;http://i.stack.imgur.com/LiuD4.jpg&quot; alt=&quot;actuals and cleansed&quot;&gt; . Note that there were 6 influential one-time events that needed to be identified or purged from the original series in order to &quot;see&quot; the five local trends. The number of unusal points has nothing to do with the number of trends and in general is not statistically relatable.&#10;Proof that the model is reasonably adequate is found in the plot of the residuals and the ACF of the residuals &lt;img src=&quot;http://i.stack.imgur.com/Tv7vz.jpg&quot; alt=&quot;enter image description here&quot;&gt; .  Shows the ACF of the residuals &lt;img src=&quot;http://i.stack.imgur.com/EBhA2.jpg&quot; alt=&quot;enter image description here&quot;&gt;The forecast for CohortB in conjuction with the actual and fit is &lt;img src=&quot;http://i.stack.imgur.com/lzTm5.jpg&quot; alt=&quot;enter image description here&quot;&gt; . In summary evaluating the the slopes of the 4 different breakpoints shows a continuing reduction in the expected value of CohortB (without the influence of CohortA). In summary CohortB is growing at a decreasing rate&#10;and the &quot;reason&quot; for the decreasing rate is due to CohortA ( see previous discussion ).&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;[X1(T)][(+  .850)]    :TIME TREND          1&lt;/p&gt;&#10;&#10;&lt;p&gt;+[X2(T)][(-  .188)]    :TIME TREND          9&lt;/p&gt;&#10;&#10;&lt;p&gt;+[X3(T)][(-  .159)]    :TIME TREND         19&lt;/p&gt;&#10;&#10;&lt;p&gt;+[X4(T)][(-  .0996)]   :TIME TREND         29&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In summary if you have the number of local trends then you can use the equation to de-trend thus avoiding your unit root issue. I hope this helps you.&lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis fulfills the typical statistical formulary to suggest powerful new ways to detrend data by&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;obtaining a useful description of the data ( five local time trends )&lt;/li&gt;&#10;&lt;li&gt;individual deviations are isolated from exceptional deviations&lt;/li&gt;&#10;&lt;li&gt;the final model residuals suggest the degree of randomness&lt;/li&gt;&#10;&lt;li&gt;It is based upon peer-reviewed methodology by many senior statisticians / time-series experts and can be easily programmed by the references I have given .&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-11-19T22:09:51.697" Id="18648" LastActivityDate="2011-11-20T19:21:05.560" LastEditDate="2011-11-20T19:21:05.560" LastEditorUserId="3382" OwnerUserId="3382" ParentId="18336" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Including a trend and drift term when they are not necessary reduce the power of the test---that is, its ability to reject the null hypothesis of stationarity. Contrarily, the test is biased when these parameters are needed, but missing.&lt;/p&gt;&#10;&#10;&lt;p&gt;In economics, we typically don't worry about the trend term, which would imply a trend that was quadratic in time in our variable of interest. Drift implies a linear trend and is commonly incorporated. &lt;/p&gt;&#10;&#10;&lt;p&gt;You may plot a time series of your variable and look at the pattern to see if a trend is noticeable. A basic linear regression of the variable on a linear time trend may give you an idea of whether there is a linear trend as well (of course, you shouldn't pay attention to official hypothesis tests here because serial corrleation/non-stationarities could be biasing your results). Using a spline may also indicate whether there is a linear or quadratic trend in the variable. These visual cues are often good indicators of how you should conduct your Dickey-Fuller test.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-20T01:49:48.233" Id="18660" LastActivityDate="2011-11-20T01:49:48.233" OwnerUserId="401" ParentId="18133" PostTypeId="2" Score="2" />
  
&#10;$$&#10;and&#10;$$
&#10;$$&#10;so $\mathbb E X^{(n)} = \lambda^n$. We get to &quot;skip&quot; to the $n$th index for the start of the sum in the first equality since for any $0 \leq k &amp;lt; n$, $k(k-1)\cdots(k-n+1) = 0$ since exactly one term in the product is zero.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-20T04:57:14.430" Id="18666" LastActivityDate="2011-11-20T14:41:03.957" LastEditDate="2011-11-20T14:41:03.957" LastEditorUserId="2970" OwnerUserId="2970" ParentId="18661" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;If all your numbers are positive, then a measure of relative change which is sometimes useful is  $\log_e(A/B) = \log_e(A)-\log_e(B)$.  The advantage of this is that it is symmetric with a change of sign and that two successive changes can be added together.  For small changes it is also close the percentage change.&lt;/p&gt;&#10;&#10;&lt;p&gt;So for example if you had the numbers 400, 500 and 625: 500 is a 25% increase on  400; 600 is a 20% increase on 500; but 600 is a 50% increase on 400 (not 25%+20%); and 400 is 20% reduction on 500.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Restate this following my suggestion and 500 is about 0.223 up from 400; 625 is about 0.182 up from 500; 625 is about 0.405 up from 400 (0.223+0.182); and 400 is about 0.223 down from 500 (the same magnitude as in the opposite direction but with the sign reversed).  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-20T10:49:06.813" Id="18672" LastActivityDate="2011-11-20T10:49:06.813" OwnerUserId="2958" ParentId="18655" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;This answer is potential expansion of the suggestion made by Aaron, who has suggested to use Pedigreem. The pedigreem can compute relationship from the projects as following syntax, I am unaware how we can use such relation output from different way. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# just example from the manual to create pedigree structure and relation matrix &#10;  # (although you have already the matrix in place) &#10;p1 &amp;lt;- new(&quot;pedigree&quot;,&#10;sire = as.integer(c(NA,NA,1, 1,4,5)),&#10;dam = as.integer(c(NA,NA,2,NA,3,2)),&#10;label = as.character(1:6))&#10;p1&#10;(dtc &amp;lt;- as(p1, &quot;sparseMatrix&quot;)) # T-inverse in Mrode’s notation&#10;solve(dtc)&#10;inbreeding(p1) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The mixed model fit of the package is based on lme4 for the syntax for the main function is similar to lme4 package function lmer function except you can put the pedigree object in it.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pedigreemm(formula, data, family = NULL, REML = TRUE, pedigree = list(),&#10; control = list(),&#10;start = NULL, verbose = FALSE, subset, weights, na.action, &#10;  offset, contrasts = NULL, model = TRUE, x = TRUE, ...)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I know this is not perfect answer to your question, however this can help a little bit. i am glad you asked this question, interesting to me ! &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-20T10:59:38.880" Id="18673" LastActivityDate="2011-11-20T10:59:38.880" OwnerUserId="7244" ParentId="18563" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have performed multiple linear regression analyses with different combinations of transformed and untransformed variables--both explanatory (independent) and response (dependent) variables. All transformations were $\log_{10}(X+1)$ which seem to fit/better fit assumptions of normality. Also, I have indicator (dummy) response variables as explanatory variables. I'm trying to figure out how to interpret the regression estimates, so I would be much obliged if someone could point me toward a good web-based source of information on this, and/or answer the questions below.  Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering:&lt;/p&gt;&#10;&#10;&lt;p&gt;When back transforming--do I subtract the constant (1) from the the regression estimates (after raising 10 to power of the estimate), or just when reporting the mean/median for Y? In other words, does adding the constant to the response variable (before log transformation) matter as far as reporting the regression estimates for the explanatory variables? &lt;/p&gt;&#10;&#10;&lt;p&gt;When do I subtract the constant from the explanatory variable if it is transformed? &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, for example, after constructing the regression model for a response variable which was log-transformed (x+1), my indicator (explanatory) variable estimate is: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Estimate: 0.008&#10;SE0: 0.007&#10;t: 1.110&#10;P: 0.2660   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;with a 95% Confidence interval (in the log10 scale) of -0.0059 TO 0.0213. &#10;I do a back transformation and get: an estimate of 1.017871372 (95% CI from 0.9865  to 1.05). I interpret this as &quot;median Rel Abnd of RESPONSE VARIABLE (which is aprox 0.04) at INDICATOR variable sites is 1.0179 times greater (95% CI = 0.9865 to 1.05) than the median Rel Abund at sites where INDICATOR variable not present, after accounting for other factors&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone could let me know if I'm on the right track, or how to get on the right track, that would be great. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-21T06:44:19.107" Id="18694" LastActivityDate="2011-11-21T13:58:08.700" LastEditDate="2011-11-21T13:58:08.700" LastEditorUserId="919" OwnerUserId="7496" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;logarithm&gt;" Title="Back-transformation and interpretation of $\log(X+1)$ estimates in multiple linear regression" ViewCount="1496" />
  <row AnswerCount="3" Body="&lt;p&gt;Let us assume I have two data sets with &lt;em&gt;n&lt;/em&gt; observations of data pairs of independent variable &lt;em&gt;x&lt;/em&gt; and dependent variable &lt;em&gt;y&lt;/em&gt; each. Let us further assume I want to generate a distribution of regression slopes for each data set by bootstrapping the observations (with replacement) &lt;em&gt;N&lt;/em&gt; times and calculating the regression &lt;em&gt;y&lt;/em&gt; = &lt;em&gt;a&lt;/em&gt; + &lt;em&gt;bx&lt;/em&gt; each time. How do I compare the two distributions in order to say the slopes are significantly different? A U-test for testing the difference between the medians of the distributions would be heavily dependent on N, that is, the more often I repeat the bootstrapping the more significant will be the difference. How do I have to calculate the overlap between the distributions to determine a significant difference?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-21T14:14:04.737" FavoriteCount="7" Id="18707" LastActivityDate="2011-11-25T17:19:43.110" LastEditDate="2011-11-21T14:17:16.320" LastEditorUserId="919" OwnerUserId="7417" PostTypeId="1" Score="12" Tags="&lt;regression&gt;&lt;statistical-significance&gt;&lt;bootstrap&gt;" Title="How do I compare bootstrapped regression slopes?" ViewCount="1890" />
  <row AcceptedAnswerId="18888" AnswerCount="5" Body="&lt;p&gt;I want to fit mixed model using lme4, nlme, baysian regression package or any available. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Mixed model in Asreml- R  coding conventions&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = Xτ + Zu + e ........................(1) ; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the usual mixed model with, y denotes the n × 1 vector of observations,where τ is the p×1 vector of ﬁxed eﬀects, X is an n×p design matrix of full column rank which associates observations with the appropriate combination of ﬁxed eﬀects, u is the q × 1 vector of random eﬀects, Z is the n × q design matrix which associates observations with the appropriate combination of random eﬀects, and e is the n × 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eﬀects model. It is assumed &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gxdur.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where the matrices G and R are functions of parameters γ and φ, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;The parameter θ is a variance parameter which we will refer to as the scale parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;In mixed eﬀects models with more than one residual variance, arising for example in the&#10;analysis of data with more than one section or variate, the parameter θ is&#10;ﬁxed to one. In mixed eﬀects models with a single residual variance then θ is equal to&#10;the residual variance (σ2). In this case R must be correlation matrix. Further details on the models are provided in the &lt;a href=&quot;http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf&quot;&gt;Asreml manual (link)&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Variance structures for the errors: R structure and Variance structures for the random eﬀects: G structures can be specified.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/or4Gj.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/oXTgc.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows&#10;within columns (plots within blocks) the variance of the residuals might then be&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SPE5b.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/IcikW.jpg&quot; alt=&quot;enter image description here&quot;&gt; are correlation matrices for the row model (order r, autocorrelation parameter ½r) and column model (order c, autocorrelation parameter ½c)&#10;respectively. More specifically, a two-dimensional separable autoregressive spatial structure&#10;(AR1 x ­ AR1) is sometimes assumed for the common errors in a field trial analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;The example data:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example models&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I need something equivalent to the asreml-R codes:&lt;/p&gt;&#10;&#10;&lt;p&gt;The simple model syntax will look like the follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; rcb.asr &amp;lt;- asreml(yield ∼ Variety, random = ∼ Replicate, data = nin89)  &#10; .....model 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The linear model is specified in the fixed (required), random (optional) and rcov (error&#10;component) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. &lt;/p&gt;&#10;&#10;&lt;p&gt;The following model1 is more complex in which both G (random) and R (error) structure are specified.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 1:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(nin89)&#10;&#10;&#10; # Model 1: RCB analysis with G and R structure&#10;     rcb.asr &amp;lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), &#10;      rcov = ~ idv(units), data = nin89)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;# Model 2: two-dimensional spatial model with correlation in one direction&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  sp.asr &amp;lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally&#10;specified as this is the default.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;# model 3: two-dimensional spatial model, error structure in both direction&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; sp.asr &amp;lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  &#10; data = nin89)&#10;sp.asr &amp;lt;- asreml(yield ~ Variety, random = ~ units, &#10; rcov = ~ ar1(Column):ar1(Row), data = nin89)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;similar to above model  2, however the correlation is two direction - autoregressive one. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. &lt;strong&gt;&lt;em&gt;Even if the bouty of +50 can stimulate to develop such package will be of great help !&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;See MAYSaseen has provided output from each model and data  (as answer)  for comparision.&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Edits: &#10;The following is suggestion I received in mixed model discussion forum:&lt;/em&gt;&lt;/strong&gt;&#10;&quot; You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem.&quot; &lt;/p&gt;&#10;" CommentCount="16" CreationDate="2011-11-01T02:25:59.023" FavoriteCount="4" Id="18709" LastActivityDate="2013-04-11T05:19:03.177" OwnerDisplayName="John " OwnerUserId="7244" PostTypeId="1" Score="9" Tags="&lt;r&gt;" Title="lme4 or other open source R package code equivalent to asreml-R" ViewCount="2346" />
  
  <row Body="&lt;p&gt;This is actually a quote that (unintendedly) happens to be a joke:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;&lt;em&gt;Every American should have above average income, and my Administration is going to see they get it.&lt;/em&gt;&quot; (Bill Clinton on campaign trail)&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2011-11-21T16:45:26.710" CreationDate="2011-11-21T16:45:26.710" Id="18719" LastActivityDate="2011-11-21T16:45:26.710" OwnerUserId="6129" ParentId="1337" PostTypeId="2" Score="31" />
  <row Body="&lt;p&gt;This has been answered on MO by Pietro Majer &lt;a href=&quot;http://mathoverflow.net/questions/80084/proving-a-sequence-of-integrals-increases-iterated-minimax-distributions&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-21T17:33:45.613" Id="18723" LastActivityDate="2011-11-21T17:33:45.613" OwnerUserId="5471" ParentId="17022" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Bootstrapping is done to get a more robust picture of the &lt;em&gt;sampling distribution&lt;/em&gt; than that which is assumed by large sample theory.  When you bootstrap, there is effectively no limit to the number of `bootsamples' you take; in fact you get a better approximation to the sampling distribution the more bootsamples you take.  It is common to use $B=10,000$ bootsamples, although there is nothing magical about that number.  Furthermore, you don't run a test on the bootsamples; you have an estimate of the sampling distribution--use it directly.  Here's an algorithm:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;take a bootsample of one data set by sampling $n_1$ boot-observations with replacement.  [Regarding the comments below, one relevant question is what constitutes a valid 'boot-observation' to use for your bootsample.  In fact, there are several legitimate approaches; I will mention two that are robust and allow you to mirror the structure of your data:  When you have observational data (i.e., the data were sampled on all dimensions, a boot-observation can be an ordered n-tuple (e.g., a row from your data set).  For example, if you have one predictor variable and one response variable, you would sample $n_1$ $(x,y)$ ordered pairs.  On the other hand, when working with experimental data, predictor variable values were not sampled, but experimental units were assigned to intended levels of each predictor variable.  In a case like this, you can sample $n_{1j}$ $y$ values from within each of the $j$ levels of your predictor variable, then pair those $y$s with the corresponding value of that predictor level.  In this manner, you would not sample over $X$.]&lt;/li&gt;&#10;&lt;li&gt;fit your regression model and store the slope estimate (call it $\hat\beta_1$)&lt;/li&gt;&#10;&lt;li&gt;take a bootsample of the other data set by sampling $n_2$ boot-observations with replacement&lt;/li&gt;&#10;&lt;li&gt;fit the other regression model and store the slope estimate (call it $\hat\beta_2$)&lt;/li&gt;&#10;&lt;li&gt;form a statistic from the two estimates (suggestion: use the slope difference $\hat\beta_1-\hat\beta_2$)&lt;/li&gt;&#10;&lt;li&gt;store the statistic and dump the other info so as not to waste memory&lt;/li&gt;&#10;&lt;li&gt;repeat steps 1 - 6, $B=10,000$ times&lt;/li&gt;&#10;&lt;li&gt;sort the bootstrapped sampling distribution of slope differences&lt;/li&gt;&#10;&lt;li&gt;compute the % of the bsd that overlaps 0 (whichever is smaller, the right tail % or the left tail %)&lt;/li&gt;&#10;&lt;li&gt;multiply this percentage by 2&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The logic of this algorithm as a statistical test is fundamentally similar to classical tests (e.g., t-tests) but you are not assuming the the data or the resulting sampling distributions have any particular distribution.  (For example, you are not assuming normality.)  The primary assumption you are making is that your data are representative of the population you sampled from / want to generalize to.  That is, the sample distribution is similar to the population distribution.  Note that, if your data are not related to the population you're interested in, you are flat out of luck.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Some people worry about using, e.g., a regression model to determine the slope if you're not willing to assume normality.  However, this concern is mistaken.  The Gauss-Markov theorem tells us that the estimate is unbiased (i.e., centered on the true value), so it's fine.  The lack of normality simply means that the true sampling distribution may be different from the theoretically posited one, and so the p-values are invalid.  The bootstrapping procedure gives you a way to deal with this issue.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Two other issues regarding bootstrapping:  If the classical assumptions are met, bootstrapping is less efficient (i.e., has less power) than a parametric test.  Second, bootstrapping works best when you are exploring near the center of a distribution:  means and medians are good, quartiles not so good, bootstrapping the min or max necessarily fail.  Regarding the first point, you may not need to bootstrap in your situation; regarding the second point, bootstrapping the slope is perfectly fine.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-21T18:09:03.647" Id="18726" LastActivityDate="2011-11-25T17:19:43.110" LastEditDate="2011-11-25T17:19:43.110" LastEditorUserId="7290" OwnerUserId="7290" ParentId="18707" PostTypeId="2" Score="15" />
  
  
  
  <row Body="&lt;p&gt;I'm really not sure what the answer would be in the absence of crossvalidation. But if we are crossvalidating, and we find that, say, one ethnic group out of 6 is substantially different from the others wrt Y, I can't seem to see anything wrong with using only that group's dummy variable in the followup equation.  If membership/nonmembership in that group, and none other, is helping to predict the outcome (or to explain it, for that matter), why gummy up the equation with a bunch of unhelpful predictor dummies, which would only figure to add noise to the prediction?  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-22T02:13:39.357" Id="18751" LastActivityDate="2011-11-22T02:44:20.483" LastEditDate="2011-11-22T02:44:20.483" LastEditorUserId="2669" OwnerUserId="2669" ParentId="18745" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;They have 12 cards, there are 46 unseen cards...  I like 12/46 = 26%.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-22T08:29:24.383" Id="18759" LastActivityDate="2011-11-22T08:29:24.383" OwnerUserId="6369" ParentId="18758" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Feel free to direct me to where an answer can be found, but I've banged my head against this for a while and turn to you good people:&lt;/p&gt;&#10;&#10;&lt;p&gt;Scenario:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a bunch of non-exclusive two-dimensional data (location data) for several thousand unique observations of independent subjects:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Subject1, CategoryA, TypeB, two-dimensional_distribution&#10;Subject2, CategoryB, TypeB, two-dimensional_distribution&#10;Subject3, CategoryA, TypeA, two-dimensional_distribution&#10;etc...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The two dimensional distribution is quite simple containing only three possible x and y coordinates with presence(1)/absence(0) for each. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;C----0-----1-----1&#10;B----1-----1-----0&#10;A----1-----0-----0&#10;-----X-----Y-----Z&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I combine all the obs, the fallout is something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;C---15%---62%---25%&#10;B---10%---35%---12%&#10;A----9%---11%----7%&#10;-----X-----Y-----Z&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I go about performing an ANOVA/MANOVA to look at the influence of &quot;category&quot; and &quot;type&quot; on the distributions and determine whether there is a significant difference in distributions among groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tricky bit:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The frequency distributions don't equal to 100% (as you each observation can have presence in more than one location.)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Ultimately I would like to run this through R - so if you have any ideas in that direction... &lt;/p&gt;&#10;&#10;&lt;p&gt;Apologies if this a totally pedestrian question. Thanks for your help.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-22T14:27:39.750" Id="18770" LastActivityDate="2011-11-23T01:21:03.690" OwnerUserId="7534" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;" Title="ANOVA with dependent variable(s) as 2-dimensional distribution? Is it possible?" ViewCount="270" />
  
  <row Body="&lt;p&gt;Hein, &lt;/p&gt;&#10;&#10;&lt;p&gt;there are a lot of tools and libs with the functionality available.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which to choose depends whether you would like to use a gui for your work or if you would like to embed it in some other program.&lt;/p&gt;&#10;&#10;&lt;p&gt;Standalone Data mining tools (there are ohters like WEKA with Java interface):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Rapid Miner&lt;/li&gt;&#10;&lt;li&gt;Orange&lt;/li&gt;&#10;&lt;li&gt;Rattle gui for R&lt;/li&gt;&#10;&lt;li&gt;KNIME&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Text based:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;GNU R&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Libs:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Scikit for Python&lt;/li&gt;&#10;&lt;li&gt;Mahout on Hadoop&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If you know a programming language well enough I would use a lib for that language or give R a try. If not you may try one of the tools with gui.&lt;/p&gt;&#10;&#10;&lt;p&gt;A tree example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# we are using the iris dataset&#10;data(iris)&#10;&#10;# for our tree based model we use the rpart package&#10;# to download it type install.packages(&quot;rpart&quot;)&#10;library(rpart)&#10;&#10;# Building the tree&#10;fit &amp;lt;- rpart(Species ~ Petal.Length + Petal.Width, method=&quot;class&quot;, data=iris)&#10;&#10;# Plot the tree&#10;plot(fit)&#10;text(fit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As suggested the analysis with R requires you to code yourself, but you will find a package for most classification tasks which will work out of the box. An overview can be found here &lt;a href=&quot;http://cran.r-project.org/web/views/MachineLearning.html&quot; rel=&quot;nofollow&quot;&gt;Machine Learning Task View&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To get started with RapidMinder you should have a look at Youtube. There are some screencasts, even for decision trees.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-22T15:42:06.457" Id="18779" LastActivityDate="2011-11-22T17:56:25.940" LastEditDate="2011-11-22T17:56:25.940" LastEditorUserId="7284" OwnerUserId="7284" ParentId="18774" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Yes, the Central Limit Theorem tells us this is true. So long as you avoid extremely heavy-tailed traits, non-Normality presents no problems in moderate-to-large samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a helpful review paper;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546&quot; rel=&quot;nofollow&quot;&gt;http://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The Wilcoxon test (mentioned by others) can have terrible power when the alternative is not a location shift of the original distribution. Furthermore, the way it measures differences between distributions is not transitive.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-11-22T18:51:48.433" Id="18790" LastActivityDate="2011-11-22T18:51:48.433" OwnerUserId="7497" ParentId="9573" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;(I have just a bare understanding of AdaBoost, please correct/downvote if I'm wrong!)&lt;/p&gt;&#10;&#10;&lt;p&gt;You didn't write anything about the performance of your classifier. Did you actually test it? If it does well, that tells me that your &quot;weak&quot; learners can easily learn the statistics and therefore already perform well on their own, thus the broad consensus among them. As you observed, AdaBoost doesn't gain you much in this case. What you could do is&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Use just one weak learner as a strong learner&lt;/li&gt;&#10;&lt;li&gt;Use computationally cheaper and thus weaker learners and then use AdaBoost to get a faster strong learner from them.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2011-11-22T19:38:50.570" Id="18794" LastActivityDate="2011-11-22T19:38:50.570" OwnerUserId="4916" ParentId="18773" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Here's some;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Cleveland and McGill (1984, JASA) Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods&lt;/li&gt;&#10;&lt;li&gt;Cleveland and McGill (1987, JRSSA) Graphical Perception: The Visual Decoding of Quantitative Information on Graphical Displays of Data&lt;/li&gt;&#10;&lt;li&gt;Lewandowsky and Spence (1989) Discriminating Strata in Scatterplots&lt;/li&gt;&#10;&lt;li&gt;Spence and Lewandowsky (1991) Displaying Proportions and Percentages&lt;/li&gt;&#10;&lt;li&gt;Spence Kutlesa and Rose (1999) Using Color to Code Quantity in Spatial Displays&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Ask the Google for the full references&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-22T22:28:45.797" Id="18802" LastActivityDate="2011-11-22T22:28:45.797" OwnerUserId="7497" ParentId="18431" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;You are supposed to form your hypotheses before seeing any of the actual data.  These hypotheses come from some sort of conceptual frame work.  Your best bet may be to form a hypothesis on a topic of interest to you and then try to find a data set to test that hypothesis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-23T00:36:54.257" Id="18807" LastActivityDate="2011-11-23T00:36:54.257" OwnerUserId="2310" ParentId="18803" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The Poisson process involves a &quot;memoryless&quot; waiting time until arrival of the next customer.  Suppose the average time from one customer to the next is $\theta$.  A memoryless continuous probability distribution until the next arrival is one in which the probability of waiting an additional minute, or second, or hour, etc., until the next arrival, does not depend on how long you've been waiting since the last one.  That you've already waited five minute since the last arrival does not make it more likely that a customer will arrive in the next minute, than it would be if you'd only waited 10 seconds since the last arrival.&lt;/p&gt;&#10;&#10;&lt;p&gt;This automatically implies that the waiting time $T$ until the next arrival satisfies $\Pr(T&amp;gt;t) = e^{-t/\theta}$.  I.e., it's an exponential distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;And that in turn can be shown to imply that the number $X$ of customers arriving during any time interval of length $t$ satisfies $\Pr(X=x) = \dfrac{e^{-t/\theta} (t/\theta)^x}{x!}$, i.e. it has a Poisson distribution with expected value $t/\theta$.  Moreover, it implies that the numbers of customers arriving in non-overlapping time intervals are probabilistically independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;So memorylessness of watinging times leads to the Poisson process.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-23T03:56:45.953" Id="18822" LastActivityDate="2011-11-23T03:56:45.953" OwnerDisplayName="user11667" OwnerUserId="5176" ParentId="18821" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I’m somewhat lost as to how to approach a problem I have and was hoping someone can suggest the most appropriate method.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a data set consisting of concentration values for 8 molecules. These are measured on approx. 230 samples from approx. 70 patients. For many patients there are multiple samples taken at different dates. However, this isn’t a controlled timecourse but independent (presumably, but probably not quite) events for each patient. As such the number and time of samples for each patient differ significantly so this data would not fit into a nice balanced repeated measures model.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each sample we can assign 2 clinical phenotypes, let’s call them A and B, which are presumably independent (although probably not entirely). So we could assign 4 categories based on A or B, but we could also sub classify both A and B based on severity. For now, I think binary categories would be sufficient. Some preliminary analysis someone else did on the data suggests that these molecules will not be able to distinguish A, but may be able to distinguish B, independent of A. An additional complication is that patients come from 2 different institutes (about half from each).  Also, the data appear to deviate fairly significantly from normality, even after a log transformation. Most of the variances (comparing B+ vs B-) are similar after log transformation. &lt;/p&gt;&#10;&#10;&lt;p&gt;The goal is to determine if any of the 8 molecules, or some combination of them, can predict clinical phenotype B. Ideally, predict A and B, but B is the more important one for this study.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My initial thought is to do some kind of mixed model analysis, two-way across A and B, with institute and patient as random effects. This would help determine if A and B are truly independent and if there is any significant confounding effect from institute. This should probably use a non-parametric test.&lt;/p&gt;&#10;&#10;&lt;p&gt;So does this make sense (my confusion suggests otherwise), and if so, what exactly is the test I should be using (I’m using R for most of my analysis)? If not, what is a better way to look at this data?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Response to Chi's questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;When I plot the molecule concentrations they are quite skewed (even after a log2 transformation) and Shapiro-Wilk Normality Tests on each molecule (B- vs B+ samples) fails to support normal distribution. Therefore, I felt parametric tests would not be justified due to deviation from an assumed normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, I've read that with sample sizes &gt; 30, due to central limit theory (something I'll have to refresh myself on...) parametric tests are still applicable even if the underlying distribution is non-normal. In my case there are around 170 B- and 60 B+ samples in total (although they are not from individual patients) so perhaps parametric tests would be the best choice? This is something I'm still confused about. &lt;/p&gt;&#10;&#10;&lt;p&gt;I did do, as a preliminary test, both t-test and Mann-Whitney tests between B- and B+ for each molecule, followed by a Benjamini &amp;amp; Hochberg FDR adjustment (Which raises another question: is B &amp;amp; H appropriate for such a small number of tests?). With a cutoff of q &amp;lt; 0.05, I got a significant difference for 3 molecules from the Mann-Whitney test, whereas only 2 of those were sig from the t-test of log2 transformed data. Not certain what this means. Thanks!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Some updates:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed there are also some identical concentration across several samples for some molecules. I don't fully understand the source of this artifact, but was told had something to do with data validation. You might see this with readings that are below the sensitivity of the assay, but I'm seeing identical values that are not all the lowest value. On a Q-Q plot these show up as horizontal lines and I'm not sure how this will affect the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to set up a linear mixed effects model in R but just could not get it to work. Seems like the 2 most used packages are nlme or lme4. I can't figure out the syntax of the formulas and so far despite lots of searching haven't found a thorough explanation that I understand.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets call my variables phen.a and phen.b for the independent variables A and B, above, instit and patient for institute and patient id. instit and patient are factors, phen.a dn phen.b are logical but could be changed to factors, especially we decide to add severity grades. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I think I should be doing is a mixed effects model looking at:&#10;fixed effects: phen.a, phen.b, interaction:phen.a x phen.b&#10;random effects: instit, patient&lt;/p&gt;&#10;&#10;&lt;p&gt;If someone could provide the formulas I need that would be extremely helpful. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the meantime, what I've tried, since I could get it to work, is an ANOVA looking at the 2 independent variables, their interactions, and additional interactions to phen.b, which is of particular interest. The formula I used in the linear model was:&lt;/p&gt;&#10;&#10;&lt;p&gt;concentration ~ phen.a*phen.b + phen.b:instit + phen.b:patient&lt;/p&gt;&#10;&#10;&lt;p&gt;where concentration is the log2 of the concentration -- repeated for each of the 8 molecules. I then did a p.adjust(..., method=&quot;FDR&quot;) on each of the 5 resulting p values to correct for the 8 comparisons. &lt;/p&gt;&#10;&#10;&lt;p&gt;This resulted in 1 molecule being quite significant for phen.b (with no sig for phen.a and no interactions), and a second molecule sig for phen.a, with an interaction between phen.b:instit. Not sure how to interpret this interaction on a non-significant independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;So now another question is whether this model is valid and if the interactions I chose make sense. I'm also wondering since we are essentially looking at 2 x 2 categories if linear models are appropriate or if there is some kind of chi2 type test that would be better (although the dependent variable is continuous). &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks again!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Another update:&#10;So I managed to learn how to use the lme function and tried the model:&lt;/p&gt;&#10;&#10;&lt;p&gt;fixed = concentration ~ phen.a * phen.b * instit&lt;/p&gt;&#10;&#10;&lt;p&gt;random = ~1 | patient&lt;/p&gt;&#10;&#10;&lt;p&gt;to find any effect from phen.a or phen.b and determine if such effects are confounded by instit, using the random patient effect to account for differences in intra vs inter patient variation.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as I played with the data I noticed that while the concentration is a continuous variable there are 1 or 2 particular values that show up for many of the samples, sometimes accounting for half or more of all samples, and completely skewing the distribution. It turns out that these values actually represent the minimal detectable concentrations by the assays (in some cases 2 assays were used, accounting for the 2 different values that show up repeatedly). The way the data were coded, the minimal detectable concentration is used for any sample for which that molecule could not be detected. So that value really represents: &amp;lt;= x. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I deal with this type of minimal cutoff? Is it fair to leave them as is, for which I found a couple of significant results? &lt;/p&gt;&#10;&#10;&lt;p&gt;I tried just excluding these values but then I lose all significant results. I don't think that's valid, though, because the values aren't missing, they really represent a molecule that is either not present or present at very small concentrations. If there is a predominance of very low concentration correlated with a phenotype that is a very important observation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-11-23T17:50:54.400" FavoriteCount="2" Id="18836" LastActivityDate="2011-12-02T18:43:07.420" LastEditDate="2011-12-02T18:43:07.420" LastEditorUserId="7570" OwnerUserId="7570" PostTypeId="1" Score="4" Tags="&lt;mixed-model&gt;" Title="How to predict a binary outcome with unbalanced repeated measures data?" ViewCount="404" />
  
  <row Body="&lt;p&gt;... first of all you must to do it by iterations, and under some informative criterion, like AIC AICc BIC Cp; because you can get an &quot;ideal&quot; fit, if number of knots K = number od data  points N, ok. &#10;... first put K = 0; estimate L = K + 1 regressions, calculate AICc, for instance; &#10;then assume minimal number of data points at a separate segment, say L = 3 or L = 4, ok&#10;... put K = 1; start from L-th data as the first knot, calculate SS or MLE, ... and step by step the next data point as a knot, SS or MLE, up to the last knot at the N - L data; choose the arrangement with the best fit (SS or MLE) calculate AICc ... &#10;... put K = 2; ... use all previous regressions (that is their SS or MLE), but step by step divide a single segment  into all possible parts ... choose the arrangement with the best fit (SS or MLE) calculate AICc ... if the last AICc occurs greater then the previous one: stop the iterations ! This is an optimal solution under AICc criterion, ok&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-23T21:19:49.163" Id="18846" LastActivityDate="2011-11-23T21:19:49.163" OwnerUserId="7575" ParentId="18468" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I have tested 100 bacterial isolates for resistance to 6 antibiotics (tetracycline, erythromycin, kanamycin, streptomycin, gentamicin, chloramphenicol). The result for any antibiotic could be either sensitive (1) or resistant (0). &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I determine if there is any significant association between resistance to various antibiotics (I can frequently find isolates resistant to three or more antibiotics but how to know if this is significant)? I could use Chi-square to compare between any two antibiotics but how to make multiple comparisons and will this make any difference except saving time. &lt;/p&gt;&#10;&#10;&lt;p&gt;Scientists in the papers used Chi-square with Bonferroni adjustment. I am using Statistica for Windows software.&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-23T22:27:44.687" Id="18849" LastActivityDate="2013-01-09T19:31:23.207" LastEditDate="2012-05-14T07:09:29.030" LastEditorUserId="88" OwnerUserId="7576" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;multiple-comparisons&gt;" Title="How to test for association between proportions with multiple variables?" ViewCount="304" />
  <row Body="&lt;p&gt;You should consider the two terms to be synonymous.  Although they are used in slightly different ways, and come from different traditions within statistics ('interaction' is associated more with ANOVA, and 'moderator variable' is more associated with regression), there is no real difference in the underlying meaning.  In fact, statistics is littered with synonymous terms that come from different traditions that mean the same thing.  Should we call our X variables 'predictor variables', 'explanatory variables', 'factors', 'covariates', etc.?  Does it matter?  (No, not really.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The way to think about what an interaction is, is that if you were to explain your findings to someone you would use the word 'depends'.  I will make up a story using your variables (I have no way of knowing if this is accurate or even plausible):  Lets say someone asks you, &quot;if people research a product, do they purchase it?&quot;  You might respond, &quot;Well, it depends.  For men, if they research a product, they typically end up buying one, but women enjoy looking at and thinking about products for its own sake; often, a woman will research a product, but have no intention of buying it.  So, the relationship between researching a product and buying that product depends on sex.&quot;  In this story, there is an interaction between product research and sex, or sex moderates the relationship between research and purchasing.  (Again, I don't know if this story is remotely correct, and I hope no one is offended by it.  I only use men and women because that's in the question.  I don't mean to push any stereotypes.)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-23T22:38:47.470" Id="18850" LastActivityDate="2011-11-23T22:38:47.470" OwnerUserId="7290" ParentId="18848" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;(You asked whether there is a statistical reason:  I doubt it, but I'm guessing about other reasons.)  Would there be cries of &quot;moving the goalpost&quot;?  Students usually like to know when taking a test just how much each item is worth.  They might be justified in complaining upon seeing, for example, that some of their hard-worked answers didn't end up counting much.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many teachers and professors use unsystematic, subjective criteria for scoring tests.  But those who do use systems are probably wary about opening those systems up to specific criticism -- something they can largely avoid if hiding behind more subjective approaches.  That might explain why item analysis and IRT are not used more widely than they are.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-24T03:23:06.197" Id="18864" LastActivityDate="2011-11-24T03:23:06.197" OwnerUserId="2669" ParentId="18862" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;You are mixing three unrelated concepts: empirical Bayes, bootstrapping and MCMC. The first two items are statistical procedures, while the third one is a simulation procedure. Thus, you cannot take a statistical procedure to reply to a simulation question and vice-versa. Here are some elements of answer:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If you have an MCMC outcome from an empirical Bayes approach, this means you have simulated a sample of parameters for a given (posterior) distribution, $\pi_1$. If you want to get the answer for the &quot;full Bayesian treatment&quot;, it means you are changing the target (posterior) distribution, to $\pi_2$. (The fact that one is empirical and the other fully Bayes does not matter at this level.) Therefore, if you weight your original sample $\theta_1,\ldots,\theta_T$ with importance weights &#10;$$
  
  <row AcceptedAnswerId="18919" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Score_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; tells us that the score plays an important role in the Cramér–Rao inequality. It also phrases out the definition:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$V = \frac{\partial}{\partial \theta} \log{L(\theta; X)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I cannot find an &lt;strong&gt;intuitive&lt;/strong&gt; explanation of what this quantity expresses. Obviously, it somehow measures how a small change of $\theta$ will affect the log-likelihood of the observed data $X$, but what exactly does that mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;The wikipedia article also mentions that the expected value $\mathbb{E} [V \mid \theta] = 0$. Can this be interpreted somehow?&lt;/p&gt;&#10;&#10;&lt;p&gt;Going a bit further, in class we were told that the Fisher information (for which I have no intuitive understanding either) is $I(\theta) = \mathbb{E} [V^2 \mid \theta]$. Combined with $\mathbb{E} [V \mid \theta] = 0$ that would imply $I(\theta) = \text{Var}[V]$, is this correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: This is not homework.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-24T12:17:33.660" FavoriteCount="1" Id="18876" LastActivityDate="2011-11-25T08:04:45.953" LastEditDate="2011-11-25T05:03:17.803" LastEditorUserId="5594" OwnerUserId="4916" PostTypeId="1" Score="6" Tags="&lt;intuition&gt;&lt;probability&gt;" Title="What is the intuition behind the score function?" ViewCount="666" />
  
  
  <row Body="&lt;p&gt;I think that you might be confusing an extra-sum-of-squares F-test with a likelihood ratio test. Although, both are used to compare two models.&lt;/p&gt;&#10;&#10;&lt;p&gt;A likelihood ratio statistic, denoted by $\Lambda$, is given by &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Lambda = \frac{L\text{(reduced model})}{L(\text{full model})}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking $-2\log\Lambda$ produces a statistic that has $\chi^2_{d.f(\text{reduced model})-d.f(\text{full model})}$ distribution. That is to say that taking $-2\log$ of the $\Lambda$ gives you a $\chi^2$ distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have not used SAS so I cannot comment on the output, but I hope that I have been able to answer your question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: that $\Lambda$ is equivalent to your L&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Janne: For linear regression you could use either the likelihood ratio test or the extra-sum-squares F-test and you should end up with the same p-value. Despite, this they are not the same thing. &lt;/p&gt;&#10;&#10;&lt;p&gt;As has been mentioned above the likelihood ratio test produces a statistic that has $\chi^2_{d.f(\text{reduced model})-d.f(\text{full model})}$ distribution. Where as an extra-sum-of-squares F-test, given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F = \frac{(SSR_{\text{reduced model}}-SSR_{\text{full model}})/d.f_{\text{reduced model}} - d.f_{\text{full model}}}{\hat{\sigma}^2_\text{full model}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;producing a statistic that has $F_{d.f(\text{reduced model})-d.f(\text{full model}),d.f(\text{full model})}$ distribution.&#10;Where SSR is the sum of squared residuals and $\hat{\sigma}^2$ is our standard estimate.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-24T17:32:55.240" Id="18895" LastActivityDate="2011-11-25T08:21:10.863" LastEditDate="2011-11-25T08:21:10.863" LastEditorUserId="7045" OwnerUserId="7045" ParentId="18885" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;Basically, it sounds like you want to bootstrap your data:&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A good (and relatively cheap) reference is:&#10;&quot;Bootstrap Methods and Their Applications&quot; by A. C. Davison and D. V. Hinkley (1997, CUP). &lt;/p&gt;&#10;&#10;&lt;p&gt;which has an associated R package, &quot;boot&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;BUT... there's a lot that can go wrong in bootstrapping and it's very easy to get misleading results if you don't know what you're doing (which, to be blunt, sounds likely).  It would help a lot if you explained exactly what the problem is that you're trying to solve.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-24T19:34:33.683" Id="18902" LastActivityDate="2011-11-24T19:34:33.683" OwnerUserId="7591" ParentId="18699" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="18918" AnswerCount="2" Body="&lt;p&gt;&lt;em&gt;I am not a statistician, so please excuse my lack of statistics knowledge/terminology.&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I have bunch of network nodes that I want to run cluster analysis on and identify clusters. So as far as I understand, I can follow the following steps to run a hierarchical agglomerative analysis (HAC):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Identify variables&lt;/li&gt;&#10;&lt;li&gt;Define a distance function&lt;/li&gt;&#10;&lt;li&gt;Run the algorithm to join closer clusters and create one big cluster&lt;/li&gt;&#10;&lt;li&gt;Cut the dendrogram tree at the height that makes meaningful clusters based on context&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My question is related to the second step even though it is not yet clear for me how I am going to do the last step.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data I want to analyse is bunch of computer network nodes which are down (not responding). I have the following information for each network node:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;location (longitude, latitude)&lt;/li&gt;&#10;&lt;li&gt;time node went down&lt;/li&gt;&#10;&lt;li&gt;network provider&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;These are the most relevant information I believe I have to take into consideration for my clustering. Basically I want to cluster the nodes that went down probably because of the same reason in a region. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example if bunch of nodes went down at about the same time and physically they are close to each other and they have the same provider, so probably they fall into the same cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the question is how do I derive my distance function and include all these variables in it such that it would make sense? In other words &lt;strong&gt;what is the mechanism to derive a distance function based on multiple variables?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also as you notice the variables are of different types. So, in the distance function should I take care of this by considering &lt;strong&gt;Gower's coefficient of similarity&lt;/strong&gt;? How?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any examples or suggestion regarding whether I am in the right direction or not can be very helpful too.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-25T02:35:10.507" Id="18913" LastActivityDate="2013-03-02T09:34:12.273" LastEditDate="2011-11-25T10:13:41.870" LastEditorUserId="930" OwnerUserId="7573" PostTypeId="1" Score="2" Tags="&lt;clustering&gt;&lt;multilevel-analysis&gt;&lt;distance-functions&gt;&lt;dendrogram&gt;&lt;hac&gt;" Title="How to derive a distance function based on multiple variables for cluster analysis?" ViewCount="1196" />
  
  <row AcceptedAnswerId="18920" AnswerCount="2" Body="&lt;p&gt;Please first have a look at the following little problem:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There are two indistinguishable light bulbs A and B. A flashes red&#10;  light with prob .8 and blue with prob .2; B red with .2 and blue .8.&#10;  Now with .5 prob you are presented with either A or B. You're supposed&#10;  to observe its flash color to make a best guess (maximizing&#10;  probability of correct guessing) which bulb it is. Before you start to&#10;  make observations, however, you must decide how many times you want to&#10;  observe it (say n times, then you observe it flashing n times and make&#10;  your guess). Suppose flashes are independent.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Intuitively, one would think the more observations one makes, the better one's chances will be. Curiously though, it is easy calculation to show that n=2 doesn't improve upon n=1, and n=4 doesn't improve upon n=3. I didn't go further but I speculate n=2k doesn't improve upon n=2k-1. I'm not able to prove it for the general case. But is it true? If so, how can one intuitively understand the result? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-25T07:35:37.263" Id="18917" LastActivityDate="2011-11-26T08:24:56.407" LastEditDate="2011-11-25T15:24:30.663" LastEditorUserId="919" OwnerUserId="7600" PostTypeId="1" Score="8" Tags="&lt;bayesian&gt;&lt;binomial&gt;" Title="Light bulb color problem" ViewCount="289" />
  <row Body="&lt;p&gt;It might be easy to make the mistake (if you are careless) and interpret it as stratified sampling instead of quota sampling. As whuber pointed out in the comments below, stratified sampling requires the the partitioning of the population into groups BEFORE sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;This is known as stratified sampling. It does not make it &quot;less random&quot; than simple random sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;For smaller samples, it is actually more appropriate. With simple random sampling, your samples are more likely to over-represent a category of the population.&quot;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-11-25T11:45:29.027" Id="18931" LastActivityDate="2011-11-25T23:28:29.370" LastEditDate="2011-11-25T23:28:29.370" LastEditorUserId="6845" OwnerUserId="6845" ParentId="18929" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have carried out a study on change of call volume over a year (oct 07-mar 09).  What statistical test on SPSS software can I use to show the difference in call volume on a day by day basis and on a monthly basis?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-11-25T14:30:08.520" Id="18941" LastActivityDate="2012-03-02T17:44:52.507" LastEditDate="2011-12-06T00:54:15.170" LastEditorUserId="183" OwnerUserId="7605" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;spss&gt;" Title="How to test changes in call volume over time?" ViewCount="353" />
  <row Body="&lt;p&gt;I doubt there is much that will help with the idiosyncratic, jagged shape of temperature, but with rainfall there are data-transformations you can try.  You can search for that tag on this site and you'll come up with many useful posts.  (Note that &quot;normalization&quot; is defined differently from &quot;data-transformation&quot; and it is the latter that you want in order to create something closer to a bell curve.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;The first thing I would try would be to take the square root of each value.  The result may still be skewed, but probably a lot less so.  In other words, it would probably not &quot;pass&quot; a normality test (those are pretty unreliable anyway), but it would likely be a more workable variable for most purposes.  As many on this site have reminded us, in multivariate analysis it is typically the residuals, rather than the univariate distributions, that must be close to normal in order to satisfy the normality assumption.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-25T14:42:55.577" Id="18942" LastActivityDate="2011-11-25T15:04:42.103" LastEditDate="2011-11-25T15:04:42.103" LastEditorUserId="2669" OwnerUserId="2669" ParentId="18937" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The data you have is sometimes called repeated cross section data. &#10;There are techniques that can be used with this kind of data. You can have a look at the Difference-in-Difference estimators (and their &quot;matching&quot; variants) presented by &lt;a href=&quot;http://ideas.repec.org/a/ifs/fistud/v21y2000i4p427-468.html&quot; rel=&quot;nofollow&quot;&gt;Blundell and Costa Dias (2000)&lt;/a&gt; as well as &lt;a href=&quot;http://ideas.repec.org/a/eee/econom/v125y2005i1-2p305-353.html&quot; rel=&quot;nofollow&quot;&gt;Smith and Todd (2005)&lt;/a&gt;. There are some assumptions to be fulfilled. It's your role to check this and to argue that these estimators can be applied.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For a really complete picture, you can have a look at &lt;a href=&quot;http://ideas.repec.org/h/eee/labchp/3-31.html&quot; rel=&quot;nofollow&quot;&gt;Heckman, Lalonde and Smith (2001)&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-25T15:09:10.263" Id="18944" LastActivityDate="2011-11-25T15:09:10.263" OwnerDisplayName="user5644" ParentId="18585" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I don't actually know R package, but if you were able to decrease the cost of &lt;strong&gt;False Positives&lt;/strong&gt; it should work. This is also the rationale of MetaCost by Domingos, that is implemented in WEKA. However, increasing the &lt;em&gt;sensitivity&lt;/em&gt; you are going to decrease the &lt;em&gt;specificity&lt;/em&gt;, naturally. From the information retrieval point of view, as long as you increase the &lt;em&gt;recall&lt;/em&gt; the &lt;em&gt;precision&lt;/em&gt; will decrease.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because Random Forest use Decision Trees as base classifiers and they can output probabilities, you can decrease the cut-off that enable a tree to classify a record as positive. This will make you Random Forest more sensitive but less precise. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-25T16:09:57.670" Id="18947" LastActivityDate="2011-11-25T16:09:57.670" OwnerUserId="2719" ParentId="18938" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;It is quite common to discuss what metrics to be used for a model/algorithm/method. But when it comes to metrics of datasets, it is much less discussed.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder, what are the general metrics for datasets? That is to say, how does one quantitively measure if a dataset is good or not for its purpose?&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand, there may not be too many general metrics, given the purpose and nature of datasets differ vastly. So, more specifically, I am interested in the follow few cases:&lt;/p&gt;&#10;&#10;&lt;p&gt;what are the metrics for 1) text datasets, 2) classification datasets, 3) unbalanced classification datasets?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any knowledge, insights, interesting ideas are welcome and appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-25T18:01:30.127" Id="18950" LastActivityDate="2011-12-03T15:12:15.013" OwnerUserId="6169" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;" Title="Metrics of datasets?" ViewCount="224" />
  
  <row Body="&lt;p&gt;The Kalman filter is a very simple version of a hidden markov chain in which the states are described as normal distributions. There are several tutorials and introductory texts dealing with kalman filters (easily found through google). &lt;/p&gt;&#10;&#10;&lt;p&gt;If this is too simplistic, there are several extensions (Extended Kalman Filter, switching Markov models, etc.). It really depends on the specific problem in which you are interested in applying them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-25T22:38:16.317" Id="18962" LastActivityDate="2011-11-25T22:38:16.317" OwnerUserId="1913" ParentId="18928" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;These power point &lt;a href=&quot;http://www.integrativestatistics.com/typesofsurveyinfo.pptx&quot; rel=&quot;nofollow&quot;&gt;exerpts&lt;/a&gt; have some info to supplement what jthetzel and Max Gordon have given.  It's oriented toward survey data, and it's not rigorous or formal, but then if you wanted that type of answer you'd probably be looking in textbooks on measurement theory or survey methods.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-26T00:34:50.453" Id="18967" LastActivityDate="2011-11-26T00:34:50.453" OwnerUserId="2669" ParentId="18945" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="19013" AnswerCount="1" Body="&lt;p&gt;I want to use model-based clustering to classify 1,225 time series (24 periods each). I have decomposed these time series using the fast Fourier transform and selected the harmonics that explain at least a threshold percentage of time series variance for all time series in the sample. I want to do model-based clustering on the real and imaginary parts for each transform element of a give time series because it would potentially save me from having to account for temporal autocorrelation in model based clustering across periods of a time series. I know that each complex element of the fast Fourier transform is independent from other elements, but I do not know if the imaginary and real parts of the output for a given output element are independent. I would like to know because if they were, it would allow me to maintain the default assumption of the Mclust package in R for model-based clustering that the variables analyzed have a multivariate Gaussian distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;NOTE: The input is real-valued, and I have converted from a two-sided to a one-sided spectrum by removing redundant frequency elements and multiplying the positive frequencies (other than the mean component) by two per the advice I got from another StackOverflow answer here: &lt;a href=&quot;http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in&quot;&gt;http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2011-11-26T01:37:42.280" FavoriteCount="1" Id="18969" LastActivityDate="2011-11-27T23:54:34.710" LastEditDate="2011-11-27T23:54:34.710" LastEditorUserId="7616" OwnerUserId="7616" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;independence&gt;&lt;fourier-transform&gt;&lt;model-based-clustering&gt;" Title="Are real and imaginary components of frequency element of fft correlated?" ViewCount="765" />
  <row AcceptedAnswerId="18979" AnswerCount="6" Body="&lt;p&gt;Based on this post, &lt;a href=&quot;http://quant.stackexchange.com/questions/111/how-can-i-go-about-applying-machine-learning-algorithms-to-stock-markets&quot;&gt;http://quant.stackexchange.com/questions/111/how-can-i-go-about-applying-machine-learning-algorithms-to-stock-markets&lt;/a&gt;, I want to digest Elements of Statistical Learning. Fortunately it is available for free and I started reading it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't have enough knowledge to understand it. Can you recommend a book that is a better introduction to the topics in the book? Hopefully something that will give me the knowledge needed to understand it?&lt;/p&gt;&#10;&#10;&lt;h3&gt;Related&lt;/h3&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/40808/how-to-get-an-introductory-understanding-of-machine-learning&quot;&gt;How to get an introductory understanding of Machine Learning?&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2011-11-26T03:12:43.130" FavoriteCount="33" Id="18973" LastActivityDate="2013-11-19T02:26:06.940" LastEditDate="2012-10-23T16:26:16.493" LastEditorUserId="919" OwnerUserId="6329" PostTypeId="1" Score="23" Tags="&lt;machine-learning&gt;&lt;references&gt;&lt;learning&gt;&lt;education&gt;" Title="Can you recommend a book to read before Elements of Statistical Learning?" ViewCount="4486" />
  <row AnswerCount="1" Body="&lt;p&gt;Here are four graphs, &lt;/p&gt;&#10;&#10;&lt;p&gt;1, autocorrelation, autocovariance, partial-correlation and cross-correlation calculated from a time series are given.&lt;/p&gt;&#10;&#10;&lt;p&gt;2, The time series&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to do some predictions on them.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are 4 :&lt;/p&gt;&#10;&#10;&lt;p&gt;1, I do not quite understand what these four graphs can tell or show. Can anyone explain?&lt;/p&gt;&#10;&#10;&lt;p&gt;2, Does the four subplots tell if it is appropriate to do forecasting for the time series&lt;/p&gt;&#10;&#10;&lt;p&gt;3, If so, any method to do forecasting?&lt;/p&gt;&#10;&#10;&lt;p&gt;4, I am wondering if particle filter can be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am a little clueless about this problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/K18Aa.png&quot; alt=&quot;descriptive stats&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Gc1ZU.png&quot; alt=&quot;The time series which seems &amp;quot;chaotic&amp;quot;&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-26T04:41:55.797" Id="18976" LastActivityDate="2012-04-01T10:54:48.323" LastEditDate="2011-12-05T02:35:21.230" LastEditorUserId="3382" OwnerUserId="7259" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;self-study&gt;&lt;forecasting&gt;&lt;descriptive-statistics&gt;&lt;particle-filter&gt;" Title="Forecasting a &quot;chaotic&quot; time series" ViewCount="355" />
  <row Body="&lt;p&gt;About the combinatorial question: the proof follows from the identity&#10;$$
&#10;\sum_{i=0}^{k-1} {2k\choose i} p^{2k-i} (1-p)^i = 
&#10;$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-26T08:24:56.407" Id="18983" LastActivityDate="2011-11-26T08:24:56.407" OwnerUserId="7224" ParentId="18917" PostTypeId="2" Score="2" />
&#10;2000 &amp;amp;= \frac{2T}{\chi^2_{2n+2}(1-\alpha)} \\
  <row Body="&lt;p&gt;As rolando2 already pointed out, that also the actor_id is a number, it does not represent a numerical variable. The selection of the correct scale of a variable depends on its meaning. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Level_of_measurement&quot; rel=&quot;nofollow&quot;&gt;this wikipedia page&lt;/a&gt; for an overview of scales.&lt;/p&gt;&#10;&#10;&lt;p&gt;By checking the IMDB-site I got a rough idea what your data may looks like. Roughly said, your data contains of two parts:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;fixed features with exactly one value for each movie (like: Category, Budget etc.). &lt;/li&gt;&#10;&lt;li&gt;varying number of &quot;information-pieces&quot;. This includes actors,&#10;directors (happens), Genre (IMDB has multiple for one film ... like a highlevel tag), Plot keywords etc. etc.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Strategies to model these &quot;information-pieces&quot;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Boolean Feature&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One boolean feature of each value of a field, where the value indicates whether the field is active for the current movie or not. In the actor example this means that you will have a feature for every actor who has ever appeared in a movie (given your data), where &quot;true&quot; means that the actor appeared in the particular movie, &quot;false&quot; not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where this approach does not loses information by summarization, it has cleary the drawback that the number of features will implode.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not recommend this approach as long as you are not planning to model your data with graphs (but this is advanced stuff).&lt;/p&gt;&#10;&#10;&lt;p&gt;Text Mining has to deal with the same issue: Here the information-pieces are words (or tags) and the label is (in general) the category. I am pretty sure you can grab some ideas by checking the work in this area.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Numeric Feature (Summarization)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of given every value a single feature and calculating the impact on this value of the rating, you just calculate the expected rating for all the values of this particular field (in advance) and create a single feature for it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example feature &lt;em&gt;actors&lt;/em&gt;: Tom Cruise and Cameron Diaz played in Movie &quot;Knight and Day&quot;. Now you calculate the average rating of the movies with Tom Cruise, the average rating the movies with Cameron Diaz and then calculate e.g. the average of both values weighted by the number of movies both appeared in respectively. This will be the value for the field &quot;actors&quot; for the Movie &quot;Knight and Day&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The advantage of this method is a small overall number of features. The drawback is that too much summarization can harm the quality of the resulting model. You may want to combine this strategy with the next one (only using the most important actors of a movie, not all).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Nominal Feature (Truncation)&lt;/strong&gt;&#10;Instead of using every value which has appears on a particular movie site, why not restrict the data to the most important ones. You can create features like&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;first_actor,second_actor, ...&lt;/li&gt;&#10;&lt;li&gt;director (one field)&lt;/li&gt;&#10;&lt;li&gt;genre (one field)&lt;/li&gt;&#10;&lt;li&gt;first_keyword, etc..&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;and use simply the value as it is (actor_id in case of first_actor, etc...). In this way the number of features wont explode and you have a good point to start.&lt;/p&gt;&#10;&#10;&lt;p&gt;A clear drawback explained in the actor case: In a Jim Carey Movie, the first actor is important, the second and third ... less. In Ocean's Eleven, the first (do not know, 11 ?) actors are important.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I am pretty sure by working with the data (i.e. putting the unstructured information from IMDB into tabular format) you will have more (domain specific) ideas. The most important part of Data Mining is to prepare the data the right way, so that a solid model only has to &quot;read&quot; the information. After you have find a good data representation, you can deal with finding the best algorithm or algorithm parameters to get from solid/good to a very good model quality.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-26T12:03:33.103" Id="18993" LastActivityDate="2011-11-26T12:03:33.103" OwnerUserId="264" ParentId="18911" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The paper&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0969212611001444&quot; rel=&quot;nofollow&quot;&gt;Maxim V. Shapovalov, Roland L. Dunbrack Jr., A Smoothed Backbone-Dependent Rotamer Library for Proteins Derived from Adaptive Kernel Density Estimates and Regressions, Structure, Volume 19, Issue 6, 8 June 2011, Pages 844-858, ISSN 0969-2126, 10.1016/j.str.2011.03.019.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;uses adaptive kernel density estimation in order to make their density estimation smooth in regions where the data is sparse.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-26T18:43:11.080" Id="18997" LastActivityDate="2011-11-26T20:26:33.493" LastEditDate="2011-11-26T20:26:33.493" LastEditorUserId="5594" OwnerUserId="501" ParentId="3556" PostTypeId="2" Score="0" />
&#10;\Im{\widehat{\mathbf{a}}}_1 = -\Im{\widehat{\mathbf{a}}}_3 &amp;amp;= \frac{1}{2}(b-d).\\
&#10;&amp;amp;&amp;lt;\Re{\widehat{\mathbf{a}}}, \Im{\widehat{\mathbf{a}}}&amp;gt; \\
  <row AnswerCount="2" Body="&lt;p&gt;I have a data set with ~80 records, with ~8 features. I want to predict one of the features in future records. The feature is numeric and discrete. It ranges between -30 up to 140 with steps of 5. Until now I wanted to predict another feature which is boolean, so I used logistic regression. &#10;Which method should I use here? Maybe some kind of particle filter?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-26T23:40:04.117" Id="19015" LastActivityDate="2011-11-27T18:10:09.607" OwnerDisplayName="Noam Peled" OwnerUserId="7639" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;regression&gt;" Title="How can I predict a discrete feature using regression?" ViewCount="388" />
  <row AnswerCount="2" Body="&lt;p&gt;Given a Bayesian network that looks like the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A-&amp;gt;B-&amp;gt;C&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do we compute P(A|C)?  My initial guess would be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;P(A|C) = P(A|B) * P(B|C) + P(A|not B) * P(not B|C)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-11-27T23:40:14.060" Id="19024" LastActivityDate="2011-11-28T02:55:46.497" OwnerDisplayName="ArKitect" PostTypeId="1" Score="4" Tags="&lt;probability&gt;" Title="What is the P(A|C) if we know B depends on A and C depends on B?" ViewCount="137" />
  <row Body="&lt;p&gt;&lt;em&gt;The simulation strategy&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Collect &lt;em&gt;m&lt;/em&gt; random samples of size &lt;em&gt;n&lt;/em&gt; from the set &lt;em&gt;S&lt;/em&gt;. For each of the &lt;em&gt;m&lt;/em&gt; samples, compute the number &lt;em&gt;u&lt;/em&gt; of unique values and divide by &lt;em&gt;n&lt;/em&gt; to normalize. From the simulated distribution of normalized &lt;em&gt;u&lt;/em&gt;, compute summary statistics of interest (e.g., mean, variance, interquartile range). Multiply the simulated mean of normalized &lt;em&gt;u&lt;/em&gt; by the cardinality of &lt;em&gt;S&lt;/em&gt; to estimate the number of unique values.&lt;/p&gt;&#10;&#10;&lt;p&gt;The greater are &lt;em&gt;m&lt;/em&gt; and &lt;em&gt;n&lt;/em&gt;, the more closely your simulated mean will match the true number of unique values.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-11-28T07:56:59.047" Id="19039" LastActivityDate="2011-11-28T08:04:23.490" LastEditDate="2011-11-28T08:04:23.490" LastEditorUserId="7616" OwnerUserId="7616" ParentId="19014" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Normally to perform supervised learning you need two types of data sets: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;In one dataset (your &quot;gold standard&quot;) you have the input data together with correct/expected output, This dataset is usually duly prepared either by humans or by collecting some data in semi-automated way. But it is important that you have the expected output for every data row here, because you need for supervised learning.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The data you are going to apply your model to. In many cases this is the data where you are interested for the output of your model and thus you don't have any &quot;expected&quot; output here yet.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;While performing machine learning you do the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Training phase: you present your data from your &quot;gold standard&quot; and train your model, by pairing the input with expected output. &lt;/li&gt;&#10;&lt;li&gt;Validation/Test phase: in order to estimate how good your model has been trained (that is dependent upon the size of your data, the value you would like to predict, input etc) and to estimate model properties (mean error for numeric predictors, classification errors for classifiers, recall and precision for IR-models etc.)&lt;/li&gt;&#10;&lt;li&gt;Application phase: now you apply your freshly-developed model to the real-world data and get the results. Since you normally don't have any reference value in this type of data (otherwise, why would you need your model?), you can only speculate about the quality of your model output using the results of your validation phase.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The validation phase is often split into two parts&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;In the first part you just look at your models and select the best performing approach using the validation data (=validation)&lt;/li&gt;&#10;&lt;li&gt;Then you estimate the accuracy of the selected approach (=test).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hence the separation to 50/25/25.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case if you don't need to choose an appropriate model from several rivaling approaches, you can just re-partition your set that you basically have only training set and test set,  without performing the validation of your trained model. I personally partition them 70/30 then.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also &lt;a href=&quot;http://stats.stackexchange.com/questions/9357/why-only-three-partitions-training-validation-test&quot;&gt;this question&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-11-28T11:50:48.520" Id="19051" LastActivityDate="2014-12-15T03:08:32.340" LastEditDate="2014-12-15T03:08:32.340" LastEditorUserId="-1" OwnerUserId="7647" ParentId="19048" PostTypeId="2" Score="42" />
  
&#10;* P(product.brand|buys)}{P(product.type,product.brand)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;As we compare P(buys) and P(does not buy), the denominator is a constant and we can compare P(buys) and P(does not buy) based on their numerator only.&lt;/p&gt;&#10;&#10;&lt;p&gt;Determining the probabilities $P(product.type|buys)$ is easy by looking at what the user bought in the past. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I'm not sure how I would determine $P(buys)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it the number of products bought divided by the total products available in the shop? But the number of products available varies with time. Is it fine to take the current number of products, which will be different from the number of products that were available when the user made his last order (an extreme example: the user can have ordered 3 of the 5 products that were available at launch, but now there are 10000 products).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-28T13:28:01.680" FavoriteCount="2" Id="19055" LastActivityDate="2012-02-03T16:40:23.847" LastEditDate="2011-11-28T13:58:27.593" LastEditorUserId="7653" OwnerUserId="7653" PostTypeId="1" Score="3" Tags="&lt;modeling&gt;&lt;naive-bayes&gt;" Title="Is Naive Bayes fine for simple &quot;Suggested products&quot; solution?" ViewCount="168" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a logistic regression model with several categorical explanatory variables and one interaction term (between two binary variables, named A and B). I know how to calculate the odds ratios for the different levels of A and B (for A=1, e.g., I need to add the coeff for A and coeff for A*B, then exponentiate), but how do I get a confidence interval for this OR? I need to do this in R please, this is the only statistical package I have access to.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-28T19:04:17.027" Id="19082" LastActivityDate="2011-11-28T22:59:09.210" OwnerUserId="7666" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;" Title="How to calculate OR and confidence interval for two binary explanatory variables with interaction" ViewCount="504" />
  <row Body="&lt;pre&gt;&lt;code&gt;?auto.arima&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Returns best ARIMA model according to either AIC, AICc or BIC value.&#10;  The function conducts a search over possible model within the order&#10;  constraints provided.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This means it tries all the possible parameters (within the constraints provided) and returns the model with the lowest AIC, AICc or BIC.&lt;/p&gt;&#10;&#10;&lt;p&gt;For more information, you can &lt;a href=&quot;http://www.jstatsoft.org/v27/i03/paper&quot;&gt;read the article referenced&lt;/a&gt; in &lt;code&gt;?auto.arima&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-28T20:54:52.140" Id="19088" LastActivityDate="2011-11-28T20:54:52.140" OwnerUserId="2817" ParentId="19080" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Here is a whole paper about the problem, with a summary of various approaches.  It's called &lt;a href=&quot;http://www.vldb.org/conf/1995/P311.PDF&quot; rel=&quot;nofollow&quot;&gt;Distinct Value Estimation&lt;/a&gt; in the literature.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I had to do this myself, without having read fancy papers, I'd do this.  In building language models, one often has to estimate the probability of observing a previously unknown word, given a bunch of text.  A pretty good approach at solving this problem for language models in particular is to use the number of words that occurred exactly once, divided by the total number of tokens.  It's called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation&quot; rel=&quot;nofollow&quot;&gt;Good Turing Estimate&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let u1 be the number of values that occurred exactly once in a sample of m items.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;P[new item next] ~= u1 / m.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let u be the number of unique items in your sample of size m.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you mistakenly assume that the 'new item next' rate didn't decrease as you got more data, then using Good Turing, you'll have&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;total uniq set of size s ~= u + u1 / m * (s - m) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This has some nasty behavior as u1 becomes really small, but that might not be a problem for you in practice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-28T21:49:35.367" Id="19090" LastActivityDate="2011-11-28T23:05:36.703" LastEditDate="2011-11-28T23:05:36.703" LastEditorUserId="4164" OwnerUserId="4164" ParentId="19014" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I'll make a leap of faith, and assume that you are referring to a spherical spatial correlation structure. &lt;/p&gt;&#10;&#10;&lt;p&gt;A spherical spatial correlation structure has two parameters: $n$, the &quot;nugget&quot; effect, which acts to reduce all the correlations between two observations more than 0 distance apart, and $d$, the range (distance) over which the correlations will be nonzero.  Slightly rephrasing the documentation from R's nlme package:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The correlation between two observations a distance $r &amp;lt; d$ apart is,&#10;  if the nugget effect is zero, $1 - 1.5(r/d) + 0.5(r/d)^3$.  If $r\ge d$&#10;  the correlation is zero.  If there is a nugget effect $n$, the&#10;  correlation is just $(1-n)(1 - 1.5(r/d) + 0.5(r/d)^3)$ for all&#10;  observations for which $r &amp;gt; 0$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The &quot;spherical&quot; refers to its symmetry with respect to direction, like a sphere with respect to the origin, rather than to the shape of the surface from which the data was collected, although, confusingly, it is also the name of the structure.  However, it does seem to me that, if there is correlation between your observations that is related to distance between them regardless of direction, a spherical spatial correlation structure would be a reasonable first try.  There are other spatial correlation structures, though, e.g., Gaussian or exponential (which also are symmetric with respect to direction.)&lt;/p&gt;&#10;&#10;&lt;p&gt;A reference is &quot;Statistics for Spatial Data&quot;, by N. A. C. Cressie, 1993.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-29T02:54:57.657" Id="19094" LastActivityDate="2011-11-29T15:44:58.713" LastEditDate="2011-11-29T15:44:58.713" LastEditorUserId="7555" OwnerUserId="7555" ParentId="19072" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row Body="&lt;p&gt;The polynomial kernel $K(x,y) = (x \cdot y + 1)^d$ is easily represented in terms of monomials. The degree $d$ is the maximum degree of the polynomial computed by the kernel and therefore also the maximum degree of any contained monomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem of determining the number of monomials of degree &lt;em&gt;exactly&lt;/em&gt; $d$ in $p$ input variables is the same as the problem of finding the number of combinations to draw $d$ elements from a bin with $p$ different elements. The number of such &lt;a href=&quot;http://en.wikipedia.org/wiki/Combination#Example_of_counting_multicombinations&quot;&gt;$d$-combinations&lt;/a&gt; is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(\!\!{d \choose p}\!\!\right) = {d + p - 1 \choose d} = {d + p - 1 \choose p - 1}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, we need to consider all monomials of degree $k = 0, \ldots, d$. Using $\sum_{j=k}^{n}{j \choose k} = {n+1 \choose k+1}$ we get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sum_{k=0}^{d}{p - 1 + k \choose p - 1} = \sum_{k=(p-1)}^{(p-1)+d}{k \choose p - 1} = {p + d \choose p} = {p + d \choose d}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Addendum&lt;/strong&gt;: This formula also nicely showcases the power of kernel functions: Just consider the case where $p = 256$ and $d = 2$. The kernel calculates the scalar product of two vectors (representing the scalar in front of each monomial base function) in a space with dimension ${d + p \choose d} = {256 + 2 \choose 2} = 33153$. An explicit scalar product computation thus involves 33153 multiplications and additions, while the kernel needs $p + d - 1 = 257$ multiplications and $p = 256$ additions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-11-29T22:46:23.967" Id="19132" LastActivityDate="2011-11-29T22:51:46.453" LastEditDate="2011-11-29T22:51:46.453" LastEditorUserId="4916" OwnerUserId="4916" ParentId="9844" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Let's say you are considering catastrophic health insurance, and there is a 1% probability of getting sick which would cost 1 million dollars.  The expected cost of getting sick is thus 10,000 dollars. The insurance company, wanting to make a profit, will charge you 15,000 for the policy.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Buying the policy gives an expected cost to you of 15,000, which has a variance of 0 but can be thought of as biased since it is 5,000 more than the real expected cost of getting sick.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not buying the policy gives an expected cost of 10,000, which is unbiased since it is equal to the true expected cost of getting sick, but has a very high variance.&#10;The tradeoff here is between an approach that is consistently wrong but never by much and an approach that is correct on average but is more variable. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-30T01:53:50.603" Id="19140" LastActivityDate="2011-11-30T01:53:50.603" OwnerUserId="4797" ParentId="4284" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="19327" AnswerCount="2" Body="&lt;p&gt;I have difficulty in solving unit root test by using both &lt;code&gt;adf.test&lt;/code&gt; and &lt;code&gt;pp.test&lt;/code&gt; in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The result p-value using &lt;code&gt;adf.test&lt;/code&gt; is 0.2677 (mean that it has unit root),&#10;and the result p-value using &lt;code&gt;pp.test&lt;/code&gt; is 0.01696 (mean that is has no unit root).&lt;/p&gt;&#10;&#10;&lt;p&gt;What means that? I wonder whether I have to use diff function to remove unit root or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;(&lt;code&gt;k&lt;/code&gt; argument in &lt;code&gt;adf.test&lt;/code&gt;, How do I use this? what means exactly for &lt;code&gt;k&lt;/code&gt;?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Raw data is below.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2935833&#10;2622529&#10;2719635&#10;2625179&#10;2311187&#10;2101758&#10;2552638&#10;2883423&#10;3128904&#10;2959348&#10;2759000&#10;2233755&#10;2560858&#10;2548821&#10;2625675&#10;2326076&#10;1662956&#10;1772409&#10;1797275&#10;2639852&#10;2799990&#10;3133285&#10;2438296&#10;2583766&#10;2610157&#10;2493415&#10;2094163&#10;2174301&#10;2283420&#10;2505128&#10;2873785&#10;2339727&#10;2985829&#10;3037351&#10;1828265&#10;1038562&#10;1474727&#10;1523331&#10;2122667&#10;2571006&#10;2252161&#10;2422347&#10;2155973&#10;2294976&#10;2809652&#10;2436293&#10;2561852&#10;2199544&#10;2674423&#10;2551363&#10;3110508&#10;3177925&#10;3046952&#10;2850904&#10;3002830&#10;2910913&#10;2809172&#10;3136842&#10;3355368&#10;3604565&#10;3013310&#10;3125751&#10;2548605&#10;2646575&#10;2231458&#10;1962095&#10;1958019&#10;2143073&#10;2305966&#10;2620302&#10;2356447&#10;2427571&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-11-30T02:29:49.867" Id="19143" LastActivityDate="2012-02-10T11:08:04.987" LastEditDate="2011-11-30T13:54:04.863" LastEditorUserId="1036" OwnerUserId="7699" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;stationarity&gt;" Title="How do I interpret conflicting results from adf.test and pp.test in R?" ViewCount="2142" />
  
  <row Body="&lt;p&gt;The assumption of normality is not generally one of the assumptions of a Pearson chi-square test.  Typically the assumptions are that you must have a large enough n in each cell of the test, that the sample is selected randomly, and that the samples are independent.  That's it.&lt;/p&gt;&#10;&#10;&lt;p&gt;As to the implied way you're thinking about statistics, you might want to read &lt;a href=&quot;http://pss.sagepub.com/content/22/11/1359.short&quot;&gt;this&lt;/a&gt;.  It may or may not be in your field but the principles apply broadly.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-30T05:08:49.500" Id="19152" LastActivityDate="2011-11-30T05:15:38.613" LastEditDate="2011-11-30T05:15:38.613" LastEditorUserId="601" OwnerUserId="601" ParentId="19147" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;First, it isn't terribly complicated to check for the association of Z on X or Y, even in survival analysis. Propensity scores and Inverse Probability of Treatment Weights (both common methods for adjusting for confounding in a survival context), along with other somewhat more esoteric methods are based on estimating the relationship between the covariate and the exposure or outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can compare the adjusted and unadjusted score to evaluate whether or not there is confounding, but only if you have reason to believe there's confounding there in the first place. Just a raw comparison of the adjusted and unadjusted estimates run the risk of having actually &lt;em&gt;induced&lt;/em&gt; confounding by adjusting for a variable that is &lt;em&gt;affected by&lt;/em&gt; both the exposure and the outcome. Check up on the literature on directed acyclic graphs for covariate selection, and read about &quot;colliders&quot; for an explanation of this phenomena.&lt;/p&gt;&#10;&#10;&lt;p&gt;But once something is believed to be a confounder for any number of reasons - subject matter expertise, the use of a DAG, establishing that the variable meets the criteria for a confounder, one can use what you're suggesting - which is normally called a change-in-estimate approach - to check whether or not its a &quot;problem&quot;, based on how much the estimate changes. The threshold for what is a problem varies, but in Epidemiology, it's often a 10% change in estimate that's used to say something confounds the exposure-disease relationship enough to be worth adjusting for.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-11-30T05:26:48.823" Id="19153" LastActivityDate="2011-11-30T05:26:48.823" OwnerUserId="5836" ParentId="19139" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="19160" AnswerCount="1" Body="&lt;p&gt;I have 2 copora:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;List 1 contains 1 million words in total. This represents general written usage.&lt;/li&gt;&#10;&lt;li&gt;List 2 contains 5 million words in total. This represents a sub-genre of literature.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would like to isolate those words that are special to that genre; by comparing the two lists. Using some software I wrote, I can get data like this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The word &quot;elephant&quot; appears 20 times in list 1.&lt;/li&gt;&#10;&lt;li&gt;The word &quot;elephant&quot; appears 100 times in list 2.&lt;/li&gt;&#10;&lt;li&gt;The word &quot;forest&quot; appears 20 times in list 1.&lt;/li&gt;&#10;&lt;li&gt;The word &quot;forest&quot; appears 300 times in list 2.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In this case, it seems clear to me that &quot;elephant&quot; is not so special to the genre (20/1000000 = 100/5000000), however, &quot;forest&quot; is probably special (20/1000000 &amp;lt; 300/5000000).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there are some cases where it seems more difficult to make a decision:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The word &quot;bridge&quot; appears 20 times in list 1.&lt;/li&gt;&#10;&lt;li&gt;The word &quot;bridge&quot; appears 150 times in list 2.&lt;/li&gt;&#10;&lt;li&gt;The word &quot;raccoon&quot; appears 1 time in list 1.&lt;/li&gt;&#10;&lt;li&gt;The word &quot;raccoon&quot; appears 10 times in list 2.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In the first example, &quot;bridge&quot; appeared more frequently in list 2, but is that difference significant? In the second example, &quot;raccoon&quot; appears 10 times more frequently in list 2, but given that the total occurrences are so low, is that significant?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How can I obtain a cut-off point for deciding which words are specific to the genre?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-11-30T07:44:39.630" Id="19158" LastActivityDate="2011-11-30T13:40:56.907" LastEditDate="2011-11-30T13:40:56.907" LastEditorUserId="7704" OwnerUserId="7704" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;" Title="Comparing words in 2 corpora to find significant differences" ViewCount="138" />
  
  <row AcceptedAnswerId="19172" AnswerCount="2" Body="&lt;p&gt;I'm trying to implement the extended binomial density function with support on c( 0 : (floor(N) + 1)), but I'm running into (I think) precision issues, as running:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;########################&#10;#---DENSITY FUNCTION---#&#10;########################&#10;debinom &amp;lt;- function(k, n, p, sum) {&#10;    if (k &amp;lt;=  n) {&#10;  return( choose(n, k) * p^k * (1-p)^(n-k) )&#10;  } else {&#10;    return (1.0 - sum)&#10;    }&#10;}#END: pebinom&#10;&#10;##########################################&#10;#---CUMULATIVE DISTRIBUTION FUNCTION 2---#&#10;##########################################&#10;pebinom &amp;lt;- function(x, n, p) {&#10;&#10;  # point mass at 0&#10;  totalDensity = cumProb = debinom(0.0, n, p, 0.0)&#10;&#10;  k = 0&#10;  while (k &amp;lt;= (x)) {&#10;    density2 = debinom(k, n, p, totalDensity)&#10;    totalDensity = totalDensity + density2&#10;    cumProb = cumProb + density2&#10;    k = k + 1&#10;  }&#10;&#10;  k = k + 1&#10;  density = debinom(k, n, p, totalDensity)&#10;  cumProb = cumProb + density * (x - k)&#10;&#10;  return (cumProb) &#10;}#END: debinom&#10;&#10;############&#10;#---TEST---#&#10;############&#10;for (i in 0:10) {&#10;x = i + runif(1)&#10;cat(x, &quot; &quot;, pebinom(x, 100, 0.1), &quot;\n&quot;)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives a negative probabilities for tail values. &lt;/p&gt;&#10;&#10;&lt;h1&gt;EDIT&lt;/h1&gt;&#10;&#10;&lt;p&gt;I have changed, and mostly simplified the routines along the comments and answers I've received:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#########################################&#10;#---PROBABILITY DISTRIBUTION FUNCTION---#&#10;#########################################&#10;&#10;debinom &amp;lt;- function(k, n, p) {&#10;&#10;if (k &amp;lt;=floor(n)) {&#10;&#10;  return( choose(n, k) * p^k * (1-p)^(n-k) )&#10;&#10;  } else if(k == (floor(n)+1)) {&#10;&#10;    cumProb = 0.0&#10;    for(i in 0 : floor(n)) {&#10;      cumProb = cumProb + debinom(i, n, p)  &#10;    }&#10;&#10;    return (1.0 - cumProb)&#10;&#10;    } else {&#10;&#10;  return(0.0)&#10;    }&#10;&#10;}#END: pebinom&#10;&#10;########################################&#10;#---CUMULATIVE DISTRIBUTION FUNCTION---#&#10;########################################&#10;pebinom &amp;lt;- function(x, N, P) {&#10;&#10;cumProb = 0&#10;for(i in 0 : (floor(x)) ) {&#10; cumProb = cumProb + debinom(i, N, P)&#10;}&#10;&#10;return(cumProb)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2011-11-30T15:21:44.893" Id="19169" LastActivityDate="2011-12-02T08:33:45.343" LastEditDate="2011-12-02T08:33:45.343" LastEditorUserId="6494" OwnerUserId="6494" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;distributions&gt;&lt;binomial&gt;" Title="Extended binomial distribution" ViewCount="291" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Typically, when one encounters continuous but skewed outcome measures in a longitudinal design (say, with one between-subjects effect) the common approach is to transform the outcome to normality. If the situation is extreme, such as with truncated observations, one might get fancy and use a Tobit growth curve model, or some such. &lt;/p&gt;&#10;&#10;&lt;p&gt;But I am at a loss when I see outcomes that are normally distributed at certain time points and then heavily skewed at others; transformation may plug one leak but spring another. What might you suggest in such a case? Are there &quot;non-parametric&quot; versions of mixed effects models that I am not aware of?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: an applied example would be knowledge test scores pre/post a series of educational interventions. Scores begin normal but then cluster at the high end of the scale later on.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-11-30T22:12:02.947" FavoriteCount="3" Id="19189" LastActivityDate="2013-02-27T06:42:30.627" LastEditDate="2011-11-30T22:17:04.790" LastEditorUserId="930" OwnerUserId="6941" PostTypeId="1" Score="12" Tags="&lt;repeated-measures&gt;&lt;data-transformation&gt;&lt;skewness&gt;" Title="What to do when some time points have heavily skewed responses and some do not in a repeated measures study?" ViewCount="414" />
  <row Body="&lt;p&gt;It is not entirely clear what you are asking, and this makes it difficult to help with an answer.  I will try to throw out several ideas and we can see if something sticks.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let me change the setup a little bit because I have a hard time visualizing many temperature readings within 1 second, but you can change it back if you need--it's just a story to motivate the discussion.  So, let's say you take a reading from a thermometer and write it down every 6 seconds.  Thus you have 10 readings per minute, and you continue this procedure over the course of an hour.  Now you could calculate the mean of each set of 10 readings as a single measure of the temperature during that minute.  In addition, you could calculate:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the Standard Deviation ($SD$) of the readings for each minute.  The equation is &#10;$SD_m=\sqrt{\frac{\Sigma_i(x_{im}-\bar{x}_m)^2}{n_m-1}}$&#10;Where $n_m$ is the number of measurements in the $m$th interval (which is always 10 in our story), the $x_{im}$ are the individual measurements and $\bar{x}_m$ is the mean for that interval.  This tells you how much the data are varying around your mean, just as you say.  In the end you would have 60 of these.  &lt;/li&gt;&#10;&lt;li&gt;the pooled Standard Deviation ($SD_{pooled}$), of all of your measurements.  The equation is $SD_{pooled}=\sqrt{\frac{\Sigma_m(n_m-1)*SD_m^2}{(\Sigma_mn_m)-m}}$  Thus, you multiply the number of measurements minus one  by the estimated standard deviation squared for each interval, and then sum those.  This is divided by the &lt;em&gt;total&lt;/em&gt; number of data minus the number of means used (60 here), and the square root is taken of the quotient.  The procedure outlined here is a more accurate estimate of measurement variability in your study, because it uses more data, and it is valid under the assumption that the true variability was constant.&lt;/li&gt;&#10;&lt;li&gt;the Standard Error ($SE$), an estimate of how much the means would vary on repeated sampling.  The equation is $SE=\frac{SD_m}{\sqrt{n_m}}$  This can be done for each interval in the study (which would give you 60 estimates of the SE).&lt;/li&gt;&#10;&lt;li&gt;the Standard Deviation of your means ($SD_{\bar{x}}$) from each minute.  The equation is $SD_{\bar{x}}=\sqrt{\frac{\Sigma_m(\bar{x}_m-\bar{x}_.)^2}{m-1}}$  Where $\bar{x}_.$ is the mean of all of your interval means.  Since you have calculated many means (60), this is an empirical measure of how much they vary on repeated sampling.  (Now things get a touch more complicated.)  This is valid under the assumption that all of these means come from the same population distribution.  In our example, these means are sampled over time.  Thus, this approach is valid under the assumption that the system is stationary, which is typically not true of time-series data.  For example, if you are sampling outside air temperature, that varies over the course of the day, so #4 would not be valid.  On the other hand, if you are sampling inside air temperature, and you have an awesome heating / air conditioning system, maybe it could be.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I'm honestly not sure which of these you're asking about.  From the question, you clearly understand #1, and from the comments, I gather you're familiar with #3.  (Which, as you recognize, is related to the central limit theorem; specifically, if your data are normally distributed, the sampling distribution of the mean will also be normally distributed with a standard deviation estimated by #3.)  When you ask about how to &quot;combine&quot; these, I'm guessing you're looking for either #2 or #4.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-01T05:14:55.380" Id="19197" LastActivityDate="2011-12-01T05:14:55.380" OwnerUserId="7290" ParentId="18809" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="20071" AnswerCount="2" Body="&lt;p&gt;I am looking for help in identifying what is the best statistical approach to address the following market research question:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) I have a dataset from an online website that helps users rent their vacation homes.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each vacation home I have information on:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Their attributes (predictor variables): location (city and distance&#10;from city center), # of rooms, guest rating (10 stars),price, etc.&lt;/li&gt;&#10;&lt;li&gt;Their sales performance (dependent variables): conversion rate, # of nights sold, sales volume in $&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(2) The website rents vacation homes mainly for US and Canada customers&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; I would like to be able to identify which combinations of predictor variables are associated with better performance, in order to be more targeted in gathering new homes for the site. Because most of the website users (renters) originate from the US and Canada (and I have a sense that they have different preferences) I would be especially interested in understanding which combinations of attributes work best for each country?&lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine that for Canadian customers you could have 3 combinations of attributes that would work best. For instance, one combination could be: price $50-$75/day, 2 rooms, far from the city center, etc...&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be really appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-01T07:19:30.060" Id="19200" LastActivityDate="2011-12-20T21:06:32.333" OwnerUserId="7720" PostTypeId="1" Score="1" Tags="&lt;predictive-models&gt;" Title="How to test which combination of product attributes generates better sales performance?" ViewCount="126" />
  <row Body="&lt;p&gt;There are two methods to consider:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Only use the last N input samples. Assuming your input signal is of dimension D,&#10;then you have N*D samples per ground truth label. This way you can train using any classifier you like, including logistic regression. This way, each output is considered independent from all other outputs.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Use the last N input samples and the last N outputs you have generated. The problem is then similar to &lt;a href=&quot;http://en.wikipedia.org/wiki/Viterbi_algorithm&quot; rel=&quot;nofollow&quot;&gt;viterbi decoding&lt;/a&gt;. You could generate a non-binary score based on the input samples, and combine the score of multiple samples using a viterbi decoder.&#10;This is better than method 1. if you now something about the temporal relation between the outputs.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-12-01T15:15:01.567" Id="19212" LastActivityDate="2011-12-01T15:15:01.567" OwnerUserId="3867" ParentId="13172" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One thing I always ask myself before standardizing is, &quot;How will I interpret the output?&quot;  If there is a way to analyze data without transformation, this may well be preferable purely from an interpretation standpoint.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-01T18:36:56.400" Id="19219" LastActivityDate="2011-12-01T18:36:56.400" OwnerUserId="101" ParentId="19216" PostTypeId="2" Score="7" />
  
  <row AnswerCount="4" Body="&lt;p&gt;Given a sequence of inputs, I need to determine whether this sequence has a certain desired property. The property can only be true or false, that is, there are only two possible classes that a sequence can belong to.&lt;/p&gt;&#10;&#10;&lt;p&gt;The exact relationship between the sequence and the property is unclear, but I believe it is very consistent and should lend itself to statistical classification. I have a large number of cases to train the classifier on, although it might be slightly noisy, in the sense that there's a slight probability that a sequence is assigned the wrong class in this training set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example training data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sequence 1: (7 5 21 3 3) -&amp;gt; true&#10;Sequence 2: (21 7 5 1) -&amp;gt; true&#10;Sequence 3: (12 21 7 5 11 1) -&amp;gt; false&#10;Sequence 4: (21 5 7 1) -&amp;gt; false&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In rough terms, the property is determined by the set of values in the sequence (e.g. the presence of an &quot;11&quot; means that the property will almost certainly be false), as well as the &lt;em&gt;order&lt;/em&gt; of the values (e.g. &quot;21 7 5&quot; significantly increases the chance that the property is true).&lt;/p&gt;&#10;&#10;&lt;p&gt;After training, I should be able to give the classifier a previously unseen sequence, like &lt;code&gt;(1 21 7 5 3)&lt;/code&gt;, and it should output its confidence that the property is true.&#10;Is there a well-known algorithm for training a classifier with this kind of inputs/outputs?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have considered the naive Bayesian classifier (which is not really adaptable to the fact that the order matters, at least not without severely breaking the assumption that the inputs are independent). I've also investigated the hidden Markov model approach, which appears to be inapplicable because only a single output is available, instead of one output per input. What did I miss?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-01T20:20:16.263" Id="19226" LastActivityDate="2011-12-06T19:37:10.453" OwnerUserId="1553" PostTypeId="1" Score="11" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;modeling&gt;" Title="Which statistical classification algorithm can predict true/false for a sequence of inputs?" ViewCount="655" />
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;With distributions like this, if there are 21 different numbers occurring in your sequences, you would have to estimate $21 \cdot 21 \cdot 2 = 882$ parameters $\pi(x_t, x_t, c)$ plus $21 \cdot 2 = 42$ parameters for $p(x_0 \mid c)$ plus $2$ parameters for $p(c)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the assumptions of your model are not met, it can help to fine-tune the parameters directly with respect to the classification performance, for example by minimizing the average log-loss&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  <row Body="&lt;p&gt;Given a max length of 12 on the sequence, then a neural network with 12 inputs and one output may work, but you would have to pad the end of each sequence with zeroes or some inert value.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-01T21:44:06.007" Id="19229" LastActivityDate="2011-12-01T21:44:06.007" OwnerUserId="7038" ParentId="19226" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The expression &quot;combining models&quot; is vague, but my guess is that you are asking about ensemble learning methods. The best reference to learn about them is perhaps Rich Caruana's papers: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf&quot;&gt;http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There is no actual code in this paper, but the algorithm is clearly described, so you shouldn't have any problem coding it in any language you prefer. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-01T23:00:06.023" Id="19230" LastActivityDate="2011-12-01T23:00:06.023" OwnerUserId="5264" ParentId="19224" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;Suggestion:  take the lognormally-distributed variable and show how it is distributed (obtain a histogram) separately for each level of the other, categorical variable.  That will show you more about the nature of the relationship than truncating into 3 categories and doing a &quot;binary,&quot; rather sterile hypothesis test. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-02T01:05:28.437" Id="19238" LastActivityDate="2011-12-02T01:05:28.437" OwnerUserId="2669" ParentId="19147" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The common problem with plain text format is that it cannot store metadata. How do you define missing data? How do you define 1=strongly disagree, 2=disagree, ... kinds of stuff in plain text format? With plain text format, you have to use another document to define those metadata. And it is not easy to do in XML. &lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes this issue can be very disturbing.&lt;/p&gt;&#10;&#10;&lt;p&gt;My solution is to use SPSS data format, which is self-contained and easy to edit in SPSS. I know this is not a right answer to your question, but I have been struggled on the same problem for a very long time and this is my current solution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-02T03:11:09.513" Id="19240" LastActivityDate="2011-12-02T03:11:09.513" OwnerUserId="400" ParentId="5249" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Package &lt;code&gt;car&lt;/code&gt; has quite a lot of useful functions for diagnostic plots of linear and generalized linear models. Compared to vanilla R plots, they are often enhanced with additional information. I recommend you try &lt;code&gt;example(&quot;&amp;lt;function&amp;gt;&quot;)&lt;/code&gt; on the following functions to see what the plots look like. All plots are described in detail in chapter 6 of Fox &amp;amp; Weisberg. 2011. An R Companion to Applied Regression. 2nd ed.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;residualPlots()&lt;/code&gt; plots Pearson residuals against each predictor (scatterplots for numeric variables including a Lowess fit, boxplots for factors)&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;marginalModelPlots()&lt;/code&gt; displays scatterplots of the response variable against each numeric predictor, inluding a Lowess fit&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;avPlots()&lt;/code&gt; displays partial-regression plots: for each predictor, this is a scatterplot of a) the residuals from the regression of the response variable on all other predictors against b) the residuals from the regression of the predictor against all other predictors&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;qqPlot()&lt;/code&gt; for a quantile-quantile plot which includes a confidence envelope&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;influenceIndexPlot()&lt;/code&gt; displays each value for Cook's distance, hat-value, p-value for outlier test, and studentized residual in a spike-plot against the observation index&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;influencePlot()&lt;/code&gt; gives a bubble-plot of studentized residuals against hat-values, with the size of the bubble corresponding to Cook's distance, also see &lt;code&gt;dfbetaPlots()&lt;/code&gt; and &lt;code&gt;leveragePlots()&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;boxCox()&lt;/code&gt; displays a profile of the log-likelihood for the transformation parameter $\lambda$ in a Box-Cox power-transform&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;crPlots()&lt;/code&gt; is for component + residual plots, a variant of which are CERES plots (Combining conditional Expectations and RESiduals), provided by &lt;code&gt;ceresPlots()&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;spreadLevelPlot()&lt;/code&gt; is for assessing non-constant error variance and displays absolute studentized residuals against fitted values&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;scatterplot()&lt;/code&gt; provides much-enhanced scatterplots inluding boxplots along the axes, confidence ellipses for the bivariate distribution, and prediction lines with confidence bands&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;scatter3d()&lt;/code&gt; is based on package &lt;code&gt;rgl&lt;/code&gt; and displays interactive 3D-scatterplots including wire-mesh confidence ellipsoids and prediction planes, make sure to run &lt;code&gt;example(&quot;scatter3d&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In addition, have a look at &lt;code&gt;bplot()&lt;/code&gt; from package &lt;code&gt;rms&lt;/code&gt; for another approach to illustrating the common distribution of three variables.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-02T20:19:45.093" Id="19270" LastActivityDate="2011-12-02T20:19:45.093" OwnerUserId="1909" ParentId="19227" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;Like Chase suggests, funnels or funnel-like charts are a classic way to share this information. Another thing you might check out is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Waterfall_chart&quot; rel=&quot;nofollow&quot;&gt;Waterfall chart&lt;/a&gt;, but I think those are good at absolute sizes, not relative sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would suggest one very different approach - spreadsheet simulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Often we find ourselves in situations where intuitive reasoning is hard to shake, even with the facts right in front of us. I have found that a good way to make those facts more influential is through simulation that shows how much a change can affect the end result. A simple conversion funnel like this can be easily converted into a spreadsheet model that starts off with 1,000 users, and reduces them with a % conversion at each step. Then you can play with different strategies - &quot;what if we made it 5% easier to choose a secure password by using live AJAX-y feedback&quot; or &quot;what if we made the email more interesting so 10% more people would click the link?&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;Such a simulation is a very basic and limited way of forecasting. In reality, a change in the email that increases the email click rate can easily diminish conversion across all of the following steps, because you're reaching users who are further from your core target, and it could also easily increase conversion across the following steps because you increased people's interest and motivation in finishing. But the value of this is that it helps to focus people on the big picture - which changes, at a first glance, will have the most impact at the end of the funnel?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-03T00:15:54.780" Id="19279" LastActivityDate="2011-12-03T00:15:54.780" OwnerUserId="3331" ParentId="19248" PostTypeId="2" Score="1" />
  
&#10;\dfrac{t+1}{t+2}\,.
  
  <row Body="&lt;p&gt;For unbalanced datasets, please refer to &lt;a href=&quot;http://www.cs.gmu.edu/~hrangwal/kd-hcm/proc/papers/3-Perez-Baranauskas.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.gmu.edu/~hrangwal/kd-hcm/proc/papers/3-Perez-Baranauskas.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-03T15:12:15.013" Id="19299" LastActivityDate="2011-12-03T15:12:15.013" OwnerUserId="7765" ParentId="18950" PostTypeId="2" Score="1" />
  
&#10;\lambda=1-(A+B) \\
  
  
  <row AcceptedAnswerId="19347" AnswerCount="1" Body="&lt;p&gt;I have a variable X1 = (a - b) / (a + b). This variable shows a higher correlation to Y that any of (a, Y) and (b, Y).&lt;/p&gt;&#10;&#10;&lt;p&gt;In a multiple regression model like Y ~ X1, X2, does it make sense to use the X1 formula, or should I always use the base variables a and b?&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;a href=&quot;http://stats.stackexchange.com/questions/13056/building-a-linear-model-for-a-ratio-vs-percentage&quot;&gt;this&lt;/a&gt; post somebody pointed out that &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Intuitively you'd be a lot more confident about inferences from an observed&#10;  ratio of 1 (boys to girls) if it came from seeing 100 boys and 100 girls than &gt; from seeing 2 and 2. Consequently, if you have covariates you'll have more&#10;  information about their effects and potentially a better predictive model.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Fine, but can the multiple linear model rebuild the same (X1, Y) predictive relationship just by a and b least square analysis?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-04T22:18:25.540" Id="19342" LastActivityDate="2011-12-04T22:50:51.250" OwnerUserId="7795" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Independent variables formulas in multiple regression" ViewCount="117" />
  
  
  <row Body="&lt;p&gt;The short answer is no.  The relationship $Y = \beta_1 (a-b)/(a+b) + \beta_2 X_2 + e$ is not the same as $Y = \beta_1 a + \beta_2 b + \beta_3 X_2 + e$, and you can't get from one to the other. &lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that, there's nothing intrinsically wrong with transforming your right hand side variables before running a regression.   Consider predicting the weight of a tree from its measured height and circumference at its base; you're much better off computing its approximate volume from height and circumference, and estimating a regression with weight as the dependent variable and volume as the independent variable, than using height and circumference as the independent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the transformed variable will have a lot of measurement error associated with it, which seems to be your concern, then other problems will arise, known generically as &quot;errors in the variables.&quot;  What to do about that depends in part upon how severe the problem is, whether it's localized to a small subset of the data, and other factors - but that's a different question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-04T22:50:51.250" Id="19347" LastActivityDate="2011-12-04T22:50:51.250" OwnerUserId="7555" ParentId="19342" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Here is the plot of your data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Va9ng.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It is clear then, that it is probably not generated by statistical model. It is then not surprising that AR(1) predictions look suspicious. I suspect that some S-curve type function can be perfectly fitted to your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; What comes below is complement to IrishStat answer with R code illustrations. &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, as IrishStat pointed out, AR(1) model is useful here. In fact we have the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; bb&amp;lt;- structure(list(V1 = c(2459853L, 2481777L, 2496666L, 2506778L, &#10;2513645L, 2518309L, 2521476L, 2523627L, 2525088L, 2526080L, 2526754L, &#10;2527211L), index = 1:12, lV1 = c(NA, 2459853L, 2481777L, 2496666L, &#10;2506778L, 2513645L, 2518309L, 2521476L, 2523627L, 2525088L, 2526080L, &#10;2526754L)), .Names = c(&quot;V1&quot;, &quot;index&quot;, &quot;lV1&quot;), row.names = c(NA, &#10;-12L), class = &quot;data.frame&quot;)&#10;&#10;&amp;gt; summary(lm(V1~lV1,data=bb))&#10;&#10;Call:&#10;lm(formula = V1 ~ lV1, data = bb)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-0.44873 -0.16852  0.00065  0.20665  0.28504 &#10;&#10;Coefficients:&#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 8.112e+05  9.046e+00   89681   &amp;lt;2e-16 ***&#10;lV1         6.791e-01  3.605e-06  188386   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 0.246 on 9 degrees of freedom&#10;  (1 observation deleted due to missingness)&#10;Multiple R-squared:     1,  Adjusted R-squared:     1 &#10;F-statistic: 3.549e+10 on 1 and 9 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So we have a perfect fit (in the precision of original data):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fitted(lm(V1~lV1,data=bb))-bb$V1[-1]&#10;            2             3             4             5             6 &#10;-0.0431514978  0.2081317329 -0.2217918197  0.1431496162 -0.2695231647 &#10;            7             8             9            10            11 &#10; 0.1938952347 -0.0006489181 -0.1915123449  0.0177617292 -0.2850446492 &#10;           12 &#10; 0.4487340818 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The forecast  for future values is :&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_{t+h}=\alpha(1+\rho+...+\rho^{h-1})+\rho^hY_t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $Y_t$ is the last point of the data, $h$ -- the forecasting horizon, and $\alpha$ and $\rho$ are estimated coefficients:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; coef(lm(V1~lV1,data=bb))&#10; (Intercept)          lV1 &#10;8.112164e+05 6.791302e-01 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since your data is without stochastic error, the usual methods might behave strangely, which is illustrated by the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; auto.arima(ts(bb$V1))&#10;Series: ts(bb$V1) &#10;ARIMA(2,2,1)                    &#10;&#10;Coefficients:&#10;         ar1      ar2     ma1&#10;      1.9691  -0.9691  0.8809&#10;s.e.     NaN      NaN     NaN&#10;&#10;sigma^2 estimated as 37898:  log likelihood=-70.44&#10;AIC=148.88   AICc=156.88   BIC=150.09&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or even&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; arima(ts(bb$V1),order=c(1,0,0))&#10;Series: ts(bb$V1) &#10;ARIMA(1,0,0) with non-zero mean &#10;&#10;Coefficients:&#10;         ar1   intercept&#10;      0.9569  2497162.58&#10;s.e.  0.0575    27950.09&#10;&#10;sigma^2 estimated as 80110030:  log likelihood=-127.46&#10;AIC=260.92   AICc=263.92   BIC=262.37&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you see the AR(1) coefficient is estimated incorrectly. The fit is also very bad compared to OLS fit:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ts(bb$V1)-fitted(arima(ts(bb$V1),order=c(1,0,0)))&#10;Time Series:&#10;Start = 1 &#10;End = 12 &#10;Frequency = 1 &#10; [1] -10830.884  20317.315  14226.441  10090.616   7281.075   5373.793&#10; [7]   4077.641   3198.024   2600.654   2194.570   1919.289   1731.313&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The precise explanation why is that can be complicated, but the general rule is, that algorithms might perform poorly on corner cases (zero errors in this case).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-05T09:52:46.147" Id="19363" LastActivityDate="2011-12-05T14:56:57.733" LastEditDate="2011-12-05T14:56:57.733" LastEditorUserId="2116" OwnerUserId="2116" ParentId="19355" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The effect you mention happens because of response set, which can be controlled by the phrasing of the reactives, and the interest of the respondent, and the order of the questions. I've seen this happen in my own experience, but it's by no means inescapable: one good way to avoid the spurious tendency is to, say, put two agreement questions, then a dichotomy, then a couple more agreement ones, then the dichotomy, that usually does the trick for me. &lt;/p&gt;&#10;&#10;&lt;p&gt;A good way to test for that effect, if it's a scale you're building, is to compare the Cronbach's alpha of the entire scale with and without the dichotomies. &lt;/p&gt;&#10;&#10;&lt;p&gt;yes, factor loadings might be affected by the range of possible responses, but it won't neccesarily be dominated by it, if you take the measures you would normally take to avoid response set. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-05T18:09:43.737" Id="19379" LastActivityDate="2011-12-05T18:09:43.737" OwnerUserId="7810" ParentId="10945" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="19386" AnswerCount="1" Body="&lt;p&gt;I'm training a bunch of SVM models to do one-against-all multiclass classification (a test instance is classified as the class which produces the largest positive SVM response).&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the best way to do cross-validation for selection of the regularization parameter?&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I do cross-validation separately for each SVM model I train, potentially getting a different regularization parameter for each of the models? Or should I cross-validate as a group, where I try a particular regularization parameter across all models?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, what is a good metric for cross-validation? Accuracy? Precision? F-Measure?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-05T19:15:53.563" FavoriteCount="1" Id="19385" LastActivityDate="2011-12-05T19:33:34.073" OwnerDisplayName="user7814" PostTypeId="1" Score="3" Tags="&lt;cross-validation&gt;&lt;svm&gt;" Title="Cross validating one-against-all SVMs" ViewCount="479" />
  <row Body="&lt;p&gt;If I am understanding the problem... one test would be as follows.  Calculate the value at the peak using your peak locating algorithm, call this $T_0$.  Then randomly select 500 positions from your data, and calculate the value at the (not real) peak as located using your peak locating algorithm.  Do this 1,000 times or so, depending upon how much time it takes.  Keep track of the 1,000 &quot;peak&quot; values $T_1 \dots T_{1000}$.  Then compare $T_0$ to the $T_{1 \dots 1000}$.  If $T_0$ is greater than, say, 95% of the $T_i$, then you could reject a null hypothesis that the peak as detected by your algorithm was a random artifact at the 95% level of confidence.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-05T19:31:16.333" Id="19387" LastActivityDate="2011-12-05T19:31:16.333" OwnerUserId="7555" ParentId="19375" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Alright, first, I actually think your question is more suited for the new &quot;Computational Science&quot; site that should be coming into Public Beta through Area 51 soon. There are a number of issues behind the modeling of infectious diseases that are &lt;em&gt;not&lt;/em&gt; really within the scope of a statistical analysis site.&lt;/p&gt;&#10;&#10;&lt;p&gt;Answering your questions in order:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;Easy to interpret probabilities&quot; is somewhat vague - probabilities of what? The disease moving to that region? Likelihood of being infected, as expressed through something like the final prevalence of disease?&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some common ways to model this problem however. The first is an extension of the classic SIR type model, which are commonly known as &quot;meta-population&quot; models. Essentially, rather than a single set of SIR equations, you have a series of them, one for each region, with parameters in the model governing the interaction between the populations (in your case, map regions). The can be deterministic, at which point very little math is needed, or stochastic, which produces nice distributions of results for further statistical analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another, less &quot;classic&quot; way as mentioned by @Spacedman, is to use an agent-based model to track individuals. This kind of model is somewhat more difficult to implement, but has the advantage of producing individual level data that can be analyzed using more conventional statistical techniques.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also simulate this by representing the map as a set of nodes in a network, and modeling the spread of disease over that network using something like a percolation model.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, approaches to your problem abound.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In terms of specifics, it depends very much on your disease system, what you're trying to model, and what type of assumptions you're willing to settle for. Along with how much either programming or mathematical complexity you're willing to tolerate. It depends so much that this question is essentially unanswerable, on the scale of &quot;What variables should I put in a regression model&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is likely: A fair number. By way of example, even for a model that &lt;em&gt;doesn't&lt;/em&gt; have geographic spread, but could, one model I'm working on has roughly 23 parameters. The agent based version has more. My best advice is to consult an expert.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Spatio-temporal models, meta-population models, spatially discrete models, disease percolation models...there are tons of names. I'd say meta-population models are probably the one which will yield a number of example models the most swiftly, but there are lots of different names. One way to search is also to search for models of the disease you're interested in, to see if there's an approach you could replicate.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2011-12-05T20:17:06.517" Id="19395" LastActivityDate="2011-12-05T20:17:06.517" OwnerUserId="5836" ParentId="19253" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Also have a look at &lt;a href=&quot;http://elki.dbs.ifi.lmu.de/&quot; rel=&quot;nofollow&quot;&gt;ELKI&lt;/a&gt;, an open-source data mining software. Wikimedia commons has a gallery with &lt;a href=&quot;http://commons.wikimedia.org/wiki/ELKI&quot; rel=&quot;nofollow&quot;&gt;images produced with ELKI&lt;/a&gt;, many of which are related to cluster analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-06T08:58:06.367" Id="19414" LastActivityDate="2011-12-06T08:58:06.367" OwnerUserId="7828" ParentId="1475" PostTypeId="2" Score="2" />
  
  
  
  
  <row AcceptedAnswerId="20316" AnswerCount="2" Body="&lt;p&gt;I am trying to model sales data for stores at the &lt;a href=&quot;http://www.census.gov/geo/www/cob/bg_metadata.html&quot; rel=&quot;nofollow&quot;&gt;Census block group&lt;/a&gt; level in order to predict sales at potential new restaurants. For example, I know that store 2, which has a giant flashing neon sign, has $2K in sales from block group 101, which is 2.5 miles away from the store 2 and where there are 600 households and 50 people living in college dorm. So far this is pretty standard.&lt;/p&gt;&#10;&#10;&lt;p&gt;The fly in the ointment is that the average store has ~30% of the sales data that cannot be geocoded for some reason (new construction, college dormitories, military bases, lazy employees who take down the address in shorthand, and so on), so that I only know the store that handled the sales and not where those customers reside.&lt;/p&gt;&#10;&#10;&lt;p&gt;My approach to modeling the ungeocoded data starts with aggregating demographic data from around the store's trade area and all the ungeocoded sales, so that even if I don't know where those customers are, I can at least take a stab at understanding the sales behavior based on what's around the store. For example, if my store is near a college campus or has lots of construction, I would it expect it to have higher ungeocoded sales, all else being equal. &lt;/p&gt;&#10;&#10;&lt;p&gt;This works reasonably well, but the geocoded and ungeocoded sales models are not linked in any way, which is problematic. Essentially, my geocoded sales are measured with non-spherical error that is correlated with my explanatory variables. It's also the case that ungeocoded sales generally increase with geocoded sales. I tried to remedy the first by including the fraction of total sales that are ungeocoded in the geocoded sales model, and total geocoded sales in the ungeocoded model, but I don't know how to define those variables for potential sites whose sales I am interested in forecasting. I guess I can just set ungeocoded sales at 30%, then predict geocoded sales, and use that to forecast ungeocoded sales, but is there a better way to link the two models for the better estimation and forecasting?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-06T16:34:36.510" Id="19445" LastActivityDate="2011-12-28T16:13:32.840" LastEditDate="2011-12-28T16:13:32.840" LastEditorUserId="1080" OwnerUserId="7071" PostTypeId="1" Score="5" Tags="&lt;forecasting&gt;&lt;spatial&gt;&lt;econometrics&gt;&lt;missing-data&gt;&lt;measurement-error&gt;" Title="How to model the relationship between geocoded and ungeocoded sales data?" ViewCount="163" />
  <row AnswerCount="0" Body="&lt;p&gt;I am experimenting with a scheme for aggregating $N$ scored ratings. The gathered ratings themselves are integers, e.g. a chosen value between $1$ to $5$, and each rating $r_i$ is assigned with a score $s_i$. That is, a data point/record here is a $(rating, score)$ pair. However, $\sum_is_i\neq1$. Naturally, if we sort these $N$ ratings by their scores, we end up with a ranking of them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I would like to aggregate all the ratings into a single measure, but face a potential problem: for example, given two records $(r_1=5, s_1=0.7)$ and $(r_2=4, s_2=0.8)$, if we simply multiply the rating with its associated score and use the resulting product for aggregation, then we will see a larger influence from $r_1$ than from $r_2$ on the result of aggregation (as $5\times0.7&gt;4\times0.8$) even though $r_2$ scores &lt;em&gt;considerably higher&lt;/em&gt; than $r_1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder what are the appropriate/fair methods/techniques for aggregating in this context. Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: in response to whuber's comment below, here are some clarifications&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The score for a rating is separately calculated for each rating. We can think of the score as a measure of the 'soundness' of a given rating. As the scores may well be different for different ratings, we can then sort the ratings by their scores in descending order to produce a ranking of ratings.&lt;/li&gt;&#10;&lt;li&gt;One score for one rating, so more precisely, the dataset consists of $N$ number of records each of which is a $(rating, score)$ pair, e.g. $(r_1, s_1)=(5, 0.7)$, $(r_2, s_2)=(4, 0.8)$ as the examples given above.&lt;/li&gt;&#10;&lt;li&gt;By aggregation, I mean combining all given ratings (i.e. the whole dataset) into one single number which is still within the range of individual ratings. In other words, if any individual rating $r_i$ is $1\leq r_i \leq5$, then the aggregated $r^*$ should also be $1\leq r^* \leq5$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-12-06T17:29:45.120" Id="19448" LastActivityDate="2011-12-06T18:41:49.247" LastEditDate="2011-12-06T18:41:49.247" LastEditorUserId="7844" OwnerUserId="7844" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;ranking&gt;&lt;aggregation&gt;&lt;rating&gt;" Title="Influences of scored/weighted ratings" ViewCount="90" />
  <row Body="&lt;p&gt;The short answer is &quot;No.&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;First, it would be helpful to know if your underlying discrete-time Markov chain is aperiodic, unless you are using the phrase &quot;steady state probabilities&quot; loosely to mean &quot;long-run proportion of the time the CTMC is in the various states&quot; or something else other than &quot;stationary distribution.&quot; Aperiodicity in combination with irreducibility is sufficient to guarantee a unique stationary distribution in the case of finite state-spaces, which you are assuming.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, the lack of irreducibility means that you will have either some transient states and/or more than 1 closed communicating class.  Each class will have its own steady state probabilities (given aperiodicity) and there may be stationary distributions that span multiple communicating classes. Which class you wind up in depends upon what happened during the transient part of the chain's operation, and perhaps upon the initial state.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a two-state discrete-time MC with transition matrix $P(1,1) = P(2,2)=1, P(1,2)=P(2,1) = 0$.  Clearly it is aperiodic but not irreducible.  Any steady-state distribution $\pi$ satisfies $\pi = \pi P$, is nonnegative, and sums to one. Obviously this is satisfied for any $\pi$ that is nonnegative and sums to one.  So &lt;em&gt;every&lt;/em&gt; distribution on $\{1,2\}$ is a steady-state distribution for this example. (Hence, clearly, a &lt;em&gt;unique&lt;/em&gt; one does not exist.)  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-06T18:23:15.480" Id="19450" LastActivityDate="2011-12-07T02:22:03.893" LastEditDate="2011-12-07T02:22:03.893" LastEditorUserId="2970" OwnerUserId="7555" ParentId="19442" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data on a number of studies comparing response rates to various treatments of a disorder. There are six different treatments, and some studies only test a single treatment. For each treatment in each study, I have the response rate and the sample size (plus some other variables). &lt;/p&gt;&#10;&#10;&lt;p&gt;Normally, if there were only two treatments, and each study compared the two, I might use something like&#10;$$D_i=\beta_0+\beta_1\cdot SS_i+u_i+\epsilon_i$$&#10;where $D_i$ is the difference between the logits of the response rates in study $i$, $\epsilon_i$ is the corresponding sampling error, $SS_i$ is the sample size of study $i$, and $u_i$ is the residual error term.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to sensibly analyze my data in a similar way? Are there keywords / references I should be aware of?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I'd like to use something like&#10;$${\rm logit}(RR_{ij})=\beta_0+\beta_1\cdot SS_{ij}+\beta_2\cdot T_j+u_i+\epsilon_{ij}$$&#10;where $i$ is the study and $j$ is the treatment (1 to 6), $T_j$ is an indicator variable for the treatment (alternatively, could be turned into a second random effect), and $u_i$ and $\epsilon_{ij}$ are as above.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-06T22:26:26.747" Id="19457" LastActivityDate="2011-12-06T22:26:26.747" OwnerUserId="2739" PostTypeId="1" Score="2" Tags="&lt;mixed-model&gt;&lt;multilevel-analysis&gt;&lt;meta-analysis&gt;" Title="Multilevel Meta-analysis with several treatments" ViewCount="142" />
  
  <row Body="&lt;p&gt;It sounds like you're looking for an n-order Markov model and not a hidden Markov model. Typically, an HMM would be useful if you wanted to identify hidden states at each selection in a series of selections. An n-order Markov model would give you the conditional probability of choosing the n+1'st state given the previous n states, which would be useful in the example you just described.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-06T23:11:39.627" Id="19458" LastActivityDate="2011-12-06T23:11:39.627" OwnerUserId="92" ParentId="19455" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="24697" AnswerCount="1" Body="&lt;p&gt;I am attempting to construct a survival analysis in Stata whereby subjects fail if the failure condition is met (Staph) but are also removed from the model once they are over 1100 days old.  My current script is -&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;stset new_end_age, id(CPID) failure(Staph==1) exit(time new_end_age&amp;gt;=1100) origin(time birth_dt) scale(1)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Which results in the following output - &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id:  CPID&#10;&#10;failure event:  Staph == 1&#10;&#10;obs. time interval:  (new_end_age[_n-1], new_end_age]&#10;&#10;exit on or before:  time new_end_age&amp;gt;=1100&#10;&#10;t for analysis:  (time-origin)&#10;&#10;origin:  time birth_dt&#10;&#10;&#10;54592  total obs.&#10; 1898  ignored because never entered&#10;36124  obs. end on or before enter()&#10;16387  obs. begin on or after exit&#10;&#10;  183  obs. remaining, representing&#10;  183  subjects&#10;    0  failures in single failure-per-subject data&#10;  421168  total analysis time at risk, at risk from t =         0&#10;                         earliest observed entry t =         0&#10;                              last observed exit t =     11892&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However this is incorrect as I know that there are multiple entries for individual subjects that should remain, and some of these should fail in my time period of interest.&#10;I need a command that prioritises the failure condition and if this is not met subjects are removed at &lt;code&gt;t=1100&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-07T11:40:17.553" Id="19471" LastActivityDate="2013-07-20T23:09:25.280" LastEditDate="2013-07-20T23:09:25.280" LastEditorUserId="22047" OwnerUserId="7421" PostTypeId="1" Score="1" Tags="&lt;stata&gt;&lt;survival&gt;&lt;failure&gt;" Title="How can I censor entries at failure or an exit age in Stata?" ViewCount="886" />
  
  <row Body="&lt;p&gt;If the random variable $H = H(X,Y,Z)$ is a function of three integer-valued random variables $X, Y$, and $Z$ that take on values in $[0, n]$&#10;and whose joint probability mass function is &#10;$$p_{X,Y,Z}(n_1,n_2,n_3) = P\{X = n_1, Y = n_2, Z = n_3\}, ~0 \leq n_1,n_2,n_3 \leq n,$$&#10;then &#10;$$E[H] = \sum_{n_1=0}^n\,\sum_{n_2=0}^n\,\sum_{n_3=0}^nH(n_1,n_2,n_3)p_{X,Y,Z}(n_1,n_2,n_3).$$&#10;It is &lt;em&gt;not&lt;/em&gt; necessary that $H$ be expressible as a &quot;nice&quot; formula &#10;such as $X+Y+Z$ in order to use the above formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, &#10;$$p_{X,Y,Z}(n_1,n_2,n_3) = 
&#10;0, &amp;amp; \text{otherwise},\end{cases}$$&#10;is a $(n+1)\times(n+1)\times(n+1)$ array with lots of zeroes in it,&#10;as is $H(n_1,n_2,n_3)$ which is an array of zeroes and ones.  Thus,&#10;$E[H]$ is actually the probability of an event.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, $H(n_1,n_2,n_3)/S(n_1,n_2,n_3)$ is an array with lots of zeroes&#10;in it but the nonzero entries are $1/S(n_1,n_2,n_3)$ and&#10;$$E\left [\frac{H}{S}\right]
  
  <row AcceptedAnswerId="19485" AnswerCount="1" Body="&lt;p&gt;In GARCH model how should I handle outliers?&#10;Just remove it from my dataset and skip to next data entry?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-07T14:04:43.180" FavoriteCount="2" Id="19479" LastActivityDate="2011-12-07T22:12:42.513" LastEditDate="2011-12-07T14:13:17.013" LastEditorUserId="930" OwnerUserId="6908" PostTypeId="1" Score="2" Tags="&lt;outliers&gt;&lt;garch&gt;" Title="How to handle outliers in GARCH model?" ViewCount="345" />
  <row Body="&lt;p&gt;generally speaking, outliers should &lt;em&gt;never&lt;/em&gt; be dealt with that way. You have even more reasons not handle them this way in time series context for obvious reasons. The solution is to use an outlier robust estimation technique (one where the suspect observations are left in the data set, but where their influence over the final estimates is bounded). Two pointers to robust garch estimation models:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ulises.ic.fcen.uba.ar/preprints/muler-yohai2005.pdf&quot; rel=&quot;nofollow&quot;&gt;Yohai and Muler (2005)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.riskresearch.org/files/KB-JD-SL-38.pdf&quot; rel=&quot;nofollow&quot;&gt;Boudt and &lt;em&gt;al.&lt;/em&gt; (2011)&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-07T15:11:02.743" Id="19485" LastActivityDate="2011-12-07T22:12:42.513" LastEditDate="2011-12-07T22:12:42.513" LastEditorUserId="603" OwnerUserId="603" ParentId="19479" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to use the Perl &lt;a href=&quot;http://search.cpan.org/~iawelch/Statistics-Regression-0.53/Regression.pm&quot; rel=&quot;nofollow&quot;&gt;Statistics::Regression&lt;/a&gt; package to calculate a multivariate regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The results I am getting per each of my variables are: &lt;code&gt;Theta&lt;/code&gt; (the coefficient), &lt;code&gt;StdErr&lt;/code&gt; (I am not really sure about the meaning of this one) and &lt;code&gt;T-stat&lt;/code&gt; (which is the coefficient / &lt;code&gt;StdErr&lt;/code&gt;)&#10;On top of that I get &lt;code&gt;R^2&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional parameters the package calculates are: &lt;code&gt;rsq&lt;/code&gt;, &lt;code&gt;adjrsq&lt;/code&gt;, &lt;code&gt;sigmasq&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt;, and &lt;code&gt;ybar&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What would signify a good result: high &lt;code&gt;R^2&lt;/code&gt; value or high &lt;code&gt;T-stat&lt;/code&gt; values?&lt;/li&gt;&#10;&lt;li&gt;How do I determine the level of confidence of the result?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;edit: Thank you very much, chl, for the link below. I think it is very helpful and certainly made several things clearer. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that my lack of background in statistics is far from ideal but I was hoping I'd still be able to achieve something. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if I understand correctly, in order to appreciate the level of confidence it is suggested to examine the F-value (supposed to give an estimation of the model as a whole) and the P-values (supposed to give an estimation of each of the variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is this correct?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What is the exact formula to calculate the P-values? I understand they are dependent on the degrees of freedom but what would be the exact way to calculate it?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;F = model mean square / error mean square. Is that correct? If so, is the model mean square calculated by dividing the model sum of squares by the model degrees of freedom?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I see that the larger the F the more confidence I should have in rejecting the null hypothesis. Any rules of thumb of how large is large?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks once again and apologies for the very basic non-intelligent questions...&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-07T15:45:44.573" Id="19487" LastActivityDate="2011-12-08T08:46:12.127" LastEditDate="2011-12-08T08:46:12.127" LastEditorUserId="7863" OwnerUserId="7863" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;&lt;least-squares&gt;&lt;multiple-regression&gt;" Title="Confidence level for a multivariate linear regression" ViewCount="294" />
  
  <row Body="&lt;p&gt;Second suggestion (MANOVA is not my strong point but no-one else has attempted to answer this question so I'll have another go):&lt;/p&gt;&#10;&#10;&lt;p&gt;50 measured characters may be too many $Y$ variables with a sample size of 853. The various tests all assume the $Y$ variables have a multivariate normal distribution and even then the $F$ tests are based on asymptotic approximations (for more than 2 $Y$ variables and more than 2 model d.f.). Though (at least some of) the tests are reasonably robust to departures from multivariate normality with reasonable sample sizes and a handful of $Y$ variables, I suspect that for many $Y$ variables the tests may become highly sensitive to departures from multivariate normality and/or the asymptotic approximations start to require really huge sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Possibly someone else with more knowledge / relevant textbooks could confirm or deny this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-07T17:47:12.190" Id="19498" LastActivityDate="2011-12-07T17:47:12.190" OwnerUserId="449" ParentId="19427" PostTypeId="2" Score="0" />
  
  
&#10;$$&#10;he said if this quantity is small(kinda ambiguous), then $\eta_i$ is somewhat a nice approximation to $e_i$ spatially(i.e. $\eta_1$ is near $e_1$, something like that), but he didn't give any name of this statistic and what test he was trying to perform. I wonder what does this $\mu$ is trying to measure? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance, and I am no expert in this, so I may have chosen the wrong tag, correct me if I were wrong.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-12-08T04:11:41.267" Id="19512" LastActivityDate="2011-12-08T21:37:21.083" LastEditDate="2011-12-08T20:47:55.637" LastEditorUserId="4357" OwnerUserId="4357" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;" Title="Is there a name for this statistic? What type of distances it is measuring between two data sets?" ViewCount="108" />
  
  <row Body="&lt;p&gt;Yes, this is simply to ensure that all of the weak learners in the final solution classify the training data better than chance (which is essential for the theoretical properties of AdaBoost to hold).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-08T11:27:29.687" Id="19531" LastActivityDate="2011-12-08T11:27:29.687" OwnerUserId="7365" ParentId="10535" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You can be confident of your models if you cross-validate them!  You can do by &quot;rolling&quot; through your dataset one observation at a time, forecasting 1 step ahead.  At the end of the process compare your 1-step forecasts to the actual values to get some idea of how your model performs out-of-sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can find this process explained in more detail at &lt;a href=&quot;http://robjhyndman.com/researchtips/crossvalidation/&quot;&gt;the end of this post&lt;/a&gt;. You can find R code for cross-validating time series &lt;a href=&quot;http://robjhyndman.com/researchtips/tscvexample/&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://moderntoolmaking.blogspot.com/2011/11/functional-and-parallel-time-series.html&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://moderntoolmaking.blogspot.com/2011/11/time-series-cross-validation-2.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-08T19:20:06.463" Id="19563" LastActivityDate="2011-12-08T19:20:06.463" OwnerUserId="2817" ParentId="16545" PostTypeId="2" Score="7" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm trying to look at multiple comparisons (across levels of my between subjects factor) for a model with one between-subject factor and one within-subject factor. I was trying to use TukeyHSD on an aovlist object, and then found out that I couldn't do that.&lt;/p&gt;&#10;&#10;&lt;p&gt;After some browsing, I found how to do what I wanted using &lt;code&gt;lme()&lt;/code&gt; and &lt;code&gt;glht()&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;options(contrasts=c(&quot;contr.sum&quot;, &quot;contr.poly&quot;))&#10;lme_model=lme(dv ~ between*within, data frame, random=~1|ID,&#10;              correlation=corCompSymm(form=~1|ID))&#10;anova(lme_model, type=&quot;marginal&quot;)&#10;summary(glht(lme_model, linfct=mcp(between=&quot;Tukey&quot;)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It seems to work (the &lt;code&gt;anova()&lt;/code&gt; results seem to jive with what I was getting from &lt;code&gt;aov()&lt;/code&gt;), but I guess the problem is that I don't really understand what &lt;code&gt;lme()&lt;/code&gt; is, or what it's doing. Does anyone have any documentation that I can look at?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-08T23:19:08.893" FavoriteCount="1" Id="19571" LastActivityDate="2011-12-11T22:24:19.650" LastEditDate="2011-12-09T10:19:12.727" LastEditorUserId="930" OwnerUserId="7895" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;anova&gt;&lt;multiple-comparisons&gt;&lt;lme&gt;" Title="Multiple comparisons with ANOVA including one between and one within-subject effect" ViewCount="1872" />
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/19265/deciphering-output-from-r2winbugs&quot;&gt;Deciphering output from R2WinBUGS&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;How do I read in the quantiles from the following output? I am able to read in the mean, median and standard deviation by saying&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;bugs.output[[1]]$mean$p[i]&#10;bugs.output[[1]]$median$p[1]&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is how my output (called &lt;code&gt;bugs.output[[i]]&lt;/code&gt;) looks in R2WinBUGS:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          mean       sd      2.5%       25%       50%       75%     97.5%&#10;alpha      4.20338  1.14447   2.44975   3.39950   4.01550   4.82925   7.05405&#10;p[1]       0.16691  0.04036   0.09974   0.13720   0.16445   0.19222   0.25308&#10;p[2]       0.14286  0.03812   0.07665   0.11670   0.14020   0.16702   0.22630&#10;p[3]       0.21238  0.04240   0.13740   0.18295   0.21050   0.23845   0.30360&#10;p[4]       0.13051  0.03445   0.07023   0.10585   0.12845   0.15405   0.20300&#10;p[5]       0.14276  0.03917   0.07737   0.11640   0.14040   0.16550   0.22521&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I had posted a question related to this named as &lt;a href=&quot;http://stats.stackexchange.com/questions/19265&quot;&gt;Deciphering output from R2WinBUGS&lt;/a&gt;. I was unable to repost on the same thread.&lt;/p&gt;&#10;" ClosedDate="2011-12-10T09:44:15.570" CommentCount="3" CreationDate="2011-12-09T05:04:55.230" Id="19583" LastActivityDate="2011-12-09T10:23:04.923" LastEditDate="2011-12-09T10:23:04.923" LastEditorUserId="930" OwnerUserId="7899" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;bugs&gt;" Title="How to read quantiles from R2WinBUGS?" ViewCount="41" />
  
  <row Body="&lt;p&gt;The issue of multiple comparisons is a really big topic.  There have been many opinions and many disagreements.  This is due to many things; among others, it is partly because the issue is really important, and partly because there really is no ultimate rule or criterion.  Take a prototypical case:  You conduct an experiment with $k$ treatments and get a significant ANOVA, so now you wonder which treatment means differ.  How should you go about this, run $k(k-1)/2$ t-tests?  Although these tests would individually hold $\alpha$ at .05, the 'familywise' $\alpha$ (i.e., the probability that at least 1 type I error will occur) will explode.  In fact, the familywise error rate will be $1-(1-\alpha)^k$.  The question is, what defines a 'family'?  And there is no ultimate answer, beyond the trivial one that a 'family' is a set of contrasts.  Whether any particular set of contrasts should be considered a family is a subjective decision.  The 3rd, 17th, and 42nd analyses that I ever conducted in my life are a set of contrasts, and I could have adjusted my $\alpha$ threshold to insure that the probability of type I errors amongst them was held at 5%, but no one would find this sensical.  The question for you is whether you consider your contrasts to be a set in a meaningful sense, and only you can make that judgment.  I will offer some standard approaches.  Many analysts believe that if a set of contrasts come from the same experiment / data set, they should be treated as a family, and procedures (such as $\alpha$ adjustment) are necessary.  Others believe that even when contrasts come from the same experiment, if they are a-priori and orthogonal, special procedures are not required.  Both of these positions can be defended.  Finally, note also that procedures to control familywise error rates come at a cost--viz. increased type II error rates.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-09T06:54:05.997" Id="19587" LastActivityDate="2011-12-09T06:54:05.997" OwnerUserId="7290" ParentId="19572" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="19598" AnswerCount="1" Body="&lt;p&gt;How Breusch-Pagan can't reject the null for a series like that?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x = c(rnorm(10), rnorm(100,sd=10), rnorm(100,sd=25))&#10;&amp;gt; mod = lm(x[-1]^2~x[-210]^2)&#10;&amp;gt; plot(mod$res,type='l')&#10;&amp;gt; bptest(mod)&#10;&#10;        studentized Breusch-Pagan test&#10;&#10;data:  mod &#10;BP = 1.1085, df = 1, p-value = 0.2924&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The variance change drastically, could someone explain the reason?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Aidn1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-09T08:31:50.873" Id="19590" LastActivityDate="2011-12-09T15:37:03.017" LastEditDate="2011-12-09T10:18:00.520" LastEditorUserId="4257" OwnerUserId="5405" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;homogeneity&gt;" Title="Why does the Breusch-Pagan test fail?" ViewCount="595" />
  <row Body="&lt;p&gt;You might read my own current opinion about binary variables &lt;a href=&quot;http://stats.stackexchange.com/questions/16331/how-to-perform-principal-components-analysis-on-binary-yes-no-data-using-spss/16335#16335&quot;&gt;here&lt;/a&gt;. In short, it is not a sin to use binary vars with PCA if you use the analysis simply as variable-reduction technique - for example, for plotting purpose, - without attempting to interpret the components as latent features. If you go as far as to interpret you should better use factor analysis in proper sense, not PCA; and then binary variables posit a problem since factor analysis assumes contunuous variables, what binary variables are clearly not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-09T13:00:28.590" Id="19595" LastActivityDate="2011-12-09T13:00:28.590" OwnerUserId="3277" ParentId="19591" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have been collecting the number of new followers every day for about 6 months for 30 different twitter accounts and I know the exact time that they started following. I have been also collecting all tweets for these accounts during this time period.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm interested in whether some independent variables (tweet rate, number of mentions, retweets, links, sentiment of the tweets) are related to an increase in followers (or the dependent variable.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering what the appropriate approach is for this time series data. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I could use linear regression to see if the total amount of tweets per day predict the amount of new followers per day. However, I don't think that would be appropriate because actions people take don't immediately affect the number of followers. But I'm not sure what the time delay would be or if there is a different approach that would be more appropriate for this kind of data and the question I am asking. I am using R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-09T17:02:15.377" FavoriteCount="5" Id="19602" LastActivityDate="2012-08-27T01:00:32.277" OwnerUserId="7908" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;time-series&gt;" Title="Twitter data and regression time series" ViewCount="547" />
  
  <row Body="&lt;p&gt;Singular value decomposition (SVD) is not the same as reducing the dimensionality of the data.  It is a method of decomposing a matrix into other matrices that  has lots of wonderful properties which I won't go into here.  For more on SVD, see the &lt;a href=&quot;http://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;Wikipedia page&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Reducing the dimensionality of your data is sometimes very useful.  It may be that you have a lot more variables than observations; this is not uncommon in genomic work.  It may be that we have several variables that are very highly correlated, e.g., when they are heavily influenced by a small number of underlying factors, and we wish to recover some approximation to the underlying factors.  Dimensionality-reducing techniques such as principal component analysis, multidimensional scaling, and canonical variate analysis give us insights into the relationships between observations and/or variables that we might not be able to get any other way.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A concrete example: some years ago  I was analyzing an employee satisfaction survey that had over 100 questions on it.  Well, no manager is ever going to be able to look at 100+ questions worth of answers, even summarized, and do more than guess at what it all means, because who can tell how the answers are related and what is driving them, really?  I performed a factor analysis on the data, for which  I had over 10,000 observations, and came up with five very clear and readily interpretable factors which could be used to develop manager-specific scores (one for each factor) that would summarize the entirety of the 100+ question survey.  A much better solution than the Excel spreadsheet dump that had been the prior method of reporting results! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-09T20:28:04.357" Id="19614" LastActivityDate="2011-12-09T20:39:35.053" LastEditDate="2011-12-09T20:39:35.053" LastEditorUserId="7555" OwnerUserId="7555" ParentId="19607" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;It sounds like you might be more interested in estimating errors using the &lt;a href=&quot;http://www.jstatsoft.org/v29/i05/paper&quot;&gt;maximum-entropy bootstrap&lt;/a&gt;, rather than cross-validation.  This will allow you to generate multiple bootstraps of you data,  which you can then split into as many train/test sets as you like to calculate confidence intervals for your forecasts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rob Hyndman has some further discussion of &lt;a href=&quot;http://robjhyndman.com/researchtips/tscvexample/&quot;&gt;time series cross-validation on his blog&lt;/a&gt;, where he implements several different methods of &quot;rolling&quot; and forecasting, but it's mostly focused on implementation.  I have some &lt;a href=&quot;http://moderntoolmaking.blogspot.com/2011/11/time-series-cross-validation-2.html&quot;&gt;further implementations on my blog&lt;/a&gt; as well. Maybe the simplest approach would be to average your error across all of the time windows, and therefore ignore and potential correlations in errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as I can tell, the theoretical state of cross-validation for time-series data is somewhat behind the theoretical state of general cross-validation.  Intuitively, I expect error to increase as the horizon increases, which suggests that you should expect correlated errors across various forecast horizons.  Why does this worry you?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-09T21:54:04.850" Id="19618" LastActivityDate="2011-12-09T21:54:04.850" OwnerUserId="2817" ParentId="17932" PostTypeId="2" Score="5" />
  
&#10;$$&#10;$$
  
  <row Body="&lt;p&gt;The answer is in the title: this is called sequential Monte Carlo or particle filtering or population Monte Carlo. It is validated in wider generality as an iterated importance sampling scheme where each importance sample is used to generate the following sample. This is for instance covered in Chapter 14 of &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441919392&quot; rel=&quot;nofollow&quot;&gt;Monte Carlo Statistical Methods&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The specific issue of using the whole sequence of simulation is found in the liteature, but not in the direct way you propose: using all samples at once with the same weights does not behave nicely when some of the weights are huge (as when one starts with a poor guess). This is covered in the fantastic multiple mixture paper by &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.2813&quot; rel=&quot;nofollow&quot;&gt;Owen and Zhou&lt;/a&gt; (2000, JASA) and in our more recent adaptive version (when $T$ depends on the iteration $t$ and on the past simulations) called &lt;a href=&quot;http://xianblog.wordpress.com/2009/07/08/whats-amis/&quot; rel=&quot;nofollow&quot;&gt;AMIS&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-10T09:52:22.957" Id="19633" LastActivityDate="2011-12-10T09:52:22.957" OwnerUserId="7224" ParentId="19593" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Check out the &quot;&lt;a href=&quot;http://matthias.vallentin.net/probability-and-statistics-cookbook/&quot; rel=&quot;nofollow&quot;&gt;Probability and Statistics Cookbook&lt;/a&gt;&quot; by Matthias Vallentin.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-10T12:20:55.950" Id="19637" LastActivityDate="2011-12-10T12:20:55.950" OwnerUserId="2802" ParentId="19632" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="19663" AnswerCount="3" Body="&lt;p&gt;I have gotten a student job in the management department of a chain of 50 grocery stores. The job includes gathering daily statistics on the economy of the stores.&lt;/p&gt;&#10;&#10;&lt;p&gt;Every day a &lt;strong&gt;per-store revenue statistic&lt;/strong&gt; is made, comparing the revenue to matching day last year (sunday/sunday, monday/monday) and an accumulated statistic for the month. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also a &lt;strong&gt;per-store gross margin percentage statistic&lt;/strong&gt; is made.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now these are simple measures and I feel that there could be a lot more to be gained from the data, the data is specific down to the (cost price)/(sale price)/(number of sales)-level on every type of item.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore no or very few graphics are made and the data from the previous years are not taken into use.&lt;/p&gt;&#10;&#10;&lt;p&gt;Have any of you seen a similar problem? Do you have any ideas on how to proceed? Any easy-to-read, informative types of graphs you would want to share regarding these types of data?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-11T09:59:53.797" FavoriteCount="1" Id="19661" LastActivityDate="2011-12-13T14:28:10.463" LastEditDate="2011-12-11T15:35:09.283" LastEditorUserId="88" OwnerUserId="2807" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;application&gt;" Title="Ideas for visualising and analysing grocery store data" ViewCount="330" />
  
&#10;&amp;amp;= (1-\alpha)^2,\\
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have to regress the following equation in Stata:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$R_{i,t} = \delta{i} + \sum_i \alpha_i*W_i + \sum_i \beta_i * R_{i,t−n} + \mu_{i,t}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm ok with this regression. My problem comes out when I have to regress the second regression of the model I'm studying, that is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$|μ_{i,t}| =  \omega_i + ϕiM_{i,t} + \sum_i \rho_i*|μ_{i,t−j}| + \phi_iiA_{i,t} + \eta_{i,t} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;In this regression I regress the residual I found in the first regression on its past value.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I get the value of absolute residual? Is exist a particular formula by computing? Or do I have to consider simply the absolute value of the number $\mu_{i,t}$ I find in the first regression?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-12-12T12:32:44.637" Id="19696" LastActivityDate="2011-12-12T17:40:23.503" LastEditDate="2011-12-12T17:40:23.503" LastEditorDisplayName="user5644" OwnerUserId="7947" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;stata&gt;&lt;residuals&gt;" Title="Estimate absolute residuals in Stata" ViewCount="462" />
  
  
  <row Body="&lt;p&gt;If the data points are aggregates (for example state or country level averages) then it might not be a bad idea to fit a regression with just three points, since they'd represent many more underlying observations.  Weighted least squares would probably be a better strategy than just OLS, but it would depend on the situation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-12T19:36:30.640" Id="19708" LastActivityDate="2011-12-12T19:36:30.640" OwnerUserId="7591" ParentId="19706" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Let $x_i$ be the decision of person $i$, define $x_i=1$ if she wants to swim and define $P(x_i=1)=0.35=p_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you want to find the probability that $Y=\sum_{i=1}^{50} x_i \geq 24$.&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(Y \geq 24) = \sum_{i=24}^{50}C{{n}\choose{i}}p_i^i(1-p_i)^{n-i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;that is, all the possible combinations giving more than 24 people willing to swim.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;NB&lt;/em&gt;: this assumes, of course, that the decision of each person is iid.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-12T21:52:46.300" Id="19720" LastActivityDate="2011-12-12T21:52:46.300" OwnerUserId="6300" ParentId="19710" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;&lt;em&gt;However, the second measurement has a lower mean, which can therefore drive overall variance.&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know this for sure?  If that's so, you need to figure out what the relationship is and transform the variance to a more stable scale (e.g. log).&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise, try the naive approach and use a folded F test, dividing the larger sample variance by the smaller one, and find the p-value for the resulting F statistic.  This approach assumes the samples are independent, which might be reasonable if your overall sequence is stationary or the sample times are widely separated.  If not, you probably will need to fit some sort of time series model to the data (ARIMA maybe), and compare the variance of the &lt;strong&gt;residuals&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-12T22:44:11.660" Id="19728" LastActivityDate="2011-12-12T22:44:11.660" OwnerUserId="5792" ParentId="19724" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;It is also important to keep in mind that a strong assumption in the usual F test for equality of variances is that of normality -- the test is very sensitive to the assumption of normality so the resulting p-vlaues can be very distorted.  Levene suggested a simple alternative and Brown-Forsythe followed with a test statistic that aims at increasing robustness.&#10;You can start in Wikipedia&#10; &lt;a href=&quot;http://en.wikipedia.org/wiki/Brown%E2%80%93Forsythe_test&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Brown%E2%80%93Forsythe_test&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-12-13T00:11:56.670" Id="19730" LastActivityDate="2011-12-13T00:11:56.670" OwnerUserId="7967" ParentId="19724" PostTypeId="2" Score="1" />
&#10;p_{a,\infty}(z)/g_{\alpha}(z) \propto e^{- \alpha(z - a )}e^{-z^{2}/2}
&#10;\end{cases}
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset&quot; rel=&quot;nofollow&quot;&gt;scikit-learn 20-newsgroups-text-dataset&lt;/a&gt; has&#10;11314 train + 7532 test samples with 10,000 or more sparse features.&#10;The newsgroup categories are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;alt.atheism&#10;comp.graphics&#10;comp.os.ms-windows.misc&#10;comp.sys.ibm.pc.hardware&#10;comp.sys.mac.hardware&#10;comp.windows.x&#10;misc.forsale&#10;rec.autos&#10;rec.motorcycles&#10;rec.sport.baseball&#10;rec.sport.hockey&#10;sci.crypt&#10;sci.electronics&#10;sci.med&#10;sci.space&#10;soc.religion.christian&#10;talk.politics.guns&#10;talk.politics.mideast&#10;talk.politics.misc&#10;talk.religion.misc&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For 2, 3, 4, 5 of these newsgroups (the worst ones) I get&lt;br&gt;&#10;83.2 82.6 82.2 80.6 % correct,&#10;using the fast&#10;&lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot; rel=&quot;nofollow&quot;&gt;sgd classifier&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;(The first run of &lt;code&gt;fetch_20newsgroups&lt;/code&gt; will take a while to download and cache the data.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-13T11:03:25.910" Id="19749" LastActivityDate="2011-12-13T11:03:25.910" OwnerUserId="557" ParentId="19123" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/16230/fourier-data-with-non-integer-periods-correcting-for-phase-bias&quot;&gt;Fourier data with non-integer periods, correcting for phase bias&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;I have a not perfectly but quit periodic signal, with a length of only a few periods.  What methods are available for automatically measuring its period?&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried using the maximum of the Fourier transform, but this essentially only tells me how many integer periods are there in the full length of the signal.  If I only have a few repetitions, this will give me an inaccurate estimate of the period.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let me illustrate: suppose we have a perfectly periodic function $sin(x)$ sampled at intervals of 0.01 between 0 and 30.  The Fourier transform will tell me that the signal is about 5 periods long, which gives an inaccurate period length of 30/5 = 6, instead of the true one which is $2\pi \approx 6.28$.  Since this example signal is almost perfectly periodic and sampled at a high density, it is possible to do much better than this.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What robust ways are there for period detection that might work in this scenario?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;  &lt;a href=&quot;http://ge.tt/8Lt9S8B?c&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is an example signal of 1000 sample points to test on. I am expecting a period of around ~150, by manual measurement (i.e. 6 repetitions in a length of ~898). You may argue that it should be double that value if you look closely at the alternating height of the small humps, but as the signal is not &lt;em&gt;perfectly&lt;/em&gt; periodic, for this application I need ~150.&lt;/p&gt;&#10;" ClosedDate="2011-12-13T20:34:11.080" CommentCount="4" CreationDate="2011-12-13T13:28:16.597" Id="19761" LastActivityDate="2011-12-13T20:27:21.417" LastEditDate="2011-12-13T20:27:21.417" LastEditorUserId="4764" OwnerUserId="4764" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;signal-processing&gt;" Title="Detecting the period of a signal with a length of only a few periods" ViewCount="80" />
  <row Body="&lt;p&gt;The team behind randomJungle claims that is an order of magnitude faster than &#10;the R randomForest implementation and uses an order magnitude less memory.&#10;A package for randomJungle is being developed for R but I can't get to build yet.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://r-forge.r-project.org/projects/rjungler/&quot; rel=&quot;nofollow&quot;&gt;https://r-forge.r-project.org/projects/rjungler/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-13T14:58:42.663" Id="19767" LastActivityDate="2011-12-13T14:58:42.663" OwnerUserId="7982" ParentId="10001" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If the spike is in the pacf and pacf(4) is absolutely greater than .5 , you would prefer an AR representation of the form y(t)-phi1*y(t-4)=a(t) due to invertability restrictions otherwise an ma model would probably be adequate.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data is quarterly this can be accomplished with (0,0,0)(1,0,0) in terms of pdq PDQ&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively you might use software that allows you to constrain a p=4 model with three coefficients being absent. &lt;/p&gt;&#10;&#10;&lt;p&gt;From your description , I would think that you may have some deterministic structure present like a first quarter effect rather than an auto-regressive structure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-13T17:44:41.707" Id="19775" LastActivityDate="2011-12-13T17:44:41.707" OwnerUserId="3382" ParentId="19519" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="19780" AnswerCount="1" Body="&lt;p&gt;Consider $Y_{ij} = \mu + u_i + e_{ij}$ where $\mu$ is a constant and $u_i ∼ N(0,\sigma_u^2
  
&#10;= {\rm var}(u_{i}) = \sigma^{2}_{u}
  
  
  
  <row Body="&lt;p&gt;One good technique for solving problems that seem to involve arbitrary numbers (like 4, 3, and 2) is to &lt;strong&gt;look at a variant involving simpler or smaller values of those numbers&lt;/strong&gt;.  Let's use this to revisit the parts of the question:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;What are the chances of drawing a red marble after a green?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because one point of possible confusion is that there are three colors, consider a vase having just &lt;em&gt;one&lt;/em&gt; of each color, red, white, and green.  This makes it feasible to enumerate all the possibilities of two draws.  Abbreviating the colors as R, W, and G, and writing the outcome of the first draw at the left, all possible results are&lt;/p&gt;&#10;&#10;&lt;p&gt;{RW, RG, WR, WG, GR, GW}.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of these, only {GR, GW} consist of drawing a green marble first.  They have equal probabilities, because after the green was drawn, there remained just one R and one W in the vase.  Therefore the chance of drawing a red after a green is 1/2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let us consider how to generalize this reasoning.  It looks like things got simpler once we &lt;em&gt;limited the list only to those results where G was drawn first.&lt;/em&gt;  After that, there was one less G in the vase and the question became &quot;in a vase with 1-1 = 0 greens, 1 white, and 1 red, what is the chance of drawing a red?&quot;  In the present setting, we need to replace these numbers by the arbitrary quantities in the problem statement: &quot;in a vase with 2-1 = 1 greens, 4 white, and 3 red, what is the chance of drawing a red?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;If this newer problem still seems difficult (because there are three colors), consider that &lt;em&gt;the restatement refers to one color only.&lt;/em&gt;  The chances therefore must be the same as if we were color blind and could not tell green from white: &quot;in a vase with 3 red marbles and 1+4 = 5 &lt;em&gt;non-red&lt;/em&gt; marbles, what is the chance of drawing a red?&quot;  The answer is obvious.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;What are the chances of drawing two white marbles?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Once more we can capitalize on the color-blindness argument and rephrase this question as &quot;in a vase with 4 white marbles and 3+2 = 5 non-white marbles, what are the chances of drawing two white marbles?&quot;  Again, let's consider a simpler version in which the vase contains only two whites and one non-white (abbreviated &quot;B&quot;).  Because the two white marbles are distinct, we must label them to keep them apart: call them W1 and W2, say.  The possible results are&lt;/p&gt;&#10;&#10;&lt;p&gt;{W1W2, W1B, W2W1, W2B, BW1, BW2}.&lt;/p&gt;&#10;&#10;&lt;p&gt;All are equally likely (because at any stage each &lt;em&gt;individual&lt;/em&gt; marble has neither greater nor lesser chances of being drawn than any other individual).  In two of the six cases both whites are drawn, whence the answer to the simplified question is 2/6.&lt;/p&gt;&#10;&#10;&lt;p&gt;Turning to the original question with four whites and 5 non-whites, let's distinguish them as W1, W2, W3, W4 and B1, B2, B3, B4, B5.  The larger quantities create a challenge, because there are many possibilities to enumerate ($\binom{9}{2} = 36$) so let's see whether the problem can be analyzed in stages.  Go back to the case of two whites and one non-white.  For the &lt;em&gt;first&lt;/em&gt; draw, the chance of a white obviously is 2/3.  This leaves a vase with one less white and the same number of non-whites: one of each.  Evidently the chance of drawing a white, &lt;em&gt;given&lt;/em&gt; that a white has just be drawn (i.e., removed from the vase), is 1/2.&lt;/p&gt;&#10;&#10;&lt;p&gt;This reasoning is reflected in our earlier enumeration: of the six cases listed, 2/3 of them (that is, 2/3 * 6 = 4) consist of an initial white draw: {W1W2, W1B, W2W1, W2B}.  Of these, 1/2 (that is 1/2 * 4 = 2) are followed by another white draw, {W1W2, W2W1}.  It appears that the correct mathematical operations to follow are these:&lt;/p&gt;&#10;&#10;&lt;p&gt;a. Find the total number of possibilities, 6.&lt;/p&gt;&#10;&#10;&lt;p&gt;b. &lt;em&gt;Multiply&lt;/em&gt; by the chances of drawing a white, 2/3.&lt;/p&gt;&#10;&#10;&lt;p&gt;c. &lt;em&gt;Multiply&lt;/em&gt; that by the chances of drawing a white &lt;em&gt;after a white has been removed,&lt;/em&gt; 1/2.&lt;/p&gt;&#10;&#10;&lt;p&gt;d. Finally, as always, divide the result by the total number of possibilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is 6 * 2/3 * 1/2 / 6.  But the sixes represent the same thing--the total number of possibilities--and they &lt;em&gt;cancel&lt;/em&gt; in the calculation, leaving just 2/3 * 1/2 = 1/3, exactly the answer we obtained directly.&lt;/p&gt;&#10;&#10;&lt;p&gt;The beauty of this is that &lt;em&gt;we do not need to know how many total possibilities there are.&lt;/em&gt;  We just multiply.&lt;/p&gt;&#10;&#10;&lt;p&gt;The general rule is, that when one outcome follows another, to find the chance of both outcomes in succession, &lt;em&gt;you multiply the probabilities.&lt;/em&gt;  In math notation this can be written like&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr[WW] = \Pr[W] \times \Pr[W\ \vert\ W].$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To solve the original problem, compute the chances represented by the right hand side and multiply.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;What are the chances you draw a white and a red marble?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to solve this is to compute the chance of drawing a white and then a red and also compute the chance of drawing a red and then a white.  The first result represents a collection of possible outcomes; so does the second; &lt;em&gt;and no outcomes are common to both results.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Look at the situation with, say, a vase with two whites (W1 and W2), a red, and a green.  Here are all the possible results:&lt;/p&gt;&#10;&#10;&lt;p&gt;{W1W2, W1R, W1G, W2W1, W2R, W2G, RW1, RW2, RG, GW1, GW2, GR}.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 12 of them.  The cases where a white and then a red are drawn are&lt;/p&gt;&#10;&#10;&lt;p&gt;{W1R, W2R}&lt;/p&gt;&#10;&#10;&lt;p&gt;and the cases where a red and then a white are drawn are&lt;/p&gt;&#10;&#10;&lt;p&gt;{RW1, RW2}.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore the total number of cases is 2 + 2 = 4, because no case appears in both lists.  (If it did, it would be wrong to sum the counts because the common case(s) would be doubly counted.)  Consequently, reasoning as before, the chance of obtaining a white and a red equals 4/12 = 1/3.&lt;/p&gt;&#10;&#10;&lt;p&gt;This reasoning suggests that when two events have no outcomes in common, the chances &lt;em&gt;add&lt;/em&gt;.  Consequently we would compute&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr[\text{W and R}] = \Pr[WR] + \Pr[RW] = 1/6 + 1/6 = 1/3$$&lt;/p&gt;&#10;&#10;&lt;p&gt;using the rule from the second question.  It should be clear now how to proceed with the original question and the numbers in it.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;To summarize,&lt;/strong&gt; by looking at simplified versions of the questions, we were able to develop two simple rules:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;When one outcome follows another, to find the chance of both outcomes in succession, you multiply the probabilities.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When two events have no outcomes in common, you add the probabilities.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;With these rules, most elementary probability problems can be solved easily.&lt;/p&gt;&#10;&#10;&lt;p&gt;I advocate &lt;em&gt;not memorizing&lt;/em&gt; mathematical formulas like those shown here.   Doing so is unenlightening.  Of far greater value is remembering the reasoning that led up to them.  The fundamental methods used here included:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Analyze simpler versions of a problem, using a complete enumeration of all results if need be.  Then generalize.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When necessary, label distinct objects that are indistinguishable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Simplify the problem by removing distinguishing elements of the objects when it does not matter (the &quot;color blind&quot; argument).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Reduce the evaluation of probabilities to situations where you are drawing one of $n$ equally likely objects out of a vase and want to know the chance that it is one of $k$ such objects, because then the chance is obviously $k/n$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-12-14T20:12:41.857" Id="19831" LastActivityDate="2011-12-14T20:43:52.820" LastEditDate="2011-12-14T20:43:52.820" LastEditorUserId="919" OwnerUserId="919" ParentId="19827" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;One classic approach is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rand_index&quot; rel=&quot;nofollow&quot;&gt;adjusted Rand index&lt;/a&gt;, which is a chance-corrected measure of similarity between two partitions (a clustering is, after all, a partition).  This is already implemented in R, in the mclust package (see &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/mclust/html/adjustedRandIndex.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;).  This value of the adjusted Rand index always lies between -1 and 1, and the index is not a metric (e.g., it doesn't satisfy the triangle inequality).  It has the nice property of being able to compare partitions of different sizes (i.e., clusterings containing different numbers of clusters).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-14T20:39:41.563" Id="19834" LastActivityDate="2011-12-14T20:39:41.563" OwnerUserId="8009" ParentId="15548" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In general, because continuous data usually has a dispersion attribute, the Pearson-Chi Square test doesn't make any sense. This is because the expected value in the denominator is attributed to the mean-variance relationship of categorical data. If you're looking for a generalized version of the Pearson chi-squared test statistic, you must specify what attribute of the chi-square statistic is interesting to you. Because it is the score test for a logistic regression model, you could consider using a score test for any other regression model, like linear regression, or use a different test statistic like those from the Wald or Likelihood ratio tests. This addresses the inferential aspect of the test, whether the row and column variables are independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternately, you may be interested in this test statistic as a measure of goodness of fit or calibration for a predictive model. If you can sensibly bin the data into distinct groups, you can still use the chi-square test statistic, or a kappa statistic measuring agreement between observed and predicted data, especially useful with more than 2 distinct groups in the outcome. I would prefer using the continuous scale and using the MSE as a measure of predictive accuracy, more preferrably the cross-validated MSE of your predictive model. This can also be standardized into a measure like the R^2 value which has associated significance tests as well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-15T00:35:15.920" Id="19846" LastActivityDate="2011-12-15T00:35:15.920" OwnerUserId="8013" ParentId="19845" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;For question 2, I would start by looking at the distributions of the percentages of each group that have a given trait.  e.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A &amp;lt;- c(126,86,63,54,47,40,32,32,29,29,27,26,20,18,14)&#10;B &amp;lt;- c(357,196,185,137,95,74,45,69,64,49,54,80,62,41,56)&#10;C &amp;lt;- c(348,139,162,126,82,69,35,63,40,42,40,55,44,29,35)&#10;N &amp;lt;- c(382,207,193,143,100,80,45,70,70,53,55,84,67,42,57)&#10;&#10;A &amp;lt;- A/N&#10;B &amp;lt;- B/N&#10;C &amp;lt;- C/N&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then you can make some stem and leaf plots to examine the distributions:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; stem(A)&#10;&#10;  The decimal point is 1 digit(s) to the left of the |&#10;&#10;  2 | 5&#10;  3 | 01338&#10;  4 | 123679&#10;  5 | 05&#10;  6 | &#10;  7 | 1&#10;&#10;&amp;gt; stem(B)&#10;&#10;  The decimal point is 2 digit(s) to the left of the |&#10;&#10;   90 | 4&#10;   92 | 5555&#10;   94 | 70289&#10;   96 | 6&#10;   98 | 226&#10;  100 | 0&#10;&#10;&amp;gt; stem(C)&#10;&#10;  The decimal point is 1 digit(s) to the left of the |&#10;&#10;  5 | 7&#10;  6 | 15679&#10;  7 | 389&#10;  8 | 2468&#10;  9 | 01&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All three look somewhat normal, but that's pretty subjective.  Only the .71 in &lt;code&gt;stem(A)&lt;/code&gt; appears to be an outlier to me.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-15T15:10:48.473" Id="19867" LastActivityDate="2011-12-15T15:10:48.473" OwnerUserId="2817" ParentId="19852" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;In his machine learning online course, &lt;a href=&quot;http://www.ml-class.org/course/auth/welcome&quot;&gt;Andrew Ng&lt;/a&gt; suggests using Octave/Matlab.&lt;/p&gt;&#10;&#10;&lt;p&gt;I recommend you enroll in the next edition of this course: it is really useful and you will learn many things about Octave and about the different machine learning algorithms.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 1&lt;/strong&gt;: I agree with other people who prefer to work in R. However, in solving the problems of machine learning, most of your calculations will be in matrix form, and as pointed out by @Wayne, Matlab or Octave languages are very popular because of their power.&#10;You may want to have a look at the solutions to machine learning course exercises proposed by other students; surely you can learn some things from them:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://github.com/gkokaisel/MachineLearning&quot;&gt;Gkokaisel Github&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://github.com/merwan/ml-class&quot;&gt;Merwan Github&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-16T06:30:11.250" Id="19896" LastActivityDate="2011-12-17T20:49:08.403" LastEditDate="2011-12-17T20:49:08.403" LastEditorUserId="919" OwnerUserId="221" ParentId="19889" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;According to &lt;em&gt;Hamilton (1994)&lt;/em&gt;, page 1-5:&lt;/p&gt;&#10;&#10;&lt;p&gt;suppose a process where:&#10;$y_{t} = b \times y_{t-1} + w_{t}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $y_{t-1}$ is the realisation in the previous period and $w_{t}$ is some random innovation.&lt;/p&gt;&#10;&#10;&lt;p&gt;The long run effect therefore is the effect on $y_{t+1}$ from a permanent increase in $w$. Hence the long-run effect is ${1}/{(1-b)}$ for this special case (or the expected value of the function). For the short run effect I cannot find a proper source right now but I remember it being the coefficient ($b$ in this case).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as far as I know the term is far more common for Vector Error Correction models. Hope this helps a bit.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-16T12:36:11.233" Id="19909" LastActivityDate="2011-12-16T15:40:10.897" LastEditDate="2011-12-16T15:40:10.897" LastEditorUserId="8041" OwnerUserId="8041" ParentId="19907" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;All the statistics should be performed on the residuals of the final model. Using Johansen's test you can identify the cointegration rank and get the estimates of the cointegration relationships. Then you can reestimate VAR model (in levels or in VECM form) using these relationships and then you can test the residuals from this model. In R, there is a special function, which converts from VECM to VAR. The following example (page 85) in the book &quot;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387759662&quot; rel=&quot;nofollow&quot;&gt;Analysis of integrated and cointegrated time series with R&quot;&lt;/a&gt; I think is very illustrative in your case.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(vars)&#10;vecm.level &amp;lt;− vec2var(vecm,r=2) &#10;arch.test(vecm.level)   &#10;normality.test(vecm.level)  &#10;serial.test(vecm.level) &#10;predict(vecm.level) &#10;irf(vecm.level,boot=FALSE)  &#10;fevd(vecm.level)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-12-16T13:17:08.140" Id="19910" LastActivityDate="2011-12-16T13:17:08.140" OwnerUserId="2116" ParentId="19890" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="19923" AnswerCount="1" Body="&lt;p&gt;In a sequential (on-line) tree algorithm, I'm trying to estimate class label densities using a histogram. The algorithm grows a tree by generating test functions at each decision node, which correspond to the features in the data set. That is to say each feature is used to create a test in the form of $g(x) &amp;gt; 0$ that's used to decide the left/right propagation of samples down each node of the binary tree. If the result of that test exceeds some threshold theta, then samples are propagated down the right branch, otherwise they're propagated down the left branch.&lt;/p&gt;&#10;&#10;&lt;p&gt;When training the tree, I have to choose the best test to use at each decision node, which is accomplished using a quality measurement in the form&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IqiZG.png&quot; alt=&quot;gain with respect to a test s&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $R_{jls}$ and $R_{jrs}$ are the left and right partitions made by test s, which corresponds to the data greater than or less than the threshold theta, $(p_i^j)$ is the label density of class $i$ in node $j$ (or $jls/jrs$ for the label densities of class $i$ in the left or right child nodes), $K$ is the total number of classes, and $|x|$ denotes the number of samples in a partition.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, the gain with respect to a test $s = g(x)$ depends on the entropy of class labels within that feature. This requires knowing the class label density at each decision node.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, my uncertainty lies in how can this be accomplished with a histogram. A histogram for each feature would have N bins of size range/N, where range is the difference between the max and min values that each feature takes. If we don't have a priori knowledge of this range, we can keep track of the max/min values as we get more training data. This histogram would track the number of samples falling within each range, but it tells nothing about the class labels.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another option is to have a histogram for each class, for each feature, for each node. So, you'd be keeping track of the number of samples having a particular class label given a range of feature values (each bin).&lt;/p&gt;&#10;&#10;&lt;p&gt;This solution seems to be more in line with the above equation, since we need the label densities for each class and for each node, but I just want to verify that I'm on the right track. For reference, see part 2.1.2 of &lt;a href=&quot;http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=0150862F81118642C61ED6CDF409828F?doi=10.1.1.150.1671&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Saffari et al., 2009. On-line Random Forests. &lt;em&gt;Computer Vision&#10;  Workshops (ICCV Workshops), 2009 IEEE 12th International Conference&lt;/em&gt;.&#10;  DOI 10.1109/ICCVW.2009.5457447&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2011-08-31T10:00:07.517" FavoriteCount="2" Id="19915" LastActivityDate="2013-07-05T23:31:25.823" LastEditDate="2013-07-05T23:31:25.823" LastEditorUserId="805" OwnerDisplayName="kmore" OwnerUserId="6164" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;histogram&gt;" Title="Using a histogram to estimate class label densities in a tree learner" ViewCount="348" />
  
  
  
  
  <row Body="&lt;p&gt;Perhabs &lt;a href=&quot;http://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf&quot; rel=&quot;nofollow&quot;&gt;nonlinear Quantile Regression&lt;/a&gt;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-18T23:01:53.750" Id="19973" LastActivityDate="2011-12-18T23:01:53.750" OwnerUserId="1050" ParentId="19912" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What do you want to know, is that really about R or about the $F_\max$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;From what I understand, $F_\max(df,k)$ is the distribution of the maximum ratio between empiric variances computed on k gaussian samples (all estimating the same &quot;true variance&quot; σ²), all with the same degrees of freedom df (= sample size - 1). &lt;/p&gt;&#10;&#10;&lt;p&gt;As empiric variances follow a (scaled) χ² distribution, you can easily generate $F_\max$ values :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;k &amp;lt;- 5; N &amp;lt;- 10000; df &amp;lt;- 14&#10;X &amp;lt;- matrix( rchisq(k*N, df=df), ncol=k)&#10;maxv &amp;lt;- apply(X,1,max)&#10;minv &amp;lt;- apply(X,1,min)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot their histogram:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hist(maxv/minv,freq=FALSE,breaks=100)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And superimpose the distribution given by &lt;code&gt;dmaxFratio()&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- seq(0,50,by=0.1)&#10;lines(x, dmaxFratio(x, df, k), col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now that we see that this is working, it appears that the first block of code could have been shorcuted by  &lt;code&gt;rmaxFratio(N, df, k)&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can use this in anova to test for homogeneity of variance between $k$ groups&#10;all with $df$ degrees of freedom. To get a $p$-value, compute the ratio &lt;code&gt;r&lt;/code&gt; of the smallest variance to the highest, the $p$-value is &lt;code&gt;pmaxFratio(r, df, k, lower.tail=FALSE)&lt;/code&gt;. You can also compare &lt;code&gt;r&lt;/code&gt; to the quantile &lt;code&gt;qmaxFratio(0.95, df, k)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have more specific questions, please... be more specific :)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-19T07:40:49.553" Id="19979" LastActivityDate="2011-12-19T07:46:44.553" LastEditDate="2011-12-19T07:46:44.553" LastEditorUserId="8076" OwnerUserId="8076" ParentId="19975" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Your problem is much like any other machine learning problem: you have some data and you use some of it as training data, to tweak your algorithms, then you use some of of the data (that wasn't used for training) to choose a &quot;winner&quot; algorithm, and finally you use some of the data (that wasn't used for training or testing) to give yourself an idea of how your winner will work in the Real World.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, the initial data is artificial. The question is: how well does the artificial data mimic the Real World where you intend to use your technique? Your technique reflects a model of the world that is simplified. The main fear would be that your data-generation method reflects the same simplifications, and thus provides data that fits your model well, as opposed to your model fitting the Real World well.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Even if your data were not artificial, the issue still arises: when you say a technique is &quot;superior&quot;, all that you can really say is that you were able to adjust it to work better on the data on which you tested it. If you did your work well and the technique is sound, it will probably be a good choice on similar data. You have to be careful with what you mean by &quot;superior&quot;.)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-19T13:42:54.023" Id="19984" LastActivityDate="2011-12-19T13:42:54.023" OwnerUserId="1764" ParentId="19972" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;If you want to use something out of box, Weka could be a great starting point. There is no need to program anything. You import your data, visualize it and play around with different models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next up in chain would be R. There is some learning curve associated - especially with munging your data to fit into R data structures but once you get over that, you have tons of libraries which offer all the machine learning capabilities without much effort.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next up would be hand programming the machine learning algorithms. Since you are already using Octave and looking for alternatives, maybe what you want is not to hand code algorithms in some other system but to just use the libraries written by other people.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you go down the R path, you might find book by Luis Torgo (Data Mining with R: Learning with Case Studies) very useful (disclosure: no affiliation). It describes in depth case studies which you can adapt to your problem.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-19T14:25:27.700" Id="19987" LastActivityDate="2011-12-19T14:25:27.700" OwnerUserId="7994" ParentId="19889" PostTypeId="2" Score="6" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm familiar with post-hoc testing with &lt;code&gt;ANOVA&lt;/code&gt; for exploring differences between a sequence of groups, but recently I've been reading about Change Point Analysis (especially the &lt;code&gt;R&lt;/code&gt; packages &lt;code&gt;bcp&lt;/code&gt;, &lt;code&gt;changepoint&lt;/code&gt; and &lt;code&gt;strucchange&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like those packages only handle data where there is one data point per unit of time. I'm curious if they can be used with data where there are multiple data points per unit of time. Here's some example data representing the measurement of a single continuous variable on a number of specimens that have been dated to specific moments in time (no repeated measurements):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a&amp;lt;-data.frame(time=&quot;1000&quot;,x=rnorm(10,12,3))&#10;b&amp;lt;-data.frame(time=&quot;2000&quot;,x=rnorm(50,13,4))&#10;c&amp;lt;-data.frame(time=&quot;3500&quot;,x=rnorm(50,12,4))&#10;d&amp;lt;-data.frame(time=&quot;5000&quot;,x=rnorm(7,14,5))&#10;e&amp;lt;-data.frame(time=&quot;7000&quot;,x=rnorm(20,10,3))&#10;f&amp;lt;-data.frame(time=&quot;7500&quot;,x=rnorm(15,11,3))&#10;g&amp;lt;-data.frame(time=&quot;9000&quot;,x=rnorm(15,10,5))&#10;h&amp;lt;-data.frame(time=&quot;9500&quot;,x=rnorm(35,30,2))&#10;i&amp;lt;-data.frame(time=&quot;10000&quot;,x=rnorm(30,28,4))&#10;a2i&amp;lt;-rbind(a,b,c,d,e,f,g,h,i) &#10;&#10;library(ggplot2)&#10;a2i$time&amp;lt;-as.numeric(levels(a2i$time))[a2i$time] &#10;ggplot(a2i,aes(time,x))+stat_smooth()+geom_point()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cASXS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's what I'd be most grateful for some advice on...&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1. Would it be valid to do the Change Point Analysis on a vector like the means or medians of the groups? That would allow me to start with a 'one data point per unit of time' input format which would suit the &lt;code&gt;R&lt;/code&gt; packages, as I understand them. I've seen it done with environmental data like monthly gas concentrations (from daily observations), but I thought I'd check.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Q2. Is there a kind of Change Point Analysis that I can do on the raw data in &lt;code&gt;a2g&lt;/code&gt; that will give me some measures of the probabilities of changes across the sequence? For example, something that will detect the change from time=9000 to time=9500, using all the data points in the sample? I'm guessing that if it was possible, someone would already have implemented and I just need a pointer to the relevant function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q3. In case Q2 can be answered 'yes', would the method change if the distribution of each group's values was non-normal (unlike my sample data)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Q4. If Change Point Analysis is completely the wrong approach here, please let me know. I'm basically just curious about methods other than &lt;code&gt;ANOVA&lt;/code&gt; for these kinds of data. Any other suggestions would be most welcome.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T10:11:27.207" FavoriteCount="1" Id="20026" LastActivityDate="2013-05-23T15:24:02.640" OwnerUserId="7744" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;time-series&gt;&lt;anova&gt;&lt;change-point&gt;" Title="Is is possible (or advisable) to do Change Point Analysis on sequence of groups with R?" ViewCount="477" />
  
  
  <row AcceptedAnswerId="20054" AnswerCount="2" Body="&lt;p&gt;I wish to better understand the pros/cons for using either loess or a smoothing splines for smoothing some curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another variation of my question is if there is a way to construct a smoothing spline in a way that will yield the same results as using loess.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any reference or insight are welcomed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T13:22:16.890" FavoriteCount="3" Id="20032" LastActivityDate="2011-12-21T13:55:46.500" OwnerUserId="253" PostTypeId="1" Score="14" Tags="&lt;regression&gt;&lt;splines&gt;&lt;loess&gt;" Title="Comparing smoothing splines vs loess for smoothing?" ViewCount="2361" />
  <row Body="&lt;p&gt;This does not handle orthogonal polynomials and implements ordinary (not orthogonal which IMHO are not worth the effort with modern stat computing algorithms) polynomials and restricted cubic splines (natural splines), but the R &lt;code&gt;rms&lt;/code&gt; package is useful in a similar context.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(rms)&#10;f &amp;lt;- ols(y ~ x1 + rcs(x2,4) + pol(x3,3)) # spline for x2 with 4 knots; cubic for x3&#10;Function(f)   # derive R function containing algebraic expression for predictions&#10;latex(f)      # compose LaTeX markup for pretty algebraic form of fitted model&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T13:33:49.180" Id="20033" LastActivityDate="2011-12-20T14:37:15.770" LastEditDate="2011-12-20T14:37:15.770" LastEditorUserId="4253" OwnerUserId="4253" ParentId="15814" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;An easy way to build an ensemble is by using a random forest.  I'm fairly sure weka has a random forest algorithm, and if other tree-based models are performing well it's worth trying out.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also build your own ensemble by training multiple (say 50 or 100) J48 decision trees and using them to &quot;vote&quot; on the classification of each object.  For example, if 60 tress say a given observation belongs to class &quot;A&quot;, and 40 say it belongs to class &quot;B&quot;, you classify the object as class &quot;A.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can further improve such an ensemble by training each tree on a random sub-sample of the training data.  This is called &quot;bagging,&quot; and the random sub-samples are usually created with replacement.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you can additionally give each tree a random subset of variables from the training set.  This is called a &quot;random forest.&quot;  While your professor will probably be impressed if your write your own random forest algorithm, it's probably best to use an existing implementation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T14:26:49.473" Id="20036" LastActivityDate="2011-12-20T14:26:49.473" OwnerUserId="2817" ParentId="19996" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Sorry, but this is a question every textbook will deliver. I would suggest &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387293175&quot; rel=&quot;nofollow&quot;&gt;Time Series and its Applications&lt;/a&gt;. So I will restrict my answer to the basics and a small example in R:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;AR&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(2,3))&#10;&#10;ar1&amp;lt;-arima.sim(n = 100, list(ar =.95))&#10;plot(ar1)&#10;acf(ar1)&#10;pacf(ar1)&#10;&#10;ar2&amp;lt;-arima.sim(n = 100, list(ar =-.95))&#10;plot(ar2)&#10;acf(ar2)&#10;pacf(ar2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where the parameters of AR are 0.95 in the first and -0.95 in the second case.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dOG9L.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;MA&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot.new()&#10;par(mfrow=c(2,3))&#10;&#10;ma1&amp;lt;-arima.sim(n = 100, list(ma =.95))&#10;plot(ma1)&#10;acf(ma1)&#10;pacf(ma1)&#10;&#10;ma2&amp;lt;-arima.sim(n = 100, list(ma =-.95))&#10;plot(ma2) &#10;acf(ma2)&#10;pacf(ma2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here we vary the MA parameter with .95 in the first and -.95 in the second case.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IPmUX.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-20T14:28:14.460" Id="20037" LastActivityDate="2011-12-20T14:28:14.460" OwnerUserId="8041" ParentId="20039" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Calculating the p-value for an F-Test is a quite straightforward job in every statistical programme. Consider for example &lt;code&gt;r&lt;/code&gt; (&lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;r-project&lt;/a&gt;). Here's an example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pf(F, df1=dfa, df2=dfb)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;F&lt;/code&gt; is the value from the statistic and &lt;code&gt;dfa&lt;/code&gt; and &lt;code&gt;dfb&lt;/code&gt;  are the degrees of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps you!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-20T14:33:00.733" Id="20038" LastActivityDate="2011-12-20T14:33:00.733" OwnerUserId="8041" ParentId="20035" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;ad 1) sorry I don't get what you mean!&lt;/p&gt;&#10;&#10;&lt;p&gt;ad 2) According to &lt;em&gt;Hamilton (1994), page 48&lt;/em&gt; the variance of an MA(1) process is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\left(1+\Theta^2\right)\sigma^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;this explains why the increasing variance, right?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-20T14:58:52.100" Id="20041" LastActivityDate="2011-12-20T14:58:52.100" OwnerUserId="8041" ParentId="20039" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The MIC method is based on &lt;a href=&quot;http://en.wikipedia.org/wiki/Mutual_information&quot;&gt;Mutual information&lt;/a&gt; (MI), which quantifies the dependence between the joint distribution of X and Y and what the joint distribution would be if X and Y were independent (See,e.g., the &lt;a href=&quot;http://en.wikipedia.org/wiki/Mutual_information&quot;&gt;Wikipedia entry&lt;/a&gt;). Mathematically, MI is defined as $$MI=H(X)+H(Y)-H(X,Y)$$ where $$H(X)=-\sum_i p(z_i)\log p(z_i)$$ is the entropy of a single variable and $$H(X,Y)=-\sum_{i,j} p(x_i,y_j)\log p(x_i,y_j)$$ is the joint entropy of two variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;The authors' &lt;a href=&quot;http://obs.rc.fas.harvard.edu/turnbaugh/Papers/Reshef_Science2011.pdf&quot;&gt;main idea&lt;/a&gt; is to discretize the data onto many different two-dimensional grids and calculate normalized scores that represents the mutual information of the two variables on each grid. The scores are normalized to ensure a fair comparison&#10;between different grids and vary between 0 (uncorrelated) and 1 (high correlations).&lt;/p&gt;&#10;&#10;&lt;p&gt;MIC is defined as the highest score obtained and is an indication of how strongly the two variables are correlated. In fact, the authors &lt;a href=&quot;http://obs.rc.fas.harvard.edu/turnbaugh/Papers/Reshef_Science2011.pdf&quot;&gt;claim&lt;/a&gt; that for noiseless functional relationships MIC values are comparable to the coefficient of determination ($R^2$).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T15:57:58.560" Id="20049" LastActivityDate="2011-12-20T15:57:58.560" OwnerUserId="5919" ParentId="20011" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;You should definitely not consider linear regression as you have time series data which has auto-correlative structure.  Your post suggests a simplistic approach (30 day rolling means) which is an attempt to characterize your series. An ARIMA model is an optimization of &quot;how many data points to use&quot; and &quot;how to weight them&quot; . You are assuming an ARIMA model with 30 lags and each of the coefficients in this ARIMA model are identical (1/30). Now on to your problem. What you have is a Parent-To-Child problem ( items within a class ... markets within a business line ). This statistical problem also arises when you need to seamlessly integrate hourly forecasts and daily forecasts , sometimes called a mixed frequency problem . The way we have handled this problem is to create a composite series across markets and form an ARMAX Model ( that is ARIMA plus fixed variables ) incorporating variables like market population and other user-suggested support series. Then develop individual ARMAX models for each market and incorporate the total market sales as a predictor for each of the market models and any individual market predictors. Perform a reconciliation that is parent is boss or children are boss to reconcile your forecasts. Now what is very important is that when you form all of the ARMAX models you should be sensitive to Pulses, Level  Shifts , Seasonal Pulses and/or Local Time Trends and of course any needed ARIMA structure. We have seen many applications around this kind of data. Unfortunately the rudimentary/simplistic tools/solutions that are usually available for free or often worth what you pay, thus you might need to acquire/develop this functionality. I am suggesting that &quot;state-of-the-art solutions&quot; are available in a number of commercially available software offerings. I have suggested elsewhere a reasonable approach to the idea of &quot;picking the minds of software developers&quot; to find out how you can do this yourself (or not ! ) &lt;a href=&quot;http://stats.stackexchange.com/questions/18882/how-do-i-calculate-projected-figures-for-the-next-year-based-on-past-performance/19938#19938&quot;&gt;How do I calculate projected figures for the next year based on past performance?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T18:01:54.880" Id="20058" LastActivityDate="2011-12-20T18:11:32.790" LastEditDate="2011-12-20T18:11:32.790" LastEditorUserId="3382" OwnerUserId="3382" ParentId="20053" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I like to think of the chain rule as:&#10;$$P(A\cap B) = P(B)P(A\mid B) = P(A)P(B \mid A)$$&#10;instead of $P(A\cap B) = P(B)P(A\mid B)$ which is the way G. Jay Kerns expressed&#10;it in his comment on the question.&#10;There is no mathematical difference, of course, but&#10;we can think of $P(A\cap B) = P(A)P(B \mid A)$ as encapsulating the&#10;following heuristic way of thinking.  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;We want to find $P(A \cap B)$, the probability that both $A$ and $B$ &#10;  occurred.  Clearly, $A$ must have occurred (which has probability $P(A)$),&#10;  and if we assume that $A$ has occurred, then in order for $A \cap B$ to&#10;  occur, $B$ must occur too, which has probability $P(B\mid A)$,&#10;  (note: &lt;strong&gt;not&lt;/strong&gt; $P(B)$&#10;  since we have already assumed that $A$ has occurred), and so&#10;  $P(A \cap B) = P(A)P(B \mid A)$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Generalizing this argument (which fortunately can be backed up&#10;by straight mathematical calculations from the definition of&#10;conditional probability),&#10;$$P(A\cap B \cap C) = P(A)P(B\mid A)P(C\mid (A \cap B))$$&#10;or, proceeding in the opposite direction,&#10;$$P(A\cap B \cap C) = P(C)P(B\mid C)P(A\mid (B \cap C)).$$&#10;Dividing this last equation on both sides by $P(C)$ gives&#10;$$\begin{align*}
&#10;\mbox{var}(x^t\hat{\beta}) = x^t \mbox{var}(\hat{\beta}) x
  <row AnswerCount="1" Body="&lt;p&gt;I have seen the terms &quot;frequent pattern mining&quot;, &quot;subspace clustering&quot;, and &quot;biclustering&quot;.  They all pertain to finding clusters using subsets of the data attributes.  What's the difference?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-20T19:49:10.027" Id="20065" LastActivityDate="2011-12-21T08:22:42.870" OwnerUserId="7128" PostTypeId="1" Score="2" Tags="&lt;clustering&gt;&lt;data-mining&gt;" Title="What is the difference between frequent pattern mining, subspace clustering, and biclustering?" ViewCount="321" />
&#10;
&#10;\end{align}
&#10;&amp;amp;\frac{P(\mathbf{A}|\mathbf{B}_1,\mathbf{C})P(\mathbf{B}_1,\mathbf{C}) + P(\mathbf{A}|\mathbf{B}_2,\mathbf{C})P(\mathbf{B}_2,\mathbf{C})}{P(\mathbf{A})} &amp;amp;=&amp;amp; P(\mathbf{C}|\mathbf{A}) &amp;amp;&amp;amp; \text{Algebra}~~~~~~~~~~~~~~\\
  <row Body="&lt;p&gt;To accomplish the goal of sampling from an uneven mixture of distributions, the most straightforward approach is to sample separately, in proportion to the desired ratio:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; p &amp;lt;- 0.70 #P(from N(mu1, sd1)) &#10; n.samps &amp;lt;- 10000&#10; mu1 &amp;lt;- 0&#10; sd1 &amp;lt;- 1&#10; mu2 &amp;lt;- 100&#10; sd2 &amp;lt;- 10&#10;&#10; x &amp;lt;- vector()&#10; for(i in 1:n.samps){&#10;    b &amp;lt;- runif(1, 0, 1)&#10;    if(b &amp;lt; p){&#10;       x[i] &amp;lt;- rnorm(1, mu1, sd1)&#10;     } else { &#10;       x[i] &amp;lt;- rnorm(1, mu2, sd2)&#10;     }&#10;   }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;this can be done ~50 x faster:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; binary &amp;lt;- runif(n.samps, 0, 1) &amp;gt; p&#10; x &amp;lt;- c(rnorm(sum(binary), 1, 2), rnorm(sum(!binary), 5, 4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then to draw a sample:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sample(x, 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or to reshuffle: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- sample(x, n.samp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2011-12-20T20:58:15.683" Id="20069" LastActivityDate="2011-12-21T16:07:01.667" LastEditDate="2011-12-21T16:07:01.667" LastEditorUserId="1381" OwnerUserId="1381" ParentId="20068" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;If you want to sample unequally (with probability 0.7 and 0.3) from two gaussians with parameters $(\mu_1,\sigma_1^2)$ and $(\mu_2,\sigma_2^2)$, then you can probably try something like that:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 100&#10;yn &amp;lt;- rbinom(n, 1, .7)&#10;# draw n units from a mixture of N(0,1) and N(100,3^2)&#10;s &amp;lt;- rnorm(n, 0 + 100*yn, 1 + 2*yn)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In fact, this is one of the illustrations provided in &lt;em&gt;Modern Applied Statistics with S&lt;/em&gt;, by Venables and Ripley (Springer, 2002; §5.2, pp. 110-111).&lt;/p&gt;&#10;&#10;&lt;p&gt;With different parameters, you can use an &lt;code&gt;ifelse&lt;/code&gt; expression to select the mean and SD according to the binomial sequence given in &lt;code&gt;yn&lt;/code&gt;, e.g. &lt;code&gt;rnorm(n, mean=ifelse(yn, 21, 26), sd=ifelse(yn, 3.3, 4))&lt;/code&gt;. (No need to cast &lt;code&gt;yn&lt;/code&gt; to a logical with &lt;code&gt;as.logical&lt;/code&gt;.)&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2011-12-20T21:32:04.443" Id="20074" LastActivityDate="2011-12-20T21:32:04.443" OwnerUserId="930" ParentId="20068" PostTypeId="2" Score="5" />
&#10;d&amp;amp; P(\xi &amp;lt; 1)\\
&#10;2 &amp;amp; 0.39 \\
&#10;8     &amp;amp; 0.14\\
&#10;9     &amp;amp; 0.089\\
  <row Body="&lt;p&gt;I have Machine Learning: An Algorithmic Perspective by Stephen Marsland and find it very useful for self-learning.  Python code is given throughout the book.&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree with what is said in this favourable review:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://blog.rtwilson.com/review-machine-learning-an-algorithmic-perspective-by-stephen-marsland/&quot; rel=&quot;nofollow&quot;&gt;http://blog.rtwilson.com/review-machine-learning-an-algorithmic-perspective-by-stephen-marsland/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-21T01:02:29.200" Id="20085" LastActivityDate="2011-12-21T01:02:29.200" OwnerUserId="2310" ParentId="12386" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I recently received this email from a graduate student, and I get similar questions often enough, that I thought I'd post it here:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I'm using factor analysis, multiple regression, and SEM and currently&#10;  checking statistical assumptions.  I have found numerous univariate&#10;  and multivariate outliers.  If I deleted them all, it would mean a&#10;  large chunk out of my sample size ($N \approx 350$).  I also have&#10;  problems with non-normality, non-linearity, heteroscedasticity&#10;  (Multiple regression), and large standardised residual covariances&#10;  (SEM).&lt;/p&gt;&#10;  &#10;  &lt;p&gt;I have tried reducing the influence of the outliers (allocating them a&#10;  value one unit larger/smaller than the next most extreme non-outlier&#10;  value), and transformations (mostly the variables remained skewed and&#10;  some outliers remain).  When I compare original results with altered&#10;  data, there is little effect.  Given this, I am wondering whether it&#10;  would be acceptable to leave the data as it is? I'm inclined to,&#10;  particularly because this data is from a non-clinical population and I&#10;  have used clinical measures.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2011-12-21T04:59:30.427" FavoriteCount="1" Id="20093" LastActivityDate="2012-04-11T10:00:07.573" OwnerUserId="183" PostTypeId="1" Score="5" Tags="&lt;factor-analysis&gt;&lt;outliers&gt;&lt;normality&gt;&lt;sem&gt;" Title="Whether to leave the data unaltered in the face of outliers and non-normality when performing structural equation modelling?" ViewCount="674" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to assess the correlation between a 7-category ordinal variable&#10;(X) and a number of other variables some of which are ordinal with 3-6&#10;categories, others are continuous and a couple are dichotomous. The&#10;dataset includes only 24 observations and lack of normality and tied&#10;observations are therefore issues to take into account. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Would it be best to use the Kendall coefficient to assess the correlation between X and each of the other variables? If so, which one (i.e. tau-a or tau-b; I know that Roger Newson favours the former)? &lt;/li&gt;&#10;&lt;li&gt;Would it be reasonable to use Spearman as well? &lt;/li&gt;&#10;&lt;li&gt;And what would be the best test to assess the correlation with each of the dichotomous variables?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2011-12-21T18:37:31.110" FavoriteCount="1" Id="20124" LastActivityDate="2012-02-17T14:17:37.740" LastEditDate="2012-01-18T13:24:10.623" LastEditorUserId="88" OwnerUserId="8135" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;ordinal&gt;&lt;spearman&gt;&lt;rank-correlation&gt;" Title="Spearman or Kendall correlation?" ViewCount="940" />
  <row AnswerCount="1" Body="&lt;p&gt;I have box plots of 13 groups that I show in one plot. The groups have unbalanced populations and are not normally distributed. I want to show which pairs are statistically similar (i.e., have kruskal.test p-value &amp;lt; 0.05) by putting a,b,c, etc. on top of boxes that match.&#10;Here is a pseudo code to show what I have:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A = c(1, 5, 8, 17, 16, 3, 24, 19, 6) &#10;B = c(2, 16, 5, 7, 4, 7, 3) &#10;C = c(1, 1, 3, 7, 9, 6, 10, 13) &#10;D = c(2, 15, 2, 9, 7) &#10;junk = list(g1=A, g2=B, g3=C, g4=D) &#10;boxplot(junk) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is an plot I found that does what I want (except I have 13 groups in one row):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;https://www.agronomy.org/images/publications/jeq/32/3/1036f3.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-21T18:44:46.580" FavoriteCount="4" Id="20125" LastActivityDate="2011-12-22T21:51:02.053" LastEditDate="2011-12-21T21:20:44.567" LastEditorUserId="930" OwnerUserId="8136" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;statistical-significance&gt;&lt;boxplot&gt;" Title="Highlighting significant results from non-parametric multiple comparisons on boxplots" ViewCount="3457" />
  <row Body="&lt;p&gt;This sounds a lot like &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrap_aggregating&quot; rel=&quot;nofollow&quot;&gt;bagging&lt;/a&gt; which is a frequently used technique.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-21T19:22:46.560" Id="20127" LastActivityDate="2011-12-21T19:22:46.560" OwnerUserId="2817" ParentId="418" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;Does anybody know of any data sets that are publicly available and huge (i.e., in the petabyte ballpark). Ideally, the data set would be relevant to machine learning analyses. Maybe something from the physics, chemistry, natural-language-processing domains? I guess you could say the Web itself is such a data set, but I'm looking for something a little more structured than that. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-21T22:15:28.813" Id="20136" LastActivityDate="2011-12-22T15:37:36.013" OwnerUserId="7040" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;" Title="Publicly available petabyte-scale data set?" ViewCount="294" />
  <row AcceptedAnswerId="20153" AnswerCount="2" Body="&lt;p&gt;I have a system whose output I am analyzing.  I've used a very simple hypothesis test with great results and now am curious &lt;em&gt;why&lt;/em&gt; did I get such great results -- did I perhaps stumble on some kind of an &quot;optimal&quot; test without much formal statistics training?&lt;/p&gt;&#10;&#10;&lt;p&gt;My system can be in two states.  When it's in state 0 the output can is independently drawn from standard normal distribution $\mathcal{N}(0,1)$ (well, technically, very, &lt;em&gt;very&lt;/em&gt; close approximation to it-- but it's normal enough for my purposes).  When it's in state 1, the output is still independently normally-distributed with variance 1 but the means are perturbed.  Thus, the output $i$ in state 1 has a mean-shifted normal distribution $\mathcal{N}(\mu_i,1)$.  For large number $n$ of observations of the system in state 1, the average $\frac{1}{n}\sum_{i=1}^n\mu_i=0$.  I also know that quantity $\frac{1}{n}\sum_{i=1}^n\mu_i^2=M&amp;lt;\infty$ since this is a real-world system (though I do not know the exact value of $M$, just the fact that it's finite).  Besides those two facts, there is no structure to means in state 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I devised the following hypothesis test to determine whether the system is in state 0 or 1, where the null hypothesis $H_0$ corresponds to the system being in state 0, and alternate $H_1$ to state 1.  First, I collect a sequence of $n$ observations $\{x_i\}_{i=1}^n$.  Then I compute the following test statistic:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$S_n=\left(\frac{1}{n}\sum x_i^2\right)-1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I then pick a threshold $t&amp;gt;0$ and accept null hypothesis if $S_n&amp;lt;t$, rejecting it if $S_n\geq t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This test just made sense to me and is surprisingly easy to analyze (at least surprising to me without much formal background in mathematical statistics). The mean of $S_n$ when the system is in state 0 is 0, and the variance is $2/n$.  In state 1 the mean of $S_n$ is $M$ and the variance is $(4M+2)/n$.  Thus, I can upper-bound both the false-positive error probability $\alpha$ and the probability that I will accept the null hypothesis in error $\beta$ for a given $n$ and $M$ using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Chebyshev%27s_inequality&quot; rel=&quot;nofollow&quot;&gt;Chebyshev's Inequality&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{rcl}\alpha&amp;amp;\leq&amp;amp;\frac{2}{nt^2}\\
  
  
  <row Body="&lt;p&gt;If you want to use K-means clustering, then you need a way to compare the $K=1$ and $K=3$ cases. One approach would be to use the &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;ved=0CCwQFjAB&amp;amp;url=http://www-stat.stanford.edu/~tibs/ftp/gap.ps&amp;amp;ei=_-byTqWjEqWlsQK3vc3VAQ&amp;amp;usg=AFQjCNF9sG6CbcPIxnDLxmLoxaypuaH0uQ&quot; rel=&quot;nofollow&quot;&gt;gap statistic&lt;/a&gt; from Tibshirani et al. and choose the $K$ that provides the better value. There's an R implementation available in &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/SLmisc/html/kmeansGap.html&quot; rel=&quot;nofollow&quot;&gt;SLmisc&lt;/a&gt;, though that particular function will try $K=1,2,3$, so you will need to take care to ensure that only $K=1$ or $K=3$ can be returned as the optimal value.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-22T08:18:11.137" Id="20156" LastActivityDate="2011-12-22T08:18:11.137" OwnerUserId="2111" ParentId="20115" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;May you help me to decide what is the minimal sample size for a uniform distributed sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that I've find the sample average, standard deviation and the $\alpha$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-22T09:10:00.073" FavoriteCount="1" Id="20158" LastActivityDate="2011-12-28T09:24:14.390" LastEditDate="2011-12-28T09:24:14.390" LastEditorUserId="8076" OwnerUserId="8155" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;sample-size&gt;&lt;uniform&gt;" Title="Determining sample size for uniform distribution" ViewCount="1736" />
&#10;\end{array}\right.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To get a confidence interval of the form&#10;$$\left[ {1\over2}(m+M) - {\gamma\over 2}(M-m) ;{1\over2}(m+M) +
&#10;{1\over (1+\gamma)^{n-1}}$,&#10;so to get an CI of level $1-\alpha$ you just put $\gamma =
  <row AcceptedAnswerId="20168" AnswerCount="4" Body="&lt;p&gt;If I have a variable with 4 levels, in theory I need to use 3 dummy variables.  In practice, how is this actually carried out?  Do I use 0-3, do I use 1-3 and leave the 4's blank?  Any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;NOTE: I'm going to be working in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE:  What would happen if I just use one column that uses 1-4 corresponding to A-D?  Will that work or introduce problems?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-22T16:28:47.673" FavoriteCount="1" Id="20166" LastActivityDate="2011-12-24T17:04:20.697" LastEditDate="2011-12-24T09:47:33.990" LastEditorUserId="88" OwnerUserId="7411" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;regression&gt;&lt;categorical-data&gt;" Title="How to implement dummy variable using n-1 variables?" ViewCount="5027" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm using the DLM package to estimate a multivariate time series, I wanna check the out of sample forecasting, by estimating the residuals for 1, 6, 12 months ahead forecast? How can I calculate the 6 and 12 months ahead forcast like the kalman filter does for 1 month ahead forecast?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this question is too general I update it.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question was: I have a times series which go from 1970 to 1990, and I want to check if my model gives a good out of sample fit. In order to do so I divide my dataset in two parts and starting from January 1980 I calculate 1 month ahead forecast errors, by dlm (f). Than I want to calculate 12 months ahead forecast errors, so once my t is january 1980 then february 1980, and so on. I would like to know if there's a way to do so? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe is better to specify my question a little more, because I did a mistake, sorry.&#10;I estimate the model recursevely from 1970:1 to 1980:1 (dlm), , at t=1980:1 I estimate y(t+12) and I compare it with the real y(t+12), then I estimate y(t+12) but t=1980:2, and so on. I would like to know which is the  way to do it automically?&#10;Cause I thought that i can ran a dlm and use the dlmForecast and change every time the dataset through the window command, but I don't think it's the right way. &#10;Maybe &#10;for (i in 1:10){ fit = dlmFilter((window(data, start=1, end=12+i),mod), dlmForcast(FIT, nahed=12)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-23T11:45:55.663" FavoriteCount="1" Id="20187" LastActivityDate="2012-05-01T15:13:24.797" LastEditDate="2012-01-02T16:12:17.800" LastEditorUserId="8173" OwnerUserId="8173" PostTypeId="1" Score="1" Tags="&lt;dlm&gt;&lt;out-of-sample&gt;" Title="DLM out of sample errors" ViewCount="179" />
  <row Body="&lt;p&gt;I think something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mu[i] &amp;lt;- alpha[date[i]]&#10;prec[i]&amp;lt;- SampleSize[i]/(alpha[date[i]]*(1-alpha[date[i]])&#10;e[i] ~ dunif(-0.5,0.5)&#10;mu.round[i] &amp;lt;- mu[i] + e[i]&#10;y[i] ~ dnorm(mu.round[i],prec[i])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;would do the job.  The &lt;code&gt;e[i]&lt;/code&gt; represent the difference between the observed (rounded) and true value of &lt;code&gt;y[i]&lt;/code&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What we'd like to do, for clarity, is adjust the observation by: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y.true[i] &amp;lt;- y[i] - e[i]&#10;y.true[i] ~ dnorm(mu[i], prec[i])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which certainly won't work in JAGS, but may in BUGS (which I don't have).  Instead, I'm adding &lt;code&gt;e[i]&lt;/code&gt; to the right hand side of both lines of the above, which means it gets added to &lt;code&gt;mu[i]&lt;/code&gt;, and this allows us to work with the observed data directly.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-23T16:34:40.180" Id="20191" LastActivityDate="2011-12-25T16:36:22.930" LastEditDate="2011-12-25T16:36:22.930" LastEditorUserId="7555" OwnerUserId="7555" ParentId="20119" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;whuber told you in the comments that coding a 0-3 or 1-4 coding instead of creating dummy variables isn't what you want.  This is try - I am to hopefully explain what you would be doing with that model and why it is wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do code a variable X such that if A then X=1, if B then X=2, if C then X=3, if D then X=4 then when you do the regression you'll only get one parameter.  Let's say it ended up be that the estimated parameter associated with X was 2.  This would tell you that the expected difference between the mean of B and the mean of A is 2.  It also tells you that the expected difference between the mean of C and the mean of B is 2. Some for D and C.  You would be forcing the differences in the means for these groups to follow this very strict pattern.  That one parameter tells you exactly how all of your group means relate to each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you did this kind of coding you would need to assume that not only did you get the ordering correct (because in this case if you expect an increase from A to B then you need to expect an increase from B to C and from C to D) but you also need to assume that that difference is the same!&lt;/p&gt;&#10;&#10;&lt;p&gt;If instead you do the dummy coding that has been suggested you're allowing each group to have its own mean - no restrictions.  This model is much more sensible and answers the questions you want.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-24T17:04:20.697" Id="20207" LastActivityDate="2011-12-24T17:04:20.697" OwnerUserId="4737" ParentId="20166" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Both are popular and useful, but primarily for different uses.  The permutation test is best for testing hypotheses and bootstrapping is best for estimating confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Permutation tests test a specific null hypothesis of exchangability, i.e. that only the random sampling/randomization explains the difference seen.  This is the common case for things like t-tests and anova.  It can also be expanded to things like time series (null hypothesis that there is no serial correlation) or regression (null hypothesis of no relationship).  Permutation tests can be used to create confidence intervals, but it requires many more assumptions, that may or may not be reasonable (so other methods are prefered).  The Mann-Whitney/Wilcoxin test is actually a special case of a permutation test, so they are much more popular than some realize.&lt;/p&gt;&#10;&#10;&lt;p&gt;The bootstrap estimates the variability of the sampling process and works well for estimating confidence intervals.  You can do a test of hypothesis this way but it tends to be less powerful than the permutation test for cases that the permutation test assumptions hold.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-25T01:42:55.193" Id="20218" LastActivityDate="2011-12-25T01:42:55.193" OwnerUserId="4505" ParentId="20217" PostTypeId="2" Score="24" />
  
  
  <row AcceptedAnswerId="20253" AnswerCount="1" Body="&lt;p&gt;I have started my PhD in statistics this year, and I am looking for your best-practices, advice and (meta-advises) regarding how to grow and become a good academic researcher in the fields of statistics/ML.&lt;/p&gt;&#10;&#10;&lt;p&gt;General thoughts and links are welcomed, but in order to start the ball rolling, here are a bunch of questions gathered from Michael Steele's great article &quot;&lt;a href=&quot;http://www-stat.wharton.upenn.edu/~steele/Rants/AdviceGS.html&quot;&gt;Advice For Graduate Students in Statistics&lt;/a&gt;&quot; (if I am missing important questions, or if some of the questions are meaningless - please also comment on it):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Papers vs Thesis - how much should one focus on publishing papers during his PhD work?  How many papers should one realistically aspire to write?&lt;/li&gt;&#10;&lt;li&gt;In what journals should one strive to get published in? (relevant questions &lt;a href=&quot;http://stats.stackexchange.com/questions/1970/what-is-a-statistical-journal-with-quick-turnaround&quot;&gt;link1&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/2008/measures-of-publication-importance-in-statistics&quot;&gt;link2&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;How many hours a day should one spend on research (developing/dealing with your research question), and on learning (reading new papers/ attending courses)&lt;/li&gt;&#10;&lt;li&gt;Where does one go to find &quot;hot topic&quot;, or even better - a &quot;soon to be hot topic&quot;? (&lt;a href=&quot;http://stats.stackexchange.com/questions/2379/what-are-the-big-problems-in-statistics&quot;&gt;link1&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/9693/hot-topics-in-mathematical-statistics&quot;&gt;link2&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;Once a &quot;hot topic is found&quot; how should one balance learning the basics of many aspect of the problem, with focusing on one aspect?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Obviously these questions are VERY general, and there are many angles for thinking/answering them - I hope to read your perspective on how to think about these general issues.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2011-12-26T21:24:29.307" CreationDate="2011-12-26T13:58:33.543" FavoriteCount="7" Id="20243" LastActivityDate="2011-12-27T07:58:35.333" OwnerUserId="253" PostTypeId="1" Score="7" Tags="&lt;careers&gt;&lt;phd&gt;&lt;academia&gt;" Title="Advice For Graduate Students in Statistics" ViewCount="615" />
  
  <row AcceptedAnswerId="20289" AnswerCount="1" Body="&lt;p&gt;I need to classify large numbers of short answer, free response data from a study with a between-group design.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to reduce the amount of manual labor costs, I was thinking of manually coding a small sample set, running the rest of the responses through an SVM classifier, and then coding a random sample of the SVM classifier to obtain a classical statistical measures of the automatically coded data-set.&lt;/p&gt;&#10;&#10;&lt;p&gt;The original, and overly verbose, title to this question was, &quot;Is applying random sampling to output from a machine learning classifier a statistically valid way to calculate confidence intervals?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have already done a conceptual sanity check with a friend of mine who worked with machine-learning algorithms and atmospheric modeling, but I wanted to run it past some real statisticians before I start basing my workflow around this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-26T20:01:34.860" Id="20248" LastActivityDate="2011-12-27T19:08:16.183" OwnerUserId="1689" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;confidence-interval&gt;" Title="Sampling machine learning output to calculate confidence intervals" ViewCount="311" />
  <row Body="Statistical significance refers to the probability that, if, in the population from which this sample were drawn the true effect were 0 (or some hypothesized value) a test statistic as extreme or more extreme than the one gotten in the sample could have occurred. " CommentCount="0" CreationDate="2011-12-27T05:00:38.850" Id="20260" LastActivityDate="2012-09-30T22:59:29.877" LastEditDate="2012-09-30T22:59:29.877" LastEditorDisplayName="user7997" LastEditorUserId="686" OwnerUserId="686" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Statistical classification is the problem of identifying the sub-population to which new observations belong, where the identity of the sub-population is unknown, on the basis of a training set of data containing observations whose sub-population is known. Therefore these classifications will show a variable behavior which can be studied by statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-27T05:02:52.683" Id="20261" LastActivityDate="2011-12-27T08:23:34.827" LastEditDate="2011-12-27T08:23:34.827" LastEditorDisplayName="user7997" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row AcceptedAnswerId="20274" AnswerCount="1" Body="&lt;p&gt;Assuming I have the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y(t) = \alpha {e}^{- \beta t} + \gamma + n(t) $$&#10;Where $ n(t) $ is additive white Gaussian noise (&lt;a href=&quot;http://en.wikipedia.org/wiki/Additive_white_Gaussian_noise&quot; rel=&quot;nofollow&quot;&gt;AWGN&lt;/a&gt;) and $ \alpha, \beta, \gamma $ are the unknown parameters to be estimated.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case linearization using the logarithm function won't help.&lt;/p&gt;&#10;&#10;&lt;p&gt;What could I do?&#10;What would be the ML Estimator / LS Method?&#10;Is there a special treatment to non positive data points?&lt;/p&gt;&#10;&#10;&lt;p&gt;How about the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y(t) = \alpha {e}^{- \beta t} + n(t) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, it would be helpful to use the logarithm function, yet, I could I handle the non positive data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-12-27T10:49:25.207" Id="20271" LastActivityDate="2011-12-29T09:49:26.670" LastEditDate="2011-12-27T15:00:54.253" LastEditorUserId="2970" OwnerUserId="6244" PostTypeId="1" Score="2" Tags="&lt;nonlinear-regression&gt;&lt;curve-fitting&gt;" Title="Exponential curve fitting with a constant" ViewCount="1319" />
  <row Body="&lt;p&gt;Consider an RBF kernel with different scale factors for each feature&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathcal{K}(\vec{x},\vec{x}&amp;#39;) = \exp\left\{\sum_{i=1}^d \eta_i(x_i - x_i&amp;#39;)^2\right\}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then duplicting a feature with the a different sign is equivalent to just doubling the scale factor for that feature.  So if you had perfect model selection (which chooses optimal values for all $\eta_i$ and C), then adding the extra feature would have no effect.  However, model selection for SVMs is more tricky than it looks as while the SVM is based on theory that give protection against over-fitting in fitting the model, it provides not protection against over-fitting in model selection, so in practice it probably will make a difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the dataset is large and you have many features it probably won't make much difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would leave the duplicate out as it provides no additional information.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-27T17:08:51.877" Id="20283" LastActivityDate="2011-12-28T10:25:24.017" LastEditDate="2011-12-28T10:25:24.017" LastEditorUserId="887" OwnerUserId="887" ParentId="20273" PostTypeId="2" Score="4" />
  
  
  
&#10;= \frac{|x_2-x_1|}{\sqrt{2}} \approx 0.707|x_2-x_1|
  
  <row AnswerCount="2" Body="&lt;p&gt;Background: I am developing a Python Statistics Framework, not because the ones out there are bad but because it will help me learn Python and Statistics. I have taken AP Stats, and read scattered books and articles beyond. I am more than willing to read up on whatever shiny technique will solve my problem: The issue is that I don't know the name of said technique yet. &lt;/p&gt;&#10;&#10;&lt;p&gt;Problem: Given two or more things, each of which takes in an X value and returns the probability of that result(Within a certain fixed range, so .00 to .01, .01 to .02 would each be separate blocks catching all the x values within that range and returning .005 and .015 respectively), and a data set, quantitatively figure out which function best matches the data. &#10;Doing the reverse(taking in a probability and returning an X value) would be a bonus.&lt;/p&gt;&#10;&#10;&lt;p&gt;Idea: Be able to compare Logistic Regression, a Data Tree, and &quot;If yes within the past 3 years then .8 else .01&quot; style predictions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a sane way to do this? Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-12-28T17:32:36.143" Id="20324" LastActivityDate="2014-03-26T20:55:37.370" LastEditDate="2011-12-29T17:04:11.167" LastEditorUserId="7711" OwnerUserId="7711" PostTypeId="1" Score="1" Tags="&lt;python&gt;&lt;curve-fitting&gt;&lt;nonparametric-bayes&gt;" Title="How do I compare multiple arbitrary predictions for a given data set?" ViewCount="261" />
  <row Body="&lt;p&gt;While I largely agree with what @DikranMarsupial said, it is typically a bad idea to form $A&amp;#39; A$ since it squares the condition number of $A$. It is more stable to recognize that changing the $i$th row of $A$ from $a&amp;#39;$ to $\hat a&amp;#39;$ is equivalent to perform the rank-one update $\hat A = A + e_i (\hat a-a)&amp;#39;$, where $e_i$ is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_basis&quot; rel=&quot;nofollow&quot;&gt;standard basis vector&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, given a QR decomposition of $A$, the QR decomposition of $\hat A$ can be computed in $\mathcal{O}(n^2)$ work using, for example, &lt;a href=&quot;http://www.mathworks.com/help/techdoc/ref/qrupdate.html&quot; rel=&quot;nofollow&quot;&gt;MATLAB's qrupdate&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Roughly speaking, this approach should yield twice as many digits of accuracy as an approach which explicitly forms $A&amp;#39; A$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-12-28T17:54:12.847" Id="20326" LastActivityDate="2011-12-28T17:54:12.847" OwnerUserId="8233" ParentId="20241" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working on an algorithm that estimates two parameters of its input data. I have a representative set of samples with the true parameters, to act as a ground truth. As this algorithm uses a threshold that has to be adjusted manually by the user, I was wondering if it could be possible to use my &quot;ground truth&quot; samples to find the best threshold for my training set. &#10;How can I express an error metric for two dependant variables, since for each threshold value and sample I'll have two parameters estimated by the algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would really appreciate any hint.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-12-29T13:09:17.480" Id="20357" LastActivityDate="2015-02-21T12:23:14.140" LastEditDate="2011-12-29T14:30:28.003" LastEditorUserId="6142" OwnerUserId="6142" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;modeling&gt;&lt;error&gt;" Title="Error metric for a regression model with two dependent variables" ViewCount="302" />
  
  
  <row Body="&lt;p&gt;Check out &quot;&lt;a href=&quot;http://www.worldscinet.com/aada/02/0202/S1793536910000422.html&quot; rel=&quot;nofollow&quot;&gt;Complimentary Ensemble Empirical Mode Decomposition&lt;/a&gt;&quot; (CEEMD). It's rather compute-intensive, but provides a nicely adaptive approach to characterizing trends across multiple time scales. The original EMD method was published by Huang et al in &lt;em&gt;Proc Royal Soc A&lt;/em&gt; back in 1998, but subsequently it seems Huang has been publishing his work in his own journal &lt;em&gt;Advances in Adaptive Data Analysis&lt;/em&gt;. If you go to his &lt;a href=&quot;http://rcada.ncu.edu.tw/research1.htm&quot; rel=&quot;nofollow&quot;&gt;research website&lt;/a&gt; you can find (at the bottom) links to the full text of every article published in AADA.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://gist.github.com/1536557&quot; rel=&quot;nofollow&quot;&gt;Here is some R code&lt;/a&gt; and data demonstrating running CEEMD in parallel in R.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-12-29T22:42:59.417" Id="20371" LastActivityDate="2011-12-29T22:52:59.107" LastEditDate="2011-12-29T22:52:59.107" LastEditorUserId="364" OwnerUserId="364" ParentId="20366" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I've had good luck for this using tools like &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; that let you program around how your charts look and–for websites–the &lt;a href=&quot;http://code.google.com/apis/chart/&quot; rel=&quot;nofollow&quot;&gt;Google Chart Tools&lt;/a&gt; (or other javascript-based chart tools, e.g., &lt;a href=&quot;http://mbostock.github.com/d3/&quot; rel=&quot;nofollow&quot;&gt;d3&lt;/a&gt;, &lt;a href=&quot;http://www.jscharts.com/&quot; rel=&quot;nofollow&quot;&gt;jscharts&lt;/a&gt;, and I think the site you are looking at is using &lt;a href=&quot;http://www.fusioncharts.com/&quot; rel=&quot;nofollow&quot;&gt;Fusion Charts&lt;/a&gt;) for exactly this sort of problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;With R what you do is you set up the basic way that you want the chart to look.  R is highly customizable (with somewhat hideous defaults, though not as bad as, say, MS Excel) and so you write a script that reads the data from a particular location or source and tweaks the values. It can change around where the axis are, the tick marks, where you place the median, and any tags that go with it.  This isn't perfect, but you can use programming logic and with some careful choices get things to look pretty good consistently. &lt;/p&gt;&#10;&#10;&lt;p&gt;With the javascript variations (where you can get charts that look similar to the ones listed) you just set up the code that will display the chart and where it gets its data. So you can create a service (e.g., a webservice of some variety) that retrieves/processes the data and then feed this to the chart API.  The downside is that you get less customization in this and sometimes getting it into the format expected by the API is painful, but the defaults tend to look pretty good, you can easily put the result on a webpage, and some of the APIs give you some additional visual candy (e.g., like the ones on the site you linked to). &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-12-30T04:07:20.517" CreationDate="2011-12-30T04:07:20.517" Id="20374" LastActivityDate="2011-12-30T04:07:20.517" OwnerUserId="7818" ParentId="18771" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;What you can do, to fit it into the classical HMM framework, is define a new hidden state space equal to the product of your current two state space and the output space.  Given five possible outputs, this will result in a state space with 10 states. Let us define $S_t$ as the state at time $t$ and $O_t$ as the output at time $t$, which I'll assume can take on values $1, 2, \dots, 5$ for simplicity.  $S_t = 1$ might correspond to your original state 1 and $O_{t-1} = 1$,  $S_t=2$ to original state 1 and $O_{t-1}=2$, and so forth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Transition probabilities - The transitions between states $S_t$ and $S_{t+1}$ are very limited once you've seen the output at time $t$, as there are only two states in the state space which it is possible to transition into.   You can retain the simplicity of your current transition matrix by making the assumption that the transition probability from $S_t$ to $S_{t+1}$ depends solely on the &quot;part&quot; of $S_t$ that is due to your original definition of state.  If you make this assumption, which seems to be what you want to do, you will only have two transition probabilities to estimate, and consequently will not suffer from an explosion of the number of transition probabilities in your model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-30T16:53:07.170" Id="20391" LastActivityDate="2011-12-30T18:10:52.913" LastEditDate="2011-12-30T18:10:52.913" LastEditorUserId="7555" OwnerUserId="7555" ParentId="20388" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a number of predictors to use for a binary (Classes 0 and 1) classification task. Let us call them $x_1, x_2, x_3, ... x_n$. The way these are calculated, my naive heuristic assumption is that $S = \sum_{k=1}^n x_k$ should predict Class 1. That is higher $S$ means it is more likely that it is Class 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this did not perform as well as I had hoped, I want to show that my scores $x_k$ can collectively achieve a better performance, than, say some other baseline methods and even my own single scores such as $x_1$. Due to the way I justify my calculations of $x_k$, I do not want to use a black-box technicque such as SVM or Random Forest. More concretely, if I use a very complex model, there is a greater burden on me to prove that I am not over-fitting or just getting as lucky as any other combination of various predictors used in the field.&lt;/p&gt;&#10;&#10;&lt;p&gt;I first tried to use logistic regression and its performance was not very spectacular. I then proceeded to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines&quot; rel=&quot;nofollow&quot;&gt;MARS&lt;/a&gt; models, and I got much better classification performance (using Precision-Recall and ROC curves). But scientifically, I am not sure I can simply justify using MARS models and comparing it to logistic regression. So I want to know which models are the logical extensions to use after logistic regression to add a little more complexity. I have thought of splines, MARS, additive models, adding binary interactions $x_i x_j$ as inputs to the logistic regression. I have planned to use PR and ROC curves on cross-validation data sets to do the comparisons. For logistic regression, one can use model likelihood as a measure but are there any other measures to use across models such as these?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-30T22:48:56.187" Id="20402" LastActivityDate="2012-01-03T03:50:05.507" OwnerUserId="2728" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;classification&gt;&lt;model-selection&gt;&lt;model-comparison&gt;" Title="Logical extensions after logistic regression" ViewCount="186" />
  <row AcceptedAnswerId="20467" AnswerCount="1" Body="&lt;p&gt;We are setting up a manipulative experiment in forest where we will test the effect of management on a number of health indicators.&lt;br&gt;&#10;We have 1 factor, 3 levels, 3 plots each replicated 3 times; experimental design is completely randomized. &#10;Our concerns and our two questions are about the best way to assign the factor levels to the plots. &lt;/p&gt;&#10;&#10;&lt;p&gt;Plots are scattered in the forest but may be adjacent with one or more side shared between each one. Reducing plot size is not an option nor it is increasing replicates number.&#10;To assure independency of plots we are going to put a few constraints to the random assignment of levels to plots in order to assure that different factor levels be assigned to adjacent plots. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First question&lt;/strong&gt;&lt;br&gt;&#10;Do you think this compromise between randomization and  interspersion would bias further analyses?&lt;/p&gt;&#10;&#10;&lt;p&gt;We would like to guarantee that initial conditions of plots, as far as our response variables are concerned, are as much similar as possible. We have enough data on initial conditions and may test for equality of a response variable, for each combination of level assignment configuration to plots, before starting the experiment. Then we could pick up the levels configuration on plots that exceeds a significancy threshold (or simply maximizes the &lt;em&gt;p&lt;/em&gt; value), or minimizes the variance of the response variable explained by the factor level.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second question&lt;/strong&gt;&lt;br&gt;&#10;Where is the correct balance between randomization and assuring even initial condition between the experimental units?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-31T06:55:21.440" Id="20414" LastActivityDate="2012-01-02T09:48:00.267" LastEditDate="2011-12-31T10:24:30.273" LastEditorUserId="930" OwnerUserId="8251" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;experiment-design&gt;&lt;interpretation&gt;&lt;independence&gt;&lt;randomization&gt;" Title="Balance between randomization, interspersion and even initial conditions of experimental units" ViewCount="173" />
  <row Body="&lt;p&gt;The following proposition is surely perfectible:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;zucchini &amp;lt;- function(st, en, mingap=1)&#10;{&#10;  i &amp;lt;- order(st, en-st);&#10;  st &amp;lt;- st[i];&#10;  en &amp;lt;- en[i];&#10;  last &amp;lt;- r &amp;lt;- 1&#10;  while( sum( ok &amp;lt;- (st &amp;gt; (en[last] + mingap)) ) &amp;gt; 0 )&#10;  {&#10;    last &amp;lt;- which(ok)[1];&#10;    r &amp;lt;- append(r, last);&#10;  }&#10;  if( length(r) == length(st) )&#10;    return( list(c = list(st[r], en[r]), n = 1 ));&#10;&#10;  ne &amp;lt;- zucchini( st[-r], en[-r]);&#10;  return(list( c = c(list(st[r], en[r]), ne$c), n = ne$n+1));&#10;}&#10;&#10;coliflore &amp;lt;- function(st, en, mingap = 1)&#10;{&#10;  zu &amp;lt;- zucchini(st, en, mingap);&#10;  plot.new();&#10;  plot.window( xlim=c(min(st), max(en)), ylim = c(0, zu$n+1));&#10;  box(); axis(1);&#10;  for(i in seq(1, 2*zu$n, 2))&#10;  {&#10;    x1 &amp;lt;- zu$c[[i]];&#10;    x2 &amp;lt;- zu$c[[i+1]];&#10;    for(j in 1:length(x1))&#10;      rect( x1[j], (i+1)/2,  x2[j], (i+1)/2+0.5, col=&quot;gray&quot;, border=NA );&#10;  }&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Application:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; st &amp;lt;- runif(20,0,50)&#10;&amp;gt; en &amp;lt;- st + runif(20, 5,20)&#10;&amp;gt; st&#10; [1] 25.571385 17.074676  4.564936 27.247745 23.832638 11.045469  2.845222&#10; [8]  2.824046 23.319625 19.684993 42.610242 48.185618 47.748637 39.813871&#10;[15]  9.235512 40.299425 13.797027 21.079956 31.638772 24.152991&#10;&amp;gt; en&#10; [1] 35.43667 32.20029 19.37133 44.30378 35.73845 16.63794 11.52551 16.06469&#10; [9] 32.22477 26.05563 49.51284 67.77664 67.27914 49.35472 28.27657 50.49421&#10;[17] 27.29273 37.87611 48.76251 39.89335&#10;&#10;&amp;gt; coliflore(st, en)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5C39M.png&quot; alt=&quot;example&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Happy new year!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-12-31T23:08:56.090" Id="20433" LastActivityDate="2011-12-31T23:08:56.090" OwnerUserId="8076" ParentId="20421" PostTypeId="2" Score="3" />
&#10;&amp;amp;= 1 - \sum_x \max\{p(X=x, A), p(X=x, B)\}
&#10;P(E) &amp;amp;= 1 - \sum_x \max\{p(x|A)P(A), p(x|B)P(B)\}\\
&#10;&amp;amp;= 1 - \sum_x \frac{p(x,A) + p(x,B)+ |p(x,A) - p(x,B)|}{2}\\
  <row Body="&lt;p&gt;Great question. You know, I've struggled myself with wrapping my head around ANOVA for a very long time.  I always find myself going back to the &quot;between versus within&quot; intuition, and I've always tried to imagine what this would look like in my head.  I'm glad this question came up, and I've been amazed by the varied approaches to this in the answers above.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, for a long time (years, even) I've been wanting to collect several plots in one place where I could see what was happening simultaneously from a lot of different directions:  1) how far apart the &lt;em&gt;populations&lt;/em&gt; are, 2) how far apart &lt;em&gt;the data&lt;/em&gt; are, 3) how big's the &lt;em&gt;between&lt;/em&gt; compared to the &lt;em&gt;within&lt;/em&gt;, and 4) how do the &lt;em&gt;central&lt;/em&gt; versus &lt;em&gt;noncentral&lt;/em&gt; F distributions compare?&lt;/p&gt;&#10;&#10;&lt;p&gt;In a truly &lt;em&gt;great&lt;/em&gt; world, I could even play with sliders to see how &lt;em&gt;sample size&lt;/em&gt; changes things.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I've been playing with the &lt;code&gt;manipulate&lt;/code&gt; command in &lt;a href=&quot;http://rstudio.org/&quot;&gt;RStudio&lt;/a&gt;, and holy cow, it works!  Here is one of the plots, a snapshot, really:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9UnBA.png&quot; alt=&quot;visualizeANOVA&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have RStudio you can get the code for making the above plot (sliders and all)! &lt;a href=&quot;http://gist.github.com/bba174e7fdd688d148b7&quot;&gt;on Github here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;After playing with this for awhile, I am surprised at how well the F statistic distinguishes the groups, even for moderately small sample sizes.  When I look at the populations, they really aren't that far apart (to my eye), yet, the &quot;within&quot; bar is consistently dwarfed by the &quot;between&quot; bar.  Learn something every day, I guess.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-01T01:03:06.000" Id="20436" LastActivityDate="2012-01-01T01:03:06.000" OwnerUserId="1108" ParentId="5278" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The naive VC matrix of the parameter estimates is stored in the &lt;code&gt;vbeta.naiv&lt;/code&gt; component of the GEE fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an illustration with the Ohio data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(geepack)&#10;data(ohio)&#10;fm &amp;lt;- resp ~ age*smoke&#10;gee.fit &amp;lt;- geese(fm, id=id, data=ohio, family=binomial, &#10;                 corstr=&quot;exch&quot;, scale.fix=TRUE)&#10;summary(gee.fit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is a basic model where the working correlation is considered symmetric with correlation $\rho$. Here, the within-cluster correlation is estimated at $\hat\rho=0.355$. Estimates with robust standard error (computed from a sandwich estimator, as detailed in the &lt;a href=&quot;http://www.jstatsoft.org/v15/i02/paper&quot;&gt;JSS paper&lt;/a&gt;, pp. 4-5) are shown below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            estimate  san.se     wald       p&#10;(Intercept) -1.90050 0.11909 254.6860 0.00000&#10;age         -0.14124 0.05820   5.8889 0.01524&#10;smoke        0.31383 0.18784   2.7912 0.09478&#10;age:smoke    0.07083 0.08828   0.6438 0.42234&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The robust and naive VC matrices are obtained as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; gee.fit$vbeta&#10;          [,1]      [,2]      [,3]      [,4]&#10;[1,]  0.014182  0.002677 -0.014182 -0.002677&#10;[2,]  0.002677  0.003387 -0.002677 -0.003387&#10;[3,] -0.014182 -0.002677  0.035285  0.005348&#10;[4,] -0.002677 -0.003387  0.005348  0.007793&#10;&amp;gt; gee.fit$vbeta.naiv&#10;         [,1]      [,2]      [,3]      [,4]&#10;[1,]  0.01407  0.002400 -0.014072 -0.002400&#10;[2,]  0.00240  0.003139 -0.002400 -0.003139&#10;[3,] -0.01407 -0.002400  0.034991  0.005373&#10;[4,] -0.00240 -0.003139  0.005373  0.007938&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can check that the Wald statistics computed using those values (as the ratio of the estimates to their standard errors, which are the diagonal entries of the VC matrix) match the ones displayed in the summary table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; (gee.fit$beta/sqrt(diag(gee.fit$vbeta)))^2&#10;(Intercept)         age       smoke   age:smoke &#10;   254.6860      5.8889      2.7912      0.6438 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(If you use &lt;code&gt;geeglm&lt;/code&gt; instead, coefficients are available through the accessor &lt;code&gt;coef()&lt;/code&gt;, and the robust VC matrix is stored in &lt;code&gt;gee.fit$geese$vbeta&lt;/code&gt; where &lt;code&gt;gee.fit&lt;/code&gt; now holds the results of the call to &lt;code&gt;geeglm&lt;/code&gt;.)&lt;/p&gt;&#10;&#10;&lt;p&gt;A more detailed account of GEE computing is available in this excellent tutorial, &lt;a href=&quot;http://gbi.agrsci.dk/statistics/courses/phd07/material/Day10/gee-lecture.pdf&quot;&gt;Generalized Estimating Equations (GEE)&lt;/a&gt;, by Søren Højsgaard and Ulrich Halekoh.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-01T11:22:33.997" Id="20440" LastActivityDate="2012-01-01T11:22:33.997" OwnerUserId="930" ParentId="20438" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="20451" AnswerCount="2" Body="&lt;p&gt;Both the likelihood ratio test and the AIC are tools for choosing between two models and both are based on the log-likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, why the likelihood ratio test can't be used to choose between two non-nested models while AIC can?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-01T12:15:03.760" FavoriteCount="2" Id="20441" LastActivityDate="2013-10-16T20:18:28.967" OwnerUserId="7064" PostTypeId="1" Score="5" Tags="&lt;aic&gt;&lt;likelihood-ratio&gt;" Title="Non-nested model selection" ViewCount="1697" />
  
  <row Body="&lt;p&gt;@John has a nice answer.  I particularly like the discussion about fishing expeditions and how alpha-adjustment may not be necessary.  I want to add one additional aspect to this discussion.  With hypothesis testing, there are two different kinds of errors to worry about: type I and type II (also called alpha error and beta error).  Both kinds are bad, and we want to avoid both of them.  When people talk about alpha-adjustment, they are focusing only on the possibility of type I errors (that is, saying there is a difference when there isn't one).  However, adjusting alpha to minimize type I errors necessarily decreases power.  Thus, it necessarily &lt;em&gt;increases&lt;/em&gt; the probability of type II errors (that is, saying there isn't a difference when in fact there is).  In addition, it's worth noting that a-priori there is no reason to believe that type I errors are worse than type II errors (despite the fact that everyone seems to assume that this must be so).  Rather, which is worse will vary from situation to situation and is a judgment that must be made by the researcher.  In other words, deciding on a strategy for testing multiple comparisons (e.g., an alpha-adjustment strategy) one must consider the effect of the strategy on &lt;em&gt;both&lt;/em&gt; type I and type II errors and balance these effects relative to: the severity of these errors, how much data you have, and the cost of gathering more.  &lt;/p&gt;&#10;&#10;&lt;p&gt;On a different note, from your description it seems to me that your situation would best be analyzed by using a factorial ANOVA, with sex as factor 1, marital status as factor 2, language as factor 3, and age as factor 4.  From the description (and I recognize that it is sparse) I don't see why a cell means approach (i.e., one-way ANOVA) is preferable.  If you have no interest in interactions, the main effects from the factorial ANOVA are already orthogonal (at least if the $n$s are the same), and Bonferroni corrections are not relevant.  Certainly it would still be possible to have more than 5% type I errors, but I'm a big believer in @John's fourth paragraph; when I'm testing theoretically suggested, a-priori, orthogonal contrasts, I don't use alpha-adjustments.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-01T19:22:16.813" Id="20453" LastActivityDate="2012-01-01T21:59:21.940" LastEditDate="2012-01-01T21:59:21.940" LastEditorUserId="7290" OwnerUserId="7290" ParentId="20447" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;Here is an approach to calculate how many players have an actual win rate of over 0.3 dollar per hand.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0: Check whether individual player results are normally distributed.&#10;1: Calculate the standard deviation for each player.&#10;2: Given his mean and standard deviation, calculate the probability that his win rate is over 0.3&#10;3: Add all these probabilities of all players and this is a good estimate of how many players actually have a win rate of over 0.3.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Perhaps you may have to stop at step 0 in real life, but you can try to find a proper destribution or just use this as a guideline.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-02T14:25:17.040" Id="20475" LastActivityDate="2012-01-02T14:25:17.040" OwnerUserId="8205" ParentId="11848" PostTypeId="2" Score="1" />
  
  <row Body="" CommentCount="0" CreationDate="2012-01-02T18:07:38.740" Id="20485" LastActivityDate="2012-01-02T18:07:38.740" LastEditDate="2012-01-02T18:07:38.740" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="Any statistical process which seeks to approximate an unknown value, such as a distribution, a point estimate (e.g. mean), or confidence interval." CommentCount="0" CreationDate="2012-01-02T18:08:36.180" Id="20488" LastActivityDate="2012-01-28T20:22:22.133" LastEditDate="2012-01-28T20:22:22.133" LastEditorUserId="8605" OwnerUserId="8605" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Structured data files in any format, collected together with the documentation that explains their production or use.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-02T18:09:27.410" Id="20489" LastActivityDate="2012-01-02T18:17:13.217" LastEditDate="2012-01-02T18:17:13.217" LastEditorDisplayName="user7997" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="Structured data files in any format, collected together with the documentation that explains their production or use." CommentCount="0" CreationDate="2012-01-02T18:09:27.410" Id="20490" LastActivityDate="2012-01-02T18:17:10.377" LastEditDate="2012-01-02T18:17:10.377" LastEditorDisplayName="user7997" OwnerUserId="-1" PostTypeId="4" Score="0" />
  <row Body="A test (typically of distribution, independence, or goodness of fit) or a family of distributions related to such a test." CommentCount="0" CreationDate="2012-01-02T18:11:33.490" Id="20494" LastActivityDate="2012-01-02T18:18:40.187" LastEditDate="2012-01-02T18:18:40.187" LastEditorUserId="919" OwnerUserId="919" PostTypeId="4" Score="0" />
  
  
  <row Body="&lt;p&gt;I wanted to make a clarification regarding the original question. In item response theory, the discrimination (i.e. item slope or factor loading) is not indicative of difficulty. Using a model that allows for varying discrimination for each item is effectively weighting them according to their estimated correlation to the latent variable, not by their difficulty.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, a more difficult item could be weighted down if it estimated to be fairly uncorrelated with the dimension of interest and vice versa, an easier item could be weighted up if is estimated to be highly correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree with previous answers that point to (a) the lack of awareness of item response methods among practitioners, (b) the fact that using these models require some technical expertise even if one is aware of their advantages (specially the ability of evaluating the fit of the measurement model), (c) the student's expectations as pointed out by @rolando2, and last but not least (d) the theoretical considerations that instructors may have for weighting different items differently. However, I did want to mention that:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Not all item response theory models allow for variation of the discrimination parameter, where the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rasch_model&quot; rel=&quot;nofollow&quot;&gt;Rasch model&lt;/a&gt; is probably the best known example of a model where the discriminations across items are held constant. Under the Rasch family of models, the sum score is a sufficient statistic for the item response score, therefore, there will be no difference in the order of the respondents, and the only practical differences will be appreciated if the 'distances' between the score groups are considered.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are researchers that defend the use of classical test theory (which relies on the traditional use of sum scores or average correct) for both theoretical and empirical reasons. Perhaps the most used argument is the fact that scores generated under item response theory are effectively very similar to the ones produced under classical test theory. See for example the work by Xu &amp;amp; Stone (2011), &lt;a href=&quot;http://epm.sagepub.com/content/early/2011/10/08/0013164411419846.abstract&quot; rel=&quot;nofollow&quot;&gt;Using IRT Trait Estimates Versus Summated Scores in Predicting Outcomes&lt;/a&gt;, &lt;em&gt;Educational and Psychological Measurement&lt;/em&gt;,  where they report correlations over .97 under a wide array of conditions.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2012-01-02T21:52:47.593" Id="20502" LastActivityDate="2012-01-02T22:35:24.127" LastEditDate="2012-01-02T22:35:24.127" LastEditorUserId="930" OwnerUserId="8293" ParentId="18862" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Shouldn't a student's score be based on what they know and answer on the test rather than what everyone else in the class does?  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you gave the same test 2 different years and you had 2 students (1 in each) who answered the exact same questions correctly (without cheating), does it really make sense that they would received different grades based on how much the other students in their class studied?&lt;/p&gt;&#10;&#10;&lt;p&gt;And personally, I don't want to give any students motivation to sabatoge their class mates in place of learning the material themselves.&lt;/p&gt;&#10;&#10;&lt;p&gt;IRT can give some insight into the test, but I would not use it to actively weight the scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I think of weights, I think that someone should get more points for getting a hard question correct, but they should lose more points for getting an easy question wrong.  Combine those and you still end up with equal weighting.  Or I actually try to weight based on time or effort needed to answer the question, so that someone who answers the questions in a different order does not have an advantage on a timed test.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-03T01:05:47.433" Id="20513" LastActivityDate="2012-01-03T01:05:47.433" OwnerUserId="4505" ParentId="18862" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It depends on how much math you want.  For a less mathematically-intense treatment, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470505397&quot;&gt;&lt;em&gt;Applied Econometric Time Series&lt;/em&gt; by Enders&lt;/a&gt; is well-regarded.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-01-04T08:04:10.023" CreationDate="2012-01-03T01:45:42.970" Id="20516" LastActivityDate="2012-01-03T01:45:42.970" OwnerUserId="7290" ParentId="20514" PostTypeId="2" Score="10" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have read an &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3173385/table/T2/&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; and I wonder if that is a proper way to present a McNemar's Test statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;The study try to measure the change in knowledge before and after a lesson. They performed McNemar's test to calculate the p-value, and tabulate it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't feel comfortable when reading the table because:&lt;/p&gt;&#10;&#10;&lt;p&gt;McNemar's test just shows if the difference in the count of the discordant pairs (Pre-T + Post-F) OR (Pre-F + Post-T) are statistically significant. It does not give any clue on whether the question improve or worsen attendees' knowledge. But the table seems to claim that the education program caused an increased number of attendees improving their knowledge, support by McNemar's test, which I don't feel comfortable about that statement.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if the count of the discordant pairs are included in the table (number of Pre-T + Post-F and number of Pre-F + Post-T), will give a different picture.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is:&lt;/p&gt;&#10;&#10;&lt;p&gt;By aggregating the counts in the 2x2 table into Pre-correct count and Post-correct and then present it with the McNemar's test statistics, is it appropriate? I tried to find one similar article in Lancet or NEMJ but I can't.&lt;/p&gt;&#10;&#10;&lt;p&gt;If it is not appropriate, how can it be presented in a more proper way? Thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-03T11:23:49.557" Id="20533" LastActivityDate="2012-01-03T11:23:49.557" OwnerUserId="588" PostTypeId="1" Score="1" Tags="&lt;p-value&gt;&lt;interpretation&gt;&lt;presentation&gt;" Title="How to properly present McNemar's Test Statistics" ViewCount="640" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a planner that can evaluate N arbitrary states, and calculate their fitness. The domains it evaluates have no explicit &quot;end state&quot;, so it has an infinite horizon.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are good methods for calculating when it should stop evaluating states?&lt;/p&gt;&#10;&#10;&lt;p&gt;My current approach is to estimate the probability of the next evaluation being the next &quot;best&quot; state, and to stop if this probability falls below a certain threshold.&lt;/p&gt;&#10;&#10;&lt;p&gt;e.g. After evaluating each state, I increment a counter, and after a new best fitness if found, I reset that counter, resulting in a sequence like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;steps = [0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can then iterate over this and form a simple discrete probability of each step preceding the next best fitness:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from collections import defaultdict&#10;counts = defaultdict(int)&#10;totals = defaultdict(int)&#10;for step,next in zip(steps, steps[1:]):&#10;    counts[step] += next == 0&#10;    totals[step] += 1&#10;for k in sorted(counts.keys()):&#10;    print k,counts[k],totals[k],'%.2f' % (counts[k]/float(totals[k]),)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;resulting in:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0 0.43&#10;1 0.00&#10;2 0.50&#10;3 0.00&#10;4 0.00&#10;5 0.00&#10;6 0.00&#10;7 0.00&#10;8 0.00&#10;9 0.00&#10;10 1.00&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do I take these discrete probabilities and form a general approximation for the probability of &lt;code&gt;P(next step is the best | current step since last best)&lt;/code&gt; given an arbitrary step integer (especially for high step counts I may not have reached)?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-03T15:44:43.630" Id="20545" LastActivityDate="2012-01-03T16:45:26.083" LastEditDate="2012-01-03T16:45:26.083" LastEditorUserId="88" OwnerUserId="741" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;estimation&gt;&lt;conditional-probability&gt;&lt;continuous-data&gt;" Title="Calculating probability of discovery" ViewCount="52" />
  
  <row Body="&lt;p&gt;Given $n$ data points $(x_i,y_i), i = 1,2,\ldots n$, in the plane, &#10;let us draw a straight line&#10;$y = ax+b$.  If we predict $ax_i+b$ as the value $\hat{y}_i$ of $y_i$, then&#10;the &lt;em&gt;error&lt;/em&gt; is $(y_i-\hat{y}_i) = (y_i-ax_i-b)$, the &lt;em&gt;squared error&lt;/em&gt; is&#10;$(y_i-ax_i-b)^2$, and the &lt;em&gt;total squared error&lt;/em&gt; $\sum_{i=1}^n (y_i-ax_i-b)^2$.&#10;We ask&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What choice of $a$ and  $b$ minimizes &#10;  $S =\displaystyle\sum_{i=1}^n (y_i-ax_i-b)^2$?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Since $(y_i-ax_i-b)$ is the vertical distance of $(x_i,y_i)$ from&#10;the straight line, we are asking for the line such that the&#10;sum of the squares of the vertical distances of the points from&#10;the line is as small as possible.  Now $S$  is a&#10;quadratic function of both $a$ and $b$ and attains its minimum&#10;value when $a$ and $b$ are such that&#10;$$\begin{align*}
  <row AcceptedAnswerId="20569" AnswerCount="2" Body="&lt;p&gt;I have a process with binary output. Is there a standard way to test if it is a Bernoulli process? The problem translates to checking if every trial is independent of the previous trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have observed some processes where a result &quot;sticks&quot; for a number of trials.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-03T23:00:40.627" FavoriteCount="1" Id="20563" LastActivityDate="2012-01-04T15:30:44.907" LastEditDate="2012-01-04T15:30:44.907" LastEditorUserId="919" OwnerUserId="3678" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;" Title="Is there a test for independence in a Bernoulli process?" ViewCount="759" />
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/346/what-is-a-good-algorithm-for-estimating-the-median-of-a-huge-read-once-data-set&quot;&gt;What is a good algorithm for estimating the median of a huge read-once data set?&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Imagine you have a large, multivariate dataset that resides on disk.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any known methods to efficiently compute median with a minimum number of passes through the data ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've found a candidate for variance/stddev in the name of &lt;a href=&quot;http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#On-line_algorithm&quot; rel=&quot;nofollow&quot;&gt;Welfod/Knutt algorithm&lt;/a&gt;, but what about median ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks  &lt;/p&gt;&#10;" ClosedDate="2012-01-04T15:40:50.367" CommentCount="4" CreationDate="2012-01-04T08:14:46.963" Id="20568" LastActivityDate="2012-01-04T08:14:46.963" OwnerUserId="6664" PostTypeId="1" Score="1" Tags="&lt;median&gt;&lt;online&gt;" Title="How to compute median in an online fashion?" ViewCount="91" />
  
  <row Body="&lt;p&gt;You describe a sequence of rejection samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;By definition, after selecting $k$ objects in the process of obtaining a weighted sample of $m$ objects without replacement from a list of $n$ objects, you draw one of the remaining $n-k$ objects according to their relative weights.  In your description of the alternative sampling scheme, at the same stage you will be drawing from all $n$ objects according to their relative weights but you will throw back any object equal to one of the first $k$ and try again: that's the rejection.  So all you have to convince yourself of is that the chance of drawing one of the remaining $n-k$ objects is the same in either case.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this equivalence isn't perfectly clear, there's a straightforward mathematical demonstration.  Let the weights of the first $k$ objects be $w_1, \ldots, w_k$ and let the weights of the remaining $n-k$ objects be $w_{k+1}, \ldots, w_n$.  Write $w = w_1 + w_2 + \cdots + w_n$ and let $w_{(k)} = w_1 + w_2 + \cdots + w_k$.  The chance of drawing object $i$ ($k \lt i \le n$) without replacement from the $n-k$ remaining objects is of course &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{w_i}{w_{k+1} + w_{k+2} + \cdots + w_n}  =\frac{w_i}{w-w_{(k)}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the alternative (rejection) scheme, the chance of drawing object $i$ equals &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{a=0}^\infty \left[\left(\frac{w_{(k)}}{w}\right)^a \frac{w_i}{w}\right] =\frac{1}{1 - w_{(k)}/w}\frac{w_i}{w} = \frac{w_i}{w-w_{(k)}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;exactly as before.  The sum arises by partitioning the event by the number of rejections ($a$) that precede drawing element $i$; its terms give the chance of a sequence of $a$ draws from the first $k$ elements followed by drawing the $i^\text{th}$ element: because these draws are independent, their chances multiply.  It forms a geometric series which is elementary to put into closed form (the first equality).  The second equality is a trivial algebraic reduction.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-04T22:14:21.957" Id="20592" LastActivityDate="2012-01-04T22:41:08.683" LastEditDate="2012-01-04T22:41:08.683" LastEditorUserId="919" OwnerUserId="919" ParentId="20590" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="20609" AnswerCount="2" Body="&lt;p&gt;I know that with a model that is not identifiable the data can be said to be generated by multiple different assignments to the model parameters. I know that sometimes it's possible to constrain parameters so that all are identifiable, as in the example in Cassella &amp;amp; Berger 2nd ed, section 11.2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a particular model, how can I evaluate whether or not it's identifiable?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-05T02:59:55.333" FavoriteCount="7" Id="20608" LastActivityDate="2012-01-05T04:46:25.957" OwnerUserId="8207" PostTypeId="1" Score="22" Tags="&lt;identifiability&gt;" Title="What is model identifiability?" ViewCount="2731" />
&#10;$$&#10;is (almost) identically zero.  The only line which does such a thing is the one which has slope 0 and y-intercept zero.  Hopefully you can see the rest.&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, if you can tell by looking at your model that it isn't identifiable (sometimes you can), then it is common to introduce additional constraints on it to make it identifiable (as you mentioned).  This is akin to recognizing that the function $f(y) = y^{2}$ isn't one-to-one for $y$ in $[-1,1]$, but it &lt;em&gt;is&lt;/em&gt; one-to-one if we restrict $y$ to lie inside $[0,1]$.  In more complicated models the equations are tougher but the idea is the same.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-05T04:28:45.660" Id="20609" LastActivityDate="2012-01-05T04:28:45.660" OwnerUserId="1108" ParentId="20608" PostTypeId="2" Score="20" />
  <row Body="&lt;p&gt;Here is a two cents suggestion.&lt;/p&gt;&#10;&#10;&lt;p&gt;Denote $X_t$ the differenced series. Given $\Delta &amp;gt; 0$ and a point $t$, define&#10;$$a(\Delta,t) = {1\over 2\Delta + 1} |X_t|.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For let’s says $\Delta = 50$, the value of $a(\Delta,t)$ characterizes the off/on zones by low/high values.&lt;/p&gt;&#10;&#10;&lt;p&gt;An anomalous step is a point $t$ where $|X_t| &amp;gt; \alpha a(\Delta,t)$ – you’ll need to do some tuning on $\alpha, \Delta$ to detect what you want, and avoid false positive when the machine turns on. I’d try first with $\Delta = 50$ and $\alpha = 4$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you can look at points $t$ where $a(\delta,t) &amp;gt; \alpha a(\Delta,t)$ for a $\delta\ll\Delta$ (eg $\delta = 10$, $\Delta = 100$), that may help the fine tuning (in that case, you would take a smaller value for $\alpha$).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-05T09:42:21.320" Id="20615" LastActivityDate="2012-01-05T09:42:21.320" OwnerUserId="8076" ParentId="20612" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;What are the meanings of &quot;stochastically dependent&quot; and &quot;functionally dependent&quot;? What is the difference? &#10;(I saw the usage of the above terms in the paper &quot;&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00225/abstract&quot; rel=&quot;nofollow&quot;&gt;Maximum entropy sampling and optimal Bayesian experimental design&lt;/a&gt;&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-05T13:16:53.437" Id="20623" LastActivityDate="2012-01-05T13:16:53.437" OwnerUserId="8348" PostTypeId="1" Score="2" Tags="&lt;experiment-design&gt;&lt;mathematical-statistics&gt;" Title="What are the meanings of &quot;stochastically dependent&quot; and &quot;functionally dependent&quot;?" ViewCount="264" />
  <row AnswerCount="0" Body="&lt;p&gt;I have some data I'm studying where functional data analysis seems&#10;like a promising approach.  But having never tackled FDA&#10;before, I'm having trouble wrapping my head around it.&lt;/p&gt;&#10;&#10;&lt;p&gt;For background, I have Ramsay and Silverman's &quot;Functional Data&#10;Analysis&quot; and Ramsay, Hooker, and Graves &quot;Functional Data Analysis&#10;with R and Matlab&quot; but only got them recently, and am still very much&#10;the newbie with FDA. I am using R with package fda as the analysis software.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data are a sample of hundreds of young adult subjects, each of whom&#10;were measured annually by performing a task with a binary outcome&#10;over some number of trials. The outcome of interest is the success&#10;rate on the task.  I'll include a short sample of data at the end of&#10;my question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Several parameters were collected on each subject, including height,&#10;weight, education level, amount of prior training on the task, and the&#10;results of an aptitude pre-test.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The year-to-year change in success rate over time is the curve I'd&#10;like to model with FDA.  There is significant variation across&#10;subjects in overall performance, but I am mostly interested in how a&#10;subject's success rate evolves over time rather than a subject's&#10;actual performance.   &lt;/p&gt;&#10;&#10;&lt;p&gt;The subjects generally follow a pattern of starting off with a low&#10;success rate, improving yearly until reaching a personal maximum, then&#10;declining with age.  The typical subject enters the study at age 20&#10;with a low success rate, improves until age 25, then declines until&#10;age 30, at which point no further data is collected.  One of the goals&#10;is to characterize the &quot;canonical&quot; development curve for the&#10;population.  FDA seems like a good fit for this aspect of the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some of my challenges are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Subjects may reach their maximum success rate at different ages,&#10;and may improve/decline faster than others, so both phase variation and&#10;amplitude variation are potentially present. So it seems like curve&#10;registration will be an important part of the analysis. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The number of trials in the test is relatively small each year, so the&#10;outcome data is noisy, and registering the data properly seems&#10;daunting since the noise in one subject can obscure the landmarks in the data. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am also interested in whether the other variables predict the&#10;registration characteristics of the subject's curve.  i.e. Do college-educated&#10;subjects reach their maximum performance at an earlier age than others&#10;(phase), or do particularly tall subjects sustain their peak&#10;performance longer than short subjects do (amplitude)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We would like to be able to produce a customized curve for a&#10;subject.  e.g. If we observed the tests through age 23, we would be&#10;able to predict how much improvement to expect, at what age the&#10;maximum performance would be reached, and how sharp the decline phase&#10;would be through age 30. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My questions include:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is FDA an appropriate method for this problem?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Are there techniques in FDA for predicting a subject's phase and amplitude&#10;variation using other variables?  What I've read so far seems to&#10;treat registration as a correction for noise rather than as containing&#10;features worth examination on its own.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What are some good ways to handle registration where the individual&#10;curves have a lot of noise?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Should I be able to produce individualized forecast curves that include subject-specific predicted phase/amplitude variation?  What difficulties should I anticipate?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you for any and all suggestions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample data &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Subject  Education    Ht   Wt   Training   Aptitude&#10;&#10;A        College      72  200   0          Medium&#10;B        High School  77  250   100        High&#10;C        High School  68  160   50         Low&#10;&#10;Subject Age Trials  Success Success%&#10;A   20  15  3   20%&#10;A   21  18  5   28%&#10;A   22  30  7   23%&#10;A   23  28  8   29%&#10;A   24  32  13  41%&#10;A   25  8   2   25%&#10;A   26  20  8   40%&#10;A   27  40  11  28%&#10;A   28  33  10  30%&#10;A   29  18  5   28%&#10;A   30  10  2   20%&#10;&#10;B   20  24  4   17%&#10;B   21  27  5   19%&#10;B   22  30  8   27%&#10;B   23  33  2   6%&#10;B   24  41  8   20%&#10;B   25  39  5   13%&#10;B   26  39  5   13%&#10;&#10;C   24  13  4   31%&#10;C   25  19  6   32%&#10;C   26  18  5   28%&#10;C   27  23  6   26%&#10;C   28  16  6   38%&#10;C   29  9   3   33%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-01-05T19:15:55.437" FavoriteCount="1" Id="20639" LastActivityDate="2012-01-05T19:15:55.437" OwnerUserId="5755" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;curve-fitting&gt;&lt;functional-data-analysis&gt;" Title="Predicting curve registration parameters in functional data analysis with noisy data" ViewCount="243" />
  
  <row AcceptedAnswerId="20646" AnswerCount="1" Body="&lt;p&gt;Possible warning: basic question ahead.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I model out this relationship with a logistic regression model in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod = glm(shoes ~ weather, data=mydat, family=binomial(link=&quot;logit&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering if modeling is a requirement for finding this information?&#10;If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-05T22:20:02.403" Id="20645" LastActivityDate="2012-01-05T23:23:18.627" LastEditDate="2012-01-05T22:32:44.217" LastEditorUserId="930" OwnerUserId="3310" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;logistic&gt;" Title="Extracting meaningful information from a logistic regression model" ViewCount="396" />
&#10;-\frac{1}{n} \log \beta_n &amp;amp;\rightarrow D(P_\lambda ||P_2)
  <row AcceptedAnswerId="20679" AnswerCount="1" Body="&lt;p&gt;My hairdresser Stacey always puts on a happy face, but is often stressed about managing her time.&#10;Today Stacey was overdue for my appointment and very apologetic. While getting my haircut I wondered: &#10;How long should her standard appointments be? (if the customer's preference for clean round numbers could be ignored, for a moment).&lt;/p&gt;&#10;&#10;&lt;p&gt;Something to consider is a certain 'ripple effect' where one very late customer can lead to a string of delayed appointments. In reality, hair-dressers intuitively learn to space appointments longer and longer as they fear of these stressful days. But an optimum, elegant solution must be achievable by some statistical genius out there.. (if we dumb down reality a little)&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume &lt;/p&gt;&#10;&#10;&lt;p&gt;a) hair cutting times are normally distributed and&lt;/p&gt;&#10;&#10;&lt;p&gt;b) there's only one hair dresser.&lt;/p&gt;&#10;&#10;&lt;p&gt;The cost of setting appointments &lt;em&gt;too long&lt;/em&gt; is obviously the hairdresser's time wasted waiting for the next appointment. Let's cost this wasted time $1 per minute.&lt;/p&gt;&#10;&#10;&lt;p&gt;But if the appointment's not long enough, the next customer is kept waiting, which is a heavier cost of $3 per minute to customer-loving Stacey.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Stacey works up to 8 hours per day, and has enough demand that she can fill as many appointments as she can fit in&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The mean hair cut takes her 30 minutes, with a std. dev of 10 minutes. (let's also assume men's cuts and women's cuts are the same!) &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;EDIT - some have rightly pointed out that Stacey could attend to EARLY customers ahead of their appointed time. This adds another layer of complexity, but if we treat this as quite a realistic problem we need to include it. Let's forget my 90/10 assumption and try for an assumption perhaps a little closer to reality. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Some customers are late and some are early. The mean of customers are 2 minutes late with a standard deviation of 2 minutes (sounds reasonably near reality no?) &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Exactly how long should her appointments be?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;@alexplanation sorry I've moved the goal posts on you! I'm sure R readers appreciate your answer. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-06T05:44:49.810" FavoriteCount="3" Id="20655" LastActivityDate="2012-01-12T00:59:51.060" LastEditDate="2012-01-12T00:59:51.060" LastEditorUserId="5206" OwnerUserId="5206" PostTypeId="1" Score="11" Tags="&lt;normal-distribution&gt;&lt;optimization&gt;&lt;queueing&gt;&lt;decision-theory&gt;" Title="A hair dresser's conundrum" ViewCount="600" />
  
  <row Body="&lt;p&gt;Take a look at these links.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cdc.gov/nchs/nvss/mortality_tables.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.cdc.gov/nchs/nvss/mortality_tables.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cdc.gov/nchs/products/life_tables.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.cdc.gov/nchs/products/life_tables.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;These links have all kinds of data, including life expectancy, causes of death of people etc. etc. for USA.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-06T11:01:40.133" Id="20658" LastActivityDate="2012-01-06T11:01:40.133" OwnerUserId="8033" ParentId="20656" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="20662" AnswerCount="1" Body="&lt;p&gt;I have a set of ordinal data, generated by a hypothetical turing machine. The data consists of a sequence of symbols such as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;LB1, LB2, UU, UB1 ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The labels have a natural ordering, and the machine &quot;prints&quot; out the symbols at an irregular pace. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to analyze, for the purposes of determining (with a degree of certainty), given the history of printed symbols:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How long before the next symbol is printed by the machine.&lt;/li&gt;&#10;&lt;li&gt;The most likely next symbol to be generated by the machine.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2012-01-06T11:23:32.567" Id="20660" LastActivityDate="2012-01-06T13:21:19.470" OwnerUserId="8333" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;" Title="Which machine learning methods can I use to study/analyze this ordinal data? (generated by a hypothetical turing machine)" ViewCount="125" />
  <row Body="&lt;p&gt;You can make a $R^2$ type quantity, by simply noting what $R^2$ is for normal OLS, but in the framework of an exponential family.  An exponential family likelihood (with dispersion) can be written as follows&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(y_i|\mu_i,\phi)=\exp\left(\frac{y_ib(\mu_i)-c(\mu_i)}{\phi}+d(y_i,\phi)\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $b(.),c(.),d(.;.)$ are known functions.  For normal OLS, we have $b(\mu_i)=\mu_i$, $c(\mu_i)=\frac{1}{2}\mu_i^2$ and $d(y_i,\phi)=-\frac{1}{2}\left(log(2\pi\phi)+\frac{1}{\phi}y_i^2\right)$.  A goodness of fit test for each observation, or residual, can be obtained by using the scaled likelihood ratio test&lt;/p&gt;&#10;&#10;&lt;p&gt;$$d_i^2=2\phi\left(log[f(y_i|\mu_i=y_i,\phi)]-log[f(y_i|\mu_i=\hat{\mu}_i,\phi)]\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=2\left[y_ib(y_i)-y_ib(\hat{\mu}_i)-c(y_i)+c(\hat{\mu}_i)\right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This means that, in the OLS case, the squared deviance residual is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$d_i^2=2\left[y_i^2-y_i\hat{\mu}_i-\frac{1}{2}y_i^2+\frac{1}{2}\hat{\mu}_i^2\right]=[y_i-\hat{\mu}_i]^2=e_i^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which is just the ordinary squared residual.  For OLS, we have $R^2=1-\frac{SSE}{SST}$ where SSE is the sum of squared residuals from the fitted model, and SST is the sum of squared residuals from the intercept only model.  Hence, we can analogously define $R^2$ for GLMs as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$R^2_{GLM}=1-\frac{\sum_id_{i,model}^2}{\sum_id_{i,null}^2}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-06T13:34:32.687" Id="20663" LastActivityDate="2012-01-06T13:34:32.687" OwnerUserId="2392" ParentId="11590" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One can show that if $X$ has density $f(t)$, then $Y = 1/X$ has density $g(t) = {1\over t^2} f\left( {1\over t} \right)$ (for $t\ne0$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The density of $T$ with $k$ degrees of freedom is &#10;$$\frac{1}{\sqrt{k\pi}}\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})}\frac{1}{(1+\frac{t^2}{k})^{\frac{k+1}{2}}}$$&#10;so the density of $1 \over T$ is&#10;$$\frac{1}{\sqrt{k\pi}}\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})}\frac{1}{t^2(1+\frac{1}{kt^2})^{\frac{k+1}{2}}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that for $k = 1$ this is the same density (in this case $t$ is the quotient of two iid centered gaussian variables). &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the allure of this density for $k=1, 2, 30$. As whuber says in the comments when $k&amp;gt;1$ it is bimodal, and all moments diverge.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7Nu4K.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; How do you show with elementary tools $g(t) = {1\over t^2} f\left( {1\over t} \right)$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;One possible solution is to first verify (draw a graph) that :&#10;$$\mathbb P(Y\le t) = \left\{\begin{array}{ll}
  
  
&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Equate these with the corresponding moments of the Weibull distribution (which equal $442626$, $1.56725 \times 10^{11}$, and $1.04716 \times 10^{17}$, respectively) and solve.  The general solution is a little messy but easy to compute:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  
  
&#10;=&amp;amp; 55 \pm 6.31 \sqrt{ {50\over 2}}\\
&#10;\end{align*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, if you are only interested in a lower bound of the concentration (which is Hahn question, does the average exceed 40 ?), you can say that $[23.4,+\infty)$ is a 95% one-side confidence interval.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-01-07T14:05:40.993" Id="20718" LastActivityDate="2012-01-22T08:35:22.590" LastEditDate="2012-01-22T08:35:22.590" LastEditorUserId="8076" OwnerUserId="8076" ParentId="20638" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two bivariate distributions $A$ and $B$. For some new observation $x$, I would like to calculate the probability that it belongs to $A$ and not $B$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I can assume that $A$ and $B$ are bivariate normal distributions (but if there is a method that doesn't assume that I'd like to know), but I can't assume their covariance matrices are the same. So far, I've calculated the mahalanobis distance from $x$ to $A$ and from $x$ to $B$, but I'm not sure where to go from there.&lt;/p&gt;&#10;&#10;&lt;p&gt;I should also point out that I'll be doing this for many subjects for whom the distance between $A$ and $B$ will not be the same. If there is some way to take this into account when evaluating $P(x\in A ~\wedge \notin B)$, that'd be nice too.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-07T19:46:26.403" Id="20723" LastActivityDate="2012-01-07T19:46:26.403" OwnerUserId="287" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;probability&gt;&lt;classification&gt;" Title="Classifying new observations into two bivariate categories" ViewCount="58" />
  
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  <row Body="&lt;p&gt;The p-value isnt as mysterious as most analysts make it out to be. It is a way of not having to calculate the confidence interval for a t-test but simply determining the confidence level with which null hypothesis can be rejected.&lt;/p&gt;&#10;&#10;&lt;p&gt;ILLUSTRATION.&#10;You run a test. The p-value comes up as 0.1866 for Q-variable, 0.0023 for R-variable. (These are expressed in %).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are testing at a 95% confidence level to reject the null hypo;&lt;/p&gt;&#10;&#10;&lt;p&gt;for Q: 100-18.66= 81.34%&lt;/p&gt;&#10;&#10;&lt;p&gt;for R: 100-0.23= 99.77%.&lt;/p&gt;&#10;&#10;&lt;p&gt;At a 95% confidence level, Q gives an 81.34% confidence to reject. This falls below 95% and is unacceptable. ACCEPT NULL.&lt;/p&gt;&#10;&#10;&lt;p&gt;R gives a 99.77% confidence to reject null. Clearly above the desired 95%. We thus reject the null.&lt;/p&gt;&#10;&#10;&lt;p&gt;I just illustrated the reading of the p-value through a 'reverse way' of measuring it up to the confidence level at which we reject the null hypo.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-08T03:36:29.823" Id="20736" LastActivityDate="2012-01-08T03:54:05.520" LastEditDate="2012-01-08T03:54:05.520" LastEditorUserId="8400" OwnerUserId="8400" ParentId="31" PostTypeId="2" Score="-4" />
  <row Body="&lt;p&gt;If you want to cluster your data and don't know how many clusters you need, see &lt;a href=&quot;http://stats.stackexchange.com/a/2599/6725&quot;&gt;this answer&lt;/a&gt;, including this Wikipedia article: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set&quot; rel=&quot;nofollow&quot;&gt;Determining the number of clusters in a data set&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;And there is also this question: &lt;a href=&quot;http://stats.stackexchange.com/questions/9016/how-to-define-number-of-clusters-in-k-means-clustering&quot;&gt;How to define number of clusters in K-means clustering?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-08T11:19:30.117" Id="20748" LastActivityDate="2012-01-08T11:19:30.117" OwnerUserId="6725" ParentId="20715" PostTypeId="2" Score="2" />
&#10;
  
&#10; &amp;amp;= \mathbb P(X\le d) \cdot 1_{y_0\le y} + \left\{\begin{array}{l}
&#10;\end{array}$$&#10;and the density of $Y$ conditional to $X&amp;gt;d$ is &#10;$$g(y|X&amp;gt;d) = \left\{\begin{array}{l}
&#10;  {1\over 1-p}f(y+d) \text{ if } y \ge 0.
&#10;  \end{array}\right. $$&lt;/p&gt;&#10;&#10;&lt;p&gt;As whuber pointed out in the comments, a natural interpretation of this is to change the sample space $\Omega$ to $\Omega&amp;#39; = \{ \omega \in \Omega \&amp;gt;:\&amp;gt; X(\omega) &amp;gt; d \}$. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-01-08T13:31:46.920" Id="20759" LastActivityDate="2012-01-12T21:51:51.137" LastEditDate="2012-01-12T21:51:51.137" LastEditorUserId="8076" OwnerUserId="8076" ParentId="20752" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;What you have is correct. Your units for &lt;code&gt;dati.ts&lt;/code&gt; is seconds, and your delta is every 0.05 seconds. Just keep everything in seconds and don't try to do anything in years.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the help for &lt;code&gt;ts&lt;/code&gt; (obtained by &lt;code&gt;?ts&lt;/code&gt;) doesn't mention years or months. As it says, if you use a frequency of 4 or 12, display methods will assume quarterly or monthly data, respectively, but that has nothing to do with the actual internal representation or what you can do with it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-08T16:08:47.740" Id="20765" LastActivityDate="2012-01-08T16:08:47.740" OwnerUserId="1764" ParentId="20763" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Just to make sure we are on the same page: You a sequence of 1000 samples with 7 features each. There is a sequential pattern in there which is why you process them with an RNN. At each timestep &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;It depends. It might get better if you use different normalizations, hard to tell.&lt;/li&gt;&#10;&lt;li&gt;To me it just sounds like classification. I am not sure what you mean by ranking exactly.&lt;/li&gt;&#10;&lt;li&gt;No reason to be skeptical. Normally, training error drops like that--extremly quick for few iterations, very slow afterwards.&lt;/li&gt;&#10;&lt;li&gt;No, absolutely not. For some tasks, less than 100 iterations (= passes over the training set) suffice.&lt;/li&gt;&#10;&lt;li&gt;You are the one who has to say whether the error is small enough. :) We can't tell you without knowing what you are using the network for.&lt;/li&gt;&#10;&lt;li&gt;Hard to tell. You should use early stopping instead. Train the network until the error on some held out validation set rises--that's the moment from which on you only overfit. Use the weights found then to evaluate on a test set. (That makes it three sets: training, validation, test set).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Here are some tips that I can give:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;make sure to clamp your maximal updates to some fixed value. E.g. when you do a learning step, don't apply updates bigger than 0.1 (RPROP can already do this),&lt;/li&gt;&#10;&lt;li&gt;try &lt;a href=&quot;http://en.wikipedia.org/wiki/Long_short_term_memory&quot; rel=&quot;nofollow&quot;&gt;Long Short-Term Memory&lt;/a&gt;,&lt;/li&gt;&#10;&lt;li&gt;try Hessian free optimization (&lt;a href=&quot;http://www.cs.toronto.edu/~ilya/&quot; rel=&quot;nofollow&quot;&gt;Ilya Sutskever has code on his webpage&lt;/a&gt;).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="8" CreationDate="2012-01-08T16:12:36.910" Id="20766" LastActivityDate="2012-01-08T16:12:36.910" OwnerUserId="2860" ParentId="20756" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="20814" AnswerCount="2" Body="&lt;p&gt;Using the Stata &lt;code&gt;graph twoway&lt;/code&gt; command, I have created a scatterplot with a quadratic best fit line, using the &lt;code&gt;qfit&lt;/code&gt; command.  How can I get the equation of the best fit line?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;graph tw (scatter y x) (qfit y x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-01-08T19:44:18.447" Id="20773" LastActivityDate="2012-01-09T13:35:52.733" LastEditDate="2012-01-08T21:03:54.370" LastEditorUserId="930" OwnerUserId="8410" PostTypeId="1" Score="2" Tags="&lt;stata&gt;&lt;fitting&gt;" Title="How to get parameters from a quadratic fit in Stata?" ViewCount="4380" />
  <row AcceptedAnswerId="21182" AnswerCount="1" Body="&lt;p&gt;I have the survival dataset of a population with a special disease. I´d like to compare this population with the general population to see whether this population has a decreased life-expectancy overall. What I had in mind was to create a control for each patient in the dataset and enter the age, sex and cohort specific life-expectancy from the national statistics databank and just run a Kaplan-Meier analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I´m unsure as to how I should deal with the censoring issue. Should I just censor the control if the life-expectancy for the x-aged, y-sexed, z-cohort exceeds todays date, i.e.: a 50 year old male in 2000 was expected to live 28 years in the general population? My take is that he should enter with 11 years and a censoring status.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or is there some other more mathematically savvy way of doing this taking into account the uncertainty with the projected life-expectancy for the population?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-08T20:50:36.827" FavoriteCount="1" Id="20777" LastActivityDate="2012-01-16T19:30:26.123" LastEditDate="2012-01-08T21:07:23.147" LastEditorUserId="930" OwnerUserId="1291" PostTypeId="1" Score="4" Tags="&lt;survival&gt;&lt;epidemiology&gt;&lt;life-expectancy&gt;" Title="Compare survival of one population to the general population" ViewCount="162" />
  <row AnswerCount="1" Body="&lt;p&gt;I have been asked by a journal editor to combine two separate measures of child disruptive behaviour into one variable. One meaure is parent rated and is from the 36-item Eyberg Child Behaviour Checklist. It is measured on a 7-point likert scale and gives a total score. The other measure is teacher-rated and is from the Total Difficulties score from the Strengths and Difficulties Questionnaire. It is based on a 3-point likert scale. They want me to combine these two measures into one variable, however I do not know how I should do this as they are separate measures from separate rating scales. I wondering about standardising these two variables but then  did not know whether an average of these two standardised variables would suffice. Your help would be very appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-09T02:01:21.637" Id="20790" LastActivityDate="2012-01-09T15:06:21.140" LastEditDate="2012-01-09T15:06:21.140" LastEditorUserId="930" OwnerUserId="8416" PostTypeId="1" Score="2" Tags="&lt;scales&gt;&lt;composite&gt;" Title="How do I combine two outcome measures from two different questionnaires into one variable?" ViewCount="614" />
  
  
  <row Body="&lt;p&gt;When you did ordinary (linear) PCA on your likert-scale variables you already treated those variables as scale, or continuous, variables (variables with evenly spaced measuring benchmarks). To put in other words, you haven't regarded them as ordinal so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;To recognize their ordinal (i.e. potentially not evenly spaced) nature you might consider to perform categorical PCA (CATPCA) which quantifies measuring levels nonlinearly to achieve the &quot;best&quot; principal components.&lt;/p&gt;&#10;&#10;&lt;p&gt;And yes, principal components are continuous, so usual regression is apt to them.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-09T07:55:33.810" Id="20800" LastActivityDate="2012-01-09T07:55:33.810" OwnerUserId="3277" ParentId="20791" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Maybe you would like to try regularisation that works on the levels of the categorical variable, for example a type of group LASSO approach. See e.g. Gertheiss, J. and Tutz, G. (2010): Sparse Modeling of Categorial Explanatory Variables, The Annals of Applied Statistics, 4, 2150-2180 &lt;a href=&quot;http://arxiv.org/abs/1101.1421&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/abs/1101.1421&lt;/a&gt;&lt;br&gt;&#10;This will automatically cluster the categories in a LASSO path and conduct variable selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, please take care not to use linear regression in the case of binary responses or their probabilities, it is suboptimal (rather use a glm for binary data or proportions).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Another possibility would be to use recursive partitioning (classification trees), which will greedily search for a stepwise function through the whole space spanned by the categories of the predictors and select one at each step. If interpretation of the tree structure is of interest, use an unbiased algorithm such as GUIDE or ctree, if prediction is more important CART or C4.5 will do.   &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-09T15:27:53.387" Id="20821" LastActivityDate="2012-01-09T15:27:53.387" OwnerUserId="8413" ParentId="20796" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Denote with $\alpha*$ the corrected significance level, then Bonferroni works like this: Divide the significance level $\alpha$ by the number $n$ of tests, i.e. $\alpha*=\alpha/n$. Sidak works like this (if the test are independent)  $\alpha*=1 − (1 − \alpha)^{1/n}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because $\alpha/n &amp;lt; 1 − (1 − \alpha)^{1/n}$, the Sidak correction is a bit more powerful (i.e. you get significant results more easily) but Bonferroni is a bit simpler to handle. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you need an even more powerful procedure you might want to use the Bonferroni-Holm procedure. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-09T17:03:47.253" Id="20828" LastActivityDate="2012-01-09T17:19:49.490" LastEditDate="2012-01-09T17:19:49.490" LastEditorUserId="8413" OwnerUserId="8413" ParentId="20825" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You are right, these data might likely be overdispersed. Quasipoisson is a remedy: It estimates a scale parameter as well (which is fixed for poisson models as the variance is also the mean) and will provide better fit. However, it is no longer maximum likelihood what you are then doing, and certain model tests and indices can't be used. A good discussion can be found in Venables and Ripley, Modern Applied Statistics with S.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative is to use a negative binomial model, e.g. the &lt;code&gt;glm.nb()&lt;/code&gt; function in package &lt;code&gt;MASS&lt;/code&gt;.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-09T17:15:20.653" Id="20830" LastActivityDate="2012-01-09T17:15:20.653" OwnerUserId="8413" ParentId="20826" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;When trying to determine what sort of glm equation you want to estimate, you should think about plausible relationships between the expected value of your target variable given the right hand side (rhs) variables and the variance of the target variable given the rhs variables.  Plots of the residuals vs. the fitted values from from your Normal model can help with this.  With Poisson regression, the assumed relationship is that the variance equals the expected value; rather restrictive, I think you'll agree.  With a &quot;standard&quot; linear regression, the assumption is that the variance is constant regardless of the expected value.   For a quasi-poisson regression, the variance is assumed to be a linear function of the mean; for negative binomial regression, a quadratic function.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you aren't restricted to these relationships.  The specification of a &quot;family&quot; (other than &quot;quasi&quot;) determines the mean-variance relationship.  I don't have The R Book, but I imagine it has a table that shows the family functions and corresponding mean-variance relationships.  For the &quot;quasi&quot; family you can specify any of several mean-variance relationships, and you can even write your own; see the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html&quot;&gt;R documentation&lt;/a&gt;.  It may be that you can find a much better fit by specifying a non-default value for the mean-variance function in a &quot;quasi&quot; model. &lt;/p&gt;&#10;&#10;&lt;p&gt;You also should pay attention to the range of the target variable; in your case it's nonnegative count data.  If you have a substantial fraction of low values - 0, 1, 2 - the continuous distributions probably won't fit well, but if you don't, there's not much value in using a discrete distribution.  It's rare that you'd consider Poisson and Normal distributions as competitors.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-09T17:40:20.180" Id="20831" LastActivityDate="2012-01-09T17:40:20.180" OwnerUserId="7555" ParentId="20826" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Changes in variance occur quite often in time series.We employ a search process based upon R. Tsay's  innovative work to find the point in time that the variance of the errors has changed. This leads directly to Generalized Least Squares or otherwise known as Weighted Least Squares. His work appeared in the Journal of Forecasting Vol 7 1-20 1988 and has been largely ignored by the major developers of commercial time series software but not by all .In our world we become aware of innovative research and then we implement the important improvements in analysis. This paper is very important. Note that one has to form an ARIMA model free of Anomalies (Pulses , Level Shifts, Seasonal Pulses and appropriately dertended/demeaned ) and then employ his approach otherwise false positives/false negatives would ensue. It would appear that you have at least two points in time where the variance (of the errors) has substantively changed.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-09T17:59:17.590" Id="20833" LastActivityDate="2012-01-09T18:06:14.303" LastEditDate="2012-01-09T18:06:14.303" LastEditorUserId="3382" OwnerUserId="3382" ParentId="20822" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="20844" AnswerCount="2" Body="&lt;p&gt;I have a data set with about 15,000 labeled observations of a single continuous value.&#10;What is the best way to plot this type of data? I'm playing around with various histograms and density plots, but I can't seem to figure out the best way to plot this data set. Any suggestions? &lt;/p&gt;&#10;&#10;&lt;p&gt;My data looks like this:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; label     value  &#10;-------   -------  &#10;  foo       1.2  &#10;  bar       6.2  &#10;  baz       0.2  &#10;  qux       4.7  &#10;  ...       ...  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This data set contains 15,000 values, each with a unique label.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking as to how best to create a visualization of the distribution of the data and see outliers.  Here are two candidate plots I've generated. Both simplified the data more than I would like. Are there any additional ways I could plot the data and somehow integrate the labels into this plot?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MNqcS.png&quot; alt=&quot;density plot&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/MXQ2y.png&quot; alt=&quot;boxplot&quot;&gt;&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-01-09T18:13:34.940" FavoriteCount="1" Id="20834" LastActivityDate="2012-01-10T21:44:07.687" LastEditDate="2012-01-09T19:13:43.190" LastEditorUserId="919" OwnerUserId="4593" PostTypeId="1" Score="7" Tags="&lt;data-visualization&gt;" Title="How to improve standard visualizations of a univariate distribution?" ViewCount="322" />
  <row Body="&lt;p&gt;This generalized linear model supposes the outcome associated with an independent value of $x$ has a binomial distribution whose log odds (&quot;logit&quot;) vary linearly with $x$.  The output provides the coefficients of that linear relation; namely, the intercept is estimated as -0.9781 and the slope (&quot;our_bid&quot;) as -0.002050.  You can see them in the &lt;code&gt;Estimate&lt;/code&gt; column:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;              Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -9.781e-01  2.836e-02  -34.49   &amp;lt;2e-16 ***&#10;our_bid     -2.050e-03  7.576e-05  -27.07   &amp;lt;2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The probability, which you wish to plot, is related to the log odds by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{probability} = \frac{1}{1 + \exp(-\text{log odds})}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;R calls this the &quot;inverse logit&quot; function, &lt;code&gt;inv.logit&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Putting these together gives the equation&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{probability} = \frac{1}{1 + \exp\left(-[-0.9781 - 0.00205 x]\right)}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;An R command to plot it would be&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(inv.logit(-0.9781 - 0.00205*(0:1000)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ty4Yn.png&quot; alt=&quot;Plot output&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, you should extract these coefficients with the &lt;code&gt;coefficients&lt;/code&gt; command rather than transcribing them (as I did here, because I do not have access to your data).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-09T18:34:45.560" Id="20838" LastActivityDate="2012-01-09T19:39:49.853" LastEditDate="2012-01-09T19:39:49.853" LastEditorUserId="919" OwnerUserId="919" ParentId="20835" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;You may find the following links helpful &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://blog.gribblelab.org/2009/03/09/repeated-measures-anova-using-r/&quot; rel=&quot;nofollow&quot;&gt;Repeated Measures ANOVA using R&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and&#10;&lt;a href=&quot;http://www.ats.ucla.edu/stat/R/seminars/Repeated_Measures/repeated_measures.htm&quot; rel=&quot;nofollow&quot;&gt;Repeated Measures Analysis with R&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-09T19:52:50.647" Id="20841" LastActivityDate="2012-01-09T19:52:50.647" OwnerUserId="339" ParentId="20767" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model&quot;&gt;Including the interaction but not the main effects in a model&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;I'm studying logistic regression now. And I have a question:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I have a logistic regression model as below:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3(x_1 x_2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_0$ is the &quot;constant&quot; coeffficient (I wonder if that is a correct name), $\beta_1$ is the coefficient for $x_1$, $\beta_2$ for $x_2$, and $\beta_3$ is the coefficient for the interaction term $x_1 x_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;During the variable selection processing, is it appropriate to include $x_2$ as an interaction term only, but not the main effect? I think this imply that $x_2$ itself has no effect on the outcome, but has effect when interacting with $x_1$. I wonder this kind of &lt;em&gt;partial&lt;/em&gt; variable selection is an appropriate way to do modelling.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" ClosedDate="2012-01-10T14:02:42.790" CommentCount="1" CreationDate="2012-01-10T07:22:46.387" FavoriteCount="0" Id="20862" LastActivityDate="2012-01-10T12:42:58.207" LastEditDate="2012-01-10T07:42:26.493" LastEditorUserId="88" OwnerUserId="588" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;modeling&gt;&lt;interaction&gt;&lt;interpretation&gt;" Title="Covariate present in a logistic regression model as a effect modifier, but not as main effect" ViewCount="66" />
  
&#10;\mathcal{P}(u) = 1 - \sum_{i=1}^p \varrho_i u^i
  <row Body="&lt;p&gt;The hyper-parameters should be expected to change somewhat, if only because scaling the target will also scale the noise variance.  Maximising the evidence is not really a proper Bayesian thing to do, ideally you should place an appropriate prior over the hyper-parameters and integrate them out as well, however in practice most people don't do this due to the computational expense involved.  I suspect if you integrated the hyper-parameters out rather than maximising the evidence, then both models would give similar performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;The trouble with poor performance with a small scaling factor may be due to the starting point for the hyper-parameter search being a long way from the optimal value, on a plateau, or worse near a local minima.  Changing the scale will change the noise variance, which might be enough to move the initial hyper-parameter values to somewhere nearer the optimal solution.  Try maximising the evidence starting from different (random?) initial points and see whether they call converge to the same solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Model selection for GPs is not straightforward, especially by evidence maximisation.  The evidence is noisy (it is based on a small sample of data, so small changes in the particular sample used can give susbtantial changes in the evidence) and may well have local minima etc.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Caveat: I haven't used Gaussian process regression much, but most of the comments above stem from my experiments with Gaussian Process CLassification.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-10T13:01:31.677" Id="20872" LastActivityDate="2012-01-10T13:01:31.677" OwnerUserId="887" ParentId="20866" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://www2.ece.ohio-state.edu/~randy/publications/RLM_journal/J11.pdf&quot;&gt;Schur-Cohn&lt;/a&gt; algorithm has $d=2$; this is what I learned in a computational statistics class at Berkeley some years ago.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-01-10T15:28:12.370" Id="20880" LastActivityDate="2012-01-10T15:28:12.370" OwnerUserId="7555" ParentId="20870" PostTypeId="2" Score="7" />
  
  
  <row Body="&lt;p&gt;According to &lt;a href=&quot;http://www.unc.edu/courses/2010fall/ecol/563/001/docs/solutions/assign10.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.unc.edu/courses/2010fall/ecol/563/001/docs/solutions/assign10.htm&lt;/a&gt;, two gotchas that may apply are that &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the upper truncation point may need to be adjusted (e.g., from 10 to 10.0001)&lt;/li&gt;&#10;&lt;li&gt;initial values that seed the chains need to be specified explicitly, because WinBUGS may pick ones that lie outside of ($\alpha$,$\beta$).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-01-10T21:48:25.910" Id="20906" LastActivityDate="2012-01-10T21:48:25.910" OwnerUserId="8207" ParentId="20881" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Check out Leon Bouttou's averaged stochastic gradient. It's all on &#10;&lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot; rel=&quot;nofollow&quot;&gt;his site&lt;/a&gt;. There is also a good &lt;a href=&quot;http://www.youtube.com/watch?v=A3Q5SpB7Lns&quot; rel=&quot;nofollow&quot;&gt;video lecture&lt;/a&gt; on it. &lt;/p&gt;&#10;&#10;&lt;p&gt;What he is saying is that in your setting SGD is the best you can do. If you are uncertain about the hyperparameters, run several different configurations on a small (a few thousand examples) subset of your data and check which work best.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that, I want to point out that there is a form of Online LBFGS and LBFGS even works if you don't use all the data every step (just 1000 examples for example). Maybe this is 'online enough'. Then there is resilient propagation (which, as opposed to common belief, not only works with neural networks but is an ordinary optimizer).&lt;/p&gt;&#10;&#10;&lt;p&gt;From my experience, you should try out everything. You can never reliably say which method works best a priori.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-11T06:06:04.520" Id="20914" LastActivityDate="2012-01-11T06:06:04.520" OwnerUserId="2860" ParentId="20912" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;If I have a coin that is not necessarily expected to turn up heads half of the time, then is it correct to call the coin biased?  Or does &lt;em&gt;bias&lt;/em&gt; in statistics only mean the bias of an estimator in which case it would be inappropriate to describe the coin itself as biased?&lt;/p&gt;&#10;&#10;&lt;p&gt;This question does not mean to be about issues like &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/diceRev2.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;, so maybe a different example would have been better.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-11T22:00:56.250" FavoriteCount="1" Id="20946" LastActivityDate="2013-01-16T00:39:41.607" LastEditDate="2012-01-11T22:30:41.867" LastEditorUserId="930" OwnerUserId="8476" PostTypeId="1" Score="4" Tags="&lt;interpretation&gt;&lt;bias&gt;" Title="Can a coin be biased?" ViewCount="600" />
  
  <row AcceptedAnswerId="20971" AnswerCount="1" Body="&lt;p&gt;I'm trying to build a prediction model with SVMs on fairly unbalanced data. My labels/output have three classes, positive, neutral and negative. I would say the positive example makes about 10 - 20% of my data, neutral about 50 - 60%, and negative about 30 - 40%. I'm trying to balance out the classes as the cost associated with incorrect predictions among the classes are not the same. One method was resampling the training data and producing an equally balanced dataset, which was larger than the original. Interestingly, when I do that, I tend to get better predictions for the other class (e.g. when I balanced the data, i increased the number of examples for the positive class, but in out of sample predictions, the negative class did better). Anyone can explain generally why this occurs? If I increase the number of example for the negative class, would I get something similar for the positive class in out of sample predictions (e.g., better predictions)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also very much open to other thoughts on how I can address the unbalanced data either through imposing different costs on misclassification or using the class weights in LibSVM (not sure how to select/tune those properly though).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-11T23:21:39.240" FavoriteCount="2" Id="20948" LastActivityDate="2012-01-17T04:03:24.390" LastEditDate="2012-01-12T15:00:19.733" LastEditorUserId="930" OwnerUserId="6434" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;predictive-models&gt;&lt;svm&gt;&lt;unbalanced-classes&gt;" Title="Best way to handle unbalanced multiclass dataset with SVM" ViewCount="3067" />
  
  
  <row Body="&lt;h3&gt;General References&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0130329290&quot; rel=&quot;nofollow&quot;&gt;Hair et al&lt;/a&gt; has a fairly extensive non-mathematical discussion of  issues of multivariate data cleaning and assumption testing that you might find accessible.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;First step: Understand your data&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Why are the distributions  as they are?&lt;/li&gt;&#10;&lt;li&gt;What is causing the outliers?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You might want to think about whether the skew and outliers are a natural part of the phenomena or reflect data entry errors, erroneous measurements, or participants for which your model is not intended to generalise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another point, transforming data will often remove or reduce issues of outliers.&lt;/p&gt;&#10;&#10;&lt;h3&gt;What to do with non-normal data&lt;/h3&gt;&#10;&#10;&lt;p&gt;There is some discussion of strategies for performing structural equation modelling with non-normal data here:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://rudyanto62.blogspot.com/2008/01/handling-non-normal-data-in-sem.html&quot; rel=&quot;nofollow&quot;&gt;http://rudyanto62.blogspot.com/2008/01/handling-non-normal-data-in-sem.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://ssc.utexas.edu/software/faqs/amos#Amos_7&quot; rel=&quot;nofollow&quot;&gt;http://ssc.utexas.edu/software/faqs/amos#Amos_7&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In general, it should give you greater confidence in your results that your results are not sensitive to the form of transformation and outlier adjustments that you make.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Large standardised residual covariances&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;This may suggest that your proposed model provides a poor fit to the data. It's important to think about the implications of this. What changes to your model do these residuals suggest that you make? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-01-12T03:48:45.970" Id="20958" LastActivityDate="2012-01-12T03:48:45.970" OwnerUserId="183" ParentId="20093" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Having different penalties for the margin slack variables for patterns of each class is a better approach than resampling the data.  It is asymptotically equivalent to resampling anyway, but is esier to implement and continuous, rather than discrete, so you have more control.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, choosing the weights is not straightforward.  In principal you can work out a theoretical weighting that takes into account the misclassification costs and the differences between training set an operational prior class probabilities, but it will not give the optimal performance.  The best thing to do is to select the penalties/weights for each class via minimising the loss (taking into account the misclassification costs) by cross-validation.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-12T10:54:49.557" Id="20971" LastActivityDate="2012-01-12T10:54:49.557" OwnerUserId="887" ParentId="20948" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I am analyzing (in SPSS 19) the data from a field experiment using a Linear Mixed Model with Repeated Measures. When checking the model, I always obtain negative values in the information criteria table. I know the rule of thumb “smaller is better”, but how should these negative values be interpreted? As example, when I select a type of covariance “Unstructured” the Bayesian information criterion (BIC) reads $-642.868$ and with a “Compound symmetry” covariance it reads $-497.270$. So, which type of covariance, “Unstructured” or “Compound symmetry”, is better?&lt;/p&gt;&#10;&#10;&lt;p&gt;The second question is that I receive the following warning message: &quot;The Final Hessian matrix is not positive definite although all convergence criteria are satisfied. The MIXED procedure continues despite the warning. Validity of subsequent cannot be ascertained&quot;. May I ignore it or I have a real trouble? Is there some way to fix it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-12T11:51:23.760" Id="20973" LastActivityDate="2012-01-12T14:55:23.873" LastEditDate="2012-01-12T14:55:23.873" LastEditorUserId="930" OwnerUserId="8490" PostTypeId="1" Score="2" Tags="&lt;mixed-model&gt;&lt;model-selection&gt;" Title="Linear mixed model, negative information criteria values and Hessian matrix not positive definite" ViewCount="1187" />
  <row Body="&lt;p&gt;I believe that it is the adjusted R2, but you can check this by using a regular REGRESSION command that matches your linear specification to see if the statistics match.&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-12T12:42:02.797" Id="20978" LastActivityDate="2012-01-12T12:42:02.797" OwnerUserId="6768" ParentId="20970" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I would suggest adding Linear to the list.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I asked a &lt;a href=&quot;http://math.stackexchange.com/q/87914/15941&quot;&gt;question&lt;/a&gt; &#10;on math.SE about what I, as an engineer, think of as &lt;em&gt;linear&lt;/em&gt; &#10;minimum mean square error estimation of a random variable $Y$ &#10;given the value of a random variable $X$ (meaning estimating&#10;$Y$ as $\hat{Y} = aX+b$ with $a$ and $b$ being chosen so as to&#10;minimize $E[(Y-aX-b)^2]$), and gave  a partial answer.  One of &#10;the comments on the question said&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;I am somewhat uncomfortable with your language, since I fear that this way of using the word &quot;linear&quot; might feed into the popular misunderstanding that the reason why linear regression in called linear regression is that one is fitting a line. People who think that then find it confusing when a statistician insists that one is doing linear regression when one fits a parabola or a sine wave, etc.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what &lt;em&gt;does&lt;/em&gt; linear regression mean to a statistician?&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2012-01-12T13:58:53.760" CreationDate="2012-01-12T13:58:53.760" Id="20982" LastActivityDate="2012-01-12T18:50:38.980" LastEditDate="2012-01-12T18:50:38.980" LastEditorUserId="4856" OwnerUserId="6633" ParentId="20977" PostTypeId="2" Score="13" />
  <row Body="&lt;p&gt;Here is some &lt;a href=&quot;http://robjhyndman.com/researchtips/tscvexample/&quot; rel=&quot;nofollow&quot;&gt;example code&lt;/a&gt; for cross-validating time series models.  I &lt;a href=&quot;http://moderntoolmaking.blogspot.com/2011/12/time-series-cross-validation-3.html&quot; rel=&quot;nofollow&quot;&gt;expanded on this code in my blog&lt;/a&gt;, incorporating the &lt;a href=&quot;http://cran.r-project.org/web/packages/foreach/index.html&quot; rel=&quot;nofollow&quot;&gt;foreach package&lt;/a&gt; to speed things up and allowing for a possible xreg term in the cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a copy of the code from Rob Hyndman's blog:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(fpp) # To load the data set a10&#10;plot(a10, ylab=&quot;$ million&quot;, xlab=&quot;Year&quot;, main=&quot;Antidiabetic drug sales&quot;)&#10;plot(log(a10), ylab=&quot;&quot;, xlab=&quot;Year&quot;, main=&quot;Log Antidiabetic drug sales&quot;)&#10;&#10;k &amp;lt;- 60 # minimum data length for fitting a model&#10;n &amp;lt;- length(a10)&#10;mae1 &amp;lt;- mae2 &amp;lt;- mae3 &amp;lt;- matrix(NA,n-k,12)&#10;st &amp;lt;- tsp(a10)[1]+(k-2)/12&#10;&#10;for(i in 1:(n-k))&#10;{&#10;  xshort &amp;lt;- window(a10, end=st + i/12)&#10;  xnext &amp;lt;- window(a10, start=st + (i+1)/12, end=st + (i+12)/12)&#10;  fit1 &amp;lt;- tslm(xshort ~ trend + season, lambda=0)&#10;  fcast1 &amp;lt;- forecast(fit1, h=12)&#10;  fit2 &amp;lt;- Arima(xshort, order=c(3,0,1), seasonal=list(order=c(0,1,1), period=12), &#10;      include.drift=TRUE, lambda=0, method=&quot;ML&quot;)&#10;  fcast2 &amp;lt;- forecast(fit2, h=12)&#10;  fit3 &amp;lt;- ets(xshort,model=&quot;MMM&quot;,damped=TRUE)&#10;  fcast3 &amp;lt;- forecast(fit3, h=12)&#10;  mae1[i,1:length(xnext)] &amp;lt;- abs(fcast1[['mean']]-xnext)&#10;  mae2[i,1:length(xnext)] &amp;lt;- abs(fcast2[['mean']]-xnext)&#10;  mae3[i,1:length(xnext)] &amp;lt;- abs(fcast3[['mean']]-xnext)&#10;}&#10;&#10;plot(1:12, colMeans(mae1,na.rm=TRUE), type=&quot;l&quot;, col=2, xlab=&quot;horizon&quot;, ylab=&quot;MAE&quot;,&#10;     ylim=c(0.65,1.05))&#10;lines(1:12, colMeans(mae2,na.rm=TRUE), type=&quot;l&quot;,col=3)&#10;lines(1:12, colMeans(mae3,na.rm=TRUE), type=&quot;l&quot;,col=4)&#10;legend(&quot;topleft&quot;,legend=c(&quot;LM&quot;,&quot;ARIMA&quot;,&quot;ETS&quot;),col=2:4,lty=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pd376.png&quot; alt=&quot;Results&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-12T16:16:21.987" Id="20989" LastActivityDate="2012-01-12T16:16:21.987" OwnerUserId="2817" ParentId="8807" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The (biggest) problem is that because the group variable(s) and the covariate(s) are together on the predictor side of the equation, the group variable(s) is(are) no longer the group variable(s), they are those variables with the covariate partialed out, so are no longer recognizable or interpretable as the group variables that you thought you were studying. Huge problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;The key line is on page 45 &quot;ANCOVA removes meaningful variance from &quot;Group&quot;, leaving an uncharacterized, vestigal residual Group variable with an uncertain relationship to the construct that Group represented&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My current solution is to partial the covariate out of the DV, and then submit the DV residual to a regular ANOVA, as an alternative to using ANCOVA.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-12T17:37:53.343" Id="20993" LastActivityDate="2012-01-12T17:37:53.343" OwnerUserId="8498" ParentId="1998" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The jackknife is a special case of bootstrapping. See the &lt;a href=&quot;http://en.wikipedia.org/wiki/Resampling_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-13T04:04:28.903" Id="21026" LastActivityDate="2012-01-13T04:04:28.903" OwnerUserId="8144" ParentId="21023" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to use Discriminant Analysis between two non normal populations in R. Can anybody tell me the name of the R function to do so?&#10;Could also anybody tell me how accurate my results will be if I violate the normality assumption?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-13T11:39:34.483" Id="21036" LastActivityDate="2013-08-20T21:03:34.547" LastEditDate="2012-01-13T13:11:21.553" LastEditorUserId="930" OwnerUserId="8107" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;classification&gt;&lt;nonparametric&gt;&lt;discriminant-analysis&gt;" Title="Non-parametric discriminant analysis in R" ViewCount="769" />
  
  
  
  <row Body="&lt;p&gt;Let me address the &quot;what is its practical importance&quot; part of the question.  There are many situations in which we have the ability to compute matrix vector products $Ax$ efficiently even if we don't have a stored copy of the matrix $A$ or don't have enough storage to save a copy of $A$.   For example, $A$ might be of size 100,000 by 100,000 and fully dense- it would require 80 gigabytes of RAM to store such a matrix in double preciison floating point format.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Randomized algorithms like this can be used to estimate the trace of $A$ or (using a related algorithm) individual diagonal entries of $A$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Some applications of this technique to large scale geophysical inversion problems are discussed in &lt;/p&gt;&#10;&#10;&lt;p&gt;J. K. MacCarthy, B. Borchers, and R. C. Aster. Efficient stochastic estimation of the model resolution matrix diagonal a nd generalized cross validation for large geophysical inverse problems. Journal of Geophysical Research, 116, B10304, 2011.&#10;&lt;a href=&quot;http://dx.doi.org/10.1029/2011JB008234&quot; rel=&quot;nofollow&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-13T15:41:56.677" Id="21052" LastActivityDate="2012-01-13T15:41:56.677" OwnerUserId="8200" ParentId="11974" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Taylor_series&quot; rel=&quot;nofollow&quot;&gt;Taylor series&lt;/a&gt; approximations tell us that pretty much any smooth function can be approximated by a polynomial, so including terms like $x^2$ or $x^3$ (where x is age for your example) let us estimate the coefficients for the approximation for a known or unknown non-linear function of $x$, or age in your case.  Testing these coefficients is also a simple way to test if the relationship is reasonably linear or if non-linear terms will give a better fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on the ultimate goal of the analysis the non-linear terms can be kept for prediction, or plots of the prediction can be used to suggest the actual functional relationship.  There are other tools, such as cubic splines, that can be used instead of polynomial terms to accomplish similar goals, but adding a squared term is a quick and easy way to do this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-13T17:13:58.473" Id="21057" LastActivityDate="2012-01-13T17:13:58.473" OwnerUserId="4505" ParentId="19823" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;You could fit the model you state using the &lt;code&gt;nls&lt;/code&gt; (non-linear least squares) function in &lt;code&gt;R&lt;/code&gt;, but as you said that will violate many of the assumptions and still probably will not make much sense (you are saying the predicted outcome is random around a step function, not integer values around a smoothly increasing relationship).&lt;/p&gt;&#10;&#10;&lt;p&gt;The more common way to fit count data is using Poisson regression using the &lt;code&gt;glm&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;, the first example on the help page is a Poisson regression, though if you are not that familiar with statistics it would be best to consult with a statistician to make sure that you are doing things correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the value of 8 is an absolute maximum (impossible to ever see a higher count, not just that is what you saw) then you might consider proportional odds logistic regression, there are a couple of tools to do this in packages for &lt;code&gt;R&lt;/code&gt;, but you really should get a statistician involved if you want to do this. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-13T17:44:25.977" Id="21060" LastActivityDate="2012-01-13T17:44:25.977" OwnerUserId="4505" ParentId="21058" PostTypeId="2" Score="10" />
&#10;&amp;amp;= \log\left( e^{ta} \left( -\frac{a}{b-a} e^{t(b-a)} + \frac{b}{b-a} \right)\right)\\
  
  
  
  <row AcceptedAnswerId="21095" AnswerCount="1" Body="&lt;p&gt;Consider the model in spatial econometrics denoted SAR by James P LeSage:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = \rho W y + X \beta+ \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;spdep&lt;/code&gt; and the econometric toolbox by LeSage from &lt;a href=&quot;http://www.spatial-econometrics.com&quot; rel=&quot;nofollow&quot;&gt;www.spatial-econometrics.com&lt;/a&gt;. However, when comparing the way these two compute the residuals there are differences. The &lt;code&gt;R&lt;/code&gt; package by Roger Bivand computes them by&lt;/p&gt;&#10;&#10;&lt;p&gt;$\dot{y}=y - \rho W y$ &lt;/p&gt;&#10;&#10;&lt;p&gt;and then linearly regressing $\dot{y}$ on the $X$ matrix. Obtaining residuals from this fit. (&lt;code&gt;lm(y-rWy~X-1&lt;/code&gt;). Yet LeSage computes the residuals the way (I originally thought would be the right way):&lt;/p&gt;&#10;&#10;&lt;p&gt;$y-(\rho W y)^{-1}Xb$&lt;/p&gt;&#10;&#10;&lt;p&gt;So here're my questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why do the two authors compute the residuals differently?&lt;/li&gt;&#10;&lt;li&gt;Is there an objection against one or the other way of computing them?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you for kind help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-14T13:29:05.353" Id="21092" LastActivityDate="2012-01-16T11:45:51.623" OwnerUserId="8041" PostTypeId="1" Score="4" Tags="&lt;econometrics&gt;&lt;spatial&gt;&lt;residuals&gt;" Title="Spatial econometrics -- computing residuals" ViewCount="268" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;Crux of the question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $q \sim F$ with support $[0,1]$. Let $q_j$ be the $j$th order statistic of $N$ draws from $F$. Let $z_j \sim \text{Bernoulli}(q_j)$. See that these draws are independent, given the $q$ probabilities. We need to know&#10;$$\begin{equation*}\Pr(z_{1} = 0, \dots, z_{j-1} = 0 | q_{j} = x);\end{equation*}$$&#10;in words, what's the chance of $j-1$ failures given the probability of success on the $j$th trial is $x$?&lt;/p&gt;&#10;&#10;&lt;p&gt;For a known $F$, sequence $z_1, \dots z_{j-1}$, and a particular $x$, how can we find this probability? Some kind of simulation might do it, but I'd be integrating this probability over continuous $x$, seemingly ruling out a simulation strategy (this probability is a component in an application of Bayes' rule---see below).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Scenario: I look through a page of search results, clicking on the first, then the second, and so on down the list until I find the information that I seek. I want to estimate the probability that the next site that I visit is the right one, given that the others that I have visited were not. &lt;/p&gt;&#10;&#10;&lt;p&gt;Each site has a certain probability of being the right one for me of $q_j$; we can think of this as saying that fraction $q_j$ of the population finds what they are looking for on a given site. There may be many sites that fulfill a viewer's needs, so these probabilities do not sum to 1. Let $z_j$ be an indicator for site $j$ fulfilling my needs. After visiting site $j$, I only learn $z_j$, not $q_j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine each site reaching into a distribution $F$ to find its $q_j$. Then, the sites are placed in decreasing order of $q$. Hence, $q_j$ is the $j$th order statistic after $N$ draws from $F$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to calculate&#10;$$     \begin{align*}
&#10;     \end{align*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a standard application of Bayesian updating techniques. The problem, however, is that there isn't a single parameter to be updated---$q$ varies across the sites.&lt;/p&gt;&#10;&#10;&lt;p&gt;Two ways to get around the problem are to use heuristics: for the sake of calculation, (a) $q$ is assumed to be the same across sites or (b) $q$ is assumed to change in a predictable way, e.g., $q_j = q_1 - a(j-1)$ with known $a$, giving updating over a single parameter $q_1$. Case (a) would give an estimate that is always too big. In either case, the prior distribution $\Pr(q_j=x)$ can use the order statistic properties of $F$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach would be to assume that I know $F$ and to use the fact that we have order statistics. But, for a given $x$ in this integral, there isn't a unique set of probabilities for sites 1 to $j-1$. For example, in the case of $j=2$ and in calculating the integral for the case $q_2=x$, $\Pr(z_1=0)$ could fall anywhere between 0 and $1-x$. The question becomes which of these values to plug in? One obvious answer would be the expected value of each order statistic. This doesn't seem like it works to me. We might be able to simulate the quantity inside the integral for each value of $x$, but integrating across continuous $x$ seems to make this approach too intensive a strategy.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-14T17:24:14.250" FavoriteCount="1" Id="21096" LastActivityDate="2012-01-14T17:40:39.963" LastEditDate="2012-01-14T17:40:39.963" LastEditorUserId="401" OwnerUserId="401" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;order-statistics&gt;" Title="Calculating probabilities related to order statistics" ViewCount="149" />
&#10;C\left(\frac{1}{\sqrt{2\pi}}e^{-\frac{\beta_0^2}{2}}\right)\left(\frac{1}{\sqrt{2\pi}}e^{-\frac{\beta_1^2}{2}}\right)\left(\frac{1}{\sqrt{2\pi}}e^{-\frac{\beta_2^2}{2}}\right)\mu^{0.1-1}e^{-0.1\mu}
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But I still don't know how to derive the posterior distribution, neither do I know how to do the sampling with R. Can anyone give me some references? Thanks very much.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-15T06:06:48.280" Id="21106" LastActivityDate="2012-01-15T08:06:10.657" LastEditDate="2012-01-15T08:06:10.657" LastEditorUserId="7224" OwnerUserId="4864" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;gibbs&gt;&lt;posterior&gt;" Title="Gibbs sampling from posterior distribution using R" ViewCount="1217" />
  
  <row AcceptedAnswerId="21118" AnswerCount="1" Body="&lt;p&gt;Having a map such as:&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/0NLWV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;&lt;strong&gt;How to find major directions of variation&lt;/strong&gt; i.e., for the example given here: north-east--south-west? in an elegant, quick and clean way?&#10;Any ideas are more than welcome.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-15T15:00:30.263" FavoriteCount="2" Id="21115" LastActivityDate="2012-01-15T16:36:29.390" LastEditDate="2012-01-15T16:31:58.530" LastEditorUserId="88" OwnerUserId="6489" PostTypeId="1" Score="4" Tags="&lt;multivariate-analysis&gt;&lt;algorithms&gt;" Title="Major directions on a 2D map " ViewCount="84" />
  <row AcceptedAnswerId="22594" AnswerCount="7" Body="&lt;p&gt;Can you recommend a book with good information that can be applied to developing a recommender system?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-15T17:04:27.533" FavoriteCount="6" Id="21121" LastActivityDate="2014-01-15T16:09:28.650" LastEditDate="2012-01-15T22:08:33.240" LastEditorUserId="88" OwnerUserId="6329" PostTypeId="1" Score="7" Tags="&lt;machine-learning&gt;&lt;books&gt;&lt;recommender-system&gt;" Title="Recommendation for a book about recommender systems" ViewCount="1058" />
  <row Body="&lt;p&gt;As a warning - fitting polynomials like this is fraught with danger in several respects.  Even for smallish $N$ you risk having an ill-conditioned problem, especially if the range of $x$ values is large, with substantial, perhaps total, loss of significant digits of accuracy.  Additionally, polynomials often are poor functions for approximating functions that are not themselves polynomials.  You often wind up with way too much &quot;wiggliness&quot; in regions where there aren't many x-values.  Splines tend to do much better in both regards (as in R package mgcv), and also get around the problem of selecting polynomial degree.  Unfortunately, automated spline estimation packages won't help you with your derivative constraint, at least not easily.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your testing procedure also relies on an $F-$test to determine whether a polynomial of degree $N$ adequately explains the variance in $y$, but that won't work.  To see this, consider $y = x + e$, a first degree polynomial, with $x$ distributed, say, $U(0,1)$.  The variance of $y$ depends very heavily on the variance of $e$, in fact, making standard assumptions, $\sigma^2_y = \sigma^2_e + 1/12$.  The amount of the variance of $y$ that can be explained by the polynomial in $x$ can be arbitrarily close to either 0% or 100%, depending on $\sigma^2_e$, but whatever that amount is, the model $y = x + e$ is correct.  Clearly no test, other than human judgment, can determine whether the amount of the variance of $y$ that can be explained by the polynomial in $x$ is adequate or not.  The issue of the definition of &lt;em&gt;adequate&lt;/em&gt; in a mathematically formal way also arises; adequate is determined by some external, human, objective, and is often not readily quantifiable.&lt;/p&gt;&#10;&#10;&lt;p&gt;On to your question - the constrained polynomial equation can be transformed into an unconstrained equation with two fewer parameters, then standard techniques can be applied to the resultant equation.  Consider the boundary constraint $P_N(0)=0$.  Your expression has a constant term; just eliminate the constant term from the polynomial, and, when $x=0$, the polynomial will equal 0 as well.  The derivative constraint can be incorporated directly into the expression by writing out the derivative and observing that the parameter $a_1$ can be expressed as a polynomial in $L$:  $a_1 = - \sum_{i=2}^Nia_iL^{i-1}$.  This relationship has to hold for the derivative constraint to be satisfied, and vice versa.  Then you can substitute this expression for $a_1$, rearrange terms, and estimate the resulting unconstrained linear equation instead.  For example, for $N=3$:  &lt;/p&gt;&#10;&#10;&lt;p&gt;$a_1 = -2a_2L -3a_3L^2$&#10;$p_N = (-2a_2L -3a_3L^2)x + a_2x^2 + a_3x^3 = (-2Lx+x^2)a_2 + (-3L^2x + x^3)a_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;By creating new right hand side variables, e.g., $z_1 = -2Lx+x^2$, you recover the linear regression form $p_N = a_2z_1 + a_3z_2$ etc., and you can run that regression instead, testing for improvement as you increment $N$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-15T18:13:57.127" Id="21124" LastActivityDate="2012-01-15T19:59:15.973" LastEditDate="2012-01-15T19:59:15.973" LastEditorUserId="7555" OwnerUserId="7555" ParentId="21120" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&quot;... since the SNR isn't one to one function of the parameters, it cannot be calculated using them&quot; isn't always true.  In this case, the MLE of the SNR is exactly what you'd hope, just plug in the MLEs of $\alpha$ and $\sigma^2$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To see this heuristically, assume the MLE of the SNR, label it $\widehat{SNR}$, is something else.  Then the likelihood calculated using $\widehat{SNR}$ and the MLE of, say, $\alpha$ and using the relationship $SNR = \alpha^2p / \sigma^2$ will be lower than it would be if the MLEs of both $\alpha$ and $\sigma^2$ were used, because the estimate of $\sigma^2$ will be different from the MLE of $\sigma^2$.  Therefore $\widehat{SNR}$ isn't maximizing the likelihood.  Therefore it isn't the MLE of the SNR.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-15T21:01:56.333" Id="21134" LastActivityDate="2012-01-15T21:01:56.333" OwnerUserId="7555" ParentId="21111" PostTypeId="2" Score="1" />
&#10;$$&#10;where $\Z$ is $n \times p$ with orthonormal columns and $\G = (g_{ij})$ is $p \times p$ upper triangular. I say &quot;almost&quot; since the algorithm is only specifying $\Z$ up to the norms of the columns, which will not in general be one, but can be made to have unit norm by normalizing the columns and making a corresponding simple adjustment to the coordinate matrix $\G$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming, of course, that $\m X \in \mathbb R^{n \times p}$ has rank $p \leq n$, the unique least squares solution is the vector $\bhat$ that solves the system&#10;$$
&#10;\G^T \G \bhat = \G^T \Z^T \m y \&amp;gt; ,
&#10;$$&#10;It is not hard to see (verify this as a check of understanding!) that $g_{pp} = \|\z_p\|$ and so this yields the solution. (&lt;strong&gt;Caveat lector&lt;/strong&gt;: I've used $\z_i$ already normalized to have unit norm, whereas in the book they have &lt;em&gt;not&lt;/em&gt;. This accounts for the fact that the book has a squared norm in the denominator, whereas I only have the norm.)&lt;/p&gt;&#10;&#10;&lt;p&gt;To find &lt;strong&gt;all&lt;/strong&gt; of the regression coefficients, one needs to do a simple backsubstitution step to solve for the individual $\bhat_i$. For example, for row $(p-1)$,&#10;$$
&#10;$$&#10;be &lt;em&gt;any&lt;/em&gt; QR decomposition of $\m X$. Then, using exactly the same reasoning and algebraic manipulations as above, we have that the least-squares solution $\bhat$ satisfies&#10;$$
&#10;\m R \bhat = \m Q^T \m y \&amp;gt; .
  <row Body="&lt;p&gt;I like @nikhil 's answer. I would use the order as the ID, for ease of interpretation. That is 2143 would be the ID for that rank.&lt;/p&gt;&#10;&#10;&lt;p&gt;But it may be possible to simplify. Are you interested in predicting all of the 24 combinations? Suppose person A ranks the object 2134 and person B ranks them 2143. Do you care? If you only care about which is ranked first, you have only 4 levels; only first and second, 12 levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;IF this is what you want, it has several advantages:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Smaller required sample size&lt;/li&gt;&#10;&lt;li&gt;More precise estimates&lt;/li&gt;&#10;&lt;li&gt;Easier interpretation&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2012-01-16T12:35:53.663" Id="21158" LastActivityDate="2012-01-16T12:35:53.663" OwnerUserId="686" ParentId="21145" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Though they could be viewed as an entirely separate construct from the random variable, &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_measure&quot; rel=&quot;nofollow&quot;&gt;random measures&lt;/a&gt; map into a set of measures. However, the measures are generally parameterized by real valued random variables, which kind of make this a somewhat unsatisfying answer to your question. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, one might wish to pick a random colour or random word or something similar, but, in these cases, one can just parameterize the space you are mapping to by real numbers and use random variables under the standard definition. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-16T19:55:34.907" Id="21184" LastActivityDate="2012-01-16T19:55:34.907" OwnerUserId="8242" ParentId="21179" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;In R i would tell you to see if the functions related with &quot;bootweights&quot; in the survey package suit you in any way. But since you have already gone over that package I don't think you will find many alternatives ... I also looked for a similiar thing a couple of weeks ago and ended up implementing my own code.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the discussion of bootstrapping and survey weights in general you can find some references in this &lt;a href=&quot;http://repec.org/snasug08/kolenikov_snasug08.pdf&quot; rel=&quot;nofollow&quot;&gt;presentation&lt;/a&gt; which also contains references to an implementation of a bootstrapping procedure for complex survey designs in STATA.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-16T20:15:31.777" Id="21186" LastActivityDate="2012-01-16T20:15:31.777" OwnerUserId="8528" ParentId="21002" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;The two most common methods (in my experience) for comparing signals are the correlation and the mean squared error. Informally, if you imagine your signal as a point in some N-dimensional space (this tends to be easier if you imagine them as 3D points) then the correlation measures whether the points are in the same direction (from the &quot;origin&quot;) and the mean squared error measures whether the points are in the same place (independent of the origin as long as both signals have the same origin). Which works better depends somewhat on the types of signal and noise in your system.&lt;/p&gt;&#10;&#10;&lt;p&gt;The MSE appears to be roughly equivalent to your example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mse = 0;&#10;for( int i=0; i&amp;lt;N; ++i )&#10;    mse += (x[i]-y[i])*(x[i]-y[i]);&#10;mse /= N;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;note however that this isn't really Pearson correlation, which would be more like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xx = 0;&#10;xy = 0;&#10;yy = 0;&#10;&#10;for( int i=0; i&amp;lt;N; ++i )&#10;{&#10;    xx += (x[i]-x_mean)*(x[i]-x_mean);&#10;    xy += (x[i]-x_mean)*(y[i]-y_mean);&#10;    yy += (y[i]-y_mean)*(y[i]-y_mean);&#10;}&#10;&#10;ppmcc = xy/std::sqrt(xx*yy);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;given the signal means x_mean and y_mean. This is fairly close to the pure correlation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;corr = 0;&#10;for( int i=0; i&amp;lt;N; ++i )&#10;    corr += x[i]*y[i];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;however, I think the Pearson correlation will be more robust when the signals have a strong DC component (because the mean is subtracted) and are normalised, so a scaling in one of the signals will not cause a proportional increase in the correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, if the particular example in your question is a problem then you could also consider the mean absolute error (L1 norm):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mae = 0;&#10;for( int i=0; i&amp;lt;N; ++i )&#10;    mae += std::abs(x[i]-y[i]);&#10;mae /= N;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm aware of all three approaches being used in various signal and image processing applications, without knowing more about your particular application I couldn't say what would be likely to work best. I would note that the MAE and the MSE are less sensitive to exactly how the data is presented to them, but if the mean error is not really the metric you're interested in then they won't give you the results you're looking for. The correlation approaches can be better if you're more interested in the &quot;direction&quot; of your signal than the actual values involved, however it is more sensitive to how the data are presented and almost certainly requires some centring and normalisation to give the results you expect.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to look up &lt;a href=&quot;http://en.wikipedia.org/wiki/Phase_correlation&quot;&gt;Phase Correlation&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross_correlation&quot;&gt;Cross Correlation&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross_correlation#Normalized_cross-correlation&quot;&gt;Normalised Correlation&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Matched_filter&quot;&gt;Matched Filters&lt;/a&gt;. Most of these are used to match some sub-signal in a larger signal with some unknown time lag, but in your case you could just use the value they give for zero time lag if you know there is no lag between the two signals.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-16T11:16:28.737" Id="21205" LastActivityDate="2012-01-16T12:57:21.130" OwnerDisplayName="Adam Bowen" OwnerUserId="8571" ParentId="21203" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I have some gene measurements from a microarray experiment.&#10;On each array, I have a set of &quot;non expressed&quot; background genes.&#10;I have three replicate measurements for some tissues.&#10;How to use the background genes (not normally distributed) to evaluate if the other genes are expressed in each tissue?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-17T07:33:24.773" Id="21218" LastActivityDate="2012-01-17T12:36:40.730" LastEditDate="2012-01-17T12:36:40.730" LastEditorUserId="930" OwnerUserId="8143" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;microarray&gt;" Title="Using background genes to evaluate gene expression level" ViewCount="95" />
  
  <row Body="&lt;p&gt;First, you need to understand that these two multiple testing procedures do not control the same thing. Using your example, we have two groups with 18,000 observed variables, and you make 18,000 tests in order to identify some variables which are different from one group to the other.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Bonferroni correction controls the &lt;a href=&quot;http://en.wikipedia.org/wiki/Familywise_error_rate&quot;&gt;Familywise error rate&lt;/a&gt;, that is the probability, assuming all the 18,000 variables have identical distribution in the two groups, that you are falsely claiming &quot;here I have some significant differences&quot;. Usually, you decide that if this probability is &amp;lt; 5%, your claim is credible.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Benjamini-Hochberg correction controls the &lt;a href=&quot;http://en.wikipedia.org/wiki/False_discovery_rate&quot;&gt;False discovery rate&lt;/a&gt;, that is, the expected proportion of false positives among the variables for which you claim the existence of a difference. For example, if with FDR controlled to 5% 20 tests are positive, &quot;in average&quot; only 1 of these tests will be a false positive. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now, when the number of comparison increases... well, it depends on the number of marginal null hypotheses that are true. But basically, with both procedures, if you have a few, let’s says 5 or 10, truly associated variables, you have more chances to detect them among 100 variables than among 1,000,000 variables. That should be intuitive enough. There’s no way to avoid this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-17T11:18:16.163" Id="21225" LastActivityDate="2012-01-17T11:52:21.580" LastEditDate="2012-01-17T11:52:21.580" LastEditorUserId="8076" OwnerUserId="8076" ParentId="21193" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;If your variables are of incomparable units (e.g. height in cm and weight in kg) then you should standardize variables, of course. Even if variables are of the same units but show quite different variances it is still a good idea to standardize before K-means. You see, K-means clustering is &quot;isotropic&quot; in all directions of space and therefore tends to produce more or less round (rather than elongated) clusters. In this situation leaving variances unequal is equivalent to putting more weight on variables with smaller variance, so clusters will tend to be separated along variables with greater variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;A different thing also worth to remind is that K-means clustering results are sensitive to the order of objects in the data set. A justified practice would be to run the analysis several times, randomizing objects order; then average the cluster centres of those runs and input the centres as initial ones for one final run of the analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-17T11:23:12.703" Id="21226" LastActivityDate="2012-01-17T11:33:33.450" LastEditDate="2012-01-17T11:33:33.450" LastEditorUserId="3277" OwnerUserId="3277" ParentId="21222" PostTypeId="2" Score="18" />
  
  <row Body="&lt;p&gt;To compare this two forms, you will need the number os users who filled this forms. More precisely, you need a sample from each form where the variables is the amount of money made with the user. Considering $\theta_{1}$ the vector of samples for the form with no ads and $\theta_{2}$, the samples for the form with ads, you can then do a hypothesis test on the sample mean:&#10;$$ H_0: \bar{\theta_1} = \bar{\theta_2}$$&#10;$$H_1: \bar{\theta_1} \neq \bar{\theta_2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the test statistic is:&#10;$$
&#10;$$&#10;$n_1$ and $n_2$ are the sample sizes, $s_{\theta_1}^2$ and $s_{\theta_2}^2$ are the sample variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;The null hypothesis is reject if $|z_0| &amp;gt; z_{\alpha/2}$, considering an $\alpha$ level of significance.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-17T16:02:38.963" Id="21244" LastActivityDate="2012-01-17T16:02:38.963" OwnerUserId="8447" ParentId="19507" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;There's no way to avoid setting up a full probability model.  With only 15 observations your credible interval will likely to depend on the structure of your likelihood and prior.  Nonparametric Bayesian approaches may be useful, but it's hard to imagine that you're going to do much better than just eyeballing your data and trying to transform it to a reasonable parametric family. &lt;/p&gt;&#10;&#10;&lt;p&gt;A &lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/readings/escobar-west.pdf&quot;&gt;Dirichlet Process Mixture&lt;/a&gt; model of some bounded distribution may be a good starting point.  Alternatively, the model you describe would essentially be a &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1176345338&quot;&gt;Bayesian Boostrap&lt;/a&gt;.  However the point of that paper is that using the empirical distribution is not a reasonable model for some inferences.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-18T01:25:35.737" Id="21270" LastActivityDate="2012-01-18T01:25:35.737" OwnerUserId="493" ParentId="21172" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Here is an attampt of doing what you wanted.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; # Setting up some sample data&#10; require(dummies)&#10; df &amp;lt;- data.frame(categorial=rep(c(1,2,3), each=20), x=rnorm(60))&#10; flevels &amp;lt;- dummy(df$categorial)&#10; df$categorial &amp;lt;- factor(df$categorial)&#10;&#10; df$y=20 + df$x*3 + flevels%*%c(3,1,2) + rnorm(60)*2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I use a regression in order to obtain the minimum factor level and then reorder:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; # Now we start with trying to find the minimum cateogry. However, note that this does not work in every context!&#10; summary(helpreg &amp;lt;- lm(y~x+factor(categorial) - 1, data=df))&#10;&#10;     Coefficients:&#10;                         Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;     x                     2.9944     0.2334   12.83   &amp;lt;2e-16 ***&#10;     factor(categorial)1  22.9640     0.4472   51.35   &amp;lt;2e-16 ***&#10;     factor(categorial)2  21.0720     0.4390   48.00   &amp;lt;2e-16 ***&#10;     factor(categorial)3  22.1300     0.4364   50.71   &amp;lt;2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then I start to sort out the minimum:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; factors &amp;lt;- grep('categorial', names(coef(helpreg))) # --- replace categorial with your variable name&#10;&#10; minimumf &amp;lt;- which(coef(helpreg)[factors]==min(coef(helpreg)[factors]))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is then releveled&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; df$categorial &amp;lt;- relevel(df$categorial, ref=minimumf)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And in my case it works - probably it works for you as well....&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; summary(lm(y~x+factor(categorial), data=df))&#10;&#10;                           Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;       (Intercept)          21.0720     0.4390  48.003  &amp;lt; 2e-16 ***&#10;       x                     2.9944     0.2334  12.828  &amp;lt; 2e-16 ***&#10;       factor(categorial)1   1.8920     0.6341   2.984  0.00421 ** &#10;       factor(categorial)3   1.0580     0.6193   1.708  0.09310 . &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Comments of course highly appreciated!&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-01-18T06:54:02.957" Id="21276" LastActivityDate="2012-01-18T06:54:02.957" OwnerUserId="8041" ParentId="21257" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="21325" AnswerCount="2" Body="&lt;p&gt;I have six scatterplots (such as the one below) that show an insignificant interaction of the factor (X2) (e.g. gender, age, education etc.) with the predictor (X1) on Y. (X1 is significant.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cq0GH.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The simplest interpretation is that as X1 increases, Y increases for both levels of the factor (in this case, males and females). In other words, gender does not affect the relationship between X1 and Y and the difference in slope is due to sampling error.&lt;/p&gt;&#10;&#10;&lt;p&gt;These two statements apply to all other factors (e.g. age, which has three categories; education, which has three levels etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;One option is to present the six scatterplots and repeat the above statements as explanation. However, this seems tedious. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there a better (and succint way) of presenting these findings?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(I guess this is not a statistical question but it certainly relates to data analysis and interpretation!)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-18T12:16:11.700" Id="21283" LastActivityDate="2012-01-19T13:06:46.230" LastEditDate="2012-01-18T12:21:34.373" LastEditorUserId="6096" OwnerUserId="6096" PostTypeId="1" Score="-1" Tags="&lt;scatterplot&gt;" Title="How to present several ANCOVA findings" ViewCount="657" />
  
&#10;\mathbb P(21.5 &amp;lt; Y &amp;lt; 22.5) &amp;amp; \simeq 0.2397501 - 0.1610994 \simeq 0.07865066.\end{align*}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-18T21:55:21.540" Id="21302" LastActivityDate="2012-01-19T12:23:30.287" LastEditDate="2012-01-19T12:23:30.287" LastEditorUserId="8076" OwnerUserId="8076" ParentId="21297" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="21306" AnswerCount="1" Body="&lt;p&gt;I am trying to find out the rejection region for a permutation test about the difference between medians where:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; H0: Difference between medians equals 0; &#10; Ha: Difference between medians is greater than 0. &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Considering that I have the distribution of my test statistics on resampled datasets, how can I find the critical value for the test so that I can infer the rejection region?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(permstats&amp;gt;=obs.median)/length(permstats) #pvalue&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It is not completely clear to me how can I find the rejection region according to the permutations distribution( and maybe the p-value??)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-18T22:11:13.170" Id="21303" LastActivityDate="2012-01-18T23:15:03.480" LastEditDate="2012-01-18T22:49:51.810" LastEditorUserId="8353" OwnerUserId="8353" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;permutation&gt;" Title="How to define the rejection region for a permutation test?" ViewCount="278" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://www.stat.lsa.umich.edu/~gmichail/ada_final.pdf&quot;&gt;This paper on Adaboost&lt;/a&gt; gives some suggestions and code (page 17) for extending 2-class models to K-class problems.  I would like to generalize this code, such that I can easily plug in different 2-class models and compare the results.  Because most classification models have a formula interface and a &lt;code&gt;predict&lt;/code&gt; method, some of this should be relatively easy.  Unfortunately, I haven't found a standard way of extracting class probabilities from 2-class models, so each model will require some custom code.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a function I wrote to break up a K-class problem into 2-class problems, and return K models:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;oneVsAll &amp;lt;- function(X,Y,FUN,...) {&#10;    models &amp;lt;- lapply(unique(Y), function(x) {&#10;        name &amp;lt;- as.character(x)&#10;        .Target &amp;lt;- factor(ifelse(Y==name,name,'other'), levels=c(name, 'other'))&#10;        dat &amp;lt;- data.frame(.Target, X)&#10;        model &amp;lt;- FUN(.Target~., data=dat, ...)&#10;        return(model)&#10;    })&#10;    names(models) &amp;lt;- unique(Y)&#10;    info &amp;lt;- list(X=X, Y=Y, classes=unique(Y))&#10;    out &amp;lt;- list(models=models, info=info)&#10;    class(out) &amp;lt;- 'oneVsAll'&#10;    return(out)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's a prediction method I wrote to iterate over each model and make predictions:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;predict.oneVsAll &amp;lt;- function(object, newX=object$info$X, ...) {&#10;    stopifnot(class(object)=='oneVsAll')&#10;    lapply(object$models, function(x) {&#10;        predict(x, newX, ...)&#10;    })&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And finally, here's a function to turn normalize a &lt;code&gt;data.frame&lt;/code&gt; of predicted probabilities and classify the cases. Note that it is up to you to construct the K-column &lt;code&gt;data.frame&lt;/code&gt; of probabilities from each model, as there is not a unified way to extract class probabilities from a 2-class model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;classify &amp;lt;- function(dat) {&#10;    out &amp;lt;- dat/rowSums(dat)&#10;    out$Class &amp;lt;- apply(dat, 1, function(x) names(dat)[which.max(x)])&#10;    out&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's an example using &lt;code&gt;adaboost&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ada)&#10;library(caret) &#10;X &amp;lt;- iris[,-5]&#10;Y &amp;lt;- iris[,5]&#10;myModels &amp;lt;- oneVsAll(X, Y, ada)&#10;preds &amp;lt;- predict(myModels, X, type='probs')&#10;preds &amp;lt;- data.frame(lapply(preds, function(x) x[,2])) #Make a data.frame of probs&#10;preds &amp;lt;- classify(preds)&#10;&amp;gt;confusionMatrix(preds$Class, Y)&#10;Confusion Matrix and Statistics&#10;&#10;            Reference&#10;Prediction   setosa versicolor virginica&#10;  setosa         50          0         0&#10;  versicolor      0         47         2&#10;  virginica       0          3        48&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is an example using &lt;code&gt;lda&lt;/code&gt; (I know lda can handle multiple classes, but this is just an example):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;myModels &amp;lt;- oneVsAll(X, Y, lda)&#10;preds &amp;lt;- predict(myModels, X)&#10;preds &amp;lt;- data.frame(lapply(preds, function(x) x[[2]][,1])) #Make a data.frame of probs&#10;preds &amp;lt;- classify(preds)&#10;&amp;gt;confusionMatrix(preds$Class, Y)&#10;Confusion Matrix and Statistics&#10;&#10;            Reference&#10;Prediction   setosa versicolor virginica&#10;  setosa         50          0         0&#10;  versicolor      0         39         5&#10;  virginica       0         11        45&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These functions should work for any 2-class model with a formula interface and a &lt;code&gt;predict&lt;/code&gt; method.  Note that you have to manually split up the X and Y components, which is a little ugly, but writing a formula interface is beyond me at the moment.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this approach make sense to everyone?  Is there any way I can improve it, or is there an existing package to solve this issue?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-19T16:31:56.033" FavoriteCount="7" Id="21343" LastActivityDate="2012-03-15T23:54:38.203" OwnerUserId="2817" PostTypeId="1" Score="9" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;&lt;statistical-learning&gt;&lt;multi-class&gt;" Title="Extending 2-class models to multi-class problems" ViewCount="2524" />
  <row AcceptedAnswerId="21360" AnswerCount="2" Body="&lt;p&gt;Given a population to be divided between treatment / control, if I believe a variable is distributed lognormal in the population, how do I know what the necessary control size is in order to guarantee a difference in means is significant at some arbitrary level for a specific effect size? &lt;/p&gt;&#10;&#10;&lt;p&gt;(Reference to R or SAS example appreciated)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-19T19:53:39.747" Id="21355" LastActivityDate="2012-01-19T22:31:54.513" OwnerUserId="2149" PostTypeId="1" Score="1" Tags="&lt;power-analysis&gt;&lt;lognormal&gt;" Title="For lognormal data, what are necessary sample sizes for a difference in means between treatment / control" ViewCount="373" />
  
  
  
  
  
  <row Body="&lt;p&gt;PDF, ps,SVG, or eps are &lt;a href=&quot;http://en.wikipedia.org/wiki/Vector_graphics&quot; rel=&quot;nofollow&quot;&gt;vector based graphics&lt;/a&gt; device. JPG, TIFF,PNG,...etc are Raster graphics. When it comes raster graphics, the trade off is between the size and quality.  By adjusting width,height and other formatting you could keep high quality in &lt;a href=&quot;http://en.wikipedia.org/wiki/Raster_graphics&quot; rel=&quot;nofollow&quot;&gt;raster graphics&lt;/a&gt;. You could take PNG or JPG format to insert the image.&lt;/p&gt;&#10;&#10;&lt;p&gt;SVG images may insert in MSWORD document. You can check &lt;a href=&quot;http://macolabels.com/wordpress/?p=995&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-20T10:26:10.697" Id="21382" LastActivityDate="2012-01-20T10:26:10.697" OwnerUserId="7788" ParentId="21380" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If you are on Windows try &lt;code&gt;win.metafile&lt;/code&gt;. It is vector format, if I remember correctly, and it  plays nicely with Word. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-20T11:43:53.310" Id="21385" LastActivityDate="2012-01-20T11:43:53.310" OwnerUserId="2116" ParentId="21380" PostTypeId="2" Score="7" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using &lt;strong&gt;kalman filter&lt;/strong&gt; to estimate my state variables for a time series data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    S(t) = A*S(t-1) + B*X(t) + Q   ----------Sate process&#10;    Z(t) = H*S(t)   + W*D(t) + R   ----------Measurement process&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   T: the total number of time slots in the time series data&#10;   N: number of state variable&#10;   M: number of measurement variable&#10;   S: state variable, N*T dimension matrix&#10;   Z: measurement variable, M*T dimension matrix&#10;   Q: state process noise&#10;   R: measurement process noise&#10;   X: input in state process, L dimension for a time slot&#10;   D: input in measurement process, I dimension for a time slot&#10;   A, B and H: parameter matrix&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I try to use optim() in R to find the best A, B, and H matrix for the KF.&#10;All parameters in the model are constrained, which means that they have boundaries.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are my codes for seting parameters, caclualte KF and ML.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;setParameterKF &amp;lt;- function(lambda, beta, delta1, delta2, gamma, N, M){&#10;&#10;    A &amp;lt;- lambda*diag(N);&#10;    B &amp;lt;- beta;&#10;    if(N &amp;gt; 1) {B &amp;lt;- diag(B);}&#10;    Q &amp;lt;- delta1/N * diag(N);&#10;    R &amp;lt;- delta2/M * diag(M);&#10;    H &amp;lt;- t(matrix(rep(1, N)));&#10;    W &amp;lt;- gamma;&#10;    return(list(&quot;A&quot;=A,&quot;B&quot;=B,&quot;Q&quot;=Q,&quot;R&quot;=R,&quot;H&quot;=H,&quot;W&quot;=W));&#10;}&#10;&#10;kfilter &amp;lt;- function(A, B, H, W, Q, R, X, D, Z, S0, P0)&#10;{   &#10;    ################################&#10;    #get matrix dimention&#10;    ################################&#10;    # number of state variable: N&#10;    nStateVar &amp;lt;- dim(H)[2];&#10;    # number of measurement variable: M&#10;    nMeasureVar &amp;lt;- dim(H)[1];&#10;    # number of independent variable in state process: L1 &#10;    nInputInState &amp;lt;- dim(X)[1];&#10;    # number of independent variable in measurement process: L2&#10;    nInputInMeasurement &amp;lt;- dim(D)[1];&#10;    # number of time point&#10;    T &amp;lt;- dim(Z)[2];&#10;&#10;    # Initialization of KF&#10;    #S0 &amp;lt;- matrix(c(rep(sIni, nStateVar)));&#10;    #P0 &amp;lt;- matrix(c(rep(pIni, nStateVar*nStateVar)), nrow = nStateVar);&#10;&#10;    SPost = S0;&#10;    PPost = P0;&#10;&#10;    SPreList    &amp;lt;- matrix(0*c(1:(T*nStateVar)), nrow = nStateVar);&#10;    PPreList    &amp;lt;- list();&#10;    KList       &amp;lt;- list();&#10;    SPostList   &amp;lt;- matrix(0*c(1:(T*nStateVar)), nrow = nStateVar);&#10;    PPostList   &amp;lt;- list();&#10;    VList       &amp;lt;- matrix(0*c(1:(T*nMeasureVar)), nrow=nMeasureVar);&#10;    omegaList   &amp;lt;- list();&#10;&#10;    for(t in 1:T)&#10;    {&#10;        # Predict process&#10;        SPre &amp;lt;- A %*% SPost + B%*%X[, t];   &#10;        PPre &amp;lt;- A %*% PPost %*% t(A) + Q;&#10;&#10;        # Correct process&#10;        temp &amp;lt;- H %*% PPre;&#10;        omegaPre &amp;lt;- temp %*% t(H) + R;&#10;&#10;        K &amp;lt;- PPre %*% t(H) %*% ginv(omegaPre);&#10;&#10;        V &amp;lt;- Z[,t] - H %*% SPre - W %*% D[,t];&#10;        SPost &amp;lt;- SPre + K %*% V;&#10;&#10;        PPost &amp;lt;- PPre - K %*% temp;&#10;&#10;        SPreList[,t]    &amp;lt;- SPre;&#10;        PPreList[[t]]   &amp;lt;- PPre;&#10;        KList[[t]]      &amp;lt;- K;&#10;        SPostList[,t]   &amp;lt;- SPost;&#10;        PPostList[[t]]  &amp;lt;- PPost;&#10;        VList[,t]       &amp;lt;- V;&#10;        omegaList[[t]]   &amp;lt;- omegaPre;&#10;&#10;&#10;    }&#10;&#10;    return(list(&quot;SPre&quot; = SPreList, &#10;                &quot;PPre&quot; = PPreList, &#10;                &quot;SPost&quot; = SPostList,&#10;                &quot;PPost&quot; = PPostList,&#10;                &quot;K&quot; = KList,&#10;                &quot;V&quot; = VList,&#10;                &quot;omega&quot; = omegaList,&#10;                &quot;T&quot; = T));&#10;}&#10;&#10;loss &amp;lt;- function(omegaList, VList, T)&#10;{   &#10;    # Calculate -log(ML) as loss function&#10;    ML &amp;lt;- 0;&#10;    for(t in 1:T)&#10;    {   &#10;        omega &amp;lt;- omegaList[[t]];&#10;        V &amp;lt;- VList[,t];&#10;        ML &amp;lt;- ML + log(det(omega)) + t(V)%*%ginv(omega)%*%V;&#10;&#10;    }&#10;    return(ML);&#10;}&#10;&#10;getKFML &amp;lt;- function(pSet, N, M, X, D, Z, S0, P0)&#10;{&#10;    lambda  &amp;lt;- pSet[1];&#10;    beta    &amp;lt;- pSet[2 : (length(pSet) - 2- dim(D)[1])];&#10;    gamma   &amp;lt;- pSet[(length(lambda)+length(beta)+1) : (length(pSet)-2)]&#10;    delta1  &amp;lt;- pSet[length(pSet) - 1];&#10;    delta2  &amp;lt;- pSet[length(pSet)];&#10;&#10;&#10;&#10;    parameterSet &amp;lt;- setParameterKF(lambda, beta, delta1, delta2, gamma, N, M);&#10;    A &amp;lt;- parameterSet$A;&#10;    B &amp;lt;- parameterSet$B;&#10;    H &amp;lt;- parameterSet$H;&#10;    W &amp;lt;- parameterSet$W;&#10;    Q &amp;lt;- parameterSet$Q;&#10;    R &amp;lt;- parameterSet$R;&#10;&#10;    kfModel &amp;lt;- kfilter(A, B, H, W, Q, R, X, D, Z, S0, P0);&#10;    omegaList &amp;lt;- kfModel$omega;&#10;    VList &amp;lt;- kfModel$V;&#10;    T &amp;lt;- as.integer(kfModel$T);&#10;    MLValue &amp;lt;- loss(omegaList, VList, T);   &#10;&#10;    return(MLValue);&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In my case, I have 7 state variables, 7 input variables in the state process, and 9 input variables in the measurement process. In total I have 15 parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;part of my data looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    1;0.27;-5.91;0.2;0;1.01;0;0;0;0;0;0.27;0;0;0;0.27;10922;0.0024;0;0;0,4;0;0;0;0;0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The first one is bias variable, which is always 1.&#10;Some variables have lots of zeros and non-zero values for a few time slots.&#10;It does not matter which method I used in my optim(). It is either not convergent at all after 1500 iterations or very slow. And the change of MLL(maximum likelihood) is very slow most of the time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anybody give me any suggestions --- why it is so slow. Is that due to number of parameters or ill-conditioned data? Since I set constrains on my parameters, I used L-BFGS-B method in optim(). If I do not use box-constrained methods, I will get non-finite problem within optim. Since I use optim(), it is hard for me to kown what is going on during the optim process. &lt;/p&gt;&#10;&#10;&lt;p&gt;I used the following code to call optim.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     optim(pSet, getKFML, NULL, method = &quot;L-BFGS-B&quot;, hessian = TRUE, control = list(trace=TRUE, REPORT=2, maxit = 1500), lower=ourlowerlimits, upper=ourupperlimits,  N = N, M=M, X = X, D = D, Z = Z, S0 = S0, P0 = P0);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What is the real problem causes slow convergence and how can I solve this problem?&#10;Did you have similar problems?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips are welcome! Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-20T14:30:09.860" Id="21392" LastActivityDate="2012-01-23T09:09:57.117" LastEditDate="2012-01-23T09:09:57.117" LastEditorUserId="8639" OwnerUserId="8639" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;optimization&gt;&lt;kalman-filter&gt;" Title="optim() is very slow to converge for my Kalman Filter" ViewCount="475" />
  <row Body="&lt;p&gt;This may come a wee late, but the question should be rephrased: as defined by &lt;a href=&quot;http://xianblog.wordpress.com/2011/03/30/jaynes-back-on-tracks/&quot;&gt;Jaynes&lt;/a&gt;, maximum entropy is a way to construct a prior distribution that (a) satisfies the constraints imposed by $E$ and (b) has the maximum entropy, relative to a reference measure in the continuous case: &#10;$$
  
  <row AcceptedAnswerId="21424" AnswerCount="1" Body="&lt;p&gt;I try to (re)produce the results of &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/03610910903528301#preview&quot; rel=&quot;nofollow&quot;&gt;Austin, PC. (2010).&lt;/a&gt;. The question is to obtain an approximated $\beta$, which leads to a specific risk difference(difference of proportions) $p_0 - p_1 = 0.02$, where $p_0$ &amp;amp; $p_1$ are proportions of subjects in the control and trt groups that have the response $Y = 1$, respectively. There are 4 covariates, 1 treatment indicator, and 1 outcome variable:$x_1, x_2, x_3,$ and $x_4$, which&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;N = 10000&#10;x1=rnorm(N);x2=rnorm(N);&#10;x3=rbinom(N,1,0.5);x4=rbinom(N,1,0.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where $N$ represents the number of subjects in each dataset and there are total $s = 1000$ simulated datasets.&#10;For each subject, we compute the $Pr(Y = 1)$ if a subject is in  the trt $(t=1)$ or control $(t=0)$is&lt;/p&gt;&#10;&#10;&lt;p&gt;$p_{i,1} = \frac{1}{1+e^{-(\alpha_0+\alpha_1x_{1i}+\alpha_2x_{2i}+\alpha_3x_{3i} + \alpha_4x_{4i} + \beta_{(k)})}}$, or &#10;$p_{i,0} = \frac{1}{1+e^{-(\alpha_0+\alpha_1x_{1i}+\alpha_2x_{2i}+\alpha_3x_{3i} + \alpha_4x_{4i})}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the values of $\alpha_0=-1, \alpha_1=\alpha_2=\alpha_3=\alpha_4=1$ are fixed, but $\beta_{(k)}$ is approximated by monte carlo integrations with $k$th iterative process by using bisection method :&#10;$\bar{p}_1 = \quad \idotsint_{x_1} \frac{1}{1+e^{-(\alpha_0+\alpha_1x_{1i}+\alpha_2x_{2i}+\alpha_3x_{3i} + \alpha_4x_{4i} + \beta_{(k)})}} f(x_1)...f(x_4)dx_1...dx_4$ and $\bar{p}_0 = \quad \idotsint_{x_1} \frac{1}{1+e^{-(\alpha_0+\alpha_1x_{1i}+\alpha_2x_{2i}+\alpha_3x_{3i} + \alpha_4x_{4i})}} f(x_1)...f(x_4)dx_1...dx_4$.   &lt;/p&gt;&#10;&#10;&lt;p&gt;$\bar{p}_0^{mc(k)} = \frac{1}{10000}\sum_{i=1}^{10000}p_{i,0}$ and $\bar{p}_1^{mc(k)} = \frac{1}{10000}\sum_{i=1}^{10000}p_{i,1}$, where $mc$ represents monte carlo. The empirical marginal risk difference of $s^{th}$ dataset is&#10;$\gamma_{(s)}^{(k)} = \bar{p}_0^{mc(k)} - \bar{p}_1^{mc(k)}$. We repeated over 1000 datasets, the mean empirical marginal risk difference after $k^{th}$ iterative process (bisection method) is $\gamma^{(k)} = \frac{1}{1000}\sum_{s=1}^{1000}\gamma_{(n)}^{(k)} =0.02$&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I write a R code to obtain approximated $\beta$ for $p_0 - p_1 = \gamma^{(k)}=0.02$? many thanks in advance.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-20T20:23:38.733" Id="21415" LastActivityDate="2012-01-21T12:50:14.537" LastEditDate="2012-01-21T01:34:26.753" LastEditorUserId="4559" OwnerUserId="4559" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;distributions&gt;&lt;logistic&gt;&lt;cross-validation&gt;" Title="Using monte carlo integration with bisection method to find a true value in R" ViewCount="285" />
  
  <row Body="&lt;p&gt;I have used &lt;code&gt;SAS&lt;/code&gt; for 15 years, and have started using &lt;code&gt;R&lt;/code&gt; seriously the past 6 months, with some tinkering around in it for a couple of years ahead of that. From a programming perspective, &lt;code&gt;R&lt;/code&gt;  does data manipulations directly, there is no equivalent to &lt;code&gt;DATA&lt;/code&gt; or &lt;code&gt;PROC SQL&lt;/code&gt;procedures because they're not needed (the latter being more efficient in &lt;code&gt;SAS&lt;/code&gt; when there is a lot of data manipulation to do from external data sources, e.g. administrative data). This means that, now I'm getting the hang of it, data manipulation is faster in &lt;code&gt;R&lt;/code&gt; and requires much less code.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main issue I have encountered is memory. Not all R packages allow &lt;code&gt;WEIGHT&lt;/code&gt; type specifications, so if you have &lt;code&gt;SAS&lt;/code&gt; datasets with variables used in &lt;code&gt;FREQ&lt;/code&gt; or &lt;code&gt;REPLICATE&lt;/code&gt; statements, you may have issues. I have looked at the &lt;code&gt;ff&lt;/code&gt; and &lt;code&gt;bigmemory&lt;/code&gt; packages in R but they do not appear to be compatible with all R packages, so if you have very large datasets that require analyses that are relatively uncommon, and have been aggregated, you may have issues with memory.&lt;/p&gt;&#10;&#10;&lt;p&gt;For automation, if you have &lt;code&gt;SAS macros&lt;/code&gt; then you should be able to programme the equivalent in &lt;code&gt;R&lt;/code&gt; and run as batch.&lt;/p&gt;&#10;&#10;&lt;p&gt;For coding in &lt;code&gt;R&lt;/code&gt;, I was using &lt;code&gt;Notepad++&lt;/code&gt; and setting the language to &lt;code&gt;R&lt;/code&gt;, and am now discovering the joys of &lt;code&gt;R Studio&lt;/code&gt;. Both these products are free, and do language mark up like the improved &lt;code&gt;SAS&lt;/code&gt; syntax GUI (I've only ever used the syntax screen in &lt;code&gt;SAS&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a &lt;a href=&quot;http://r4stats.com/&quot;&gt;website&lt;/a&gt;, and related book, for people swapping from &lt;code&gt;SAS&lt;/code&gt; to &lt;code&gt;R&lt;/code&gt;. I found them useful for trying to work out how to translate some &lt;code&gt;SAS&lt;/code&gt; commands into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: one thing that drove me nuts when coming to &lt;code&gt;R&lt;/code&gt; is that &lt;code&gt;R&lt;/code&gt; doesn't assume everything is a data set (&lt;code&gt;data frame&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt; parlance), because it's not a statistical package in the way that &lt;code&gt;SAS&lt;/code&gt;, &lt;code&gt;SPSS&lt;/code&gt;, &lt;code&gt;Stata&lt;/code&gt;, etc are. So, for example, it took me a while to get &lt;code&gt;if&lt;/code&gt; statements working because I kept getting the help for &lt;code&gt;if&lt;/code&gt; statements with vectors (or maybe matrices) whereas I needed an &lt;code&gt;if&lt;/code&gt; statement that worked with &lt;code&gt;data frames&lt;/code&gt;. So the help pages probably need to be read more closely than you would normally, because you'll need to check that the command you want to do will operate with the data object type you have. &lt;/p&gt;&#10;&#10;&lt;p&gt;The bit that still drives me crazy when learning a new &lt;code&gt;R&lt;/code&gt; command (e.g. analysis method in a contributed package) is that the help for commands is often not entirely self-contained. I will go to the help page to try to learn the command and the usage notes often have &lt;code&gt;...&lt;/code&gt; contained in them. Sometimes trying to work out what can or should go where the &lt;code&gt;...&lt;/code&gt; is has lead me into a recursive loop. The relative brevity of the help notes, coming from &lt;code&gt;SAS&lt;/code&gt; which provides detailed examples of syntax and worked examples with an explanation of the study in the example, was quite a large shock.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-21T00:00:08.947" Id="21430" LastActivityDate="2012-01-22T18:24:58.320" LastEditDate="2012-01-22T18:24:58.320" LastEditorUserId="8605" OwnerUserId="8605" ParentId="21398" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;Have you tried a Rasch analysis to look at the item and respondent fits? You could have one or more &quot;problem&quot; items or respondents that could be causing issues with your analysis. Respondent fits could show you where natural breaks fall between respondent clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: having a quick look at your data in &lt;code&gt;Winsteps&lt;/code&gt;, items B, F, and H seem to be very similar. Items D and E are similar. Items C and I look similar. Does this have any meaning to you? I have a person map as well as a text file - do you have somewhere I can send this to you? Caveat - this was a quick Rasch examination of the data and I haven't done any cleaning. However, convergence was quick - 6 iterations. :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-21T00:04:25.757" Id="21431" LastActivityDate="2012-01-21T05:39:35.133" LastEditDate="2012-01-21T05:39:35.133" LastEditorUserId="8605" OwnerUserId="8605" ParentId="21421" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;I got a little curious of how the &lt;code&gt;violinplot&lt;/code&gt; works when I saw this question. This also led me to the &lt;a href=&quot;http://www.jstatsoft.org/v28/c01/paper&quot; rel=&quot;nofollow&quot;&gt;beanplot&lt;/a&gt; that might be on the same theme.&lt;/p&gt;&#10;&#10;&lt;p&gt;The base data creation for all three plots:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;business &amp;lt;- runif(50, min = 65, max = 100)&#10;law &amp;lt;- runif(50, min = 60, max = 95)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;The violin plot&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(vioplot)&#10;vioplot(business, law, names=c(&quot;Business&quot;, &quot;Law&quot;), &#10;        horizontal=T, col=c(&quot;lightblue&quot;), rectCol=c('gold'))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives below, different colors aren't possible without a tweak:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Kc6Hu.png&quot; alt=&quot;Basic violin plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For getting different colors I found this slightly more advanced solution from &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2008-August/170208.html&quot; rel=&quot;nofollow&quot;&gt;Ben Bolker&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(1,1,ylim=c(0,2.5),xlim=range(c(business, law)),type=&quot;n&quot;,&#10;     xlab=&quot;&quot;,ylab=&quot;&quot;,axes=FALSE)&#10;## bottom axis, with user-specified labels&#10;axis(side=2,at=1:2,labels=c(&quot;Business&quot;, &quot;Law&quot;))&#10;axis(side=1)&#10;vioplot(business,at=1,col=&quot;blue&quot;,add=TRUE, horizontal=T)&#10;vioplot(law,at=2,col=&quot;gold&quot;,add=TRUE, horizontal=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And it looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tbF7u.png&quot; alt=&quot;Violin plot with different colors&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;The beanplot&lt;/h3&gt;&#10;&#10;&lt;p&gt;In my search I also stumbled across the beanplot from Peter Kampstra that seems interesting: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(beanplot)&#10;beanplot(business, law, horizontal=T, &#10;         names=c(&quot;Business&quot;, &quot;Law&quot;), &#10;         col=c(&quot;blue&quot;, &quot;gold&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/g4TFB.png&quot; alt=&quot;Beanplot&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-21T21:58:46.257" Id="21472" LastActivityDate="2012-01-21T21:58:46.257" OwnerUserId="5429" ParentId="21370" PostTypeId="2" Score="1" />
  
&#10;= p_X(1)p_Z(0)
  
&#10;F_A(a) = \int_{-\infty}^a f_A(x)\text{d}x
  <row Body="&lt;p&gt;Since you certainly do not know all possible values of this unknown distribution/population, you cannot calculate it's &lt;strong&gt;true&lt;/strong&gt; properties. The only thing you have is a sample, no matter how many tests you run. If you run the test an billion number of times, your estimated properties are converging to the true values, but an error will still remain (maybe at the 100th decimal position, but it is there). &lt;/p&gt;&#10;&#10;&lt;p&gt;You can claim, that your sample is identical to the population, but to make such a statement you have to know the population which makes the drawing of the sample obsolete.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-22T18:01:57.780" Id="21505" LastActivityDate="2012-01-22T18:01:57.780" OwnerUserId="264" ParentId="21484" PostTypeId="2" Score="3" />
  <row AnswerCount="3" Body="&lt;p&gt;I'd like to compare the execution speed of two different methods (say &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; in some language like Ruby). I wish I was more knowledgeable in stats to tackle this problem...&lt;/p&gt;&#10;&#10;&lt;p&gt;I am tempted to compare the means of the time taken by &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; over a number of samples, but I'm thinking it might be more appropriate to compare the minimum values measured.&lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine we can assume that, for a given platform, there is an absolute minimum of time required for the execution of &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; (let's call them &lt;code&gt;min(foo)&lt;/code&gt; and &lt;code&gt;min(bar)&lt;/code&gt;) and that what I'm really after is their ratio. I should be getting values always which are &lt;code&gt;min(foo) + ∆_i&lt;/code&gt;, where &lt;code&gt;∆_i &amp;gt;= 0&lt;/code&gt; and has some known distribution? What would this distribution be?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-22T20:59:30.047" FavoriteCount="1" Id="21512" LastActivityDate="2014-02-15T06:36:37.187" OwnerUserId="8674" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;code&gt;" Title="Distribution of execution time for benchmarking?" ViewCount="224" />
  
  <row Body="&lt;p&gt;You can compare the &lt;a href=&quot;http://en.wikipedia.org/wiki/Five-number_summary&quot; rel=&quot;nofollow&quot;&gt;five number summaries&lt;/a&gt; of each sample.  Also a boxplot can be informative.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-22T22:26:01.093" Id="21522" LastActivityDate="2012-01-22T22:26:01.093" OwnerUserId="8669" ParentId="21512" PostTypeId="2" Score="0" />
  
&#10;$$&#10;which is not maximised for $q=p_2$. (There is no maximum.) Same thing if you consider two Poisson distributions, $\mathcal{P}(\lambda_1)$ and $\mathcal{P}(\lambda_2)$:&#10;$$
  <row Body="&lt;p&gt;Of course, the Anscombe 4 datasets are very good for teaching - they look very different, yet have identical simple statistical properties.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also suggest KDD Cup datasets &lt;a href=&quot;http://www.kdd.org/kddcup/&quot; rel=&quot;nofollow&quot;&gt;http://www.kdd.org/kddcup/&lt;/a&gt;  because they have been well studied and there are many solutions, so students can compare their results and see how they rank.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In my data mining course I provided a Microarray dataset competition which can be used by professors  &lt;a href=&quot;http://www.kdnuggets.com/data_mining_course/&quot; rel=&quot;nofollow&quot;&gt;http://www.kdnuggets.com/data_mining_course/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-01-23T13:02:54.457" CreationDate="2012-01-23T13:02:54.457" Id="21552" LastActivityDate="2013-11-09T22:24:23.247" LastEditDate="2013-11-09T22:24:23.247" LastEditorUserId="930" OwnerUserId="8693" ParentId="21486" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;My question is about the DLM package and the dlmMLE.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lest's say that I have a bivariate model of this kind:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y(t)= Fb(t)+e(t)$$&#10;$$b(t) = u + Gb(t) +w(t)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$b(t)=(b_1(t),b_2(t)),\quad u = (u_1,u_2)$$&#10;Rewriting the model in matrix notation, the state equation becomes:&#10;$$d(t) = H d(t) +w(t),$$&#10;where $d(t)= (b(t),1,1)$  and $H$ is a matrix 4x4 given by  the merging the 2 matrices 2x4 $[G,\text{diag}(u)]$ and $ [0,\text{diag}(1,p)]$, and the variance of the innovations $w$ is a matrix of zeros except for the upper right 2x2. Morover I assume that $C_0=\text{diag}(1e3,1e3,0,0)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since it doesn't work well I was wondering if it is the right way to write it? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks I would appreciate any suggestions.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-23T13:40:25.087" Id="21554" LastActivityDate="2012-02-23T14:40:53.283" LastEditDate="2012-01-24T14:22:37.370" LastEditorUserId="88" OwnerUserId="8695" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;maximum-likelihood&gt;&lt;dlm&gt;" Title="DLM package, state equation, maximum likelihood with constant terms" ViewCount="249" />
  
  <row Body="&lt;p&gt;You can use several R packages for Kalman filtering. You have to code, though, any&#10;constrains you may have in matrices A, B. H, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Among the outputs of the Kalman filter, most packages will give you the likelihood, for given values of the parameters. You then have to invoke &lt;code&gt;optim()&lt;/code&gt; or an alternative function to maximize that likelihood. Or, if you use package &lt;code&gt;dlm&lt;/code&gt;, you can simply invoke &lt;code&gt;dlmMLE&lt;/code&gt; (which in fact is a wrap function for &lt;code&gt;optim()&lt;/code&gt;). You have one example in &lt;a href=&quot;http://stackoverflow.com/questions/8852038/var1-with-dlm-package-maximum-likelihood-estimation&quot;&gt;this answer&lt;/a&gt; to another query.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-23T14:56:35.483" Id="21559" LastActivityDate="2012-01-23T14:56:35.483" OwnerUserId="892" ParentId="21557" PostTypeId="2" Score="2" />
&#10;= \int_{-\infty}^{\infty} f_{A,B}(z-b,b) \mathrm db.$$&#10;When $A$ and $B$ are independent, the joint density function factors into the&#10;product of the marginal density functions: $f_{A,B}(a,z-a)=f_{A}(a)f_{B}(z-a)$ &#10;and we get the more familiar&#10;convolution formula for independent random variables.  A similar result&#10;applies for discrete random variables as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Things are more complicated if $A$ and $B$ are not jointly continuous, or&#10;if one random variable is continuous and the other is discrete.  However,&#10;in all cases, one can always find the cumulative probability distribution&#10;function $F_{A+B}(z)$ of $A+B$ as the total probability mass in the region of &#10;the plane specified as $\{(a,b) \colon a+b \leq z\}$ and compute the probability&#10;density function, or the probability mass function, or whatever, from the&#10;distribution function.  Indeed the above formula is obtained by writing&#10;$F_{A+B}(z)$ as a double integral of the joint density function over the&#10;specified region and then &quot;differentiating under the integral&#10;sign.''&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-23T15:08:08.837" Id="21561" LastActivityDate="2012-01-23T17:17:36.287" LastEditDate="2012-01-23T17:17:36.287" LastEditorUserId="6633" OwnerUserId="6633" ParentId="21549" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;A man and a woman asked, what is the probability that he/she went out into the street, meet a dinosaur.&lt;/p&gt;&#10;&#10;&lt;p&gt;Man begins to count and, finally, gives his version: &quot;Taking into account all the possible factors - zero point three trillion.&lt;/p&gt;&#10;&#10;&lt;p&gt;She also meets once: &quot;Fifty-fifty. &quot;Why?&quot; - Say it. &quot;It's very simple: either meeting or not meeting.&quot;&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-01-23T16:38:09.653" CreationDate="2012-01-23T16:38:09.653" Id="21564" LastActivityDate="2012-01-23T16:38:09.653" OwnerUserId="4763" ParentId="1337" PostTypeId="2" Score="-3" />
  <row AcceptedAnswerId="21566" AnswerCount="1" Body="&lt;p&gt;I see a similar constrained regression here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/12484/constrained-linear-regression-through-a-specified-point&quot;&gt;Constrained linear regression through a specified point&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;but my requirement is slightly different. I need the coefficients to add up to 1. Specifically I am regressing the returns of 1 foreign exchange series against 3 other foreign exchange series, so that investors may replace their exposure to that series with a combination of exposure to the other 3, but their cash outlay must not change, and preferably (but this is not mandatory), the coefficients should be positive. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried to search for constrained regression in R and Google but with little luck. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-23T16:42:28.997" FavoriteCount="7" Id="21565" LastActivityDate="2012-01-24T14:10:35.613" LastEditDate="2012-01-24T14:10:35.613" LastEditorUserId="88" OwnerUserId="4705" PostTypeId="1" Score="11" Tags="&lt;r&gt;&lt;regression&gt;" Title="How do I fit a constrained regression in R so that coefficients total = 1?" ViewCount="6331" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm using Softmax regression for a multi-class classification problem. I don't have equal prior probabilities for each of the classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know from Logistic Regression (softmax regression with 2 classes) that the prior probabilities of the classes is implicitly added to the bias ($\log(p_0/p_1)$). &lt;/p&gt;&#10;&#10;&lt;p&gt;Usually what I do is to manually remove this term from the bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, what is the corresponding term in softmax regression bias?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-23T18:12:24.527" FavoriteCount="3" Id="21568" LastActivityDate="2012-01-24T14:09:58.563" LastEditDate="2012-01-24T14:09:58.563" LastEditorUserId="88" OwnerUserId="5946" PostTypeId="1" Score="4" Tags="&lt;logistic&gt;&lt;prior&gt;&lt;unbalanced-classes&gt;" Title="Softmax regression bias and prior probabilities for unequal classes" ViewCount="271" />
  <row Body="&lt;p&gt;Based on the information that you provide, I think you're after a consensus-type measure, where the number of experts agreeing at each star level is weighted by how much you believe their ratings are accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, for example, say your 6 raters have the following &quot;expert&quot; weights:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Raters A and B each have a weight of 1 &lt;/li&gt;&#10;&lt;li&gt;Rater C has a weight of 0.8&lt;/li&gt;&#10;&lt;li&gt;Rater D has a weight of 0.5 &lt;/li&gt;&#10;&lt;li&gt;Raters E and F each have a weight of 0.25&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now consider one bottle of wine. You receive the following ratings:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Rater     Rating&#10;A         5&#10;B         4&#10;C         4&#10;D         5&#10;E         4&#10;F         4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Rating 5 is based basically on 1.5 &quot;reliable&quot; Raters &lt;code&gt;(1(A) + 0.5(D))&lt;/code&gt;&#10;Rating 4 is based basically on 2.3 &quot;reliable&quot; Raters &lt;code&gt;(1(B)+0.8(C)+0.25(E)+0.25(F))&lt;/code&gt;&#10;so the consensus is 4 stars.&lt;/p&gt;&#10;&#10;&lt;p&gt;We don't adjust the rating based on rater reliability, we adjust the number of &quot;experts&quot; giving each rating based on rater reliability. Then we pick the rating based on the most number of experts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Update based on comments and the answer from @gung: if the experts are varying wildly across wines, and therefore you get an aggregate score that you don't think is a good summary of the wine, the problem is likely to lie with your expert panel rather than the method used for aggregation. If the expert ratings are not reliable or are invalid for reasons other than unreliability, it won't matter what aggregation method you use because the GIGO principle will apply.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-23T23:22:00.077" Id="21582" LastActivityDate="2012-01-24T23:30:48.563" LastEditDate="2012-01-24T23:30:48.563" LastEditorUserId="8605" OwnerUserId="8605" ParentId="21578" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If the coin is unbiased then the probability of 'heads' is $\frac{1}{2}$. Therefore, the number of heads thrown in 900 tries, $X$, has a ${\rm Binomial}(900,\frac{1}{2})$ distribution under the null hypothesis of a fair coin. So, the $p$-value - the probability of seeing a result this extreme or more extreme given that the coin is far, is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P( X \geq 490 ) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;If you seek the 2-sided $p$-value, that would be &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ 1 - P(410 &amp;lt; X &amp;lt; 490 ) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I'll leave it to you to describe why that is the case. &lt;/p&gt;&#10;&#10;&lt;p&gt;We know that the mass function for $ Y \sim {\rm Binomial}(n,p)$, is  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(Y = y) = \binom{n}{y} p^y (1-p)^{n-y} $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I'll leave it to you to calculate $p$-value you seek.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The sample size here is sufficiently large that you could use the normal approximation to the binomial distribution. I've detailed above how to calculate the exact $p$-value. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-01-23T23:55:24.820" Id="21584" LastActivityDate="2012-01-24T00:00:34.143" LastEditDate="2012-01-24T00:00:34.143" LastEditorUserId="4856" OwnerUserId="4856" ParentId="21581" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Here's a user defined function I keep in my .First library that puts Christopher Aden's response to a menu style interface.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;critical.t &amp;lt;- function(){&#10;    cat(&quot;\n&quot;,&quot;\bEnter Alpha Level&quot;,&quot;\n&quot;)&#10;        alpha&amp;lt;-scan(n=1,what = double(0),quiet=T)&#10;    cat(&quot;\n&quot;,&quot;\b1 Tailed or 2 Tailed:\nEnter either 1 or 2&quot;,&quot;\n&quot;)&#10;        tt&amp;lt;-scan(n=1,what = double(0),quiet=T)&#10;    cat(&quot;\n&quot;,&quot;\bEnter Number of Observations&quot;,&quot;\n&quot;)&#10;        n&amp;lt;-scan(n=1,what = double(0),quiet=T)&#10;    cat(&quot;\n\nCritical Value =&quot;,qt(1-(alpha/tt), n-2), &quot;\n&quot;)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-01-24T07:06:19.257" Id="21598" LastActivityDate="2012-01-24T07:06:19.257" OwnerUserId="7482" ParentId="21596" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Some idea might be to generate something like the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Madelon&quot; rel=&quot;nofollow&quot;&gt;Madelon set&lt;/a&gt; from NIPS 2003 challenge; it fits your requirements pretty well. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can generate a set like this starting with &lt;code&gt;mlbench.xor&lt;/code&gt; (or &lt;code&gt;mlbench.hypercube&lt;/code&gt;, might be easier) form mlbench package, then you combine classes it generated into two groups to make the dichotomous response and add new attributes to increase dimensionality -- some being random linear combinations of the original ones, some being just random noise.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-24T14:17:51.467" Id="21622" LastActivityDate="2012-01-24T14:17:51.467" OwnerUserId="88" ParentId="21556" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a real-time domain where I need to assign an action to N actors involving moving one of O objects to one of L locations. At each time step, I'm given a reward R, indicating the overall success of all actors.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 10 actors, 50 unique objects, and 1000 locations, so for &lt;em&gt;each actor&lt;/em&gt; I have to select from 500000 possible actions. Additionally, there are 50 environmental factors I may take into account, such as how close each object is to a wall, or how close it is to an actor. This results in 25000000 potential actions &lt;em&gt;per actor&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nearly all reinforcement learning algorithms don't seem to be suitable for this domain.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, they nearly all involve evaluating the expected utility of each action in a given state. My state space is huge, so it would take forever to converge a policy using something as primitive as Q-learning, even if I used function approximation. Even if I could, it would take too long to find the best action out of a million actions in each time step.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, most algorithms assume a single reward per actor, whereas the reward I'm given might be polluted by the mistakes of one or more actors.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I approach this problem? I've found no code for domains like this, and the few academic papers I've found on multi-actor reinforcement learning algorithms don't provide nearly enough detail to reproduce the proposed algorithm.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-01-24T15:02:29.503" FavoriteCount="2" Id="21623" LastActivityDate="2014-08-23T19:50:02.770" OwnerUserId="741" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;reinforcement-learning&gt;" Title="Reinforcement learning of a policy for multiple actors in large state spaces" ViewCount="163" />
  <row Body="&lt;p&gt;After much researching I the following reference was the most useful to me when trying to interpret the findings of a vecm:&lt;/p&gt;&#10;&#10;&lt;p&gt;Helmut Lütkepohl, Markus Krätzig&lt;/p&gt;&#10;&#10;&lt;p&gt;Structural Vector Autoregressive Modeling and Impulse Responses pp. 159-196. In: Applied time-series economics.&lt;/p&gt;&#10;&#10;&lt;p&gt;A link to the chapter is given below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ebooks.cambridge.org/chapter.jsf?bid=CBO9780511606885&amp;amp;cid=CBO9780511606885A036&quot; rel=&quot;nofollow&quot;&gt;http://ebooks.cambridge.org/chapter.jsf?bid=CBO9780511606885&amp;amp;cid=CBO9780511606885A036&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-24T15:46:06.003" Id="21624" LastActivityDate="2012-01-24T15:46:06.003" OwnerUserId="3136" ParentId="17263" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I saw the Johnson family of distributions in context of reliability and demand modeling for supply chains, but I am not sure if they are bringing real benefit to pay for their relatively more complex format. What are their major features and when one should use them? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-24T16:53:34.280" Id="21628" LastActivityDate="2012-01-24T17:32:21.107" LastEditDate="2012-01-24T17:32:21.107" LastEditorUserId="449" OwnerUserId="5657" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;probability&gt;&lt;reliability&gt;&lt;project-management&gt;" Title="What is the Johnson family of distributions good for? What are their main features and applications?" ViewCount="98" />
  <row Body="&lt;p&gt;I assume you mean 'multivariable' regression, not 'multivariate'.  'Multivariate' refers to having multiple dependent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is not considered to be acceptable statistical practice to take a continuous predictor and to chop it up into intervals.  This will result in residual confounding and will make interactions misleadingly significant as some interactions can just reflect lack of fit (here, underfitting) of some of the main effects.  There is a lot of unexplained variation within the outer quintiles.  Plus, it is actually impossible to precisely interpret the &quot;quintile effects.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;For comparisons of interest, it is easiest to envision them as differences in predicted values.  Here is an example using the R &lt;code&gt;rms&lt;/code&gt; package.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(rms)&#10;f &amp;lt;- ols(y ~ x1 + rcs(x2,3)*treat)  # or lrm, cph, psm, Rq, Gls, Glm, ...&#10;# This model allows nonlinearity in x2 and interaction between x2 and treat.&#10;# x2 is modeled as two separate restricted cubic spline functions with 3&#10;# knots or join points in common (one function for the reference treatment&#10;# and one function for the difference in curves between the 2 treatments)&#10;contrast(f, list(treat='B', x2=c(.2, .4)),&#10;            list(treat='A', x2=c(.2, .4)))&#10;# Provides a comparison of treatments at 2 values of x2&#10;anova(f) # provides 2 d.f. interaction test and test of whether treatment&#10;# is effective at ANY value of x2 (combined treat main effect + treat x x2&#10;# interaction - this has 3 d.f. here)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-01-24T17:10:44.107" Id="21629" LastActivityDate="2012-01-24T17:10:44.107" OwnerUserId="4253" ParentId="3653" PostTypeId="2" Score="3" />
  
  
&#10;\overline{Y}_{male(adj)} = \overline{Y}_{male} - b_1 (\overline{X}_{male} - \overline{X}_{T})
  <row Body="&lt;p&gt;I have derived an answer to my own question, which is expressed upon the data I gave in my question, in &lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0ArXZTADO2Z54dE1DZ2VzZjFBbWUyRkFMYW82eFpYWVE&quot; rel=&quot;nofollow&quot;&gt;this Google docs spreadsheet&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was inspired by Zach's answer, which suggested calculating a predicted value, and then using RMSE. I realised that by taking the square root of each of the &lt;code&gt;y&lt;/code&gt; data I would I obtain a linear series. I can then do linear regression to get get a predicted value which can then be squared.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect I might be doing something terrible here, insomuch as I'm not sure what my error value of &lt;code&gt;936.22&lt;/code&gt; for &lt;code&gt;y2&lt;/code&gt; actually corresponds to.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-25T00:10:07.397" Id="21654" LastActivityDate="2012-01-25T00:53:02.303" LastEditDate="2012-01-25T00:53:02.303" LastEditorUserId="8725" OwnerUserId="8725" ParentId="21649" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;We have the standard linear multivariate regression model&#10;$y=\beta_0+\beta_1x_1+\beta_2x_2+u$ under the Gauss-Markov assumptions.&#10;Suppose we estimate $\gamma_0, \gamma_1, \gamma_2$ from $x_2=\gamma_0+\gamma_1x_1+\gamma_2y+v$ and get $\hat{\gamma_0}, \hat{\gamma_1}, \hat{\gamma_2}$. Is $\frac{1}{\hat{\gamma_2}}$ an unbiased estimate of $\beta_2$?&#10;I don't know how to answer the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all it seems that to run the regression of $x_2$ on $y$ and $x_1$ we need $\beta_2$ be nonzero. And is the expression &#10;$x_2=\frac{-\beta_0}{\beta_2}+\frac{-\beta_1}{\beta_2}x_1+\frac{1}{\beta_2}y+\frac{-1}{\beta_2}u$ somehow related to the regression model $x_2=\gamma_0+\gamma_1x_1+\gamma_2y+v$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Another question is whether the Gauss-Markov assumptions hold in the case of this new regression, seems like some of them can fail, for instance $E(v|x_1,y)=0$ may not be true(is it important here?).&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe stupid questions, but I am new to this topic.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-01-25T05:55:08.900" FavoriteCount="1" Id="21665" LastActivityDate="2012-01-25T20:35:48.800" LastEditDate="2012-01-25T18:30:44.713" LastEditorUserId="7972" OwnerUserId="8733" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;econometrics&gt;" Title="Multivariate linear regression model" ViewCount="253" />
  <row Body="&lt;p&gt;To supplement these fine responses, I would mention use of gradient boosted trees (e.g. the &lt;a href=&quot;http://cran.r-project.org/web/packages/gbm/gbm.pdf&quot;&gt;GBM Package in R&lt;/a&gt;). In R, I prefer this to random forests because missing values are allowed as compared to randomForest where imputation is required. Variable importance and partial plots are available (as in randomForest) to aid in feature selection and nonlinear transformation exploration in your logit model. Further, variable interaction is addressed with Friedman’s H-statistic (&lt;code&gt;interact.gbm&lt;/code&gt;) with reference given as &lt;code&gt;J.H. Friedman and B.E. Popescu (2005). “Predictive Learning via Rule Ensembles.” Section 8.1&lt;/code&gt;. A commercial version called TreeNet is available from Salford Systems and this video presentation speaks to their take on variable interaction estimation &lt;a href=&quot;http://fora.tv/2009/06/10/Dan_Steinberg_on_Interaction_Detection_with_TreeNet&quot;&gt;Video&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-25T14:24:53.883" Id="21686" LastActivityDate="2012-01-25T14:24:53.883" OwnerUserId="2040" ParentId="21152" PostTypeId="2" Score="10" />
&#10;h_1C_1PT_1&amp;amp;=&amp;amp;x_1\\
&#10;h_iC_iPT_i&amp;amp;=&amp;amp;x_i
  
&#10;\frac{\sum_{i,j,i&amp;lt;j} ratio_{i,j}}{n(n-1)/2},
  <row AcceptedAnswerId="21734" AnswerCount="2" Body="&lt;p&gt;I'm reviewing a conference paper (not a stats conference). The authors have done some funny things with their methodology. The experiment setup has 4 treatments, each applied to the same 30 subjects. This is repeated in 10 different circumstances.&lt;/p&gt;&#10;&#10;&lt;p&gt;ANOVA is used to determine whether the choice of treatment has any effect, separately for each of the 10 different circumstances. They find that only one circumstance had a significant difference, at p=0.03.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two questions regarding their methodology:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;It seems to me that the circumstances should be used as a separate factor in ANOVA, rather than doing 10 separate ANOVAS, and that if one did the test 10 times, one should multiply the p-values by 10 (i.e. a Bonferroni correction). Does this make sense?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In spite of their earlier results, the authors then did pairwise t-tests between all the different treatment groups in all 10 circumstances. They report a statistically significant difference in means between many of the groups in circumstances which ANOVA suggested had no differences. I suspect this means they are not doing a Bonferroni correction for the p-values in their pairwise tests. Is this correct? Is there another way that ANOVA could suggest no significance, but a a significant difference could exist between the groups?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-01-25T22:30:44.070" Id="21724" LastActivityDate="2012-01-26T01:14:34.550" OwnerUserId="6446" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;experiment-design&gt;&lt;t-test&gt;" Title="Normalization in pairwise hypothesis testing" ViewCount="228" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Yes, they should do multiple testing correction for trying to show the differences in 10 different conditions. This is assuming that they tried 10 conditions, with the starting hypothesis that some of them may show a difference. If the starting (a priori) hypothesis of their experiments was that this one particular condition will show a difference, and the rest won't, then they don't need to do multiple testing corrections.&lt;/li&gt;&#10;&lt;li&gt;Yes, multiple pairwise t-tests between different treatments are too liberal when the correct test is ANOVA. For 4 treatments, the Bonferroni correction would be dividing by $4*3/2=6$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2012-01-25T23:03:25.980" Id="21726" LastActivityDate="2012-01-25T23:03:25.980" OwnerUserId="2728" ParentId="21724" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Looking back, it was a very simple mistake. Interpolating between models is as simple as sampling between the outputs of the two models. So for example between $A$ and $B$, for every sample choose the output of $B$ with probability $p_B = (f-f_A)/(f_B-f_A)$ and the output of $A$ with probability $1-p_B$. Doing this, I get the expected output:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/uWa4a.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The (simplified) code for this is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 1e5&#10;&#10;labels &amp;lt;- c(rep(0,n/2),rep(1,n/2))&#10;fA &amp;lt;- .2; tA &amp;lt;- .6; fB &amp;lt;- .7; tB &amp;lt;- .95&#10;A &amp;lt;- c(rbinom(n/2,1,fA),rbinom(n/2,1,tA))&#10;B &amp;lt;- c(rbinom(n/2,1,fB),rbinom(n/2,1,tB))&#10;&#10;library(ROCR)&#10;&#10;pred.a &amp;lt;- prediction(A,labels)&#10;pred.b &amp;lt;- prediction(B,labels)&#10;&#10;pdf(&quot;ROC_Interpolated.pdf&quot;)&#10;&#10;# plot a and b&#10;plot(performance(pred.a,&quot;tpr&quot;,&quot;fpr&quot;),col=&quot;blue&quot;,lty=0,type='o',pch=2)&#10;plot(performance(pred.b,&quot;tpr&quot;,&quot;fpr&quot;),col=&quot;blue&quot;,lty=0,type='o',pch=3,add=T)&#10;&#10;i &amp;lt;- 0&#10;C = list()&#10;pred.c = list()&#10;p = list()&#10;f = list()&#10;for (fgoal in ((1:29)/30)) {&#10;    i &amp;lt;- i+1&#10;    f[[i]] = fgoal&#10;    if (fgoal&amp;lt;=fA) {&#10;        p[[i]] = fgoal/fA&#10;        C[[i]] &amp;lt;- sapply(1:n,function(x) {if (runif(1)&amp;lt;=p[[i]]) A[x] else 0})&#10;    } else if (fgoal&amp;lt;=fB) {&#10;        p[[i]] = (fgoal-fA)/(fB-fA)&#10;        C[[i]] &amp;lt;-  sapply(1:n,function(x) {if (runif(1)&amp;lt;=p[[i]]) B[x] else A[x]})&#10;    } else {&#10;        p[[i]] = (fgoal-fB)/(1-fB)&#10;        C[[i]] &amp;lt;-  sapply(1:n,function(x) {if (runif(1)&amp;gt;p[[i]]) 1 else B[x]})&#10;    }&#10;    pred.c[[i]] = prediction(C[[i]],labels)&#10;    plot(performance(pred.c[[i]],&quot;tpr&quot;,&quot;fpr&quot;),col=&quot;red&quot;,lty=0,type='o',pch=5,add=T)&#10;}&#10;&#10;lines(c(0,fA,fB,1),c(0,tA,tB,1),lty=2,col=&quot;black&quot;)&#10;legend(.4,0.4,legend=c(&quot;A&quot;,&quot;B&quot;,&quot;interpolated by sampling&quot;,&quot;theoretical&quot;),lty=c(0,0,0,2),pch=c(2,3,5,NaN),col=c(&quot;blue&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;black&quot;))&#10;&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-01-25T23:40:08.443" Id="21728" LastActivityDate="2012-01-25T23:40:08.443" OwnerUserId="2728" ParentId="21725" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm working with multiple time series signals ${\{X_i\}}$ and one method to remove suspected noise is with PCA.  But unlike most methods which remove the components with least variance, we remove much more substantial components (often the second or third).  This is often done because empirically it's been shown to improve results in certain analyses (like simple pairwise correlation) but has been detrimental to other analysis like clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I was curious of general metrics to compare before and after dimensionality reduction.  Would looking at things like entropy be useful?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-26T02:39:39.777" Id="21742" LastActivityDate="2012-01-30T15:09:46.800" OwnerUserId="4127" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;information-theory&gt;" Title="Measuring what's 'lost' in PCA dimensionality reduction?" ViewCount="272" />
  <row Body="&lt;p&gt;If they are very strongly correlated, the result will likely be better when you remove them first (just as with any known trend in the data that you do not want to discover, actually).&lt;/p&gt;&#10;&#10;&lt;p&gt;Other than that, the classic EM Gaussian Mixtures clustering can deal with &lt;em&gt;linear&lt;/em&gt; correlations quite well, as the covariance matrix will adjust the distribution to that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-26T07:13:48.380" Id="21746" LastActivityDate="2012-01-26T07:13:48.380" OwnerUserId="7828" ParentId="21543" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Can anyone please tell me what the &lt;code&gt;alternative&lt;/code&gt; argument is in &lt;code&gt;fisher.test()&lt;/code&gt; in R? What do greater and less imply?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,&#10;            control = list(), or = 1, alternative = &quot;two.sided&quot;,&#10;            conf.int = TRUE, conf.level = 0.95,&#10;            simulate.p.value = FALSE, B = 2000)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I tried the help section and all it says is that alternative hypothesis must be one of &lt;code&gt;&quot;two.sided&quot;, &quot;greater&quot;, or &quot;less&quot;&lt;/code&gt;. When should I use each one? I'm looking at over-representation in my set against a control set.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-26T10:49:58.950" Id="21752" LastActivityDate="2013-07-23T06:42:18.647" LastEditDate="2013-07-23T06:42:18.647" LastEditorUserId="27276" OwnerUserId="8748" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;fishersexact&gt;" Title="Alternative argument in Fisher's exact test in R" ViewCount="577" />
  
  <row Body="&lt;p&gt;This is the alternative hypothesis, commonly denoted by $H_1$. &#10;For example if you type or=1 and alternative=less, the $H_1$ hypothesis is $\psi&amp;lt;1$ where $\psi$ is the odds ratio (or).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-26T11:02:51.953" Id="21753" LastActivityDate="2012-01-26T11:02:51.953" OwnerUserId="8402" ParentId="21752" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;An S-shaped normal probability plot doesn't suggest curvature in the data.  The normal probability plot is a test of distribution not pattern to the residuals.  If you have an S-Shaped normal probability then your residuals are not normal.  Depending on the direction of the &quot;S&quot;, this means you either have a distribution for the error term that is thicker in the tails than the normal distribution or vice versa thinner in the tails than the normal distribution.  Logs or power transformation will not likely help.  Might mean trying some robust regression methods which are unfortunately not in Minitab.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-26T18:22:01.440" Id="21780" LastActivityDate="2012-01-26T18:22:01.440" OwnerUserId="7893" ParentId="21768" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;There is a technique called weighted SVM (see ref below), that appears to be supported by LibSVM (which I've never actually used). Weighted SVM solves the problem of having two classes with unequal training data. In this case, classification is biased towards the class with more observations. To compensate, W-SVM sets the penalty parameter C in proportion to the size of the class.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same idea can be applied to confidence information by giving each observation its own C; though I'm not sure if LibSVM supports this. In this sense, you give a larger penalty to observations in which you have a lot of confidence, and a small penalty to observations with which you have little confidence. The end result is that the hyperplane is determined by weighting each observation by its confidence interval, as you desire.&lt;/p&gt;&#10;&#10;&lt;p&gt;Huang, &amp;amp; Du (2005). Weighted support vector machine for classification with uneven training class sized. Proc. of the 4th Int. Conf. on Machine Learning and Cybernetics, 4365-4369. Retrieved from &lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=1527706&quot; rel=&quot;nofollow&quot;&gt;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=1527706&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-27T00:15:48.620" Id="21794" LastActivityDate="2012-01-27T00:15:48.620" OwnerUserId="1977" ParentId="21659" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;PCA probably isn't what you're after here. You probably want Factor Analysis, which is typically used to investigate the latent factors supposedly underlying a measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it seems from your question that you want to reduce the number of items on your scale (which is a laudable goal too often forgotten in social science), so I would suggest that you investigate &lt;a href=&quot;http://en.wikipedia.org/wiki/Item_response_theory&quot; rel=&quot;nofollow&quot;&gt;item response theory&lt;/a&gt;. Item response theory is a method which allows you to decompose the scores on your measure into two parts; person abilities and item difficulties. &lt;/p&gt;&#10;&#10;&lt;p&gt;It was originally developed for answers with a true/false answer, but has been generalised to personality/achievement tests since then. I personally use R for my statistical analyses, and there are three packages that would probably help with this kind of modelling; namely &lt;code&gt;mokken&lt;/code&gt;, &lt;code&gt;eRm&lt;/code&gt; and &lt;code&gt;ltm&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;A suggested workflow might be the following: &lt;/p&gt;&#10;&#10;&lt;p&gt;use the &lt;code&gt;mokken&lt;/code&gt; package to test the assumptions required for IRT. &#10;Following this, attempt to fit a rasch model (the simplest form of IRT) using &lt;code&gt;eRm&lt;/code&gt;. &lt;code&gt;eRm&lt;/code&gt; has an extremely good    vingette which you can get access to by calling &lt;code&gt;vignette(eRm)&lt;/code&gt;. If these models do not fit, then you can use the &lt;code&gt;ltm&lt;/code&gt; package, which can fit two and three parameter models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your problem is actually quite interesting, in that you are aiming to discern which items people find most important. This can be done using an IRT approach by looking at which items are perceived as the least difficult, these will be the items that most people consider important. &lt;/p&gt;&#10;&#10;&lt;p&gt;One major advantage of IRT is that you can use it to select questions which will assess importance more quickly, by only keeping those which are most informative. HTH. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-27T14:07:37.597" Id="21810" LastActivityDate="2012-01-27T14:07:37.597" OwnerUserId="656" ParentId="21808" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;You can stack the data so that you have $x_1 ... x_n$ followed by $x_2 ... x_{n+1}$ in one column, then the next column would have $1 ... n$ repeated twice, then a third column would have $0$'s coresponding to the 1st group and $1$'s for the second group.  Now do the regression including an interaction between the $1 ... n$ and the $0/1$ group variable.  The slope for the interaction represents the difference between the 2 slopes you are interested in, testing the interaction will test if the slopes are different.  However, the standard regression assumptions may not hold here (your data certainly is not independent), so you should do some type of simulation under the null hypothesis to determine the critical region to use to determine significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some example R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y1 &amp;lt;- anscombe$y1[order(anscombe$x1)]&#10;y2 &amp;lt;- anscombe$y2[order(anscombe$x2)]&#10;&#10;&#10;df1 &amp;lt;- data.frame( y=c(y1[-11], y1[-1]), x=rep(1:10, 2), g=rep(0:1, each=10))&#10;df2 &amp;lt;- data.frame( y=c(y2[-11], y2[-1]), x=rep(1:10, 2), g=rep(0:1, each=10))&#10;&#10;fit1 &amp;lt;- lm( y ~ x*g, data=df1 )&#10;fit2 &amp;lt;- lm( y ~ x*g, data=df2 )&#10;&#10;tmpfun &amp;lt;- function(n, beta, sigma) {&#10;    x &amp;lt;- 1:n&#10;    y &amp;lt;- beta*x + rnorm(n,0,sigma)&#10;    df &amp;lt;- data.frame( y=c(y[-n],y[-1]), x=rep(seq(to=n-1), 2), &#10;        g=rep(0:1, each=n-1) )&#10;    fit &amp;lt;- lm( y~x*g, data=df )&#10;    coef(fit)[4]&#10;}&#10;&#10;tmpfit &amp;lt;- lm(y1 ~ seq_along(y1))&#10;out1 &amp;lt;- replicate(1000, tmpfun(11, coef(tmpfit)[2], summary(tmpfit)$sigma) )&#10;hist(out1)&#10;abline( v=quantile(out1, c(0.025, 0.975)), col='red')&#10;abline( v=coef(fit1)[4], col='blue' )&#10;&#10;tmpfit &amp;lt;- lm(y2 ~ seq_along(y1))&#10;out2 &amp;lt;- replicate(1000, tmpfun(11, coef(tmpfit)[2], summary(tmpfit)$sigma) )&#10;hist(out2)&#10;abline( v=quantile(out2, c(0.025, 0.975)), col='red')&#10;abline( v=coef(fit2)[4], col='blue' )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you tell us more about what you are trying to accomplish we may be able to suggest better methods.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-27T16:41:05.817" Id="21819" LastActivityDate="2012-01-27T16:59:52.620" LastEditDate="2012-01-27T16:59:52.620" LastEditorUserId="4505" OwnerUserId="4505" ParentId="21815" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;@Ondrej has given some good advice, so I will focus on your question around how software treats imported data. With character data, &quot;Category 1&quot; and &quot;Category 2&quot;, the software automatically treats these as groups or factors because mathematical operations cannot be conducted on these pieces of data. This means that you will be prevented from entering anything from those categories (alternatively, you'll get an error if you try if you're using syntax or a command line instead of a menu-driven system) into an analysis that requires numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;For data like your &quot;Number 1&quot; and &quot;Number 2&quot;, the software reads these as numeric. If you have any groups/factors that contain purely numeric data, you will need to instruct your software that these are groups/factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes dates can be imported badly into statistical software. Once you import your data, you should see that the data type in your statistical software is showing some form of &quot;date&quot; type for &quot;Date&quot;. If you see the data type as anything other than date, you have an issue. Even if it is showing as date, check the import of some rows where you have dates like days like the 13th or 25th of the month - depending on how the software is set up, sometimes the American/British date formatting causes screwy data from import, because of the reversal of the day/month.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-27T18:55:30.443" Id="21823" LastActivityDate="2012-01-27T18:55:30.443" OwnerUserId="8605" ParentId="21809" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I had this example in my machine learning lecture.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $X_2,\ldots,X_n$ be identically distributed (but not independent) copies of $X_1$ drawn from $\mathcal N(0,1)$. Then $X_n$ converges to $Y = -X_1$ as $n \rightarrow \infty$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There isn't any explanation for why this sequence of random variables converges to $Y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Convergence in the sense&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lim_{n \rightarrow \infty} F_{X_n}(t)$ where $F_{X_n}$ is the CDF of $X_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help me out?&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2012-01-28T02:36:32.637" Id="21847" LastActivityDate="2012-03-09T13:42:00.090" LastEditDate="2012-02-03T12:58:52.130" LastEditorUserId="2970" OwnerDisplayName="user8795" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;random-variable&gt;&lt;convergence&gt;" Title="Convergence of identically distributed normal random variables" ViewCount="787" />
&#10;\end{align*}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-28T03:33:25.930" Id="21848" LastActivityDate="2012-01-28T03:33:25.930" OwnerUserId="6633" ParentId="21822" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I'm going to run through the whole Naive Bayes process from scratch, since it's not totally clear to me where you're getting hung up.&lt;/p&gt;&#10;&#10;&lt;p&gt;We want to find the probability that a new example belongs to each class: $P(class|feature_1, feature_2,..., feature_n$). We then compute that probability for each class, and pick the most likely class. The problem is that we usually don't have those probabilities. However, Bayes' Theorem lets us rewrite that equation in a more tractable form.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bayes' Thereom is simply$$P(A|B)=\frac{P(B|A) \cdot P(A)}{P(B)}$$ or in terms of our problem:&#10;$$P(class|features)=\frac{P(features|class) \cdot P(class)}{P(features)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We can simplify this by removing $P(features)$. We can do this because we're going to rank $P(class|features)$ for each value of $class$; $P(features)$ will be the same every time--it doesn't depend on $class$. This leaves us with&#10;$$ P(class|features) \propto P(features|class) \cdot P(class)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The prior probabilities, $P(class)$, can be calculated as you described in your question. &lt;/p&gt;&#10;&#10;&lt;p&gt;That leaves $P(features|class)$. We want to eliminate the massive, and probably very sparse, joint probability $P(feature_1, feature_2, ..., feature_n|class)$. If each feature is independent , then $$P(feature_1, feature_2, ..., feature_n|class) = \prod_i{P(feature_i|class})$$ Even if they're not actually independent, we can assume they are (that's the &quot;naive&quot; part of naive Bayes). I personally think it's easier to think this through for discrete (i.e., categorical) variables, so let's use a slightly different version of your example. Here, I've divided each feature dimension into two categorical variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/eqeH2.png&quot; alt=&quot;Discrete Example Data&quot;&gt;.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Example: Training the classifer&lt;/h2&gt;&#10;&#10;&lt;p&gt;To train the classifer, we count up various subsets of points and use them to compute the prior and conditional probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;The priors are trivial: There are sixty total points, forty are green while twenty are red. Thus $$P(class=green)=\frac{40}{60} = 1/3 \text{    and   } P(class=red)=\frac{20}{60}=2/3$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, we have to compute the conditional probabilities of each feature-value given a class. Here, there are two features: $feature_1$ and $feature_2$, each of which takes one of two values (A or B for one, X or Y for the other). We therefore need to know the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$P(feature_1=A|class=red)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_1=B|class=red)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_1=A|class=green)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_1=B|class=green)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=X|class=red)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=Y|class=red)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=X|class=green)$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=Y|class=green)$&lt;/li&gt;&#10;&lt;li&gt;(in case it's not obvious, this is all possible pairs of feature-value and class)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;These are easy to compute by counting and dividing too. For example, for $P(feature_1=A|class=red)$, we look only at the red points and count how many of them are in the 'A' region for $feature_1$. There are twenty red points, all of which are in the 'A' region, so $P(feature_1=A|class=red)=20/20=1$. None of the red points are in the B region, so $P(feature_1|class=red)=0/20=0$. Next, we do the same, but consider only the green points. This gives us $P(feature_1=A|class=green)=5/40=1/8$ and $P(feature_1=B|class=green)=35/40=7/8$. We repeat that process for $feature_2$, to round out the probability table. Assuming I've counted correctly, we get &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$P(feature_1=A|class=red)=1$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_1=B|class=red)=0$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_1=A|class=green)=1/8$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_1=B|class=green)=7/8$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=X|class=red)=3/10$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=Y|class=red)=7/10$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=X|class=green)=8/10$&lt;/li&gt;&#10;&lt;li&gt;$P(feature_2=Y|class=green)=2/10$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Those ten probabilities (the two priors plus the eight conditionals) are our model&lt;/p&gt;&#10;&#10;&lt;h2&gt;Classifying a New Example&lt;/h2&gt;&#10;&#10;&lt;p&gt;Let's classify the white point from your example. It's in the &quot;A&quot; region for $feature_1$ and the &quot;Y&quot; region for $feature_2$. We want to find the probability that it's in each class. Let's start with red. Using the formula above, we know that:&#10;$$P(class=red|example) \propto P(class=red)  \cdot P(feature_1=A|class=red) \cdot P(feature_2=Y|class=red)$$&#10;Subbing in the probabilities from the table, we get &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(class=red|example) \propto \frac{1}{3} \cdot 1 \cdot \frac{7}{10} = \frac{7}{30}$$&#10;We then do the same for green:&#10;$$P(class=green|example) \propto P(class=green) \cdot P(feature_1=A|class=green) \cdot P(feature_2=Y|class=green) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Subbing in those values gets us 0 ($2/3 \cdot 0 \cdot 2/10$). Finally, we look to see which class gave us the highest probability. In this case, it's clearly the red class, so that's where we assign the point.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Notes&lt;/h2&gt;&#10;&#10;&lt;p&gt;In your original example, the features are continuous. In that case, you need to find some way of assigning P(feature=value|class) for each class. You might consider fitting then to a known probability distribution (e.g., a Gaussian). During training, you would find the mean and variance for each class along each feature dimension. To classify a point, you'd find $P(feature=value|class)$ by plugging in the appropriate mean and variance for each class. Other distributions might be more appropriate, depending on the particulars of your data, but a Gaussian would be a decent starting point. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not too familiar with the DARPA data set, but you'd do essentially the same thing. You'll probably end up computing something like P(attack=TRUE|service=finger), P(attack=false|service=finger), P(attack=TRUE|service=ftp), etc. and then combine them in the same way as the example. As a side note, part of the trick here is to come up with good features. Source IP , for example, is probably going to be hopelessly sparse--you'll probably only have one or two examples for a given IP. You might do much better if you geolocated the IP and use &quot;Source_in_same_building_as_dest (true/false)&quot; or something as a feature instead. &lt;/p&gt;&#10;&#10;&lt;p&gt;I hope that helps more. If anything needs clarification, I'd be happy to try again!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-01-28T03:44:43.823" Id="21849" LastActivityDate="2012-01-28T20:48:38.523" LastEditDate="2012-01-28T20:48:38.523" LastEditorUserId="7250" OwnerUserId="7250" ParentId="21822" PostTypeId="2" Score="19" />
  <row AcceptedAnswerId="21855" AnswerCount="3" Body="&lt;p&gt;The margin of error is driven by the size of the sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a consultant's report (which is confidential at this stage), they collected responses from 10 store managers (out of a total of 200 store managers i.e. target population) and went on to make statements such as &quot;only 20% of the respondents were happy with the sales performance of their stores&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The margin of error in this case is around 32%. Assuming a confidence level of 95%, the true response is between -12 and 52%.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How does one interpret this result (especially the negative part)?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;It is wise to use margin of error for small sample sizes (e.g. sample of 15 out of a target population of 60)?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-01-28T06:36:07.067" FavoriteCount="2" Id="21854" LastActivityDate="2014-11-14T02:15:01.287" LastEditDate="2012-01-31T07:34:51.290" LastEditorUserId="7972" OwnerUserId="6096" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;sampling&gt;" Title="Interpreting a negative confidence limit for a proportion" ViewCount="4005" />
  
  
  
  
  <row Body="Multivariate refers to analyses where there is &gt;1 response / dependent variable of interest in the statistical analysis. This can be contrasted w/ *multivariable* analyses, which typically implies &gt;1 predictor / independent variable." CommentCount="0" CreationDate="2012-01-28T19:57:45.783" Id="21883" LastActivityDate="2013-11-11T22:52:44.317" LastEditDate="2013-11-11T22:52:44.317" LastEditorUserId="7290" OwnerUserId="8605" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Yes, if you're referring to the &lt;em&gt;Z&lt;/em&gt; test of the difference between independent proportions.  Btw in this situation you can also use a Chi-Square test and you'll get  the same &lt;em&gt;p&lt;/em&gt;-value but you won't be able to generate a confidence interval for the population difference.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-01-29T01:47:45.553" Id="21892" LastActivityDate="2012-01-29T01:47:45.553" OwnerUserId="2669" ParentId="21889" PostTypeId="2" Score="2" />
  
&#10;$$&#10;where $c_1+c_2=1$. This seems to improve the fit significantly. The question remains, how to fit the mixed distribution. From what I've read, it seems reasonable to consider minimum-distance estimators, like Anderson-Darling to achieve a maximum-goodness of fit. I did not find any implemented algorithms for the minimum-distance procedure. So I wanted to use some numerical optimization algorithm that allows constraints (which I need) and implement it myself.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this approach make sense? Recommendations for the optimization method? I do not have an analytic Jacobian, of course. Is there any tested, implemented method? Should I use a different approach? MLE is “involved” as there is no distribution function of the stable distribution&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; Computational effort is not relevant. However, as this is only a minor issue of the thesis, I'd rather spend not too much time on it. Parameter estimation of heavy-tailed distributions is a vast field, combined with a lack of experience this can end in a disaster. So I'd be happy if someone points me into the right direction.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-29T06:58:22.017" FavoriteCount="1" Id="21900" LastActivityDate="2012-01-29T18:47:01.303" LastEditDate="2012-01-29T18:47:01.303" LastEditorUserId="7224" OwnerUserId="8809" PostTypeId="1" Score="6" Tags="&lt;estimation&gt;&lt;maximum-likelihood&gt;&lt;optimization&gt;&lt;mixture&gt;&lt;anderson-darling&gt;" Title="Minimum-Distance estimation of mixed/mixture distributions" ViewCount="345" />
  
  
  <row Body="&lt;p&gt;If you believe the scores are well defined by the poisson distribution and that they are independent, you could quickly simulate the probability of certain spreads.  Here's some simple r-code that runs 10,000 simulations and finds how often team b wins by 5.5+ points:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;mean( (rpois(n=10000,lambda=3.0755) + 5.5) &amp;lt; rpois(n=10000,lambda=3.3894) )&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I get about a 2% chance.  I know nothing of this domain (sports stats) so you might want to closely visit that independence assumption.  You could go deeper and look at alternative discrete distributions like the negative binomial (assuming heterogeneous player contributions?)  Or go baysian and treat your lambda as a random variable to capture more of your uncertainty...&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-01-29T19:05:25.523" Id="21917" LastActivityDate="2012-01-29T19:05:25.523" OwnerUserId="8120" ParentId="21914" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Yes, this is a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, it is inappropriate even though at least it is transparent about what it is doing (it gets points for transparency, but still is not satisfactory).&lt;/p&gt;&#10;&#10;&lt;p&gt;I doubt there is an &quot;easy way&quot; to fix this.  I don't know much about the approaches taken to meta analysis but if there is specific meta-analysis software and research like this is produced using it and gets published, this may well be the common approach.  Either of your proposed responses loses some granularity of information from each study (ie the opposite problem of what the publishers have done).&lt;/p&gt;&#10;&#10;&lt;p&gt;The obvious solution is a mixed-effects (ie multilevel) model with study as a random factor.  I would suggest using a specialist statistical package for this if meta-analysis software cannot do it.  You could still use the meta-analysis software for data storage and processing, and just export data to R, Stata or SAS for the analysis.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-29T19:08:16.743" Id="21918" LastActivityDate="2012-01-29T19:08:16.743" OwnerUserId="7972" ParentId="16547" PostTypeId="2" Score="2" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I can't seem to understand how PCA works. My (lack of) math knowledge don't help me either. I have read that the new set of variables has to be a linear combination of the old set.&lt;/p&gt;&#10;&#10;&lt;p&gt;What does that mean exactly? That there should be a way to multiply numbers a, b, c, ... with dimensions/variables x, y, z... and get the old bigger(!) set? &lt;/p&gt;&#10;&#10;&lt;p&gt;Can you answer my example: if I have 8 variables/dimension can they be reduced to 3? Or a vector of 3 components (sorry for my lack of proper terminolgy, English is not my native language) is not a linear combination of 8 and thus no? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-01-30T05:07:43.170" FavoriteCount="1" Id="21933" LastActivityDate="2013-11-18T17:59:16.413" LastEditDate="2012-01-30T09:51:59.747" LastEditorUserId="88" OwnerUserId="8828" PostTypeId="1" Score="6" Tags="&lt;pca&gt;" Title="How can 8 dimensions be reduced to 3?" ViewCount="459" />
  
  <row AcceptedAnswerId="21947" AnswerCount="1" Body="&lt;p&gt;I am trying to derive a formula for my collaborative algorithm problem to calculate popularity rating of an item.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am considering three factors to calculate rating for an item based on three different ratings $a$, $b$ and $c$, each with weight $w_1$, $w_2$ and $w_3$. The cumulative rating is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$w_1  a + w_2  b + w_3  c$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In this equation, I want to give $b$ less weight than $a$, i.e. $w_1 &amp;gt; w_2 &amp;gt; w_3$.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the problem is that how should I decide/calculate the weights such that they maintain this relative ordering when normalized (I am not looking for linear normalization) i.e.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$w_1 a &amp;gt; w_2 b &amp;gt; w_3 c$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to calculate $w_1$, $w_2$, $w_3$ dynamically and what normalization process I should use or is recommended?&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be really helpful to get comments on the approach as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-30T04:50:03.263" FavoriteCount="0" Id="21946" LastActivityDate="2012-01-30T11:58:42.823" LastEditDate="2012-01-30T11:58:42.823" LastEditorUserId="88" OwnerDisplayName="krammer" OwnerUserId="8832" PostTypeId="1" Score="1" Tags="&lt;algorithms&gt;" Title="Relatively normalizing values for collaborative filtering" ViewCount="109" />
  
  <row Body="&lt;p&gt;You can use the Adjusted Rand Index or the Adjusted Mutual Information to measure the similarity (agreement) of the overall results of two clustering algorithms on an overlapping dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both scores are adjusted for chance which means that 2 random clusterings will likely have an ARI or AMI close to 0.0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore you can use those measure for model selection (e.g. finding the number of k in k-means) by running the clustering algorithm twice on 2 overlapping samples of the datasets and measuring the agreement on the overlap. The assumption is that a high agreement on the overlap means a higher stability of the algorithm and hence a better value for k (it captures better the real structure of the dataset).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ee.unsw.edu.au/~nguyenv/K_detection_15.pdf&quot; rel=&quot;nofollow&quot;&gt;A Novel Approach for Automatic Number of Clusters Detection in&#10;Microarray Data based on Consensus Clustering&lt;/a&gt; by Nguyen and Epps is probably the best reference for this method and it is further applied to microarray data.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-01-30T17:32:18.480" Id="21958" LastActivityDate="2012-01-30T17:32:18.480" OwnerUserId="2150" ParentId="21955" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The &quot;margin of error&quot; of 38% is computed using a formula for 0/1 results that are obtained independently and randomly from a large population.  &lt;strong&gt;None of these apply.&lt;/strong&gt; The analogous formula for the present case (with 1..7 results obtained from a small population assuming random non-response) would involve a sample standard deviation with a &lt;a href=&quot;http://en.wikipedia.org/wiki/Margin_of_error#Effect_of_population_size&quot; rel=&quot;nofollow&quot;&gt;finite population correction&lt;/a&gt;.  It would not be very helpful in sorting out the confusion.  Instead, let's explore the concept starting from its foundations.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The purpose of a margin of error is to indicate how much the population might differ from the sample, assuming that the data are a &lt;em&gt;random&lt;/em&gt; sample of the population.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem we have is we don't know what the four missing respondents would have said.  We have to cover all possible cases that are consistent with the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;An interesting possibility&lt;/strong&gt; is that seven of the 11 CEOs would give a reply of 7, two of them would reply with 5, and the other 2 with 1: this is (hypothetically) the population.  How consistent are the data with this scenario?  In this case, the chance of observing six 7's and one 5 at random is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\binom{7}{6}\binom{2}{1}\binom{2}{0}}{\binom{11}{7}} = \frac{7 \times 2 \times 1}{330} \approx 4.24\%.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine that observing seven 7's ($1/330$ chance) or five 7's and two 5's ($21/330$ chance) would also be considered a &quot;very satisfied&quot; overall rating.  The total chance of observing this rating in the sample would therefore equal $(14 + 1 + 22)/330$ = $11.2\%$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;This is about the worst situation that is consistent with the recorded observations&lt;/strong&gt; in the sense that the conclusion made from our sample (&quot;very satisfied&quot;) has at least a 5% chance of arising from a random selection.  A reasonable way to express the &quot;margin of error,&quot; then, is to note that as many as two of the CEOs, but not more than that, might have been extremely dissatisfied.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;It's a good idea to go further with the analysis&lt;/strong&gt;, because we have no evidence to support the assumption that the seven respondents actually are a representative sample.  In fact, most likely they are not.  Perhaps, for example, four of the CEOs &lt;em&gt;would&lt;/em&gt; reply with 1's but did not care to because they did not want to reveal their dissatisfaction: the missingness is not at random.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A frank and thoughtful exposition of the results would bring up both these points&lt;/strong&gt; when assessing the credibility of the results and their applicability to the entire population.  Its conclusions would necessarily be tentative.  They might be stated thus:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In this self-selected sample of 7 of the 11 CEOs, six reported being &quot;very satisfied&quot; and one &quot;somewhat satisfied&quot; with their bonus.  Nothing is known about what the remaining four CEOs feel.  We can conclude that as a group, the majority of the 11 CEOs were highly satisfied, but there may be anywhere from none through a significant minority (four) who were dissatisfied.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;One advantage of this plain exposition&lt;/strong&gt; is that it makes no unnecessary technical demands on the reader (or the writer) by invoking a &quot;margin of error&quot; formula which would need to be explained and interpreted and might well be incorrect by not accounting for the small population size or possibility of non-random response.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The techniques illustrated here apply to similar problems with small population sizes, non-random responses, or qualitative ordinal scales.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-30T21:35:53.633" Id="21979" LastActivityDate="2012-01-30T21:35:53.633" OwnerUserId="919" ParentId="21927" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;If I can add an example of when $R^2$ is dangerous.  Many years ago I was working on some biometric data and being young and foolish I was delighted when I found some statistically significant $R^2$ values for my fancy regressions which I had constructed using stepwise functions.  It was only afterwards looking back after my presentation to a large international audience did I realize that given the massive variance of the data - combined with the possible poor representation of the sample with respect to the population, an $R^2$ of 0.02 was utterly meaningless even if it was &quot;statistically significant&quot;...&lt;/p&gt;&#10;&#10;&lt;p&gt;Those working with statistics need to understand the data!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-01-31T09:55:17.470" Id="22011" LastActivityDate="2012-01-31T09:55:17.470" OwnerUserId="6579" ParentId="13314" PostTypeId="2" Score="15" />
  <row AnswerCount="2" Body="&lt;p&gt;We want to add a forecasting tool to our website -- it would draw off a historical database and users can choose the model (Box Jenkins, Simple Linear Regression, etc.), confidence interval and play what-if.   Is anyone aware of such package we can plug into our website?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-01-31T23:23:20.197" FavoriteCount="0" Id="22037" LastActivityDate="2012-03-03T05:45:07.287" OwnerUserId="8875" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;time-series&gt;" Title="Web based forecasting tools" ViewCount="159" />
  <row AcceptedAnswerId="22112" AnswerCount="3" Body="&lt;p&gt;There have been several good questions and sets of answers on introductory books or approaches to learning R eg &lt;a href=&quot;http://stats.stackexchange.com/questions/15670/best-way-to-get-started-with-and-learn-r&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/138/resources-for-learning-r&quot;&gt;here&lt;/a&gt;.  But I have a slightly different problem - the best way to run an hour long session (or several such sessions) in a computer lab that will get people started in R, familiar with its basic approach etc.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My current plan would be to effectively work through the introductory chapter/s of something like Verzani's SimpleR and then introduce a familiar dataset, but is there any other approach that people have found useful?  For example, is it good to introduce real data straight away, or address issues in more abstract way?  Should I exhaustively go through how to use the square brackets, or excite people with examples of lattice graphics?&lt;/p&gt;&#10;&#10;&lt;p&gt;My target audience are familiar with statistics (although not experts) and competent SPSS users; not familiar with programming languages beyond the sort of macro and scripting you'd get in SPSS and similar things.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips or references to lessons plans would be appreciated. However, I don't want to duplicate the many good lists of on-line material introducing R - strictly references to the face to face instructional question. &lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2012-02-01T01:10:21.503" CreationDate="2012-01-31T23:42:48.097" FavoriteCount="9" Id="22040" LastActivityDate="2012-02-01T21:31:31.750" LastEditDate="2012-02-01T10:18:33.540" LastEditorUserId="88" OwnerUserId="7972" PostTypeId="1" Score="12" Tags="&lt;r&gt;&lt;teaching&gt;" Title="What's a good approach to teaching R in a computer lab?" ViewCount="371" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to understand the difference between logistic regression probabilities and linear regression prediction intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, let's say we have a database of student test scores in the range of 1 to 100 and some predictors. The goal of this study is to build a model to predict if other students will reach &lt;em&gt;at least a score of 60 with an 80% confidence&lt;/em&gt;. To simplify, we are assuming that all the linear modeling requirements in the data are verified.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first method would be to run a linear regression on the observed data, then calculate the 80% prediction intervals and finally determine whether a student will reach a score of 60 or higher based on the lower end of the prediction interval. The other approach is to categorize our data and run a logistic regression on each student score &amp;lt; 60 (0) or &gt;= 60 (1) observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any benefit in using the logistic regression approach in this case? Or does linear regression will result in the same level of accuracy when using prediction intervals?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-02-01T01:14:40.783" Id="22045" LastActivityDate="2012-02-01T01:28:29.573" OwnerUserId="7795" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;linear-model&gt;" Title="Difference between linear regression prediction intervals and logistic regression targets" ViewCount="244" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;If there is a species that is hyperabundant in a dataset and makes up 284/1638 individuals yet originated from just 1 of 10 traps. Is a quassi-poisson GLM sufficient providing residual deviance is under 1? or does this species need to be relativized or otherwise before proceeding to model selection? I really dont know how to combat this species....&lt;/p&gt;&#10;&#10;&lt;p&gt;Platypezid&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-02-01T12:49:15.690" Id="22065" LastActivityDate="2012-02-01T12:49:15.690" OwnerUserId="5397" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;overdispersion&gt;&lt;type-i-errors&gt;" Title="Relativization and tackling an overdispersed dataset" ViewCount="63" />
  
