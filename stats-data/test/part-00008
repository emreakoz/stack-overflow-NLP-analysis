  
  <row Body="&lt;p&gt;The modified Mathisen test (Hettmansperger &amp;amp; McKean (1998), &lt;em&gt;Robust Nonparametric Statistical Methods&lt;/em&gt;, Ch. 2.11) tests for equal medians under the null hypothesis while making no assumptions about the distributions of either sample other than that they're continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;But do bear in mind what @Michael said&amp;mdash;a difference in medians is usually interesting only when you think there's a location parameter shifting. More often in your situation people use the Mann&amp;ndash;Whitney&amp;ndash;Wilcoxon test to assess stochastic dominance: that's whether one group has a lower or higher cumulative distribution function of the response for all its values &amp;amp; would correspond to the idea of seeing whether the toxin has a good, a bad, or not much effect. The assumption here is that the population cdfs don't cross, &amp;amp; is usually checked by examining a plot of the sample cdfs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-15T10:11:40.500" Id="61817" LastActivityDate="2013-07-25T16:00:21.250" LastEditDate="2013-07-25T16:00:21.250" LastEditorUserId="17230" OwnerUserId="17230" ParentId="61786" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="61825" AnswerCount="1" Body="&lt;p&gt;I have two random variables following normal distributions,&#10;$X\sim N(\mu_{x},\sigma_{x})$&#10;and&#10;$Y\sim N(\mu_{y},\sigma_{y})$&lt;/p&gt;&#10;&#10;&lt;p&gt;I know the covariance is $\operatorname{Cov}(X,Y) = c$.&lt;/p&gt;&#10;&#10;&lt;p&gt;And I believe (correct me if I'm wrong) that I can estimate the expected value of $Y$ given $X=\tau$ as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(Y|X=\tau) = \rho \frac{\sigma_{y}}{\sigma_{x}}(\tau -\mu_{x})+\mu_{y}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where&#10;$\rho = c/(\sigma_{x}\sigma_{y})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe this is a decent estimate. But now I want some way of estimating our &quot;certainty&quot; that our calculated value is correct. Obviously for $|\rho| \sim 1$ we can be very confident. For $\rho \sim 0.5$ a little bit confident. And for $\rho \sim 0$ I suppose our confidence isn't changed at all. i.e. since no correlation exists we have not changed any ideas about how confident we are about our expected $Y$ value.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I express this numerically? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-15T12:50:49.143" Id="61821" LastActivityDate="2013-06-15T19:34:57.187" LastEditDate="2013-06-15T19:34:57.187" LastEditorUserId="17230" OwnerUserId="26851" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;conditional-expectation&gt;&lt;confidence&gt;" Title="Bivariate conditional probability confidence" ViewCount="69" />
  
  
  <row Body="&lt;p&gt;If you believe that your Likert item DV is tapping a continuous underlying quantity, eg. a strength of preference for or against something (which I presume you do), then your regression model should assume an ordinal DV because only then will you by trying to estimate the quantities you want: the effects of changes in IVs on the actual satisfaction level underlying the DV.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If for some reason you doubt that the DV is really measuring something continuous then you might fit a multinomial regression to see whether particular combinations of IVs predict that people choose particular categories of the DV.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You might also fit a multinomial on a collapsed set of DV categories if respondents don't use the whole range or if you don't have enough data to get an ordinal model going.  An ordinal model has to estimate the boundaries between DV categories as well as the slopes for the IVs, so it can demand more from your data. &lt;/p&gt;&#10;&#10;&lt;p&gt;The type and distribution of the IVs makes no difference since no regression models make any distributional assumptions about them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-15T20:34:51.133" Id="61840" LastActivityDate="2013-06-15T20:34:51.133" OwnerUserId="1739" ParentId="61834" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have little knowledge of statistics and was wondering what tests to use for an evidence-based project (not research, so I am not using a control group) that I am conducting. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;What statistical test would I use for pre and post knowledge tests after a student educational intervention? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Also, are numeric test results considered interval data and would I being looking for the difference in means between the pre and post knowledge tests? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I also have pre- and post-confidence scores that have one negative value:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0: do not agree &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and two positive values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1: somewhat agree and &#10;2: strongly agree; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What statistical test would I use for the confidence survey? Would this be ordinal data and again would I be looking for the mean difference between the pre- and post-confidence surveys?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-16T17:56:24.803" Id="61878" LastActivityDate="2013-06-16T18:40:23.553" LastEditDate="2013-06-16T18:40:23.553" LastEditorUserId="6029" OwnerUserId="26951" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;" Title="Analyzing pre and post-intervention measurements without control group" ViewCount="292" />
  <row Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Autocorrelation&quot; rel=&quot;nofollow&quot;&gt;Autocorrelation&lt;/a&gt; is the correlation of a series of data with itself at some lag.  For example, if a given value were higher than average, it might be more likely than not that the next value would also be higher than average; if so, there is a positive autocorrelation at lag 1.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The examination of autocorrelation is central to time-series analysis, where it can inform sources of non-stationarity.  Another context where autocorrelations are important is the analysis of residuals from a linear regression model to assess the possibility that a variable has a misspecified functional form.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-16T20:13:54.233" Id="61880" LastActivityDate="2013-06-16T22:08:45.220" LastEditDate="2013-06-16T22:08:45.220" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;[NB: the title of this post used to be &quot;Please help me decipher Fisher on ANOVA&quot;.  I've edited the question to make it more focused, and modified the title accordingly.]&lt;/p&gt;&#10;&#10;&lt;p&gt;Since anything written by R. A. Fisher seems to be fertile ground for misunderstanding, I will quote him at length.&lt;/p&gt;&#10;&#10;&lt;p&gt;At the beginning of ch VII (&quot;Intraclass correlations and the analysis of variance&quot;), on pp. 213-214 of his &lt;em&gt;Statistical methods for research workers&lt;/em&gt; (1973, 14th ed. rev. &amp;amp; enl.), Fisher writes:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;If we have measurements of $n^\prime$ pairs of brothers, we may ascertain the correlation between brothers in two slightly different ways.  In the first place we may divide the brothers into two classes, as for instance elder brother and younger brother, and find the correlation between these two classes exactly as we do with parent and child.  If we proceed in this manner we shall find the mean of the measurements of the elder brothers, and separately that of the younger brothers.  Equally the standard deviations about the mean are found separately for the two classes.  The correlation so obtained, being that between two classes of measurements, is termed for distinctness an &lt;strong&gt;interclass&lt;/strong&gt; correlation.  Such a procedure would be imperative if the quantities to be correlated were, for example, the &lt;em&gt;ages&lt;/em&gt;, or some characteristic sensibly dependent upon age, at a fixed date.  On the other hand, we may not know, in each case, which measurement belongs to the elder and which to the younger brother, or, such a distinction may be quite irrelevant to our purpose&amp;nbsp;; in these cases it is usual to use a common mean derived from all the measurements, and a common standard deviation about that mean.  If $x_1,\; {x^\prime}_{\!1}; \;\; x_2,\;{x^\prime}_{\!2}; \;\; \cdots ;\;\;x_{n^\prime},\;{x^\prime}_{\!n^\prime}$ are the pairs of measurements given, we calculate $$\begin{array}&#10;&amp;amp; &amp;amp; \\&#10;\overline{x} &amp;amp; = &amp;amp; \frac{1}{2n^\prime} \mathrm{S}(x + x^\prime), \\&#10;s^2 &amp;amp; = &amp;amp; \frac{1}{2n}\{\mathrm{S}(x - \overline{x})^2 + \mathrm{S}(x^\prime - \overline{x})^2 \}, \\&#10;r &amp;amp; = &amp;amp; \frac{1}{ns^2} \mathrm{S}\{(x-\overline{x})(x^\prime - \overline{x})\}.&#10;\end{array}$$&#10;  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;When this is done, $r$ is distinguished as an &lt;strong&gt;intraclass&lt;/strong&gt; correlation, since we have treated all the brothers as belonging to the same class, and having the same mean and standard deviation.  (&amp;hellip;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(I've made the point to reproduce Fisher's words, punctuation, typography, and notation &lt;em&gt;exactly&lt;/em&gt;.  You may take this to mean that a &quot;sic&quot; applies to the entire passage quoted above, including all the mathematical expressions.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If I were to &lt;em&gt;attempt&lt;/em&gt; to interpret these expressions using more modern notation (and displaying a modicum of elementary courtesy towards the reader), I'd write&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}&#10;&amp;amp; &amp;amp; \\&#10;\overline{x} &amp;amp; = &amp;amp; \frac{1}{2n^\prime} \sum_{i=1}^{n^\prime}(x_i + {x^\prime}_{\!i}), \\&#10;s^2 &amp;amp; = &amp;amp; \frac{1}{2n}&#10;\left\{ \sum_{i=1}^{n^\prime}(x_i - \overline{x})^2 + \sum_{i=1}^{n^\prime}({x^\prime}_{\!i} - \overline{x})^2 \right\}, \\&#10;r &amp;amp; = &amp;amp; \frac{1}{ns^2} \sum_{i=1}^{n^\prime}(x_i-\overline{x})({x^\prime}_{\!i} - \overline{x}).&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if we &lt;em&gt;assume&lt;/em&gt; that $n = n^\prime - 1$, then the last two definitions become&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}&#10;&amp;amp; &amp;amp; \\&#10;s^2 &amp;amp; = &amp;amp; \frac{1}{2n^\prime - 2}&#10;\left\{ \sum_{i=1}^{n^\prime}(x_i - \overline{x})^2 + \sum_{i=1}^{n^\prime}({x^\prime}_{\!i} - \overline{x})^2 \right\}, \\&#10;r &amp;amp; = &amp;amp; \frac{1}{(n^\prime - 1)s^2} \sum_{i=1}^{n^\prime}(x_i-\overline{x})({x^\prime}_{\!i} - \overline{x}).&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In this form, the definition of $r$ matches that of a sample correlation coefficient for the case in which both variables have the same estimated mean $\overline{x}$ and estimated variance $s^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I'm still thrown off by the expression for this estimated variance $s^2$.  According to Fisher's description $s^2$ should be the square of &quot;a &lt;em&gt;common&lt;/em&gt; standard deviation about [a &lt;em&gt;common&lt;/em&gt;] mean&quot;.  Therefore, I expected that the denominator in its pre-factor would be $2n^\prime - 1$.  IOW, I expected the whole expression would be&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;s^2 = \frac{1}{2n^\prime - 1}&#10;\left\{ \sum_{i=1}^{n^\prime}(x_i - \overline{x})^2 + \sum_{i=1}^{n^\prime}({x^\prime}_{\!i} - \overline{x})^2 \right\}\,.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But if $n = n^\prime - 1$, then Fisher uses a denominator of $2n = 2n^\prime - 2$.  Does anyone know why?&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the quote above comes from a 14th revised edition, it seems to me unlikely that this is merely a typo.  Furthermore, in the following page (p. 215) he extends the discussion to the the case of &quot;trios&quot; of brothers, and there he gives the expression:&#10;$$&#10;s^2 = \frac{1}{3n}\{\mathrm{S}(x - \overline{x})^2 + \mathrm{S}(x^\prime - \overline{x})^2  + \mathrm{S}(x^{\prime\prime} - \overline{x})^2\}\,,&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;or, in my notation, and still assuming that $n = n^\prime - 1$,&#10;$$&#10;s^2 = \frac{1}{3n^\prime - 3}&#10;\left\{ \sum_{i=1}^{n^\prime}(x_i - \overline{x})^2 + \sum_{i=1}^{n^\prime}({x^\prime}_{\!i} - \overline{x})^2 + \sum_{i=1}^{n^\prime}({x^{\prime\prime}}_{\!i} - \overline{x})^2 \right\}\,.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, if we're dealing with typos here, they are &lt;em&gt;matched typos&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, for a sufficiently large $n^\prime$, the difference between a denominator of $2n^\prime - 1$ and one of $2n^\prime - 2$ is negligible, but my understanding is that Fisher is explicitly assuming that we are working with values of $n^\prime$ that are in general too small to justify this approximation.  In fact, if the approximation $n^\prime \approx n^\prime - 1$ were valid, Fisher's notational distinction between $n^\prime$ and $n = n^\prime - 1$ would be hard to understand. &lt;/p&gt;&#10;&#10;&#10;" CommentCount="6" CreationDate="2013-06-17T01:23:16.673" Id="61896" LastActivityDate="2013-06-18T11:05:48.227" LastEditDate="2013-06-18T11:05:48.227" LastEditorUserId="4769" OwnerUserId="4769" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;anova&gt;&lt;fisher&gt;" Title="Is Fisher overestimating the variance here?" ViewCount="108" />
  <row AnswerCount="2" Body="&lt;p&gt;Consider the following (in R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;plot(stack.loss~Air.Flow,data=stackloss)&#10;regression &amp;lt;- rlm(stack.loss~Air.Flow,data=stackloss)&#10;abline(regression)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For each of the points I would like to to test against the null hypothesis that it really is located on the line (rather than where it really is) - in essence quantifying the degree to which these are outliers with respect to the regression model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Possibly using hopeless search term combinations, I have been unable to identify appropriate methodology.  Would it be valid to use for each residual a one-sample t-test (ar equivalent) against the residual population to establish such a measure?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Further reading seems to indicate that &quot;Tolerance Interval&quot; is what I might be looking for - the &quot;tolerance&quot; R package provides calculations of that.&#10;I am however puzzled by the apparent contradictory nature of such intervals for regressions as defined by different publications. In the &lt;a href=&quot;http://www.jstatsoft.org/v36/i05/&quot; rel=&quot;nofollow&quot;&gt;R package case&lt;/a&gt; the tolerance bands seem to track the regression line parallely (see Fig. 12 and below example), while alternative sources such as &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0169743907000457&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; operate with bands that are reminiscent of confidence intervals in shape (see Fig. 4). The latter seems more intuitive, but other's opinions would be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each point in a sample I am actually looking for &quot;which x/95% tolerance band is running through this point&quot; ...&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's how usage of &quot;tolerance&quot; might look like (adding to above code) for 95%/95% 2-sided nonparametric regression tolerance bounds:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(tolerance)&#10;tol.bounds &amp;lt;- npregtol.int(x=stackloss$Air.Flow,        &#10;    y=stackloss$stack.loss, y.hat=regression$fit, side=2, alpha=0.05, P=0.95, &#10;    method=&quot;WILKS&quot;)&#10;lines(tol.bounds$x,y=tol.bounds$&quot;2-sided.lower&quot;,col=&quot;red&quot;)&#10;lines(tol.bounds$x,y=tol.bounds$&quot;2-sided.upper&quot;,col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2013-06-17T11:46:54.153" FavoriteCount="1" Id="61914" LastActivityDate="2013-06-24T11:56:27.737" LastEditDate="2013-06-24T11:56:27.737" LastEditorUserId="26968" OwnerUserId="26968" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;probability&gt;" Title="Regression and point-specific p-values (using R for explanation)" ViewCount="142" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a system to which I make certain changes. I want to compare the impact of those changes on the system performance. For this, I record the performance as a function of a set of inputs multiple times, before and after the changes have been made. How do I quantify the impact of these changes given that random environmental effects apply on the system at all times?&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe this is a generic problem in statistics, and have read ANOVA, Student's t-test and K-S test. However, what considerations have to be made while deciding on a method? There is no assumption of normal distribution, so it seems that K-S test is the best solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you point me to a resource that discusses this class of problems? I am pretty confused by the fragmented explanations I have found online so far.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-17T13:43:08.567" Id="61932" LastActivityDate="2013-08-23T11:05:59.913" LastEditDate="2013-08-23T11:05:59.913" LastEditorUserId="805" OwnerUserId="26975" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;t-test&gt;&lt;multiple-comparisons&gt;&lt;goodness-of-fit&gt;&lt;kolmogorov-smirnov&gt;" Title="Comparing multiple datasets from a controlled experiment" ViewCount="47" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Exercise 5.1 from Myers' &quot;Classical and modern regression with applications&quot; asks you to prove that:&lt;/p&gt;&#10;&#10;&lt;p&gt;The regression coefficient associated with $X_{j}$ in the multiple regression of $y$ against  $(X_{1}, \ldots,X_{n})$ is equal to the slope of the regression of &#10;$e_{y/X_{-j}} $ against $e_{x_{j}/X_{-j}}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I try the following :&lt;/p&gt;&#10;&#10;&lt;p&gt;$P_X=H=X(X' X)^{-1}X'⇒$ &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$H'=H$&lt;/li&gt;&#10;&lt;li&gt;$H^{2}=H$&lt;/li&gt;&#10;&lt;li&gt;$X_{j} ∈ X ⇒P_XX_{j}=X_{j}$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$e_{y/X} =(I-P_X )y  $&lt;/p&gt;&#10;&#10;&lt;p&gt;$e_{y/X_{-j}} =(I-P_{X{-j}} )y  $&lt;/p&gt;&#10;&#10;&lt;p&gt;$e_{X/x_{-j}} =(I-P_{X{-j}} )x_{j}  $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbf{b}=\begin{bmatrix}b_{-j} \\ ... \\ b_{j}\end{bmatrix}$;&#10;$\mathbf{X}=\begin{bmatrix}X_{-j} &amp;amp; ⋮  &amp;amp; x_{j}\end{bmatrix}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = \beta x + \epsilon=X_{-j}b_{-j}+x_{j} b_{j}+ \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;then I premultiply $(I-P_{X{-j}} )  $ &#10;using $(I-P_{X{-j}} )X_{-j}=X_{-j}-X_{-j}$&#10;⇒&#10;$e_{y/X_{-j}} =b_{j}e_{X/x_{-j}}+\epsilon ^{*}$ ⇒&lt;/p&gt;&#10;&#10;&lt;p&gt;$b_{j}= e_{y/X_{-j}}'e_{X/x_{-j}}   &#10;      /(e_{X/x_{-j}}'e_{X/x_{-j}})=&#10;           x_{j}'(I-P_{X{-j}})y   &#10;        / x_{j}'(I-P_{X{-j}})x_{j}$&lt;/p&gt;&#10;&#10;&lt;p&gt;But I need to show $b_{j}$ is the slope of the regression of &#10;$e_{y/X_{-j}} $ against $e_{x_{j}/X_{-j}}'$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I need to calculate the regression coefficient in a partitioned matrix&#10;$\mathbf{b}=\begin{bmatrix}b_{-j} \\ ... \\ b_{j}\end{bmatrix}=(&#10;\begin{bmatrix}X_{-j} &amp;amp; ⋮  &amp;amp; x_{j}\end{bmatrix}'\begin{bmatrix}X_{-j} &amp;amp; ⋮  &amp;amp; x_{j}\end{bmatrix})^{-1}\begin{bmatrix}X_{-j} &amp;amp; ⋮  &amp;amp; x_{j}\end{bmatrix}'Y$&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-06-22T22:06:31.740" CreationDate="2013-06-17T16:41:48.890" Id="61950" LastActivityDate="2013-07-01T10:54:26.177" LastEditDate="2013-07-01T10:54:26.177" LastEditorUserId="2116" OwnerUserId="26956" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;econometrics&gt;&lt;linear-model&gt;" Title="Regression coefficient in a partitioned matrix" ViewCount="157" />
  <row AnswerCount="1" Body="&lt;p&gt;I wonder what the difference between multivariate standard normal distribution and Gaussian copula is since when I look at the density function they seem the same to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;My issue is why the Gaussian copula is introduced or what benefit the Gaussian copula generates or what its superiority is when Gaussian copula is nothing but a multivariate standard normal function itself. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also what is the concept behind probability integral transformation in copula? I mean we know that a copula is a function with uniform variable. Why does it have to be uniform? Why not use the actual data like multivariate normal distribution and find the correlation matrix? (Normally we plot the two asset returns to consider their relationships but when it is copula, we plot the Us which are probabilities instead.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Another question. I also doubt whether the correlation matrix from MVN could be non-parametric or semi-parametric like those of copula (for copula parameter can be kendall's tau, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very thankful for your help since I'm new in this area. (but I have read a lot of papers and these are the only things that I don't understand) &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-17T17:49:56.433" FavoriteCount="3" Id="61956" LastActivityDate="2013-06-18T13:09:25.177" LastEditDate="2013-06-17T18:09:36.700" LastEditorUserId="6029" OwnerUserId="26979" PostTypeId="1" Score="7" Tags="&lt;normal-distribution&gt;&lt;copula&gt;" Title="Difference between multivariate standard normal distribution and Gaussian copula" ViewCount="1591" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Given a joint probability distribution over the variables $X_1,X_2,\dots,X_n$. Is there an algorithm for constructing the corresponding Bayesian Network?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-17T20:30:18.170" Id="61967" LastActivityDate="2013-07-17T13:00:05.670" LastEditDate="2013-07-17T13:00:05.670" LastEditorUserId="22468" OwnerUserId="26990" PostTypeId="1" Score="0" Tags="&lt;algorithms&gt;&lt;causal-inference&gt;&lt;joint-distribution&gt;&lt;bayes-network&gt;" Title="Finding the corresponding bayesian network of a predefined joint probability distribution" ViewCount="90" />
  <row Body="&lt;p&gt;If the distributions are similar (in particular have the same variance) and the group sizes are identical (balanced design), you probably have no reason to worry. Formally, the normality assumption &lt;em&gt;is&lt;/em&gt; violated and it can matter but it is less important than the equality of variance assumption and simulation studies have shown ANOVA to be quite robust to such violations &lt;em&gt;as long as the sample size and the variance are the same across all cells of the design.&lt;/em&gt; If you combine several violations (say non-normality and heteroscedasticity) or have an unbalanced design, you cannot trust the F test anymore.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, the distribution will also have an impact on the error variance and even if the nominal error level is preserved, non-normal data can severely reduce the power to detect a given difference. Also, when you are looking at skewed distributions, a few large values can have a big influence on the mean. Consequently, it's possible that two groups really have different means (in the sample and in the population) but that most of the observations (i.e. most of the test runs in your case) are in fact very similar. The mean therefore might not be what you are interested in (or at least not be all you are interested in).&lt;/p&gt;&#10;&#10;&lt;p&gt;In a nutshell, you could probably still use ANOVA as inference will not necessarily be threatened but you might also want to consider alternatives to increase power or learn more about your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also note that strictly speaking the normality assumption applies to the distribution of the residuals, you should therefore look at residual plots or at least at the distribution in each cell, not at the whole data set at once.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-17T20:42:42.757" Id="61969" LastActivityDate="2013-06-19T17:22:26.950" LastEditDate="2013-06-19T17:22:26.950" LastEditorUserId="6029" OwnerUserId="6029" ParentId="61961" PostTypeId="2" Score="5" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;The data set which I am trying to analyse is Student Test Data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a data of responses (either 1/correct response or 0/incorrect response) on some questions of a set of students. I have fitted a &quot;3 parameter logistic model&quot; for each question and then calculated the goodness of fit estimate and hence the respective p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now my problem is I don't know how to aggregate these individual goodness of fit estimates to get a total goodness of fit for the whole model, Is there any measure (preferably in the programming language R) which can suggest about the whole model depending on the p-values of multiple tests. The package which I used is ltm inside R.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-18T06:09:44.490" FavoriteCount="1" Id="61996" LastActivityDate="2013-06-18T18:43:55.520" LastEditDate="2013-06-18T18:43:55.520" LastEditorUserId="6029" OwnerUserId="27011" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;p-value&gt;&lt;irt&gt;" Title="Overall goodness-of-fit/p-value for multiple items IRT model in R (ltm)" ViewCount="149" />
  <row AnswerCount="0" Body="&lt;p&gt;I am interested in calculating the correlation between sets of expected and observed data. I am trying to compare observed (n&amp;lt;12) and expected values that are small proportions ranging from 0 to 7.4 and that are non-normally distributed. I have tried a number of transformations, but none seem to transform my observed data enough to meet tests which require normally distributed data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently using non-parametric tests from the &lt;code&gt;rococo&lt;/code&gt; R package that need me to specify similarity measures based on fuzzy orderings. The PDF for this package outlining how to choose the appropriate similarity measure for the data is somewhat hard to translate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone point out some tips/resources on which measure to use, for example, linear, gauss(ian) or exponential? I have tried multiple similarity measures using the different measures, and they give quite different correlation coefficients. I would really appreciate understanding which measure is best for different situations.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-06-18T07:09:37.827" Id="61998" LastActivityDate="2013-06-19T01:21:33.217" LastEditDate="2013-06-19T01:21:33.217" LastEditorUserId="3826" OwnerUserId="27013" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="How to determine which similarity measure to use when computing the robust gamma rank correlation coefﬁcient?" ViewCount="74" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;AIC is often a good indicator of model quality. Lower AIC = better model. Nevertheless, don't blindly trust AIC or any other statistical measure. The only true measure of a forecasting model's quality is out of sample forecasting accuracy.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Yes, &lt;code&gt;auto.arima()&lt;/code&gt; already includes season. Be sure to tell &lt;code&gt;auto.arima()&lt;/code&gt; that these are monthly data, i.e., don't simply plug in a vector of length 36, but a &lt;code&gt;ts&lt;/code&gt;  object.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Depending on your data, you may want to look at log or other transforms if residuals are &quot;really not normal&quot;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Three years of data should be (barely) enough for an ARIMA model. You can also look at exponential smoothing/state space models using &lt;code&gt;ets()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://otexts.com/fpp/&quot; rel=&quot;nofollow&quot;&gt;Here is a book recommendation on forecasting.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-18T08:07:45.013" Id="62001" LastActivityDate="2013-06-18T08:07:45.013" OwnerUserId="1352" ParentId="61943" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am wondering about the correct statistical analysis for an experiment involving 2 independent variables (treatment A, 4 levels and treatment B, 2 levels) and one dependent variable (C, score). Because subjects undergo all levels of both treatments, I am assuming a two-way within-subject (repeated-measures) ANOVA (4x2) should be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there is a complicating factor: we used a setup in which we measured C for 2 subjects together, according to the following scheme:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array}{| l | c | r |} \hline&#10;                &amp;amp;  \text{B level 1} &amp;amp;  \text{B level 2}\\ \hline&#10;    \text{A level 1}   &amp;amp;  \text{Subject 1} &amp;amp; \text{Subject 2}\\&#10;    \text{A level 2}   &amp;amp;  \text{Subject 2} &amp;amp; \text{Subject 1}\\&#10;    \text{A level 3}   &amp;amp;  \text{Subject 1} &amp;amp; \text{Subject 2}\\&#10;    \text{A level 4}   &amp;amp;  \text{Subject 2} &amp;amp; \text{Subject 1}\\ \hline&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;When looking at treatment A, all subjects undergo levels 1 to 4, so within-subject design. Also, when looking at treatment B, all subjects undergo levels 1 and 2, so again within-subject design.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it is not the case that all subjects undergo all levels of A in conjunction with all levels of B, e.g. subject 1 has undergone A level 1 in conjunction with B level1, but not with B level 2. And vice versa for subject 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, I am inclined to think that regarding interaction between treatment A and B, a within-subject design might not be correct. Any suggestions would be appreciated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-18T13:12:47.247" Id="62013" LastActivityDate="2013-06-20T09:53:28.243" LastEditDate="2013-06-20T09:53:28.243" LastEditorUserId="805" OwnerUserId="27023" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="Not all subjects undergo conjunction of two treatments, two-way within-subject (repeated-measures) ANOVA correct?" ViewCount="100" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using the &lt;a href=&quot;http://cran.r-project.org/web/packages/mlogit/index.html&quot; rel=&quot;nofollow&quot;&gt;mlogit package&lt;/a&gt; in R to estimate a choice model, I started with the famous &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/Ecdat/html/Yogurt.html&quot; rel=&quot;nofollow&quot;&gt;Yogurt dataset&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mlogit)&#10;&#10;data(Yogurt)&#10;names(Yogurt)&#10;&#10;Yogdata &amp;lt;- mlogit.data(Yogurt, choice = &quot;choice&quot;, shape = &quot;wide&quot;, varying = c(2:9), &#10;                       sep=&quot;.&quot;)&#10;&#10;Yog1 &amp;lt;- mlogit(choice ~ feat+price , data = Yogdata)&#10;&#10;summary(Yog1)&#10;phat      &amp;lt;- Yog1$probabilities&#10;Yname     &amp;lt;-colnames(phat)&#10;Y1hat     &amp;lt;- max.col(phat)&#10;Y1hatname &amp;lt;- Yname[Y1hat]&#10;&#10;preds = predict(Yog1,newdata=Yogdata)&#10;preds[100:120,]&#10;Yogurt[100:120,c('choice')]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output &lt;code&gt;preds&lt;/code&gt; includes probabilities that sum up to 1 for each row. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         dannon     hiland    weight   yoplait&#10; [1,] 0.3881972 0.03053851 0.2200040 0.3612603&#10; [2,] 0.6834572 0.01580043 0.1138286 0.1869137&#10; [3,] 0.4136804 0.03137184 0.2344462 0.3205015&#10; [4,] 0.4136804 0.03137184 0.2344462 0.3205015&#10; [5,] 0.4136804 0.03137184 0.2344462 0.3205015&#10; [6,] 0.4136804 0.03137184 0.2344462 0.3205015&#10; [7,] 0.4136804 0.03137184 0.2344462 0.3205015&#10; [8,] 0.4136804 0.03137184 0.2344462 0.3205015&#10;&#10; [1] dannon  dannon  yoplait yoplait dannon  dannon  dannon  yoplait dannon &#10;[10] dannon  yoplait yoplait dannon  dannon  dannon  dannon  dannon  dannon &#10;[19] dannon  dannon  dannon &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The choice would be the highest probability in each row. Would this be correct? When I compare this choice against real choices coming from Yogurt, for the same rows, I see matches are not too great. Is there a way to improve this model? Is this the right way to judge the success of this model? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-18T15:13:05.620" Id="62020" LastActivityDate="2013-06-18T21:47:18.880" LastEditDate="2013-06-18T21:47:18.880" LastEditorUserId="7290" OwnerUserId="9577" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;multinomial&gt;&lt;logit&gt;" Title="Multinomial Logit with mlogit and Yogurt Data" ViewCount="227" />
  
  <row Body="&lt;p&gt;From the title it does not sound like it but the book &quot;Mostly Harmless Econometrics&quot; by Angrist and Pischke gives a thorough explanation of causal effects estimation, the underlying rationale and a wide discussion of techniques useful for applied worked. They explain all techniques and their basic ideas with real-life examples, though the majority is related to economics if you don't mind that.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you would like to have a more technical treatment of the idea of counterfactuals, a major paper in this respect is by Angrist, Imbens and Rubin (1996) in the Journal of the American Statistical Association. In there they establish a causal effects framework built on counterfactuals which uses instrumental variables to identify local average treatment effects.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-18T16:12:11.637" Id="62029" LastActivityDate="2013-06-18T16:12:11.637" OwnerUserId="26338" ParentId="62025" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am often disappointed with PCA plots in the scientific literature. Typically PCA plots do not provide a breakdown of the variables and their weights, just something like PCA1 (70% variance explained), PCA2 (10% variance explained). How could one tell which variables are strongly loaded into a component?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there PCA visualizations that can provide more insight into the data?&lt;/p&gt;&#10;" ClosedDate="2015-01-27T00:14:12.663" CommentCount="4" CreationDate="2013-06-18T16:32:33.057" FavoriteCount="6" Id="62034" LastActivityDate="2013-07-04T16:50:10.600" LastEditDate="2013-07-04T15:57:40.333" LastEditorUserId="26226" OwnerUserId="684" PostTypeId="1" Score="7" Tags="&lt;data-visualization&gt;&lt;pca&gt;" Title="Are there examples of more informative PCA plots?" ViewCount="567" />
  
  <row Body="&lt;p&gt;With Philosophy, a good starting point is always the work of &lt;strong&gt;Bertrand Russell&lt;/strong&gt;. There's no doubt that you'd find sections in Russell's &lt;em&gt;History of Western Philosophy&lt;/em&gt; that cover the philosophy of causation/causal inference, but given its size and broad scope, it would be difficult for me to pin down for you exactly where to look in this book. Taking the longer term view, though, this is &lt;em&gt;the&lt;/em&gt; book to start with if you want to deepen your knowledge of philosophy - its evolution - and philosophers themselves.&lt;/p&gt;&#10;&#10;&lt;p&gt;A second book by Bertrand Russell that's worth consulting is &lt;em&gt;Human Knowledge&lt;/em&gt;. &lt;em&gt;Part V&lt;/em&gt; of this book covers &lt;em&gt;Probability&lt;/em&gt; while &lt;em&gt;Part VI&lt;/em&gt; is about the &lt;em&gt;Postulates of Scientific Inference&lt;/em&gt;. Both of these topics are discussed from the philosopher's standpoint. To give you a taste of the book, I've added two extracts from the Introduction below.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Introduction to the book, Bertrand tells us a little bit about Part V &lt;em&gt;Probability&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Since it is admitted that scientific inferences, as  rule, only confer&#10;  probability on the conclusions, Part V proceeds to the examination of&#10;  Probability. This term is capable of various interpretations , and has&#10;  been differently defined by different authors. These interpretations&#10;  and definitions are examined, and so are the attempts to connect&#10;  induction with probability. In this matter the conclusion reached is,&#10;  in the main, that advocated by Keynes: that inductions do not make&#10;  their conclusions probable unless certain conditions are fulfilled, and that&#10;  experience alone can never prove that these conditions are fulfilled.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;And on Part VI &lt;em&gt;Postulates of Scientific Inference&lt;/em&gt;, Bertrand says (again, from the Introduction):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Part VI, on the postulates of scientific inference, endeavours to&#10;  discover what are the minimum assumptions, anterior to experience,&#10;  that are required to justify us in inferring laws from a collection of&#10;  data; and further, to inquire in what sense, if any, we can be said to&#10;  know that these assumptions are valid. The main logical function that&#10;  the assumptions have to fulfill is that of conferring a high&#10;  probability on the conclusions and inductions that satisfy certain&#10;  conditions. For this purpose, since only probability is in question,&#10;  we do not need to assume that such-and-such a connection of events&#10;  occurs always, but only that it occurs frequently. For example, one of the &#10;  assumptions that appear necessary is that of separable causal chains,&#10;  such as are exhibited by light-rays or sound-waves. This assumption&#10;  can be stated as follows: when an event having a complex space-time&#10;  structure occurs, it frequently happens that it is one of a train of&#10;  events having the same or a very similar structure. (A more exact&#10;  statement will be found in Chapter 6 of this Part.) This is part of a&#10;  wider assumption of regularity, or natural law, which, however,&#10;  requires to be stated in more specific forms than is usual, for in its&#10;  usual form it turns out to be a tautology.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;That scientific inference requires, for its validity, principles which&#10;  experience cannot render even probable, is, I believe, an inescapable&#10;  conclusion from the logic of probability. For empiricism, it is an&#10;  awkward conclusion.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;But I think it can be rendered somewhat more palatable by the analysis of the &#10;  concept of &quot;knowledge&quot; undertaken in Part II. &quot;Knowledge&quot;, in my&#10;  opinion, is a much less precise concept than is generally thought, and&#10;  has its roots more deeply embedded in unverbalized animal behaviour&#10;  than most philosophers have been willing to admit. The logically basic&#10;  assumptions to which our analysis leads us are psychologically the end&#10;  of a long series of refinements which start from habits of expectation&#10;  in animals, such as that what has a certain kind of smell will be good&#10;  to eat. To ask, therefore, whether we &quot;know&quot; the postulates of&#10;  scientific inference, is not so definite a question as it seems. The&#10;  answer must be: in one sense, yes, in another sense, no; but in the&#10;  sense in which &quot;no&quot; is the right answer we know nothing whatever, and&#10;  &quot;knowledge&quot; in this sense is a delusive vision. The perplexities of&#10;  philosophers are due, in a large measure, to their unwillingness to&#10;  awaken from this blissful dream.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you decide to take things further (down the academic line), I'd also suggest searching &quot;causal inference&quot; in the &lt;a href=&quot;http://www.oxfordjournals.org/our_journals/mind/about.html&quot; rel=&quot;nofollow&quot;&gt;Oxford Journal &lt;em&gt;Mind&lt;/em&gt;&lt;/a&gt;. There is a search tool on the Journal's website.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-18T17:21:32.797" Id="62039" LastActivityDate="2013-06-18T20:04:56.413" LastEditDate="2013-06-18T20:04:56.413" LastEditorUserId="24617" OwnerUserId="24617" ParentId="62025" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="62072" AnswerCount="1" Body="&lt;p&gt;Say one has the fatality and survivor rates of a pathogen in terms of geographical location (e.g., the latest data on the &lt;a href=&quot;http://www.cdc.gov/coronavirus/mers/overview.html&quot; rel=&quot;nofollow&quot;&gt;MERS-CoV&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;France                      2 (1)&#10;Italy                       3 (0)&#10;Jordan                      2 (2)&#10;Qatar                       2 (0)&#10;Saudi Arabia               49 (32)&#10;Tunisia                     2 (0)&#10;United Kingdom (UK)         3 (2)&#10;United Arab Emirates (UAE)  1 (1)&#10;______________________________________&#10;Total                      64 (38)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The total fatality rate is ~59%. However, I am not sure that this is a good measure: the rate for Saudi Arabia is ~65%, while the smaller cases have rates of 0 to 100%. &lt;/p&gt;&#10;&#10;&lt;p&gt;My thoughts are that the size of the outbreak should weight the outbreak's impact on the overall rate. However, I don't think averaging the individual rates would be correct. Is there a more appropriate measure that would allow me to take the size of the outbreak into account when calculating the overall impact?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-19T02:51:32.107" Id="62064" LastActivityDate="2013-06-19T14:02:35.493" LastEditDate="2013-06-19T08:15:22.737" LastEditorUserId="22047" OwnerUserId="21691" PostTypeId="1" Score="2" Tags="&lt;mean&gt;&lt;epidemiology&gt;&lt;population&gt;" Title="How to best account for population sizes in disease outbreaks" ViewCount="43" />
  
  <row Body="" CommentCount="0" CreationDate="2013-06-19T09:27:23.533" Id="62077" LastActivityDate="2013-06-19T09:27:23.533" LastEditDate="2013-06-19T09:27:23.533" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a 2x2 experimental design. In the experiment, I also collected the participants' professional qualifications (categorical variable- yes /no). I would like to test the effect of controlling this variable on the MSE. I understand that in ANCOVA, the covariate is either an interval or ordinal variable. So should I refer to the test as an ANCOVA if I include professional qualification as an additional variable? Or should it be a blocking variable since &quot;prof. qualification&quot; is categorical. Thanks in advance for your replies!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-19T11:52:12.550" Id="62086" LastActivityDate="2013-06-19T11:59:17.703" OwnerUserId="27059" PostTypeId="1" Score="3" Tags="&lt;categorical-data&gt;&lt;ancova&gt;&lt;blocking&gt;" Title="Can covariates be categorial variables?" ViewCount="513" />
  <row AnswerCount="0" Body="&lt;p&gt;I am having trouble evaluating the covariance of a non-linear transformation for two vectors $s_0^*$ and $s_1^*$&#10;$$\left| \begin{array}{c} s_0^*\\s_1^* \end{array} \right| \sim  N\left( \left| \begin{array}{c}\mu_0^*\\ \mu_1^* \end{array} \right|,\left| \begin{array}{c}\Sigma_s &amp;amp; \Gamma_{01} \\ \Gamma_{10} &amp;amp; \Sigma_s \end{array} \right| \right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to find the variance expression of &#10;$$ s_1 -s_0=\frac{\exp(s_1^*)}{1+\exp(s_1^*)}-\frac{\exp(s_0^*)}{1+\exp(s_0^*)}=\text{logit}(s_1^*)-\text{logit}(s_0^*)\\$$&#10;$\text{Var} \left(\frac{\exp(s_1^*)}{1+\exp(s_1^*)}-\frac{\exp(s_0^*)}{1+\exp(s_0^*)}\right)= \text{Var}( s_1)+\text{Var}(s_0)-2\text{cov}(s_1,s_0)$,&lt;/p&gt;&#10;&#10;&lt;p&gt;I know how to find the variance of the two variance expressions from by the delta method, but I'm and having trouble finding an expression for the covariance between the two.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm guessing that I should use Taylor expansions somehow, but it stops for me pretty quickly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips on how I should go about finding this covariance? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am familiar with the relation $\text{cov}(s_0,s_1)=\text{E}(s_0s_1)+\text{E}(s_0)\text{E}(s_1)$ so is it the case that all I have to do is find $\text{E}(s_0 s_1)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm pretty stuck here so any help is greatly appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-19T11:54:28.023" Id="62087" LastActivityDate="2013-06-19T12:03:09.987" LastEditDate="2013-06-19T12:03:09.987" LastEditorUserId="805" OwnerUserId="27061" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;&lt;random-variable&gt;&lt;covariance&gt;" Title="Covariance of logit transformed variables" ViewCount="52" />
  
  <row AcceptedAnswerId="62147" AnswerCount="7" Body="&lt;p&gt;I'm studying pattern recognition and statistics and almost every book I open on the subject I bump into the concept of &lt;strong&gt;Mahalanobis distance&lt;/strong&gt;. The books give sort of intuitive explanations, but still not good enough ones for me to actually really understand what is going on. If someone would ask me &quot;What is the Mahalanobis distance?&quot; I could only answer: &quot;It's this nice thing, which measures distance of some kind&quot; :) &lt;/p&gt;&#10;&#10;&lt;p&gt;The definitions usually also contain eigenvectors and eigenvalues, which I have little trouble connecting to the Mahalanobis distance. I understand the definition of eigenvectors and eigenvalues, but how are they related to the Mahalanobis distance? Does it have something to do with changing the base in Linear Algebra etc.?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have also read these former questions on the subject:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/41222/what-is-mahanalobis-distance-how-is-it-used-in-pattern-recognition&quot;&gt;What is Mahanalobis distance, &amp;amp; how is it used in pattern recognition?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://math.stackexchange.com/questions/261557/intuitive-explanations-for-gaussian-distribution-function-and-mahalanobis-distan&quot;&gt;Intuitive explanations for Gaussian distribution function and mahalanobis distance (Math.SE)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have also read &lt;a href=&quot;http://www.jennessent.com/arcview/mahalanobis_description.htm&quot;&gt;this explanation&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answers are good and pictures nice, but still I don't &lt;strong&gt;really&lt;/strong&gt; get it...I have an idea but it's still in the dark. Can someone give a &quot;How would you explain it to your grandma&quot;-explanation so that I could finally wrap this up and never again wonder what the heck is a Mahalanobis distance? :) Where does it come from, what, why? &lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is something which helps understanding the Mahalanobis formula: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://math.stackexchange.com/questions/428064/distance-of-a-test-point-from-the-center-of-an-ellipsoid&quot;&gt;http://math.stackexchange.com/questions/428064/distance-of-a-test-point-from-the-center-of-an-ellipsoid&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-19T12:41:02.850" FavoriteCount="48" Id="62092" LastActivityDate="2014-10-03T01:55:38.740" LastEditDate="2013-06-25T13:11:02.620" LastEditorUserId="22047" OwnerUserId="18528" PostTypeId="1" Score="43" Tags="&lt;normal-distribution&gt;&lt;mathematical-statistics&gt;&lt;distance&gt;&lt;pattern-recognition&gt;&lt;intuition&gt;" Title="Bottom to top explanation of the Mahalanobis distance?" ViewCount="17295" />
  
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I assumed that the chaotic nature of catching bubbles in my images would mean that if I plot a histogram of the number of bubbles per image it would be roughly Gaussian&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Maybe, maybe not. My first guess would have been Poisson (which can be approximated as Gaussian if there are &quot;enough&quot; bubbles).&lt;/p&gt;&#10;&#10;&lt;p&gt;Train of thoughts: one of the famous examples for producing a Poisson distribution is measuring radioactive decay. From a chemical kinetics point of view, that is a first order kinetics. So if the production of your bubbles can sensibly be described or approximated as (pseudo) first order, Poisson distribution would be a sensible assumption. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd think that bubble kinetics are somewhat more complicated (there's seeding and growth) but that may be a first start.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-19T14:14:04.690" Id="62107" LastActivityDate="2013-06-19T16:01:53.030" LastEditDate="2013-06-19T16:01:53.030" LastEditorUserId="22047" OwnerUserId="4598" ParentId="62091" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;A multilayer feed-forward network is a universal function approximator, so yes, it can approximate a GLM (or any other function you happen to dream up). The key word there, however, is &quot;approximator&quot; and the quality of the approximation depends on the number of hidden units, the network's training, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;The classic paper on this is &lt;a href=&quot;http://actcomm.dartmouth.edu/gvc/papers/approx_by_superposition.pdf&quot; rel=&quot;nofollow&quot;&gt;Cybenko (1989)&lt;/a&gt;, who showed that it was true for sigmoidal activation functions. This was followed up by &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/089360809190009T&quot; rel=&quot;nofollow&quot;&gt;Hornik (1991)&lt;/a&gt;, who showed the multilayer architecture, not the activation function per se, was the key. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-19T16:42:45.557" Id="62124" LastActivityDate="2013-06-19T16:42:45.557" OwnerUserId="7250" ParentId="62123" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Note, you have no noise in your data. The dependent variable is a deterministic function of all covariates - no wonder your regression gets the exact coefficients (betas) in the first regression. You should also generate a random noise term, use this in the computation of your &quot;response&quot;, but not include it as predictor in your model. That was just an aside, it doesn't explain nor change the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;First I thought what you demonstrated is an &lt;a href=&quot;http://en.wikipedia.org/wiki/Omitted-variable_bias&quot; rel=&quot;nofollow&quot;&gt;omitted variable bias&lt;/a&gt;, but it can't be, since your predictors are uncorrelated by construction. Thus, theoretically, your second regression should estimate a coefficient of 2. I reproduced your example in Stata, and the problem vanishes if you have enough observations; doing 1000 instead of 10 gives me a coefficient of $2.5$ instead of $-27$ on predictor 2 (I have a different seed though) - much closer. Thus, asymptotically, this should not be a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the effect of predictor2 need not be underestimated (negative). Run your regression a couple of times more (no seed) and you will see that it can also be overestimated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the new explanation: each observation of predictor2 $x_i$ is a realization of the random normal variable $N(3,1)$. $y$ (response) is a function of $x$ and $z$ (predictor1) and some noise. What OLS tries to do is attribute changes in $y$ to the predictors, in this case only $x$. This is what $$\hat{\beta}=\frac{Cov(x,y)}{Var(x)}$$ reflects. Now $Cov(y,x)=1/N\sum_i^N (x_i-\bar{x})(y_i-\bar{y})$, that is, it tells us how much $x$ and $y$ &quot;co-move&quot; (tend to go above their means $\bar{x},\bar{y}$ or below together). Since we omit predictor1 $z$, there can be 4 different cases for a given observation $i$. (1) $y_i$ is large (above mean) and $x_i$ is large or (2) both are small (below mean). Since both move in the same direction, this tends to make Cov positive. If $y_i$ is above the mean, then $z_i$ (realization of predictor1) was in most cases positive as well. But because the effect of $z$ is 25 times the effect of $x$, the coefficient of $x$ in these cases is overestimated. &lt;/p&gt;&#10;&#10;&lt;p&gt;Example: It's like having two employees in a firm producing crossword puzzles, but employee 1 is 25x as productive as employee 2. Now an accountant tries to estimate how much the employees contribute to the firms' crossword puzzle production, but he forgets employee 1 in the calculation. He does this by regressing firm production on employee time clocked. Suppose both employees always work together long or short hours (always begin and leave together). The accountant then overestimates the productivity of employee 2, because overall production and employee 2 working times co-move a lot. To the accountant, it looks as if employee 2 is responsible for all the output, consequently he must have been very productive in the time he worked. But in reality, this is all employee 1's work.&lt;/p&gt;&#10;&#10;&lt;p&gt;In remaining case (3) when $y_i$ is large and $x_i$ is small, it actually looks as if small values of $x$ lead to large values of $y$, so the effect of $x$ on $y$ is underestimated. The same holds for the remaining case (4) ($y_i$ is small and $x_i$ is large). Because we have only 10 observations, it is quite random whether cases (1) and (2), or cases (3) and (4) prevail. This is why it can go either way: the effect can be over- or underestimated. In very large samples, however, those over- and underestimations should cancel out, and we get the right coefficient for $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;OLS results for 10 observations (again demonstrating the problem):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      Source |       SS       df       MS              Number of obs =      10&#10;-------------+------------------------------           F(  1,     8) =    1.53&#10;       Model |   3575.0212     1   3575.0212           Prob &amp;gt; F      =  0.2516&#10;    Residual |  18726.9444     8  2340.86805           R-squared     =  0.1603&#10;-------------+------------------------------           Adj R-squared =  0.0553&#10;       Total |  22301.9656     9  2477.99618           Root MSE      =  48.383&#10;&#10;------------------------------------------------------------------------------&#10;    response |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;  predictor2 |  -27.06526   21.90087    -1.24   0.252    -77.56875    23.43823&#10;       _cons |   343.1629   138.8042     2.47   0.039      23.0798     663.246&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OLS results for 1000 observations (solving the problem):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      Source |       SS       df       MS              Number of obs =    1000&#10;-------------+------------------------------           F(  1,   998) =    2.58&#10;       Model |  6396.16144     1  6396.16144           Prob &amp;gt; F      =  0.1088&#10;    Residual |  2477369.09   998  2482.33376           R-squared     =  0.0026&#10;-------------+------------------------------           Adj R-squared =  0.0016&#10;       Total |  2483765.25   999   2486.2515           Root MSE      =  49.823&#10;&#10;------------------------------------------------------------------------------&#10;    response |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;  predictor2 |   2.509032   1.563063     1.61   0.109    -.5582357    5.576299&#10;       _cons |   148.5405    9.53532    15.58   0.000     129.8289    167.2521&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My Stata code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;clear&#10;set obs 10 //10 observations&#10;set seed 111&#10;gen predictor1=rnormal(3,1) //generate variables&#10;gen predictor2=rnormal(6,1)&#10;gen noise=rnormal(0,3)&#10;gen response=2+50*predictor1+2*predictor2+noise //include random error&#10;&#10;reg response predictor1 predictor2 //ols&#10;reg response predictor2&#10;&#10;clear&#10;set obs 1000&#10;set seed 111&#10;gen predictor1=rnormal(3,1)&#10;gen predictor2=rnormal(6,1)&#10;gen noise=rnormal(0,3)&#10;gen response=2+50*predictor1+2*predictor2+noise //include random error&#10;&#10;reg response predictor2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-06-19T17:37:22.473" Id="62129" LastActivityDate="2013-06-21T23:17:20.667" LastEditDate="2013-06-21T23:17:20.667" LastEditorUserId="22543" OwnerUserId="22543" ParentId="62125" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Where can I get an explanation of the procedure used when making a prediction using SVD? &lt;/p&gt;&#10;&#10;&lt;p&gt;Let me elaborate a bit more. Suppose you have data in a matrix $A$ containing two classes. In particular, you could have $m$ attributes and $n$ objects. Half of the objects belong to the first class and the other half belong to the second class. Using SVD we obtain the matrices $U$, $S$, $V^{*}$. From the factor interpretation, we can regard the rows of matrix $U$ as a different view of the $m$ attributes of matrix $A$ and the columns of $V$ as a different view of the objects of matrix $A$. Therefore, it's possible to separate the objects depending on their class from $V^{*}$. For example, supposing that $V^{*}$ looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;  V^{*} = \begin{pmatrix} &#10;  v_{1,1} &amp;amp; v_{1,2} &amp;amp; \cdots &amp;amp; v_{1,n} \\\&#10;  v_{2,1} &amp;amp; v_{2,2} &amp;amp; \cdots &amp;amp; v_{2,n} \\\ &#10;  \vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\\  &#10;  v_{n,1} &amp;amp; v_{n,2} &amp;amp; \cdots &amp;amp; v_{n,n}  \end{pmatrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we can separate the first $n/2$ columns and regard them as an alternative view of objects from the fist class. We can do the same with the last $n/2$ columns as an alternative view of objects from the second class. In practice, I have seen we take the matrix $\Sigma V^{*}$ of the decomposition:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\Sigma V^{*} =   &#10;\begin{pmatrix} &#10;  \sigma_{1} v_{1,1} &amp;amp; \sigma_{1}v_{1,2} &amp;amp; \cdots &amp;amp; \sigma_{n} v_{1,n} \\\&#10;  \sigma_{2} v_{2,1} &amp;amp; \sigma_{2}v_{2,2} &amp;amp; \cdots &amp;amp; \sigma_{2}v_{2,n} \\\ &#10;  \vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\\  &#10;  \sigma_{n}v_{n,1} &amp;amp; \sigma_{n}v_{n,2} &amp;amp; \cdots &amp;amp; \sigma_{n}v_{n,n}  &#10;\end{pmatrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now that I think about it, I don't know why. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now we have enough information to calculate the mean of each class in this rather unusual view of our objects. So you can use Fisher's discriminant analysis to get projections of elements from each class in such a way to minimize the variances within elements of a given class and maximize the means between classes. This is succinctly described by the following equation: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\vec{w} = \text{ arg max }_{\vec{w}} \displaystyle \frac{\vec{w}S_{b}\vec{w}}{\vec{w}S_{w}\vec{w}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution is found by solving the associated &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_eigenvalue_problem#Generalized_eigenvalue_problem&quot; rel=&quot;nofollow&quot;&gt;generalized eigenvalue problem&lt;/a&gt;. This produces a set of eigenvectors and eigenvalues. Apparently, we need to choose the eigenvector corresponding to the largest eigenvalue. Why? After getting this $\vec{w}$ we have everything we need to start making predictions. For simplicity, consider a new object $\vec{b}$ that has the same kind of attributes as $A$. Supposedly, a prediction starts by multiplying $U^{T}$ and $\vec{b}$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ &#10;  U^{T}\vec{b} = \begin{pmatrix} &#10;  u_{1,1} &amp;amp; u_{1,2} &amp;amp; \cdots &amp;amp; u_{1,n} \\\&#10;  u_{2,1} &amp;amp; u_{2,2} &amp;amp; \cdots &amp;amp; u_{2,n} \\\ &#10;  \vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\\  &#10;  u_{m,1} &amp;amp; u_{m,2} &amp;amp; \cdots &amp;amp; u_{m,n}  &#10;  \end{pmatrix} \begin{pmatrix} &#10;  b_{1}  \\&#10;  b_{2}  \\ &#10;  \vdots   \\  &#10;  b_{n}   &#10;  \end{pmatrix} &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here the columns of $U^{T}$ represent attributes and the rows represent hidden factors. Likewise, the vector $\vec{b}$ represents a single object with attributes in the rows. As a result of this multiplication, we have an object described in terms of $m$ factors. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, to predict a class for our new object $\vec{b}$, we multiply $\vec{w}$ by the previous result:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$w^{T}U^{T}B = \begin{pmatrix} &#10;  w_{1} &amp;amp; w_{2} &amp;amp; ... &amp;amp; w_{m}   &#10;  \end{pmatrix} \begin{pmatrix} &#10;  u_{1,1} &amp;amp; u_{1,2} &amp;amp; \cdots &amp;amp; u_{1,n} \\\&#10;  u_{2,1} &amp;amp; u_{2,2} &amp;amp; \cdots &amp;amp; u_{2,n} \\\ &#10;  \vdots  &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots  \\\  &#10;  u_{m,1} &amp;amp; u_{m,2} &amp;amp; \cdots &amp;amp; u_{m,n}  &#10;  \end{pmatrix} \begin{pmatrix} &#10;  b_{1}  \\&#10;  b_{2}  \\ &#10;  \vdots   \\  &#10;  b_{n}   &#10;  \end{pmatrix} &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\vec{w}$ is the vector that works as a projection to produce a decision threshold, I think it should live in the same space that other points from $V^{*}$. Therefore, its columns should represent factors. The result of $w^{T}U^{T}B$ produces a number. The comparison of this number with the threshold allows to make a decision on the predicted class.&lt;/p&gt;&#10;&#10;&lt;p&gt;All of this seems rather reasonable. These operations work well and, of course, produce valid operations.  However, I want to have a better grasp of this procedure.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-20T03:02:22.800" FavoriteCount="1" Id="62167" LastActivityDate="2013-07-01T16:36:06.287" LastEditDate="2013-07-01T16:36:06.287" LastEditorUserId="2676" OwnerUserId="2676" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;prediction&gt;&lt;discriminant-analysis&gt;&lt;svd&gt;" Title="Prediction using SVD and Fisher's linear discriminant" ViewCount="262" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Firth's penalized maximum likelihood estimates, exact logistic regression and Bayesian logistic regression (e.g. bayesglm) can account for separation in logistic regression. But how to account for probability weights and complex survey designs in these methods (e.g. in R or Stata)? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-20T05:47:23.243" Id="62175" LastActivityDate="2013-06-20T06:23:38.230" LastEditDate="2013-06-20T06:23:38.230" LastEditorUserId="22047" OwnerUserId="5546" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;survey&gt;" Title="Separation in logistic regression in a complex survey?" ViewCount="93" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a hypothesis such as &quot;People consult internet before making a purchase decision&quot; for a thesis research. In order to test it, I created a game where people had the option to consult &quot;internet option&quot; or not. I have a similar questions, but I have now realised that all my variables for this particular point are nominal; (the answers look 0: did not consult internet, 1: consulted internet) &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I can't calculate a mean or do any of the tests like $t$-test, anova since it doesn't make sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering academically, my results would be enough to confirm (not cannot reject) that people consult internet if I have 80% of the same that consulted internet? Several different questions show similar results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-20T13:00:05.197" Id="62195" LastActivityDate="2013-06-20T13:36:49.807" LastEditDate="2013-06-20T13:32:37.527" LastEditorUserId="22047" OwnerUserId="27113" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;nominal&gt;" Title="Can test a hypothesis with only nominal variables?" ViewCount="162" />
  
  
  <row AcceptedAnswerId="62212" AnswerCount="1" Body="&lt;p&gt;I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know other questions look like this but I'm not confident that they answer this question.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(plyr) # functions&#10;## Define Function&#10;simfunktion &amp;lt;- function() {&#10;   # Number in each sample&#10;  antal &amp;lt;- 6000&#10;  beta0 &amp;lt;- log(0.16) # logit in reference group&#10;  beta1 &amp;lt;- log(1.1)  # logit given smoking&#10;  beta2 &amp;lt;- log(1.1)  # logit given SNP(genevariation)&#10;  beta3 &amp;lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning&#10;   ## Smoking variable, with probabilities defined according to empirical studies.&#10;  smoking  &amp;lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))&#10;   ## SNP variables with probabilities defined according to empirical studies&#10;  SNP      &amp;lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))&#10;   ## calculated probabilites given the model:&#10;  pi.x     &amp;lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / &#10;              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )&#10;   ## binoial events given the probabilities:&#10;  sim.y    &amp;lt;- rbinom( n = antal, size = 1, prob = pi.x)  &#10;  sim.data &amp;lt;- data.frame(sim.y, smoking, SNP)&#10;   #################### p-value of the interaction is extracted:&#10;   ## the model is run:&#10;  glm1     &amp;lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, &#10;                  family=binomial )&#10;   ## p-value of the interactionterm is extracted:&#10;  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, &#10;                  family=binomial ))$coef[4,4]&#10;}&#10;pvalue     &amp;lt;- as.vector(replicate( 100 , simfunktion()))&#10;mean(pvalue &amp;lt; 0.05)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-06-20T16:56:02.200" Id="62208" LastActivityDate="2013-08-07T23:35:14.590" LastEditDate="2013-08-07T23:35:14.590" LastEditorUserId="17230" OwnerUserId="16382" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;simulation&gt;&lt;power-analysis&gt;" Title="Simulate data for power analysis of logistic regression model - include covariance variance of variables?" ViewCount="1051" />
  <row AnswerCount="0" Body="&lt;p&gt;Let us say that we have a process that generates sequences of the following form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$S=[0,0,0,0,0,0,0,0,0,0,0,0,0,\mathbf{4},0,0,0,0,0,0,0,0,0,0,0,0,0,0,\mathbf{1},0,0,0,0,0,0,0,0,\mathbf{3},0,0,0,\mathbf{5},0,0,0]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best way to model something like this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\mathrm{E}(x)$ is small and the number of samples is low one would be tempted to model this using a Poisson distribution. However, this seems quite limiting as give that a Poisson is fully described with the same parameter $\mathrm{E}(x)$, so the same Poisson could model generate another sequence with more zero events and higher values, which I consider been drawn for a different process.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would it make more sense to first model the non-zero occurrences with a Binomial, draw from it and them model the number corresponding to the non zero occurrences with a Poisson?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-20T17:32:06.067" Id="62210" LastActivityDate="2013-08-30T10:22:34.780" LastEditDate="2013-08-30T10:22:34.780" LastEditorUserId="27581" OwnerUserId="27125" PostTypeId="1" Score="1" Tags="&lt;binomial&gt;&lt;sparse&gt;&lt;poisson-process&gt;" Title="Methodology of modelling sparse events" ViewCount="82" />
  <row Body="&lt;p&gt;&lt;em&gt;Let me throw out some thoughts, and we'll see if something helps you.&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Some preliminaries:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I find it a bit odd that you are defining your true betas as the log of some number; is that because you are using reported odds ratios?  (If so, this is perfectly appropriate.)  &lt;/li&gt;&#10;&lt;li&gt;It's important to realize when doing power analyses based on effects reported in the literature that the results are optimistic.  I discuss that here: &lt;a href=&quot;http://stats.stackexchange.com/questions/18028//18172#18172&quot;&gt;Desired effect size vs. expected effect size&lt;/a&gt;; you may also want to read this thread: &lt;a href=&quot;http://stats.stackexchange.com/questions/60109/&quot;&gt;Logistic regression model manipulation&lt;/a&gt;.  &lt;/li&gt;&#10;&lt;li&gt;I notice that you are simulating the expected distribution of your covariates.  That's not typically done; in general, we assume that our covariates are a set of known constants.  However, if you will be doing observational (e.g., epidemiological) research, these can well vary and this strategy is appropriate.  &lt;/li&gt;&#10;&lt;li&gt;Your covariates have the values &lt;code&gt;0, 1, 2&lt;/code&gt;.  Are these levels of a factor, or are they equal interval?  I ask because they look like the former, but are treated as the latter in the data generating process.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Your main question:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If your covariates will be correlated with each other, your power will decrease, so &lt;strong&gt;yes&lt;/strong&gt;, you should definitely try to incorporate that information into the simulation.  &lt;/li&gt;&#10;&lt;li&gt;You want to do that where you are generating the distribution of your covariates.  &lt;/li&gt;&#10;&lt;li&gt;Generating correlated data is slightly more complicated.  There is a general thread on this topic on CV here: &lt;a href=&quot;http://stats.stackexchange.com/questions/38856/&quot;&gt;How to generate correlated random numbers (given means, variances, and degree of correlation)&lt;/a&gt;.  &lt;/li&gt;&#10;&lt;li&gt;Your situation is potentially even a little more complicated still, because your values are only &lt;code&gt;0, 1, 2&lt;/code&gt;.  This can be done using &lt;a href=&quot;http://en.wikipedia.org/wiki/Copula_%28probability_theory%29&quot; rel=&quot;nofollow&quot;&gt;copulas&lt;/a&gt; (search the site, I think there are some informative threads somewhere).  Alternatively, you could list the probabilities of the nine possible combinations, use &lt;code&gt;sample()&lt;/code&gt; (similarly to what you have already), and then assign the associated values.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-06-20T17:51:53.607" Id="62212" LastActivityDate="2013-06-20T17:51:53.607" OwnerUserId="7290" ParentId="62208" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;If you have no training data even remotely close to the test data then RF cannot hope to predict it with any degree of accuracy. This stems from the way in which RF's work - which is how tree models work. The predicted value from a single tree for an observation is the mean value of the response variable of all the training set samples in the node of the tree to which the predicted sample is assigned. RFs extend this to many trees (so you average over $n$ trees, not just 1) and involve bagging and random selection of variables for each split. But the fundamental feature is an average of the training samples most similar to the predicted sample. Hence you can only operate within the range of the observed response data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-20T17:01:46.690" Id="62228" LastActivityDate="2013-06-20T17:10:28.800" OwnerDisplayName="Gavin Simpson" OwnerUserId="1390" ParentId="62227" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;I was not familiar with the definition of communicating classes for Markov chains, but found agreement in the definitions given on &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_chain#Reducibility&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; and on this &lt;a href=&quot;http://www.statslab.cam.ac.uk/~yms/M2_2.pdf&quot; rel=&quot;nofollow&quot;&gt;webpage&lt;/a&gt; from the University of Cambridge.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that $\{X_n\}_{n\geq 0}$ is a time homogenous Markov Chain. Both sources state a set of states $C$ of a Markov Chain is a communicating class if all states in $C$ communicate.  However, for two states, $i$ and $j$, to communicate, it is only necessary that there exists $n&amp;gt;0$ and $n^\prime&amp;gt;0$ such that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(X_n=i|X_0=j)&amp;gt;0&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(X_n^{\prime}=j|X_0=i)&amp;gt;0&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is not necessary that $n=n^\prime = 1$ as stated by @Varunicarus. As you mentioned, this Markov chain is indeed irreducible and thus all states of the Markov chain form a single communicating class, which is actually the definition of irreducibility given in the Wikipedia entry.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It is often helpful for problems with small transition matrices like this to draw a directed graph of the Markov chain and see if you can find a cycle that includes all states of the Markov Chain.  If so, the chain is irreducible and all states form a single communicating class.  For larger transition matrices, more theory and\or computer programming will be necessary.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-21T05:40:40.220" Id="62245" LastActivityDate="2013-06-21T05:40:40.220" OwnerUserId="12518" ParentId="58042" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="62286" AnswerCount="1" Body="&lt;p&gt;Why the peak of density plot in the data given below is higher than 1?  Shouldn't it be below 1?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x = c(0.43,0.71,0.6,0.56,0.14,0.38,0.71,0.33,0.09,0.8,0.62,0.33,0.12,0.6,0.4,0.56,0.33,0.75,0.4,0.43,0.75,0.75,0.12,0.09,0.54,1,0.46,0.33,0.33,1,0.5,0.52,1,0.25,0.2,0.71,0.6,0.54,0.75,0.67,0.2,0,0.33,0.73,0.4,0.5,0.56,0.83,0.67,0.6)&#10;&#10;plot(density(x))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I multiply all the values by 10 or 100 using &lt;code&gt;plot(density(x*10))&lt;/code&gt;, the peak is below 1. &lt;/p&gt;&#10;" ClosedDate="2013-06-21T16:49:49.067" CommentCount="0" CreationDate="2013-06-21T16:27:49.210" Id="62284" LastActivityDate="2013-06-21T16:40:27.277" OwnerUserId="18164" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;density&gt;" Title="Why `Density plot` peak value is higher than 1?" ViewCount="121" />
  <row AnswerCount="0" Body="&lt;p&gt;I have fit a mixed model with &lt;code&gt;lmer()&lt;/code&gt; and am left with 4 significant interaction terms. &lt;/p&gt;&#10;&#10;&lt;p&gt;There were found by removing the interaction term and comparing with the full model using &lt;code&gt;anova(fm1, fm2)&lt;/code&gt;. I have also left the single terms in the model. &lt;/p&gt;&#10;&#10;&lt;p&gt;When I come to report the chi-square for the single terms by removing them and using &lt;code&gt;anova(fm1, fm2)&lt;/code&gt; I am told there is 0 df and chi-square is 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't see how this is possible: is my model over-fit?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-21T19:33:28.593" Id="62304" LastActivityDate="2013-06-21T22:11:45.083" LastEditDate="2013-06-21T22:11:45.083" LastEditorUserId="22047" OwnerUserId="13038" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;lmer&gt;&lt;glmm&gt;" Title="Overfitting in GLMM" ViewCount="149" />
  
  <row AcceptedAnswerId="62335" AnswerCount="1" Body="&lt;p&gt;Suppose we draw a sample of $k$ out of $n$ numbered balls without replacement and get a sample of $N_1$ (unique) balls. We then replace all the balls in the urn and repeat the drawing a second time to get again $N_2$ (unique) balls. &lt;/p&gt;&#10;&#10;&lt;p&gt;We are interested in the set of balls that end up being drawn exactly once (over the course of the two experiments).&#10;It's clear that the probability, for one of the balls, of the event &quot;being drawn exactly once&quot; is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$2k/n(1-k/n)$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;which is maximized at $k^*=n/2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider the same experiment but &lt;em&gt;with replacement&lt;/em&gt; and we look again for the value of $k$ (denoted now $\bar{k}^*$) that maximizes the frequency of the event &quot;being drawn exactly once&quot;. Now, the expected number of distinct balls in each draw is $1-e^{-1}\approx0.632n$ and so I would have expected $\bar{k}^*=k^*/(1-e^{-1})\approx0.79n$. Running some simulations I get that the actual location of $\bar{k}^*$ seems to be much lower, probably between $0.65n$ and $0.7n$. I have a hard time understanding where is my mistake.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally (second question) I'm wondering what is the distribution function of the proportion of balls that are drawn exactly once in the second experiment (the one with replacements) as a function of $k$? Through numerical experimentations, I get the following curve ($n=100$):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7ZPVn.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h1&gt;EDIT&lt;/h1&gt;&#10;&#10;&lt;p&gt;here is the R code to reproduce the example above:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fx02&amp;lt;-function(ll,n,k){&#10;    a1&amp;lt;-matrix(0,n,2)&#10;    a1[sample(1:n,k,replace=TRUE),1]&amp;lt;-1&#10;    a1[sample(1:n,k,replace=TRUE),2]&amp;lt;-1&#10;    sum(rowSums(a1)==1)/n&#10;}&#10;&#10;ss&amp;lt;-(1:60)*5   #The grid of values of k for which we'll compute the probability.&#10;a4&amp;lt;-matrix(NA,length(ss),2)&#10;for(i in 1:length(ss)){&#10;    a2&amp;lt;-ss[i]&#10;    a3&amp;lt;-c(lapply(1:1000,fx02,n=100,k=a2),recursive=TRUE);&#10;    a4[i,]&amp;lt;-c(a2,mean(a3))&#10;}&#10;plot(a4,xlab=&quot;k&quot;,ylab=&quot;frequency of distinct draw&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2013-06-22T00:24:50.193" FavoriteCount="4" Id="62317" LastActivityDate="2013-06-22T20:29:59.580" LastEditDate="2013-06-22T14:53:06.427" LastEditorUserId="603" OwnerUserId="603" PostTypeId="1" Score="6" Tags="&lt;probability&gt;&lt;bootstrap&gt;" Title="Bootstrap size and probability of drawing distinct observations" ViewCount="496" />
  
  
  <row Body="&lt;p&gt;Expanding on  @Nick 's correct answer, I think you have three choices:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) You could code &quot;gesture&quot; as a single indicator variable (I had not heard that objection to 'dummy')&lt;/p&gt;&#10;&#10;&lt;p&gt;2) You could code each particular gesture type as an indicator variable, and if all of them are 0 then there is no gesture at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) You could run two analyses, one for communication where there is a gesture and one where there is not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of these, 1) is completely standard but may not answer your question. 3) is also fine if there is enough data to split the data set. 2) is a little less standard in my experience, but I do not see a problem with it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-22T12:26:21.770" Id="62333" LastActivityDate="2013-06-22T12:26:21.770" OwnerUserId="686" ParentId="62329" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;To extend Peter Flom's counting answer, assume we know (by counting or some other source), the following things:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$P(A) = \frac{1}{6}$,&lt;/li&gt;&#10;&lt;li&gt;$P(B) = \frac{11}{36}$, and&lt;/li&gt;&#10;&lt;li&gt;$P(A|B) = \frac{5}{11}$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Bayes' theorem lets us calculate P(B|A) from those values in a fairly straightforward way. Namely, it says that:&#10;$$P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}$$&#10;so, you'd plug in those values and get&#10;$$P(B|A) = \frac{\frac{5}{11} \cdot \frac{11}{36}}{\frac{1}{6}}= \frac{5}{6}$$&#10;This matches what you'd get by counting (see Peter Flom's answer for the enumeration). If you knew $P(B|A)$ instead, you could use a similar procedure to calculate $P(A|B)$, but you still need to do some counting to find  $P(A)$, $P(B)$ and one of $P(A|B)$ or $P(B|A)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your example is a good homework exercise, but we often find ourselves in a situation where $P(B|A)$ would be useful to know, but incredibly difficult to calculate, while $P(A|B)$ is less useful but easier to estimate. Bayes' rule lets convert the former situations into the latter. For example, we'd very much like to know P(email is spam | its contents), which is hard to recover directly. However, given a lot of labelled emails, we can easily estimate P(email contains a certain word | the source email is spam), just by counting how often a word occurs in spam and non-spam emails and the overall rate of spam.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-22T20:18:26.300" Id="62354" LastActivityDate="2013-07-10T18:58:16.603" LastEditDate="2013-07-10T18:58:16.603" LastEditorUserId="7250" OwnerUserId="7250" ParentId="62350" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="62385" AnswerCount="2" Body="&lt;p&gt;I have found that for some datasets, mean removal and variance scaling helps to fit a better model to data while for some datasets this does not help.&lt;/p&gt;&#10;&#10;&lt;p&gt;On what kind of data standardization will be helpful?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there some guidelines for applying this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-23T02:05:07.260" Id="62373" LastActivityDate="2013-07-07T20:13:23.083" LastEditDate="2013-07-07T14:35:26.567" LastEditorUserId="26102" OwnerUserId="26102" PostTypeId="1" Score="3" Tags="&lt;standardization&gt;" Title="For what kind of features will standardization be helpful?" ViewCount="275" />
  
  
  
  
  
  <row AcceptedAnswerId="62414" AnswerCount="1" Body="&lt;p&gt;When $X$ is $X_1$,...$X_n$, how do you express the following negative binomial distribution  in an exponential family form?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(k;r,p)\equiv\text{Pr}(X=k)=\binom{k+r-1}{k}(1-p)^{r}p^k~~~~~\text{for}~k=0,1,2,\ldots&#10;$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-23T14:21:31.207" Id="62409" LastActivityDate="2013-07-04T16:12:24.123" LastEditDate="2013-06-23T15:38:44.270" LastEditorUserId="21054" OwnerUserId="16934" PostTypeId="1" Score="2" Tags="&lt;negative-binomial&gt;&lt;exponential-family&gt;" Title="How should you express a negative binomial distribution in an exponential family form?" ViewCount="1865" />
  
  
  
  <row Body="&lt;p&gt;It is certainly allowable to add covariates to an equation, and to add the interaction if you like. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can keep the effect size in units of Y, there is nothing wrong with that. If you can't interpret an effect size in Y units, then you might not understand your variables. But you can also use standardized betas (most computer packages will output it for you). I don't find these particularly useful, but some people do. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-23T20:39:22.303" Id="62425" LastActivityDate="2013-06-23T20:39:22.303" OwnerUserId="686" ParentId="62415" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Empirical_distribution_function&quot; rel=&quot;nofollow&quot;&gt;empirical cumulative distribution function&lt;/a&gt; $ecdf$, which tells you what fraction of all scores are equal to or below some score.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say your scores are $\{1, 1, 1, 1, 3, 3, 3, 4, 5, 5, 5, 6, 6, 6, 8, 8, 9, 9, 9, 9\}$. Then $ecdf(3)=0.35$, meaning that student is in the top 65% of the class, there are 35% equal or below.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R you can do it like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a &amp;lt;- c(8, 6, 9, 8, 3, 5, 5, 9, 1, 6, 3, 1, 9, 5, 3, 1, 1, 4, 9, 6) # sample data&#10;f &amp;lt;- ecdf(a) # compute the ecdf&#10;f(3) # how many at or below 3?&#10;[1] 0.35 # 35%&#10;f(a) # check all students&#10;[1] 0.80 0.70 1.00 0.80 0.35 0.55 0.55 1.00 0.20 0.70 0.35 0.20 1.00 0.55 0.35 0.20 0.20 0.40 1.00 0.70&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-06-24T01:27:31.280" Id="62435" LastActivityDate="2013-06-24T01:27:31.280" OwnerUserId="11458" ParentId="62431" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="62452" AnswerCount="1" Body="&lt;p&gt;I recently asked about the Mahalanobis distance and I got pretty good answers in this post: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahanalobis-distance&quot;&gt;Bottom to top explanation of the Mahanalobis distance?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I got the idea, but what I still felt missing was the derivation of the formula for the Mahalanobis distance. So my question is: &quot;How does one derive the formula for Mahalanobis distance?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Why does the formula have the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$D(\textbf{x},\textbf{y})=\sqrt{ (\textbf{x}-\textbf{y})^TC^{-1}(\textbf{x}-\textbf{y})} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Could perhaps someone give analogous derivation as user @sjm.majewski gave on Principal component analysis on the link below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/33654#33654&quot;&gt;Making sense of principal component analysis, eigenvectors &amp;amp; eigenvalues&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: &lt;/p&gt;&#10;&#10;&lt;p&gt;From Wikipedia intuitive explanation was:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;The Mahalanobis distance is simply the distance of the test point from the center of mass divided by the width of the ellipsoid in the direction of the test point.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;So is $C^{-1}$ the width of the ellipsoid in the direction of the test point? I mean this distance I can understand: $$\displaystyle\frac{\textbf{x}-\textbf{u}}{\sigma}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But this distance confuses me...:/ &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt{ (\textbf{x}-\textbf{y})^TC^{-1}(\textbf{x}-\textbf{y})} $$&lt;/p&gt;&#10;" ClosedDate="2013-06-24T17:01:55.583" CommentCount="3" CreationDate="2013-06-24T05:10:56.997" Id="62438" LastActivityDate="2013-06-24T09:53:56.547" LastEditDate="2013-06-24T05:40:30.300" LastEditorUserId="18528" OwnerUserId="18528" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;distance&gt;&lt;pattern-recognition&gt;" Title="The derivation of the Mahalanobis distance formula" ViewCount="417" />
  <row Body="A weighted mean (or weighted average) is like an ordinary mean, but the observations don't contribute equally - more emphasis is placed on some data values than others; they are weighted by a bigger or smaller amount than 1/n." CommentCount="0" CreationDate="2013-06-24T06:59:48.063" Id="62442" LastActivityDate="2013-06-24T07:59:55.290" LastEditDate="2013-06-24T07:59:55.290" LastEditorUserId="805" OwnerUserId="805" PostTypeId="4" Score="0" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; I don't know much about association rule mining. &lt;/p&gt;&#10;&#10;&lt;p&gt;That being said from the definitions you've given you can't know the answers to questions 1, 2, or 3 from the given information. To see this simply note that you can have&#10;$$&#10;P(A \cup C) = 0.9&#10;$$&#10;for all $P(A), P(C), P(A \cap C) \in [0,1]$ such that &#10;$$&#10;\begin{align*}&#10;P(A) + P(C) - P(A \cap C) &amp;amp;= 0.9 &amp;amp;\text{with}\\&#10;P(A), P(C) &amp;amp;\ge P(A \cap C),&#10;\end{align*}&#10;$$&#10;for which you can find infinitely many solutions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-24T13:16:05.093" Id="62465" LastActivityDate="2013-06-24T13:16:05.093" OwnerUserId="6248" ParentId="62437" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;My guess based on your commentary:&lt;/p&gt;&#10;&#10;&lt;p&gt;Once the synthetic data is much bigger than the real-life data, the correlation coefficient is being weighted mainly for the synthetic data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Your real life-data are probably outliers in comparison with synthetic data values.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the reason you are getting low r² and probably high RMSE on validation phase.&lt;br&gt;&#10;Putting in another way: it was almost like fitting a model to a dataset A, and try to validate it with a dataset B with very different features from A.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-24T16:15:23.163" Id="62479" LastActivityDate="2013-06-24T16:15:23.163" OwnerUserId="22468" ParentId="61364" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to fit a linear model to a price response variable. Many of the predictor variables consist of mainly zeros. For example, one possible predictor variable is &quot;drill holes&quot;. Not many parts have a drilled hole, but if they do it would make sense that it affects the price. I am using the &lt;code&gt;caret&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; to train the model and choose the appropriate variables. I have already removed all variables with zero variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have found a lot of literature about count data for a response variable with many zeros and zero-inflation models. But what I am wondering is, how should explanatory variables with many zeros (many are NOT count data) be handled? Is there an appropriate transformation? Or are explanatory variables with many zeros allowable since I am dealing with explanatory variables and not the response variable?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-24T17:51:23.777" FavoriteCount="1" Id="62485" LastActivityDate="2013-06-24T19:14:43.187" LastEditDate="2013-06-24T19:08:17.613" LastEditorUserId="22047" OwnerUserId="27248" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;regression&gt;&lt;predictive-models&gt;&lt;linear-model&gt;&lt;zero-inflation&gt;" Title="Explanatory variables with many zeros" ViewCount="312" />
  <row AnswerCount="1" Body="&lt;p&gt;I have an experiment that is as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;4 years of data (1 year pre-treatment, 3 years during treatment)&lt;/li&gt;&#10;&lt;li&gt;20 plots total&lt;/li&gt;&#10;&lt;li&gt;5 plots of each type (Control, Treatment A, Treatment B, Treatment A+B)&lt;/li&gt;&#10;&lt;li&gt;Each plots is divided into 4 quadrants in which measurements are recorded.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Initial stats revealed pretreatment differences between plot types.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data I would like to use in a linear mixed-effects model include:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;my response variable;&lt;/li&gt;&#10;&lt;li&gt;BA - before/after (as &quot;1&quot; or &quot;2&quot;);&lt;/li&gt;&#10;&lt;li&gt;Treatment A (as &quot;yes&quot; or &quot;no&quot;);&lt;/li&gt;&#10;&lt;li&gt;Treatment B (as &quot;yes&quot; or &quot;no&quot;);&lt;/li&gt;&#10;&lt;li&gt;The interaction of treatments and Year; &lt;/li&gt;&#10;&lt;li&gt;Plot and quadrant as nested random effects.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I've created a linear mixed effects model using the nlme package in R that currently looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model&amp;lt;-lme(response variable ~ BA * treatmentA * treatmentB * year ,&#10;           method= &quot;REML&quot;, data=data, random=~1|plot/quad, na.action=na.omit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get the following error message:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Error in MEEM(object, conLin, control$niterEM) : &#10;    Singularity in backsolve at level 0, block 1&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I can run the model successfully without BA or without year but am unable to get results when I run it with both of them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice or suggestions on how to account for pretreatment differences and get my LME to work happily?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-24T18:01:28.890" FavoriteCount="2" Id="62486" LastActivityDate="2013-06-24T22:01:47.370" LastEditDate="2013-06-24T18:31:20.840" LastEditorUserId="6029" OwnerUserId="27249" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lme&gt;" Title="Accounting for pretreatment differences (linear mixed effects model) using R/nlme" ViewCount="149" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a dataset of roughly 500 features and am training a binary classifier using GBM - gradient boosted machines, an ensemble of decision trees.  Of these 500 variables, I am sure some are highly correlated with each other, though probably not to the extent where they are linearly dependent.  For example, one variable might be average age of people in city X which was collected by survey 1, and another variable is the average age of people in city X collected by survey 2.  How does such a massive set of features affect the decision trees?  In the regression setting, this should increase prediction variance, but can also be mitigated by regularization.     &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-24T18:52:49.773" Id="62490" LastActivityDate="2013-06-25T07:02:33.567" OwnerUserId="8067" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;feature-selection&gt;&lt;regularization&gt;&lt;gbm&gt;" Title="Effect of features that are highly correlated with each other on a decision tree" ViewCount="365" />
  <row Body="&lt;p&gt;My interpretation is as follows and someone please correct me if my explanation is not useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Factor analysis&lt;/strong&gt; is a good tool to reduce dimensions when you are also interested in latent variables assessed by those factors. For example, perhaps I could reduce the number of items in a measure that assesses depression (a latent construct). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Principal component analysis&lt;/strong&gt; is useful when you want to do dimension reduction and you are less interested in latent factors. For example, perhaps you want to merely use two or three factors out of eight that best &quot;captures&quot; what the eight factors measure. In this case you are less interested in how these factors do or do not measure a latent construct. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are many resources explaining the mathematical differences between these methods, but hopefully this can be useful. Tabachnick &amp;amp; Fidell wrote the following: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If you are interested in a theoretical solution uncontaminated by&#10;  unique and error variability and have designed your study on the basis&#10;  of underlying constructs that are expected to produce scores on your&#10;  observed variables, FA is your choice. If, on the other hand, you&#10;  simply want an empirical summary of the data set, PCA is the better&#10;  choice.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Tabachnick, B. G. &amp;amp; Fidell, L. S. (2007). &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0205849571&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Using multivariate statistics&lt;/em&gt;&lt;/a&gt; (5th ed.). Boston: Pearson Education.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-24T22:27:27.043" Id="62501" LastActivityDate="2013-06-24T22:40:09.183" LastEditDate="2013-06-24T22:40:09.183" LastEditorUserId="7290" OwnerUserId="3262" ParentId="62499" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;Say I want to do a logistic regression on whether it snows on a given month of the year and day of the week.  I'd have 12 months and 7 days.  My understanding is that this would translate into 17 dummies (11 for months and 6 for days).  &lt;/p&gt;&#10;&#10;&lt;p&gt;First off is this correct?  Second, how would I interpret regression results (i.e. assuming I dropped January, what's the p-value for January)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-24T23:41:57.110" Id="62503" LastActivityDate="2013-06-25T15:40:25.160" LastEditDate="2013-06-24T23:44:23.793" LastEditorUserId="22047" OwnerUserId="7411" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;categorical-data&gt;" Title="Can I use multiple groups of dummy variables? How do I interpret results for missing groups?" ViewCount="144" />
  <row Body="&lt;p&gt;As noted (for example, &lt;a href=&quot;http://en.wikipedia.org/wiki/Mean_absolute_percentage_error&quot; rel=&quot;nofollow&quot;&gt;in Wikipedia&lt;/a&gt;), MAPE can be problematic. Most pointedly, it can cause division-by-zero errors. My guess is that this is why it is not included in the sklearn metrics. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, it is simple to implement. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from sklearn.utils import check_arrays&#10;def mean_absolute_percentage_error(y_true, y_pred): &#10;    y_true, y_pred = check_arrays(y_true, y_pred)&#10;&#10;    ## Note: does not handle mix 1d representation&#10;    #if _is_1d(y_true): &#10;    #    y_true, y_pred = _check_1d_array(y_true, y_pred)&#10;&#10;    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Use like any other metric...: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; y_true = [3, -0.5, 2, 7]; y_pred = [2.5, -0.3, 2, 8]&#10;&amp;gt; mean_absolute_percentage_error(y_true, y_pred)&#10;Out[19]: 17.738095238095237&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Note that I'm multiplying by 100 and returning a percentage.) &lt;/p&gt;&#10;&#10;&lt;p&gt;... but with caution: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; y_true = [3, 0.0, 2, 7]; y_pred = [2.5, -0.3, 2, 8]&#10;&amp;gt; #Note the zero in y_pred&#10;&amp;gt; mean_absolute_percentage_error(y_true, y_pred)&#10;-c:8: RuntimeWarning: divide by zero encountered in divide&#10;Out[21]: inf&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-06-25T01:04:42.873" Id="62511" LastActivityDate="2014-01-21T19:35:11.987" LastEditDate="2014-01-21T19:35:11.987" LastEditorUserId="5387" OwnerUserId="5387" ParentId="58391" PostTypeId="2" Score="4" />
  
  <row Body="Pairs of (x,y) values plotted as points in Cartesian coordinates. Widely used as an exploratory and diagnostic tool." CommentCount="0" CreationDate="2013-06-25T05:30:21.780" Id="62518" LastActivityDate="2013-06-25T12:55:33.323" LastEditDate="2013-06-25T12:55:33.323" LastEditorUserId="805" OwnerUserId="805" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;The margin of error is usually defined as a particular way of expressing a confidence interval - see &lt;a href=&quot;http://en.wikipedia.org/wiki/Margin_of_error&quot; rel=&quot;nofollow&quot;&gt;Wikipedia's definition&lt;/a&gt;, for example.  So you are not really being asked to do anything other than re-express what the newspaper has already stated.  By changing from 65% plus or minus 4 percentage points to 0.65 plus or minus 0.04 you have only changed it from percentages to proportions, by dividing by 100 (which obviously has no importance).&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking again at your question I suspect it is actually exactly this relationship between a percentage and a proportion that is being tested here (as it is several months old I don't mind giving the answer away), not really the confidence interval issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;Exactly what a &quot;confidence interval&quot; means is another matter (see @gung's comment).  The usual explanation is that if you performed this exercise (sampling a large number of voters etc) many times and used the same technique each time to estimate your confidence interval, 90% of the time the confidence interval would include the true but unknown value.&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW if only newspapers were as clear as this, stating that the margin of error is a certain number of &quot;percentage points&quot; - all too often they omit the crucial &quot;points&quot;, which leaves a significant ambiguity in meaning.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-25T06:35:42.443" Id="62520" LastActivityDate="2013-06-25T06:35:42.443" OwnerUserId="7972" ParentId="57275" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It is valid to find the expected number of days between visits and then divide the number of days in a month by the expected days between visits to get the vitis per month &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-25T09:13:31.340" Id="62525" LastActivityDate="2013-06-25T09:13:31.340" OwnerUserId="27271" ParentId="62512" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;strong&gt;Later&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing I want to add after hearing that you have linear mixed effect models: The $AIC, AIC_{c}$ and $BIC$ can still be used to compare the models. See &lt;a href=&quot;http://areaestadistica.uclm.es/events/ModelosMixtos.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, for example. From other similar questions on the site, it seems that &lt;a href=&quot;http://star.psy.ohio-state.edu/coglab/People/roger/pdf/mathpsy04.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; is crucial.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Original answer&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What you basically want is to &lt;strong&gt;compare two non-nested models.&lt;/strong&gt; Burnham and Anderson &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441929738&quot; rel=&quot;nofollow&quot;&gt;Model selection and multimodel inference&lt;/a&gt; discuss this and recommend using the $AIC$, $AIC_{c}$ or $BIC$ etc. as the traditional likelihood ratio test is only applicable in nested models. They explicitly state that the information-theoretic criteria such as the $AIC, AIC_{c}, BIC$ etc. are &lt;em&gt;not&lt;/em&gt; tests and that the word &quot;significant&quot; should be &lt;em&gt;avoided&lt;/em&gt; when reporting the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on &lt;a href=&quot;http://stats.stackexchange.com/a/8558/21054&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/a/8519/21054&quot;&gt;this&lt;/a&gt; answers, I recommend these approaches:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Make a scatterplot matrix (SPLOM) of your dataset including smoothers: &lt;code&gt;pairs(Y~X1+X2, panel = panel.smooth, lwd = 2, cex = 1.5, col = &quot;steelblue&quot;, pch=16)&lt;/code&gt;. Check if the lines (the smoothers) are compatible with a linear relationship. &lt;a href=&quot;http://stats.stackexchange.com/a/61226/21054&quot;&gt;Refine the model&lt;/a&gt; if necessary.&lt;/li&gt;&#10;&lt;li&gt;Compute the models &lt;code&gt;m1&lt;/code&gt; and &lt;code&gt;m2&lt;/code&gt;. Do some model checks (residuals etc.): &lt;code&gt;plot(m1)&lt;/code&gt; and &lt;code&gt;plot(m2)&lt;/code&gt;.&lt;/li&gt;&#10;&lt;li&gt;Compute the $AIC_{c}$ ($AIC$ corrected for small sample sizes) for both models and calculate the absolute difference between the two $AIC_{c}$s. The &lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;http://cran.r-project.org/web/packages/pscl/index.html&quot; rel=&quot;nofollow&quot;&gt;package &lt;code&gt;pscl&lt;/code&gt;&lt;/a&gt; provides the function &lt;code&gt;AICc&lt;/code&gt; for this: &lt;code&gt;abs(AICc(m1)-AICc(m2))&lt;/code&gt;. If this absolute difference is smaller than 2, the two models are basically indistinguishable. Otherwise prefer the model with the &lt;em&gt;lower $AIC_{c}$&lt;/em&gt;.&lt;/li&gt;&#10;&lt;li&gt;Compute likelihood ratio tests for non-nested models. The &lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;http://cran.r-project.org/web/packages/lmtest/index.html&quot; rel=&quot;nofollow&quot;&gt;package &lt;code&gt;lmtest&lt;/code&gt;&lt;/a&gt; has the functions &lt;code&gt;coxtest&lt;/code&gt; (Cox test), &lt;code&gt;jtest&lt;/code&gt; (Davidson-MacKinnon J test) and &lt;code&gt;encomptest&lt;/code&gt; (encompassing test of Davidson &amp;amp; MacKinnon).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Some thoughts:&lt;/strong&gt; If the two banana-measures are &lt;em&gt;really&lt;/em&gt; measure the same thing, they both may be &lt;em&gt;equally suited&lt;/em&gt; for prediction and there might not be a &quot;best&quot; model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.rochester.edu/college/psc/clarke/450315.pdf&quot; rel=&quot;nofollow&quot;&gt;This paper&lt;/a&gt; might also be helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an exmple in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#==============================================================================&#10;# Generate correlated variables&#10;#==============================================================================&#10;&#10;set.seed(123)&#10;&#10;R &amp;lt;- matrix(cbind(&#10;  1   , 0.8 , 0.2,&#10;  0.8 , 1   , 0.4,&#10;  0.2 , 0.4 , 1),nrow=3) # correlation matrix&#10;U &amp;lt;- t(chol(R))&#10;nvars &amp;lt;- dim(U)[1]&#10;numobs &amp;lt;- 500&#10;set.seed(1)&#10;random.normal &amp;lt;- matrix(rnorm(nvars*numobs,0,1), nrow=nvars, ncol=numobs);&#10;X &amp;lt;- U %*% random.normal&#10;newX &amp;lt;- t(X)&#10;raw &amp;lt;- as.data.frame(newX)&#10;names(raw) &amp;lt;- c(&quot;response&quot;,&quot;predictor1&quot;,&quot;predictor2&quot;)&#10;&#10;#==============================================================================&#10;# Check the graphic&#10;#==============================================================================&#10;&#10;par(bg=&quot;white&quot;, cex=1.2)&#10;pairs(response~predictor1+predictor2, data=raw, panel = panel.smooth,&#10;      lwd = 2, cex = 1.5, col = &quot;steelblue&quot;, pch=16, las=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xWyNf.png&quot; alt=&quot;SPLOM&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The smoothers confirm the linear relationships. This was intended, of course.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#==============================================================================&#10;# Calculate the regression models and AICcs&#10;#==============================================================================&#10;&#10;library(pscl)&#10;&#10;m1 &amp;lt;- lm(response~predictor1, data=raw)&#10;m2 &amp;lt;- lm(response~predictor2, data=raw)&#10;&#10;summary(m1)&#10;&#10;Coefficients:&#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -0.004332   0.027292  -0.159    0.874    &#10;predictor1   0.820150   0.026677  30.743   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 0.6102 on 498 degrees of freedom&#10;Multiple R-squared:  0.6549,    Adjusted R-squared:  0.6542 &#10;F-statistic: 945.2 on 1 and 498 DF,  p-value: &amp;lt; 2.2e-16&#10;&#10;summary(m2)&#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -0.01650    0.04567  -0.361    0.718    &#10;predictor2   0.18282    0.04406   4.150 3.91e-05 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 1.021 on 498 degrees of freedom&#10;Multiple R-squared:  0.03342,   Adjusted R-squared:  0.03148 &#10;F-statistic: 17.22 on 1 and 498 DF,  p-value: 3.913e-05&#10;&#10;AICc(m1)&#10;[1] 928.9961&#10;&#10;AICc(m2)&#10;[1] 1443.994&#10;&#10;abs(AICc(m1)-AICc(m2))&#10;[1] 514.9977&#10;&#10;#==============================================================================&#10;# Calculate the Cox test and Davidson-MacKinnon J test&#10;#==============================================================================&#10;&#10;library(lmtest)&#10;&#10;coxtest(m1, m2)&#10;&#10;Cox test&#10;&#10;Model 1: response ~ predictor1&#10;Model 2: response ~ predictor2&#10;                Estimate Std. Error   z value  Pr(&amp;gt;|z|)    &#10;fitted(M1) ~ M2   17.102     4.1890    4.0826 4.454e-05 ***&#10;fitted(M2) ~ M1 -264.753     1.4368 -184.2652 &amp;lt; 2.2e-16 ***&#10;&#10;jtest(m1, m2)&#10;&#10;J test&#10;&#10;Model 1: response ~ predictor1&#10;Model 2: response ~ predictor2&#10;                Estimate Std. Error t value  Pr(&amp;gt;|t|)    &#10;M1 + fitted(M2)  -0.8298   0.151702  -5.470 7.143e-08 ***&#10;M2 + fitted(M1)   1.0723   0.034271  31.288 &amp;lt; 2.2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The $AIC_{c}$ of the first model &lt;code&gt;m1&lt;/code&gt; is clearly lower and the $R^{2}$ is much higher.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; In &lt;em&gt;linear&lt;/em&gt; models of &lt;em&gt;equal complexity&lt;/em&gt; and &lt;em&gt;Gaussian&lt;/em&gt; error distribution, $R^2, AIC$ and $BIC$ should give the same answers (see &lt;a href=&quot;http://stats.stackexchange.com/questions/61505/why-the-r2-aic-and-bic-criteria-give-different-best-model-for-models-of-eq&quot;&gt;this post&lt;/a&gt;). In &lt;em&gt;nonlinear&lt;/em&gt; models, the use of $R^2$ for model performance (goodness of fit) and model selection should be &lt;em&gt;avoided:&lt;/em&gt; see &lt;a href=&quot;http://stats.stackexchange.com/questions/60403/behavior-of-r2-in-non-linear-models/60449#60449&quot;&gt;this post&lt;/a&gt; and &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892436/pdf/1471-2210-10-6.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, for example. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-25T10:57:35.150" Id="62534" LastActivityDate="2013-06-25T15:17:00.597" LastEditDate="2013-06-25T15:17:00.597" LastEditorUserId="21054" OwnerUserId="21054" ParentId="62524" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;For confidence interval, I think you can just use the logistic regression outputs.&#10;For adjust p-values, refer to this wiki link: &lt;a href=&quot;http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To my understanding, you are first test an overall age group, then testing a nested hypothesis. I would recommend you test overall group effects first, then go deeper to test individual groups.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-25T14:34:49.920" Id="62557" LastActivityDate="2013-06-25T14:34:49.920" OwnerUserId="10449" ParentId="62500" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="83275" AnswerCount="2" Body="&lt;p&gt;I am wondering if someone knows how to compare two directed network graphs with the same node IDs. &lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, in graph A, each node represents an individual, and each edge represents some sort of relationship A between two individuals. In graph B, the same population is represented by the nodes, and each edge represents a relationship B between two individuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I test for significant difference between these two graphs? Both graphs are directed. What if the graphs are weighted?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently using igraph in R, but am looking for more of a statistical answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks so much! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-25T16:22:47.093" FavoriteCount="1" Id="62573" LastActivityDate="2014-01-24T21:33:40.490" OwnerUserId="27292" PostTypeId="1" Score="4" Tags="&lt;statistical-significance&gt;&lt;networks&gt;" Title="Significant difference between two directed network graphs with the same vertex IDs" ViewCount="183" />
  
  <row Body="&lt;p&gt;You say that nothing is known about the distribution, but you also say that the values are non-negative integers, so that tells you something, can you learn more about the theory?&lt;/p&gt;&#10;&#10;&lt;p&gt;An exact permutation test on your data gives a p-value of 0.10 and does not require assumptions about the distribution (just testing that it is identical for the 2 groups).&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data represent counts (non-negative integers) then a Poisson model may be appropriate (which gives a p-value less than 0.001), but make sure that the assumptions are reasonable.  &lt;/p&gt;&#10;&#10;&lt;p&gt;As @gung mentions, there are many ways to look at this and which is best is going to depend on the source of the data and the science that underlies it.  learning about the data, talking to the person that gave you the data, talking to other experts, etc. is going to be of the most benefit.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really cannot learn anything more about the underlying distribution or approximations to it then you may have to resort to &lt;code&gt;SnowsCorrectlySizedButOtherwiseUselessTestOfAnything&lt;/code&gt; (a function in the TeachingDemos package for R) which will give a p-value without requiring any assumptions about your data.  But note that that function is considered less useful than its documentation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-25T19:53:39.693" Id="62588" LastActivityDate="2013-06-25T19:53:39.693" OwnerUserId="4505" ParentId="62558" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using the &lt;code&gt;caret&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; that provides two kind of metrics for classification problems - accuracy and kappa. When should I prefer one over another?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-25T20:44:11.620" Id="62593" LastActivityDate="2013-06-25T22:40:18.600" LastEditDate="2013-06-25T22:40:18.600" LastEditorUserId="22047" OwnerUserId="21560" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;metric&gt;&lt;kappa&gt;" Title="When should I prefer kappa over accuracy for binary classification problems?" ViewCount="72" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to say something about sales of candy bars. I got data on sales for a population of children. There are three factors: age, income of parents, and name of a child. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I can compute from this data is, say, that 34% of all sold candy bars were bought by children of age 12, 25% by children of age 10, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;At the same time, I can compute that 52% were bought by children coming from families with monthly income X, 35% by children coming from families with monthly income Y, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;And finally, I know that say 40% of the bars were bought by Sams, 25% by Abigails, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I would like to deduce from these three characteristics is, given a child of certain age and name, and coming from a family with monthly income Z, what is the &quot;probability&quot; that he/she will buy a candy bar? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-25T20:45:16.983" Id="62594" LastActivityDate="2013-06-25T22:31:22.360" LastEditDate="2013-06-25T21:30:45.750" LastEditorUserId="22047" OwnerUserId="26790" PostTypeId="1" Score="0" Tags="&lt;predictive-models&gt;&lt;multiple-comparisons&gt;&lt;model&gt;" Title="Statistics on three variables/factors" ViewCount="51" />
  
  <row Body="&lt;p&gt;Would a time series intervention analysis suit your needs?  It estimates how much an intervention has changed a time series, if at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;how to in R: &lt;a href=&quot;http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;example use case: &lt;a href=&quot;http://stats.stackexchange.com/questions/43448/what-test-should-i-use-to-determine-if-a-policy-change-had-a-statistically-signi&quot;&gt;What test should I use to determine if a policy change had a statistically significant impact on website registrations?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;online course notes: &lt;a href=&quot;https://onlinecourses.science.psu.edu/stat510/?q=node/76&quot; rel=&quot;nofollow&quot;&gt;https://onlinecourses.science.psu.edu/stat510/?q=node/76&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-25T22:02:56.933" Id="62603" LastActivityDate="2013-06-25T22:02:56.933" OwnerUserId="27301" ParentId="61393" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The AR(1) error would help a lot. But if you want something more general and flexible, use the &lt;code&gt;tbats&lt;/code&gt; function instead.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-26T02:20:59.517" Id="62609" LastActivityDate="2013-06-26T02:20:59.517" OwnerUserId="159" ParentId="62579" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;After much searching, jumping around online forums, consulting with professors and doing A LOT of literature review, I have come to the conclusion that probably &lt;em&gt;THE&lt;/em&gt; only way to address this problem is through the use of vine copulas indeed. It gives you some control over the pairwise skewness and kurtosis (or any higher moments) - for a p-variate random vector and the freedom to specify p-1 pair of copulas and the remaining p*(p-1)/2 - (p-1) dimensions can be specified in some kind of conditional copula. &lt;/p&gt;&#10;&#10;&lt;p&gt;I welcome other methods people might've come across but at least I'm going to leave this pointer towards an answer because i cannot, for the life of me, find any other ways to address this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-26T07:16:11.510" Id="62611" LastActivityDate="2013-06-26T07:52:32.590" LastEditDate="2013-06-26T07:52:32.590" LastEditorUserId="6029" OwnerUserId="27089" ParentId="62146" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;One way to approach this question is called “equivalence testing”. You can look up information on that but generally speaking you cannot prove that two means are exactly equal (nor should you expect them to be in most cases) so you first have to define an equivalence margin (i.e. a difference that you consider negligible in practice).&lt;/p&gt;&#10;&#10;&lt;p&gt;Importantly, as Maarten already mentioned, a t-test is used to compare &lt;em&gt;population means&lt;/em&gt;. It does not tell you if the two distributions are the same, if the difference is big, if there is a lot of overlap or anything like that buy only if the observed difference in &lt;em&gt;means&lt;/em&gt; is likely to have come about purely through sampling variability. It also does not test if &lt;em&gt;samples&lt;/em&gt; are different (to determine that, you just need to look at your data directly). The whole point of statistical inference is that your sample is a noisy representation of a broader population. Generally speaking, you should not expect statistics to tell you if something is “true” or “real” but only to help you deal with random variation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-26T07:49:18.967" Id="62614" LastActivityDate="2013-06-26T11:01:15.857" LastEditDate="2013-06-26T11:01:15.857" LastEditorUserId="6029" OwnerUserId="6029" ParentId="62607" PostTypeId="2" Score="6" />
  
  
  
  <row Body="&lt;p&gt;You can look at these rules as two proportions (namely the proportion of people with a BMI under 28 in two different groups). Then you can test if those two proportions are significantly different. The fact that the group sizes are different is not necessarily a big problem, the test can take care of that.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, having a very small number of cases means that there is a lot of uncertainty in your estimates. Consequently, it will be more difficult to establish that the rule holds in the population your sample comes from but it does not mean that the relationship is smaller. Conversely, if you had a very large sample you would know these proportions precisely but it could very well be that they are very similar. The main thing to understand is that the strength of the relationships (represented by the proportion in your case) is distinct from the uncertainty about them that results from the limited sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two unrelated problems in the way the rules are presented though:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The thresholds seem arbitrary. What about people who exercise twice weekly? What a BMI of 30 or 25? Presumably there is some sort of quantitative relationship between exercise and BMI that is not fully captured by arbitrary thresholds.  &lt;/li&gt;&#10;&lt;li&gt;There will be some overlap between people who exercise and people who eat fast food and those two variables are also likely to be correlated.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For all these reasons, it's probably more fruitful to use the original data to build some model of the relationship between exercise, diet and BMI rather than reduce it to simple rules.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-26T16:07:14.527" Id="62652" LastActivityDate="2013-06-26T16:18:09.457" LastEditDate="2013-06-26T16:18:09.457" LastEditorUserId="6029" OwnerUserId="6029" ParentId="62647" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have read that inferences on heterogeneity should be applied with caution if there are few studies. I have 6 studies for meta analysis.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is 6 studies sufficient to assess heterogeneity of effect sizes?&lt;/li&gt;&#10;&lt;li&gt;How many studies are required to assess heterogeneity?&lt;/li&gt;&#10;&lt;li&gt;What is the consequence of having an insufficient number of studies to assess heterogeneity?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-06-26T22:02:45.763" Id="62674" LastActivityDate="2013-07-28T04:56:28.607" LastEditDate="2013-07-28T04:56:28.607" LastEditorUserId="183" OwnerUserId="27343" PostTypeId="1" Score="3" Tags="&lt;random-effects-model&gt;&lt;inference&gt;&lt;meta-analysis&gt;&lt;power&gt;" Title="How many studies are required to assess heterogeneity of effect size in meta-analysis?" ViewCount="106" />
  <row Body="&lt;p&gt;There are a number of ways to approach this. Because of the balanced design, you were able to take differences between various values of $X = X_i - X_n$ and $Y = Y_i - Y_n$. (subscript $n$ meaning nonideal, not sample size).&lt;/p&gt;&#10;&#10;&lt;p&gt;You are treating $X$ and $Y$ as independent samples and testing the hypothesis $\mu_X = \mu_Y$ with an ordinary T-test for unequal standard errors. Using this methodology consistently, you can calculate standard errors using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation&quot; rel=&quot;nofollow&quot;&gt;Satterthwaite DFs&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, had you had an unbalanced design, I would have advocated the regression modeling approach and a test of interaction. Using the full model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E[outcome | group, ideal] = \beta_0 + \beta_1 group + \beta_2 ideal + \beta_3 group*ideal$&lt;/p&gt;&#10;&#10;&lt;p&gt;versus the reduced model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E[outcome | group, ideal] = \beta_0 + \beta_1 group + \beta_2 ideal$&lt;/p&gt;&#10;&#10;&lt;p&gt;The benefit of doing this aside from allowing for unbalanced design is that you get explicit estimates of marginal effects, such as the group differences and the ideal versus nonideal differences.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-26T22:30:44.827" Id="62676" LastActivityDate="2013-06-26T22:30:44.827" OwnerUserId="8013" ParentId="62666" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;This doesn't exactly answer what are you looking for. However, I think you need to consider these issues before doing the analysis. In other words, I focus more on the conceptual part (part a) [I thought it to include in comment but this is very long] &lt;/p&gt;&#10;&#10;&lt;p&gt;With the repeated cross section data, you have here the pseudo panel data (with age group (age cohort) acting as individual effect and year as time effect) and not the true panel data. Theoretical stuffs on this has been discussed &lt;a href=&quot;http://stats.stackexchange.com/questions/32751/validity-of-pseudo-panel-data-constructed-from-repeated-cross-sectional-data-as&quot;&gt;here&lt;/a&gt;. The good news is that you can use the fixed effect, random effect,and  pooled estimator as being used for the true panel data. The use of different tests to choose one over another estimator has been discussed in the &lt;a href=&quot;http://cran.r-project.org/web/packages/plm/vignettes/plm.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;plm&lt;/code&gt;&lt;/a&gt; package manual. However, it is good to start with Introductory Econometrics by Woolridge if you are beginners in panel data. That being said, consider this Oxford &lt;a href=&quot;http://www.economics.ox.ac.uk/materials/papers/12049/paper615.pdf&quot; rel=&quot;nofollow&quot;&gt;discussion paper&lt;/a&gt; as a starting one. Once you understand the concept mentioned in the paper, it won't be difficult to apply in &lt;code&gt;R&lt;/code&gt;. Your question doesn't spell out other variables. Is that the case?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-26T22:53:51.377" Id="62681" LastActivityDate="2013-06-26T22:53:51.377" OwnerUserId="14860" ParentId="62646" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using JAGS to estimate a Dirichlet Process Mixture of Normals.  The code works well and the estimated density is accurate.  However, I would like to know which component each observation is assigned to and the corresponding parameters for that component.  This is hard due to the label switching problem in mixture models.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions?  Putting an order restriction on the means is one recommended way to avoid this but I have not been able to implement it well in JAGS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is the R code followed by the JAGS model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(R2jags)&#10;library(mixtools)&#10;&#10;nobs=200&#10;popmn=c(-6,-3,3); popstd=c(1.5,1.5,.7); popprob=c(.6,.3,.1)&#10;x=rnormmix(n=nobs,lambda=popprob,mu=popmn,sigma=popstd)&#10;xgrid=seq(-8,8,length=500)&#10;&#10;###true density&#10;densTrue=rep(0,length(xgrid))&#10;for(i in 1:length(xgrid))&#10;  densTrue[i]=sum(popprob*dnorm(xgrid[i],popmn,popstd))&#10;densTrue=densTrue/sum(densTrue)&#10;&#10;###Call JAGS&#10;K=10&#10;dataList = list(y=x,n=200,K=K,grid=xgrid,n2=length(xgrid))&#10;jags.inits &amp;lt;- function(){&#10;  list('mu'=rep(0,K),'tau'=rep(1,K),'prob'=rep(1/K,K))&#10;}&#10;parameters=c('f')&#10;jags.fit=jags(data=dataList,inits=jags.inits,parameters,&quot;jagsModel&quot;,n.chains=1,n.iter=800,n.burnin=300)&#10;jags.fit&#10;#contruct density&#10;DPDens2=jags.fit$BUGSoutput$mean$f&#10;DPDens2=DPDens2/sum(DPDens2)&#10;&#10;###Plot&#10;plot(xgrid,trueDens,type='l')&#10;lines(xgrid,DPDens2,col=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;JAGS Model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model{&#10; for(i in 1:n){&#10;  y[i] ~ dnorm(mu[z[i]],tau[z[i]])&#10;  z[i] ~ dcat(prob[])&#10;  for(j in 1:K){&#10;   f.d[i,j] &amp;lt;- prob[j]*coi*exp(-0.5*pow(y[i]-mu[z[i]],2)*tau[z[i]])*sqrt(tau[z[i]])&#10;  }&#10; }&#10; prob[1:K] ~ ddirch(prob.par[])&#10; for(i in 1:K){&#10;  repeat[i] &amp;lt;- step(1-dist[i])&#10;  prob.par[i] &amp;lt;- alpha/K&#10;  mu[i] ~ dnorm(0,.001)&#10;  tau[i] ~ dgamma(0.001,0.001)&#10; }&#10; # total is number of active &quot;used&quot; components&#10; total &amp;lt;- sum(repeat[])&#10; dist[1] &amp;lt;- 1&#10; for(i in 2:K){&#10;  dist[i] &amp;lt;- sum(count[i,1:i])&#10;  count[i,i] &amp;lt;- 1&#10;  for(j in 1:i-1){&#10;   count[i,j] &amp;lt;- equals(z[i],z[j])&#10;  }&#10; }&#10; alpha &amp;lt;- 1&#10;###Density&#10; for(i in 1:n2){&#10;  for(j in 1:K){&#10;   f.p[i,j] &amp;lt;- prob[j]*coi*exp(-0.5*pow(grid[i]-mu[j],2)*tau[j])*sqrt(tau[j])&#10;  }&#10;  # f[i] is predictive density over grid[i]&#10;  f[i] &amp;lt;- sum(f.p[i,1:K])&#10;  }&#10;  coi &amp;lt;- 0.3989422804&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-06-27T12:35:38.270" Id="62731" LastActivityDate="2013-06-27T12:35:38.270" OwnerUserId="2310" PostTypeId="1" Score="3" Tags="&lt;mixture&gt;&lt;jags&gt;&lt;dirichlet-process&gt;" Title="Label Switching in WinBugs/JAGS" ViewCount="472" />
  
  <row Body="&lt;p&gt;Okay, I admit there is a flaw in this one, but it comforts me nonetheless:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;I used to get angry when someone driving in front of me did something really stupid (or uncourteous) on the road, but then I found comfort in realizing that about half the people on the road are of below average intelligence...&quot;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-06-27T13:42:56.640" CreationDate="2013-06-27T13:42:56.640" Id="62735" LastActivityDate="2013-06-27T13:42:56.640" OwnerUserId="27374" ParentId="1337" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like some advice on how to implement a Kolmogorov-Smirnov test.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;This is a condensed version of my previous question -- any further detail can be provided if needed.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have data from &lt;em&gt;n&lt;/em&gt; experimental trials. Each set consists of two paired time-series signals, for which I am calculating an informative &quot;overlap&quot; measure (a single summary value), based on some threshold value. I calculate this summary value for all &lt;em&gt;n&lt;/em&gt; sets of data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to determine whether the distribution of these overlap values is non-random. The distribution of the data is not normal, and having read some details about different comparative tests of distributions, the Kolmogorov-Smirnov test seems most appropriate.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm hoping someone can give me advice on how to perform a permutation test using these data. My understanding is that this requires comparing the distribution of the actual data with those obtained from random combinations of the data, however, I'm uncertain of the procedure for implementing and interpreting the result. Any help would be much appreciated.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-06-27T14:16:04.500" FavoriteCount="2" Id="62738" LastActivityDate="2013-07-01T22:36:25.347" LastEditDate="2013-07-01T22:36:25.347" LastEditorUserId="27371" OwnerUserId="27371" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;summary-statistics&gt;&lt;kolmogorov-smirnov&gt;&lt;permutation&gt;" Title="Implementing a Kolmogorov-Smirnov permutation test" ViewCount="290" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;From &lt;a href=&quot;http://stats.stackexchange.com/q/62680/1005&quot;&gt;discussion with Nick Cox&lt;/a&gt;, and some search, I was wondering what are the definitions of model-based classification and non-model-based classification?&lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding is that for model based classification, there is a statistical model for the joint distributions of the response and predictor. Some classification method is derived under some statistical model, such as linear discriminant analysis (LDA) assumes the conditional distribution of predictor given class is normal. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then what does non-model-based classification mean? Does it mean that the statistical model of the joint distributions of the response and predictor consists of any distributions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you given some classification methods for non-model-based classification?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-27T18:02:19.147" Id="62766" LastActivityDate="2013-06-28T06:03:05.190" LastEditDate="2013-06-28T06:03:05.190" LastEditorUserId="805" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;model&gt;" Title="Classification: model based and non model based" ViewCount="69" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Find a mean and variance ($\gamma_0$) of $y_t$. How does the condition $|\Theta|&amp;lt;1$ help you to do that? Remember the assumptions of how residuals are distributed and similar.&lt;/li&gt;&#10;&lt;li&gt;Autocorrelation coefficient $\rho_1=\dfrac{\gamma_1}{\gamma_0}$, i.e. now you are missing only $\gamma_1$. Once you write down its expression the hints from the previous step are sufficient here too.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-06-27T20:11:30.337" Id="62776" LastActivityDate="2013-06-27T20:11:30.337" OwnerUserId="10172" ParentId="62747" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Is there a book that explains why there aren't better standard techniques than Tukey and ANOVA, for example?&lt;/p&gt;&#10;&#10;&lt;p&gt;For comparison consider for example I read about the null hypothesis one-sample $t$ test and didn't even bother considering that there could be better tests. But that is probably just a biased belief that there isn't anything more mathematically sophisticated that can be done with the Gaussian to improve on the one-sample $t$ test.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, Tukey and ANOVA are mathematically more complicated and it didn't feel obvious to me why they should even be considered in the first place. For example in a previous &lt;a href=&quot;http://stats.stackexchange.com/questions/61732/two-sample-t-test-vs-tukeys-method&quot;&gt;question&lt;/a&gt; I asked about why Tukey's method is needed over all pairwise two-sample $t$-tests. The answer I got for that question is that all pairwise two-sample $t$-tests suffers from false positives. But its not intuitive to me how Tukey's method is the best way to evade that problem. How does one intuitively see that generically Tukey and ANOVA are reasonably very good techniques among all possibilities?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-28T01:51:27.027" Id="62793" LastActivityDate="2013-06-28T16:57:05.893" LastEditDate="2013-06-28T16:57:05.893" LastEditorUserId="22468" OwnerUserId="26886" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;t-test&gt;&lt;references&gt;&lt;tukey-hsd&gt;" Title="Why are there not obvious improvements over Tukey's method?" ViewCount="141" />
  <row Body="&lt;p&gt;Multiply both sides by $y_{t-1}$ and take expectation. Exploit the fact that $u_t$ and $y_{t-1}$ are not correlated. &lt;/p&gt;&#10;&#10;&lt;p&gt;To calculate variance simply square both sides and then take expectation. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-28T07:17:04.680" Id="62806" LastActivityDate="2013-06-28T07:17:04.680" OwnerUserId="2116" ParentId="62747" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I got some users' history data and generated some sequences of real numbers. The length of each sequence is between 15 and 25. What's more, I do not know whether these sequences have patterns and the frequency is not known as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is using each sequence to predict its next value, and then I use auto.arima in R to do this. However, the accuracy of the prediction is low.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone have any good ideas to improve the accuracy?&lt;/p&gt;&#10;&#10;&lt;p&gt;One of these sequences is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    1.5959709882736206  &#10;    0.7300914525985718  &#10;    2.0011744499206543  &#10;    3.6755871772766113  &#10;    0.8066112399101257  &#10;    1.3413848876953125  &#10;    3.371157646179199  &#10;    0.4400146007537842  &#10;    2.637667655944824  &#10;    2.1453769207000732  &#10;    2.341433048248291  &#10;    2.3429665565490723  &#10;    1.1187453269958496  &#10;    1.4169363975524902  &#10;    3.328829050064087  &#10;    4.157748699188232  &#10;    3.9255290031433105  &#10;    2.7843635082244873 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-06-28T07:51:50.757" Id="62810" LastActivityDate="2013-06-28T15:26:03.960" LastEditDate="2013-06-28T08:11:39.490" LastEditorUserId="17230" OwnerUserId="27119" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;machine-learning&gt;&lt;data-mining&gt;&lt;arima&gt;" Title="How to improve forecasting accuracy?" ViewCount="131" />
  <row Body="&lt;p&gt;You could use the Earth-Movers-Distance (EMD), which takes into account a ground-distance between the bins and solves a transportation problem (basically, and hence the name: one histogram is a set of piles of earth, one a set of holes and you want to fill the holes as efficiently as possible). Afaik it is quite a standard distance comparing images in content-based image retrieval.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-28T11:13:37.857" Id="62822" LastActivityDate="2013-06-28T11:13:37.857" OwnerUserId="27423" ParentId="51848" PostTypeId="2" Score="1" />
  <row Body="&lt;h2&gt;Population $R^2$&lt;/h2&gt;&#10;&#10;&lt;p&gt;I'm firstly trying to understand the definition of the &lt;em&gt;population R-squared&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Quoting your comment:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Or you could define it asymptotically as the proportion of variance&#10;  explained in your sample as your sample size approaches infinity.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I think you mean this is the limit of the sample $R^2$ when one replicates the model infinitely many times (with the same predictors at each replicate).  &lt;/p&gt;&#10;&#10;&lt;p&gt;So what is the formula for the asymptotic value of the sample $R^²$ ? Write your linear model $\boxed{Y=\mu+\sigma G}$ as in &lt;a href=&quot;http://stats.stackexchange.com/a/58133/8402&quot;&gt;http://stats.stackexchange.com/a/58133/8402&lt;/a&gt;, and use the same notations as this link.&lt;br&gt;&#10;Then one can check that the sample $R^2$ goes to $\boxed{popR^2:=\dfrac{\lambda}{n+\lambda}}$ when one replicates the model $Y=\mu+\sigma G$ infinitely many times. &lt;/p&gt;&#10;&#10;&lt;p&gt;As example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ## design of the simple regression model lm(y~x0)&#10;&amp;gt; n0 &amp;lt;- 10&#10;&amp;gt; sigma &amp;lt;- 1&#10;&amp;gt; x0 &amp;lt;- rnorm(n0, 1:n0, sigma)&#10;&amp;gt; a &amp;lt;- 1; b &amp;lt;- 2 # intercept and slope&#10;&amp;gt; params &amp;lt;- c(a,b)&#10;&amp;gt; X &amp;lt;- model.matrix(~x0)&#10;&amp;gt; Mu &amp;lt;- (X%*%params)[,1]&#10;&amp;gt; &#10;&amp;gt; ## replicate this experiment k times &#10;&amp;gt; k &amp;lt;- 200&#10;&amp;gt; y &amp;lt;- rep(Mu,k) + rnorm(k*n0)&#10;&amp;gt; # the R-squared is:&#10;&amp;gt; summary(lm(y~rep(x0,k)))$r.squared &#10;[1] 0.971057&#10;&amp;gt; &#10;&amp;gt; # theoretical asymptotic R-squared:&#10;&amp;gt; lambda0 &amp;lt;- crossprod(Mu-mean(Mu))/sigma^2&#10;&amp;gt; lambda0/(lambda0+n0)&#10;          [,1]&#10;[1,] 0.9722689&#10;&amp;gt; &#10;&amp;gt; # other approximation of the asymptotic R-squared for simple linear regression:&#10;&amp;gt; 1-sigma^2/var(y)&#10;[1] 0.9721834&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h2&gt;Population $R^2$ of a submodel&lt;/h2&gt;&#10;&#10;&lt;p&gt;Now assume the model is  $\boxed{Y=\mu+\sigma G}$ with $H_1\colon\mu \in W_1$ and consider the submodel $H_0\colon \mu \in W_0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then I said above that the population $R^2$ of model $H_1$ is  $\boxed{popR^2_1:=\dfrac{\lambda_1}{n+\lambda_1}}$ where $\boxed{\lambda_1=\frac{{\Vert P_{Z_1} \mu\Vert}^2}{\sigma^2}}$ and $Z_1=[1]^\perp \cap W_1$ and then one simply has ${\Vert P_{Z_1} \mu\Vert}^2=\sum(\mu_i - \bar \mu)^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now do you define the population $R^2$ of the &lt;em&gt;submodel&lt;/em&gt; $H_0$ as the asymptotic value of the $R^2$ calculated with respect to model $H_0$ but under the distributional assumption of model $H_1$ ? The asymptotic value (if there is one) seems more difficult to find.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-06-28T12:00:32.900" Id="62828" LastActivityDate="2013-06-30T07:45:54.480" LastEditDate="2013-06-30T07:45:54.480" LastEditorUserId="8402" OwnerUserId="8402" ParentId="62795" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="62830" AnswerCount="1" Body="&lt;p&gt;I'm trying to calculate partitioned sum of squares in a linear regression. In the first model, there are two predictors. In the second model, one of these predictors in removed. In the model with two predictors versus the model with one predictor, I have calculated the difference in regression sum of squares to be 2.72 - is this correct? If this is correct, why is the difference in sum of squares 2.72 (quite small) when the difference in r-squared is ~30% (quite large).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# model two predictors&#10;mod &amp;lt;- lm(drat ~ hp + wt, mtcars)&#10;&#10;# regression sum of squares two predictors&#10;regressionSumSquares &amp;lt;- sum((predict(mod)-mean(mtcars$drat))*(predict(mod)-mean(mtcars$drat)))&#10;&#10;# residuals two predictors&#10;residualSumSquares &amp;lt;- sum((predict(mod)-mtcars$drat)*(predict(mod)-mtcars$drat))&#10;&#10;# r-squared two predictors&#10;totalSumSquares &amp;lt;- regressionSumSquares + residualSumSquares&#10;rSquared &amp;lt;- regressionSumSquares/totalSumSquares&#10;&#10;&#10;&#10;# model one predictor &#10;modJustHp &amp;lt;- lm(drat ~ hp, mtcars)&#10;&#10;# regression sum of squares one predictor&#10;regressionSumSquaresJustHp &amp;lt;- sum((predict(modJustHp)-mean(mtcars$drat))*(predict(modJustHp)-mean(mtcars$drat)))&#10;&#10;# residual sum of squares one predictor&#10;residualSumSquaresJustHp &amp;lt;- sum((predict(modJustHp)-mtcars$drat)*(predict(modJustHp)-mtcars$drat))&#10;&#10;# r-squared one predictor&#10;totalSumSquaresJustHp &amp;lt;- regressionSumSquaresJustHp + residualSumSquaresJustHp&#10;rSquaredJustHp &amp;lt;- regressionSumSquaresJustHp/totalSumSquaresJustHp&#10;&#10;# difference in sum of squares one vs two predictors&#10;regressionSumSquares - regressionSumSquaresJustHp&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-06-28T12:12:59.257" Id="62829" LastActivityDate="2013-06-28T12:22:29.820" LastEditDate="2013-06-28T12:22:23.753" LastEditorUserId="22047" OwnerUserId="12492" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;r-squared&gt;&lt;sums-of-squares&gt;" Title="Partitioned sum of squares" ViewCount="130" />
  <row Body="&lt;p&gt;Look at the output of this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; var(mtcars$drat)*length(mtcars$drat)&#10;[1] 9.148203&#10;&amp;gt; sum(residuals(mod)^2)&#10;[1] 4.35744&#10;&amp;gt; sum(residuals(modJustHp)^2)&#10;[1] 7.077585&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So the null model (intercept only) has a residual sum of squares ~9. Model with 2 regressors ~4 and model with one regressor ~7. The improvement of R-squared of 0.3 from the model with one regressor to the model with 2 regressors seems quite feasible. The idea is always to put a number in a proper context.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-28T12:22:29.820" Id="62830" LastActivityDate="2013-06-28T12:22:29.820" OwnerUserId="2116" ParentId="62829" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you are interested in this formulation for causal inference about $\beta$ then the unknown quantities represented by $c_i$ need only be stable for the duration of the study / data for fixed effects to identify the relevant causal quantity.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are concerned that the quantities represented by $c_i$ aren't stable even over this period then fixed effects won't do what you want.  Then you can use random effects instead, although if you expect correlation between random $c_i$ and $X_i$ you'd want to condition $c_i$ on $\bar{X}_i$ in a multilevel setup.  Concern about this correlation is often one of the motivations for a fixed effects formulation because under many (but not all) circumstances you don't need to worry about it then.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, your concern about variation in the quantities represented by $c_i$ is very reasonable, but mostly as it affects the data for the period you have rather than periods you might have had or that you may eventually have but don't.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-28T12:41:04.533" Id="62832" LastActivityDate="2013-06-28T12:41:04.533" OwnerUserId="1739" ParentId="62820" PostTypeId="2" Score="7" />
  
  
  <row Body="&lt;p&gt;Chelsea,&lt;/p&gt;&#10;&#10;&lt;p&gt;You have constricted your answer to a sample space that is too small.  You need to be considering deterministic variables instead of relying upon stochastic only.  This model has two trend variables.  One beginning at period 1 and another at 9.  The 4th observation is an outlier.&lt;/p&gt;&#10;&#10;&lt;p&gt;Y(T) =  1.2875&lt;br&gt;&#10;       +[X1(T)][(+  .0461)]             :TIME TREND        1&#10;       +[X2(T)][(+  .126)]              :TIME TREND        9&#10;       +[X3(T)][(+ 2.2038)]             :PULSE             4&#10;      +                    +   [A(T)]&#10;&lt;img src=&quot;http://i.stack.imgur.com/M2mCu.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-28T15:26:03.960" Id="62843" LastActivityDate="2013-06-28T15:26:03.960" OwnerUserId="3411" ParentId="62810" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Have you looked into using the strata and sampsize parameters from the R package 'random forest'? You could indicate household in the strata parameter. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is copied from the R documentation on the randomForest package:&lt;/p&gt;&#10;&#10;&lt;p&gt;strata: A (factor) variable that is used for stratified sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;sampsize: Size(s) of sample to draw. For classification, if sampsize is a vector of the length the number of strata, then sampling is stratified by strata, and the elements of sampsize indicate the numbers to be drawn from the strata.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-28T16:39:14.543" Id="62847" LastActivityDate="2013-06-28T16:39:14.543" OwnerUserId="26997" ParentId="62840" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have implemented a Gibbs Sampler for the &lt;strong&gt;Bayesian Elastic Net&lt;/strong&gt; (BEN) according to this paper on &lt;a href=&quot;http://www.stat.ufl.edu/~casella/Papers/BL-Final.pdf&quot; rel=&quot;nofollow&quot;&gt;Penalized Regression by Kyung et al.&lt;/a&gt;&lt;br&gt;&#10;In this paper, they execute a simulation study that has been used in other papers on Penalized Regression (LASSO, Bridge, Ridge) to compare the performance of the proposed models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are details of the simulation taken from the above mentioned paper:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;We simulate data from the true model&#10;  $$&#10;y=X\beta+\sigma\epsilon \quad\epsilon_i\,{\raise.17ex\hbox{$\scriptstyle\sim$}}\,\text{iid}\,N(0,1)&#10;$$&#10;  We simulate data sets with $n=20$ to fit models and $n=200$ to compare prediction errors of proposed models with eight predictors. We let $\beta=(3,1.5,0,0,2,0,0,0)$ and $\sigma=3$. The pairwise correlation between $x_i$ and $x_j$ was set to be $corr(i,j)=0.5^{|i-j|}$.&lt;br&gt;&#10;  Later on they say, that for the prediction error, they calculate the average mean squared error based on 50 replications. By average they mean the median in this case.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;To simulate this data and calculate the MSE I've used following code in R: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Number of observations&#10;n.train &amp;lt;- 20&#10;n.test  &amp;lt;- 200&#10;# Error variance&#10;sigma &amp;lt;- 3&#10;# Pairwise correlation of X&#10;cor &amp;lt;- 0.5&#10;# Number of predictors&#10;p &amp;lt;- 8&#10;# Create training and test data set (package QRM and mvtnorm required)&#10;Z &amp;lt;- equicorr(p, rho=cor)&#10;X.train &amp;lt;- rmvnorm(n.train,sigma=Z)&#10;X.test  &amp;lt;- rmvnorm(n.test,sigma=Z)&#10;# Create error &#10;error.train &amp;lt;- rnorm(n.train,mean=0,sd=1)&#10;error.test  &amp;lt;- rnorm(n.test,mean=0,sd=1)&#10;# Create beta&#10;beta.true &amp;lt;- c(3,1.5,0,0,2,0,0,0)&#10;# Create both responses&#10;Y.train &amp;lt;- X.train %*% beta.true + sigma*error.train&#10;Y.test  &amp;lt;- X.test %*% beta.true + sigma*error.test&#10;&#10;# Fit the training data set with the BEN Gibbs Sampler&#10;beta.ben &amp;lt;- BEN(X.train,Y.train, iter=11000, burn = 1000)&#10;# Calculate the predicted response&#10;Y.pred   &amp;lt;- X.test %*% beta.ben&#10;# Calculate the mean squared error (MSE)&#10;MSE      &amp;lt;- sum((Y.train - Y.pred)^2)/n.train&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My problem is that my results are not even close to comparable to the ones in the paper which makes me doubt my simulation study &quot;setup&quot;.&lt;br&gt;&#10;As one of the authors of the paper has uploaded the Gibbs Sampler code and I could check if I did something wrong, I know that the problem doesn't lie there.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Does anybody have experience with this kind of simulation study and can check if I did something wrong?&lt;/li&gt;&#10;&lt;li&gt;Is the MSE I calculate the same as the one used in the paper? In researching on this topic I found many different ways to calculate the MSE and it was also sometimes used but actually the mean squared prediction error was meant. For example the Wikipedia article on MSE alone lists three variations.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I don't need help with coding, rather more information on how this simulation is typically excecuted so I can figure out what I'm doing wrong.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-28T17:25:31.003" FavoriteCount="1" Id="62852" LastActivityDate="2013-10-20T05:30:22.570" LastEditDate="2013-06-29T00:27:45.753" LastEditorUserId="27436" OwnerUserId="27436" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;simulation&gt;&lt;hierarchical-bayesian&gt;&lt;mse&gt;&lt;elastic-net&gt;" Title="Replicate simulation study from a paper and calculate the MSE in R" ViewCount="926" />
  
  <row Body="&lt;p&gt;I ran into a similar problem with several of the plots I have been working with and wrote a basic package that uses force field simulation to adjust object locations. The advantage over some of the above-cited solutions is the dynamic adjustment for relative object proximity in 2D.  While much improvement is possible, including heuristics and integration with ggplot, etc. it seems to get the task accomplished. The following illustrates the functionality:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages(&quot;FField&quot;, type = &quot;source&quot;)&#10;install.packages(&quot;ggplot2&quot;)&#10;install.packages(&quot;gridExtra&quot;)&#10;library(FField)&#10;FFieldPtRepDemo()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For now there is no heuristics for a variety of areas and point distributions as the solution met my needs and I wanted to get something helpful to folks out quickly but I'll add these in the medium term.  At this time I recommend scaling charts to 100x100 and back and slightly tweaking the default attraction and repulsion parameters as warranted.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-28T17:42:41.117" Id="62856" LastActivityDate="2013-06-28T17:42:41.117" OwnerUserId="27440" ParentId="16057" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Two main causes:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Small sample size. Even if the assumptions are met and the reliability is decent, an estimate computed from a particular sample can be negative, just as a sample mean is not equal to the population mean. Somewhat surprisingly, whereas experimental psychologists tend to be obsessed with statistical testing and are conditioned to ask a &lt;em&gt;p&lt;/em&gt;-value for everything, psychometrics textbooks generally don't care about tests/confidence intervals for reliability estimates. One reason is that their authors assume that any scale development effort will have at the very least hundreds of observations but there is some sampling variability in reliability estimates anyway.&lt;/li&gt;&#10;&lt;li&gt;Negatively worded items/items with strong negative correlation with the underlying factor.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Remedies:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Recode negatively worded items&lt;/li&gt;&#10;&lt;li&gt;Get more data (you probably don't have enough)&lt;/li&gt;&#10;&lt;li&gt;Remove some item(s)&lt;/li&gt;&#10;&lt;li&gt;Forget alpha (it has some well-known limitations and chances are that it doesn't tell you what you think it's telling you + what were you planning to do with it anyway?)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;See also the references in &lt;a href=&quot;http://stats.stackexchange.com/questions/62758/reverse-scoring-when-question-is-stated-in-a-negative-fashion&quot;&gt;Reverse scoring when question is stated in a negative fashion&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-28T23:11:06.317" Id="62874" LastActivityDate="2013-06-28T23:47:26.367" LastEditDate="2013-06-28T23:47:26.367" LastEditorUserId="6029" OwnerUserId="6029" ParentId="62873" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;You just calculated the &lt;em&gt;limits&lt;/em&gt; on where the whiskers can go, not the whisker ends themselves (which are limited to the range of the data). &lt;/p&gt;&#10;&#10;&lt;p&gt;The actual whiskers go out to the furthest-out data value inside the (inner) fences. The fences can go outside the data, but aren't actually plotted.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8FZrn.png&quot; alt=&quot;from data to boxplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(for more details see &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/grstatgraph/63878/HTML/default/images/fences.png&quot;&gt;this image&lt;/a&gt;, which is from &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/grstatgraph/63878/HTML/default/p0ou4qi2jfcokkn1ks5mx1ct5cmw.htm&quot;&gt;this page&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-29T00:50:22.207" Id="62878" LastActivityDate="2013-06-29T06:46:15.507" LastEditDate="2013-06-29T06:46:15.507" LastEditorUserId="805" OwnerUserId="805" ParentId="62876" PostTypeId="2" Score="8" />
  
  
  
&#10;
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a list of genes identified using algorithm_A, then using algorithm_B and finally  using algorithm_C; then a second list of genes identified using algorithm_A, then using algorithm_B and then algorithm_C, and so on for a total of 100 gene lists. &lt;/p&gt;&#10;&#10;&lt;p&gt;To compare the overlap of the identified gene list using the three methods, I used the Cohen test for each gene lists. I would like to know first if it is the best choice and second how can I graphically plot the Cohen test results for the 100 gene lists. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-29T13:32:15.180" FavoriteCount="1" Id="62906" LastActivityDate="2013-06-29T17:02:29.483" LastEditDate="2013-06-29T17:02:29.483" LastEditorUserId="6029" OwnerUserId="14270" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;cohens-d&gt;" Title="Concordance test and corresponding plot" ViewCount="36" />
  
  <row Body="In statistics a pivot, or pivotal quantity is a function of unknown parameters and data whose distribution doesn't depend on the values of the unknown parameters - used to construct confidence intervals." CommentCount="0" CreationDate="2013-06-30T01:43:26.790" Id="62936" LastActivityDate="2013-06-30T09:03:29.120" LastEditDate="2013-06-30T09:03:29.120" LastEditorUserId="805" OwnerUserId="805" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;EM is not needed instead of using some numerical technique because EM is a numerical method as well. So it's not a substitute for Newton-Raphson. EM is for the specific case when you have missing values in your data matrix. Consider a sample $X = (X_{1},...,X_{n})$ which has conditional density $f_{X|\Theta}(x|\theta)$. Then the log-likelihood of this is&#10;$$l(\theta;X) = log f_{X|\Theta}(X|\theta)$$&#10;Now suppose that you do not have a complete data set such that $X$ is made up of observed data $Y$ and missing (or latent) variables $Z$, such that $X=(Y,Z)$. Then the log-likelihood for the observed data is&#10;$$l_{obs}(\theta,Y)=log \int f_{X|\Theta}(Y,z|\theta)\nu_{z}(dz)$$&#10;In general you cannot compute this integral directly and you will not get a closed-form solution for $l_{obs}(\theta,Y)$. For this purpose you use the EM method. There are two steps which are iterated for $i$ times. In this $(i + 1)^{th}$ step these are the expectation step in which you compute&#10;$$Q(\theta|\theta^{(i)}) = E_{\theta^{(i)}}[l(\theta;X|Y]$$&#10;where $\theta^{(i)}$ is the estimate of $\Theta$ in the $i^{th}$ step. Then compute the maximization step in which you maximize $Q(\theta|\theta^{(i)})$ with respect to $\theta$ and set $\theta^{(i+1)} = max Q(\theta|\theta^{i})$.&#10;You then repeat these steps until the method converges to some value which will be your estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you need more information on the method, its properties, proofs or applications just give a look at the corresponding &lt;a href=&quot;http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Wiki&lt;/a&gt; article.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-30T09:07:51.733" Id="62943" LastActivityDate="2013-06-30T09:07:51.733" OwnerUserId="26338" ParentId="62940" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;As mentioned by Glen_b, there are a number of possibilities. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example of a histogram and density plot using the &quot;lattice&quot; package. I've also provided some sample data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;mydf &amp;lt;- data.frame(V1 = sample(LETTERS[1:5], 500, replace = TRUE),&#10;                   V2 = sample(0:50, 500, replace = TRUE))&#10;head(mydf)&#10;tail(mydf)&#10;library(lattice)&#10;histogram(~V2 | V1, data = mydf)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xveb1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;densityplot(~V2 | V1, data = mydf)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/unntu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Both are with default settings applied.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-30T09:56:14.250" Id="62947" LastActivityDate="2013-06-30T09:56:14.250" OwnerUserId="13474" ParentId="62944" PostTypeId="2" Score="8" />
  
  
  
  <row AcceptedAnswerId="62989" AnswerCount="1" Body="&lt;p&gt;I'm completing scientific analysis of chemical compounds in consumer products. As a non-statistician, I would really appreciate any thoughts from the experts here. &lt;/p&gt;&#10;&#10;&lt;p&gt;My data is non-normal so I've used non-parametric tests like MW and KW for hypothesis testing between samples so far. However I now have to conduct a principal component analysis (PCA) of the different compounds measured in the different products (measured in different units). &lt;/p&gt;&#10;&#10;&lt;p&gt;The stats add-in I was using asks that the type of data format be specified (eg: observation/variable table, versus a correlation or covariance matrix). I'm working with straight data so used the observation/variable table set-up.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But it &lt;em&gt;also&lt;/em&gt; asks me to specify the PCA type from the following options (Pearson(n), Pearson (n-1), Spearman, Kendall, Covariance...).  I tested the same data set with the Pearson (n) option and the Spearman option and got very different eigenalues and eigenvectors. The final biplot is naturally quite different. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help someone can provide regarding what the difference is, and what PCA type should be used would be greatly appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: I was using XLSTAT (an Excel add-in). Is it okay to use Pearson as the &quot;PCA type&quot; when the correlations between the variables are non-linear? For example this &quot;PCA type&quot; option does not appear in other stats programs (eg: SPSS). So for example if using SPSS, the novice user would by default use Pearson &quot;pca type&quot;.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-07-01T01:33:00.190" Id="62981" LastActivityDate="2013-07-15T05:39:14.910" LastEditDate="2013-07-15T05:39:14.910" LastEditorUserId="27492" OwnerUserId="27492" PostTypeId="1" Score="2" Tags="&lt;pca&gt;&lt;nonparametric&gt;&lt;pearson&gt;&lt;spearman&gt;&lt;eigenvalues&gt;" Title="Analysis of compounds using PCA - selecting the right PCA &quot;type&quot; for the data...?" ViewCount="830" />
  <row AnswerCount="1" Body="&lt;p&gt;In the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;hdrcde&lt;/code&gt; by Rob Hyndman for Computation of highest density regions in one and two dimensions, the hdrbw function Estimates the optimal bandwidth for 1-dimensional highest density regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the given example, HDRlevelVal variable is a probability 0&#10;&#10;&lt;p&gt;HDRlevel : HDR-level as defined in Hyndman (1996). Setting ‘HDRlevel’ equal to p (0&#10;&#10;&lt;p&gt;I have not found the referenced article.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody give me some information about that article and function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there in &lt;code&gt;R&lt;/code&gt;, packages that implements this function?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;HDRlevelVal &amp;lt;- 0.55&#10;x &amp;lt;- faithful$eruptions&#10;hHDR &amp;lt;- hdrbw(x,HDRlevelVal)&#10;HDRhat &amp;lt;- hdr.den(x,prob=100*(1-HDRlevelVal),h=hHDR)&#10;Rhat &amp;lt;- hdr.den(x,prob=100*(1-HDRlevelVal),h=hHDR)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-07-01T05:25:17.717" Id="62990" LastActivityDate="2013-07-01T13:24:15.347" LastEditDate="2013-07-01T06:43:12.857" LastEditorUserId="21054" OwnerUserId="27354" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;probability&gt;&lt;density&gt;" Title="How to compute HDR probability level for Highest density regions estimates" ViewCount="223" />
  
  <row Body="&lt;p&gt;If you are happy to assume your binomial responses are coming from a spatially correlated gaussian random field via a logit link, and your non-spatial covariates have the usual log-linear form, then stuff it all into &lt;code&gt;geoRglm&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/geoRglm/vignettes/geoRglmintro.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/geoRglm/vignettes/geoRglmintro.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and once you've got your MCMC all tuned, out pops the parameter estimates.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-01T07:39:21.987" Id="62997" LastActivityDate="2013-07-01T07:39:21.987" OwnerUserId="1549" ParentId="57180" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;If you are trying to predict downtime from attributes of messages, it sounds like you want some form of regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;If downtime is a binary variable (yes/no) then you probably want logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;If downtime is continuous (e.g. in minutes or seconds) you probably want &quot;regular&quot; (ordinary least squares) regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;If downtime is in some other format, please tell us.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both these forms of regression (and others) are available in the &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;glm&lt;/code&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-01T11:18:53.093" Id="63009" LastActivityDate="2013-07-01T11:18:53.093" OwnerUserId="686" ParentId="63006" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;If you're getting a high reject rate it implies your proposal distribution is too wide.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are you getting fuzzy caterpillar plots (WinBugs makes these)?&#10;They are a good diagnostic of model behavior.&lt;/p&gt;&#10;&#10;&lt;p&gt;MCMC models don't converge in the usual sense.&#10;Rather they continually sample from the uncertainty distribution, and those samples give you estimates of mean and variance-covariance of parameters.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-01T16:40:00.267" Id="63034" LastActivityDate="2013-07-01T16:40:00.267" OwnerUserId="1270" ParentId="63033" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to know whether a time series has converged. I looked around the MCMC literature (which requires doing something similar -- knowing whether a chain has converged) and found the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.people.fas.harvard.edu/~plam/teaching/methods/convergence/convergence_print.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.people.fas.harvard.edu/~plam/teaching/methods/convergence/convergence_print.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;consider page 13 there, with running mean plot. The idea is to plot the mean up to each iteration and see whether it stabilizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, let me say that I couldn't find a mention of this method anywhere else by googling. When I look for &quot;running mean plots&quot; I mostly get back this deck of slides as a result. So I am not sure if this is indeed a well-known way to check for a MC convergence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, I am quite surprised by this idea. If I am not mistaken, any such plot will converge quite fast, just because of the central limit theorem. We have some distribution, and we sample from it, and draw the mean. Of course it will stabilize, around the mean of the distribution with rather small variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the idea is that if the MC has not converged, then we will see the mean stabilizing and then changing again after a while when the distribution has moved to a new &quot;station&quot;. But it doesn't seem to me like a good way to check for MCMC convergence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone refer me to some material about this idea of running mean plot, how it can be used to check for a series convergence or at least MC convergence?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-01T19:02:18.063" Id="63048" LastActivityDate="2013-07-01T19:07:23.700" LastEditDate="2013-07-01T19:07:23.700" LastEditorUserId="6029" OwnerUserId="24786" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;mcmc&gt;&lt;markov-chain&gt;" Title="Are running mean plots good indicators for time series convergence?" ViewCount="125" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am reading the paper: &lt;a href=&quot;http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;amp;id=pdf_1&amp;amp;handle=euclid.aos/1018031103&quot; rel=&quot;nofollow&quot;&gt;Convergence of a stochastic approximation version of the EM algorithm&lt;/a&gt; to implement this algorithm for a probability model I already have. In p. 3, the paper summarises the algorithm as follows.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lxL66.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am stuck at the E (or S) step in this algorithm. In a typical EM setup, one maximizes the integral $$Q(\theta)= \int \log p(x,y |\theta) p(x|y,\theta) dx$$&#10;In here, this integral is estimated via the samples simulated from the posterior. I understand this. But, what I do not understand is: Do authors propose to 'update' the cost function? If so, how can we maximize the new $Q$? May be I am missing a very obvious thing and can not see how to implement this algorithm (I do not understand updating the 'cost').&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-01T19:52:41.083" Id="63061" LastActivityDate="2013-07-01T20:19:03.300" OwnerUserId="16776" PostTypeId="1" Score="1" Tags="&lt;expectation-maximization&gt;" Title="E-step of the stochastic approximation EM" ViewCount="160" />
  <row Body="&lt;p&gt;Comparing overlapping 95% intervals is problematic, the nominal level to test for a difference is no longer 5%.  There is a large body of literature on this. If two CIs don't overlap, there isn't a problem, however, if they (95%CIs) do overlap, they can still be different.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Austin PC, &amp;amp; Hux JE. (2002). &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/12096281&quot; rel=&quot;nofollow&quot;&gt;A brief note on overlapping confidence intervals.&lt;/a&gt; &lt;em&gt;Journal of Vascular Surgery,&lt;/em&gt; 36 (1), 194–195.&lt;/li&gt;&#10;&lt;li&gt;Knol, M.J., Pestman, W.R., &amp;amp; Grobbee, D.E. (2011). &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3088813/&quot; rel=&quot;nofollow&quot;&gt;The (mis)use of overlap of confidence intervals to assess effect modification.&lt;/a&gt; &lt;em&gt;European Journal of Epidemiology,&lt;/em&gt; 26 (4), 253–254. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If a difference is what is of interest, then calculating a CI around the difference should be done, and not comparing two 95% CIs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-01T20:08:18.467" Id="63064" LastActivityDate="2013-07-01T20:16:45.697" LastEditDate="2013-07-01T20:16:45.697" LastEditorUserId="6029" OwnerUserId="27520" ParentId="63056" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;In &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2441782/&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, the authors present a scoring algorithm for potential transcription factor binding sites, based on the position-specific probability matrix (PSPM) for that particular transcription factor.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The binding site is 20 nucleotides long, so the idea is to scan a longer DNA sequence in 20 nucleotide chunks, i.e. score site 1 (nucleotides 1-20), then score site 2 (nucleotides 2-21), etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;The score of each site is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sum_{i=1}^{20} \ln \left[\frac{f_{i,b}}{g} + 0.01\right] $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $i$ represents the position in the consensus binding sequence, and $f_{i,b}$ is the probability of the nucleotide that happens to be in the long DNA sequence occurring at position $i$ in the binding site, and $g$ is the background frequency of that nucleotide (the authors assume it be be 0.25, that is, an equal probability of each nucleotide).  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;To illustrate:&lt;/strong&gt;&lt;br&gt;&#10;Say your PSPM starts with row 1 being $[0.2 $ $0.3$ $0.5$ $0]$. This means at position 1 in the potential binding sites (row 1), there is a 20% probability of having an A (Adenine), a 30% chance of having a C (Cytosine), a 50% probability of having a G (Guanine), and a 0% probability of having a T (Thymine). If you happen to be scoring a subsequence that has a C in the first position, the first term of the sum, $ \ln \left(\frac{f_{1,b}}{g} + 0.01\right) $, would be $ \ln \left(\frac{0.3}{g} + 0.01\right) $. &lt;/p&gt;&#10;&#10;&lt;p&gt;This procedure doesn't at all account for length of the sequence you're scanning. Thus, longer sequences are going to have higher-scoring sites, and I would like to be able to compare sites between sequences of different lengths.  &lt;/p&gt;&#10;&#10;&lt;p&gt;How could I correct this algorithm for sequence length?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-01T23:02:53.620" Id="63078" LastActivityDate="2013-07-05T21:30:34.573" LastEditDate="2013-07-05T21:30:34.573" LastEditorUserId="4301" OwnerUserId="4301" PostTypeId="1" Score="1" Tags="&lt;algorithms&gt;&lt;scoring&gt;" Title="Correcting a DNA scoring algorithm for scanned sequence length (consensus binding site)" ViewCount="46" />
  <row AnswerCount="0" Body="&lt;p&gt;My question is about updating the parameters of a regression with ARIMA errors model &#10;as new (monthly) data becomes available each month. &#10;Similar question were asked here before:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/34139/updating-arima-models-at-frequent-intervals&quot;&gt;Updating ARIMA models at frequent intervals&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/52131/how-to-update-forecasts-in-an-arima-model&quot;&gt;How to update forecasts in an ARIMA model?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/57745/what-do-you-consider-a-new-model-versus-an-updated-model-time-series&quot;&gt;What do you consider a new model versus an updated model (time series)?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My concern is, when do I know (or how can I test) that my model structure (ARMA terms, exogenous variables) is still suitable? Say in the simplest case my model is &#10;y = const + b*x + e, where e ~ ARIMA&#10;I estimated in my fitting sample b=-3.&#10;A new data point (of the next month) is available and I re-fit the model and obtain &#10;b = -3.5 &#10;and so on. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;When do I know, that something is wrong or how can I continuously (with each update) test my model for consistency?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know, that the Chow-Test a useful test for model stability. But I always have only one data-point to update. &lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Check each month the one-step-ahead forecast with the actual value. If the model gives still reasonable results -&gt; keep it. &lt;/li&gt;&#10;&lt;li&gt;Collect 6 new monthly data points (so half yearly) and check with a Chow-test the model stability between the initial fit-sample and the new sample (6 monthly data points).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-07-02T00:14:15.457" FavoriteCount="1" Id="63082" LastActivityDate="2014-02-12T08:41:03.310" LastEditDate="2013-07-02T08:52:37.447" LastEditorUserId="6029" OwnerUserId="27471" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;multiple-regression&gt;&lt;forecasting&gt;&lt;arima&gt;" Title="Updating ARIMA model" ViewCount="118" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a series of data points in this form (timestamp, lat, long) for a set of users. Each user has a trajectory when he travels from point A to point B. There might be any number of points from A to B. They are ordered data points based on time stamp. I want to transform them as a vector to do various analysis tasks. One thought I have is to look at turns and make them as a dimension. I would like to know more approaches.&#10;What I want is a one vector representing the whole trajectory, think of it like one point for a trajectory.Right now I have a collection of 3d points.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to do trajectory similarity search. If there are two trajectories that in time are travelling close to each other then they are similar. Think of it like this you are going from house to work at 9am. Somebody else at 9:10 am also his home for work and stays some distance from you. Since u have the same workplace , you will most likely have same trajectory. Something like a classifier built on top of a trajectory. I can do activity detection in a trajectory, I can do a source destination analysis too.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2013-07-02T05:13:41.480" FavoriteCount="1" Id="63096" LastActivityDate="2015-02-15T15:49:33.480" LastEditDate="2013-07-06T17:07:53.977" LastEditorUserId="3899" OwnerUserId="3899" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;multivariate-analysis&gt;&lt;data-transformation&gt;" Title="How to map a trajectory to a vector?" ViewCount="245" />
  <row AcceptedAnswerId="63133" AnswerCount="2" Body="&lt;p&gt;This question is an extension to my previous &lt;a href=&quot;http://stats.stackexchange.com/questions/60929/should-i-convert-bootstrapped-confidence-intervals-for-logistic-regression-coeff&quot;&gt;question&lt;/a&gt;, although it is not repeated.&lt;/p&gt;&#10;&#10;&lt;p&gt;My bootstrapped ORs are OK. They are equal to non-bootstrapped ORs. However, the confidence intervals of the bootstrapped and non-bootstrapped regressions differ vastly.&lt;/p&gt;&#10;&#10;&lt;p&gt;While the non-bootstrapped regression reports for example OR's 95% CI ~= 1.2 to 2.3, the bootstrapped one reports OR's 95% CI ~= 1.1 to 25000.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is &lt;strong&gt;strange&lt;/strong&gt; and &lt;strong&gt;non-practical&lt;/strong&gt;. My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is this bootstrapped OR CI &lt;strong&gt;correct&lt;/strong&gt;?&lt;/li&gt;&#10;&lt;li&gt;Can I keep my bootstrapped model in my study for its advantages, but report the non-bootstrapped OR CI?&lt;/li&gt;&#10;&lt;li&gt;Can I report both regressions?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Many thanks.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-02T07:05:34.810" Id="63101" LastActivityDate="2013-07-02T13:46:25.420" OwnerUserId="20556" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;confidence-interval&gt;&lt;bootstrap&gt;&lt;odds-ratio&gt;&lt;binary&gt;" Title="The bootstrapped confidence intervals for ORs are strangely big or small in my binary logistic regression. Are they valid in the first place?" ViewCount="182" />
  <row Body="&lt;p&gt;As I understand your problem, the main issue is the size of the data set, and not that it contains missing value (i.e. &quot;sparse&quot;). For such a problem, I would recommend doing a partial PCA in order to solve for a subset of leading PCs. The package &lt;code&gt;irlba&lt;/code&gt; allows for this by performing a &quot;Lanczos bidiagonalization&quot;. It is much faster for large matrices when you are only interested in returning a few of the leading PCs. In the following example, I have adapted a bootstrapping technique that I discussed &lt;a href=&quot;http://stats.stackexchange.com/questions/33917/how-to-determine-significant-principal-components-using-bootstrapping-or-monte-c&quot;&gt;here&lt;/a&gt; into a function that incorporates this method as well as a variable sub-sampling parameter. In the function &lt;code&gt;bootpca&lt;/code&gt;, you can define the number of variables to sample, &lt;code&gt;n&lt;/code&gt;, the number of PCs to return, &lt;code&gt;npc&lt;/code&gt;, and the number of iterations &lt;code&gt;B&lt;/code&gt; for the sub-sampling routine. For this method, I have centered and scaled the sub-sampled matrix in order to standardize the variance of the dataset and allow for comparability among the singular values of the matrix decomposition. By making a boxplot of these bootstrapped singular values, &lt;code&gt;lam&lt;/code&gt;, you should be able to differentiate between PCs that carry signals from those that are dominated by noise. &lt;/p&gt;&#10;&#10;&lt;h1&gt;Example&lt;/h1&gt;&#10;&#10;&lt;h3&gt;Generate data&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m=50&#10;n=100&#10;&#10;x &amp;lt;- (seq(m)*2*pi)/m&#10;t &amp;lt;- (seq(n)*2*pi)/n&#10;&#10;#field&#10;Xt &amp;lt;- &#10; outer(sin(x), sin(t)) + &#10; outer(sin(2.1*x), sin(2.1*t)) + &#10; outer(sin(3.1*x), sin(3.1*t)) +&#10; outer(tanh(x), cos(t)) + &#10; outer(tanh(2*x), cos(2.1*t)) + &#10; outer(tanh(4*x), cos(0.1*t)) + &#10; outer(tanh(2.4*x), cos(1.1*t)) + &#10; tanh(outer(x, t, FUN=&quot;+&quot;)) + &#10; tanh(outer(x, 2*t, FUN=&quot;+&quot;))&#10;&#10;Xt &amp;lt;- t(Xt)&#10;image(Xt)&#10;&#10;#Noisy field&#10;set.seed(1)&#10;RAND &amp;lt;- matrix(runif(length(Xt), min=-1, max=1), nrow=nrow(Xt), ncol=ncol(Xt))&#10;R &amp;lt;- RAND * 0.2 * Xt&#10;&#10;#True field + Noise field&#10;Xp &amp;lt;- Xt + R&#10;image(Xp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;load bootpca function&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(irlba)&#10;&#10;bootpca &amp;lt;- function(mat, n=0.5*nrow(mat), npc=10, B=40*nrow(mat)){&#10;  lam &amp;lt;- matrix(NaN, nrow=npc, ncol=B)&#10;  for(b in seq(B)){&#10;    samp.b &amp;lt;- NaN*seq(n)&#10;    for(i in seq(n)){&#10;        samp.b[i] &amp;lt;- sample(nrow(mat), 1)&#10;    }&#10;    mat.b &amp;lt;- scale(mat[samp.b,], center=TRUE, scale=TRUE)&#10;    E.b  &amp;lt;- irlba(mat.b, nu=npc, nv=npc)&#10;    lam[,b] &amp;lt;- E.b$d&#10;    print(paste(round(b/B*100), &quot;%&quot;, &quot; completed&quot;, sep=&quot;&quot;))&#10;  }&#10;  lam   &#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Result and plot&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;res &amp;lt;- bootpca(Xp, n=0.5*nrow(Xp), npc=15, B=999) #50% of variables used in each iteration, 15 PCs computed, and 999 iterations&#10;&#10;par(mar=c(4,4,1,1))&#10;boxplot(t(res), log=&quot;y&quot;, col=8, outpch=&quot;&quot;, ylab=&quot;Lambda [log-scale]&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kAZfL.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's obvious that the leading 5 PCs carry the most information, although there were technically 9 signals in the example data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your very large data set, you may want to use a smaller fraction of variables (i.e. rows) in each iteration, but do many iterations.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-07-02T07:49:14.890" Id="63105" LastActivityDate="2013-07-02T12:00:02.840" LastEditDate="2013-07-02T12:00:02.840" LastEditorUserId="10675" OwnerUserId="10675" ParentId="62957" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose one wants to test how many players still play the day after receiving the game, and two days after receiving it.&lt;/p&gt;&#10;&#10;&lt;p&gt;And that the results would be:&lt;/p&gt;&#10;&#10;&lt;p&gt;Game 1:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Total numbers of players = 120&lt;/li&gt;&#10;&lt;li&gt;Players playing the next day = 21,7%&lt;/li&gt;&#10;&lt;li&gt;Players playing two days after = 18,3%&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Game 2:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Total numbers of players = 62&lt;/li&gt;&#10;&lt;li&gt;Players playing the next day = 16,1%&lt;/li&gt;&#10;&lt;li&gt;Players playing two days after = 8,1%&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is it possible to calculate how accurate the result is, particularly in game 2.&#10;What is the +- percentage?&lt;/p&gt;&#10;&#10;&lt;p&gt;Could game 2 in fact be better than game 1?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-02T08:45:49.720" Id="63113" LastActivityDate="2013-07-02T20:04:41.877" LastEditDate="2013-07-02T20:04:41.877" LastEditorUserId="919" OwnerUserId="27533" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;games&gt;" Title="Two games with collected statistics, how accurate is the data?" ViewCount="130" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a problem regarding interpretation of a multiple regression. For $Y=a+bX_1+cX_2$, the influence of independent variable $X_1$ on $Y$, the dependent variable, is positive (coefficient $b$ is positive). This means that an increase of one unit of $X_1$ leads to an increase of one unit of $Y$, which is consistent with the economic interpretation. &lt;/p&gt;&#10;&#10;&lt;p&gt;But in my dataset, $X_1$ has also negative values. For those values, the interpretation is not correct anymore. An increase with 1 unit of $X_1$ will lead to a decrease of Y, meaning an inverse relation. What am I doing wrong? Is it better to use the data as absolute values ABS(X)?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-02T10:52:21.497" Id="63121" LastActivityDate="2013-07-02T12:02:45.797" LastEditDate="2013-07-02T12:01:57.777" LastEditorUserId="6029" OwnerUserId="27537" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;interpretation&gt;&lt;regression-coefficients&gt;" Title="Multiple regression with both positive and negative data" ViewCount="132" />
  <row Body="&lt;p&gt;In the &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf&quot; rel=&quot;nofollow&quot;&gt;original paper of Tibshirani&lt;/a&gt; (1995) it says that the signs of the LASSO estimate and the least squares solution may be different (p270 last paragraph). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-02T12:53:35.610" Id="63131" LastActivityDate="2013-07-02T12:53:35.610" OwnerUserId="27542" ParentId="48045" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I think calling it “hold-out validation” instead of “cross-validation” could avoid some confusion.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, I would be interested in the thinking behind it. Why not simply assess prediction accuracy on all the holdout data instead of partitioning it in &lt;em&gt;k&lt;/em&gt; samples. Also, if you have so much data in each sample that it is enough to reasonably train the model, why go to the trouble of taking additional samples instead of just taking two?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-02T14:34:20.157" Id="63139" LastActivityDate="2013-07-02T14:34:20.157" OwnerUserId="6029" ParentId="60638" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;How can you prove that the normal equations: $(X^TX)\beta = X^TY$ have one or more solutions without the assumption that X is invertible?&lt;/p&gt;&#10;&#10;&lt;p&gt;My only guess is that it has something to do with generalized inverse, but I am totally lost.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-02T14:58:56.430" FavoriteCount="3" Id="63143" LastActivityDate="2013-08-01T21:33:24.063" LastEditDate="2013-07-02T15:08:47.363" LastEditorUserId="22047" OwnerUserId="27550" PostTypeId="1" Score="6" Tags="&lt;regression&gt;&lt;proof&gt;" Title="Question about a normal equation proof" ViewCount="497" />
  <row Body="&lt;p&gt;One approach to modeling a response that is ordered categorical is proportional odds logistic regression.  But make sure that you understand what the assumptions are and what the model will tell you before fitting it.  Make sure that it answers the real question of interest.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-02T17:49:13.110" Id="63160" LastActivityDate="2013-07-02T17:49:13.110" OwnerUserId="4505" ParentId="63156" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a non-normal data set. We need to calculate the average of the numbers in this data set for planning purposes. I used “standardize” to normalize my data in Excel. Can I map the average obtained from the standardized data set back to the original so I have an a new average?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Let me try to clarify some of the questions posed here. The data set has supplies ordered/week. The average of this is going to be used for future planning purposes. Someone suggested that we normalize this data set and then find what the average would be and used the average obtained from normalizing. So my question is if I used the standardize function in Excel and get the new average from the the &quot;standardized&quot; data set can I map it back to my original data set? See example below &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;X1&quot; Mean Std Dev Standardized value &#10;282   252      55               0.52 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can I find out what 0.52 corresponds to in my original data set? as in would I get something that says 0.52 corresponds to a normalized value of 265 ? If possible i would like to do this for every reading and then find a new average Thank you!&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-07-02T20:06:16.527" Id="63171" LastActivityDate="2013-07-03T15:34:06.597" LastEditDate="2013-07-03T15:34:06.597" LastEditorUserId="919" OwnerUserId="27560" PostTypeId="1" Score="0" Tags="&lt;average&gt;&lt;standardization&gt;&lt;arithmetic&gt;" Title="Standardizing data" ViewCount="253" />
  
  <row Body="&lt;p&gt;I'm going to describe my view of this in two steps: The input-to-hidden step and the hidden-to-output step. I'll do the hidden-to-output step first because it seems less interesting (to me).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Hidden-to-Output&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The output of the hidden layer could be different things, but for now let's suppose that they come out of &lt;a href=&quot;http://en.wikipedia.org/wiki/Sigmoid_function&quot; rel=&quot;nofollow&quot;&gt;sigmoidal&lt;/a&gt; activation functions. So they are values between 0 and 1, and for many inputs they may just be 0's and 1's. &lt;/p&gt;&#10;&#10;&lt;p&gt;I like to think of the transformation between these hidden neurons' outputs and the output layer as just a translation (in the linguistic sense, not the geometric sense). This is certainly true if the transformation is &lt;a href=&quot;http://en.wikipedia.org/wiki/Invertible#Matrices&quot; rel=&quot;nofollow&quot;&gt;invertible&lt;/a&gt;, and if not then something was lost in translation. But you basically just have the hidden neurons' outputs seen from a different perspective.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Input-to-Hidden&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say you have 3 input neurons (just so I can easily write some equations here) and some hidden neurons. Each hidden neuron gets as input a &lt;strong&gt;weighted sum&lt;/strong&gt; of inputs, so for example maybe&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hidden_1 = 10 * (input_1) + 0 * (input_2) + 2 * (input_3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This means that the value of &lt;code&gt;hidden_1&lt;/code&gt; is very sensitive to the value of &lt;code&gt;input_1&lt;/code&gt;, not at all sensitive to &lt;code&gt;input_2&lt;/code&gt; and only slightly sensitive to &lt;code&gt;input_3&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you could say that &lt;code&gt;hidden_1&lt;/code&gt; is capturing a particular &lt;em&gt;aspect&lt;/em&gt; of the input, which you might call the &quot;&lt;code&gt;input_1&lt;/code&gt; is important&quot; aspect.&lt;/p&gt;&#10;&#10;&lt;p&gt;The output from &lt;code&gt;hidden_1&lt;/code&gt; is usually formed by passing the input through some function, so let's say you are using a &lt;a href=&quot;http://en.wikipedia.org/wiki/Sigmoid_function&quot; rel=&quot;nofollow&quot;&gt;sigmoid&lt;/a&gt; function. This function takes on values between 0 and 1; so think of it as a switch which says that either &lt;code&gt;input_1&lt;/code&gt; is important or it isn't.&lt;/p&gt;&#10;&#10;&lt;p&gt;So that's what the hidden layer does! It extracts &lt;em&gt;aspects&lt;/em&gt;, or &lt;em&gt;features&lt;/em&gt; of the input space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now weights can be negative too! Which means that you can get aspects like &quot;&lt;code&gt;input_1&lt;/code&gt; is important BUT ALSO &lt;code&gt;input_2&lt;/code&gt; takes away that importance&quot;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hidden_2 = 10 * (input_1) - 10 * (input_2 ) + 0 * (input_3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or &lt;code&gt;input_1&lt;/code&gt; and &lt;code&gt;input_3&lt;/code&gt; have &quot;shared&quot; importance:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hidden_3 = 5 * (input_1) + 0 * (input_2) + 5 * (input_3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;More Geometry&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you know some linear algebra, you can think geometrically in terms of projecting along certain directions. In the example above, I projected along the &lt;code&gt;input_1&lt;/code&gt; direction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's look at &lt;code&gt;hidden_1&lt;/code&gt; again, from above. Once the value at &lt;code&gt;input_1&lt;/code&gt; is big enough, the output of the sigmoid activation function will just stay at 1, &lt;em&gt;it won't get any bigger&lt;/em&gt;. In other words, more and more &lt;code&gt;input_1&lt;/code&gt; will make no difference to the output. Similarly, if it moves in the opposite (i.e. negative) direction, then after a point the output will be unaffected.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ok, fine. But suppose we don't want sensitivity in the direction of infinity in certain a direction, and we want it to be activated only for a certain &lt;em&gt;range&lt;/em&gt; on a line. Meaning for very negative values there is no effect, and for very positive values there is no effect, but for values between say, 5 and 16 you want it to wake up. This is where you would use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Radial_basis_function&quot; rel=&quot;nofollow&quot;&gt;radial basis function&lt;/a&gt; for your activation function.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The hidden layer extracts features of the input space, and the output layer translates them into the desired context. There may be much more to it than this, what with multi-layer networks and such, but this is what I understand so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; This page with its wonderful interactive graphs does a better job than my long and cumbersome answer above could ever do: &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html&quot; rel=&quot;nofollow&quot;&gt;http://neuralnetworksanddeeplearning.com/chap4.html&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-03T10:09:56.573" Id="63209" LastActivityDate="2014-09-25T08:18:07.683" LastEditDate="2014-09-25T08:18:07.683" LastEditorUserId="17541" OwnerUserId="17541" ParentId="63152" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Personally I like to use a &lt;strong&gt;stripplot&lt;/strong&gt; with jitter at least to get a feel for the data. The plot below is with lattice in R (sorry not ggplot2). I like these plots because they're very easy to interpret. As you say, one reason for this is that there isn't any transform.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df &amp;lt;- data.frame(y1 = c(rnorm(100),-4:4), y2 = c(rnorm(100),-5:3), y3 = c(rnorm(100),-3:5))&#10;df2 &amp;lt;- stack(df)&#10;library(lattice)&#10;stripplot(df2$values ~ df2$ind, jitter=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/l4SPm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/beeswarm/index.html&quot; rel=&quot;nofollow&quot;&gt;beeswarm&lt;/a&gt;&lt;/strong&gt; package offers a great alternative to stripplot (thanks to @January for the suggestion).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;beeswarm(df2$values ~ df2$ind)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hT7UA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;With your data, as it's approximately normally distributed, another thing to try might be a qqplot, &lt;strong&gt;qqnorm&lt;/strong&gt; in this case.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(1,3))&#10;for(i in 1:3) { qqnorm(df[,i]); abline(c(0,0),1,col=&quot;red&quot;) }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VrcEC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-07-03T10:28:21.607" Id="63211" LastActivityDate="2013-07-03T14:28:18.803" LastEditDate="2013-07-03T14:28:18.803" LastEditorUserId="25936" OwnerUserId="25936" ParentId="63203" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="67622" AnswerCount="2" Body="&lt;p&gt;My basic question is: What is an adaptive copula?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have slides from a presentation (unfortunately, I cannot ask the author of the slides) about adaptive copulae and I am not getting, what this means resp. what this is good for?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the slides:&#10;&lt;img src=&quot;http://i.stack.imgur.com/0F76A.png&quot; alt=&quot;sl1&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/F3H0r.png&quot; alt=&quot;sl2&quot;&gt;&#10;Then the slides continue with a change-point Test. I am wondering what this is about and why I need this in connection to copulae?&lt;/p&gt;&#10;&#10;&lt;p&gt;The slides end with an adaptively estimated parameter plot:&#10;&lt;img src=&quot;http://i.stack.imgur.com/qJXPm.png&quot; alt=&quot;sl3&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/jYIy9.png&quot; alt=&quot;sl4&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This seems to show, that my estimates are lagged behind. Any other interpretations, comments would be great!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-03T11:48:24.817" FavoriteCount="1" Id="63217" LastActivityDate="2014-03-20T05:40:32.223" OwnerUserId="27538" PostTypeId="1" Score="8" Tags="&lt;data-visualization&gt;&lt;copula&gt;" Title="What is an adaptive copula?" ViewCount="123" />
  
  
  <row AcceptedAnswerId="63247" AnswerCount="2" Body="&lt;p&gt;In the typical environment for multiple linear regression, we have that $Y = \mathbf{X}\beta + \epsilon$ where $\epsilon$ is iid $\mathcal{N}(0, \sigma^2I) $ where $\sigma^2$ is unknown. In this case, regression sum of squares (SSR) has $\text{df}= p - 1$ ($\text{df}$ = degrees of freedom) where $p$ is the number of parameters in the model. I have two questions based on this.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why is the $\text{df}= p - 1$, it seems to me that all $p$ of the&#10;parameters are free to vary.&lt;/li&gt;&#10;&lt;li&gt;If $\sigma^2$ is known, is the $\text{df}= p - 2$?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Edit:&#10;I am talking about sum of squares due to regression which is defined as $\sum (\hat{y}_i-\bar{y})^2$ this has degrees of freedom p-1 where p is the number of parameters in the model. SST which is defined as $\sum (y_i-\bar{y})^2$ has degrees of freedom n-1 and SSE (sum of squares due to error/residuals) is defined as $\sum (\hat{y}_i-y_i)^2$ and has degrees of freedom n-p.&lt;/p&gt;&#10;&#10;&lt;p&gt;SST = SSE + SSR and $df_{SST} = df_{SSR}+df_{SSE}$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-03T15:56:37.117" Id="63245" LastActivityDate="2013-12-05T16:41:41.180" LastEditDate="2013-07-03T19:08:10.803" LastEditorUserId="23801" OwnerUserId="23801" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;degrees-of-freedom&gt;" Title="DF for regression SS in multiple linear regression when sigma is known" ViewCount="420" />
  <row AnswerCount="1" Body="&lt;p&gt;Most internet resources--- papers, slides, etc--- on GP I've found take the mean function to be zero.&lt;br&gt;&#10;Chapter 15 of Murphy's book (Machine Learning: A Probabilistic Perspective) says this is because &lt;em&gt;'the GP is flexible enough to model the mean arbitrarily well, as we will see below'&lt;/em&gt;.&lt;br&gt;&#10;I didn't see that he ended up explaining this comment.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is:&lt;br&gt;&#10;Why do you not lose generality when you make a zero mean assumption?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-03T17:02:04.940" Id="63251" LastActivityDate="2013-08-03T02:48:49.080" LastEditDate="2013-07-03T17:23:41.380" LastEditorUserId="22468" OwnerUserId="27600" PostTypeId="1" Score="1" Tags="&lt;modeling&gt;&lt;gaussian-process&gt;" Title="What justifies the zero mean assumption for Gaussian processes?" ViewCount="77" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have 32/180 determinations of a laboratory marker below the limit of detection (LOD) and would like to estimate them by maximum likelihood estimation. By using Minitab's reliability / survival analysis I was able to find a well-fitting model (lognormal) and to estimate mean, SD, and percentiles. How can I estimate the single missing values that are below the LOD? Is it OK to just use the percentiles for the first 32 values (up to the 17.77th percentile)? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-03T21:18:33.603" Id="63282" LastActivityDate="2013-07-03T21:58:41.750" LastEditDate="2013-07-03T21:58:41.750" LastEditorUserId="7290" OwnerUserId="27611" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;maximum-likelihood&gt;&lt;minitab&gt;&lt;interval-censoring&gt;" Title="Estimate values below the limit of detection from a left-censored dataset" ViewCount="88" />
  
  <row Body="lm is the name of the linear model (i.e. multiple regression) function in the statistics package R. For linear models in general use the `linear-model` tag instead." CommentCount="0" CreationDate="2013-07-03T23:02:34.693" Id="63296" LastActivityDate="2013-07-03T23:12:25.070" LastEditDate="2013-07-03T23:12:25.070" LastEditorUserId="805" OwnerUserId="805" PostTypeId="4" Score="0" />
  
  <row Body="&lt;p&gt;The Kaplan-Meier estimator is &lt;em&gt;not&lt;/em&gt; biased when a large proportion of individuals are censored. One of the problems we often observe is that the majority of power for the log-rank test is derived from early failure times which are difficult to observe in KM curves. It does mean that the median survival time is an unreliable point estimate. However, the hazard ratio from a Cox model serves as a good estimate of the relative risk and is unbiased regardless of the amount of censoring that occurs. Both the log rank and the Cox model are adequate tests of survival that are unbiased in interval, right, and left censored data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The KM curves &lt;em&gt;are&lt;/em&gt; biased when there is informative censoring however. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-03T23:41:16.960" Id="63300" LastActivityDate="2013-07-03T23:41:16.960" OwnerUserId="8013" ParentId="57079" PostTypeId="2" Score="4" />
  
  <row Body="In the statistics package R, a data frame holds a set of observations (records, rows) on a number of variables (columns), arranged like a  matrix, but the columns may be of different types. 
  <row Body="&lt;p&gt;I like &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471006262&quot; rel=&quot;nofollow&quot;&gt;Gaussian Mixture models&lt;/a&gt; (GMM's).  &lt;/p&gt;&#10;&#10;&lt;p&gt;One of their features is that, in &lt;a href=&quot;http://en.wikipedia.org/wiki/Probit&quot; rel=&quot;nofollow&quot;&gt;probit domain&lt;/a&gt;, they act like piecewise interpolators.  One implication of this is that they can act like a replacement basis, a universal approximator.  This means that for non-gaussian distributions, like lognormal, weibull, or crazier non-analytic ones, as long as some criteria are met - the GMM can approximate the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you know the parameters of the AICc or BIC optimal approximation using GMM then you can project that to smaller dimensions.  You can rotate it, and look at the principal axes of the components of the approximating GMM.&lt;/p&gt;&#10;&#10;&lt;p&gt;The consequence would be an informative and visually accessible way to look at the most important parts of higher dimensional data using our 3d-viewing visual perception.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: (sure thing, whuber)&lt;/p&gt;&#10;&#10;&lt;p&gt;There are several ways to look at the shape.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You can look at trends in the means.  A lognormal is approximated by a series of Gaussians whos means get progressively closer and whose weights get smaller along the progression.  The sum approximates the heavier tail.  In n-dimensions, a sequence of such components would make a lobe.  You can track distances between means (convert to high dimension) and direction cosines between as well.  This would convert to much more accessible dimensions.&lt;/li&gt;&#10;&lt;li&gt;You can make a 3d system whose axes are the weight, the magnitude of the mean, and the magnitude of the variance/covariance.  If you have a very high cluster-count, this is a way to view them in comparison with each other.  It is a valuable way to convert 50k parts with 2k measures each into a few clouds in a 3d space.  I can execute process control in that space, if I choose.  I like the recursion of using gaussian mixture model based control on components of gaussian mixture model fits to part parameters.&lt;/li&gt;&#10;&lt;li&gt;In terms of de-cluttering you can throw away by very small weight, or by weight per covariance, or such.  &lt;/li&gt;&#10;&lt;li&gt;You can plot the GMM cloud in terms of BIC, $ R^2$, Mahalanobis distance to components or overall, probability of membership or overall.  &lt;/li&gt;&#10;&lt;li&gt;You could look at it like &lt;a href=&quot;http://www.abc.net.au/science/experimentals/experiments/episode13_1.htm&quot; rel=&quot;nofollow&quot;&gt;bubbles intersecting&lt;/a&gt;.  The location of equal probability (zero Kullback-Leibler divergence) exists between each pair of GMM clusters.  If you track that position, you can filter by probability of membership at that location.  It will give you points of classification boundaries.  This will help you isolate &quot;loners&quot;.  You can count the number of such boundaries above the threshold per member and get a list of &quot;connectedness&quot; per component.  You can also look at angles and distances between locations.&lt;/li&gt;&#10;&lt;li&gt;You can  resample the space using random numbers given the Gaussian PDFs, and then perform principle component analysis on it, and look at the eigen-shapes, and eigenvalues associated with them.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;What does shape mean?  &lt;em&gt;They say specificity is the soul of all good communication.&lt;/em&gt;&#10;What do you mean about &quot;measure&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideas about what it can mean:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Eyeball norm sense/feels of general form.  (extremely qualitative, visual accessibility)&lt;/li&gt;&#10;&lt;li&gt;measure of GD&amp;amp;T shape (coplanarity, concentricity, etc)  (extremely quantitative)&lt;/li&gt;&#10;&lt;li&gt;something numeric (eigenvalues, covariances, etc...)&lt;/li&gt;&#10;&lt;li&gt;a useful reduced dimension coordinate (like GMM parameters becoming dimensions)&lt;/li&gt;&#10;&lt;li&gt;a reduced noise system (smoothed in some way, then presented)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Most of the &quot;several ways&quot; are some variation on these.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-04T06:01:16.910" Id="63316" LastActivityDate="2013-07-06T20:45:49.527" LastEditDate="2013-07-06T20:45:49.527" LastEditorUserId="22452" OwnerUserId="22452" ParentId="39027" PostTypeId="2" Score="4" />
  
  
  
  
  
  <row AcceptedAnswerId="63399" AnswerCount="1" Body="&lt;p&gt;There are 4 groups, each one having two 2 variables, X and Y. I am trying to understand the regression between them. For X variable I have 2 categories, positive and negative. I am not understanding how to compare the slopes and intercepts between X (positive and negative) and Y. Do I have to invert the negative values?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-05T06:14:24.840" Id="63396" LastActivityDate="2013-07-08T06:57:21.573" LastEditDate="2013-07-08T06:57:21.573" LastEditorUserId="26863" OwnerUserId="26863" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;spss&gt;" Title="Compare the slopes and intercepts between 2 variables in SPSS" ViewCount="352" />
  
  <row Body="&lt;p&gt;The misunderstanding is your first premise &quot;F test and $t$-test are performed between two populations&quot;, this is incorrect or at least incomplete. The $t$-test that is next to a coefficient tests the null hypothesis that that coefficient equals 0. If the corresponding variable is binary, for example 0 = male, 1 = female, then that describes the two populations but with the added complication that you also adjust for the other covariates in your model. If that variable is continuous, for example years of education, you can think of comparing someone with 0 years of education with someone with 1 years of education, and comparing someone with 1 years of education with someone with 2 years of education, etc, with the constraint that each step has the same effect on the expected outcome and again with the complication that you adjust for the other covariates in your model. &lt;/p&gt;&#10;&#10;&lt;p&gt;An F-test after linear regression tests the null hypothesis that all coefficients in your model except the constant are equal to 0. So the groups that you are comparing is even more complex.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-05T06:54:23.500" Id="63400" LastActivityDate="2013-07-05T06:54:23.500" OwnerUserId="23853" ParentId="63398" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two groups of samples, each group of size 8. Each sample comes from an individual $i = 1 \dots 16$ and is a proportion $m_i$ of DNA methylated sites at a specific position in a genome. This can take a value between 0 and 1. We can safely assume that there is no variability between samples if they were taken from the same individual (i.e., the proportion in a single individual is calculated from a very large population of DNA strands). The individuals from both groups are matched.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, more than one position has been tested (as above). Actually, almost half a million positions were tested, each of them corresponding to a proportion as described above. Effectively I have a matrix of the size $16 \times 5\cdot 10^5$, and each number in this matrix is a value between 0 and 1 corresponding to the proportion of DNA strands that are methylated in a specific sample at a specific genomic position.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that I have a very large number of positions in a genome tested, I have an idea what the distribution of these measurements can be. Most of the sites are either methylated or not methylated -- the value $m_i$ is close to 1 or close to 0:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0EfZz.png&quot; alt=&quot;Histogram of $m_i$ values&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;H0 is that the groups do not differ. I have applied a Wilcoxon signed rank test and a t test (although clearly the assumptions of the t test do not hold, but it is generally considered robust), the results were similar. However, I wonder whether there might be better approach that takes the specific distribution of the $m_i$ values.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-05T07:06:24.370" Id="63402" LastActivityDate="2013-09-12T16:11:04.007" LastEditDate="2013-09-12T16:11:04.007" LastEditorUserId="14803" OwnerUserId="14803" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;proportion&gt;" Title="Two samples, proportions" ViewCount="76" />
  <row Body="A proportion is the fraction of some total that is of a particular kind, either (i) as a count of one type of thing out of a total count, or  (ii) as a component of a continuous variable." CommentCount="0" CreationDate="2013-07-05T07:27:07.600" Id="63404" LastActivityDate="2013-07-05T07:41:10.077" LastEditDate="2013-07-05T07:41:10.077" LastEditorUserId="805" OwnerUserId="805" PostTypeId="4" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The problem with studying the change of observables in time (such dataset is formally called &quot;time series&quot;), is that for each case observations in different points in time are &lt;em&gt;dependent&lt;/em&gt;, which forces us at least to use tests that are valid for paired data (when comparing two data points), or &lt;a href=&quot;http://en.wikipedia.org/wiki/Error_correction_model&quot; rel=&quot;nofollow&quot;&gt;VECM&lt;/a&gt; if you want to analyze all time points at once. In particular, significance test of regressions are invalid in such setup. &lt;/p&gt;&#10;&#10;&lt;p&gt;When you treat each time point as a separate independent group, you are likely to greatly exaggerate significance. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The question whether use statistical tests or not for census data depends on what is your hypothesis. If you want to infer for this particular group of people in this particular point of time (if the census actually concerns people), than there is no need for significance indeed. But very often we want implicitly to make an inference about some bigger population, e.g. whole Europe. In that case we need to treat our dataset as a sample. Or we want to make inference about our population &lt;em&gt;in future&lt;/em&gt;; in this case we would use VECM or similar technique valid for time series.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-05T07:43:52.083" Id="63408" LastActivityDate="2013-07-05T07:49:22.170" LastEditDate="2013-07-05T07:49:22.170" LastEditorUserId="10069" OwnerUserId="10069" ParentId="63370" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Often these different tests test subtly different hypotheses. For example, many tests that are used to check for normality/Gaussianity of a distribution actually look at very specific deviations from normality (e.g. no skewness) and tests differ with respect to what quantity they exactly look at. So I would start with finding out what the difference is between these tests and decide which hypothesis is the one that is really of interest to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the different tests really test the same null hypothesis and result in different $p$-values, that it is an issue of power. Typically, there is a literature comparing these tests, and you can just look up which test best applies to your situation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that, my first guess would be that your different tests actually test similar but not the same hypotheses. In general, once you understand where the differences between tests come from, the choice becomes pretty obvious. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-05T11:37:50.390" Id="63435" LastActivityDate="2013-07-05T13:58:17.760" LastEditDate="2013-07-05T13:58:17.760" LastEditorUserId="23853" OwnerUserId="23853" ParentId="63431" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;The best solution depends on the exact nature of the problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;One literature you might want to look into is the one around &quot;age-period-cohort models&quot;. We might expect that someone's age, the era in which people grow up (the cohort), and the period in which the question was asked all have their own influence on a person behavior, but they are linearly dependent: year of birth (cohort) = period - age. There is a whole set of models that put different constraints on the different parameters in order to try to identify the different effects. These models are often refered to as &quot;age-period-cohort models&quot;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-05T13:08:51.003" Id="63443" LastActivityDate="2013-07-05T13:08:51.003" OwnerUserId="23853" ParentId="63442" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;From &lt;a href=&quot;http://www.stat.yale.edu/Courses/1997-98/101/condprob.htm&quot; rel=&quot;nofollow&quot;&gt;stat.yale.edu&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;The conditional probability of an event B is the probability that the event will occur given the knowledge that an event A has already occurred. This probability is written P(B|A), notation for the probability of B given A. In the case where events A and B are independent (where event A has no effect on the probability of event B), the conditional probability of event B given event A is simply the probability of event B, that is P(B).&#10;If events A and B are not independent, then the probability of the intersection of A and B (the probability that both events occur) is defined by &#10;P(A and B) = P(A)P(B|A).&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Excerpt reference:&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_probability&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-05T14:28:21.970" Id="63458" LastActivityDate="2013-07-05T15:33:39.903" LastEditDate="2013-07-05T15:33:39.903" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  
  
  
  
  
  <row Body="&lt;p&gt;Treating Likert data as ratio would be bizarre as there is not a natural zero point. You could, for example, translate the entire scale by any constant and the meaning would be the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;interval&quot; approach is very common in the literature I am most familiar with (psychology). However, I cringe when reading a paper that treats Likert data as interval, because there is probably a different psychological &quot;distance&quot; between the upper end points of a scale than between middle end points. I am not aware of psychological research to this end specifically using Likert scales (although such studies surely must exist), but similar psychological distortion of continuous measures occurs in other settings. For example, subjective judgments of the probability of some event are not linear with respect to the event's actual probability. Instead, events with probability near the endpoints of the scale (0% and 100%) are perceived as regressing to the midpoint (50%). I will see if I can track down the reference for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;That leaves us with ordinal and nominal. Ordinal requires the assumption that the scale is perceived as 5&gt;4&gt;3&gt;2&gt;1, for example, which appears reasonable. An approach I have used is to look at the proportion of some response of interest, such as &quot;agree&quot; and &quot;strongly agree.&quot; You will have less power using such an approach than if you treat the scale as interval, but I consider this an inherent drawback to these types of scales. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-05T21:36:11.443" Id="63488" LastActivityDate="2013-07-05T21:36:11.443" OwnerUserId="11511" ParentId="63487" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In the area of machine learning, most of the algorithms are intended for small n large p problems. I am familiar with the statistical techniques of PCA, etc but was wondering what algorithms are available in the machine learning area for similar analysis work. The aim is to determine the effect of a limited number of predictor variables on the outcome using robust methods such as pca in the ML domain,&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" ClosedDate="2015-02-06T09:27:17.050" CommentCount="8" CreationDate="2013-07-06T00:57:00.230" Id="63493" LastActivityDate="2013-07-06T00:57:00.230" OwnerUserId="23891" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;pca&gt;&lt;cart&gt;" Title="Large n small p regression - Machine Learning" ViewCount="162" />
  
  <row AcceptedAnswerId="63508" AnswerCount="2" Body="&lt;p&gt;I'm new to R, so please be gentle.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was under the impression that &lt;code&gt;rstandard(model)&lt;/code&gt; returns the z-scores of the residuals in &lt;code&gt;model&lt;/code&gt;. However, when I standardized the residuals myself in z, the result was different. In fact, &lt;code&gt;rstandard(model)&lt;/code&gt; had a mean different from 0 and had a standard deviation different from 1. The differences seem to be nonnegligible. What exactly does &lt;code&gt;rstandard(model)&lt;/code&gt; do? Or am I doing something wrong here?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x = rnorm(30, mean = 100, sd = 15)&#10;&amp;gt; y = rnorm(30, mean = 100, sd = 15)&#10;&amp;gt; model = lm(y ~ x)&#10;&amp;gt; z.res = residuals(model)/sd(residuals(model)) #standardizing it myself&#10;&amp;gt; rstandard(model) - z.res #difference between rstandard and what i did&#10;            1             2             3             4             5 &#10;-4.422354e-04  1.556269e-04 -4.576832e-03 -1.274350e-03  1.048068e-01 &#10;            6             7             8             9            10 &#10;-2.333922e-02  1.820134e-02 -3.307542e-03  3.368978e-02 -1.804108e-04 &#10;           11            12            13            14            15 &#10;-1.100621e-01 -1.343715e-03 -1.300427e-03  1.509862e-03  3.246602e-03 &#10;           16            17            18            19            20 &#10; 3.734255e-03 -1.821539e-06 -1.153190e-02 -1.713254e-06 -2.185101e-02 &#10;           21            22            23            24            25 &#10;-2.681935e-02  2.562472e-03 -4.721114e-02 -1.084481e-04 -3.430827e-03 &#10;           26            27            28            29            30 &#10; 4.149684e-04  7.705807e-04  2.166815e-03  2.537837e-02  4.182761e-04 &#10;&amp;gt; mean(z.res) &#10;[1] -9.428041e-18 &#10;#as expected of z-scores, mean is 0&#10;&amp;gt; sd(z.res)&#10;[1] 1 &#10;#as expected of z-scores standard deviation is 1&#10;&amp;gt; mean(rstandard(model)) &#10;[1] -0.001990908&#10;#not really 0&#10;&amp;gt; sd(rstandard(model)) &#10;[1] 1.019699&#10;#not really 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also, the way I understood &lt;a href=&quot;http://stats.stackexchange.com/questions/52522/standardized-residuals-in-rs-lm-output&quot;&gt;Standardized residuals in R&amp;#39;s lm output&lt;/a&gt;, &lt;code&gt;rstandard&lt;/code&gt; is actually studentized residuals. But isn't there already &lt;code&gt;rstudent&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using R version 2.14.1 in Xubuntu 12.04.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-06T11:02:36.190" Id="63503" LastActivityDate="2013-07-06T15:53:54.940" OwnerUserId="25811" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;residuals&gt;&lt;standardization&gt;&lt;diagnostic&gt;&lt;lm&gt;" Title="does rstandard standardize in z?" ViewCount="508" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a dataset of 371 observations. When I run &lt;code&gt;coxph&lt;/code&gt; with numeric variables it works fine. However, when I try to add factor (categorical) variables it returns “Ran out of iterations and the model did not converge”.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of note, when I restructure all factors to binary variables with dummy and use glmnet-lasso the model converges.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are examples of the code and output (including summary description of the variables):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; maxSTree.cox &amp;lt;- coxph (Surv(time,status)~Chemo_Simple, data=dataset)&#10;&#10;Warning message:&#10;In fitter(X, Y, strats, offset, init, control, weights = weights,  :&#10;  Ran out of iterations and did not converge&#10;&#10;&amp;gt; summary (dataset$Chemo_Simple)&#10;         Anthra-HDAC       Anthra-Plus       ArsenicAtra              ATRA           ATRA-GO &#10;                0               163                 2                12                 0                 2 &#10;         ATRA_IDA Demeth-HistoneDAC          Flu-HDAC     Flu-HDAC-plus         HDAC-Clof         HDAC-only &#10;                0                34                37                 4                24                 1 &#10;        HDAC-Plus        LowArac+/-       LowDAC-Clof         MYLO+IL11    No Rx in MDACC            Phase1 &#10;                4                 8                30                 5                 1                 5 &#10;              SCT    StdARAC-Anthra      StdAraC-Plus          Targeted         VNP40101M &#10;                0                 0                 0                13                23 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-07-06T12:01:38.380" Id="63504" LastActivityDate="2013-07-06T15:51:57.087" LastEditDate="2013-07-06T12:48:46.313" LastEditorUserId="22047" OwnerUserId="27700" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;convergence&gt;&lt;cox-model&gt;" Title="R's coxph won't converge when I include factor (categorical) variables" ViewCount="516" />
  
  <row Body="&lt;p&gt;The two functions do different things, as I understand it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;residuals(model)&lt;/code&gt; gives the response minus the fitted values. (from help for &lt;code&gt;lm&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;rstandard(model)&lt;/code&gt; is part of leave one out influence diagnostics (from help for &lt;code&gt;rstandard&lt;/code&gt;), which says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;This suite of functions can be used to compute some of the regression&#10;  (leave-one-out deletion) diagnostics for linear and generalized linear&#10;  models discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg&#10;  (1982), etc.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You can examine exactly what rstandard does by looking at the code. For lm, you can do this with &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; methods(rstandard) &#10;&amp;gt; getAnywhere(rstandard.lm)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-06T13:44:49.227" Id="63507" LastActivityDate="2013-07-06T15:53:54.940" LastEditDate="2013-07-06T15:53:54.940" LastEditorUserId="1390" OwnerUserId="686" ParentId="63503" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;em&gt;There have been some great answers to this question, I just thought I'd summarise an interesting reference that provides a fairly rigorous discussion of estimators.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The virtual laboratories &lt;a href=&quot;http://www.math.uah.edu/stat/point/Estimators.html&quot; rel=&quot;nofollow&quot;&gt;page on estimators&lt;/a&gt;  defines &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a &lt;strong&gt;statistic&lt;/strong&gt; as &quot;an observable function of the outcome variable&quot;.&lt;/li&gt;&#10;&lt;li&gt;&quot;in the technical sense, a &lt;strong&gt;parameter&lt;/strong&gt; $\theta$ is a function of the distribution of X&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The concept of a function of a distribution is a very general idea. Thus, every example provided above could be seen as a function of a certain distribution. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Every quantile, including the min, median, 25th quantile, the max can be a function of a distribution. &lt;/li&gt;&#10;&lt;li&gt;Skewness is a function of a distribution. If that population distribution is normal, then these will be zero, but that does not stop the calculation of these values.&lt;/li&gt;&#10;&lt;li&gt;Counting the number of correlations greater than a certain value is a function of the covariance matrix which in turn is a function of a multivariate distribution.&lt;/li&gt;&#10;&lt;li&gt;R-squared is a function of the distribution.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-07-06T15:08:01.310" Id="63509" LastActivityDate="2013-07-06T15:08:01.310" OwnerUserId="183" ParentId="63386" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;With missing data, deciding what to do is (almost) all about deciding what assumptions you can validly make about the missingness.&lt;/p&gt;&#10;&#10;&lt;p&gt;The three main options are: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Missing Completely at Random (MCAR) - i.e. there are no possible predictors of missingness. In this case, you could validly drop those individuals with missing data and work with the complete observations only (this is called a complete case analysis). However, this is generally an unrealistic assumption in my experience. If you do a complete case analysis and the MCAR assumption does not hold, then any conclusions you want to draw from your results will only be valid for that specific set of people who have complete data. It is invalid to make inference about the whole population (or even whole sample) if MCAR does not hold, when using complete case analysis.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Missing at Random (MAR) - in this case, you know why some individuals have missing data and others do not (there are some variables which predict missingness) but within strata of those variables the data is truly randomly missing. In this case, you can use multiple imputation or other methods such as inverse probability weighting for non-missingness (IPW). Under MAR, these methods allow you to make inference about the entire sample, and therefore about the population from which your samples arose. You are right that you have a lot of data to impute, which may cause a problem. I don't have enough experience with multiple imputation to advise how best to proceed. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Not Missing at Random (NMAR) - there is a definite pattern to which data is missing and there are no variables which can be used to create strata within which the data is missing at random. If this is your situation, I believe you will need to redo the experiment as there is no valid solution I am aware of for analyzing the data as is (assuming that you want to make inference about some larger population).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;To be honest, given the very high percentage of missing data, I would probably suggest starting over if that's not too costly in terms of time and money. I would try to do multiple imputation or IPW if you have variables to predict missingness, but you may not have much power with so few subjects and so much missingness. You will also need to be think about whether missingness on one measure affects missingness on the other measure - this would make the analysis more complicated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-06T19:46:37.053" Id="63523" LastActivityDate="2013-07-06T19:46:37.053" OwnerUserId="18563" ParentId="63520" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;em&gt;[The initial title &quot;Measurement of similarity for hierarchical clustering trees&quot; was later changed by @ttnphns to better reflect the topic]&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am performing a number of &lt;strong&gt;hierarchical cluster analyses&lt;/strong&gt; on a dataframe of patient records (e.g. similar to &lt;a href=&quot;http://www.biomedcentral.com/1471-2105/5/126/figure/F1?highres=y&quot; rel=&quot;nofollow&quot;&gt;http://www.biomedcentral.com/1471-2105/5/126/figure/F1?highres=y&lt;/a&gt;) &lt;/p&gt;&#10;&#10;&lt;p&gt;I am experimenting with different &lt;strong&gt;distance&lt;/strong&gt; measures, different parameter weights and different hierarcical &lt;strong&gt;methods&lt;/strong&gt;, to understand their impact on the final clusters / structure/view of the tree (dendrogram). My question whether there is a standard calculation / measure to calculate the difference between different hierarchical trees, and how to implement this in R (e.g. to quantify that some trees are nearly identical, and that some are drastically different).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-07T07:57:20.793" FavoriteCount="5" Id="63546" LastActivityDate="2015-01-08T20:50:04.217" LastEditDate="2015-01-08T19:08:40.463" LastEditorUserId="3277" OwnerUserId="26564" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;clustering&gt;&lt;distance-functions&gt;&lt;similarities&gt;&lt;dendrogram&gt;" Title="Comparing hierarchical clustering dendrograms obtained by different distances &amp; methods" ViewCount="1335" />
  
  <row Body="&lt;p&gt;To generalize the results from survey data for the population, you need sample weights. If you are using the microdata/survey data from BLS, you can find the weights in the data. Of course, you have to look into codebook or data dictionary if you can't locate. See &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=why%20use%20weights%20in%20survey&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CCoQFjAA&amp;amp;url=http://help.pop.psu.edu/help-by-statistical-method/weighting/Introduction%2520to%2520survey%2520weights%2520pri%2520version.ppt&amp;amp;ei=J1vZUZ-OH9KTqwH01YHYCQ&amp;amp;usg=AFQjCNGRb2ouJZWMTgtIED3Y1TWll3ZDGA&amp;amp;bvm=bv.48705608,d.aWM&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; introduction slides.&lt;/p&gt;&#10;&#10;&lt;p&gt;For analysis purpose you need to use survey analysis tool which is &lt;a href=&quot;http://cran.r-project.org/web/packages/survey/index.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; in R and &lt;a href=&quot;http://www.stata.com/features/survey-commands/&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; is Stata&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-07T12:17:09.867" Id="63553" LastActivityDate="2013-07-07T12:24:07.013" LastEditDate="2013-07-07T12:24:07.013" LastEditorUserId="14860" OwnerUserId="14860" ParentId="63543" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In statistics and optimization, statistical errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its &quot;theoretical value&quot;. The error of an observed value is the deviation of the observed value from the (unobservable) true function value, while the residual of an observed value is the difference between the observed value and the estimated function value.&lt;/p&gt;&#10;&#10;&lt;p&gt;The distinction is most important in regression analysis, where it leads to the concept of studentized residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Reference: &lt;a href=&quot;http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-07T12:21:01.313" Id="63554" LastActivityDate="2013-07-07T12:30:41.387" LastEditDate="2013-07-07T12:30:41.387" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  
&#10;" CommentCount="0" CreationDate="2013-07-07T13:53:15.300" Id="63563" LastActivityDate="2013-07-07T14:05:14.730" LastEditDate="2013-07-07T14:05:14.730" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="4" Score="0" />
  <row AcceptedAnswerId="63570" AnswerCount="1" Body="&lt;p&gt;I have conducted an experiment with multiple (categorical) conditions per subject, and multiple subject measurements.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data-frame in short: A subject has one property, &lt;code&gt;is_frisian&lt;/code&gt; which is either 0 or 1 depending on the subject. And it is tested for two conditions, &lt;code&gt;person&lt;/code&gt; and &lt;code&gt;condition&lt;/code&gt;. The measurement variable is &lt;code&gt;error&lt;/code&gt;, which is either 0 or 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;My mixed linear model in R is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; model &amp;lt;- lmer(error~is_frisian*condition*person+(1|subject_id), data=output)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, the residuals plot of this model gives an unexpected (?) result.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nz2KY.png&quot; alt=&quot;Residuals lmer model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I was taught that this plot should show randomly scattered points, and they should be normal distributed. When plotting the density of the fitted and the residuals, it shows a reasonable normal distribution. The lines you can see in the graph, however, how is this to be explained? And is this okay?&lt;/p&gt;&#10;&#10;&lt;p&gt;The only thing I could come up with is that the graph has two lines due to the categorical variables. The output variable &lt;code&gt;error&lt;/code&gt; is either 0 or 1. But I do not have that much knowledge of the underlying system to confirm this. And then again, the lines also seem to have a low negative slope, is this then perhaps a problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; model &amp;lt;- glmer(error~is_frisian*condition*person + (1|subject_id), data=output, family='binomial')&#10;&amp;gt; binnedplot(fitted(model),resid(model))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives the following result:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XMXFx.png&quot; alt=&quot;binned residual plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;FINAL EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The density-plots have been omitted, they have nothing to do with satisfaction of assumptions in this case. For a list of assumptions on logistic regression (when using family=binomial), &lt;a href=&quot;https://www.statisticssolutions.com/academic-solutions/resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/&quot; rel=&quot;nofollow&quot;&gt;see here at statisticssolutions.com&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-07-07T15:29:12.987" FavoriteCount="4" Id="63566" LastActivityDate="2013-07-08T15:17:25.990" LastEditDate="2013-07-08T15:17:25.990" LastEditorUserId="6029" OwnerUserId="27719" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;residuals&gt;" Title="Unexpected residuals plot of mixed linear model using lmer (lme4 package) in R" ViewCount="3008" />
  <row AnswerCount="1" Body="&lt;p&gt;I have the following data: I have participants who answered on their favorite out of 6 positions of various information types on a display (e.g. 1 = bottom left, 2 = top left etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;I can now say for example the information “CPU status” was preferred to be shown on the top left by most of the participants, i.e. I can report the frequencies of the answers. But how can I test if these frequencies are not due to sampling or chance? In other words, how can i say that position a was significantly more often chosen than position b and c? &lt;/p&gt;&#10;&#10;&lt;p&gt;My only idea is to do a chi squared test with the two most frequent positions per information, but this seems cumbersome to me.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-07T15:45:25.290" Id="63568" LastActivityDate="2013-07-07T18:21:39.033" LastEditDate="2013-07-07T16:25:08.720" LastEditorUserId="6029" OwnerUserId="26345" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;&lt;frequency&gt;" Title="Significance of most frequent answer" ViewCount="96" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am new to statistical pattern recognition and trying to learn.To begin with I am trying to work with two class problems and trying to classify motion activities as mentioned in the paper &quot;Object Trajectory-Based Activity Classification and&#10;Recognition using Hidden Markov Models&quot;. In this paper, the authors have used GMM for estimating the PDF for each motion pattern class. But I have several doubts and shall be grateful for the following answers&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;If there are 5 experimental data with 4 feature vectors(4 columns of data) of length 1000 samples/rows for each of the two classes(say,running and walking) then is the pdf from Gaussian mixture model obtained for each of the 5 experimental data or only for each class? If it is for each class, then how is it done? I really  do not know and please pardon if this sounds too trivial.&lt;/li&gt;&#10;&lt;li&gt;Is it always necessary to find the pdf of the data before classification task in general? Does the pdf control in deciding which model based approach to choose for classification purpose? &lt;/li&gt;&#10;&lt;li&gt;Are there any such model based approaches for classification?&lt;/li&gt;&#10;&lt;li&gt;As far as my understanding goes, GMM is a clustering algorithm like k-means. So, are there other ways other than using Hidden Markov Model (I want to avoid HMM due to further complexities in understanding it) for classification with GMM?&lt;/li&gt;&#10;&lt;li&gt;Can GMM be combined with regression models like AR,MA,ARMA for model based classification? If so, then pointers to resources which explain this shall be helpful.&lt;/li&gt;&#10;&lt;li&gt;Is there a Matlab implementation of multivariate GMM for the said application of classification?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-07T19:50:11.190" Id="63584" LastActivityDate="2014-01-26T20:03:37.997" OwnerUserId="9266" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;matlab&gt;&lt;pattern-recognition&gt;&lt;unsupervised-learning&gt;&lt;gaussian-mixture&gt;" Title="How to use Gaussian mixture model for multivariate pattern classification" ViewCount="966" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have data in a dataframe with the following columns: date, time, symbol, price&#10;I am attempting to run the following model in R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; price ~ factor(time) + factor(symbol)*factor(time) + 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where the first coefficient comes from a dummy variable of the time column in the dataframe and the second coefficient comes from the product of the dummy variable of the time column and the symbol column. I used lm() to attempt to do the model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; fit&amp;lt;-lm(price~factor(time) + factor(symbol)*factor(time) + 0, data=mydata) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, that didn't work as it gets me some really weird coefficients that didn't say anything. I guessing as that was matching effects in some way and not just the straight forward product.&#10;I tried making a new variable by multiplying the product of the factors and then putting it in the regression but that didn't work as r told me that * is not in the possible operations for factor().&#10;So, how can you multiply two factors in a linear regression?&#10;Your help is much appreciated, thank you in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;edit: when I say weird I guess I should more say not what I want. I get coefficients for the firm, then coefficients for time then coefficients for firm and time. However, I only want the coefficients for firm and time to impact the model and ultimately I am really interested in what this does to the intraday effects which are approximated using the time dummy variables. If I am getting the interaction effects of the firm, time and then firm and time then this will impact the first coefficient as opposed to only firm and time affecting the first coefficient?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-06-27T04:07:50.840" Id="63597" LastActivityDate="2015-02-02T11:40:57.993" OwnerDisplayName="samooch" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;" Title="R: product of dummy variables when using factor in a linear regression" ViewCount="693" />
  <row Body="&lt;p&gt;Consider this example:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(5)            # this line will allow you to run these commands on your&#10;                       # own computer &amp;amp; get *exactly* the same output&#10;x = rnorm(50)&#10;y = rnorm(50)&#10;&#10;fit = lm(y~x)&#10;&#10;summary(fit)&#10;&#10;Call:&#10;lm(formula = y ~ x)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-2.04003 -0.43414 -0.04609  0.50807  2.48728 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)&#10;(Intercept) -0.00761    0.11554  -0.066    0.948&#10;x            0.09156    0.10901   0.840    0.405&#10;&#10;Residual standard error: 0.8155 on 48 degrees of freedom&#10;Multiple R-squared: 0.01449,    Adjusted R-squared: -0.006046 &#10;F-statistic: 0.7055 on 1 and 48 DF,  p-value: 0.4051 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The question, I'm guessing, is how to figure out the regression equation from R's summary output.  Algebraically, the equation for a simple regression model is:&lt;br&gt;&#10;$$&#10;\hat y_i = \hat\beta_0 + \hat\beta_1 x_i + \hat\varepsilon_i  \\&#10;\text{where } \varepsilon\sim\mathcal N(0,~\hat\sigma^2)&#10;$$&#10;We just need to map the &lt;code&gt;summary.lm()&lt;/code&gt; output to these terms.  To wit:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$\hat\beta_0$ is the &lt;code&gt;Estimate&lt;/code&gt; value in the &lt;code&gt;(Intercept)&lt;/code&gt; row (specifically, &lt;code&gt;-0.00761&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;$\hat\beta_1$ is the &lt;code&gt;Estimate&lt;/code&gt; value in the &lt;code&gt;x&lt;/code&gt; row (specifically, &lt;code&gt;0.09156&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;$\hat\sigma$ is the &lt;code&gt;Residual standard error&lt;/code&gt; (specifically, &lt;code&gt;0.8155&lt;/code&gt;)  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Plugging these in above yields:&lt;br&gt;&#10;$$&#10;\hat y_i = -0.00761~+~0.09156 x_i~+~\hat\varepsilon_i  \\&#10;\text{where } \varepsilon\sim\mathcal N(0,~0.8155^2)&#10;$$&#10;For a more thorough overview, you may want to read this thread: &lt;a href=&quot;http://stats.stackexchange.com/questions/5135/&quot;&gt;Interpretation of R's lm() output&lt;/a&gt;.  &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-08T03:50:19.067" Id="63602" LastActivityDate="2013-07-09T03:06:50.567" LastEditDate="2013-07-09T03:06:50.567" LastEditorUserId="7290" OwnerUserId="7290" ParentId="63600" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;This &lt;a href=&quot;http://www.cs.odu.edu/~mukka/cs495s13/Lecturenotes/Chapter5/recallprecision.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;, &lt;em&gt;Evaluating a classification model – What does precision and recall tell me?&lt;/em&gt;, from &lt;a href=&quot;http://www.compumine.com/&quot; rel=&quot;nofollow&quot;&gt;Compumine&lt;/a&gt; provides a simple introduction to the confusion matrix and the measures derived from it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It helps to create the confusion matrix, precision, recall, specificity and accuracy.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-08T03:56:07.577" Id="63603" LastActivityDate="2013-07-13T15:12:33.647" LastEditDate="2013-07-13T15:12:33.647" LastEditorUserId="7290" OwnerUserId="27737" ParentId="51296" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;If you are asking about $V=R$ then it is indeed a possibility. There is no reason all rejections can't be false when &quot;not all null hypotheses are true&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-08T07:55:50.590" Id="63614" LastActivityDate="2013-07-08T07:55:50.590" OwnerUserId="6961" ParentId="63592" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="63633" AnswerCount="1" Body="&lt;p&gt;I'm planning to conduct an experiment soon, but not really sure yet what kind of analysis I should do afterwards. What I want to know is whether including an intervention in the experiment leads to different behavior. Besides that, I also want to know if attitude also changes due to the intervention. To summarize:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I first measure attitude of people with a questionnaire (bipolar adjective scale) and then measure whether they are doing a certain behavior (dichotomous)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I introduce an intervention, measure their attitude again and see whether there is a change in doing that same behavior.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I first thought I needed to do a repeated measures to see if there is any change behavior and after the intervention, but I got kinda stuck with the nature of my variables (dichotomous). Now, I am thinking of making a variable (intervention yes/no) that would act as the IV and then test it on the behavior (yes/no) with a one sample t-test. After that test their attitude (IV) on behavior in the before and after the intervention individually.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody confirm this? And how would I be able to compare attitude on behavior before and after the intervention?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-08T11:02:52.383" Id="63628" LastActivityDate="2013-07-08T11:50:39.627" LastEditDate="2013-07-08T11:14:28.237" LastEditorUserId="6029" OwnerUserId="27749" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;t-test&gt;" Title="Repeated measures or one sample t-test" ViewCount="202" />
  
  
  
  <row Body="&lt;p&gt;First of all, you may want to have a look at the &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;Elements of Statistical Learning&lt;/a&gt;. They discuss variable selection as well as different regularization techniques in chapter 3 (never mind it being about regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you think your variables are basically not correlated, and should go either into the model or not, then you may want to have a look at random forests. They try to cope with the small sample size problem by building a large number of models from slightly varying subsets of the data (subsetting both cases and variates). In addition, they can tell you how many decision trees use which variate, which could help your variable selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you think your variates may be correlated, methods like PCA-LDA or PLS-LDA may be more appropriate. If you chain them correctly, you can even derive coefficients that tell you how much of the original variates goes into what LD function. (You can ask me for R code, if that helps). &#10;I'd go for LDA instead of logistic regression here, as LR tends to need more training cases. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-08T13:06:43.337" Id="63645" LastActivityDate="2013-07-08T13:06:43.337" OwnerUserId="4598" ParentId="63565" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="63845" AnswerCount="2" Body="&lt;p&gt;Two players are throwing each one die. The one that has higher value, receives a number of points equal to the difference of the values on both dice. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do you estimate probability for a winner to collect more than 100 points in 100 throws?&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2013-07-08T19:14:40.390" FavoriteCount="1" Id="63684" LastActivityDate="2013-07-11T15:48:31.947" LastEditDate="2013-07-08T19:17:30.450" LastEditorUserId="27772" OwnerUserId="27772" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;self-study&gt;&lt;dice&gt;" Title="Two dice problem" ViewCount="184" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using simple linear regression to predict the number of days of school a single student might miss after 250 days using their cumulative daily attendance data over the first 50 days of the student's schooling--i.e., predicting how many days of school they will have missed at 250 days based on their individual cumulative 50 day trajectory. This works fairly well for about 75% of students.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a next step, I want to refine Y = a + bX to test whether I can more accurately predict the number of days a student might miss after 250 days (based on their first 50 days), taking into account their gender, ethnicity, etc., using multiple regression.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd understand how to do this if I were using regression to predict an outcome based on a group of single co-ordinates, i.e. predicting a test result based on the number of days missed for 100 males / females. But, I'm not sure how to generate a regression model that takes into account each 50 day matrix of co-ordinates for all 100 student simultaneously. I then want to take the general effects of gender, ethnicity on the slope of this model (i.e., Y = a + b1X1 +b2X2...) and plug it back into the individual student's regression line to test if I can more accurately predict their 250 days outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone offer me some starting guidance on conducting regression in this situation please? Or indeed even if regression is the appropriate choice? I'm using Oracle Analytics for student linear regression and was planning to use SPSS for multiple regression.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-08T20:57:05.350" Id="63691" LastActivityDate="2013-07-08T21:24:05.717" LastEditDate="2013-07-08T21:24:05.717" LastEditorUserId="7290" OwnerUserId="27768" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;spss&gt;&lt;multiple-regression&gt;&lt;forecasting&gt;&lt;matrix&gt;" Title="How to fit a multiple regression model that takes into account 50 days of each individual's history" ViewCount="108" />
  
  <row AcceptedAnswerId="63709" AnswerCount="1" Body="&lt;p&gt;From Wikipedia's Multiple Comparison&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For hypothesis testing, the problem of multiple comparisons (also known as the multiple testing problem) results from the increase in type I error that occurs when statistical tests are used repeatedly. If &lt;strong&gt;n independent comparisons&lt;/strong&gt; are performed, the experiment-wide significance level $\bar{\alpha}$, also termed FWER for familywise error rate, is given by&#10;  $$&#10;    \bar{\alpha} = 1-\left( 1-\alpha_\mathrm{\{per\ comparison\}} \right)^n$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't understand how the comparisons can be independent? Let the multiple tests be $\{H_i, K_i, T_i, c_i), i \in I\}$, where the $i$-th test is $H_i$ versus $K_i$, with testing statistic $T_i$ and critical value $c_i$. Now given a sample $X$, the test statistics $T_i(X), i\in I$ can't be independent, and therefore the testing rules $I_{T_i(X) \geq c_i}$ can't be independent either. Am I wrong? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-09T00:56:23.363" Id="63707" LastActivityDate="2013-07-09T01:04:11.733" LastEditDate="2013-07-09T01:03:22.497" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="0" Tags="&lt;multiple-comparisons&gt;" Title="What does independence between comparisons in multiple comparisons mean?" ViewCount="44" />
  
  
  
  <row Body="&lt;p&gt;Multicolinearity is all about the &lt;em&gt;linear&lt;/em&gt; relationship among you independent/explanatory/right-hand-side/x-variables. That you want to use those variables in a non-linear model does not matter. The logic behind that is that if you want to add both variables to your model then you have te be able to distinguish between a unit change in one variable and a unit change in the other. If the variables are linearly related then a unit change in one coincides with $k$ units increase in the other variables, where $k$ is some constant, so we cannot determine the separate effects of both variables. If the relationship is non-linear a unit change in one variable coincides with a variable number of units change in the other, so we are able to distinguish between the variables. So if you graphically determined that there is a relationship but that relationship is non-linear then that fact alone has already solved most of your problems. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following example: if we add a quadratic curve, that is, we add a variable $x$ and a variable $x^2$ to our model, then the relationship between the variables $x$ and $x^2$ is extremely strong. Still we can estimate that model. The reason is that that relationship is non-linear. &lt;/p&gt;&#10;&#10;&lt;p&gt;I find it informative to see a situation where this can break. Consider we have a study where we want to consider year of birth, which ranges between 1950 and 1990. If we just add that and its square then you might get into trouble as the relationship between birthyear and birthyear$^2$ is almost linear, as you can see below. You can solve this by centering at a meaningful variable within the range of your data, e.g. 1960. As you can see in the second graph the relationship is now non-linear and that is usually enough to solve the problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Z0swx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I created that graph with Stata using the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;twoway function xsquare = x^2, range(1950 1990) ///&#10;    name(a,replace) title(uncentered) ytitle(&quot;x{sup:2}&quot;)&#10;twoway function xsquare = (x-1960)^2, range(1950 1990) ///&#10;    name(b, replace) title(centered) ytitle(&quot;(x-1960){sup:2}&quot;)&#10;graph combine a b, ysize(3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-07-09T07:44:31.563" Id="63731" LastActivityDate="2013-07-09T07:58:16.510" LastEditDate="2013-07-09T07:58:16.510" LastEditorUserId="23853" OwnerUserId="23853" ParentId="63730" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="64542" AnswerCount="2" Body="&lt;p&gt;I have a simple statistical question (forgive me if I use statistical terminology in a wrong way)&#10;Suppose I have a random vector with two components, $(x_1, x_2)$, where $x_1$ can take values from $1$ to $n$ and $x_2$ can take values from $1$ to $m$ &#10;and I have a two-dimensional distribution $P$ as a $n \times m$ matrix $(i, j)$-th element of $P$ is the probability to observe $(i, j)$ sequence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I have observations $x^{(k)}_1, x^{(k)}_2$ and can estimate $\widehat{P}$ from the observations (as statistical frequencies)). Now an interesting part. I have a (fixed) vector $v$ and I need to estimate, how well &lt;strong&gt;the matrix-by-vector product&lt;/strong&gt; of $\widehat{P}$ by $v$ is estimated:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E || (P - \widehat{P})v ||^2 = ?$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-09T07:56:47.113" Id="63732" LastActivityDate="2013-07-18T06:09:02.130" LastEditDate="2013-07-16T12:43:23.087" LastEditorUserId="22047" OwnerUserId="27796" PostTypeId="1" Score="2" Tags="&lt;estimation&gt;&lt;sampling&gt;&lt;standard-deviation&gt;" Title="Two-dimensional distribution and related variance computation" ViewCount="117" />
  <row Body="&lt;p&gt;If finding the cumulative distribution function does not impress you in your quest for useful statistics, remember that very fundamental quantities like the mean $X$ are obtained as $\int^{+\infty}_{-\infty} x f_X(x) dx$ and the standard deviation as $\sqrt{\int^{+\infty}_{-\infty} x^2 f_X(x) dx - \left ( \int^{+\infty}_{-\infty} x f_X(x) dx\right )^2}$. Skewness and kurtosis also require similar integrations. All these tell you what their universal, i.e., &lt;em&gt;population&lt;/em&gt; values are, for an infinite sample set. If you are not equipped with skills for integrals, you will need to stick calculating these quantities from experimental data (finite sized samples), for which you only need to do summations instead of integrations. However, these have inevitable and inherent uncertainty on their calculated values (random sampling error). So, being able to calculate such benchmark values is very useful indeed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-09T08:12:47.823" Id="63734" LastActivityDate="2013-07-09T08:12:47.823" OwnerUserId="27330" ParentId="63678" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;You certainly need to consider time, but longitudinal analysis is broader than just time series - there are multi-level models and generalized estimating equations and so on. But, given your problem, I think the best approach might be survival analysis with multiple events. This is a pretty complex area, but there are some good books e.g. &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387987843&quot; rel=&quot;nofollow&quot;&gt;Therneau &amp;amp; Grambsch&lt;/a&gt;. But if you haven't studied survival analysis, that book will be diving in the very deep end indeed. You could start with this little book by &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0803920555&quot; rel=&quot;nofollow&quot;&gt;Allison&lt;/a&gt; as a very introductory level start. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-09T11:13:00.367" Id="63756" LastActivityDate="2013-07-09T11:13:00.367" OwnerUserId="686" ParentId="63727" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Try using the following command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;log.glm = glm(y ~ x, family=gaussian(link=&quot;log&quot;), data=my.dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It works here and the AIC seems to be correct.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-09T12:08:45.857" Id="63761" LastActivityDate="2013-07-09T12:34:01.073" LastEditDate="2013-07-09T12:34:01.073" LastEditorUserId="21054" OwnerUserId="27803" ParentId="21447" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The short answer is: An MCMC is a MC, but not all MCs are MCMC. &lt;/p&gt;&#10;&#10;&lt;p&gt;The slightly longer answer: MC methods are a class of methods, of which MCMC is one possibility. Even MCMC does not uniquely define your method as there are different variations of MCMC. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can read more in: Robert, C. P., &amp;amp; Casella, G. (2004). Monte Carlo statistical methods. New York: Springer.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-09T13:04:22.517" Id="63768" LastActivityDate="2013-07-09T13:04:22.517" OwnerUserId="23853" ParentId="63767" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I believe the bias in RFs is &quot;a feature, not a bug.&quot; Decision trees have a tendency to overfit the data, so the bias in random forests counter-acts the overfit somewhat, making a random forest model more robust than a straight-forward decision tree otherwise would have been. If you want to further attack overfit, you may be interested in using regularized trees for feature selection: &lt;a href=&quot;http://arxiv.org/pdf/1201.1587.pdf&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/pdf/1201.1587.pdf&lt;/a&gt;, also check out the &lt;a href=&quot;http://cran.r-project.org/web/packages/RRF/RRF.pdf&quot; rel=&quot;nofollow&quot;&gt;RRF&lt;/a&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, this doesn't answer your question which was what options are available for tuning your model. For one automated approach, the &lt;code&gt;randomForest&lt;/code&gt; package has a &lt;code&gt;tuneRF()&lt;/code&gt; function for tuning the &lt;code&gt;mtry&lt;/code&gt; paramter (the number of variables sampled in each tree). &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-09T14:29:19.797" Id="63783" LastActivityDate="2013-07-09T14:34:31.510" LastEditDate="2013-07-09T14:34:31.510" LastEditorUserId="8451" OwnerUserId="8451" ParentId="63776" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose that one investigates the two sources of variability with data $y_{ij}$ acquired from $j$th subject under $i$th session ($i=1,2; j=1,2..., n$). A linear mixed-effects model can be formulated as follows,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_{ij} = \alpha_0 + b_i + c_j + \epsilon_{ij}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\alpha_0$ is a constant, $b_i$ and $c_j$ are the random effects for $i$th session and $j$th subject respectively, and $\epsilon_{ij}$ is a residual term. With assumptions of $b_i ~\sim N(0, \tau_1^2)$, $c_j ~\sim N(0, \tau_2^2)$, and $\epsilon_{ij} ~\sim N(0, \sigma^2)$, the intraclass correlation (ICC) values for sessions and subjects can be defined respectively as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ICC_{session} = \frac{\tau_1^2}{\tau_1^2+\tau_2^2+\sigma^2}, ICC_{subject} = \frac{\tau_2^2}{\tau_1^2+\tau_2^2+\sigma^2}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And the above ICC values can be obtained through &lt;code&gt;lmer&lt;/code&gt; in R package &lt;code&gt;lme4&lt;/code&gt;. I have two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is there a way to test the significance of the above ICC values in the context of LME model, similar to the $F$-stat for ICC defined under random-effects ANOVA?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Intuitively the bigger the variability between the two sessions (more different between the two sessions), the higher the session ICC. But how is this intuition consistent with the notion that ICC measures reliability, reproducibility, or consistency? Is the session reliability (or reproducibility, consistency) about any two subjects' responses coming from the same session, not about the difference between the two sessions?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-07-09T16:45:01.927" FavoriteCount="1" Id="63802" LastActivityDate="2014-02-25T03:40:00.253" LastEditDate="2013-12-18T16:55:59.107" LastEditorUserId="1934" OwnerUserId="1513" PostTypeId="1" Score="7" Tags="&lt;lme&gt;&lt;intraclass-correlation&gt;" Title="Intraclass correlation in the context of linear mixed-effects model" ViewCount="374" />
  <row Body="&lt;p&gt;It might help to be a little more specific about which FDR you mean, as there are several. One typical False Discovery Rate procedure, due to &lt;a href=&quot;http://www.jstor.org/stable/2346101&quot; rel=&quot;nofollow&quot;&gt;Benjamini and Hochberg&lt;/a&gt;, helps you find an FDR threshold without directly computing $q$ values. it works like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Choose an overall $\alpha$ value for the false discovery/false positive rate.&lt;/li&gt;&#10;&lt;li&gt;Perform all $m$ individual hypothesis tests ($H_1 \ldots H_m)$, whatever they may be, to produce a list of $p$-values.&lt;/li&gt;&#10;&lt;li&gt;Sort the $p$-values, so that $P_{(1)}$ is the smallest and $P_{(m)}$ is the largest. Rearrange the $H$s so the indices match.&lt;/li&gt;&#10;&lt;li&gt;Step up from $k=0$ to $k=m$ through your data to find the largest $k$ such that $P_{(k)} \le \frac{k}{m \cdot c(m)}\alpha$. The $c(m)$ is a correction factor, due to &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1013699998&quot; rel=&quot;nofollow&quot;&gt;Benjamini and Yekutieli&lt;/a&gt;, that adjusts for various kinds of dependence/correlation between the tests.&lt;/li&gt;&#10;&lt;li&gt;The null hypotheses $H_1 \ldots H_k$ (if any) can be rejected; you fail to reject the remaining null hypothesis (if any).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Note that you don't actually get a $q$ value anywhere in this procedure; it just identifies hypotheses, based on their $p$-values, that you can reject. If you had slightly different data, you might have rejected a null based on a slightly larger $p$ value too.  However, you can compute an mean $\alpha$ for your all $m$ tests (for positive dependence, where $c(m)=1$, it's $\bar{\alpha}=\frac{m+1}{2m} \alpha$). Some people also abuse notation slightly and write something like $q&amp;lt;0.05$ (FDR corrected), to distinguish it from uncorrected tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there are other procedures for controlling the FDR. &lt;a href=&quot;http://www.genomine.org/papers/directfdr.pdf&quot; rel=&quot;nofollow&quot;&gt;Storey's approach&lt;/a&gt; calculates $q$-values directly. These are intended to be like pFDR adjusted $p$-values, so you would reject null hypotheses associated with $q$-values below some threshold. In that case, you could easily write $q&amp;lt;0.05$. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-09T17:56:33.827" Id="63811" LastActivityDate="2013-07-09T21:55:49.190" LastEditDate="2013-07-09T21:55:49.190" LastEditorUserId="7250" OwnerUserId="7250" ParentId="63801" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;When dealing with data in fields such as Natural Language Processing(NLP) or Speech Recognition(ASR) and trying to model the data using Hidden Markov Model(HMM) one should first make it clear that if the available data is observable or unobservable. &lt;/p&gt;&#10;&#10;&lt;p&gt;When applying Markov property to the data, that is the current data probability at time T is only dependent on the data at time T-1 (or more). If all the variables in probability function are observed in the training data, it is called observed data, thus could be trained simply using Maximum Likelihood criterion. &lt;/p&gt;&#10;&#10;&lt;p&gt;In cases, that there are variables in the probability function, which are not observed in the training data it is called a &quot;hidden variable&quot; and so using HMM and Expectaion-Maximization(EM) training criterion, by just defining the number of hidden states, it can be trained.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to address a special case, in which there are many hidden states (e.g. 100 000 hidden states) which makes it impossible for EM to train the model. By some assumption on the model, by observing each training instance and using the assumptions, the model can restrict the model to a limited number of states among all those hidden states, e.g. every instance is limited to 10-20 states among 100 000 hidden states.&lt;/p&gt;&#10;&#10;&lt;p&gt;This special case has been worked in some problems such as &lt;a href=&quot;http://mi.eng.cam.ac.uk/~sjy/papers/heyo03a.pdf&quot; rel=&quot;nofollow&quot;&gt;Hidden Vector States&lt;/a&gt; proposed by Yulan He.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this called &quot;semi-observable&quot; case or it's just a hidden variable with some simple assumptions made it possible to be trained? How is it converted to a standard HMM problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: In case there is some ambiguities about the terms 'hidden' or 'observable', please read &lt;a href=&quot;http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; tutorial on Hidden Markov Model by Rabiner describing all these terms a great manner. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT2: &lt;a href=&quot;http://campar.in.tum.de/twiki/pub/Main/MoritzBlume/EMGaussianMix.pdf&quot; rel=&quot;nofollow&quot;&gt;This&lt;/a&gt; cool tutorial on Expectation-Maximization has a good and brief explanation on 'hidden variables' too.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-09T19:26:14.057" Id="63828" LastActivityDate="2013-07-11T18:23:06.030" LastEditDate="2013-07-11T18:23:06.030" LastEditorUserId="13594" OwnerUserId="13594" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;hidden-markov-model&gt;&lt;expectation-maximization&gt;" Title="Beyond observable and unobservable data, is there any term &quot;semi-observable&quot; defined?" ViewCount="63" />
  
  
  <row Body="&lt;p&gt;I will not write this out in detail, but give the main ideas. It becomes easier if you use the multivariate normal distribution.  Write $X=(X_1, \dots, X_n)^T$ the data in vector form, and $\bar{X}$ for the arithmetic mean.  Then the $(n-1)$-vector $(X^T, \bar{X})^T$ (in block form) has a multivariate normal distribution of dimension $n+1$, but with a singular covariance matrix (of rank $n$).  But the (matrix-variate) moment generating function is also defined in the singular case, so you can use that result with some matrix algebra, and the result will fall out!&lt;/p&gt;&#10;&#10;&lt;p&gt;A key point in the calculations will be to write&#10;$$&#10;   (X_1-\bar{X}, \dots, X_n-\bar{X}) = A \begin{pmatrix} X \\ \bar{X} \end{pmatrix}&#10;$$&#10;with $A$ the $n \times (n+1) $-matrix (in block form)&#10;$$&#10;A=  \begin{pmatrix} I_n &amp;amp; -1_n \end{pmatrix}&#10;$$&#10;where $I_n$ is the identity matrix of size $n$, and $1_n$ is the $n$-vector (column) with all components equal to $1$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-09T21:40:09.700" Id="63840" LastActivityDate="2013-07-09T21:40:09.700" OwnerUserId="11887" ParentId="63836" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am not sure what statistical test to use for comparing clustering on related data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two sets of measurements on each person. I cluster each person by each measurement. I then measure how much the two coincide. I want to know how I would test for how significance the agreement of the clustering is.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example I have 10 people and for each person I have two sets of attributes say vector A and Vector B both 10 of length 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;I cluster the 10 people into 2 clusters using vector A and get something like&lt;/p&gt;&#10;&#10;&lt;p&gt;1 2 1 1 2 1 1 1 2 2&lt;/p&gt;&#10;&#10;&lt;p&gt;That is person one is assigned to cluster one, person two to cluster two person three to cluster one etc&lt;/p&gt;&#10;&#10;&lt;p&gt;I cluster the 10 examples into 2 clusters using vector B and get something like&lt;/p&gt;&#10;&#10;&lt;p&gt;1 2 1 1 2 1 1 1 1 1&lt;/p&gt;&#10;&#10;&lt;p&gt;These two clustering agree to 80% but how significant is this? &#10;What is the formula for calculating this?&#10;Is there a variant for more than 2 clusters? &#10;would it make sense if there were a different number of clusters for each measurement or if some samples only had one set of measurements?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any thoughts :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-10T09:21:10.207" Id="63879" LastActivityDate="2013-07-11T07:59:04.500" LastEditDate="2013-07-10T09:36:41.327" LastEditorUserId="6029" OwnerUserId="27815" PostTypeId="1" Score="-1" Tags="&lt;hypothesis-testing&gt;&lt;clustering&gt;" Title="Comparing clusters from related data" ViewCount="36" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;em&gt;Note that the problem specification has changed since the original posting.  Thanks to whuber for helping me better specify the question: in an attempt to make the question general, I had left out several important constraints.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Let $y$ be distributed Poisson with rate parameter $\lambda$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(y | \lambda) = \frac{\lambda^y}{y!}e^{-\lambda}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $z^*$ be distributed Gaussian with mean $\mu$ and variance $1$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(z^* | \mu) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{(z^* - \mu)^2}{2}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Define $z$ as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$z = k$ if $\gamma_{k} \le z^* &amp;lt; \gamma_{k+1} $, where $\gamma_0 = -\infty$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\mu = \log\lambda$.  How do we choose a single sequence of $\gamma_k$ for $k = 1, 2, \ldots, \infty$, that does not depend on $\lambda$, in order to best approximate the distribution of $y$ with the distribution of $z$, across the full range of possible values of $\lambda \in (0,\infty)$.  It is not clear to me exactly what the right criterion is here for operationalizing &quot;best approximating&quot; across the full range of $\lambda$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: The idea here is to do something similar to the Albert and Chib (1993) approach to Gibbs sampling binary and categorical models.  Instead of having a single or small number of cutpoints, here we would have an infinite series of cutpoints.  The question is how to choose the cutpoints to best approximate a Poisson distribution.  As a practical matter, it is pretty easy to draw doubly-truncated normal random variates (Robert 2009).  &lt;/p&gt;&#10;&#10;&lt;p&gt;There is an existing approach to using data augmentation to fit a Poisson regression (Frühwirth-Schnatter and Wagner 2005), but it seems to involve rapidly increasing computation with increasing values of the count variable, which is a very unattractive feature for my application.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am working on a Gibbs sampler for a hierarchical model to be applied to a very large dataset of count data.  I have no particular reason to believe that the distribution of counts are particularly close to Poisson, I am mostly interested in computational efficiency and not having a probability distribution for the counts that is very far from a standard distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.cmu.edu/~brian/905-2009/all-papers/albert-chib-1993.pdf&quot; rel=&quot;nofollow&quot;&gt;James H. Albert and Siddhartha Chib, &quot;Bayesian Analysis of Binary and Polychotomous Response Data&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;sqi=2&amp;amp;ved=0CC8QFjAA&amp;amp;url=http%3A%2F%2Fwww.jku.at%2Fifas%2Fcontent%2Fe108280%2Fe146255%2Fe146306%2Fifasrp04_2004.pdf&amp;amp;ei=Dk3dUZG9GZKo0wWX4IC4AQ&amp;amp;usg=AFQjCNGG5lMcynWyra-c4ENWG9wdiTH7Lw&amp;amp;sig2=klSBWzHdAfrlygkPOIUsrA&amp;amp;bvm=bv.48705608,d.d2k&quot; rel=&quot;nofollow&quot;&gt;Sylvia Frühwirth-Schnatter and Helga Wagner, &quot;Data Augmentation and Gibbs Sampling for Regression Models of Small Counts&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/0907.4010.pdf&quot; rel=&quot;nofollow&quot;&gt;Christian Robert, &quot;Simulation of truncated normal variables&quot;&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-07-10T10:52:35.717" Id="63890" LastActivityDate="2013-07-10T15:50:08.690" LastEditDate="2013-07-10T15:50:08.690" LastEditorUserId="4881" OwnerUserId="4881" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;bayesian&gt;&lt;mcmc&gt;&lt;distance-functions&gt;" Title="Approximating a Poisson distribution using a partially observed Gaussian" ViewCount="98" />
  
  <row Body="&lt;p&gt;You only need to use maximal cliques. That is, if you have two cliques {a,b,c} and {a,b}, you only use {a,b,c}. Other than that, yes, you might generate unnecessary cliques, but how many depends also on the triangulation strategy (minimum fill-in is an NP-hard problem, so you won't really be able to do it perfectly in all cases).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-10T12:02:13.360" Id="63892" LastActivityDate="2013-07-10T12:02:13.360" OwnerUserId="27854" ParentId="46032" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working on a problem where we elicit medical residents perceptions regarding the importance of specific core medical competencies (professionalism, collaboration, medical expertise, etc.) on a 5 point Likert scale. For example, &quot;In your current residency role how important is professionalism in your day-to-day activities&quot;. We elicited responses from two schools (School1, School2) and from residents in 5 types of post-graduate residency programs (Family Medicine, Psych, Surgery, Internal, Other). We have data on approximately 800 respondents. Our hypothesis is that the culture surrounding the specific schools in the sample, and the specific residency programs under question may influence the self-reported importance of these roles.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously, the response data live on 5 points. And in our case, the responses are fairly skewed to the right, with many residents finding these core roles important. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have graphically investigated the relationship between Perception (5 point Likert Scale) and my two independent variables (School, Residency Program). I have used mean plots, also considered mosaic plots. I have a sense of what is going on qualitatively but want estimated covariate effects, p-values, etc...corresponding to the impact of School, Residency and their interaction on self-reported importance. &lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, I could run this as a 2-way ANOVA (or linear regression model). However, diagnostic investigation of residuals suggests an issue of non-normality. &lt;/p&gt;&#10;&#10;&lt;p&gt;I considered the proportional odds logistic regression model next; however, in many instances the assumption of &quot;proportional odds&quot; was not satisfied. &lt;/p&gt;&#10;&#10;&lt;p&gt;From there I moved on to the multinomial logistic regression model; however, given the distribution of the response data (skewed to the right), very little information exists in cells {1}, {2}, so estimated covariate effects (relative to these response levels) appear to be estimated without a great deal of precision. &lt;/p&gt;&#10;&#10;&lt;p&gt;Next, I considered non-parametric tests...extensions of Kruskal Wallis test from one-way layouts to two-way (and higher order) layouts. I found the following:&#10;&lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1207/s15327906mbr1503_4#.Ud1uCdpzY3E&quot; rel=&quot;nofollow&quot;&gt;http://www.tandfonline.com/doi/abs/10.1207/s15327906mbr1503_4#.Ud1uCdpzY3E&lt;/a&gt;&#10;However, I could not find a statistical package to implement such a method. Does anyone know of one? I wouldn't expect these tests to elucidate the size of the covariate effect; however, they may at least be able to derive a valid overall p-value (something like the global F-test in regression/ANOVA)...or perhaps even more valuable LRT style p-values corresponding to each covariate effect (i.e. School, Program, School*Program). &lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine this is a fairly common problem in applied research settings...just wonder what other people have done in the past?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&#10;Chris&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-10T14:44:38.987" Id="63906" LastActivityDate="2013-07-10T17:02:29.653" LastEditDate="2013-07-10T17:02:29.653" LastEditorUserId="6029" OwnerUserId="27855" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;multiple-regression&gt;&lt;generalized-linear-model&gt;&lt;likert&gt;" Title="Estimating covariate effects on Likert-type response variable" ViewCount="113" />
  <row Body="&lt;p&gt;I am also puzzled by the rest of the question but to comment on the multiple comparisons issue: Bonferroni adjustment works the way you want it to work. What I mean by that is that it is based on very general results regarding probabilities and can be applied to many different situations. It provides strong control of the family-wise error rate, which means that the probability of even one false rejection in a family of tests is at most $\alpha$ for every combination of true and false null hypotheses.&lt;/p&gt;&#10;&#10;&lt;p&gt;Defining what a “family” of related tests is is up to you (and the reviewers, colleagues, clients, etc. you want to convince). If your criteria to choose which tests to run are based on the same data and are somehow related to the relevant effect size, there could be some concerns about “data dredging”, “double dipping” or “capitalizing on chance”. It might then make sense to adjust the error level for all potential comparisons and not only for the tests you end up performing. This is something you have to judge for yourself, there is no “technically correct” solution and nothing about the Bonferroni correction that prevents it to be used one way or the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that there are many other simultaneous inference techniques, some similar to Bonferroni but more powerful (in particular Holm's method) and some controlling a different but possibly more relevant error rate. If you reveal more about your data and your objectives, others might also be able to suggest approaches that sidestep the issue entirely.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-10T16:29:51.340" Id="63914" LastActivityDate="2013-07-10T17:37:31.393" LastEditDate="2013-07-10T17:37:31.393" LastEditorUserId="6029" OwnerUserId="6029" ParentId="63902" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;$R^{2}$ will always increase as you add more variables to the model.  It seems what you are asking is what variables to include in the model.  This is called variable selection.  You can accomplish this in &lt;code&gt;R&lt;/code&gt; by the &lt;code&gt;step&lt;/code&gt; function.  This function bases the best model on a statistic called AIC.  Make sure you use the full model, the one with the most variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;step(yourfullmodel)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the final output will give you your the best subset model under Call:&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-10T21:54:00.410" Id="63947" LastActivityDate="2013-07-10T21:57:31.537" LastEditDate="2013-07-10T21:57:31.537" LastEditorUserId="21054" OwnerUserId="27875" ParentId="63944" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;On his blog, Larry Wasserman has a &lt;a href=&quot;http://normaldeviate.wordpress.com/2012/08/13/what-to-teach/&quot;&gt;post&lt;/a&gt; about what he planned to cover in his course last fall. He notes that he was abandoning some classical topics in favor of more modern issues. One topic that he mentions is Hoeffding’s inequality. What makes this result especially important for students and practitioners?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-10T23:03:10.720" FavoriteCount="1" Id="63957" LastActivityDate="2013-08-10T08:56:53.517" OwnerUserId="401" PostTypeId="1" Score="8" Tags="&lt;probability-inequalities&gt;" Title="What makes Hoeffding's inequality an important statistical concept?" ViewCount="121" />
  
  <row Body="&lt;p&gt;The quote is a paraphrase of a &lt;a href=&quot;http://en.wikiquote.org/wiki/Donald_Knuth&quot;&gt;Donald Knuth quote&lt;/a&gt;, one which he has himself attributed to Hoare. Three extracts from the above page:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;Premature optimization is the root of all evil (or at least most of it) in programming.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;$ $&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;Premature optimization is the root of all evil.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;$ $&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Knuth refers to this as &quot;Hoare's Dictum&quot; 15 years later ... &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't know that I agree with the statistics paraphrase*.  There's &lt;em&gt;plenty&lt;/em&gt; of 'evil' in statistics that doesn't relate to optimization. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Should statisticians &amp;amp; ML practitioners always be wary of over-optimizing their models even when adhering to strict cross validation protocols (e.g. 100 nested 10-fold CV) ? If so, how do we know when to stop searching for &quot;the best&quot; model?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I think the critical thing is to fully understand (or as fully as is feasible) the properties of what procedures you undertake. &lt;/p&gt;&#10;&#10;&lt;p&gt;$\,^\text{* I won't presume to comment on Knuth's use of it, since there's little I could}$ $\quad ^\text{say that he couldn't rightly claim to understand ten times as well as I do.}$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-11T01:09:16.140" Id="63969" LastActivityDate="2013-07-11T01:21:21.927" LastEditDate="2013-07-11T01:21:21.927" LastEditorUserId="805" OwnerUserId="805" ParentId="63968" PostTypeId="2" Score="7" />
  <row AnswerCount="0" Body="&lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;hypotheses suggested by the data, if tested using the data set that&#10;  suggested them, are likely to be accepted even when they are not true.&#10;  This is because circular reasoning would be involved: something seems&#10;  true in the limited data set, therefore we hypothesize that it is true&#10;  in general, therefore we (wrongly) test it on the same limited data&#10;  set, which seems to confirm that it is true. Generating hypotheses&#10;  based on data already observed, in the absence of testing them on new&#10;  data, is referred to as post hoc theorizing.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The correct procedure is to test any hypothesis on a data set that was&#10;  not used to generate the hypothesis.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;For Post Hoc analysis of ANOVA,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Henry Scheffé's simultaneous test&lt;/strong&gt; of all contrasts in multiple comparison problems is the most[citation needed] well-known remedy in the case of analysis of variance.&lt;a href=&quot;https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; It is a method designed for testing hypotheses suggested by the data while avoiding the fallacy described above.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So I was wondering how &lt;a href=&quot;https://en.wikipedia.org/wiki/Scheff%C3%A9_test&quot; rel=&quot;nofollow&quot;&gt;Scheffé's test&lt;/a&gt; avoids the fallacy of data snooping?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-11T01:34:10.900" Id="63972" LastActivityDate="2013-07-11T01:34:10.900" OwnerUserId="1005" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;multiple-comparisons&gt;" Title="How does Scheffe test deal with data snooping?" ViewCount="101" />
  
  <row Body="&lt;p&gt;Three things:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The adjusted $R^2$ value of your 2nd model is larger than your first. This means it explains more of the variation in the data than the first one. Unlike the standard $R^2$ value it is not impacted by the number of variables added to the model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;To more rigorously compare them you can run an ANOVA test like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova(regression1.model, regression2.model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Look for the p-value. The closer to zero the better, though common practice is it should be at least less than .05.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Make sure it makes sense. If you created regression 2 because you thought it might be a better fit and the above other points are met, you've found yourself an improved model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-07-11T01:51:04.800" Id="63975" LastActivityDate="2013-07-11T01:56:24.400" LastEditDate="2013-07-11T01:56:24.400" LastEditorUserId="7290" OwnerUserId="9426" ParentId="63944" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;There is no hard and fast rule for that, Its depend on your data. Please specify your data for more detail and you may try other methods to check the normality of data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T06:17:56.303" Id="63988" LastActivityDate="2013-07-11T06:17:56.303" OwnerUserId="27576" ParentId="63986" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to design an algorithm to determine which softwares are installed on a computer.&#10;I have several millions of files each corresponding to the list of softwares which are on a computer.&#10;I've built several regexps which allow me to recognize few key softwares. I've computed joint probabilities in order to know better which software are almost always installed on a same computer and conditional probabilities.&#10;With these I'd like to be able to determine which softwares are installed on a computer having just to detect few key softwares.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I do ? What rule can I derive from joint et conditional probabilities ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-11T08:21:33.913" Id="63997" LastActivityDate="2013-07-11T08:21:33.913" OwnerUserId="27895" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;algorithms&gt;&lt;computational-statistics&gt;&lt;pattern-recognition&gt;" Title="How to built a software recognition algorithm" ViewCount="30" />
  <row Body="&lt;p&gt;I realize you do not want to hear this, but hunting around for significant effects is unlikely to help you. In all likelihood your dataset just does not contain enough information to find those interaction effects. No statistical method can &quot;invent&quot; information that is not present in your data, and that is a good (but sometimes frustrating) thing.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T08:44:47.047" Id="64001" LastActivityDate="2013-07-11T08:44:47.047" OwnerUserId="23853" ParentId="63998" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It depends on the multicollinearity of the data if your significant model experiences vast changes in the significance of its parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The random tinkering pretty much depends not only on the new values, but also on the changes.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: Does the empirical distribution of your residuals of your regression  change a lot? If so, your model is probably not specified correctly.&lt;br&gt;&#10;Or, how do the significance values for the model (for example F-Test) and parameters of your regression change? Is one stable and the other one not? That might be a problem in multicollinerity.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general model specification is not exact, and including/excluding variables depends on the case. There are also issues related to successively adding and removing arguments to the regression.&lt;br&gt;&#10;In general, however, I'd say that when it comes to picking your regression methodology, so the kind of model you want to use, you should definitly have a reason for doing so.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T08:47:17.980" Id="64002" LastActivityDate="2013-07-11T08:47:17.980" OwnerUserId="18459" ParentId="63998" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;There are several problems here. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Each row should correspond to one case/individual; each column to one variable.&lt;br&gt;&#10;If I understand your description correctly, that means you need to transpose your data.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;This also means that you have more variates than individuals, thus the variance-covariance matrix is not of full rank which leads to problems during its inversion inside &lt;code&gt;lda&lt;/code&gt;.&lt;br&gt;&#10;You need to drastically reduce the number of variates or increase the number of individuals before performing LDA (if I correctly understood your description of the data).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;MASS::lda&lt;/code&gt; expects grouping to be a factor with one value per case (= row), not a matrix.&lt;br&gt;&#10;That's why it is complaining that &lt;code&gt;length (grouping)&lt;/code&gt; is not the same as &lt;code&gt;nrow (x)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It does not make any sense to give the same data for &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;grouping&lt;/code&gt;: &lt;code&gt;x&lt;/code&gt; should be the matrix with the independent variates, &lt;code&gt;grouping&lt;/code&gt; is the dependent.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It is very unusual to give &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;grouping&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code&gt;data&lt;/code&gt;.&lt;br&gt;&#10;Either give &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;formula&lt;/code&gt;: with that you call the formula interface (&lt;code&gt;lda.formula&lt;/code&gt;).&lt;br&gt;&#10;&lt;em&gt;Or&lt;/em&gt; give &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;grouping&lt;/code&gt;: that calls &lt;code&gt;lda.default&lt;/code&gt; (a bit faster than the first option). &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;edit: &lt;/p&gt;&#10;&#10;&lt;p&gt;The formula version &lt;code&gt;lda (grouping ~ x)&lt;/code&gt; is equivalent to &lt;code&gt;lda (x = x, grouping = grouping)&lt;/code&gt;. If you have a data.frame &lt;code&gt;data&lt;/code&gt; with columns &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;grouping&lt;/code&gt;, then you'd use &lt;code&gt;lda (grouping ~ x, data = data)&lt;/code&gt;. Note that a column of a data.frame can hold a whole matrix.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-11T11:13:43.967" Id="64019" LastActivityDate="2013-07-11T11:42:20.940" LastEditDate="2013-07-11T11:42:20.940" LastEditorUserId="4598" OwnerUserId="4598" ParentId="63959" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The regularized regression problem is most often applied when the matrix inversion is ill-conditioned, i.e. the matrix is nearly singular. So this is the property you need to concern yourself with when considering regularization. This comes into play very often in the case of colinear regressors. That said, both overdetermined and underdetermined systems can be ill-conditioned so the fact that your matrix is overdetermined does not necessarily preclude the need for regularization. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now assuming the matrix is invertible, overdetermined systems do indeed have an OLS estimator but it is approximate. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Overdetermined_system#Approximate_solutions&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. The $QR$ factorization improves the situation; see &lt;a href=&quot;http://glowingpython.blogspot.ca/2012/03/solving-overdetermined-systems-with-qr.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T11:37:27.837" Id="64021" LastActivityDate="2013-07-11T11:37:27.837" OwnerUserId="26357" ParentId="63993" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have an &lt;code&gt;ARIMA(0,2,1)&lt;/code&gt; model. How do i estimate the $\hat{e}_t$ component of the model. I have read a whole lot of theories that confuses me the more. Is there any practical way of estimating this $\hat{e}_t$? Does &lt;code&gt;R&lt;/code&gt; offer any help to that too?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that my &lt;code&gt;ARIMA(0,2,1)&lt;/code&gt; model can be written as $Y_{t} = 2Y_{t-1} - Y_{t-2} + e_{t} + \theta e_{t-1}$. I want to forecast 1-time ahead into the future. In that case, my forecast equation is given as $\hat Y_{t}(1) = 2Y_{t} -Y_{t-1} + \theta \hat e_{t}$. I know my value for $ Y_{t} =7.8$ and $ Y_{t-1} =7.8 $. I know my value of \theta as -0.6816, which i obtained from my &lt;code&gt;R&lt;/code&gt; output. My problem now is, how do i determine the value for my $\hat e_{t} $ so i could find $ \hat Y_{t}(1)$? I have an &lt;code&gt;R&lt;/code&gt; code that gives me all these forecasts though, but i want to know how &lt;code&gt;R&lt;/code&gt; generated my first forecast and how it found the estimate for $\hat e_{t} $.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for looking!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-11T13:07:50.193" FavoriteCount="1" Id="64033" LastActivityDate="2013-07-16T16:44:23.523" LastEditDate="2013-07-11T14:08:31.047" LastEditorUserId="24238" OwnerUserId="24238" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;arima&gt;" Title="How do I estimate the $e_t$ from a moving average model?" ViewCount="263" />
  <row AcceptedAnswerId="64256" AnswerCount="2" Body="&lt;p&gt;I have a biological problem which involves using a 1-sided two-sample t-test. To simplify the problem, let’s assume an experiment comparing quantities in a control (C) and a treatment (T) group, each having 3 replicates, called C1, C2, C3, T1, T2, T3. Here, &lt;strong&gt;C1 and T1 are samples from the same subject, C2 and T2 are samples from the same subject, and so on.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I obtain a number for each of the following combinations. &quot;within&quot; = within group (i.e. both are C or both are T). Also, the number after C or T indicate whether they're from the same subject (sub) or not. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;C1_C2 (within: different sub)&#10;C1_C3 (within: different sub)&#10;C1_T1 (between: same sub)&#10;C1_T2 (between: different sub)&#10;C1_T3 (between: different sub)&#10;C2_C3 (within: different sub)&#10;C2_T1 (between: different sub)&#10;C2_T2 (between: same sub)&#10;C2_T3 (between: different sub)&#10;… (a total of 15 pairwise quantities)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We are essentially interested in comparing the quantities &quot;within&quot; group and the quantities &quot;between&quot; group, but &lt;strong&gt;hoping to incorporating the &quot;subject&quot; effect somehow in the t-test&lt;/strong&gt;. Is there a way to distinguish “different sub” and “same sub” in the test? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much for your suggestions!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-11T14:53:13.493" Id="64043" LastActivityDate="2013-07-13T21:29:29.127" OwnerUserId="14156" PostTypeId="1" Score="3" Tags="&lt;t-test&gt;" Title="1-sided two sample t-test with covariate adjustment" ViewCount="509" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have another problem with 14000 features and 500 training samples. It is a binary classification problem and approximately in the form of an ellipse. My classification accuracy using the 2nd degree polynomial Kernel and via CV is ~ 80%. However, I've randomly tried projecting the data onto 2-D, that is I just pick out two features and project them, and find that there are several combinations that give me 100% separation. I've also used the RB kernel and the classification accuracy is ~70% for that. Does anyone have a reason why this is happening?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the code I'm using for libsvm...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% train SVM model&#10;model = svmtrain2(Y(train_vec,1),X(train_vec,:),['-t 1 ' '-d 2 ' '-g ' num2str(grid_data(i,1)) ' ' '-r ' num2str(grid_data(i,2)) ' ' '-c ' num2str(C)]);&#10;% test model on test set &#10;[predict_label, accuracy, dec_values] = svmpredict(Y(test_vec,1),X(test_vec,:),model); &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2013-07-11T16:04:58.810" FavoriteCount="2" Id="64053" LastActivityDate="2013-07-11T20:45:00.117" LastEditDate="2013-07-11T17:53:42.207" LastEditorUserId="7290" OwnerUserId="27525" PostTypeId="1" Score="4" Tags="&lt;svm&gt;&lt;underdetermined&gt;" Title="SVM has relatively low classification rate for high-dimensional data even though 2-D projections show they are separable" ViewCount="495" />
  
  <row AcceptedAnswerId="64139" AnswerCount="1" Body="&lt;p&gt;i am working to create a mapping of the adjacencies between 100 consumer goods. I have created an adjacency matrix based on product characteristics, and then create an i-graph of the graph/network (using tkplot / igraph)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using the fruchterman reingold force-directed mapping algo to place the vertices, and get a nice picture of a 8-10 connected cliques. &lt;/p&gt;&#10;&#10;&lt;p&gt;Probably partly due to lack of understanding on how forec-directed mapping algos really work(and maybe should require me to do a lot more basic studying), but does the positioning of different cliques across the &quot;page&quot; by the fruchterman-rheingold algorhytm contain any information. E.g. - does the fact that i have two cliques in the bottom right corner, a small clique in the middle and a massive clique in the top left corner say anything about the characteristics / similarities between these cliques, or is just random spacing across the space to improve visual clarity?&lt;/p&gt;&#10;&#10;&lt;p&gt;Apologies if not clear - W &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IoaO0.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T16:37:22.310" Id="64057" LastActivityDate="2013-07-12T12:46:28.997" LastEditDate="2013-07-11T22:29:46.383" LastEditorUserId="26564" OwnerUserId="26564" PostTypeId="1" Score="7" Tags="&lt;data-visualization&gt;&lt;graph-theory&gt;&lt;networks&gt;&lt;igraph&gt;" Title="Force-directed methods to draw graphs" ViewCount="151" />
  
  <row Body="&lt;p&gt;There are (at least) two aspects to your question:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) What do you mean by &quot;faster&quot;?&#10;2) Once you've established that, how do you test it?&lt;/p&gt;&#10;&#10;&lt;p&gt;The first is one that you will have to answer: What is a &quot;faster&quot; program depends on what the nature of the program is. If it's a sort, for example, how many items? How sorted before? How linked to other things? etc.  For more complex programs, more things can vary. E.g. a program to do regression would vary on a dozen parameters or so. So, you'd have to figure that out.  Then you'd have to test the two programs for a variety of conditions. You might have to test more than once for each condition, if there is much variation over time (I don't know if there is). &lt;/p&gt;&#10;&#10;&lt;p&gt;For 2) it might be possible to do something as simple as a t-test, but probably you will be interested in how the relative speed varies as a function of the parameters. (e.g. program A is faster when there are 10,000,000 items to sort, but B is faster when there are 1,000,000,000 items or something like that).  That might call for some sort of regression model&lt;/p&gt;&#10;&#10;&lt;p&gt;Speed ~ program + parameter1 + parameter2 .....&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T20:28:53.593" Id="64080" LastActivityDate="2013-07-11T20:28:53.593" OwnerUserId="686" ParentId="64075" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here is one common way to avoid underflow/overflow.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $m = \max_i \log(\theta_i)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $ \theta_i' = \exp( \log(\theta_i) - m )$. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can sample from $\theta' = [\theta_1' , \theta_2',...]$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-11T20:45:12.220" Id="64085" LastActivityDate="2013-07-11T20:45:12.220" OwnerUserId="27827" ParentId="64081" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;An alternative approach is the &lt;strong&gt;brglm&lt;/strong&gt; package. For example, using the same data/model as @chl's Answer&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(sex2)&#10;fm &amp;lt;- case ~ age + oc + vic + vicl + vis + dia&#10;fit &amp;lt;- brglm(fm, data = sex2)&#10;&#10;predict(fit, newdata = sex2[1:5, ], type = &quot;response&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That yields the&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; predict(fit, newdata = sex2[1:5, ], type = &quot;response&quot;)&#10;        1         2         3         4         5 &#10;0.3389307 0.9159945 0.9159945 0.9159945 0.9159945&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that in the &lt;code&gt;brglm()&lt;/code&gt; case, because of the way the function works, what you see above is simply the result of the standard &lt;code&gt;predict.glm()&lt;/code&gt; function/method in R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-11T21:43:46.473" Id="64089" LastActivityDate="2013-07-11T21:43:46.473" OwnerUserId="1390" ParentId="27297" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The overall $F$ test contains a perfect multiplicity adjustment, though doesn't achieve optimum power in every direction.  Proceeding to compare means after an insignificant $F$ is somewhat dangerous in my view.  One way to proceed, though, is to adjust the confidence intervals for multiplicity.  The R &lt;code&gt;rms&lt;/code&gt; functions &lt;code&gt;ols&lt;/code&gt; and &lt;code&gt;contrast.rms&lt;/code&gt; make this easy to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the distribution of alcohol intake?  Is it measured as a continuous variable?  If instead it is ordinal you might use a proportional odds model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-11T22:03:42.977" Id="64093" LastActivityDate="2013-07-11T22:03:42.977" OwnerUserId="4253" ParentId="64049" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="65161" AnswerCount="4" Body="&lt;p&gt;My understanding is that with cross validation and model selection we try to address two things:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P1&lt;/strong&gt;. Estimate the expected loss on the population when training with our sample&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P2&lt;/strong&gt;. Measure and report our uncertainty of this estimation (variance, confidence intervals, bias, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Standard practice seems to be to do repeated cross validation, since this reduces the variance of our estimator.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when it comes to reporting and analysis, my understanding is that &lt;strong&gt;internal validation is better than external validation&lt;/strong&gt; because:&lt;/p&gt;&#10;&#10;&lt;p&gt;It is better to report:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The statistics of our estimator, e.g. its confidence interval, variance, mean, etc. on the full sample (in this case the CV sample). &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;than reporting: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The loss of our estimator on a hold-out subset of the original sample, since:&lt;/p&gt;&#10;&#10;&lt;p&gt;(i) This would be a &lt;strong&gt;single measurement&lt;/strong&gt; (&lt;strong&gt;&lt;em&gt;even if we pick our estimator with CV&lt;/em&gt;&lt;/strong&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;(ii) Our estimator for this single measurement would have been trained on a set (e.g. the CV set) that is smaller than our initial sample since we have to make room for the hold-out set. This results in a &lt;strong&gt;more biased&lt;/strong&gt; (pessimistic) estimation in &lt;strong&gt;P1&lt;/strong&gt; .&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is this correct? If not why? &lt;/p&gt;&#10;&#10;&lt;h3&gt;Background:&lt;/h3&gt;&#10;&#10;&lt;p&gt;It is easy to find textbooks that recommend dividing your sample into two sets: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The &lt;strong&gt;CV&lt;/strong&gt; set, which is subsequently and repeatedly divided into &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;validation&lt;/strong&gt; sets.&lt;/li&gt;&#10;&lt;li&gt;The &lt;strong&gt;hold-out&lt;/strong&gt; (test) set, only used at the end to report the estimator performance&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My question is an attempt to understand the merits and advantages of this textbook approach, considering that our goal is to really address the problems &lt;strong&gt;P1&lt;/strong&gt; and &lt;strong&gt;P2&lt;/strong&gt; at the beginning of this post. It looks to me that reporting on the hold-out test set is &lt;strong&gt;bad practice&lt;/strong&gt; since the analysis of the CV sample is more informative.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Nested K-fold vs repeated K-fold:&lt;/h3&gt;&#10;&#10;&lt;p&gt;One can in principle combine &lt;strong&gt;hold-out&lt;/strong&gt; with regular &lt;strong&gt;K-fold&lt;/strong&gt; to obtain  &lt;strong&gt;nested K-fold&lt;/strong&gt;. This would allow us to measure the variability of our estimator, but it looks to me that for the same number of total models trained (total # of folds) repeated K-fold would yield estimators that are &lt;strong&gt;less biased&lt;/strong&gt; and &lt;strong&gt;more accurate&lt;/strong&gt; than nested K-fold. To see this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Repeated K-fold uses a larger fraction of our total sample than nested K-fold for the same K (i.e. it leads to lower bias)&lt;/li&gt;&#10;&lt;li&gt;100 iterations would only give 10 measurements of our estimator in nested K-fold (K=10), but 100 measurements in K-fold (more measurements leads to lower variance in &lt;strong&gt;P2&lt;/strong&gt;)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What's wrong with this reasoning?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-12T13:31:31.367" FavoriteCount="11" Id="64147" LastActivityDate="2013-07-22T18:19:32.650" LastEditDate="2013-07-22T18:15:31.417" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="14" Tags="&lt;estimation&gt;&lt;cross-validation&gt;&lt;references&gt;" Title="Internal vs external cross-validation and model selection" ViewCount="1642" />
  <row Body="&lt;p&gt;There are tons of literature on the subject, but I have here a slightly subjective view that, I hope, does not duplicate other answers.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first important thing -- performance of an ML algorithm is not the only criterion that one should consider. For me, the two other criteria are (i) simplicity and (ii) transparency, and arguably both of these are linked.&lt;/p&gt;&#10;&#10;&lt;p&gt;Simplicity means the simplicity of the algorithm used. This has many ramifications; first of all, in my subjective view, statistics in science is about convincing yourself and your peers that the effect seen is &quot;real&quot; (or, that it is unlikely to be random). If you have to rely on a magic box which works in mysterious ways (or you just don't understand how it works), you will not be able to trust its results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Transparency means that you are able to discover &lt;em&gt;why&lt;/em&gt; your ML model classifies the data like it does, select important variables based on the ML model (e.g. using loadings from PLS or Gini index from random forests), construct a simplified model etc.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-12T13:31:40.870" Id="64148" LastActivityDate="2013-07-12T13:31:40.870" OwnerUserId="14803" ParentId="64114" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This is an interesting problem. Your data suggests some regularity (periodic $x^2$ like functions) but has sharp peaks at transitions. All this suggests a slightly complex model. I would model these data by a succession of $x_2$ functions parametrized by a coefficient and a displacement parameter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-12T15:03:58.553" Id="64159" LastActivityDate="2013-07-12T15:28:29.757" LastEditDate="2013-07-12T15:28:29.757" LastEditorUserId="7290" OwnerUserId="27961" ParentId="45731" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="64180" AnswerCount="1" Body="&lt;p&gt;I'm trying to understand the whole variance/std error thing of a time series of financial returns, and I think I'm stuck.&#10;I have a series of monthly stock return data (let's call it $X$), which has expected value 1.00795, and variance 0.000228 (std. dev is 0.01512).&#10;I'm trying to calculate the worst case of the annual return (let's say expected value minus twice the standard error). Which way is the best way to do it?&lt;br&gt;&lt;br&gt;&#10;&lt;strong&gt;A&lt;/strong&gt;. Calculate it for a single month ( $\mu_X-2\cdot \sigma_X=0.977$ ), and multiply it by itself 12 times (=&lt;strong&gt;0.7630&lt;/strong&gt;).&lt;br&gt;&lt;br&gt;&#10;&lt;strong&gt;B&lt;/strong&gt;. Assuming the months are independent, define $Y=X\cdot X\cdot ...\cdot X$ 12 times, find it's expected value $E[Y]=(E[X])^{12}$) and variance $\operatorname{var}[Y]=(\operatorname{var}[X]+(E[X])^2)^{12}&#10;- ((E[X]^2)^{12}$. The standard dev in this case is 0.0572, and the expected value minus twice the std. dev is &lt;strong&gt;0.9853&lt;/strong&gt;.&lt;br&gt;&lt;br&gt;&#10;&lt;strong&gt;C&lt;/strong&gt;. Multiply the monthly std. dev with $\sqrt{12}$ to get the annual one. use it to find  the worst case annual value ($\mu - 2\cdot \sigma$). It comes out as &lt;strong&gt;0.9949&lt;/strong&gt;.&#10;&lt;br&gt;&lt;br&gt;&#10;Which one is correct? What is the right way to calculate the expected annual value minus twice the std. dev if you know these properties only for the monthly data? (In general - if $Y=X\cdot X\cdot ...\cdot X$ 12 times and $\mu_X$, $\sigma_X$ are known, what is $\mu_Y-2\cdot \sigma_Y$ ?)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-12T18:18:06.653" Id="64172" LastActivityDate="2013-07-12T21:54:28.963" LastEditDate="2013-07-12T20:18:12.753" LastEditorUserId="27965" OwnerUserId="27965" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;variance&gt;&lt;independence&gt;&lt;finance&gt;" Title="Variance of annual return based on variance of monthly return" ViewCount="2626" />
  <row Body="&lt;p&gt;BMI is primarily used nowadays because of its ability to approximate abdominal visceral fat volume, useful in studying cardiovascular risk.  For a case study analyzing the adequacy of BMI in screening for diabetes see Chapter 15 of &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/CourseBios330&quot;&gt;http://biostat.mc.vanderbilt.edu/CourseBios330&lt;/a&gt; under &lt;em&gt;Handouts&lt;/em&gt;.  Several assessments are there.  You will see that a better power of height is closer to 2.5 but you can do better than using height and weight.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-12T19:12:56.520" Id="64175" LastActivityDate="2013-07-12T19:12:56.520" OwnerUserId="4253" ParentId="64171" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Here is a suggestion:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## Parameters&#10;n     &amp;lt;- 20                         # number of samples&#10;rho   &amp;lt;- 0.5                        # correlation between predictors&#10;beta  &amp;lt;- c(0.8,3,1.5,0,0,2,0,0,0)   # regression coefficients&#10;sigma &amp;lt;- 0.4                        # std error of noise (assumed Gaussian)&#10;## Covariance matrix&#10;d     &amp;lt;- length(beta)&#10;Q     &amp;lt;- toeplitz(c(1, rep(rho, d - 2)))&#10;## Design matrix&#10;X &amp;lt;- cbind(1, matrix(rnorm((d-1) * n), ncol = d-1) %*% chol(Q))&#10;## Dependent variable&#10;Y &amp;lt;- X %*% beta + rnorm(n, 0, sigma)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2013-07-12T20:08:34.060" Id="64178" LastActivityDate="2013-07-12T20:08:34.060" OwnerUserId="27403" ParentId="64150" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have satellite data that provides radiance which I use to compute the Flux (using surface and cloud info). Now using a regression method, I can develop a mathematical model directly relating radiance and flux and can be used to predict the flux for new radiance values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to do same using decision trees or regression trees? In a regression there is mathematical equation connecting dependent and independent variable? Using decision trees, how  could you develop such a model?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-13T00:15:44.423" Id="64201" LastActivityDate="2013-07-13T16:17:52.530" LastEditDate="2013-07-13T16:17:52.530" LastEditorUserId="21054" OwnerUserId="27926" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;cart&gt;" Title="Decision tree for output prediction" ViewCount="374" />
  
  <row AcceptedAnswerId="64215" AnswerCount="2" Body="&lt;p&gt;I'm not an expert, so forgive me if some of the terminology is a little clumsy. Happy to provide more information where required.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two vectors of 50 paired numeric values in R. I want to perform a two-tailed &lt;a href=&quot;http://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests&quot; rel=&quot;nofollow&quot;&gt;randomisation or permutation test&lt;/a&gt; to determine whether their differences are due to chance or not.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I want to do this type of test because I believe the distributions of the values in the vectors violate the assumptions of other tests such as the t-test (for example, many of the numeric values in the vector are 0).&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/BHH2/html/permtest.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;permtest&lt;/code&gt;&lt;/a&gt; function in &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/BHH2/html/permtest.html&quot; rel=&quot;nofollow&quot;&gt;the BHH2 library&lt;/a&gt;, almost does what I want, but it operates on all $2^{50}$ permutations, which will take too long. Instead, I want to estimate the p-value, by sampling a large number of the possible permutations. I had a look in the &lt;a href=&quot;http://cran.r-project.org/web/packages/coin/index.html&quot; rel=&quot;nofollow&quot;&gt;coin&lt;/a&gt; package, but nothing in there seems to do a permutation test with sampling from paired numeric vectors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some googling lead me to &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2011-April/274501.html&quot; rel=&quot;nofollow&quot;&gt;this email&lt;/a&gt;, which suggests that the reason I can't find a package to do it is that it's a one-liner in R. Unfortunately, I'm not experienced enough with R to be able to produce that one-liner.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there a package or method that will perform a two-tailed paired permutation test using only a sample of the permutation space?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;If not, would somebody be able to share a short bit of R code to do it?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-13T04:36:06.530" FavoriteCount="1" Id="64212" LastActivityDate="2014-11-08T23:25:56.593" OwnerUserId="27984" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;statistical-significance&gt;&lt;permutation&gt;" Title="Randomisation/permutation test for paired vectors in R" ViewCount="1012" />
  <row Body="&lt;p&gt;In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tag wiki reference: &lt;a href=&quot;http://en.wikipedia.org/wiki/Missing_data&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-13T11:59:48.543" Id="64220" LastActivityDate="2013-07-13T13:29:24.497" LastEditDate="2013-07-13T13:29:24.497" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  <row Body="When the data present lack of information (gaps), i.e., are not complete. Hence, it is important to consider this feature when performing a analysis or test." CommentCount="0" CreationDate="2013-07-13T11:59:48.543" Id="64221" LastActivityDate="2013-07-13T13:29:35.187" LastEditDate="2013-07-13T13:29:35.187" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;First, you are using &quot;terminal node&quot; incorrectly. The terminal nodes are the ones at the bottom of the tree that are not split.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am pretty sure that what you mean is that a non-terminal node is split by &quot;Income &gt; 5000&quot; and then the child node that answers &quot;yes&quot; is then split by &quot;income &amp;lt; 11000&quot;. If this is the entire tree, then you have 3 terminal nodes: Income &amp;lt;= 5000, income between 5000 and 11,000 and income over 11,000. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-07-13T12:24:25.407" Id="64223" LastActivityDate="2013-07-13T12:24:25.407" OwnerUserId="686" ParentId="64211" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In the regularisation context a &quot;large&quot; coefficient means that the estimate's magnitude is larger than it would have been, if a &lt;em&gt;fixed model specification&lt;/em&gt; had been used. It's the impact of obtaining not just the estimates, but also the model specification, from the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider what a procedure like stepwise regression will do for a given variable. If the estimate of its coefficient is small relative to the standard error, it will get dropped from the model. This could be because the true value really is small, or simply because of random error (or a combination of the two). If it's dropped, then we no longer pay it any attention. On the other hand, if the estimate is large relative to its standard error, it will be retained. Notice the imbalance: our final model will reject a variable when the coefficient estimate is small, but we will keep it when the estimate is large. Thus we are likely to overestimate its value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Put another way, what overfitting means is you're overstating the impact of a given set of predictors on the response. But the only way that you can overstate the impact is if the estimated coefficients are too big (and conversely, the estimates for your excluded predictors are too small).&lt;/p&gt;&#10;&#10;&lt;p&gt;What you should do is incorporate into your experiment a variable selection procedure, eg stepwise regression via &lt;code&gt;step&lt;/code&gt;. Then repeat your experiment multiple times, on different random samples, and save the estimates. You should find that all the estimates of the coefficients $\beta_3$ to $\beta_{10}$ are systematically too large, when compared to not using variable selection. Regularisation procedures aim to fix or mitigate this problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example of what I'm talking about.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;repeat.exp &amp;lt;- function(M)&#10;{&#10;    x &amp;lt;- seq(-2, 2, len=25)&#10;    px &amp;lt;- poly(x, 10)&#10;    colnames(px) &amp;lt;- paste0(&quot;x&quot;, 1:10)&#10;    out &amp;lt;- setNames(rep(NA, 11), c(&quot;(Intercept)&quot;, colnames(px)))&#10;    sapply(1:M, function(...) {&#10;        y &amp;lt;- x^2 + rnorm(N, s=2)&#10;        d &amp;lt;- data.frame(px, y)&#10;        b &amp;lt;- coef(step(lm(y ~ x1, data=d), y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, trace=0))&#10;        out[names(b)] &amp;lt;- b&#10;        out&#10;    })&#10;}&#10;&#10;set.seed(53520)&#10;z &amp;lt;- repeat.exp(M=1000)&#10;&#10;# some time later...&#10;rowMeans(abs(z), na.rm=TRUE)&#10;&#10;(Intercept)          x1          x2          x3          x4          x5          x6          x7          x8          x9         x10 &#10;   1.453553    3.162100    6.533642    3.108974    3.204341    3.131208    3.118276    3.217231    3.293691    3.149520    3.073062 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Contrast this to what happens when you don't use variable selection, and just fit everything blindly. While there is still some error in the estimates of $\beta_3$ to $\beta_{10}$, the average deviation is much smaller.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;repeat.exp.base &amp;lt;- function(M)&#10;{&#10;    x &amp;lt;- seq(-2, 2, len=25)&#10;    px &amp;lt;- poly(x, 10)&#10;    colnames(px) &amp;lt;- paste0(&quot;x&quot;, 1:10)&#10;    out &amp;lt;- setNames(rep(NA, 11), c(&quot;(Intercept)&quot;, colnames(px)))&#10;    sapply(1:M, function(...) {&#10;        y &amp;lt;- x^2 + rnorm(N, s=2)&#10;        d &amp;lt;- data.frame(px, y)&#10;        b &amp;lt;- coef(lm(y ~ ., data=d))&#10;        out[names(b)] &amp;lt;- b&#10;        out&#10;    })&#10;}&#10;&#10;set.seed(53520)&#10;z2 &amp;lt;- repeat.exp.base(M=1000)&#10;&#10;rowMeans(abs(z2))&#10;(Intercept)          x1          x2          x3          x4          x5          x6          x7          x8          x9         x10 &#10;   1.453553    1.676066    6.400629    1.589061    1.648441    1.584861    1.611819    1.607720    1.656267    1.583362    1.556168 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also, both L1 and L2 regularisation make the implicit assumption that all your variables, and hence coefficients, are in the same units of measurement, ie a unit change in $\beta_1$ is equivalent to a unit change in $\beta_2$. Hence the usual step of standardising your variables before applying either of these techniques.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-13T16:34:34.630" Id="64235" LastActivityDate="2013-07-13T18:58:29.547" LastEditDate="2013-07-13T18:58:29.547" LastEditorUserId="1569" OwnerUserId="1569" ParentId="64208" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Since the &quot;same sub&quot; samples are only in the &quot;between&quot; group, we need to know if they affect its mean. I would suggest running another test to compare the mean difference in the &quot;between&quot; group &lt;strong&gt;with&lt;/strong&gt; the &quot;same sub&quot; samples and the mean difference in this group &lt;strong&gt;without&lt;/strong&gt; the &quot;same sub&quot; samples (without C1_T1, C2_T2, C3_T3).&lt;br&gt;&#10;If this test will show no significant difference, then those samples do not affect the mean of the group, and you can run the test between the 2 groups. &lt;br&gt;&#10;If the test will show a significant difference, then you practically have 3 groups (within, between different subs, between same sub), and you can continue and assign a biological meaning to it.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-13T18:02:59.800" Id="64240" LastActivityDate="2013-07-13T18:02:59.800" OwnerUserId="27965" ParentId="64043" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;This paper describes a technique for online regularization which they apply to various algorithms, including logistic regression: &lt;a href=&quot;http://ai.stanford.edu/~chuongdo/papers/proximal.pdf&quot; rel=&quot;nofollow&quot;&gt;http://ai.stanford.edu/~chuongdo/papers/proximal.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-13T21:17:54.417" Id="64253" LastActivityDate="2013-07-13T21:17:54.417" OwnerUserId="8451" ParentId="64224" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can try and use linear regression using &lt;a href=&quot;https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;dummy variables&lt;/a&gt;. Try to regress the difference of each pair (let's call it $Y_i$) with a dummy variable marking if it's a &quot;between&quot; group ($D_1$ which can get 1 if it's &quot;between&quot; or 0 if it's &quot;within&quot;) and an interaction dummy variable $D_1\cdot D_2$ ($D_2$ gets 1 if it's &quot;same sub&quot; of 0 otherwise). &lt;br&gt;&lt;br&gt;So the regression equation will be:&lt;br&gt;&#10;$Y_i=\beta_0+\beta_1\cdot D_{1,i} + \beta_2\cdot D_{1,i}\cdot D_{2,i} + \epsilon_i$&lt;br&gt;&#10;where:&lt;br&gt;&#10;$Y_i$ - each pairwise difference&lt;br&gt;&#10;$D_{1,i}$ - 1 if it's &quot;between&quot;, 0 otherwise&lt;br&gt;&#10;$D_{2,i}$ - 1 if it's &quot;same sub&quot;, 0 otherwise&lt;br&gt;&#10;&lt;br&gt;&#10;The results can be interpreted:&lt;br&gt;&#10;$\beta_0$ - the mean difference in the &quot;within&quot; group&lt;br&gt;&#10;$\beta_0 + \beta_1$ - the mean difference in the &quot;between&quot; group, without &quot;same sub&quot; &lt;br&gt;&#10;$\beta_0 + \beta_1+ \beta_2$ - the mean difference in the &quot;between&quot; group, with &quot;same sub&quot;&lt;br&gt;&lt;br&gt;&#10;If you run this regression in &lt;a href=&quot;http://www3.nd.edu/~rwilliam/stats2/l51.pdf&quot; rel=&quot;nofollow&quot;&gt;stata&lt;/a&gt;, for example, you will get the t-test result for each $\beta$, so you could know if it's statistically significant. If $\beta_1$ is significant, it means that the change between the groups is significant. If $\beta_2$ is significant, it means that the &quot;same sub&quot;/&quot;different sub&quot; distinction is significant.&lt;br&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://www.sagepub.com/upm-data/21120_Chapter_7.pdf&quot; rel=&quot;nofollow&quot;&gt;Dummy variables - additional explanation&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-13T21:29:29.127" Id="64256" LastActivityDate="2013-07-13T21:29:29.127" OwnerUserId="27965" ParentId="64043" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Uniform_distribution&quot; rel=&quot;nofollow&quot;&gt;uniform distribution&lt;/a&gt; (either continuous or discrete).&lt;/p&gt;&#10;&#10;&lt;p&gt;See also&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Point_estimation#Bayesian_point-estimation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Point_estimation#Bayesian_point-estimation&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Description&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Description&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you use a uniform prior on a set that contains the MLE, then MAP=MLE always. The reason for this is that under this prior structure, the posterior distribution and the likelihood are proportional.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-14T00:06:11.393" Id="64265" LastActivityDate="2013-07-14T08:26:12.663" LastEditDate="2013-07-14T08:26:12.663" LastEditorUserId="21054" OwnerUserId="28000" ParentId="64259" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I got it! Well, with help. I found the part of the book that gives steps to work through when proving the $Var \left( \hat{\beta}_0 \right)$ formula (thankfully it doesn't actually work them out, otherwise I'd be tempted to not actually do the proof). I proved each separate step, and I think it worked. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using the book's notation, which is:&#10;$$&#10;SST_x = \displaystyle\sum\limits_{i=1}^n (x_i - \bar{x})^2,&#10;$$&#10;and $u_i$ is the error term.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Show that $\hat{\beta}_1$ can be written as $\hat{\beta}_1 = \beta_1 + \displaystyle\sum\limits_{i=1}^n w_i u_i$ where $w_i = \frac{d_i}{SST_x}$ and $d_i = x_i - \bar{x}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This was easy because we know that&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\hat{\beta}_1 &amp;amp;= \beta_1 + \frac{\displaystyle\sum\limits_{i=1}^n (x_i - \bar{x}) u_i}{SST_x} \\&#10;&amp;amp;= \beta_1 + \displaystyle\sum\limits_{i=1}^n \frac{d_i}{SST_x} u_i \\&#10;&amp;amp;= \beta_1 + \displaystyle\sum\limits_{i=1}^n w_i u_i&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Use part 1, along with $\displaystyle\sum\limits_{i=1}^n w_i = 0$ to show that $\hat{\beta_1}$ and $\bar{u}$ are uncorrelated, i.e. show that $E[(\hat{\beta_1}-\beta_1) \bar{u}] = 0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;E[(\hat{\beta_1}-\beta_1) \bar{u}] &amp;amp;= E[\bar{u}\displaystyle\sum\limits_{i=1}^n w_i u_i] \\&#10;&amp;amp;=\displaystyle\sum\limits_{i=1}^n  E[w_i \bar{u} u_i] \\&#10;&amp;amp;=\displaystyle\sum\limits_{i=1}^n w_i E[\bar{u} u_i] \\&#10;&amp;amp;= \frac{1}{n}\displaystyle\sum\limits_{i=1}^n w_i E\left(u_i\displaystyle\sum\limits_{j=1}^n u_j\right) \\&#10;&amp;amp;= \frac{1}{n}\displaystyle\sum\limits_{i=1}^n w_i \left[E\left(u_i u_1\right) +\cdots + E(u_i u_j) + \cdots+ E\left(u_i u_n \right)\right] \\&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;and because the $u$ are i.i.d., $E(u_i u_j) = E(u_i) E(u_j)$ when $ j \neq i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;When $j = i$, $E(u_i u_j) = E(u_i^2)$, so we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;&amp;amp;= \frac{1}{n}\displaystyle\sum\limits_{i=1}^n w_i \left[E(u_i) E(u_1) +\cdots + E(u_i^2) + \cdots + E(u_i) E(u_n)\right] \\&#10;&amp;amp;= \frac{1}{n}\displaystyle\sum\limits_{i=1}^n w_i E(u_i^2) \\&#10;&amp;amp;= \frac{1}{n}\displaystyle\sum\limits_{i=1}^n w_i \left[Var(u_i) + E(u_i) E(u_i)\right] \\&#10;&amp;amp;= \frac{1}{n}\displaystyle\sum\limits_{i=1}^n w_i \sigma^2 \\&#10;&amp;amp;= \frac{\sigma^2}{n}\displaystyle\sum\limits_{i=1}^n w_i \\&#10;&amp;amp;= \frac{\sigma^2}{n \cdot SST_x}\displaystyle\sum\limits_{i=1}^n (x_i - \bar{x}) \\&#10;&amp;amp;= \frac{\sigma^2}{n \cdot SST_x} \left(0\right)&#10;&amp;amp;= 0&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Show that $\hat{\beta_0}$ can be written as $\hat{\beta_0} = \beta_0 + \bar{u} - \bar{x}(\hat{\beta_1} - \beta_1)$. This seemed pretty easy too:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\hat{\beta_0} &amp;amp;= \bar{y} - \hat{\beta_1} \bar{x} \\&#10;&amp;amp;= (\beta_0 + \beta_1 \bar{x} + \bar{u}) - \hat{\beta_1} \bar{x} \\&#10;&amp;amp;= \beta_0 + \bar{u} - \bar{x}(\hat{\beta_1} - \beta_1).&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Use parts 2 and 3 to show that $Var(\hat{\beta_0}) = \frac{\sigma^2}{n} + \frac{\sigma^2 (\bar{x}) ^2} {SST_x}$:&#10;\begin{align}&#10;Var(\hat{\beta_0}) &amp;amp;= Var(\beta_0 + \bar{u} - \bar{x}(\hat{\beta_1} - \beta_1)) \\&#10;&amp;amp;= Var(\bar{u}) + (-\bar{x})^2 Var(\hat{\beta_1} - \beta_1) \\&#10;&amp;amp;= \frac{\sigma^2}{n} + (\bar{x})^2 Var(\hat{\beta_1}) \\&#10;&amp;amp;= \frac{\sigma^2}{n} + \frac{\sigma^2 (\bar{x}) ^2} {SST_x}.&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe this all works because since we provided that $\bar{u}$ and $\hat{\beta_1} - \beta_1$ are uncorrelated, the covariance between them is zero, so the variance of the sum is the sum of the variance. $\beta_0$ is just a constant, so it drops out, as does $\beta_1$ later in the calculations.&lt;/p&gt;&#10;&#10;&lt;p&gt;5) Use algebra and the fact that $\frac{SST_x}{n} = \frac{1}{n} \displaystyle\sum\limits_{i=1}^n x_i^2 - (\bar{x})^2$:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;Var(\hat{\beta_0}) &amp;amp;= \frac{\sigma^2}{n} + \frac{\sigma^2 (\bar{x}) ^2} {SST_x} \\&#10;&amp;amp;= \frac{\sigma^2 SST_x}{SST_x n} + \frac{\sigma^2 (\bar{x})^2}{SST_x} \\&#10;&amp;amp;= \frac{\sigma^2}{SST_x} \left( \frac{1}{n} \displaystyle\sum\limits_{i=1}^n x_i^2 - (\bar{x})^2 \right) + \frac{\sigma^2 (\bar{x})^2}{SST_x} \\&#10;&amp;amp;=  \frac{\sigma^2 n^{-1} \displaystyle\sum\limits_{i=1}^n x_i^2}{SST_x}&#10;\end{align}&lt;/p&gt;&#10;" CommentCount="22" CreationDate="2013-07-14T00:23:22.433" Id="64266" LastActivityDate="2013-07-18T17:40:48.827" LastEditDate="2013-07-18T17:40:48.827" LastEditorUserId="27969" OwnerUserId="27969" ParentId="64195" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;&lt;em&gt;I agree it's a tricky situation. These are just a few thoughts.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Whether to average d effect sizes:&lt;/strong&gt;&#10;If you're not interested in subscales, then my first choice would be to take the average effect size for the subscales in a given study. &lt;/p&gt;&#10;&#10;&lt;p&gt;That assumes that all subscales are equally relevant to your research question. If some scales are more relevant, then I might just use those subscales.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are interested in differences between subscales, then it makes sense to include the effect size for each subscale coded for type.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Standard error of d effect sizes:&lt;/strong&gt; Presumably you are using a formula to calculate the standard error of d based on the value of d and the group sample sizes. Adapting &lt;a href=&quot;http://stats.stackexchange.com/a/8508/183&quot;&gt;this formula&lt;/a&gt;, we get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$se(d) = \sqrt{\left( \frac{n_1 + n_2}{n_1 n_2} + \frac{d^2}{2(n_1+n_2-2)}\right) \left(\frac{n_1 + n_2}{n_1+n_2-2} \right)}, $$&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;where $n_1$ and $n_2$ are the sample sizes of the two groups being&#10;  compared and $d$ is Cohen's $d$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I imagine you could apply such a formula to calculate the standard error to the average d value for the subscales.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-14T12:12:17.507" Id="64291" LastActivityDate="2013-07-15T10:53:56.857" LastEditDate="2013-07-15T10:53:56.857" LastEditorUserId="183" OwnerUserId="183" ParentId="64289" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have to compare the mean between two groups over time. The difficulty here is that I have to make this coparison about 80 times. And it is not enough to know whether there is a difference, but I also have to say &lt;em&gt;when&lt;/em&gt; it occurs.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the context: a behavioral marker measured in 2 groups of 5 mice each, measured over and over again in different conditions, in day- and night-time, before and after some treatments etc.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-15T09:47:03.750" Id="64342" LastActivityDate="2013-07-15T09:48:51.623" LastEditDate="2013-07-15T09:48:51.623" LastEditorUserId="21054" OwnerUserId="28036" PostTypeId="1" Score="3" Tags="&lt;repeated-measures&gt;" Title="Repeated comparisons over time between two groups" ViewCount="66" />
  
  <row Body="&lt;p&gt;Bootstrapping typically involves sampling with replacement from your sample data. &lt;strong&gt;The size of each bootstrapped sample should be the same as your actual sample size&lt;/strong&gt; (i.e., n=50). This allows you to study the sampling distribution of your estimators in your actual sample size. &lt;/p&gt;&#10;&#10;&lt;p&gt;As @Andy notes, you can arbitrarily increase your bootstrapped sample size (lets call it $b$).  As $b$ approaches $\infty$ the standard error of your estimator will tend towards zero. All non-zero coefficients will by definition be statistically significantly different from zero. I remember reading a journal article where an author artificially inflated their sample size using bootstrapping and reported the resulting p-values. This is very wrong! It also may help to explain some people's naive mistrust in the concept of bootstrapping.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bootstrapping is not a tool to make standard errors smaller. It is a tool to, among other things, give an estimate of standard error. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your sample size is small (i.e., n =50) so presumably you have larger standard errors than you would like. This is the reality. Thus, you can accept the large standard errors or collect more data and increase your n.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-15T11:25:40.627" Id="64349" LastActivityDate="2013-07-17T11:27:36.030" LastEditDate="2013-07-17T11:27:36.030" LastEditorUserId="183" OwnerUserId="183" ParentId="60334" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="64358" AnswerCount="1" Body="&lt;p&gt;Let $p$ be a matrix, where each row represents an observation of a 4-variate normally distributed random variable $\mathcal{N}_4(\mu,\Sigma)$. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Is there any Bootstrap methode to get a good estimation for Σ?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If not, is the number of the following sample enough to bootstrap the distribution of any statistic T which operates on the population where Y comes from? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Here's what I tried so far:&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y&amp;lt;-data.frame(response=c(10,19,27,28,9,13,25,29,4,10,20,18,5,6,12,17),&#10;               treatment=factor(rep(1:4,4)),&#10;               subject=factor(rep(1:4,each=4))&#10;               )&#10;&#10;p&amp;lt;-matrix(Y$response,4,4,byrow=T)&#10;B&amp;lt;-1000&#10;sampleB&amp;lt;-sample(1:4,4*B,replace=T)&#10;fit&amp;lt;-lm(p[sampleB,]~1)&#10;cov(residuals(fit))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I also tried &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(nlme)&#10;require(mgcv)&#10;nSubj &amp;lt;- 20&#10;sampleB&amp;lt;-sample(1:4,nSubj,replace=T)&#10;y&amp;lt;-data.frame(response=c(t(p[sampleB,])),&#10;           treatment=factor(rep(1:4,nSubj)),&#10;           subject=factor(rep(1:nSubj,each=4))&#10;           )&#10;&#10;fit &amp;lt;- lme(response~-1+treatment,y,random=~1|subject,correlation=corSymm())&#10;extract.lme.cov(fit,y)[1:4,1:4]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but I get the error code: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in lme.formula(response ~ -1 + treatment, y, random = ~1 | subject,  : &#10;nlminb problem, convergence error code = 1&#10;message = iteration limit reached without convergence (10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2013-07-15T11:43:24.877" Id="64355" LastActivityDate="2013-07-16T13:12:12.377" LastEditDate="2013-07-16T13:12:12.377" LastEditorUserId="27879" OwnerUserId="27879" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;mathematical-statistics&gt;&lt;bootstrap&gt;&lt;covariance&gt;" Title="Is bootstrapping appropriate for estimating a multivariate normal covariance matrix using a small sample size?" ViewCount="847" />
  <row Body="&lt;p&gt;From: &lt;a href=&quot;http://www.cs.waikato.ac.nz/ml/weka/index.html&quot; rel=&quot;nofollow&quot;&gt;The University of Waikato&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;...The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Found only on the islands of New Zealand, the Weka is a flightless bird with an inquisitive nature. The name is pronounced like this, and the bird sounds like this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Weka is open source software issued under the GNU General Public License.&quot;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-15T14:46:51.353" Id="64373" LastActivityDate="2013-07-15T14:57:30.147" LastEditDate="2013-07-15T14:57:30.147" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;R has a full &lt;a href=&quot;http://cran.r-project.org/web/views/Robust.html&quot; rel=&quot;nofollow&quot;&gt;task view&lt;/a&gt; listing the major implementations. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-15T15:40:08.183" Id="64380" LastActivityDate="2013-07-15T15:40:08.183" OwnerUserId="603" ParentId="43577" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Say we have a batch of data we use to learn an SVM, a Regressor, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;And we use the learned model to do online classification, the learning is not done online, the online part is only the processing.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do we go about Normalization, Whitening, Rescaling, etc &lt;/p&gt;&#10;&#10;&lt;p&gt;Since most of these techniques require a batch of data to work, and even then, if the features do not have the same normalization, it may not work as advertised.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" ClosedDate="2013-07-15T19:03:23.500" CommentCount="0" CreationDate="2013-07-15T16:33:00.777" Id="64389" LastActivityDate="2013-07-15T17:19:31.670" OwnerUserId="6327" PostTypeId="1" Score="0" Tags="&lt;normalization&gt;" Title="Online feature preprocessing" ViewCount="45" />
  <row Body="&lt;p&gt;I'll go out on a limb and disagree with @whuber here.  I don't think there's anything wrong with putting bands around pdfs, as long as you understand what they are: pointwise errors. It's like the similar confidence bands around smooth curves for additive regression models. If the bands are wide enough, they make it look like the curve could be far away from the center of the data, but people generally understand what's going on.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'd do is obtain the confidence bands and median of your density function directly from the bootstrap samples. You have 1000 estimated functions, so you can obtain your quantiles as the 2.5%, 50% and 97.5% points of the distribution of these functions at any given x.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main downside is that there's no guarantee the resulting quantiles will actually be members of your parametric density family. Depending on your application, this may or may not be a problem. I doubt it would be, since the bootstrap is generally used only to get estimates of error, and you already have a parametric result from your original sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a simple example using the gamma density and moment estimators. You can easily substitute the inverse gamma and maximum likelihood.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# the data&#10;x &amp;lt;- rgamma(100, 2, scale=1)&#10;&#10;# get 1000 bootstrapped estimates of the density, over a regular grid&#10;xs &amp;lt;- seq(0, 8, len=201)&#10;ests &amp;lt;- sapply(1:1000, function(i) {&#10;    xi &amp;lt;- sample(x, size=length(x), replace=TRUE)&#10;    vi &amp;lt;- var(xi)&#10;    mi &amp;lt;- mean(xi)&#10;    dgamma(xs, shape=mi^2/vi, scale=vi/mi)&#10;})&#10;&#10;# plot the individual estimates&#10;plot(xs, ests[, 1], type=&quot;l&quot;, col=rgb(.6, .6, .6, .1), ylim=range(ests))&#10;for(i in 2:ncol(ests)) lines(xs, ests[, i], col=rgb(.6, .6, .6, .1))&#10;&#10;# get the 2.5%, 50% and 97.5% quantiles of the density estimate at each grid point&#10;quants &amp;lt;- apply(ests, 1, quantile, c(0.025, 0.5, 0.975))&#10;lines(xs, quants[1, ], col=&quot;red&quot;, lwd=1.5, lty=2)&#10;lines(xs, quants[3, ], col=&quot;red&quot;, lwd=1.5, lty=2)&#10;lines(xs, quants[2, ], col=&quot;darkred&quot;, lwd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9loby.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-15T17:43:32.083" Id="64397" LastActivityDate="2013-07-15T17:43:32.083" OwnerUserId="1569" ParentId="64371" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I found part of the solution from colleagues on SAS-L and at SAS:&lt;/p&gt;&#10;&#10;&lt;p&gt;In SAS PROC MIXED, if you use the default (REML) AIC is affected only by the random effects and covariance structure. If you use ML, then AIC is affected by the fixed effects (but not the covariance structure).&lt;/p&gt;&#10;&#10;&lt;p&gt;Apparently, the reasons for doing this (which still seems counterintuitive to me) is found in &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1590475003&quot; rel=&quot;nofollow&quot;&gt;Littell et al&lt;/a&gt; and also in a paper by &lt;a href=&quot;http://www.biostatistiikanseura.org/NLM_workshop/SC_2DAY.pdf&quot; rel=&quot;nofollow&quot;&gt;Vonesh and Chinchilli&lt;/a&gt;  but I have not read these. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have a new example where the LL improves a lot with the addition of a nonsignificant variable; I will post that as a new question. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-15T18:11:27.537" Id="64401" LastActivityDate="2013-07-15T18:11:27.537" OwnerUserId="686" ParentId="64227" PostTypeId="2" Score="1" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;The bibliography states that if q is a symmetric distribution the ratio q(x|y)/q(y|x) becomes 1 and the algorithm is called Metropolis. Is that correct? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes, this is correct. The Metropolis algorithm is a special case of the MH algorithm.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What about &quot;Random Walk&quot; Metropolis(-Hastings)? How does it differ from the other two?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In a random walk, the proposal distribution is re-centered after each step at the value last generated by the chain. Generally, in a random walk the proposal distribution is gaussian, in which case this random walk satisfies the symmetry requirement and the algorithm is metropolis. I suppose you could perform a &quot;pseudo&quot; random walk with an asymmetric distribution which would cause the proposals too drift in the opposite direction of the skew (a left skewed distribution would favor proposals toward the right). I'm not sure why you would do this, but you could and it would be a metropolis hastings algorithm (i.e. require the additional ratio term).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;How does it differ from the other two?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In a non-random walk algorithm, the proposal distributions are fixed. In the random walk variant, the center of the proposal distribution changes at each iteration.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What if the proposal distribution is a Poisson distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Then you need to use MH instead of just metropolis. Presumably this would be to sample a discrete distribution, otherwise you wouldn't want to use a discrete function to generate your proposals. &lt;/p&gt;&#10;&#10;&lt;p&gt;In any event, if the sampling distribution is truncated or you have prior knowledge of its skew, you probably want to use an asymmetric sampling distribution and therefore need to use metropolis-hastings.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Could someone give me a simple code (C, python, R, pseudo-code or whatever you prefer) example?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here's metropolis:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Metropolis &amp;lt;- function(F_sample # distribution we want to sample&#10;                      , F_prop  # proposal distribution &#10;                      , I=1e5   # iterations&#10;               ){&#10;  y = rep(NA,T)&#10;  y[1] = 0    # starting location for random walk&#10;  accepted = c(1)&#10;&#10;  for(t in 2:I)    {&#10;    #y.prop &amp;lt;- rnorm(1, y[t-1], sqrt(sigma2) ) # random walk proposal&#10;    y.prop &amp;lt;- F_prop(y[t-1]) # implementation assumes a random walk. &#10;                             # discard this input for a fixed proposal distribution&#10;&#10;    # We work with the log-likelihoods for numeric stability.&#10;    logR = sum(log(F_sample(y.prop))) -&#10;           sum(log(F_sample(y[t-1])))    &#10;&#10;    R = exp(logR)&#10;&#10;    u &amp;lt;- runif(1)        ## uniform variable to determine acceptance&#10;    if(u &amp;lt; R){           ## accept the new value&#10;      y[t] = y.prop&#10;      accepted = c(accepted,1)&#10;    }    &#10;    else{&#10;      y[t] = y[t-1]      ## reject the new value&#10;      accepted = c(accepted,0)&#10;    }    &#10;  }&#10;  return(list(y, accepted))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let's try using this to sample a bimodal distribution. First, let's see what happens if we use a random walk for our propsal:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(100)&#10;&#10;test = function(x){dnorm(x,-5,1)+dnorm(x,7,3)}&#10;&#10;# random walk&#10;response1 &amp;lt;- Metropolis(F_sample = test&#10;                       ,F_prop = function(x){rnorm(1, x, sqrt(0.5) )}&#10;                      ,I=1e5&#10;                       )&#10;y_trace1 = response1[[1]]; accpt_1 = response1[[2]]&#10;mean(accpt_1) # acceptance rate without considering burn-in&#10;# 0.85585   not bad&#10;&#10;# looks about how we'd expect&#10;plot(density(y_trace1))&#10;abline(v=-5);abline(v=7) # Highlight the approximate modes of the true distribution&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5gOs9.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's try sampling using a fixed proposal distribution and see what happens:&lt;/p&gt;&#10;&#10;&lt;p&gt;response2 &amp;lt;- Metropolis(F_sample = test&#10;                            ,F_prop = function(x){rnorm(1, -5, sqrt(0.5) )}&#10;                            ,I=1e5&#10;    )&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y_trace2 = response2[[1]]; accpt_2 = response2[[2]]&#10;mean(accpt_2) # .871, not bad&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This looks ok at first, but if we take a look at the posterior density...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(density(y_trace2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sdPhu.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;we'll see that it's completely stuck at a local maximum. This isn't entirely surprising since we actually centered our proposal distribution there. Same thing happens if we center this on the other mode: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;response2b &amp;lt;- Metropolis(F_sample = test&#10;                        ,F_prop = function(x){rnorm(1, 7, sqrt(10) )}&#10;                        ,I=1e5&#10;)&#10;&#10;plot(density(response2b[[1]]))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can try dropping our proposal &lt;em&gt;between&lt;/em&gt; the two modes, but we'll need to set the variance really high to have a chance at exploring either of them&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;response3 &amp;lt;- Metropolis(F_sample = test&#10;                        ,F_prop = function(x){rnorm(1, -2, sqrt(10) )}&#10;                        ,I=1e5&#10;)&#10;y_trace3 = response3[[1]]; accpt_3 = response3[[2]]&#10;mean(accpt_3) # .3958! &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Notice how the choice of the center of our proposal distribution has a significant impact on the acceptance rate of our sampler.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(density(y_trace3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UYLDx.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(y_trace3) # we really need to set the variance pretty high to catch &#10;               # the mode at +7. We're still just barely exploring it&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We &lt;em&gt;still&lt;/em&gt; get stuck in the closer of the two modes. Let's try dropping this directly between the two modes.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;response4 &amp;lt;- Metropolis(F_sample = test&#10;                        ,F_prop = function(x){rnorm(1, 1, sqrt(10) )}&#10;                        ,I=1e5&#10;)&#10;y_trace4 = response4[[1]]; accpt_4 = response4[[2]]&#10;&#10;plot(density(y_trace1))&#10;lines(density(y_trace4), col='red')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mkO7V.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, we're getting closer to what we were looking for. Theoretically, if we let the sampler run long enough we can get a representative sample out of any of these proposal distributions, but the random walk produced a usable sample very quickly, and we had to take advantage of our knowledge of how the posterior was supposed to look to tune the fixed sampling distributions to produce a usable result (which, truth be told, we don't quite have yet in &lt;code&gt;y_trace4&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll try to update with an example of metropolis hastings later. You should be able to see fairly easily how to modify the above code to produce a metropolis hastings algorithm (hint: you just need to add the supplemental ratio into the &lt;code&gt;logR&lt;/code&gt; calculation).&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-07-15T18:17:04.690" Id="64402" LastActivityDate="2013-07-15T19:04:09.550" LastEditDate="2013-07-15T19:04:09.550" LastEditorUserId="8451" OwnerUserId="8451" ParentId="64293" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="64412" AnswerCount="1" Body="&lt;p&gt;In &lt;a href=&quot;http://stats.stackexchange.com/questions/64227/improvements-in-aic-when-adding-a-variable-that-is-not-close-to-signficant/64401#64401&quot;&gt;this question&lt;/a&gt; I asked about changes to AIC when adding a variable. It turns out to be partly due to the way SAS figures AIC.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I now have two models where the log likelihood improves a lot:&lt;/p&gt;&#10;&#10;&lt;p&gt;This code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;title 'Test - only forced';&#10;ods select FitStatistics SolutionF;&#10;proc mixed data = menfat.data method = ml;&#10; class &amp;amp;classvar;&#10; model  bmd_legneck_change_per_year = &amp;amp;bmddef bmdlegneck_1/solution;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;had  -2LL of 1631.9&lt;/p&gt;&#10;&#10;&lt;p&gt;adding one variable with this code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;title 'Test - add packyrs';&#10;ods select FitStatistics SolutionF;&#10;proc mixed data = menfat.data method = ml;&#10; class &amp;amp;classvar;&#10; model  bmd_legneck_change_per_year = &amp;amp;bmddef packyrs bmdlegneck_1/solution;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;had a -2LL of 1607.6 (which is highly significant difference of 24.3 on 1 df) but the parameter estimate for packyrs was 0.0016 with an SE of 0.005 and a p of 0.77.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could this be due to improvement of the fit of other variables in the model? &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Responding to Macro; the model is a linear regression where the dependent variable is a change per year of a physical quality (bone density) and the independent variables are a mix of yes/no variables and continuous ones. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-07-15T18:41:03.823" FavoriteCount="0" Id="64409" LastActivityDate="2013-07-15T19:47:20.810" LastEditDate="2013-07-15T19:09:54.897" LastEditorUserId="686" OwnerUserId="686" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;model-selection&gt;&lt;linear-model&gt;&lt;likelihood&gt;" Title="Log likelihood improves with addition of a nonsignificant variable" ViewCount="193" />
  <row AnswerCount="1" Body="&lt;p&gt;Given a sequence of i.i.d. random variables, say, $X_i \in [0,1]$ for $i = 1,2,...,n$, I'm trying to bound the expected number of times the empirical mean $\frac{1}{n}\sum_{i=1}^n X_i$ will exceed a value, $c \geq 0$, as we continue to draw samples, that is:&#10;$$&#10;\mathcal{T} \overset{def}{=} \sum_{j=1}^n \mathbb{P} \left(\left\{ \frac{1}{j}\sum_{i=1}^j X_i \geq c\right\}\right)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we assume that $c = a + \mathbb{E}[X]$ for some $a &amp;gt; 0$, we can use &lt;a href=&quot;http://en.wikipedia.org/wiki/Hoeffding%27s_inequality&quot;&gt;Hoeffding's inequality&lt;/a&gt; to arrive at &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\mathcal{T} &amp;amp; \leq \sum_{j=1}^n e^{-2ja^2} \\&#10;&amp;amp; = \frac{1 - e^{-2 a^2 n}}{e^{2 a^2}-1}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Which looks nice (maybe) but is actually quite a loose bound, are there any better ways of bounding this value? I expect there may be a way since the different events (for each $j$) are clearly not independent, I'm not aware of any way to exploit this dependence.  Also, it would be nice to remove the restriction that $c$ is greater than the mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;: The restriction on $c$ being greater than the mean can be removed if we use &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov%27s_inequality&quot;&gt;Markov's Inequality&lt;/a&gt; as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\mathcal{T} &amp;amp; \leq \sum_{j=1}^n \frac{\frac{1}{j}\mathbb{E}[X]}{c} \\&#10;&amp;amp; = \frac{\mathbb{E}[X]H_n}{c}&#10;\end{align}&#10;Which is more general, but much worse than the above bound, although it's clear that $\mathcal{T}$ must diverge whenever $c \leq \mathbb{E}[X]$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-15T23:55:20.803" FavoriteCount="1" Id="64432" LastActivityDate="2014-02-04T02:32:16.440" LastEditDate="2013-07-16T23:07:34.667" LastEditorUserId="5321" OwnerUserId="5321" PostTypeId="1" Score="10" Tags="&lt;mathematical-statistics&gt;&lt;expected-value&gt;&lt;bounds&gt;" Title="Expected number of times the empirical mean will exceed a value" ViewCount="186" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $A$ and $B$ denote two events, and suppose that  &lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{P}(A) = 0.2$, $\text{P}(B) = 0.3$, and $\text{P}(A\cap B) = 0.1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are the following computations correct?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$\rm{P}(A \cup B) = \rm{P}(A) + \rm{P}(B) - \rm{P}(A\cap B) = 0.4$&lt;/li&gt;&#10;&lt;li&gt;$\rm{P}(B') = 1 - \rm{P}(B) = 0.7$, where $B'$ denotes the complement of $B$&lt;/li&gt;&#10;&lt;li&gt;$\rm{P}(B\cap A') = \rm{P}(B)\times \rm{P}(A') = 0.24$&lt;/li&gt;&#10;&lt;li&gt;$\rm{P}(A' \cup B') = \rm{P}(A') + \rm{P}(B') - \rm{P}(A'\cap B') = 0.94$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;[NB: $\cup$ = &quot;or&quot; and $\cap$ = &quot;and&quot;]&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-07-16T00:26:05.030" Id="64435" LastActivityDate="2013-08-28T21:32:07.110" LastEditDate="2013-08-28T21:32:07.110" LastEditorUserId="27581" OwnerUserId="28068" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;proof&gt;" Title="How to calculate the probability $P(A' \cup B')$ and related properties?" ViewCount="84" />
  
  <row AcceptedAnswerId="64447" AnswerCount="1" Body="&lt;p&gt;How can reliability and validity of &lt;a href=&quot;http://en.wikipedia.org/wiki/Content_analysis&quot;&gt;content analysis&lt;/a&gt; be quantified when there is only one person coding the data?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-16T01:10:12.387" Id="64437" LastActivityDate="2013-07-17T17:41:11.233" LastEditDate="2013-07-16T05:09:20.950" LastEditorUserId="183" OwnerUserId="22478" PostTypeId="1" Score="5" Tags="&lt;reliability&gt;&lt;validity&gt;&lt;qualitative&gt;" Title="How can reliability and validity of content analysis be quantified when there is only one person coding the data?" ViewCount="304" />
  <row Body="&lt;p&gt;Here is a counter example. Let $X_{n}$, $n=1,2,\ldots$, be&#10;a sequence of random variables, whose distributions are defined as&#10;follows,&#10;$$&#10;X_{n}=\begin{cases}&#10;0, &amp;amp; \mbox{with probability }\frac{n-1}{n},\\&#10;n^{2}, &amp;amp; \mbox{with probability }\frac{1}{n}.&#10;\end{cases}&#10;$$&#10;To see $X_{n}\sim O_{p}\left(1\right)$, let $\varepsilon&amp;gt;0$ be an&#10;arbitrary positive number, and there exists $M_{\varepsilon}=\left\lfloor \varepsilon^{-1}+1\right\rfloor ^{2}$&#10;($\left\lfloor \varepsilon^{-1}\right\rfloor $ is the integer part&#10;of $\varepsilon^{-1}$) such that&#10;\begin{eqnarray*}&#10;\sup_{n}\Pr\left(\left|X_{n}\right|\geq M_{\varepsilon}\right) &amp;amp; = &amp;amp; \Pr\left(X_{\left\lfloor \varepsilon^{-1}+1\right\rfloor }=\left(\left\lfloor \varepsilon^{-1}+1\right\rfloor \right)^{2}\right)\\&#10; &amp;amp; = &amp;amp; \frac{1}{\left\lfloor \varepsilon^{-1}+1\right\rfloor }\\&#10; &amp;amp; &amp;lt; &amp;amp; \varepsilon.&#10;\end{eqnarray*}&#10;However, $\mathrm{{E}}\left(X_{n}\right)=n$ by the design, and $\sup_{n}\mathrm{{E}}X_{n}=\infty$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Intuitively, 'bounded in probability' only restricts the probability placed on the extreme values; it does not say anything about the ratio between the extreme value and its associated probability. I hope this example could clarify something for your problem.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-16T06:38:58.030" Id="64450" LastActivityDate="2013-07-16T06:38:58.030" OwnerUserId="27693" ParentId="64400" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Once the Fisher's z transformations are done it is just a case of obtaining p-values&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Correlations    &#10;cor.test (df1$a, df1$b, method = &quot;p&quot;)&#10;cor.test (df2$a, df2$b, method = &quot;p&quot;)&#10;&#10;# function to do fisher transformations &#10;fisher.z&amp;lt;- function (r1,r2,n1,n2) ((0.5*log((1+r1)/(1-r1)))-(0.5*log((1+r2)/(1-r2))))/((1/(n1-3))+(1/(n2-3)))^0.5&#10;&#10;# or this (either version will suffice) &#10;fisher.z&amp;lt;- function (r1,r2,n1,n2) (atanh(r1) - atanh(r2)) / ((1/(n1-3))+(1/(n2-3)))^0.5&#10;&#10;#input n and r from correlations manually (two tailed test)&#10;2*(1-pnorm(abs(fisher.z(r1= ,r2= ,n1= ,n2= ))))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;See the final four slides of &lt;a href=&quot;http://courses.education.illinois.edu/EdPsy580/lectures/correlation-ha.pdf&quot; rel=&quot;nofollow&quot;&gt;this presentation&lt;/a&gt; and the &lt;code&gt;pnorm()&lt;/code&gt; function in r.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-16T08:16:54.593" Id="64456" LastActivityDate="2013-07-16T09:15:27.603" LastEditDate="2013-07-16T09:15:27.603" LastEditorUserId="16542" OwnerUserId="16542" ParentId="64152" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="64460" AnswerCount="1" Body="&lt;p&gt;I'm a bit lost here. I have a matrix of response variables, $y$, and I fit a model to account for a number of predictor variables, say $x_1$, $x_2$ and $x_3$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm.m &amp;lt;- lm( y ~ x1 + x2 + x3 )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can then extract the estimated covariance matrix of the residuals (if I understand correctly), for example using the &lt;code&gt;estVar&lt;/code&gt; function from R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;c &amp;lt;- estVar( lm.m )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What should I do to turn this into a correlation matrix? Divide each cell by the product of square roots variances of the residuals? Or the original variables? I'm confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'm trying to achieve is to calculate the correlation matrix of the residuals from the model. Something that could be achieved by&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y2 &amp;lt;- apply( y, 2, function( yy ) lm( yy ~ x1 + x2 + x3 )$residuals )&#10;c &amp;lt;- cor( y2 )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2013-07-16T08:42:02.057" Id="64459" LastActivityDate="2013-07-16T09:21:24.517" OwnerUserId="14803" PostTypeId="1" Score="1" Tags="&lt;linear-model&gt;&lt;covariance&gt;&lt;variance-covariance&gt;" Title="Using the covariance matrix to calculate correlations" ViewCount="233" />
  <row Body="&lt;p&gt;Generally speaking, you would want to use the scale scores (e.g. the mean rating across all three impulsiveness items, etc.) in any subsequent analysis (correlation, test of differences, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason for that is that having several items increases the reliability (and therefore ensures a lower standard error of measurement, more precise estimation, higher statistical power, etc.). Intuitively, item-specific error variance “averages out” and the scale score provides a better measure of the underlying construct with a higher “resolution” than the original 1-7 format. This is the whole point of multi-item scales (and, in fact, of Rensis Likert's work as he focused on the combination of several items and not particularly on the item's response format commonly associated with his name).&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, this only works well if the items really do measure the same thing and satisfy some other assumptions. Traditional scale building techniques (factor analysis, etc.) are intended to document that and select “good” items to include in a scale. Since what you have is apparently an &lt;em&gt;ad hoc&lt;/em&gt; scale with a limited number of items and probably a small number of observations, there is not much else you can do but assume the items really do form a reasonable scale based on your personal judgment of their meaning.&lt;/p&gt;&#10;&#10;&lt;p&gt;The worse thing that could happen is not so much a failure to find a meaningful effect (which is of course perfectly possible but presents less risk of being misinterpreted). Rather, it would be to find an effect that looks interesting but is in fact driven by one or two items and doesn't exactly means what you think it means based on your interpretation of the whole scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively you could still analyze each item separately (by computing correlations if I understood you correctly) but you will probably run into several problems: reduced power/attenuated correlations, potentially conflicting results, etc. If there are some meaningful differences between the items, this could be the only way to make sense of the results (imagine, say, that “spending time in store X” is mainly driven by interior design and “loyal customer” by pricing, what sense does it make to subsume them in a single measure?).&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-07-16T09:05:51.930" Id="64462" LastActivityDate="2013-07-16T09:37:04.037" LastEditDate="2013-07-16T09:37:04.037" LastEditorUserId="6029" OwnerUserId="6029" ParentId="64457" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;First order AR (or any order AR) seems to me to be more appropriate for longitudinal data, which yours doesn't seem to be (from the code) unless there is some reason to suspect that factor 1 should be closer to factor 2 than to factor 3 (and so on). This might be if they are (say) different doses of a medicine.&lt;/p&gt;&#10;&#10;&lt;p&gt;I usually test several covariance structures and use information criteria to decide on which to use (in my experience the different criteria usually agree; but people have strong opinions on which is best). I would test at least compound symmetry and unstructured. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-16T11:20:08.883" Id="64472" LastActivityDate="2013-07-16T11:20:08.883" OwnerUserId="686" ParentId="64469" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working with some exploratory spatial analysis in R using spdep package.&lt;/p&gt;&#10;&#10;&lt;p&gt;I came across an option to adjust &lt;em&gt;p&lt;/em&gt;-values of local indicators of spatial association (LISA) calculated using &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/spdep/html/localmoran.html&quot;&gt;&lt;code&gt;localmoran&lt;/code&gt;&lt;/a&gt; function. According to the docs it is aimed at:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;... probability value adjustment for multiple tests.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Further in the docs of &lt;code&gt;p.adjustSP&lt;/code&gt; I read that the options available are:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The adjustment methods include the Bonferroni correction&#10;  ('&quot;bonferroni&quot;') in which the p-values are multiplied by the number of&#10;  comparisons. Four less conservative corrections are also included by&#10;  Holm (1979) ('&quot;holm&quot;'), Hochberg (1988) ('&quot;hochberg&quot;'), Hommel (1988)&#10;  ('&quot;hommel&quot;') and Benjamini &amp;amp; Hochberg (1995) ('&quot;fdr&quot;'), respectively.&#10;  A pass-through option ('&quot;none&quot;') is also included.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The first four methods are designed to give strong control of the&#10;  family-wise error rate. There seems no reason to use the unmodified&#10;  Bonferroni correction because it is dominated by Holm's method, which&#10;  is also valid under arbitrary assumptions.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Hochberg's and Hommel's methods are valid when the hypothesis tests&#10;  are independent or when they are non-negatively associated (Sarkar,&#10;  1998; Sarkar and Chang, 1997). Hommel's method is more powerful than&#10;  Hochberg's, but the difference is usually small and the Hochberg&#10;  p-values are faster to compute.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The &quot;BH&quot; (aka &quot;fdr&quot;) and &quot;BY&quot; method of Benjamini, Hochberg, and&#10;  Yekutieli control the false discovery rate, the expected proportion of&#10;  false discoveries amongst the rejected hypotheses. The false discovery&#10;  rate is a less stringent condition than the family-wise error rate, so&#10;  these methods are more powerful than the others.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Couple of questions that appeared:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;In plain words - what is the purpose of this adjustment? &lt;/li&gt;&#10;&lt;li&gt;Is it necessary to use such corrections?&lt;/li&gt;&#10;&lt;li&gt;If yes - how to choose from&#10;available options?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2013-07-16T10:54:25.420" FavoriteCount="2" Id="64484" LastActivityDate="2013-07-18T15:34:26.243" LastEditDate="2013-07-16T13:38:34.167" LastEditorUserId="919" OwnerDisplayName="radek" OwnerUserId="22" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;bonferroni&gt;" Title="p-value adjustment for Local Moran's I statistic (LISA)" ViewCount="222" />
  
  <row Body="&lt;p&gt;First, in the Roy model, $\sigma_{\varepsilon}^{2}$ is normalized&#10;to be $1$ for identification reason (c.f. Cameron and Trivedi: Microeconometrics:&#10;methods and applications). I will maintain this normalization hereafter.&#10;To answer your question, let's show&#10;$$&#10;\mathrm{{E}}\left(U_{1}\mid\varepsilon&amp;lt;Z\right)=-\sigma_{1\varepsilon}\frac{\phi\left(Z\right)}{\Phi\left(Z\right)}&#10;$$&#10;first. Here $\phi$ and $\Phi$ are the pdf and cdf of a standard&#10;normal distribution, respectively. Note that&#10;$$&#10;\mathrm{E}\left(U_{1}\mid\varepsilon&amp;lt;Z\right)=\mathrm{E}\left(\mathrm{E}\left(U_{1}\mid\varepsilon\right)\mid\varepsilon&amp;lt;Z\right)&#10;$$&#10;by the law of iterated expectation. The vector $\left(U_{1},\varepsilon\right)$&#10;is a bivariate normal with mean $\left(0,0\right)'$ and covariance&#10;matrix&#10;$$&#10;\left[\begin{array}{cc}&#10;\sigma_{1}^{2} &amp;amp; \sigma_{1\epsilon}\\&#10; &amp;amp; 1&#10;\end{array}\right].&#10;$$&#10;The conditional mean $\mathrm{{E}}\left(U_{1}\mid\varepsilon\right)=\sigma_{1\varepsilon}\varepsilon$&#10;(note that covariance not correlation arises here because $\sigma_{\varepsilon}^{2}=1$).&#10;Thus,&#10;$$&#10;\mathrm{E}\left(U_{1}\mid\varepsilon&amp;lt;Z\right)=\sigma_{1\varepsilon}\mathrm{E}\left(\varepsilon\mid\varepsilon&amp;lt;Z\right).&#10;$$&#10;The density function of $\varepsilon\mid\varepsilon&amp;lt;Z$ is&#10;$$&#10;f\left(\varepsilon\mid\varepsilon&amp;lt;Z\right)=\begin{cases}&#10;\frac{\phi\left(\varepsilon\right)}{\Phi\left(Z\right)}, &amp;amp; -\infty&amp;lt;\varepsilon&amp;lt;Z;\\&#10;0, &amp;amp; \varepsilon\geq Z.&#10;\end{cases}&#10;$$&#10;The conditional mean $\mathrm{E}\left(\varepsilon\mid\varepsilon&amp;lt;Z\right)$&#10;is&#10;\begin{eqnarray*}&#10;\mathrm{E}\left(\varepsilon\mid\varepsilon&amp;lt;Z\right) &amp;amp; = &amp;amp; \int_{-\infty}^{Z}t\frac{\phi\left(t\right)}{\Phi\left(Z\right)}\,\mathrm{{d}}t\\&#10; &amp;amp; = &amp;amp; \frac{1}{\Phi\left(Z\right)}\int_{-\infty}^{Z}t\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}t^{2}\right)\,\mathrm{{d}}t\\&#10; &amp;amp; = &amp;amp; -\frac{1}{\Phi\left(Z\right)}\int_{-\infty}^{Z}\frac{\partial}{\partial t}\left\{ \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}t^{2}\right)\right\} \,\mathrm{{d}}t\\&#10; &amp;amp; = &amp;amp; -\frac{1}{\Phi\left(Z\right)}\left(\phi\left(Z\right)-\phi\left(-\infty\right)\right).&#10;\end{eqnarray*}&#10;Note how the negative sign comes out. Thus, $\mathrm{E}\left(\varepsilon\mid\varepsilon&amp;lt;Z\right)=-\phi\left(Z\right)/\Phi\left(Z\right)$,&#10;and the conclusion follows.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-16T17:34:24.447" Id="64506" LastActivityDate="2013-07-16T17:34:24.447" OwnerUserId="27693" ParentId="60013" PostTypeId="2" Score="8" />
  
  <row AnswerCount="2" Body="&lt;p&gt;What are some ways that people know to plot a function of three variables, in a way that is relatively easy to understand on paper?&lt;/p&gt;&#10;&#10;&lt;p&gt;Although this could be generalized to pretty much any scenario, I'm specifically looking at a process where I can control pressure, power, and temperature. My output is a single number. I'd be interested to know what kind of answers people come up with.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My closest attempt:&lt;/strong&gt;&#10;One could use a regular 3D plot, where two axes represent two of the three independent variables, and the third one represents the output. Then color can be used to indicate the third independent variable. But this plot gets cluttered with information, especially when the input space is sampled well.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-16T21:05:07.643" Id="64521" LastActivityDate="2013-07-19T19:54:09.383" OwnerUserId="28107" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;" Title="Plotting functions of three variables" ViewCount="120" />
  <row Body="&lt;p&gt;I've had some success at explaining logistic regression to educated non-statisticians by drawing the logistic curve, with the y-axis as probability from 0 to 1, and explaining how the coefficient indicates that marginal change in location along the S curve, which corresponds to a given probability.  It helps people understand that the model assumes that the coefficients are only additive on the logit scale, which is quite nonlinear and uses a different interpretation than, say, a linear probability model.  For example, a large coefficient (on a dummy variable for example) means less when the intercept is very large or small than it would if the intercept is near zero.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-16T21:13:28.243" Id="64522" LastActivityDate="2013-07-16T21:13:28.243" OwnerUserId="17359" ParentId="59853" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am doing a project for share repurchase in the US. I am trying to estimate the operating performance before and after the repurchase announcement. And I am trying to adjust the performance to the industry level. The paper saying the results was estimated by the two tailed Wilcoxon test. Regarding this test, could I just test the unadjusted performance as a whole and then the adjusted performance as a whole? or do I have to put them as two different samples to compare?&#10;And how to use Stata to obtain such results? Do I have to find a code for that? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-16T21:42:33.750" Id="64523" LastActivityDate="2015-02-05T20:04:33.143" LastEditDate="2013-07-16T21:48:53.260" LastEditorUserId="22047" OwnerUserId="28108" PostTypeId="1" Score="2" Tags="&lt;dataset&gt;&lt;stata&gt;&lt;wilcoxon&gt;" Title="Share repurchase" ViewCount="105" />
  <row Body="&lt;p&gt;Statistically- there is no reason this should violate the assumptions of ordinary least squares or Spearman's correlation analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note, however, that interpretation might be slightly harder.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-16T22:05:14.510" Id="64525" LastActivityDate="2013-07-16T22:05:14.510" OwnerUserId="6961" ParentId="64524" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="64554" AnswerCount="2" Body="&lt;p&gt;This may be a simple question but I stuck in this problem. I am using Recursive Partitioning (rpart) package in R for building a classification tree. I generated a tree from a sample data (for testing rpart). I fit the sample data using rpart's formula&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   fit =  rpart(formula, data=, method=,control=) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gave me the classification tree. I can see the summary, plot the tree, plot the result. But my question is how can I use its result for prediction? I want to supply a input data to the tree and I want the algorithm to give the correct classification for the input. But I think I have nothing to do with the tree unless I can predict. I may be interpreting the result in a wrong way. Please make me clear about this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-17T02:57:46.030" FavoriteCount="1" Id="64551" LastActivityDate="2014-08-18T21:09:05.267" OwnerUserId="27579" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;rpart&gt;" Title="How to use rpart's result in prediction" ViewCount="3252" />
  <row Body="&lt;p&gt;There's no way to know how your specific violations impact your tests because different kinds of violations do different things.  Your normality violation could be a lot of different things so I can't comment.  Commonly sphericity raises the alpha rate over the nominal value.  But for your contrasts sphericity isn't an issue because there are really only two levels in each so it cannot be violated (you need at least 3 levels to even have a concept of sphericity).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can search on the web for how to do planned contrasts in SPSS.  There are even youtube videos. When you enter your contrasts correctly the variables get combined for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, if you've already run an ANOVA and have a significant effect you might be best off just describing the pattern of data.  The pattern of values means something, that's what your ANOVA effect means.  Doing contrasts afterwards, even planned ones, may not be necessary if the pattern is clear.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-17T04:35:31.247" Id="64557" LastActivityDate="2013-07-17T04:35:31.247" OwnerUserId="601" ParentId="64547" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;In addition to John's excellent points, if you want to use something else than ANOVA/t-tests (e.g. non-parametric, rank-based or permutation test…), you can always perform separate tests and use some general multiple testing adjustment (the Bonferroni-Holm procedure or techniques based on the false discovery rate don't have any particular affinity with ANOVA or the linear model and apply to any kind of test).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-17T05:57:20.087" Id="64559" LastActivityDate="2013-07-17T05:57:20.087" OwnerUserId="6029" ParentId="64547" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The Wilcoxon-Mann-Whitney 2-sample test may still work for this problem if you handle the excessive ties correctly when computing the $P$-value.  Or use the fact that the Wilcoxon test is a special case of the proportional odds ordinal logistic model, and that model's likelihood ratio $\chi^2$ test accounts for excessive ties automatically.  For a total sample size of 2,000,000, if the number of unique non-zero values exceeds around 100 the model will take a lot of computer time to run, so you might consider rounding the non-zero values a bit to have fewer intercepts in the model.  The R &lt;code&gt;rms&lt;/code&gt; package's &lt;code&gt;orm&lt;/code&gt; function will efficiently handle thousands of unique $Y$ values if the sample size were not so large.  For your case it will probably work OK with a hundred or so unique $Y$ values.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-17T11:57:23.873" Id="64583" LastActivityDate="2013-07-17T11:57:23.873" OwnerUserId="4253" ParentId="64534" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;code&gt;glmnet&lt;/code&gt; is designed around a proper accuracy score, the (penalized) deviance.  Summaries of predictive discrimination should use proper scores, not arbitrary classifications that are at odds with costs of false positives and false negatives.  Consider a couple of accepted proper scoring rules: Brier (quadratic) score and logarithmic (deviance-like) score.  You can manipulate the proportion classified correctly in a number of silly ways.  The easiest way to see this is if the prevalence of $Y=1$ is 0.98 you will be 0.98 accurate by ignoring all the data and predicting everyone to have $Y=1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way to saying all this is that by changing from an arbitrary cutoff of 0.5 to another arbitrary cutoff, different features will be selected.  An improper scoring rule is optimized by a bogus model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-17T12:35:48.867" Id="64586" LastActivityDate="2013-07-17T12:35:48.867" OwnerUserId="4253" ParentId="64581" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;I myself would always use geometric mean to estimate a lognormal median. However, in the industry world, sometimes using the sample median gives better results. The question thus is, is there a cutoff range/point starting from which the sample median can be used reliably as an estimator for the population median?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, the sample geometric mean is MLE for median, but not unbiased. An unbiased estimator would be $\hat{\beta}_{\mbox{CGM0}}=\exp(\hat{\mu}-\sigma^2/2N)$ if $\sigma$ is known. In practice, a biased corrected estimator  $\hat{\beta}_{\mbox{CGM}}$ (see below) is used since $\sigma$ is always unknown. There are papers saying that this bias-corrected geomean estimator is better because of smaller MSE and unbiasedness. However, in reality, when we only have a sample size of 4 to 6, can I argue that the bias correction makes no sense since &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Unbiasedness means the estimator is centered around the true population parameter, neither under nor over-estimate the parameter. For positively skewed distribution, the center is the median not the mean.&lt;/li&gt;&#10;&lt;li&gt;Invariant to transformation is important property in my current area(transformation between DT50 and degradation rate k, k=log(2)/DT50). You will get different results based on original data and on the transformed data.&lt;/li&gt;&#10;&lt;li&gt;For limited sample size, mean unbiasedness is potentially misleading. Bias is not error, an unbiased estimator can give bigger error. From a Bayesian point of view, the data is known and fixed, the MLE maximizes the probability of observing the data, while the bias correction is based on fixed parameters. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The sample geometric mean estimator is MLE, median-unbiased, invariant to transformations. I think it should be preferred to the bias-corrected geomean estimator. Am I right?&lt;/p&gt;&#10;&#10;&lt;p&gt;Assumming $X_1,X_2,...,X_N \sim \mbox{LN}(\mu,\sigma^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta = \exp(\mu)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\beta}_{\mbox{GM}}= \exp(\hat{\mu})= \exp{(\sum\frac{\log(X_i)}{N})} \sim \mbox{LN}(\mu,\sigma^2/N)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\beta}_{\mbox{SM}}= \mbox{median}(X_1,X_2,...,X_N) $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\beta}_{\mbox{CGM}}= \exp(\hat{\mu}-\hat\sigma^2/2N)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where, $\mu$ and $\sigma$ are the log-mean and log-sd, $\hat\mu$ and $\hat\sigma$ are the MLEs for $\mu$ and $\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;A related question: for the variance of the sample median, there is an approximate formula $\frac{1}{4Nf(m)^2}$; what is a big enough sample size to use this formula?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-07-17T12:54:23.103" FavoriteCount="1" Id="64587" LastActivityDate="2014-03-17T22:36:29.663" LastEditDate="2013-07-17T14:51:00.927" LastEditorUserId="11148" OwnerUserId="11148" PostTypeId="1" Score="5" Tags="&lt;median&gt;&lt;unbiased-estimator&gt;&lt;lognormal&gt;" Title="When to use sample median as an estimator for the median of a lognormal distribution?" ViewCount="651" />
  <row AcceptedAnswerId="64593" AnswerCount="2" Body="&lt;p&gt;I am running an experiment which measured using Likert Scales and I have 6 variables out of 24 which either have significant skewness or kurtosis. These are mixed with some positive and some negative values.&lt;/p&gt;&#10;&#10;&lt;p&gt;I transformed my data using log transformations, Square root transformations, reciprocal transformations and reverse score transformations, but this did not solve the problem. &#10;In fact, all this did was give me a greater number of significantly skewed variables (for example square root transformations left me with 11 variables significantly skewed or suffering from kurtosis).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any other type of data transformations that anyone can recommend that may help, especially when some skews are positive and others are negative? If so, does anyone know the SPSS syntax involved?&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-07-17T13:02:23.423" FavoriteCount="1" Id="64589" LastActivityDate="2013-07-17T16:51:47.787" LastEditDate="2013-07-17T14:00:51.053" LastEditorUserId="6029" OwnerUserId="27738" PostTypeId="1" Score="0" Tags="&lt;data-transformation&gt;" Title="Transformations not correcting significant skews" ViewCount="604" />
  <row AnswerCount="2" Body="&lt;p&gt;Suppose we have $n$ observations. For example, consider $n$ people who each have their blood pressure ($x_1$), pulse ($x_2$), and blood glucose ($x_3$) levels measured. So there are are $3$ explanatory variables measured for each person. The outcome variable is presence or absence of obesity ($Y$). In this case, does logistic regression assume that the data are distributed as $\text{Bernoulli}(p_i)$? For example, for the first person, we measure $x_1,x_2,x_3$ and compute $p_1$ (the probability of observing this)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-17T14:38:45.823" Id="64603" LastActivityDate="2013-07-25T12:24:41.370" LastEditDate="2013-07-25T12:24:41.370" LastEditorUserId="17230" OwnerUserId="32299" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Distribution in logistic regression" ViewCount="146" />
  
  
  
  <row Body="&lt;p&gt;Pankaj, &lt;/p&gt;&#10;&#10;&lt;p&gt;No need to subtract any seasonality.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would recommend using a regression model with NO ARIMA component. Bring in 6 day of the week dummy variables, 11 month of the year variables, holiday variables while searching for outliers, level shifts, trend. Removing variables (&quot;stepdown&quot;) that are not necessary and bringing (&quot;stepup&quot;) in dummies for the outliers I listed. &lt;/p&gt;&#10;&#10;&lt;p&gt;We recommend using 3 years of data in order for holidays to exist on weekends and weekdays so that you can get an overall read on the lead and lag relationships. You can also search for day of the month dummy variables, but this is typically only necessary for CASH demand type problems. You can find the need to specify a week of the month dummy for special cases as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can build a pivot table in Excel to compare the coefficients in the model to the % of the total for the day of the week and month of the year to get a &quot;poor man's model&quot; to confirm that the coefficients make sense.&lt;/p&gt;&#10;" CommentCount="19" CreationDate="2013-07-17T17:30:17.117" Id="64626" LastActivityDate="2013-07-17T17:37:53.077" LastEditDate="2013-07-17T17:37:53.077" LastEditorUserId="6029" OwnerUserId="3411" ParentId="64621" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In a case such as this where the number of variables exceeds the number of observations in the training dataset, I would turn to one of the penalized regression techniques, such as lasso or elastic net, which perform both variable selection and coefficient shrinkage. Logistic regression models using lasso or elastic net can be run with the glmnet R package.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good source on lasso and elastic net is Hastie, Tibshirani, and Friedman's text &quot;The Elements of Statistical Learning: Data Mining, Inference, and Prediction&quot; available online here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-17T18:55:21.410" Id="64637" LastActivityDate="2013-07-17T18:55:21.410" OwnerUserId="13634" ParentId="64271" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;My (very limited) understanding is that u (left singular vectors) measures aspects of the rows of your matrix, and v (right singular vectors) measures aspects of the columns.  So u can be used to measure similarity between rows, and v can be used to measure similarity between columns. If you multiply u and v, you get back an approximation of the original data matrix (this is how recommender engines work).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would think you could use &lt;code&gt;u&lt;/code&gt; as well as &lt;code&gt;x %*% v&lt;/code&gt; as features in a learning algorithm.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-17T19:55:28.543" Id="64643" LastActivityDate="2013-07-17T19:55:28.543" OwnerUserId="2817" ParentId="64636" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to model 10 years of monthly time series data that is very choppy, and overall it has an upward trend. At first glance it looks like a strong seasonal series, however the test results indicate that it is definitely not seasonal. This is a pricing variable that I'm trying to model as a function of macroeconomic environment, such as interest rates and yield curves. I've tried linear OLS regression (proc reg), but I don't get a very good model with that. I've also tried autoregressive error models (proc autoreg), but it captures 7 lags of the error term as significant factors. I don't really want to include that many lag of the error term in the model. In addition, most of the macroeconomic variables become insignificant when I include all these error lags in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions on modeling method/technique that could help me model this choppy data is really appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-17T21:22:36.460" Id="64653" LastActivityDate="2015-02-03T20:54:44.210" LastEditDate="2013-07-18T12:34:03.863" LastEditorUserId="6029" OwnerUserId="28151" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;" Title="Time series modeling of choppy data" ViewCount="169" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the Kaplan-Meier estimate for the survival function which I obtained using R's survival  package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data.surv &amp;lt;- Surv(data$days_to_event, data$event_flag)&#10;data.fit &amp;lt;- survfit(data.surv ~ 1, data = data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The data are censored at 90 days after the initial signal so the survival function has 90 points and its value at the last point is equal to the ratio of the total number of non-events at 90 days after the signal to total number of observations in the data set. This value is far from 0 since the signal causes the event to happen rather infrequently. Is that to be expected?&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I have another data set where each observation has had the signal in the past (within 90 days) but no event so far. What is the proper way to apply the survival function estimate to the data to get expected number of events for some horizon? I thought $1-S(t_{current} + t_{horizon})/S(t_{current})$ for each observation is supposed to give me the probability, then adding them up would give the total count of the events expected from now to the horizon but  even applying this to the original data set gives result which is way too low. It feels like there must be some mistake but I cannot find it. Could it be because of censoring (the majority of observations is censored - if the event does not occur within 90 days, the signal is thought to have lost its ability to cause the event)? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-17T21:47:03.007" Id="64656" LastActivityDate="2013-07-18T03:53:52.490" LastEditDate="2013-07-18T03:53:52.490" LastEditorUserId="8718" OwnerUserId="8718" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;survival&gt;" Title="Applying Kaplan-Meier survival function estimate to get expected number of events" ViewCount="297" />
  <row AcceptedAnswerId="64737" AnswerCount="1" Body="&lt;p&gt;I have an experiment with two factors. The first factor, picture, has three levels: nice, ugly, absent (i.e., the picture is not presented). The second factor, information, has two levels: present, absent. The dependent variable is a continuous variable: liking.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I did not run the picture=absent + information=absent condition because it would mean not presenting anything and asking people how much they liked it, which doesn't make a lot of sense. So I'm left with 5 cells instead of 6. Any ideas on how to analyse these data?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-17T22:21:54.853" Id="64660" LastActivityDate="2013-07-18T17:14:34.413" LastEditDate="2013-07-17T22:39:52.140" LastEditorUserId="7290" OwnerUserId="28153" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;&lt;experiment-design&gt;" Title="How to analyze data from a 2x3 experiment where one cell is missing?" ViewCount="70" />
  <row Body="The scientific study of the principles of heredity and the variation of inherited traits among related organisms." CommentCount="0" CreationDate="2013-07-18T00:47:58.853" Id="64665" LastActivityDate="2013-07-18T01:07:13.253" LastEditDate="2013-07-18T01:07:13.253" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="4" Score="0" />
  
  
  
  <row AcceptedAnswerId="64741" AnswerCount="1" Body="&lt;p&gt;I just fit a model in lme4, and I'm wondering what the heck I fit...&lt;/p&gt;&#10;&#10;&lt;p&gt;I have individuals &lt;code&gt;id&lt;/code&gt;, and each is measured pass/fail on items that can be described using two factors, &lt;code&gt;f1&lt;/code&gt; and &lt;code&gt;f2&lt;/code&gt;. My theory says that &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;f2&lt;/code&gt; can interact. So I want to compare these models:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# f2 doesn't matter&#10;m1 &amp;lt;- lmer(pass ~ f1 + (1|id), family=&quot;binomial&quot;)&#10;&#10;# f2 is a fixed effect, doesn't interact with individuals&#10;m2 &amp;lt;- lmer(pass ~ f1 + f2 + (1|id), family=&quot;binomial&quot;)&#10;&#10;# f2 is a nested random effect, interacting with individuals&#10;m3 &amp;lt;- lmer(pass ~ f1 + (1|f2/id), family=&quot;binomial&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;First, am I right that there's a random effect of each level of f2, and each individual's level of f2 is partially pooled, depending on the number of observations for that individual?&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, does it make sense to fit &lt;code&gt;f2&lt;/code&gt; as a fixed effect with random individuals nested within each level of &lt;code&gt;f2&lt;/code&gt;? How does one write that?&lt;/p&gt;&#10;&#10;&lt;p&gt;Third, what's this, with f2 and id reversed in the formula? I typed it by accident, but now I want to understand it.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m3weird &amp;lt;- lmer(pass ~ f1 + (1|id/f2), family=&quot;binomial&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-07-18T08:54:55.313" FavoriteCount="1" Id="64683" LastActivityDate="2013-07-18T17:44:49.527" LastEditDate="2013-07-18T10:24:16.773" LastEditorUserId="8207" OwnerUserId="8207" PostTypeId="1" Score="1" Tags="&lt;glmm&gt;&lt;lme4&gt;" Title="Order of nested random effects in lme4" ViewCount="2336" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose that there is a field where cabbages grow. We have weighed all the cabbages in spring and got Gaussian distribution of weights. The same we did in autumn and got another Gaussian distribution. The question is how to get distribution of growth rates (times over the summer) if cabbages are not numbered? Will it be Gaussian or not?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-18T09:36:01.457" Id="64687" LastActivityDate="2013-09-16T11:18:24.353" LastEditDate="2013-07-18T09:46:47.533" LastEditorUserId="22047" OwnerUserId="28175" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;mathematical-statistics&gt;" Title="Indirect measurement of the distributions" ViewCount="70" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am researching cluster analysis, and I am interested in variables that are both categorical and continuous, for which I have read that a Gower's similarity coefficient is a good proximity measure. I am interested in first using an average linkage algorithm, and have found that some have recommended looking for the 'elbow' in the sum of squared error (SSE) scree plot as a guideline for deciding how many clusters to retain. I was wondering if the Gower's similarity coefficient (being non-metric and non-Euclidean) would allow me to create an SSE scree plot, or if that didn't make sense statistically.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-18T11:55:09.530" Id="64694" LastActivityDate="2013-09-17T14:23:27.127" OwnerUserId="28158" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;" Title="Determining number of clusters with SSE scree plot with Gower's coefficient of similarity" ViewCount="321" />
  
  
  
  <row Body="&lt;p&gt;The &quot;mean&quot; &lt;code&gt;m&lt;/code&gt; is actually your prediction. The variance &lt;code&gt;s&lt;/code&gt; can be interpreted as the uncertainty of your prediction.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-18T15:06:07.863" Id="64718" LastActivityDate="2013-07-18T15:06:07.863" OwnerUserId="16137" ParentId="23767" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have 6 variables, three of which are median splits and three of which are continuous measures of 'such and such', each of which is measured on the same 5 point scale.  I am trying to produce a graph that would plot the median splits of the three variables on the X axis against a mean score of the Y axis.  In essence, I am trying to produce three summary graphs, instead of nine.&lt;br&gt;&#10;That is, instead of plotting each median split variable against the Y axis, I would like to plot the median splits of all three variables against the mean of the variable on the Y axis.   Is there a way to do this?  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-18T17:15:30.880" Id="64738" LastActivityDate="2013-07-18T19:22:40.767" OwnerUserId="16204" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;data-visualization&gt;&lt;spss&gt;" Title="Multiple variable comparison line graph SPSS" ViewCount="1529" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I read about using scoring rules to evaluate the performance of predictive models. In the Wikipedia article about the &lt;a href=&quot;http://en.wikipedia.org/wiki/Brier_score&quot; rel=&quot;nofollow&quot;&gt;Brier score&lt;/a&gt;, it is stated:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The Brier score is appropriate for binary and categorical outcomes&#10;  that can be structured as true or false, but is inappropriate for&#10;  ordinal variables which can take on three or more values (this is&#10;  because the Brier score assumes that all possible outcomes are&#10;  equivalently &quot;distant&quot; from one another).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What are some examples of scoring rules used for evaluating predictions of ordinal variables? For example, if the set of possible outcomes is $\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}$ and a predictive model yields $p_6 = 0.3$, $p_7 = 0.5$, $p_8 = 0.2$ (where $p_i$ is the believed probability that the outcome will be $i$), this predictive model should receive a lower score if the outcome is $3$ than if it is $4$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-18T22:40:27.250" Id="64774" LastActivityDate="2013-07-20T12:46:30.030" LastEditDate="2013-07-19T23:14:39.650" LastEditorUserId="27395" OwnerUserId="27395" PostTypeId="1" Score="1" Tags="&lt;predictive-models&gt;&lt;scoring&gt;" Title="Scoring predictions of an ordinal variable" ViewCount="142" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a classifier, and I am using leave one out cross-validation to assess its performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;On each iteration, I divide the dataset into training and testing sets. The testing set is just the subject that I am going to evaluate (leave one out).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I &lt;strong&gt;divide the training set into folds,&lt;/strong&gt; and I do feature selection like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;I run my &lt;strong&gt;filter&lt;/strong&gt; feature selection algorithm on every fold. When I am done, I have a voting algorithm to obtain the final set with the features that were selected in each  fold.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that this procedure is adequate when you have a small sample like in my case (subjects = 30, features = 960).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is why, if at all, would it be a bad idea to do feature selection on the whole training set instead of dividing it into folds?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-07-18T23:11:45.960" Id="64779" LastActivityDate="2014-03-24T04:58:36.977" LastEditDate="2014-03-24T04:58:36.977" LastEditorUserId="32036" OwnerUserId="13432" PostTypeId="1" Score="3" Tags="&lt;cross-validation&gt;&lt;feature-selection&gt;&lt;train&gt;" Title="Feature selection in the training set" ViewCount="217" />
  
  
  <row Body="&lt;p&gt;I have traced the answer in new Stata 13 documentation on&#10;&lt;a href=&quot;http://www.stata.com/stata13/intraclass-correlation-coefficients/&quot; rel=&quot;nofollow&quot;&gt;ICC&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question remains on whether the F test in this case can be used given the data does not follow the assumptions of normal distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-19T06:45:02.360" Id="64792" LastActivityDate="2013-07-19T08:28:54.050" LastEditDate="2013-07-19T08:28:54.050" LastEditorUserId="22047" OwnerUserId="26267" ParentId="64725" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;So I have a data set like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Email                  Posted           Received                            Platform&#10;ss104@gmail.com 3/20/2012 1:11  Tue, 20 Mar 2012 01:07:05 -0700 (PDT)   Wikipedia&#10;ss104@gmail.com 3/20/2012 1:11  Tue, 20 Mar 2012 01:07:22 -0700 (PDT)   Mailing list&#10;ss104@gmail.com 3/20/2012 1:11  Tue, 20 Mar 2012 09:08:27 -0700 (PDT)   Twitter&#10;ss104@gmail.com 3/20/2012 1:11  Tue, 20 Mar 2012 12:22:46 -0700 (PDT)   Wikipedia&#10;ss104@gmail.com 3/20/2012 1:11  Tue, 20 Mar 2012 17:45:51 -0700 (PDT)   Mailing list&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What this means is that a certain email address was posted at a certain time on a certain platform. It received an email at the time under &lt;code&gt;Received&lt;/code&gt; column.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to find is which platform sends the most amount of emails per week. (I'll be using python and matplotlib for visualizing). CPD (cumulative probability distribution) for this data is a good start.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure how to start with this. Do I just count the occurrence of each platform, and divide it by the summation of difference in times (&lt;code&gt;Received - Posted&lt;/code&gt; in weeks) for each email of that platform?&lt;/p&gt;&#10;&#10;&lt;p&gt;And then I could also make histogram or box plot for the data, but I'm not what actually to compute.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone give me pointers?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-19T07:29:01.037" Id="64797" LastActivityDate="2013-07-19T07:29:01.037" OwnerUserId="28199" PostTypeId="1" Score="0" Tags="&lt;data-visualization&gt;&lt;python&gt;&lt;histogram&gt;&lt;boxplot&gt;&lt;matplotlib&gt;" Title="Need to find cumulative probability distribution given this data set" ViewCount="168" />
  <row Body="&lt;p&gt;SSE is the measure optimized by k-means.&lt;/p&gt;&#10;&#10;&lt;p&gt;It doesn't make much sense for any other algorithm than k-means. And even there it suffers from the fact that increasing k will decrease SSE, so you can mostly look at which point further increasing k stops yielding a substantial increase in SSE - that is essentially the vague &quot;elbow method&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There exist other criteria such as Silhouette, Davies-Bouldin index, BIC, AIC that can be used to get an &quot;alternative view&quot; of what is actually optimal.&lt;/p&gt;&#10;&#10;&lt;p&gt;But in the end, that is just a mathematical heuristic. It may not work for real data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-19T08:46:40.263" Id="64804" LastActivityDate="2013-07-19T09:04:11.180" LastEditDate="2013-07-19T09:04:11.180" LastEditorUserId="22047" OwnerUserId="7828" ParentId="64694" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="64997" AnswerCount="1" Body="&lt;p&gt;Much has been written on the ICC and Kappa, but there seems to be disagreement on the best measures to consider.&lt;/p&gt;&#10;&#10;&lt;p&gt;My purpose is to identify some measure which shows whether there was agreement between respondents an interviewee administered questionnaire.  17 people gave ratings of 0-5 to a defined list of items, rating them according to importance (NOT ranking).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not interested in whether the 17 participants all rated exactly the same, but only whether there is agreement that it should be rated high or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following suggestions here, I have used the ICC and also Kappa but different results were produced as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/PMsDA.jpg&quot; alt=&quot;Kappa results&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rd6z2.jpg&quot; alt=&quot;ICC results&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I note that given the very small sample, the validity of the ICC could be questionable due to the use of the f test &lt;a href=&quot;http://stats.stackexchange.com/questions/41898/intraclass-correlation-coefficient-non-parametric-data&quot;&gt;see this question&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What are your suggestions, and way forward to comment on this&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-07-19T14:01:10.027" FavoriteCount="1" Id="64829" LastActivityDate="2013-07-21T03:40:55.890" LastEditDate="2013-07-21T03:40:55.890" LastEditorUserId="6029" OwnerUserId="26267" PostTypeId="1" Score="2" Tags="&lt;interpretation&gt;&lt;small-sample&gt;&lt;inter-rater&gt;&lt;intraclass-correlation&gt;&lt;kappa&gt;" Title="ICC and Kappa totally disagree" ViewCount="1925" />
  <row AcceptedAnswerId="66912" AnswerCount="1" Body="&lt;p&gt;I'm trying to find statistical test (or procedure) which is able to &lt;strong&gt;discover patterns in spatial distribution of points.&lt;/strong&gt; I sketch the problem by giving the example about position of bird territories and personality of birds.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;&#10;Load this example data (&lt;em&gt;they are completely fabricated&lt;/em&gt;). Data consists of geographical coordinates of territories and the level of aggression for each bird.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example data:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x_coor&amp;lt;-rep(c(2,2.5,3,3.5,4,17,17.5,18,18.5,19,19.5,20,20.5,21,21.5,22,17.5,&#10;18,18.5,19,31.5,32,32.5,33,33.5,1,1.5,2,3,4,5,16,17,18,19,20,21,22,23,24,25,&#10;26,16,16.5,17,18,19,20,27.5,28.5,29.5,30.5,31,31.5,32,33,34,2,3,4,5,6,7,8,9,&#10;12,13,14,15,16,18,20,21,22,23,24,25,26,28,29,30,31,32,33,34,1,7,8,10,14,22,&#10;26,28,33),c(5,5,5,3,2,1,2,3,3,3,2,3,2,2,1,1,1,4,4,3,2,3,3,8,4,8,1,4,4,4,4,3,&#10;2,2,2,2,2,2,3,3,3,1,1,1,3,5,5,7,1,2,3,3,3,1,5,5,5,4,1,2,2,3,2,2,3,1,2,4,3,3,&#10;2,3,1,5,2,4,1,2,3,2,1,1,1,1,1,1,1,1,1,1,1,1,1,2))&#10;y_coor&amp;lt;-c(15,15.5,16,16.5,17,16,16.5,17,17.5,18,16,17,17.5,18,18.5,17.5,18,&#10;18.5,18,18.5,22,22,22.5,21.5,22,22.5,21.5,22,22.5,21.5,22,22.5,21.5,22,21.5, &#10;22,22.5,22,22.5,22,22.5,22.5,22.5,7,6,6.5,7,7.5,5.5,6,6.5,7,5,5.5,6,16,16.5,&#10;15,15.5,16,14,14.5,15,11,11.5,12,12.5,13,13.5,14,14.5,11.5,12,12.5,13,10,11,&#10;12,13,14,15,16,17,18,13,14,19,20,14,15,20,21,15.5,16.5,20,21,17,18,19,20,21,&#10;22,23,21,24,20,24,20,23.5,20,23.5,21,23.5,21,23.5,22,23,24,22,23,24,22,23,24,&#10;23,7,6,4,5,8,2,3,4,5,8.5,1,2,3,4,8,1,2,3,4,5,6,7,16,16,17,16,17,18,16,17,18,&#10;10,14,15,17.5,9,10,11,12,13,8,9,10,16,17,8,9,10,14,15,6,8,23,25,11,9,23,13,&#10;25,11,15,22,18,20,14,23,16,18,20,6,3,9,1,6,22,24,4,9,19,2,12,26,11,18,10,18,&#10;26,12,2,4,6,8,26,10,19,2,4,6,8,16,20,26,14,22,25,9,20,12,7,20,6,18,1,2,8,26,&#10;15,15,10,4,1,25)&#10;aggr_low&amp;lt;-1:35&#10;aggr_mid&amp;lt;-25:65&#10;aggr_big&amp;lt;-50:75&#10;aggr_max&amp;lt;-70:100&#10;aggression&amp;lt;-c(sample(aggr_low,75,replace=TRUE),sample(aggr_mid,100,&#10;replace=TRUE),sample(aggr_big,62,replace=TRUE),sample(aggr_max,10,replace=TRUE))&#10;my.data &amp;lt;- data.frame(x_coor, y_coor, aggression)&#10;rbPal &amp;lt;- colorRampPalette(c(&quot;blue&quot;,&quot;red&quot;))&#10;my.data$Col &amp;lt;- rbPal(10)[as.numeric(cut(my.data$aggression,breaks = 10))]&#10;par(mfrow=c(1,2))&#10;plot(x_coor,y_coor,pch = 20,col = my.data$Col)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;&#10;As you can see in the plot, the territories of low aggressive birds (&lt;em&gt;blue&lt;/em&gt;) have tendency to cumulate together, whereas aggressive birds are solitary (&lt;em&gt;red&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I spend a long time by searching for statistical method which could confirm such pattern (in this case clearly visible).&lt;/p&gt;&#10;&#10;&lt;p&gt;My first try was to correlate distance matrices of aggression of birds and spatial position of territories. Then applying &lt;code&gt;mantel.test&lt;/code&gt; for similarity of these two matrices.&#10;This however did not work because &lt;code&gt;mantel.test&lt;/code&gt; is &lt;strong&gt;unable to absorb the information about spatial isolation of clusters&lt;/strong&gt; of low aggressive territories. It expect, that if low aggressive birds are together, they will be together all in one big cluster (not in four different clusters as visible from the plot). So &lt;code&gt;mantel.test&lt;/code&gt; is unable to confirm or disprove the hypothesis about the influence of aggression on spatial distribution of territories.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;&#10;I have notified that four cluster do not know about each other, thanks to long distances among them, so I try different approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;I made again the distance matrix of all territories...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;geo.dist&amp;lt;-dist(cbind(x_coor, y_coor))&#10;geo.dist&amp;lt;-as.matrix(geo.dist)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...but I keep &lt;strong&gt;only distances of nearest neighbours by setting distance threshold&lt;/strong&gt; (manually&#10;to be the 3 distance units)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;geo.dist[geo.dist &amp;gt; 3] &amp;lt;- NA&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This creates the &lt;em&gt;imaginary circles&lt;/em&gt; around each territory and I was able to count the number of neighbours for each one of them.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;no.neighbours&amp;lt;-numeric(ncol(geo.dist))&#10;for (i in 1:ncol(geo.dist))&#10;no.neighbours[i]&amp;lt;-sum(!is.na(geo.dist[ ,i]))-1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And as you can see form this plot...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(aggression,no.neighbours,col = my.data$Col, pch=20)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There is &lt;strong&gt;clear negative correlation&lt;/strong&gt; between number of neighbours and aggression.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cor.test(aggression, no.neighbours)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is it correct to use this steps to confirm (or falsify) this hypothesis? (&quot;Agression of birds influence the position of territories&quot;)&#10;Does anybody know some more appropriate solution or the name of commonly used test for such problems?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-19T14:26:22.773" FavoriteCount="2" Id="64834" LastActivityDate="2013-08-11T10:36:56.577" LastEditDate="2013-08-11T10:35:14.400" LastEditorUserId="28218" OwnerUserId="28218" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;correlation&gt;&lt;spatial&gt;" Title="How to test the effect of variable on spatial distribution of points in R" ViewCount="140" />
  
  <row AcceptedAnswerId="64839" AnswerCount="1" Body="&lt;p&gt;Could you guys give me a review on the challenges and common techniques in dealing with big data problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, what I know is that in when sample size is limited and dimension is high, the estimation of covariance would be an issue, what we did is to suppressing the big eigenvalues and increasing the small eigenvalues.&lt;/p&gt;&#10;" ClosedDate="2013-07-19T17:11:50.027" CommentCount="3" CreationDate="2013-07-19T14:37:40.143" Id="64836" LastActivityDate="2013-07-19T14:48:24.060" OwnerUserId="25074" PostTypeId="1" Score="-6" Tags="&lt;machine-learning&gt;&lt;mathematical-statistics&gt;&lt;data-mining&gt;&lt;big-data&gt;" Title="what's the challenge in big data analysis?" ViewCount="118" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to use supervised machine learning method to do binary classfication, so a training set is needed. I already have the positive examples.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is that all my negative examples are randomly selected from a very large dataset which may contain only a few positive examples.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this approach right to construct the training set? Is any approach to evaluate the training set, especially negative examples?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-19T14:51:46.173" Id="64840" LastActivityDate="2013-08-18T21:07:48.453" OwnerUserId="22062" PostTypeId="1" Score="0" Tags="&lt;supervised-learning&gt;" Title="How to choose negative examples to build a training set?" ViewCount="53" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm fitting a linear regression model in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;head(data)  &#10;   `x1`         `x2`     `y`  &#10;    4.9711     32.70   0.2810632  &#10;    6.756072   31.60   0.3115225  &#10;    5.895213   56.50   0.4853171  &#10;    1.951329   29.80   0.3010127  &#10;&#10;EDIT: fit &amp;lt;- glm(y ~ x1 + x2,family = binomial(link = logit), data = data, control= list(epsilon = 0.0001, maxit = 50, trace = F)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;y&lt;/code&gt; is a proportion.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit.l.hat=predict.glm(fit,se.fit=TRUE,type=&quot;response&quot;)    &#10;ci=c(fit.l.hat$fit - 1.96*fit.l.hat$se.fit, fit.l.hat$fit + 1.96*fit.l.hat%se.fit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For the first fitted value, &lt;code&gt;x1 = 4.97&lt;/code&gt; and &lt;code&gt;x2 = 32.7&lt;/code&gt;.&#10;So from my above code, I get this output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fitted value: 0.35, Lower 95%: 0.01, Upper 95%: 0.68, Standard Error: 0.17&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is how to actually interpret this for &quot;dummies&quot;. &#10;I would say &quot;For an input of 4.97 and 32.7, 95 out of 100 samples will conclude a proportion between 0.01 and 0.68.&quot; Is that correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;What other information can I get out of this, or what other &quot;dummy&quot; ways are there to present this information? And more importantly, &lt;strong&gt;how do I quantify the error?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-19T16:52:00.257" FavoriteCount="1" Id="64861" LastActivityDate="2013-07-22T12:14:07.303" LastEditDate="2013-07-22T12:14:07.303" LastEditorUserId="25932" OwnerUserId="25932" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;confidence-interval&gt;&lt;generalized-linear-model&gt;&lt;error&gt;" Title="Sum up uncertainty from confidence intervals in R" ViewCount="249" />
  
  <row Body="Commercial statistical package for general statistical analysis and econometric analyses." CommentCount="0" CreationDate="2013-07-19T18:16:37.430" Id="64871" LastActivityDate="2013-07-19T18:18:53.933" LastEditDate="2013-07-19T18:18:53.933" LastEditorUserId="27403" OwnerUserId="27403" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.graphpad.com/prism&quot; rel=&quot;nofollow&quot;&gt;GraphPad Prism&lt;/a&gt; can easily make this kind of graph, plotting error bars from error values you enter. Create a grouped table formatted for entery of mean, - error and + error. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-19T19:40:45.807" Id="64885" LastActivityDate="2013-07-19T19:40:45.807" OwnerUserId="25" ParentId="64880" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Yes, they're the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Set intersections are commutative, so:&#10;$$A\cap B\cap C = C \cap B \cap A$$&#10;And you already know that:&#10;$$&#10;P(ABC) = P(A|BC)*P(B|C)*P(C)&#10;$$&#10;So:&#10;$$&#10;\begin{align}&#10;P(A|BC)*P(B|C)*P(C) &amp;amp; = P(ABC)\\&#10;&amp;amp; = P(CBA)\\&#10;&amp;amp; = P(C|BA)*P(B|A)*P(A)&#10;\end{align}&#10;$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-19T21:35:14.380" Id="64897" LastActivityDate="2013-07-19T21:35:14.380" OwnerUserId="24080" ParentId="64892" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I am writing a paper and the following is a code that I wrote in R. The reason that I am struggling with this is because I tried hundreds of models with different variables and the following model below had the best model with the least MAPE. I added and multiplied some variables and I am not sure how my model should be expressed in a mathematical equation and would appreciate any help. Thank you.&lt;/p&gt;&#10;&#10;&lt;p&gt;FYI, B_1 is a categorical variable and the rest are numerical variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model = glm(as.formula(y ~ (B_1+B_2)*B_3*B_4*B_5), data = train.set, family=&quot;Gamma&quot;) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-07-19T22:28:57.733" Id="64902" LastActivityDate="2013-07-24T02:36:41.143" LastEditDate="2013-07-24T02:36:41.143" LastEditorUserId="919" OwnerUserId="16934" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;interaction&gt;" Title="How to write a mathematical equation for GLM model with gamma and gaussian distribution?" ViewCount="235" />
  <row Body="&lt;p&gt;The definition of monotonic missing is that, once the subject dropped out he will drop out forever, while for non-monotonic missing the subject may come back or be missing again.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if we follow one subject for five years and he dropped out in the third year, monotonic missing is like &lt;code&gt;o o m m m&lt;/code&gt;, and one kind of non-monotonic missing can be &lt;code&gt;o o m o m&lt;/code&gt;, where &lt;code&gt;o&lt;/code&gt; indicates observed, &lt;code&gt;m&lt;/code&gt; indicates missing.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the third &lt;code&gt;o&lt;/code&gt; in the non-monotonic missing is like an &lt;code&gt;island&lt;/code&gt;. This is just to classify the pattern of missing, and generally the monotonic missing is easier to handle.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-19T23:14:04.547" Id="64907" LastActivityDate="2013-07-19T23:14:04.547" OwnerUserId="21599" ParentId="56506" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="64915" AnswerCount="1" Body="&lt;p&gt;I'm working on a project to compare different approaches to time series modeling. In the model selection process, we perform residual analysis for the fitted models. For regression, we need to check the normality assumption in the residuals by performing Shapiro-Walk's test, or plotting QQ-plot, etc. Do we also need to check whether the residuals are normally distributed after fitting a model using HW algorithm?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-19T23:52:22.767" Id="64911" LastActivityDate="2013-07-20T01:08:04.917" OwnerUserId="28241" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;model-selection&gt;&lt;smoothing&gt;&lt;exponential-smoothing&gt;" Title="Does the Holt-Winters algorithm for exponential smoothing in time series modelling require the normality assumption in residuals?" ViewCount="658" />
  
  <row Body="&lt;p&gt;Very little can be said in general terms about this sort of situation.  We certainly can't conclude that this is the best model and just submit it.  There may be other variables that are better than this one, or that complement it well by explaining the remaining randomness.  To answer the question &quot;what should I do here&quot; is basically to consider the whole world of modelling strategies.  You might want to consider a book such as Frank Harrell's on &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387952322&quot; rel=&quot;nofollow&quot;&gt;Regression Modeling Strategies&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-20T02:11:26.357" Id="64919" LastActivityDate="2013-07-20T02:11:26.357" OwnerUserId="7972" ParentId="63938" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a pre-test, post-test, non-equivalent groups experiment, with a sample of 62 (control and experimental groups have 31 each). My objective is to compare the effectiveness of a model of teaching for developing vocabulary among pupils with different learning styles. The learning styles of  the 31 of the experimental group are: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Visual = 5, &lt;/li&gt;&#10;&lt;li&gt;Auditory = 7, &lt;/li&gt;&#10;&lt;li&gt;Reading = 12, &lt;/li&gt;&#10;&lt;li&gt;Kinaesthetic = 1, &lt;/li&gt;&#10;&lt;li&gt;VA = 3, &lt;/li&gt;&#10;&lt;li&gt;VR = 1, &lt;/li&gt;&#10;&lt;li&gt;AR = 2. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What statistics should I employ to compare their performance (scores on vocabulary test)? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-20T04:27:37.010" Id="64926" LastActivityDate="2013-07-20T22:32:07.977" LastEditDate="2013-07-20T22:32:07.977" LastEditorUserId="7290" OwnerUserId="28245" PostTypeId="1" Score="2" Tags="&lt;small-sample&gt;" Title="Statistics for small and unequal samples" ViewCount="94" />
  
  
  <row Body="A measure of concordance between two random variables based on ranks." CommentCount="0" CreationDate="2013-07-20T11:57:38.280" Id="64946" LastActivityDate="2013-07-20T13:28:59.080" LastEditDate="2013-07-20T13:28:59.080" LastEditorUserId="27403" OwnerUserId="27403" PostTypeId="4" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let's assume I don't know of the existence of clocks, so I want to build a model to predict the current time of day based on a large amount of other things I can measure, for example pressure, temperature, noise, light, and so on. Some of these variables may be directly correlated with time, some may show some sort of peak shape and some may just be noise. There are about 10,000 variables in total.&lt;/p&gt;&#10;&#10;&lt;p&gt;So as my model input I have a big matrix that gives me the measurements for each of the 10,000 variables at each of the 24 hour of the day.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be the best way to go about building a model for this? Should I try to find a small set of variables for which I can build a linear model? Should I do a PCA and build a linear model on the first few principal components? Something else?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-20T12:31:11.843" FavoriteCount="1" Id="64953" LastActivityDate="2013-07-20T12:48:17.037" LastEditDate="2013-07-20T12:48:17.037" LastEditorUserId="22047" OwnerUserId="3031" PostTypeId="1" Score="1" Tags="&lt;model&gt;&lt;learning&gt;" Title="Best model for many independent variables" ViewCount="66" />
  
  
  
  
  
  
  <row Body="Fisher's z-transform is a transformation that is applied to the correlation coefficient, r, for use in hypothesis testing &amp; constructing confidence intervals." CommentCount="0" CreationDate="2013-07-21T02:08:41.553" Id="64995" LastActivityDate="2013-07-21T03:49:42.680" LastEditDate="2013-07-21T03:49:42.680" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="4" Score="0" />
  <row AcceptedAnswerId="65010" AnswerCount="1" Body="&lt;p&gt;This question comes from:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/64433/how-to-simulate-a-cox-proportional-hazards-model-with-change-point-and-code-it-i&quot;&gt;How to simulate a Cox proportional hazards model with change point and code it in R&lt;/a&gt; (See answer)&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to generate a censoring variable $C=Exponential(\theta)$ that create a desired percentage of censorship on $Y$, like 33%, 55%, etc, by choosing an appropriate value for $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to include this censoring variable in the code I did for the previous question, I added this to the dataframe generator:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cen &amp;lt;- rexp(n,theta) # Censoring variable.&#10;&#10;ycen &amp;lt;- pmin(Y, cen) # Censoring effect.&#10;di &amp;lt;- as.numeric(Y &amp;lt;= cen)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;if (surv.df) data.frame(Y,X,ycen,di) else cbind(Y,X,ycen,di)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To do something like this (in the context of the aforementioned simulation):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;coxph(Surv(ycen, di)~X)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which $\theta$ value should I choose?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-21T12:18:42.027" Id="65005" LastActivityDate="2013-07-22T06:06:59.963" OwnerUserId="12486" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;optimization&gt;&lt;censoring&gt;&lt;cox-model&gt;" Title="Get a desired percentage of censored observations in a simulation of Cox PH Model" ViewCount="921" />
  
  <row Body="&lt;p&gt;I am not sure if this answers your question but please find below some R code which follows Bender et al. (2005). They describe an approach to simulate a Cox PH regression model with given properties like the proportion of censored events (see line &quot;&lt;code&gt;dat &amp;lt;- data.frame(T = T, X, event = rbinom(n, 1, 0.30))&lt;/code&gt;&quot;, i.e. 70% of all events are censored). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Bender, Ralf, Thomas Augustin, und Maria Blettner. 2005. Generating survival times to simulate Cox proportional hazards models. Statistics in Medicine 24: 1713–1723.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;##' Generate survival data with $p$ (correlated) predictors&#10;##'&#10;##' &#10;##'&#10;##' @title Generate survival data&#10;##' @param n Sample size&#10;##' @param beta Vector of coefficients&#10;##' @param r Correlation between predictors&#10;##' @param id.iter &#10;##' @param id.study&#10;##' @return matrix with identification variables id.iter and id.study,&#10;##'         T (survival time), event (0: censored),&#10;##'         predictors X1 to X$p$&#10;##' @author Bernd Weiss&#10;##' @references Bender et al. (2005)&#10;genSurvData &amp;lt;- function(n = 100000,&#10;                        beta = c(0.8, 2.2, -0.5, 1.1, -1.4),&#10;                        r = 0.1,&#10;                        id.iter = NA,&#10;                        id.study = NA){&#10;&#10;    ## Scale parameter (the smaller lambda, the greater becomes T)&#10;    lambda &amp;lt;- 0.000001#1.7&#10;&#10;    ## Shape parameter&#10;    nue &amp;lt;- 8.9#9.4&#10;&#10;    ## Sample size&#10;    n &amp;lt;- n&#10;&#10;    ## Number of predictors&#10;    p &amp;lt;- length(beta)&#10;&#10;    ## Generate column vector of coefficients&#10;    beta &amp;lt;- matrix(beta, ncol = 1)&#10;&#10;    ## Generate correlated covariate vectors using a multivariate normal&#10;    ## distribution with X ~ N(mu, S) and a given correlation matrix R, with:&#10;    ## R: A p x p correlation matrix&#10;    ## mu: Vector of means&#10;    ## SD: Vector of standard deviations&#10;    ## S: Variance-covariance matrix&#10;    R &amp;lt;- matrix(c(rep(r, p^2)), ncol = p)&#10;    diag(R) &amp;lt;- 1&#10;    R&#10;    mu &amp;lt;- rep(0, p)&#10;    SD &amp;lt;- rep(1, p)&#10;    S &amp;lt;- R * (SD %*% t(SD))&#10;    X &amp;lt;- mvrnorm(n, mu, S)&#10;    cov(X)&#10;    cor(X)&#10;    sqrt(diag(cov(X)))&#10;&#10;    ## Calculate survival times&#10;    T &amp;lt;- (-log(runif(n)) / (lambda * exp(X %*% beta)))^(1/nue)&#10;&#10;    ## 30% (0.30) of all marriages are getting divorced, i.e. 70% of all&#10;    ## observations are censored (&quot;event = rbinom(n, 1, 0.30)&quot;)&#10;    dat &amp;lt;- data.frame(T = T, X, event = rbinom(n, 1, 0.30))&#10;    ## Also, all T's &amp;gt; 30 yrs are by definition censored and T is set to 30 yrs&#10;    dat$event &amp;lt;- ifelse(dat$T &amp;gt;= 30, 0, dat$event)&#10;        dat$T &amp;lt;- ifelse(dat$T &amp;gt;= 30, 30, dat$T)&#10;&#10;    dat$id.iter &amp;lt;- id.iter&#10;    dat$id.study &amp;lt;- id.study&#10;&#10;    ## Reorder data frame: T, event, covariates&#10;    tmp.names &amp;lt;- names(dat)&#10;    dat &amp;lt;- dat[, c(&quot;id.iter&quot;, &quot;id.study&quot;, &quot;T&quot;, &quot;event&quot;, tmp.names[grep(&quot;X&quot;, tmp.names)])]&#10;&#10;    ## Returning a matrix speeds-up things a lot... lesson learned.&#10;    dat &amp;lt;- as.matrix(dat)&#10;    return(dat)&#10;}&#10;&#10;&#10;library(survival)&#10;library(MASS)&#10;&#10;dat &amp;lt;- genSurvData(n = 1000)&#10;dat &amp;lt;- as.data.frame(dat)&#10;&#10;survfit(Surv(time = T, event = event) ~ 1, data = dat)&#10;&#10;coxph(Surv(time = T, event = event) ~ X1 + X2 + X3 +X4 +X5, data = dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-07-21T13:35:42.947" Id="65010" LastActivityDate="2013-07-22T06:06:59.963" LastEditDate="2013-07-22T06:06:59.963" LastEditorUserId="307" OwnerUserId="307" ParentId="65005" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/nlme/html/lme.html&quot; rel=&quot;nofollow&quot;&gt;lme()&lt;/a&gt; is the function for estimating Linear Mixed Effects models in the &lt;a href=&quot;http://cran.r-project.org/web/packages/nlme/index.html&quot; rel=&quot;nofollow&quot;&gt;nlme package&lt;/a&gt; for the &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R project for statistical computing&lt;/a&gt;.  For general questions about mixed effects models, use the &lt;a href=&quot;/questions/tagged/mixed-effect&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged 'mixed-effect'&quot; rel=&quot;tag&quot;&gt;mixed-effect&lt;/a&gt; tag.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-21T15:00:12.160" Id="65019" LastActivityDate="2013-08-02T02:01:34.387" LastEditDate="2013-08-02T02:01:34.387" LastEditorUserId="7290" OwnerUserId="22468" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;This is really going to depend on what format the questionnaires and/or codebooks are in. If they're in a standardized, ideally text, format, you could consider parsing the codebooks (e.g., in R with something like what is described &lt;a href=&quot;http://www.r-bloggers.com/reading-codebook-files-in-r/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;), which might allow you to easily identify and match common questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, however, I find this is almost never easy. When question wordings and codings change, so do variable names and the formats of codebooks. Take a look, for example, at the inconsistency over time of the &lt;a href=&quot;http://electionstudies.org/studypages/download/datacenter_all_NoData.php&quot; rel=&quot;nofollow&quot;&gt;American National Election Studies data&lt;/a&gt;, which use generally standard questions but alternate between text and PDF codebooks, all arranged in different ways.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I've had to combine multiple questions from different surveys, I generally work in the following workflow:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Establish the set of constructs/variables you want to have in the final data set and put all of these into a spreadsheet (i.e., as separate rows).&lt;/li&gt;&#10;&lt;li&gt;Go through all of the surveys and line-up each survey's question identifier with the relevant construct.&lt;/li&gt;&#10;&lt;li&gt;Decide how you want the data in your final dataset coded and establish the set of recoding procedures to convert all of the original surveys into the desired format.&lt;/li&gt;&#10;&lt;li&gt;Merge all of the identified questions from each survey into a single dataset.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In short, parts of this could be automated, but it all depends on what format the original questionnaires/codebooks are in.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-21T16:50:17.723" Id="65029" LastActivityDate="2013-07-21T16:50:17.723" OwnerUserId="25138" ParentId="62265" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Your link actually leads to &quot;&lt;strong&gt;quasi-maximum likelihood&lt;/strong&gt;&quot; or more formally &quot;&lt;strong&gt;composite likelihood&lt;/strong&gt;&quot;. You can find a good review about composite likelihood &lt;a href=&quot;http://www3.stat.sinica.edu.tw/statistica/oldpdf/A21n11.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Composite likelihood sometimes was called quasi-likelihood, such as &lt;em&gt;Hjort and Omre (1994), Glasbey (2001)&lt;/em&gt; and &lt;em&gt;Hjort and Varin (2008)&lt;/em&gt;. However, &lt;strong&gt;composite likelihood&lt;/strong&gt;, which can be applied in space-time models and longitudinal data, was proposed by &lt;em&gt;Besag (1974, 1975)&lt;/em&gt;, and &lt;strong&gt;quasi-likelihood&lt;/strong&gt; was introduced by &lt;em&gt;Wedderburn (1974)&lt;/em&gt; and mainly used in generalized linear models. &#10;As I discussed &lt;a href=&quot;http://stats.stackexchange.com/questions/62923/models-for-generalized-estimating-equation&quot;&gt;here&lt;/a&gt;, GEE only uses the mean ($\mu$) and variance ($V$) of the outcome and reaches the &lt;strong&gt;quasi-likelihood&lt;/strong&gt;,&#10;$$Q(\mu,y)=\int^{\mu}_y(y-t)^TV^{-1}dt,$$&#10;and the &lt;strong&gt;quasi-likelihood estimating equations&lt;/strong&gt; (quasi-score function) for the estimation is &#10;$$\sum_i\frac{\partial{\mu_i^{'}}}{\partial{\beta}}V_i^{-1}(y_i-\mu_i)=0.$$&lt;/li&gt;&#10;&lt;li&gt;GEE is an &lt;strong&gt;extension&lt;/strong&gt; of &lt;strong&gt;generalized&lt;/strong&gt; linear models to the analysis of longitudinal data. I prefer to think the word &quot;generalized&quot; came from GLM, but you can also regard it as a generalization to longitudinal data.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-07-21T20:28:37.360" Id="65049" LastActivityDate="2013-07-21T20:44:15.150" LastEditDate="2013-07-21T20:44:15.150" LastEditorUserId="21599" OwnerUserId="21599" ParentId="62939" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If I understand correctly, what you would like to have is a measure to compare the underlying true ranking $\pi$ and the predicted ranking (i.e., the tournament ranking, or the simulated ranking) $\sigma$, where $\sigma$ is a function of some input parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the statistics literature, there are a number of distance functions for rankings. I will list some of them below. Let $\pi(i)$ and $\sigma(i)$ be the ranks of item $i$ in $\pi$ and $\sigma$, respectively (e.g., $\pi(``\text{John}&quot;)=2$ and $\sigma(``\text{John}&quot;)=1$ in your example). The Kendall distance is defined as &#10;$$&#10;K(\pi,\sigma) = \# \lbrace \; (i,j) \, \vert \, \pi(i)&amp;gt;\pi(j) \text{ and } \sigma(i)&amp;lt;\sigma(j) \; \rbrace \, .&#10;$$&#10;The Spearman distance is defined as&#10;$$&#10;S(\pi,\sigma) = \sum_i \left( \pi(i) - \sigma(i)\right)^2 \, .&#10;$$&#10;The Spearman footrule distance is defined as&#10;$$&#10;F(\pi,\sigma) = \sum_i \lvert \pi(i) - \sigma(i)\rvert \, .&#10;$$&#10;These are three widely used distance functions for rankings. You could think of the distance as the &lt;strong&gt;loss&lt;/strong&gt; the predicted ranking suffers w.r.t. the true ranking, so the lower the distance, the better. If you need an &lt;strong&gt;accuracy&lt;/strong&gt; (the larger the better) instead of a loss, these distances can be easily normalized to different types of &lt;a href=&quot;http://en.wikipedia.org/wiki/Rank_correlation&quot; rel=&quot;nofollow&quot;&gt;correlation coefficients for rankings&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Regarding the bonus question:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think there are a lot of different ways to do it. For example, after you divide the items in the true rankings into several groups, you can use the &lt;strong&gt;C-index&lt;/strong&gt; to measure the performance of the predicted ranking. In the study of multipartite ranking, C-index is commonly used. It is a kind of extension of AUC (area under the ROC curve). You can check Section 4 of &lt;a href=&quot;http://www.weiweicheng.com/research/papers/cheng-ieeetfs12.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, where a short introduction of C-index is given.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-21T22:59:09.347" Id="65060" LastActivityDate="2013-07-21T22:59:09.347" OwnerUserId="27915" ParentId="65000" PostTypeId="2" Score="4" />
  
  
  <row Body="The function to fit generalized linear mixed model (GLMM) in R package lme4." CommentCount="0" CreationDate="2013-07-22T01:48:16.753" Id="65071" LastActivityDate="2013-07-22T05:20:24.937" LastEditDate="2013-07-22T05:20:24.937" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;I'm sure terminology varies, but I think it's fair to say that a &lt;em&gt;split-plot&lt;/em&gt; design (where there are two or more treatments imposed a different hierarchical levels) is a specific example of a mixed design. Mixed effect models (also called multilevel or hierarchical models; repeated measures are another special case) are so-called because they include both random and fixed effect terms. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would say that split-plot designs are &quot;both&quot; between- and within-subject designs, because (at least) one treatment is between- and (at least) one treatment is within-subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to answer the other question one would need a more specific example.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-22T02:32:04.430" Id="65073" LastActivityDate="2013-07-22T02:32:04.430" OwnerUserId="2126" ParentId="63454" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The result is not stupid. You asked SOM to work with continuous variables (distances) and there are lots of them (1000 vars). The line traces are the &quot;spectra&quot;; which, in this case, means the position on the horizontal axis indicates the &quot;variable&quot; (in your case each column of the distance matrix) and the height in y indicates the importance of those &quot;variables&quot; for each of the prototypes that represents each grid in your SOM.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you are doing doesn't make much sense, at least for &lt;code&gt;som()&lt;/code&gt;. It should be provided with the raw data that you used to generate the distance matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suppose there could be some use for this - what the plotted results are trying to show you graphically is that the upper-left cell contains those observations that are very dissimilar to the first few observations (rows) in the matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not convinced it is useful however. I would have proceeded, if using SOM, by providing the raw data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you just want a heatmap, try&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;heatmap(mat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-07-22T15:36:14.487" Id="65125" LastActivityDate="2013-07-22T15:36:14.487" OwnerUserId="1390" ParentId="65079" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;I am trying to compare correlations between subgroups of a variable. For instance, I am trying to compare the income of group 1 v. income of group 2 v. income of group 3 v. income of group 4, where the groups are specified by another categorical variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I did was create new variables separating out the income variable into four new variables (i.e. income_1, income_2, income_3, income_4). Naturally, there are missing values that look something like this (each bracketed portion represents a new line where the '.' represents missing values):   &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[income_1   income_2    income_3   income_4]  &#10;[1 . . . ]  &#10;[1 . . .]  &#10;[. 2 . .]  &#10;[. 2 . .]  &#10;[. . 1 .]  &#10;[. . 1 .]  &#10;[. . . 2]  &#10;[. . . 2]  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So when I try to perform &lt;code&gt;correlate&lt;/code&gt; on these four values, I get an error that says 'no observations' probably because &lt;code&gt;correlate&lt;/code&gt; performs listwise or pairwise deletion. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I fix this problem? I don't know how to get rid of the missing values.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-22T15:45:52.823" Id="65127" LastActivityDate="2014-11-30T03:50:13.467" LastEditDate="2013-07-27T13:25:36.823" LastEditorUserId="22047" OwnerUserId="28331" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;stata&gt;&lt;missing-data&gt;&lt;cross-correlation&gt;&lt;error-message&gt;" Title="Using correlate in Stata to compare within a variable" ViewCount="1943" />
  <row Body="&lt;p&gt;Causal inference tries to quantify the effect of a change in $X$ on $Y$ while holding constant or eliminating all other relevant factors which might influence this relationship. Such an effect is often referred to as a causal (or treatment) effect. It can be used to answer &quot;what if&quot;-type questions like: what happens to crime if we increase the police force by 10%? How will a person's future earnings change if she goes to school one year longer?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that in statistics and philosophy there is no single agreed-on definition of causation. Commonly used conceptual frameworks to think about causality are the Rubin Causal Model based on counterfactuals, structural equation modelling, and causal decision theory. Statistical tools for causal inference include instrumental variables models, regression discontinuity designs, difference-in-differences, and matching models.&lt;/p&gt;&#10;&#10;&lt;p&gt;For further references on the philosophical and statistical aspects of causal inference see: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf&quot; rel=&quot;nofollow&quot;&gt;Pearl, C. (2009) &quot;Causal Inference in Statistics: an Overview&quot;, Statistics Surveys, Vol. 3, pp. 96-146&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://books.google.de/books?id=lFTP49c5wJoC&amp;amp;lpg=PP1&amp;amp;dq=causal%20inference&amp;amp;hl=de&amp;amp;pg=PP1#v=onepage&amp;amp;q&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Morgan, S.L. and Winship, C. (2007) &quot;Counterfactuals and Causal Inference&quot;, Cambridge University Press, Cambridge, UK&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-07-22T16:25:04.743" Id="65140" LastActivityDate="2013-08-29T13:46:54.393" LastEditDate="2013-08-29T13:46:54.393" LastEditorUserId="919" OwnerUserId="26338" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;For the random variables $X_1,...,X_n$ with the joint density $f(X_1,...,X_n|\theta)$ the likelihood function $L(\theta) = f(x_1,...,x_n|\theta)$ gives the conditional probability of the observed realizations $X_1 = x_1,...,X_n=x_n$ for a given parameter $\theta$. Note that the likelihood function is not a probability density function and thus usually does not integrate to one. The likelihood function is frequently used in both Bayesian and frequentist analyses. For instance, the maximum likelihood estimator is obtained by maximizing the logarithm of the likelihood function.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-22T16:35:20.813" Id="65148" LastActivityDate="2013-09-02T10:31:55.020" LastEditDate="2013-09-02T10:31:55.020" LastEditorUserId="26338" OwnerUserId="26338" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;Presumably &quot;runtime&quot; is better when it is &lt;em&gt;low&lt;/em&gt; and &quot;solution quality&quot; is better when it is &lt;em&gt;high.&lt;/em&gt;  To make these variables more consistent in how they represent the attributes, first re-express them so that high numbers always correspond to better values.  For this question, the reciprocal of runtime (which could be interpreted as number of runs per second) would be a good choice, but the negative of runtime would work fine too.&lt;/p&gt;&#10;&#10;&lt;p&gt;With this convention established, we may plot each record in the dataset by associating the re-expressed &quot;runtime&quot; value with the x coordinate and the &quot;solution quality&quot; value with the y coordinate, creating a scatterplot.  One record $(x,y)$ &lt;em&gt;dominates&lt;/em&gt; another $(x',y')$ when simultaneously $x\ge x'$, $y\ge y'$, and $(x,y) \ne (x',y')$.  No rational actor (using solely information about &quot;runtime&quot; and &quot;solution quality&quot;) would choose the dominated option $(x',y')$ when $(x,y)$--which is at least as good on both attributes and better on at least one--is available. Therefore such an actor would focus their interest on the &lt;em&gt;non-dominated&lt;/em&gt; options and would be free to neglect all others.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lYD4g.png&quot; alt=&quot;Figure 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;All points dominated by the solid red point are shown in gray.  All possible points that it can dominate constitute the (closed) rectangle shown in pink. Because evidently no points in the scatterplot dominate the solid point, it must lie on the non-dominated frontier.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The set of non-dominated options forms the vertices of part of the &quot;quasi-convex hull&quot; or &quot;non-dominated frontier&quot; of the scatterplot.  This is an analog of the better-known convex hull.  Indeed, the quasi-convex hull will be invariant under all increasing monotonic re-expressions of the variables, $f(x)$ and $g(y)$, and one can always find such re-expressions for which the quasi-convex hull actually becomes the convex hull.  This connection, and the fact that optimal construction of the convex hull of $n$ points requires $O(n\log(n))$ computation, suggests that we seek an algorithm for the quasi-convex hull that performs at least as well.  Such algorithms exist.  Here is one, given in &lt;code&gt;R&lt;/code&gt; as requested, which is written to port easily to other computing platforms:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hull.qc &amp;lt;- function(xy) {&#10;  i &amp;lt;- order(xy[, 1], xy[, 2], decreasing=TRUE)&#10;  frontier &amp;lt;- rep(0, length(i))&#10;  k &amp;lt;- 0; y &amp;lt;- -Inf&#10;  for (j in i) {&#10;    if (xy[j, 2] &amp;gt; y) {&#10;      frontier[k &amp;lt;- k+1] &amp;lt;- j&#10;      y &amp;lt;- xy[j, 2]&#10;    }&#10;  }&#10;  return(frontier[1:k])&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It performs a decreasing lexicographic sort of the coordinates (time: $O(n\log(n))$) and then scans through the first coordinate (time: $O(n)$), putting into effect a &lt;em&gt;line scan algorithm.&lt;/em&gt; The points where a new larger value of the second coordinate is found are recorded.  Their indexes within the input array &lt;code&gt;xy&lt;/code&gt; are returned.&lt;/p&gt;&#10;&#10;&lt;p&gt;We can prove this algorithm is correct by induction.  The initial point, chosen to have a maximal first coordinate and (among such points) a maximal second coordinate, obviously is not dominated.  Using it we can eliminate all other points that it does dominate, including itself.  The first non-eliminated point next encountered in the algorithm necessarily has a larger value of its second coordinate. It obviously is not dominated by any point because if it were, that other point would already have been encountered.  Therefore the algorithm indeed selects non-dominated points and (by induction on the number of points) it finds them all, QED.&lt;/p&gt;&#10;&#10;&lt;p&gt;To illustrate these concepts and this algorithm, here is plot of a dataset of $16$K records (all with integral values, shown jittered) along with its quasi-convex hull (a dark line at the upper right), its vertices marked as requested in the question, and the &quot;dominance rectangles&quot; of those vertices colored in.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VEYwk.png&quot; alt=&quot;Figure 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The code to do the calculations, make a scatterplot, and mark the vertices of its quasi-convex hull appears below.  It includes a slightly faster &lt;code&gt;R&lt;/code&gt;-centric version of &lt;code&gt;hull.qc&lt;/code&gt;.  It processes a million points in approximately two seconds on this machine.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#&#10;# A faster solution (for R).&#10;#&#10;hull.qc &amp;lt;- function(xy) {&#10;  i &amp;lt;- order(xy[, 1], xy[, 2], decreasing=TRUE)&#10;  y &amp;lt;- xy[i, 2]&#10;  frontier &amp;lt;- which(cummax(y) &amp;lt;= y)&#10;  #&#10;  # Eliminate interior points along edges of the hull.&#10;  #&#10;  y.0 &amp;lt;- y[frontier]&#10;  frontier &amp;lt;- frontier[c(TRUE, y.0[-1] != y.0[-length(y.0)])]&#10;  return(i[frontier])&#10;}&#10;#&#10;# Generate data.&#10;#&#10;library(MASS)&#10;set.seed(17)&#10;n &amp;lt;- 2^14&#10;xy &amp;lt;- floor(mvrnorm(n, c(1,1), matrix(c(1, -1/2, -1/2, 1), 2))^2)&#10;#&#10;# Do the work.&#10;#&#10;system.time(frontier &amp;lt;- hull.qc(xy))&#10;xy.f &amp;lt;- xy[frontier, , drop=FALSE]&#10;#&#10;# Visualization.&#10;#&#10;plot(xy, xlab=&quot;X&quot;, ylab=&quot;Y&quot;, main=&quot;Quasiconvex Hull&quot;)&#10;points(xy.f, pch=19, col=&quot;Red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-07-22T17:14:02.567" Id="65157" LastActivityDate="2013-07-22T18:06:53.590" LastEditDate="2013-07-22T18:06:53.590" LastEditorUserId="919" OwnerUserId="919" ParentId="16815" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;One of the biggest differences between LDA and the other methods is that it's just a machine learning technique for data which are assumed to be normally distributed. That can be great under the case of missing data or truncation where you can use the EM algorithm to maximize likelihoods under very strange and/or interesting circumstances. &lt;em&gt;Caveat emptor&lt;/em&gt; because model misspecifications, such as multimodal data, can lead to poor performing predictions where K-means clustering would have done better. Multimodal data can also be accounted for with EM to detect latent variables or clustering in LDA.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, suppose you are looking to measure probability of developing a positive diagnosis of AIDS in 5 years based on CD4 count. Suppose further that you don't know the value of a specific biomarker that greatly impacts CD4 counts and is associated with further immunosuppression. CD4 counts under 400 are below lower limit of detection on most affordable assays. The EM algorithm allows us to iteratively calculate the LDA and biomarker assignment and the means and covariance for CD4 for the untruncated DF.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-22T18:01:29.873" Id="65163" LastActivityDate="2013-07-22T18:35:27.417" LastEditDate="2013-07-22T18:35:27.417" LastEditorUserId="3277" OwnerUserId="8013" ParentId="65160" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In Stata, both cases can be handled with the interval regression command &lt;a href=&quot;http://www.stata.com/manuals13/rintreg.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;intreg&lt;/code&gt;&lt;/a&gt;, which is generalization of the Tobit. It can handle point, interval, or left/right censored data (or a mixture of them all). It does assume error term normality, but the log transformation can often work if your data require and permit it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if there are canned non-Stata implementations, but there are formulas and references at the end of the manual link. It is a fairly straight-forward likelihood function that should be fairly easy to maximize. There's also a nice comparison between the ordered probit approach and interval regression using the value of the log likelihood for the first case scenario. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a very simple simulation with $N=5000, Y=\alpha + \beta \cdot X + \varepsilon =\frac{1}{2} + 1 \cdot X + \mathcal{N}[0,1]$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#delimit;&#10;clear all;&#10;&#10;set seed 10011979;&#10;set obs 5000;&#10;&#10;gen x = rnormal();&#10;gen ystar = 0.5 + 1*x + rnormal();&#10;gen ylb = ystar - int((5-1)*runiform());&#10;gen yub = ystar + int((5-1)*runiform());&#10;&#10;intreg ylb yub x;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Every observation has a variable interval constructed by adding/subtracting a random uniform number to the true value, so the intervals may overlap. The data basically looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1qh0V.png&quot; alt=&quot;First 10 Observations&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, 2 observations are uncensored (i.e., point data). &lt;/p&gt;&#10;&#10;&lt;p&gt;The output is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Interval regression                               Number of obs   =    5000&#10;                                                  LR chi2(1)      =    2102.49 &#10;Log likelihood = -3580.5326                       Prob &amp;gt; chi2     =     0.0000&#10;&#10;&#10;------------------------------------------------------------------------------&#10;             |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;           x |    .979912   .0192016    51.03   0.000     .9422775    1.017546&#10;       _cons |   .4757097   .0190327    24.99   0.000     .4384063    .5130131&#10;-------------+----------------------------------------------------------------&#10;    /lnsigma |   .0336532   .0143186     2.35   0.019     .0055893    .0617171&#10;-------------+----------------------------------------------------------------&#10;       sigma |   1.034226   .0148086                      1.005605    1.063661&#10;------------------------------------------------------------------------------&#10;&#10;  Observation summary:         0  left-censored observations&#10;                             326     uncensored observations&#10;                               0 right-censored observations&#10;                            4674       interval observations&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It seems like all the parameters, including the standard deviation of the error, are fairly close to the true values.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-22T21:23:07.287" Id="65184" LastActivityDate="2013-07-23T20:39:39.690" LastEditDate="2013-07-23T20:39:39.690" LastEditorUserId="7071" OwnerUserId="7071" ParentId="65179" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;yes, you simply differentiate with respect to $a$, set to zero, and back-substitute. First note that the objective, $t^2(a)$, is positively scale-invariant with respect to $a$. That is, for $k&amp;gt;0$, $t^2(ka) = t^2(a)$. This allows you to be somewhat sloppy when computing the derivative, since any (positive) constant can be dropped.&lt;/p&gt;&#10;&#10;&lt;p&gt;The derivative simply follows the quotient rule of calculus. The derivative (with respect to $a$) of the numerator of $t^2$ is, up to scale, $a^{\top}\left(\bar{x}_1 - \bar{x}_2\right)$. The derivative of the denominator is, up to scale, $S_U a$. When using the 'lo-dee-hi minus hi-dee-lo &lt;em&gt;etc&lt;/em&gt;' rule, the derivative is something like $c_1 a^{\top}\left(\bar{x}_1 - \bar{x}_2\right) - c_2 S_U a$ for positive constants $c_i$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-22T22:55:24.893" Id="65190" LastActivityDate="2013-07-22T22:55:24.893" OwnerUserId="795" ParentId="55976" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a set of data and plotted it in a graph (in Microsoft Excel) and then added a trendline. The equation I got is $y=833.71x^{-1.448}$ with $R^2=0.9511$. What should I do to get $R^2=1$ (100% fit)? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-07-22T23:20:25.187" Id="65192" LastActivityDate="2013-07-23T02:38:55.477" LastEditDate="2013-07-23T00:01:39.043" LastEditorUserId="7290" OwnerUserId="28342" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;variance&gt;&lt;excel&gt;" Title="How can I get an R-squared value of 1 (fit 100%)?" ViewCount="892" />
  <row AcceptedAnswerId="65226" AnswerCount="1" Body="&lt;p&gt;Consider the following data and the code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% The data&#10;x = [4 4.5 5 5.5 6 6.5];&#10;&#10;y1 =  [0.000159334114311,0.000184477307337,0.002931979623674,...&#10;    0.004321711975947,0.006269020390557,0.012537205790269];&#10;&#10;y2 = [0.000160708687146,0.000186102543697,0.002956862489638,...&#10;    0.004356837209873,0.006325918592142,0.012667035948290];&#10;&#10;% The fit is setup&#10;ft = fittype( 'power1' );&#10;opts = fitoptions( ft );&#10;opts.DiffMaxChange = 0.1;&#10;opts.DiffMinChange = 1e-16;&#10;opts.Display = 'Off';&#10;opts.Lower = [0 0];&#10;opts.MaxFunEvals = 99999;&#10;opts.MaxIter = 99999;&#10;opts.StartPoint = [1 10];&#10;opts.TolFun = 1e-16;&#10;opts.TolX = 1e-16;&#10;opts.Upper = [Inf Inf];&#10;&#10;% The fits are executed&#10;[fitresult, gof] = fit( x', y1', ft)&#10;[fitresult, gof] = fit( x', y1'/86400, ft)&#10;[fitresult, gof] = fit( x', y2', ft)&#10;[fitresult, gof] = fit( x', y2'/86400, ft)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I was trying to fit y1 vs. x and then y2 vs. x to a simple power law $y=ax^b$. Mainly I am interested in the exponent $b$. Don't care much for $a$. As I started playing with different units, converting y1 and y2 between days and seconds, I saw that for y1, the exponent stayed the same after scaling but for y2, the exponent changed significantly after scaling.&lt;/p&gt;&#10;&#10;&lt;p&gt;For y1, here is the output&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fitresult = &#10;&#10; General model Power1:&#10; fitresult(x) = a*x^b&#10; Coefficients (with 95% confidence bounds):&#10;   a =   1.465e-09  (-8.771e-09, 1.17e-08)&#10;   b =       8.542  (4.759, 12.32)&#10;&#10;gof = &#10;&#10;       sse: 4.265059892284385e-06&#10;   rsquare: 0.960370163394534&#10;       dfe: 4&#10;adjrsquare: 0.950462704243167&#10;      rmse: 0.001032601071601&#10;&#10;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&#10;&#10;fitresult = &#10;&#10; General model Power1:&#10; fitresult(x) = a*x^b&#10; Coefficients (with 95% confidence bounds):&#10;   a =   1.696e-14  (-1.015e-13, 1.354e-13)&#10;   b =       8.542  (4.759, 12.32)&#10;&#10;gof = &#10;&#10;       sse: 5.713439713386793e-16&#10;   rsquare: 0.960370163394534&#10;       dfe: 4&#10;adjrsquare: 0.950462704243167&#10;      rmse: 1.195140129167579e-08&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;we see that in this case the exponent is 8.542 in both cases, identical for my purposes. But for the second data set, I get&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fitresult = &#10;&#10; General model Power1:&#10; fitresult(x) = a*x^b&#10; Coefficients (with 95% confidence bounds):&#10;   a =   3.472e-08  (-1.099e-07, 1.794e-07)&#10;   b =        6.83  (4.561, 9.099)&#10;&#10;gof = &#10;&#10;       sse: 2.501974567230523e-06&#10;   rsquare: 0.977224831423532&#10;       dfe: 4&#10;adjrsquare: 0.971531039279415&#10;      rmse: 7.908815599112364e-04&#10;&#10;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&#10;&#10;fitresult = &#10;&#10; General model Power1:&#10; fitresult(x) = a*x^b&#10; Coefficients (with 95% confidence bounds):&#10;   a =    1.71e-14  (-1.019e-13, 1.361e-13)&#10;   b =       8.542  (4.774, 12.31)&#10;&#10;gof = &#10;&#10;       sse: 5.784659576382752e-16&#10;   rsquare: 0.960691723661228&#10;       dfe: 4&#10;adjrsquare: 0.950864654576535&#10;      rmse: 1.202565962471784e-08&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;now the exponent was 6.83 and then changed to 8.542 after changing units. Why did this happen?&lt;/p&gt;&#10;&#10;&lt;p&gt;The unit conversion is straightforward, just multiplication by a constant. I imagine that for a power law the exponent wouldn't depend on the scale of the data. For example, if I start with a quadratically correlated data, then multiply all of the y-coordinates by 10, the correlation will still be quadratic but the multiplicative constant will be multiplied by ten. So two questions.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Why does the exponent change when I scale the y-values? Should the exponent change so? Which behavior is more common? I thought scaling should make no difference so the first behavior should be more common than the second but then I don't understand why the second behavior should occur at all?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;More importantly, what is the difference between the two data sets? Why the different behavior? Why does one change but not the other? Is there a deep mathematical reason behind it or is it easily explainable? Is it something inherent in the data itself? Thought I can't imagine what could it be. They are both so similar. Or does it have something to do with the black-box algorithm in MATLAB? Initialization and bounding parameter values doesn't make a difference. The algorithm always converges to these same values. It doesn't matter if I initialize or if I let MATLAB initialize. Is it something inherent in non-linear least squares method? I always imagined NLLSF to a power law to be robust to scaling but apparently it isn't. Very surprising I think!&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is this phenomenon well-known/studied? If someone can point me to some references as well, that would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Addendum #1,&lt;/p&gt;&#10;&#10;&lt;p&gt;Following whuber's comment, &quot;one has to suspect that the nonlinear fitting procedure is finding its solutions with relatively poor precision&quot;, after some careful playing around with the data (which I should have done to begin with) it turns out that the culprit was indeed low tolerance error. In my posted code,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[fitresult, gof] = fit( x', y1', ft)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;should have been&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[fitresult, gof] = fit( x', y1', ft,opts)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where I forgot to include opts and hence the tolerance (TolFun) was at the default 1e-6 instead of my specified 1e-16. So now MATLAB was returning answers much closer to what R is getting. Furthermore, I see that R and Mathematica seem to be automatically using the Levenberg-Marquardt algorithm while MATLAB keeps using Trust-Region algorithm. When I force MATLAB to use Levenberg-Marquardt, it now spits out identical answers to R and Mathematica. Thanks everyone!&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-07-23T04:31:44.000" Id="65211" LastActivityDate="2013-07-24T00:24:46.280" LastEditDate="2013-07-24T00:24:46.280" LastEditorUserId="26352" OwnerUserId="26352" PostTypeId="1" Score="2" Tags="&lt;matlab&gt;&lt;least-squares&gt;&lt;nonlinear-regression&gt;&lt;power-law&gt;" Title="Why does the scaling exponent of a power law fit change so radically when the data is scaled by a constant?" ViewCount="1045" />
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;Slow feature analysis (&lt;a href=&quot;http://www.scholarpedia.org/article/Slow_feature_analysis&quot;&gt;SFA&lt;/a&gt;) uses the smalles Eigenvalues of the covariance matrix of temporal differences to find the slowest features in a time series,&lt;/li&gt;&#10;&lt;li&gt;Minor component analysis (&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0893608096000639&quot;&gt;MCA&lt;/a&gt;) uses the smallest components in a probabilistic setting--here, not directions of variations are found but constraints,&lt;/li&gt;&#10;&lt;li&gt;Extreme component analysis (&lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2003_AA18.pdf&quot;&gt;XCA&lt;/a&gt;) is a combination of probabilistic PCA and MCA,&lt;/li&gt;&#10;&lt;li&gt;In Canonical Correlation Analysis (where you analyse the correlation btw two different data sets), the smaller components of the correlation matrix correspond to so called &quot;private&quot; spaces. These represent the subspaces of each variable which do not correlate linearly with each other.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-07-23T06:43:50.423" Id="65213" LastActivityDate="2013-07-23T06:43:50.423" OwnerUserId="2860" ParentId="64750" PostTypeId="2" Score="5" />
  <row AnswerCount="2" Body="&lt;p&gt;I tried to create a neural network for regression. In order to test the concept, I created a dataset the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x1 = random('Normal',0,5,500,1);&#10;x2 = random('Normal',0,5,500,1);&#10;y = x1 + 2*x2;&#10;X = [x1 x2];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So x1 and x2 are vectors of random numbers with mean value 0 and standard deviation 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;The neural network therefore had 2 inputs, 2 or 3 units in hidden layer and one output unit. In hidden layer units I implemented sigmoid activation function and in the output layer linear function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;h(y) = 1*k20 + a1*k21+ a2*k22 + a3*k23&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and for hidden layer outputs are calculated the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a1 = sigmoid(1*k10 + x1*k11 + x2*k12)&#10;(and similarly a2 = sigmoid(1*l10 + x1*l11 + x2*l12)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Edit:&#10;The Backpropagation algorithm is implemented the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Delta_1=0;&#10;Delta_2=0;&#10;&#10;for t=1:m&#10;    currentSampleSize = sampleSize(t,:);&#10;    for i = 1 : currentSampleSize&#10;        % Step 1&#10;        a_1 = X(t,:)';&#10;        z_2 = Theta1*a_1;&#10;        a_2 = sigmoid(z_2);&#10;        a_2 = [1; a_2];&#10;        z_3 = Theta2*a_2;&#10;        a_3 = z_3;&#10;&#10;        % Step 2&#10;        delta_3 = (a_3 - y_new(t,:)');&#10;&#10;        % Step 3&#10;        grad = m^(-1) * X' * (a_3-y);&#10;        delta_2 = Theta2'*delta_3.*[1;sigmoidGradient(z_2)];&#10;        delta_2 = delta_2(2:end);&#10;&#10;        %Step 4&#10;        Delta_2 = Delta_2 + delta_3*a_2';&#10;        Delta_1 = Delta_1 + delta_2*a_1';&#10;    end&#10;end&#10;&#10;Theta1_grad = Delta_1/m;&#10;Theta2_grad = Delta_2/m;&#10;&#10;reg_1 = lambda/m.*Theta1(:,2:end);&#10;reg_2 = lambda/m.*Theta2(:,2:end);&#10;&#10;Theta1_grad(:,2:end) = Theta1_grad(:,2:end) + reg_1;&#10;Theta2_grad(:,2:end) = Theta2_grad(:,2:end) + reg_2;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As the predictions are not accurate I wonder what could be done in order to improve the regression performance. I tried to change the combinations of number of neurons in hidden layer and different regularization parameter values but the performance wasn't good enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very thankful if anyone could describe the appropriate design of neural network for regression problems - which activation functions are most appropriate and how to diagnose training and test error for choosing the optimal number of hidden units.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-23T07:23:56.680" FavoriteCount="1" Id="65214" LastActivityDate="2013-07-24T07:54:17.590" LastEditDate="2013-07-24T07:54:17.590" LastEditorUserId="315" OwnerUserId="315" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;neural-networks&gt;" Title="Regression with neural network" ViewCount="514" />
  <row AnswerCount="0" Body="&lt;p&gt;In COX PH, how can I know the model, I have, is robust? AIC and -2LL will only tell me relatively the goodness of model.&#10;Also, is there a way to compute (in R) the relative log hazard scale  as opposed to cumulative? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-07-23T08:25:24.767" Id="65219" LastActivityDate="2013-07-23T11:36:20.807" LastEditDate="2013-07-23T11:36:20.807" LastEditorUserId="603" OwnerUserId="28306" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;cox-model&gt;&lt;hazard&gt;" Title="Cox model - model robustness" ViewCount="89" />
  
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am working on a report which is being sent through to end users that should flag to them any &quot;large changes&quot; in the day-to-day values for the past 30 days. These values are day-to-day differences which we assume to have a mean of zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;To shed a bit more light on the data, the values are proportional returns to a portfolio. They are used to measure the change in the riskiness of different aspects a portfolio which is constantly managed, so the riskiness should always stay at &quot;round about the same level&quot; based upon the investing strategy associated with that given portfolio.&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said, the risk will fluctuate a bit from day-to-day, but it gets re-balanced and so the change in daily risk should fluctuate around zero - positively for roughly half the time &amp;amp; negatively for roughly half the time. &lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis is to simply point out any large daily deviations so they can be examined. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, assume we have the following data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Day:    Change from previous day:&#10;1       -40&#10;2       30&#10;3       15&#10;4       12&#10;5       -34&#10;6       -2&#10;...&#10;30      12&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And, as I stated earlier, they don't care about the direction of the change, just to flag any day where the change is of a &quot;statistically significant&quot; magnitude.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm curious as to how to calculate the statistics for this properly / what distribution would best be used to assign probability levels for this set of data.&lt;/p&gt;&#10;&#10;&lt;p&gt;What has been proposed is to, firstly, look at the square of the changes (rather than the original values since all we care about is magnitude):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;To calculate the std dev as $\sqrt{ \dfrac{\sum \left( x_i - 0 \right)^2 }{29}}$ - In other words &lt;strong&gt;assume&lt;/strong&gt; a zero expected value&lt;/li&gt;&#10;&lt;li&gt;To Calculate the std dev as $\sqrt{ \dfrac{\sum \left( x_i - \bar{x} \right)^2 }{29}}$ - In other words, use the mean we would calculate for this sample&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;And then to take each day's value and divide it by this std_dev to calculate it's magnitude.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have quite a few problems with a few things here, but especially idea #2 since I believe the mean SHOULD BE ZERO.&lt;/p&gt;&#10;&#10;&lt;p&gt;Generally speaking, what is the correct way to analyze deltas with an expected value of zero and what distribution would you use for a 30-sample data-set to find statistically significant values? Note that historical data is available / can be easily gathered.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we assumed the daily changes to be ROUGHLY normally distributed, would the square of them (only measuring deviation from zero, regardless of whether it's positive or negative) be distributed via Chi-Square? ... Or could a folded-Normal work if we took their absolute value? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm just looking for something to show how change magnitude might be distributed... Nobody would take any issue with assuming the changes to be normally distributed if necessary... &lt;/p&gt;&#10;&#10;&lt;p&gt;I hope this makes sense, but, if not, please let me know where I could clarify more.&lt;/p&gt;&#10;&#10;&lt;p&gt;THANKS!!!&lt;/p&gt;&#10;&#10;&lt;p&gt;(I originally asked this question &lt;a href=&quot;http://math.stackexchange.com/questions/447609/the-correct-probability-distribution-way-to-analyze-daily-changes?noredirect=1#comment967656_447609&quot;&gt;here&lt;/a&gt; and was recommended to ask it here - Hopefully someone here can give me a good response)&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-07-23T14:08:00.120" FavoriteCount="1" Id="65265" LastActivityDate="2013-08-24T00:03:34.663" LastEditDate="2013-07-24T18:02:15.440" LastEditorUserId="25936" OwnerUserId="28359" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;statistical-significance&gt;&lt;random-variable&gt;&lt;finance&gt;" Title="The correct probability distribution / way to identify large deviations in a set of daily changes to portfolio value" ViewCount="145" />
  
  
  
  <row Body="&lt;p&gt;When two variables are cointegrated then it suggests that there should be Granger causality in at least one direction (X causes Y or Y causes X or both). &lt;/p&gt;&#10;&#10;&lt;p&gt;Granger causality doesn't give instantaneous causality. X is said to Granger cause Y if lagged values of X is helpful in predicting Y above and beyond the information contained in lagged values of Y alone.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not necessarily. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-23T19:06:11.637" Id="65298" LastActivityDate="2013-07-23T19:06:11.637" OwnerUserId="14860" ParentId="65290" PostTypeId="2" Score="2" />
  
  
  
  <row Body="" CommentCount="0" CreationDate="2013-07-23T20:35:33.957" Id="65322" LastActivityDate="2013-07-23T20:35:33.957" LastEditDate="2013-07-23T20:35:33.957" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;Assuming you wrote the implementation yourself, you may simply have a bug in your backpropagation algorithm. Some bugs can be quite subtle and leave the algorithm partially working with poor performance. You might try adding some gradient checking code to your implementation to verify the calculated gradients. Here's an excellent video by Andrew Ng on how to do this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=12a9fsLyFes&quot; rel=&quot;nofollow&quot;&gt;http://www.youtube.com/watch?v=12a9fsLyFes&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I'm assuming you're getting poor performance on the training set, rather than the test set. You should be able to get near perfect results on the training set, at which point your implementation is likely correct and you can start dealing with overfitting by adding regularization, etc. I would disable regularization until you get to that point. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a side note, since you're dataset has a linear relationships between the inputs and the outputs you'll likely get a better result with NO hidden neurons (i.e. a single layer perceptron), but then an MLP should work too, and you can then test it on a non-linear dataset.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-23T20:50:14.137" Id="65332" LastActivityDate="2013-07-23T20:50:14.137" OwnerUserId="28387" ParentId="65214" PostTypeId="2" Score="3" />
  
  <row Body="" CommentCount="0" CreationDate="2013-07-23T21:02:06.230" Id="65336" LastActivityDate="2013-07-23T21:02:06.230" LastEditDate="2013-07-23T21:02:06.230" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="" CommentCount="0" CreationDate="2013-07-23T21:07:38.837" Id="65338" LastActivityDate="2013-07-23T21:07:38.837" LastEditDate="2013-07-23T21:07:38.837" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  
  <row Body="&lt;p&gt;You could use a chi-square test (&lt;code&gt;?chisq.test&lt;/code&gt;) or Fisher's exact test (&lt;code&gt;?fisher.test&lt;/code&gt;), or a two sample proportions test (&lt;code&gt;?prop.test&lt;/code&gt;). Each will accept a matrix as input, but instead of total count and count in subset, you need count in subset and count not in subset. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sample.data &amp;lt;- matrix(c(5,2200,50,2500),byrow=TRUE,nr=2)&#10;rownames(Sample.data) &amp;lt;- c(&quot;x&quot;,&quot;y&quot;)&#10;colnames(Sample.data) &amp;lt;- c(&quot;subset&quot;,&quot;Universe&quot;)&#10;&#10;with(as.data.frame(Sample.data),prop.test(subset,Universe))&#10;&#10;Sampletable &amp;lt;- cbind(Sample.data[,1],(Sample.data[,2]-Sample.data[,1]))    &#10;chisq.test(Sampletable)&#10;fisher.test(Sampletable)&#10;prop.test(Sampletable) # Same result as first prop.test and chisquare.test&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All three give very similar results on large samples. You might notice the p-value on &lt;code&gt;prop.test&lt;/code&gt; and &lt;code&gt;chisq.test&lt;/code&gt; is the same; when you do a two-tailed two-sample proportions test it actually does the same test by default.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-23T23:55:15.177" Id="65350" LastActivityDate="2013-07-23T23:55:15.177" OwnerUserId="805" ParentId="65288" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can ignore the differences if you use &lt;em&gt;extractor&lt;/em&gt; functions rather than delve into model objects and take what you want. In this case, the correct extractor functions is &lt;code&gt;coefficients()&lt;/code&gt; or its shorter alias &lt;code&gt;coef()&lt;/code&gt;. This function has methods for a great number of model objects in R, written by the respective package authors so you &lt;em&gt;don't&lt;/em&gt; have to wonder how to access aspects of the fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case try&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;coef(fit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The distinction between accessing via &lt;code&gt;@&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt; is, as @gung mentions, due to the use of S4 class objects in the &lt;strong&gt;VGAM&lt;/strong&gt; package. Those objects have &lt;em&gt;slots&lt;/em&gt; and the accessor function is &lt;code&gt;@&lt;/code&gt;. &lt;code&gt;$&lt;/code&gt; is used to access the components/elements of a list or data frame. S3 methods tend to return list objects describing the fit and hence you'll see people accessing those components with &lt;code&gt;$&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, however, it is best to use extractor functions where available, e.g. &lt;code&gt;coef()&lt;/code&gt;, &lt;code&gt;fitted()&lt;/code&gt;, &lt;code&gt;predict()&lt;/code&gt;, ....&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-24T03:08:01.343" Id="65360" LastActivityDate="2013-07-24T03:08:01.343" OwnerUserId="1390" ParentId="65356" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="65367" AnswerCount="3" Body="&lt;p&gt;I am doing some univariate analysis on a variable before doing regression.  I think it is very skewed.&lt;br&gt;&#10;Three histograms are of (1) the original variable; (2) log10 transformation, and (3) inverse of the data to the power of 4.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How should the variable be prepared for regression?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vIURC.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-24T05:31:42.067" Id="65366" LastActivityDate="2013-07-24T07:25:35.660" LastEditDate="2013-07-24T07:25:35.660" LastEditorUserId="22047" OwnerUserId="17801" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;&lt;multiple-regression&gt;&lt;data-transformation&gt;&lt;skewness&gt;" Title="How to prepare variables with mild skew for multiple regression?" ViewCount="297" />
  
  <row Body="&lt;p&gt;If you're looking for a transformation of the data, you might want to consider the &lt;a href=&quot;http://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation&quot; rel=&quot;nofollow&quot;&gt;Box-Cox transformation&lt;/a&gt; which is reviewed in &lt;a href=&quot;http://www.jstor.org/stable/2348250&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-24T06:49:54.097" Id="65373" LastActivityDate="2013-07-24T06:49:54.097" OwnerUserId="27403" ParentId="65366" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="65538" AnswerCount="1" Body="&lt;p&gt;What is the best way of going about dealing with few instances in support vector regression, e.g. only approximately 40? Also - is there an optimal way of dealing with outliers in this case of few instances?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-24T09:46:15.357" Id="65387" LastActivityDate="2014-06-29T22:14:18.537" LastEditDate="2013-07-24T09:49:39.590" LastEditorUserId="6029" OwnerUserId="28411" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;cross-validation&gt;&lt;outliers&gt;" Title="Dealing with few instances in support vector regression" ViewCount="62" />
  
  <row AcceptedAnswerId="65396" AnswerCount="1" Body="&lt;p&gt;As I am trying to automate the process of evaluating models for my prediction problem, I would like to verify the following concerning the process of creating a model consisting of multiple predictor variables for predicting a response variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that polynomial regression is promising for solving our prediction problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;The goal of feature selection is to select which predictor variables are useful and should be included in the model under construction. After having defined which factors are important, then we have to define what is the maximum polynomial degree for each of the &quot;useful&quot; predictor variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does it mean that we have two ways for tuning the complexity of the model: (a) the number of features, and (b) the polynomial degree of each selected feature?&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you think that the above process is sequential? Is it possible to separate the first step from the second?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-24T11:18:56.243" Id="65395" LastActivityDate="2013-07-24T15:16:25.920" LastEditDate="2013-07-24T15:16:25.920" LastEditorUserId="6029" OwnerUserId="28204" PostTypeId="1" Score="2" Tags="&lt;regression&gt;" Title="Model and feature selection in polynomial regression" ViewCount="210" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to do a logistic regression of vaccination coverage of, for example, $HPV$. Let's assume that we have 50 states ($S$). 30 states have an urban part and a country part (urbanity variable $U$). The other 20 have only a country area. 10 of the states have population areas where individuals speaks only Spanish and other areas only English ($L$). The linguistic region correlates only slightly with urbanity. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;$S$: 1,.., 50;&#10;$U$: urban/country;&#10;$L$: English/Spanish.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;S(1)-U(urban)-L(english)&lt;/li&gt;&#10;&lt;li&gt;S(1)-U(country)-L(english)&lt;/li&gt;&#10;&lt;li&gt;S(1)-U(urban)-L(spanish)&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S(1)-U(country)-L(spanish)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S(2)-U(urban)-L(english)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S(2)-U(urban)-L(spanish)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S(3)-U(urban)-L(english)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S(3)-U(country)-L(english)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S(4)-U(urban)-L(english)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;S(4)-U(country)-L(spanish)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;p&gt;where the number indicate a state.&lt;/p&gt;&#10;&#10;&lt;p&gt;The situation is that $U$ is nested in $S$ but not each $S$ has both levels of $U$. This is also the case for the linguistic variable. If $U$ would be nested completely in $S$ and $L$ completely in $S$, the model would look like (in R-terminology for lmer): &lt;code&gt;y ~ (1|S) +(1|S:U) + (1|S:L)&lt;/code&gt; where $y$: 0/1 vaccinated&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to estimate odds ratio using this model even if variables $U$ and $L$ are not completely nested?&#10;Or Should be used another model, for example:&#10;&lt;code&gt;y ~ (1|S) + U + L&lt;/code&gt; where&lt;code&gt;(1|S)&lt;/code&gt; is random variable and &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;L&lt;/code&gt; are fixed variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;With fixed levels $U$ and $L$ the nesting character would be lost and the variables would be characterstics of the individuals,that is, the aggregation of individuals would not be considered.&#10;Problems which arise using fixed variables: would give significant results or narrow CI due to the high number of individuals.&#10;I would be very happy if someone could give me some hints how to cope with partially nested variables. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-24T14:35:18.567" FavoriteCount="0" Id="65418" LastActivityDate="2013-07-25T11:40:07.063" LastEditDate="2013-07-25T11:40:07.063" LastEditorUserId="17147" OwnerUserId="17147" PostTypeId="1" Score="2" Tags="&lt;multilevel-analysis&gt;&lt;mixed&gt;" Title="Hierarchical regression with partially nested variables" ViewCount="181" />
  
  
  
  
  <row Body="&lt;p&gt;Regarding the question about the typo in the book, I found out that  it is indeed a typo.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-24T16:18:36.153" Id="65438" LastActivityDate="2013-07-24T16:18:36.153" OwnerUserId="10861" ParentId="64701" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You can try taking the first difference, run an ARIMA, and inspect the ACF afterward to see if it worked. That should work better than demeaning the series using r*t*&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-24T20:24:41.160" Id="65474" LastActivityDate="2013-07-24T20:24:41.160" OwnerUserId="28383" ParentId="65472" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Lets say your response variable is $Y$. In regression we want to model our response variable as a linear combination of our predictor variables ($X$) e.g. $Y=\beta_0 + \beta_1X + \epsilon$ or $E[Y]=\beta_0 + \beta_1X$. But what happens when our response variable is only in $[0,1]$ (i.e. it is a probability, proportion or  strictly only 0 or 1). Notice that $\beta_0 + \beta_1X$  may take any value on the real line! It could be 0 or 1 or 100 or even negative! If our response variable is strictly in $[0,1]$ it makes no sense to try to use a model that can take values outside of that range. &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, when we want to model a probability or a proportion, we instead model a function of $Y$. For example $g(E[Y])=\beta_0 + \beta_1X$. This function is called the link function. &lt;/p&gt;&#10;&#10;&lt;p&gt;$g(E[Y]) = E[Y]$  Identity link. Used in Linear regression&lt;/p&gt;&#10;&#10;&lt;p&gt;$g(P(Y=1)) = \log{\dfrac{P(Y=1)}{1-P(Y=1)}}$ Logit link, Used in logistic regression. Notice here we are modeling the probability $Y=1$ which is also the expected value. Then we can solve for what we want: $P(Y=1) = \dfrac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$g(E[Y]) = \log{E[Y]}$ log link, used in poisson regression&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 1: In the GLM R function, the family parameter allows you to specify the link function. &lt;/p&gt;&#10;&#10;&lt;p&gt;Question 2: First of all, it is not true that binomial always looks like normal. If $X\sim Binomial(n, p=0.1)$ it is a skewed distribution, which does not look like a bell shaped distribution. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-24T23:21:47.143" Id="65496" LastActivityDate="2014-09-16T08:13:15.477" LastEditDate="2014-09-16T08:13:15.477" LastEditorUserId="34623" OwnerUserId="17661" ParentId="65463" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Reference: &lt;a href=&quot;http://www.investopedia.com/terms/l/lorenz-curve.asp&quot; rel=&quot;nofollow&quot;&gt;Investopedia&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A graphical representation of wealth distribution developed by American economist Max Lorenz in 1905. On the graph, a straight diagonal line represents perfect equality of wealth distribution; the Lorenz curve lies beneath it, showing the reality of wealth distribution. The difference between the straight line and the curved line is the amount of inequality of wealth distribution, a figure described by the Gini coefficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Lorenz curve can be used to show what percentage of a nation's residents possess what percentage of that nation's wealth. For example, it might show that the country's poorest 10% possess 2% of the country's wealth.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yRkG8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Picture reference: &lt;a href=&quot;http://en.wikipedia.org/wiki/File:Economics_Gini_coefficient2.svg&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/File:Economics_Gini_coefficient2.svg&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Excerpt reference: &lt;a href=&quot;http://en.wikipedia.org/wiki/Lorenz_curve&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-25T00:58:46.187" Id="65507" LastActivityDate="2013-07-25T09:48:19.423" LastEditDate="2013-07-25T09:48:19.423" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;Here are two tests that you could use:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/White_test&quot; rel=&quot;nofollow&quot;&gt;Whiteness Test&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic&quot; rel=&quot;nofollow&quot;&gt;Independence Test&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In the whiteness test, if the autocorrelation function of the residuals is within the confidence interval of the estimates, then it is considered a pass.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Independence test, the residuals should not depend on past inputs. The Durbin-Watson statistic is used in this test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-25T04:49:07.980" Id="65527" LastActivityDate="2013-07-25T04:49:07.980" OwnerUserId="32299" ParentId="65525" PostTypeId="2" Score="2" />
  <row Body="" CommentCount="0" CreationDate="2013-07-25T07:54:58.497" Id="65534" LastActivityDate="2013-07-25T07:54:58.497" LastEditDate="2013-07-25T07:54:58.497" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row AcceptedAnswerId="65574" AnswerCount="1" Body="&lt;p&gt;I'm having a set of data's, say &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Question,&#10;Question's Main Category,Main Category followers,&#10;Question's Related Categories,Each Related Categories followers,&#10;Whether it got answer in 24hrs [Yes/No] &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, I need to analyse a database with thousands of sets of data's like given above and predict whether the new given question(with all details like main category,followers...etc) will get answer in 24hrs or not ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have no idea which algorithm or statistical analysis will be more suitable to solve this kind of problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-25T10:12:02.210" Id="65546" LastActivityDate="2013-07-25T16:10:09.853" OwnerUserId="28450" PostTypeId="1" Score="-2" Tags="&lt;statistical-significance&gt;&lt;algorithms&gt;" Title="How to Analyse and predict" ViewCount="43" />
  
  <row AcceptedAnswerId="65561" AnswerCount="1" Body="&lt;p&gt;The problem is how best to analyse data which involves multiple dependent measures, taken on multiple/repeated occasions and determine whether there's a difference between two groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to analyse data from a study involving cochlear implants. Now a cochlear implant has an array with multiple electrodes, and for each electrode it's possible to measure various different parameters. These parameters can be, and are, typically measured over time for the purpose of understanding how the parameter changes through time. Per good study design, in the problem I'm considering there's the control group and the experimental group. To note is that there's no cross-over element in the design.&lt;/p&gt;&#10;&#10;&lt;p&gt;To be specific:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Two groups of N test subjects: control and experimental&lt;/li&gt;&#10;&lt;li&gt;12 different values for any &quot;measurement set&quot;, from a single person at a single point in time&lt;/li&gt;&#10;&lt;li&gt;Multiple measurement points (in time), the repeated measures part&lt;/li&gt;&#10;&lt;li&gt;Does the control and experimental group differ, generally?&lt;/li&gt;&#10;&lt;li&gt;Is there a significant difference at the different time points?&lt;/li&gt;&#10;&lt;li&gt;Are there any differences between the 12 values through time?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thus my questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;My thought is that this is a repeated measures MANOVA analysis type.&#10;Have I understood the situation correctly?&lt;/li&gt;&#10;&lt;li&gt;How should the measurements at multiple time points be taken into account? Or maybe this is catered for with the (multiple) repeated nature of the analysis.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-25T11:07:11.053" Id="65551" LastActivityDate="2013-07-25T13:49:16.333" LastEditDate="2013-07-25T13:08:25.167" LastEditorUserId="28451" OwnerUserId="28451" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;manova&gt;" Title="ANOVA type for dependent samples through time" ViewCount="56" />
  <row Body="&lt;p&gt;McNemar-Bowker test of symmetry of k X k contingency table is inherently 2-sided: the alternative hypothesis is undirected. So, in &lt;em&gt;general&lt;/em&gt; case it cannot be used to test a one sided alternative that subdiagonal frequencies are larger/smaller than superdiagonal frequencies. But since in your case the differences are consistently in favour of subdiagonal frequencies you can use the test for the directional inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Bowker test is chi-square asymptotic-based and hence is for &quot;large sample&quot; - I've read somewhere (sorry don't remember where, so I'm not quite sure) that the sum in any two symmetric cells, if it is not 0 (the test ignores 0-0 cell pairs altogether), should be at least 10. Clearly, this isn't your case - you have only one pair of symmetric cells with the large sum. There exists an exact version of the test (&lt;a href=&quot;http://library.wolfram.com/infocenter/MathSource/7634/&quot; rel=&quot;nofollow&quot;&gt;see&lt;/a&gt;) but not in SPSS. But you can bypass the problem if you merge &quot;Once&quot;, &quot;Twice&quot;, &quot;Three+&quot; categories. Then you'll have the dichotomous case for which Bowker test becomes McNemar test with exact p-value easily computed (SPSS does it).&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want also to consider some &lt;a href=&quot;http://www.fisheries.vims.edu/hoenig/pdfs/Viewing.pdf&quot; rel=&quot;nofollow&quot;&gt;alternative tests&lt;/a&gt; of symmetry of a contingency table. Because it is questionnable whether your inquiry is isomorphic to what McNemar-Bowker tests. It tests if &lt;em&gt;every&lt;/em&gt; off-diagonal cell is equal (in population) to the cell symmetric to it. Might it be that comparing the subdiagonal and the superdiagonal &lt;em&gt;sums&lt;/em&gt; is more apt here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-25T11:32:10.920" Id="65552" LastActivityDate="2013-07-25T12:12:11.497" LastEditDate="2013-07-25T12:12:11.497" LastEditorUserId="3277" OwnerUserId="3277" ParentId="65479" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am applying logistic regression and I am using a mixture of continuous and binary features. &#10;My question would be how do you go about binary features when trying to normalize.&#10;I would very much appreciate any help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-25T13:25:23.253" Id="65558" LastActivityDate="2013-07-25T15:19:47.360" LastEditDate="2013-07-25T15:19:47.360" LastEditorUserId="28454" OwnerUserId="28454" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;normalization&gt;&lt;binary&gt;" Title="Normalize binary features for logistic regression" ViewCount="179" />
  
  <row Body="&lt;p&gt;I agree with the post above, but in situations like these, one method I like to use involves permuting the data 1000 times, reclustering them each time by the same metrics, and selecting the number of observed clusters at a tree height that significantly exceeds all tree heights identified from permutations (&lt;em&gt;P&lt;/em&gt; &amp;lt; 0.001).&lt;/p&gt;&#10;&#10;&lt;p&gt;Something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;d &amp;lt;- dist(mydata, method = &quot;euclidean&quot;) &#10;fit &amp;lt;- hclust(d,method=&quot;ward&quot;)&#10;&#10;clusters&amp;lt;-c()&#10;for (i in 1:1000){&#10;km.rand &amp;lt;- t(apply(mydata,1,sample))&#10;d &amp;lt;- dist(km.rand, method = &quot;euclidean&quot;) &#10;fit2 &amp;lt;- hclust(d,method=&quot;ward&quot;)&#10;clusters[i]&amp;lt;-length(which((fit$height-fit2$height)&amp;gt;0))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that in the above code I'm only permuting the columns, not the rows, because those were the variables for which I was interested in determining the number of clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps as a possible solution to your problem!&lt;/p&gt;&#10;&#10;&lt;p&gt;Ron&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-25T14:23:40.363" Id="65569" LastActivityDate="2013-07-25T14:23:40.363" OwnerUserId="25979" ParentId="65563" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using &lt;code&gt;cut()&lt;/code&gt; to transform data like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 10&#10;table(cut(iris$Sepal.Length,quantile(iris$Sepal.Length,probs=seq(0,1,1/N))))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, I do have many variables and I would like to determine what is the maximum acceptable &lt;code&gt;N&lt;/code&gt; for variable. I thought this will be linked to unique values, but it is not that easy:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;length(unique(iris$Sepal.Length)) # 35 unique values&#10;    N &amp;lt;- 19&#10;    ## Fails if N &amp;gt; 19&#10;    table(cut(iris$Sepal.Length,quantile(iris$Sepal.Length,probs=seq(0,1,1/N))))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there a way how to calculate &lt;code&gt;N&lt;/code&gt; or do I have to find this number iteratively?&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe this will be simpler case to work with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 4&#10;x &amp;lt;- c(1,2,2,2,2,2,2,2,3,3,3,3,4,5,6,7,8)&#10;table(cut(x,quantile(x,probs=seq(0,1,1/N))))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2013-07-25T18:26:18.507" Id="65590" LastActivityDate="2013-07-25T18:26:18.507" OwnerUserId="28311" PostTypeId="1" Score="0" Tags="&lt;r&gt;" Title="Determine minimum acceptable step in cut()" ViewCount="51" />
  
  <row Body="&lt;p&gt;The solution to your problem is as Rob points out is to combine deterministic effects (week of the year) and stochastic effects (ARIMA structure) while isolating unusual days and detecting the possible presence of one or more level shifts and/or one or more local time trends. AUTOBOX , the software used for the analysis was in part developed by me to automatically provide robust modeling for data sets like this.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have placed your data at &lt;a href=&quot;http://www.autobox.com/weather/weather.txt&quot; rel=&quot;nofollow&quot;&gt;http://www.autobox.com/weather/weather.txt&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The acf of the original data is &lt;img src=&quot;http://i.stack.imgur.com/MBBuF.jpg&quot; alt=&quot;enter image description here&quot;&gt; which lead to an automatic model selection of the form &lt;img src=&quot;http://i.stack.imgur.com/Rbedh.jpg&quot; alt=&quot;enter image description here&quot;&gt; &lt;img src=&quot;http://i.stack.imgur.com/8d3DP.jpg&quot; alt=&quot;enter image description here&quot;&gt; &lt;img src=&quot;http://i.stack.imgur.com/x5rQf.jpg&quot; alt=&quot;enter image description here&quot;&gt; . The model statistics are &lt;img src=&quot;http://i.stack.imgur.com/WgMdq.jpg&quot; alt=&quot;enter image description here&quot;&gt; with a residual plot of &lt;img src=&quot;http://i.stack.imgur.com/BZMSy.jpg&quot; alt=&quot;enter image description here&quot;&gt; The plot of the forecasts for the next 60 days is presented here &lt;img src=&quot;http://i.stack.imgur.com/zaU4a.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;THe Actual/Fit/Forecast graph is shown here .&lt;img src=&quot;http://i.stack.imgur.com/2sSOJ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It might be interesting for others to follow Prof. Hyndaman's advice and to report their final model with disgnostic checks regarding residual diagnostics and parameter tests of significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am personally uncomfortable with the suggestion about first performing a fourier analysis (possibly/probably impacted by anomalies) and then doing ARIMA on the residuals is unacceptable as it is not a simultaneous solution leading to 1 equation but rather a presumptive sequence. My equation use week-of-the-month and also included an AR(1) and remedies for the unusual data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;All software has limitations and it is good to know them. Again I reiterate why doesn't somebody try to implement Rob's suggestions and show the complete results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-25T23:52:21.137" Id="65615" LastActivityDate="2013-07-26T12:50:52.617" LastEditDate="2013-07-26T12:50:52.617" LastEditorUserId="3382" OwnerUserId="3382" ParentId="65585" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I am testing a few covariates in generalized linear model in SPSS. When the omnibus test comes out as non-significant, does that mean the model is not significant even if the my covariates came out as significant on the &quot;test of model effects&quot;? Thanks. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-26T01:47:23.197" Id="65619" LastActivityDate="2013-07-26T01:47:23.197" OwnerUserId="28481" PostTypeId="1" Score="2" Tags="&lt;generalized-linear-model&gt;" Title="Omnibus test in generalized linear model in SPSS" ViewCount="200" />
  
  
  
  <row Body="&lt;p&gt;A boosted classifier can still overfit. With boosting as you add more base learners to the classifier you can (potentially) keep decreasing the training set error rate. Eventually this training set error rate will bottom out at a minimum value (possibly 0, possibly higher). The point being made is that even after the training set error has reached its minimum, adding more base learners to the classifier can cause the error of the classifier on an unseen test set to decrease (even though the error on the training set is stable).&lt;/p&gt;&#10;&#10;&lt;p&gt;Having said all that, it is possible that the improvement you see on the training/test set is due to the classifier overfitting. For example, you may have a dataset that is poorly representative of the potential range of all data points. In this case your boosted classifier could reach 0% error rate, but still be overfit as it has poor generalisation capabilities. Alternatively, if the base learners you use are inappropriate for the problem you may end up with all the base learners being overfit, and consequently the boosted classifier will possibly be overfit.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-26T16:14:40.680" Id="65667" LastActivityDate="2013-07-26T16:14:40.680" OwnerUserId="28504" ParentId="65660" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You asked two questions &lt;/p&gt;&#10;&#10;&lt;p&gt;1) Is it important that my dataset has missing values in some of the variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is difficult to answer well because &quot;importance&quot; depends on your objectives and what we are missing. It's not clear how we can judge either. The good news is that the fraction of missing values seems fairly small. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) What is the right method to use for filling in those missing values? If it's possible, can you illustrate it using Stata?&lt;/p&gt;&#10;&#10;&lt;p&gt;I doubt there is agreement on a single &quot;right&quot; method here. Given the panel character of the data, you could try anything from numerical interpolation to multiple imputation. Interpolation could use &lt;code&gt;ipolate&lt;/code&gt; (official Stata), &lt;code&gt;cipolate&lt;/code&gt; (SSC), &lt;code&gt;csipolate&lt;/code&gt; (SSC), &lt;code&gt;pchipolate&lt;/code&gt; (SSC), &lt;code&gt;nnipolate&lt;/code&gt; (SSC). Interpolation will inevitably not restore all the variability lost. Multiple imputation is supported by the very extensive &lt;code&gt;mi&lt;/code&gt; suite, but taking account of both cross-sectional and time dependencies is challenging. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-26T21:55:16.897" Id="65697" LastActivityDate="2013-07-26T22:29:20.710" LastEditDate="2013-07-26T22:29:20.710" LastEditorUserId="22047" OwnerUserId="22047" ParentId="65682" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I first summarize the analysis of some papers that I want to imitate. The regression looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Y2(subjects) = Y2(friends) + Y1(subjects) + Y1(friends) + Z(subjects)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where &lt;code&gt;subjects&lt;/code&gt; refers to focal people and &lt;code&gt;friends&lt;/code&gt; refers to those subjects connect to. Some one may be a subject in a relationship and a friend in another. For this question, just skip the validity of this model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is to use the outcome variable of friends at time 2, &lt;code&gt;Y2(friends)&lt;/code&gt;, to explain outcome variable of subjects at that time (&lt;code&gt;Y2(subjects)&lt;/code&gt;), controlling for outcomes of both subjects and friends at time 1. Other covariates of subjects may be included, but it's not the main consideration.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following this model, I think the data should be in the &lt;strong&gt;wide format&lt;/strong&gt;, which looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;SubjID     FrID     Y1Subj     Y1Fr     Y2Subj     Y2Fr     Z&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The authors apply the &lt;strong&gt;generalized estimating equation&lt;/strong&gt; in their analysis. However, I don't know how to perform GEE with the wide data format (let's say &lt;code&gt;PROC GENMOD&lt;/code&gt; in &lt;code&gt;SAS&lt;/code&gt;), if that's the case. Can you tell if I'm missing some thing (e.g., reconstruction of data in some way so that GEE can be used)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-26T23:32:12.783" Id="65708" LastActivityDate="2013-07-27T00:53:59.203" LastEditDate="2013-07-27T00:53:59.203" LastEditorUserId="21599" OwnerUserId="21006" PostTypeId="1" Score="1" Tags="&lt;sas&gt;&lt;gee&gt;" Title="Generalized estimating equation and data format" ViewCount="106" />
  
  <row AnswerCount="2" Body="&lt;p&gt;A ARMA(p,q) process is weakly stationary, iff the root of its AR part is not on the unit circle. So its weak stationarity doesn't depend on its MA part.&#10;But what can the positions of the roots of its MA part imply?&lt;/p&gt;&#10;&#10;&lt;p&gt;In the unit root tests for ARIMA, a unit root of the MA polynomial indicates that the data were overdifferenced. Does it mean that the differenced time series is not weakly stationary? If yes, does it contradict to the earlier fact that the weak stationarity of ARMA doesn't depend on its MA part? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-27T02:17:38.970" FavoriteCount="1" Id="65716" LastActivityDate="2015-03-02T13:37:23.483" OwnerUserId="26905" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;" Title="What is the implication of unit root of MA?" ViewCount="225" />
  <row AnswerCount="0" Body="&lt;p&gt;I am running a GEE on &lt;code&gt;SPSS&lt;/code&gt; as well as &lt;code&gt;Stata&lt;/code&gt; on choice data with the following structure:&lt;/p&gt;&#10;&#10;&lt;p&gt;Each subject completed 32 trials in 4 blocks, 8 trials per block, on which he/she chose between two options. The four blocks represent two within-subjects factors crossed with each other. Subjects are assigned to one of three between-subjects conditions. Thus, I have a 3 x 2 x 2 mixed design. DV is number of times Option 1 is chosen out of the 8 trials. I used a GEE with a binomial logit link and estimated marginal means for choice percentage by condition.&lt;/p&gt;&#10;&#10;&lt;p&gt;Everything else is fine, but I am running into a little question that I do not have an answer to and I hope someone can help. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I need to test the hypothesis that the choice share of Option 1 in a particular condition is different from chance (50%).&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;The pair-wise comparisons only allow me to test differences between conditions and not between a condition and a user-defined value. Can anyone help? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T04:08:23.993" Id="65719" LastActivityDate="2013-07-27T07:02:06.523" LastEditDate="2013-07-27T07:02:06.523" LastEditorUserId="805" OwnerUserId="28525" PostTypeId="1" Score="2" Tags="&lt;spss&gt;&lt;stata&gt;&lt;gee&gt;&lt;contrasts&gt;" Title="User-defined contrast on Generalized Estimating Equations" ViewCount="83" />
  <row AnswerCount="0" Body="&lt;p&gt;Say I have a model such that the dependent variable is the change in the log of $y$, $\Delta ln(y)$ and the independent variable is the change in $x$, $\Delta x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say the coefficient is equal to -1, $y$ is the population of a state and $x$ is the crime rate (the number of crimes per person).&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I say that for a 10% increase in the crime rate, the model predicts a 1% decline (approximately) in the population?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or should I say that for a 1% increase in the crime rate, the model predicts a&#10;1% decline (approximately) in the population?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or should I say that for each additional crime, the model predicts a 1 person decline in the population?&lt;/p&gt;&#10;" ClosedDate="2013-07-28T21:58:15.413" CommentCount="4" CreationDate="2013-07-27T06:44:33.373" Id="65723" LastActivityDate="2013-07-27T08:10:23.513" LastEditDate="2013-07-27T08:10:23.513" LastEditorUserId="26338" OwnerUserId="28413" PostTypeId="1" Score="0" Tags="&lt;interpretation&gt;&lt;regression-coefficients&gt;&lt;log-linear&gt;" Title="Correct interpretation of coefficient when dependent variable is change in log of y and independent variable is change in x?" ViewCount="13" />
  <row Body="&lt;p&gt;If we approximate the log-scale logistic function $1/(1 + \exp(-B(t - \log C))$ with the linear function $1/2 + (t - \log C)B/4$, the approximation intersects the asymptotes at $t = \log C \pm 2/B$. Translating that to the $x$-scale puts the &quot;corners&quot; of the function at $x = C \exp(\pm2/B)$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T07:49:32.713" Id="65726" LastActivityDate="2013-07-27T07:49:32.713" OwnerUserId="20776" ParentId="44474" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;This won't always work, but it may be worth a try. First compute thresholds $t_1,…,t_N$, where $\newcommand{\erfcinv}[1]{\operatorname{erfcinv}\left(#1\right)}&#10;t_i = \sqrt{2} \erfcinv{2p_i}$, so that $\Pr(Z &amp;gt; t_i) = p_i$, where $Z$ is a standard normal random variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then for each pair of variables $(i,j), 1 \leq i &amp;lt; j \leq N$, solve for the tetrachoric correlation $r_{ij}$, where $\int_0^{r_{ij}} f(t_i,t_j,\rho) \mathrm{d}\rho =$ the known covariance of variables $i$ and $j$, and $f(x,y,\rho)$ is the bivariate standard normal pdf with correlation $\rho$.&lt;br&gt;&#10;(Note: the integral, which must be evaluated numerically, is more tractable with the change of variable $\rho = \sin \theta$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If the matrix $R$ of tetrachoric correlations is positive definite then you're in business. Let $F$ be any matrix that gives $FF' = R$. Then for each desired binary vector $x$, generate a vector $z$ of $N$ independent standard normals, let $y = Fz$, and for $i = 1,…,N$ let $x_i = 1 if y_i &amp;gt; t_i$, and $x_i = 0$ otherwise.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-07-27T09:46:46.207" Id="65733" LastActivityDate="2013-07-27T21:55:20.793" LastEditDate="2013-07-27T21:55:20.793" LastEditorUserId="20776" OwnerUserId="20776" ParentId="65706" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm using Kruskal-Wallis to as a non-parametric ANOVA of a non-normal distribution.  My understanding is that it assumes each group of the independent variable has the same shape.  As it turns out, three of my four groups have approximately the same shape, however the fourth does not.  The fourth is roughly the inverse of the first two. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this invalidate significance?  Are there any steps I should take to mitigate its impact?&lt;/p&gt;&#10;&#10;&lt;p&gt;Update based on comment and further steps taken:&lt;/p&gt;&#10;&#10;&lt;p&gt;@Miroslav Sabo pointed me to a similar question he asked about using Mann-Whitney when certain assumptions are not met.  That answer suggested welch's correction.  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-27T10:00:07.913" Id="65735" LastActivityDate="2014-04-26T18:34:10.820" LastEditDate="2013-07-27T13:18:29.407" LastEditorUserId="22047" OwnerUserId="28513" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;kruskal-wallis&gt;" Title="ANOVA / Kruskal-Wallis: one of four groups has different distribution" ViewCount="303" />
  <row AcceptedAnswerId="65752" AnswerCount="4" Body="&lt;p&gt;Assuming a test where &lt;em&gt;p&lt;/em&gt; &gt; alpha and &lt;em&gt;n&lt;/em&gt; is large enough for power &gt; 95% at effect size &lt;em&gt;d&lt;/em&gt;, what is the &lt;em&gt;exact&lt;/em&gt; interpretation of the test regarding the relationship between the observed data, the real effect, and power for &lt;em&gt;d&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Some details: if my &lt;em&gt;p&lt;/em&gt; is larger than my alpha, that means the observed data are not surprising from the perspective of a nil-null hypothesis. But according to Cohen (1990, p. 1309), the failed test, in combination with an estimate of power given a &lt;em&gt;d&lt;/em&gt;, also allows me to estimate something similar to, but not &lt;em&gt;actually&lt;/em&gt; the following statement: based on my sample, the real population effect is likely (where &quot;likely&quot; is somehow related to my 95% power) as close or closer to no effect than the &lt;em&gt;d&lt;/em&gt; I have calculated my power for (not the d I have measured). However, I am not aware of a precise definition, and this statement is definitely false since it interprets the data from the perspective of p(H|D) and not p(D|H)…&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for a statement comparable to “given a &lt;em&gt;p&lt;/em&gt; value below alpha, assuming a zero effect, observing data as or more extreme than the evaluated sample obtained has a lower probability than alpha”, but from the other direction.&lt;/p&gt;&#10;&#10;&lt;p&gt;One perspective on this comes from a CI: if my CI is narrow (due to high power) and includes zero, I know that only for a small range of hypotheses centred around and including zero, the probability of obtaining the current (or a smaller) measure would be less surprising than my alpha; conversely, for all hypotheses assuming an effect outside of my CI, the data would be surprising. But I am not sure how to phrase this in reference to power.&lt;/p&gt;&#10;&#10;&lt;p&gt;Admittedly, this question could probably be answered by reading Cohen 1988, however, I do not have the book with me. Also, I assume this problem is commonly enough misunderstood. I would be happy with a pointer to an authoritative source, too.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T12:21:48.813" Id="65744" LastActivityDate="2013-07-28T17:46:56.947" LastEditDate="2013-07-27T13:37:24.307" LastEditorUserId="28288" OwnerUserId="28288" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;&lt;interpretation&gt;&lt;power&gt;&lt;cohens-d&gt;" Title="Specific interpretation of non-significant test considering power and effect size" ViewCount="274" />
  <row Body="&lt;p&gt;The short answer is yes. With your training/test split you have two basic choices for performing feature selection:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Train your algorithm on the training set using a feature subset, and then test the performance on the training set.&lt;/li&gt;&#10;&lt;li&gt;Train your algorithm on the training set using a feature subset, and then test the performance on the test set.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;With 1 you can clearly suffer from overfitting as you're training and testing on the same dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;With 2 you train and test on independent datasets, and therefore avoid overfitting in a way that approach 1 doesn't. However, the problem is that you evaluate your feature subsets by repeated testing against the test set. This means that you run the risk of one of your subsets happening to perform very well on the test set by chance (similar to the problem of multiple testing when determining p values).&lt;/p&gt;&#10;&#10;&lt;p&gt;I suppose ideally you would so something like take multiple samples of your training set (e.g. multiple random splits of it into training and test sets) to determine the best feature subset. After that you could train on your entire training set using your feature subset, and then test on your test set. Something like this means that your test set has never been seen before, and therefore the feature selection could not have specifically selected features for good performance on the test set. I found &lt;a href=&quot;http://www.pnas.org/content/99/10/6562.full&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; paper (along with others that reference it) to be the most helpful when I was learning about this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T14:22:51.640" Id="65755" LastActivityDate="2013-07-27T14:22:51.640" OwnerUserId="28504" ParentId="65748" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;First I should explain what I did, and it might not be right. &#10;I have a variable that represents a test outcome, it might be positive or negative. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;I have a set of observations of one important variable (my point of interest) for the last 5 days before the test was undertaken. I have computed the average for the last 3 days and all the 5 days. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Then, I have some other (not so important to me) variables, some of them are binary (yes or no), some of them are continuous. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I want to create a logit model for my DV based on these variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;As the histograms dont seem normal enough, I have used Mann-Whitney-U on all the variables (with the outcome of the test as a grouping variable) and have seen that the test is the most significant for the 5-day average (the second most significant was the day 1 before the test was undertaken). So, I have chosen the 5-day average for an univariate logit model, and it was significant. Then I put all the other variables into the model and ran a stepwise model selection in R based on AIC. Now I have a model that contains 5 variables - the 5 day average is highly significant and two other variables, still significant, but there are also two variables that are non-significant at my chosen level (0.05). I have two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is the process of selecting one version of the &quot;important&quot; variable for the model  acceptable as I did it?&lt;/li&gt;&#10;&lt;li&gt;How do I interpret the two non-significant variables? They have not been shown to have a significant influence on the DV, but in a model without them, the other variables become insignificant. Can I actually use a model that contains non-significant values? Or can I say, that the model fits the available data well, and the two variables contribute to the model but are not significant at my chosen level. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T15:33:49.417" Id="65758" LastActivityDate="2013-07-27T16:56:34.537" LastEditDate="2013-07-27T16:56:34.537" LastEditorUserId="930" OwnerUserId="28537" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;logistic&gt;&lt;stepwise-regression&gt;" Title="Interpretation of insignificant predictors in logistic regression model" ViewCount="442" />
  
  
  
  <row Body="&lt;p&gt;The oldest form of growth curve modeling is generalized least squares which assumes multivariate normality.  Yes it can handle baseline covariates just as ANCOVA does [ANCOVA is a special case of generalized least squares], and if you measure the right variables and model them properly (e.g., allowing for nonlinearities) you will get an adequate adjustment for baseline imbalance in a nonrandomized study.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T22:37:35.803" Id="65777" LastActivityDate="2013-07-27T22:37:35.803" OwnerUserId="4253" ParentId="65769" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You can use the same preprocessing that makes your distance function &quot;work&quot; for other tasks than clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hierarchical clustering doesn't use your actual data. It only uses distances.&lt;/p&gt;&#10;&#10;&lt;p&gt;So data normalization will not affect hierarchical clustering, but it will likely affect your distance function. So make sure your preprocessing yields sensibe distance values.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-27T23:53:33.590" Id="65780" LastActivityDate="2013-07-27T23:53:33.590" OwnerUserId="7828" ParentId="65762" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;In a statistics forum it is perhaps too obvious to underline that such numbers are often called probabilities, although such numbers aren't always probabilities. &lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of English usage, proportion and fraction are also often used for such numbers. A fraction, however, can certainly exceed 1.  17/12 is such a fraction. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-28T10:09:57.517" Id="65806" LastActivityDate="2013-07-28T10:09:57.517" OwnerUserId="22047" ParentId="65753" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There is a technique invented by Trevor Hastie and Werner Stuetzle called &lt;em&gt;principal curves&lt;/em&gt;, which is a nonlinear generalisation of principal components.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the abstract for the original paper:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Principal curves are smooth one-dimensional curves that pass through the middle of a p-dimensional data set, providing a nonlinear summary of the data. They are nonparametric, and their shape is suggested by the data. The algorithm for constructing principal curves starts with some prior summary, such as the usual principal-component line. The curve in each successive iteration is a smooth or local average of the p-dimensional points, where the definition of local is based on the distance in arc length of the projections of the points onto the curve found in the previous iteration.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Some links:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.iro.umontreal.ca/~kegl/research/pcurves/&quot;&gt;http://www.iro.umontreal.ca/~kegl/research/pcurves/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hastie &amp;amp; Stuetzle's original paper: &lt;a href=&quot;http://www.stanford.edu/~hastie/Papers/Principal_Curves.pdf&quot;&gt;http://www.stanford.edu/~hastie/Papers/Principal_Curves.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A recent paper in the Journal of Machine Learning Research: &lt;a href=&quot;http://jmlr.org/papers/volume12/ozertem11a/ozertem11a.pdf&quot;&gt;http://jmlr.org/papers/volume12/ozertem11a/ozertem11a.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-28T16:14:41.097" Id="65830" LastActivityDate="2013-07-28T16:14:41.097" OwnerUserId="1569" ParentId="65829" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;Stratified sampling means that the class membership distribution is preserved in your KFold sampling. This doesn't make a lot of sense in the multilabel case where your target vector might have one than one label per observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's two possible interpretations of stratified in this sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n$ labels where at least one of them is filled that gives you $\sum\limits_{i=1}^n2^n$ unique labels. You could perform stratified sampling on the each of the unique label bins.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other option is to try and segment the training data s.t. that probability mass of the distribution of the label vectors is approximately the same over the folds. E.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;&#10;np.random.seed(1)&#10;y = np.random.randint(0, 2, (5000, 5))&#10;y = y[np.where(y.sum(axis=1) != 0)[0]]&#10;&#10;&#10;def proba_mass_split(y, folds=7):&#10;    obs, classes = y.shape&#10;    dist = y.sum(axis=0).astype('float')&#10;    dist /= dist.sum()&#10;    index_list = []&#10;    fold_dist = np.zeros((folds, classes), dtype='float')&#10;    for _ in xrange(folds):&#10;        index_list.append([])&#10;    for i in xrange(obs):&#10;        if i &amp;lt; folds:&#10;            target_fold = i&#10;        else:&#10;            normed_folds = fold_dist.T / fold_dist.sum(axis=1)&#10;            how_off = normed_folds.T - dist&#10;            target_fold = np.argmin(np.dot((y[i] - .5).reshape(1, -1), how_off.T))&#10;        fold_dist[target_fold] += y[i]&#10;        index_list[target_fold].append(i)&#10;    print(&quot;Fold distributions are&quot;)&#10;    print(fold_dist)&#10;    return index_list&#10;&#10;if __name__ == '__main__':&#10;    proba_mass_split(y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To get the normal training, testing indices that KFold produces you want to rewrite that to it returns the np.setdiff1d of each index with np.arange(y.shape[0]), then wrap that in a class with an &lt;strong&gt;iter&lt;/strong&gt; method.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-07-28T19:36:00.870" Id="65840" LastActivityDate="2013-07-30T00:03:04.767" LastEditDate="2013-07-30T00:03:04.767" LastEditorUserId="9568" OwnerUserId="9568" ParentId="65828" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;An appropriate test statistic used to measure the degree of association between binomial paired variables is the log &lt;a href=&quot;http://en.wikipedia.org/wiki/Odds_ratio&quot; rel=&quot;nofollow&quot;&gt;odds ratio&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The odds ratio expresses how much more likely there is to be a success in one case given a success in the other. for more info on odds ratios see &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/18238982&quot; rel=&quot;nofollow&quot;&gt;Grimes &amp;amp; Schulz 2008&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;the important thing is that it is equal to one &lt;a href=&quot;http://en.wikipedia.org/wiki/If_and_only_if&quot; rel=&quot;nofollow&quot;&gt;iff&lt;/a&gt; the two variables are independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;taking the log of the ratio makes it symmetrical around zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;As described in &lt;a href=&quot;http://www.crcpress.com/product/isbn/9781439808184&quot; rel=&quot;nofollow&quot;&gt;Testing Statistical Hypotheses of Equivalence and Noninferiority&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/62607/test-if-two-samples-are-equal&quot;&gt;here&lt;/a&gt;, to demonstrate equivalence one has to demonstrate (at a certain alpha) that the estimate of your statistic lies within the a predefined equivalence margin of the value you would expect for perfect equivalence.&lt;/p&gt;&#10;&#10;&lt;p&gt;The equivalence margin is the difference you would consider not &lt;em&gt;practically different&lt;/em&gt;, and does not have a spesific value. It will depend on your spesific case and you will have to exercise some judgement..&lt;/p&gt;&#10;&#10;&lt;p&gt;For the log odds ratio &lt;a href=&quot;http://www.crcpress.com/product/isbn/9781439808184&quot; rel=&quot;nofollow&quot;&gt;wellek&lt;/a&gt; suggests 0.41 and 0.85 as strict and liberal bounds respectively&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-28T20:33:08.020" Id="65843" LastActivityDate="2013-07-28T20:33:08.020" OwnerUserId="20009" ParentId="48466" PostTypeId="2" Score="1" />
  
  <row AnswerCount="4" Body="&lt;pre&gt;&lt;code&gt;plot(density(rexp(100))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Obviously all density to the left of zero represents bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking to summarize some data for non-statisticians, and I want to avoid questions about why non-negative data has density to the left of zero.  The plots are for randomization checking; I want to show the distributions of variables by treatment and control groups. The distributions are often exponential-ish.  Histograms are tricky for various reasons.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A quick google search gives me work by statisticians on non-negative kernels, e.g.:  &lt;a href=&quot;http://www.ism.ac.jp/editsec/aism/pdf/052_3_0471.pdf&quot;&gt;this&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But has any of it been implemented in R?  Of implemented methods, are any of them &quot;best&quot; in some way for descriptive statistics?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:  even if the &lt;code&gt;from&lt;/code&gt; command can solve my current problem, it'd be nice to know whether anyone has implemented kernels based on literature on non-negative density estimation&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-07-29T06:57:18.640" FavoriteCount="7" Id="65866" LastActivityDate="2013-09-27T19:52:07.913" LastEditDate="2013-07-29T08:13:19.800" LastEditorUserId="17359" OwnerUserId="17359" PostTypeId="1" Score="21" Tags="&lt;r&gt;&lt;kernel&gt;&lt;pdf&gt;&lt;gamma-distribution&gt;" Title="Good methods for density plots of non-negative variables in R?" ViewCount="1978" />
  
  <row AcceptedAnswerId="65881" AnswerCount="1" Body="&lt;p&gt;I have two data series containing 132 log-returns. One is for EURUSD, the other is for NZDUSD. The &lt;code&gt;head()&lt;/code&gt; function shows you how some of the data looks. The correlation coefficient between the two, as calculated by &lt;code&gt;cor()&lt;/code&gt; is $0.5178912$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To get a better sense of the correlation coefficient I bootstrap it by running &lt;code&gt;cor()&lt;/code&gt; 1000 times on different 132 long samples. I run this in a loop and update &lt;code&gt;euro.nzd.corr&lt;/code&gt; on every iteration. This is the R code I'm using:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;head(euro)&#10;[1] -0.001257862 -0.011637970  0.002428757  0.003602590 -0.003457319 -0.002012728&#10;head(nzd)&#10;[1]  0.008773255 -0.007744927  0.005498693  0.005642524 -0.000896363  0.003449576&#10;cor(euro,nzd)&#10;[1] 0.5178912&#10;euro.nzd.corr &amp;lt;- numeric(1000)&#10;for(i in 1:1000){&#10;euro.nzd.corr[i] = cor(euro[sample(132,132,replace=TRUE)],nzd[sample(132,132,replace=TRUE)])&#10;}&#10;plot(density(euro.nzd.corr), lwd=3, col=&quot;steelblue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Once I have the data, I plot the density chart, and get this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/7IRIENi.png&quot; alt=&quot;density&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Bootstrapped data has mean $\approx 0$ and mostly spreads between $-0.3$ and $0.3$. Where has the initial &lt;code&gt;cor()&lt;/code&gt; result of $0.5178912$ gone? What am I to make of this? That it is better to conclude the two variables are uncorrelated versus correlated with a coefficient of $\approx 0.52$? Have I made any coding mistakes, or is the applied methodology simply flawed?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-29T09:41:05.277" Id="65880" LastActivityDate="2014-01-27T16:51:21.210" OwnerUserId="27435" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;bootstrap&gt;" Title="Why does the bootstrapped correlation revolve around zero while the original correlation $\approx 0.52$?" ViewCount="243" />
  <row AcceptedAnswerId="65904" AnswerCount="1" Body="&lt;p&gt;Probably this is a very basic question, but is it possible to compute the full depth of a decision tree given the sample size of the training set, number of its categories, and the feature dimension?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-29T13:17:20.150" Id="65893" LastActivityDate="2013-07-29T14:13:31.287" LastEditDate="2013-07-29T13:36:05.517" LastEditorUserId="88" OwnerUserId="28586" PostTypeId="1" Score="1" Tags="&lt;classification-tree&gt;" Title="Maximal depth of a decision tree" ViewCount="600" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a question related to multiple regression and interaction, inspired by this CV thread: &lt;a href=&quot;http://stats.stackexchange.com/questions/64949/&quot;&gt;Interaction term using centered variables hierarchical regression analysis? What variables should we center?&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;When checking for a moderation effect I do center my independent variables and multiply the centered variables in order to calculate my interaction term. Then I run my regression analysis and check for main and interaction effects, which may show the moderation.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I redo the analysis without centering, apparently the coefficient of determination ($R^2$) does not change but the regression coefficients ($\beta$s) do. That seems clear and logical.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I do not understand: The p-values of the main effects change substantially with centering, although the interaction does not (which is right). So my interpretation of main effects could change dramatically - just determined by centering or not. (It's is still the same data, in both analyses!)&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody clarify? - Because that would mean that the option to center my variables would be mandatory and everybody should do it in order to get the same results with the same data.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Thanks a lot for distributing to that problem and your comprehensive explanations. Be assured that your help is very much appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;For me, the biggest advantage of centering is to avoid multicollinearity. It is still quite confusing to establish a rule, whether to center or not. My impression is, that most of the resources suggest to center, although there are some &quot;risks&quot; when doing it.&#10;Again I want to put out the fact, that 2 researchers dealing with the same material and data might conclude different results, because one does centering and the other does not. I just read some part of a book by Bortz (he was a Professor and kind of a Statistics Star in Germany and Europe), and he does not even mention that technique; just points out to be careful in interpreting main effects of variables when they are involved in interactions.&lt;/p&gt;&#10;&#10;&lt;p&gt;After all, when you conduct a regression with one IV, one moderator (or second IV) and a DV, would you recommend to center or not?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-07-29T13:46:05.980" FavoriteCount="4" Id="65898" LastActivityDate="2013-07-30T17:47:32.077" LastEditDate="2013-07-30T17:47:32.077" LastEditorUserId="17230" OwnerUserId="28587" PostTypeId="1" Score="9" Tags="&lt;regression&gt;&lt;interaction&gt;&lt;centering&gt;" Title="Why could centering independent variables change the main effects with moderation?" ViewCount="3777" />
  
  
  
  <row Body="&lt;p&gt;Partial R² is what you should be looking for. The partial R² is the proportion of variance explained in the dependent variable by a given predictor, controlling for the other predictors in the model. The partial R²s can be compared to establish the relative strength of the predictors in your model. Alternatively, you can be looking at &lt;em&gt;standardized&lt;/em&gt; regression coefficients, which represent the expected change in the dependent variable for each increase of 1 standard deviation on the predictor. The standardized coefficients are often used to compare the relative strength of predictors in a single model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;em&gt;unstandardized&lt;/em&gt; regression coefficients, which are what you are looking at in your question, represent the expected change in the dependent variable for each 1-unit increase in the predictor, holding all other predictors constant. Therefore, the scaling of each regression coefficient depends in part on the strength of the relationship between the predictor and the dependent variable, &lt;strong&gt;but also on the scaling of the predictor&lt;/strong&gt; (a 1-unit increase on income in dollars does not represent the same change as a 1-unit increase on income in thousands of dollars, for example). Therefore, unless the predictors are all on the same scale (and have the same variability), you should not be comparing the unstandardized regression coefficients with each other as a way to establish the relative predictive power of the predictors in your model.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-29T16:34:19.470" Id="65927" LastActivityDate="2013-07-29T16:52:33.260" LastEditDate="2013-07-29T16:52:33.260" LastEditorUserId="24808" OwnerUserId="24808" ParentId="65919" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Your variables appear to be ordinal. &lt;/p&gt;&#10;&#10;&lt;p&gt;(thought someone should make this the answer, rather than leaving them as comments) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-29T19:07:10.630" Id="65940" LastActivityDate="2013-07-29T19:07:10.630" OwnerUserId="3262" ParentId="65938" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two questions regarding when to use a Bonferroni adjustment:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is it appropriate to use a Bonferroni adjustment in all cases of multiple testing?&lt;/li&gt;&#10;&lt;li&gt;If one performs a test on a data set, then one splits that data set into finer levels (e.g. split the data by gender) and performs the same tests, how might this affect the number of individual tests that are perceived? That is, if X hypotheses are tested on a dataset containing data from both males and females and then the dataset is split to give male and female data separately and the same hypotheses tested, would the number of individual hypotheses remain as X or increase due to the additional testing?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thank you for your comments.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-29T19:16:37.670" FavoriteCount="1" Id="65942" LastActivityDate="2013-07-29T23:12:59.720" OwnerUserId="28592" PostTypeId="1" Score="10" Tags="&lt;multiple-comparisons&gt;&lt;bonferroni&gt;&lt;type-i-errors&gt;" Title="How and when to use the Bonferroni adjustment" ViewCount="1937" />
  <row Body="&lt;p&gt;The Bonferroni adjustment will always provide strong control of the family-wise error rate. This means that, whatever the nature and number of the tests, or the relationships between them, if their assumptions are met, it will ensure that the probability of having even one erroneous significant result among all tests is at most $\alpha$, your original error level. It is therefore always &lt;em&gt;available&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether it is &lt;em&gt;appropriate&lt;/em&gt; to use it (as opposed to another method or perhaps no adjustment at all) depends on your objectives, the standards of your discipline and the availability of better methods for your specific situation. At the very least, you should probably consider the Holm-Bonferroni method, which is just as general but less conservative.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your example, since you are performing several tests, you &lt;em&gt;are&lt;/em&gt; increasing the family-wise error rate (the probability of rejecting at least one null hypothesis erroneously). If you only perform one test on each half, many adjustments would be possible including Hommel's method or methods controlling the false discovery rate (which is different from the family-wise error rate). If you conduct a test on the whole data set followed by several sub-tests, the tests are no longer independent so some methods are no longer appropriate. As I said before, Bonferroni is in any case always available and guaranteed to work as advertised (but also to be very conservative…).&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also just ignore the whole issue. Formally, the family-wise error rate is higher but with only two tests it's still not so bad. You could also start with a test on the whole data set, treated as the main outcome, followed by sub-tests for different groups, uncorrected because they are understood as secondary outcomes or ancillary hypotheses.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you consider many demographic variables in that way (as opposed to just planning to test for gender differences from the get go or perhaps a more systematic modeling approach), the problem becomes more serious with a significant risk of “data dredging” (one difference comes out significant by chance allowing you to rescue an inconclusive experiment with some nice story about the demographic variable to boot whereas in fact nothing really happened) and you should definitely consider some form of adjustment for multiple testing. The logic remains the same with X different hypotheses (testing X hypotheses twice – one on each half of the data set – entails a higher family-wise error rate than testing X hypotheses only once and you should probably adjust for that).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-29T19:39:40.083" Id="65945" LastActivityDate="2013-07-29T23:12:59.720" LastEditDate="2013-07-29T23:12:59.720" LastEditorUserId="17230" OwnerUserId="6029" ParentId="65942" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;If you run KS test for testing normality distribution then you can rely on Central limit theorem. And you can predict normal distribution from 2. -&gt; 3.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-29T20:01:57.103" Id="65947" LastActivityDate="2013-07-29T20:01:57.103" OwnerUserId="28609" ParentId="65925" PostTypeId="2" Score="-3" />
  
  
  <row Body="&lt;p&gt;I haven't used the &lt;code&gt;mi&lt;/code&gt; command yet but I am using the user-written command &lt;code&gt;mim&lt;/code&gt; in Stata for multiple imputed dataset. There are basically two issues in regard to your question. First, Rubin's method can be still used for the regression coefficients in the Heckman model as being used for linear regression. Second, marginal effects can be still combined as per Rubin's method as is mentioned &lt;a href=&quot;http://www.stata.com/statalist/archive/2010-03/msg01021.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. As to why one need to invoke &lt;code&gt;cmdok&lt;/code&gt;, Yulia Marchenko  provides the following reasons (details &lt;a href=&quot;http://www.stata.com/statalist/archive/2009-08/msg00479.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; in Statalist)&lt;/p&gt;&#10;&#10;&lt;p&gt;Commands are officially supported by &lt;code&gt;mi estimate&lt;/code&gt; if they&lt;/p&gt;&#10;&#10;&lt;p&gt;1) produce the numerical results that &lt;code&gt;mi estimate&lt;/code&gt; needs and stores them &#10;        where &lt;code&gt;mi estimate&lt;/code&gt; expects (explained below)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) work properly with available &lt;code&gt;mi estimate&lt;/code&gt; postestimation tools, and&lt;/p&gt;&#10;&#10;&lt;p&gt;3)  produce good-looking &lt;code&gt;mi estimate&lt;/code&gt; output.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-29T22:59:23.743" Id="65959" LastActivityDate="2013-07-29T23:03:25.223" LastEditDate="2013-07-29T23:03:25.223" LastEditorUserId="22047" OwnerUserId="14860" ParentId="65678" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is a normalized version of an exponential family distribution still an exponential family distribution? Here &quot;normalized&quot; means making its mean zero and variance one.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the following definition of an exponential family distribution from &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_family&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;, since both variance and mean are functions of the parameter $\theta$ alone, after normalization wrt variance we still have an exponential family distribution, but after normalization wrt mean $\mu(\theta)$, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;    f_{X'}(x|\theta) = h(x+\mu(\theta))\ \exp[\ \eta(\theta) \cdot T(x+\mu(\theta))\ -\ A(\theta)\ ],&#10;$$&#10;I am not sure.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A single-parameter exponential family is a set of probability distributions whose probability density function (or probability mass function, for the case of a discrete distribution) can be expressed in the form&#10;  $$&#10;    f_X(x|\theta) = h(x)\ \exp[\ \eta(\theta) \cdot T(x)\ -\ A(\theta)\ ]&#10;$$&#10;  where $T(x), h(x), η(θ)$, and $A(θ)$ are known functions.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;An alternative, equivalent form often given is&#10;  $$&#10;    f_X(x|\theta) = h(x)\ g(\theta) \exp[\ \eta(\theta) \cdot T(x)\ ]\,&#10;$$&#10;  or equivalently&#10;  $$&#10;    f_X(x|\theta) = \exp[\ \eta(\theta) \cdot T(x)\ -\ A(\theta) + B(x)\ ]&#10;$$&#10;  The value $θ$ is called the parameter of the family.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-30T11:17:33.607" Id="66000" LastActivityDate="2014-03-15T23:34:08.033" LastEditDate="2014-03-15T23:34:08.033" LastEditorUserId="26338" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;exponential-family&gt;" Title="Is a normalized version of an exponential family distribution still an exponential family distribution?" ViewCount="304" />
  
  
  
  
  
  <row Body="&lt;p&gt;A &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_distribution&quot; rel=&quot;nofollow&quot;&gt;distribution&lt;/a&gt; assigns probabilities to events. For example, the standard normal distribution with density $\mathcal{N}_{0, 1}(x)$ assigns every event $\left[a, b\right]$ the probability&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_a^b \mathcal{N}_{0, 1}(x) \, dx.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_model&quot; rel=&quot;nofollow&quot;&gt;Statistical models&lt;/a&gt; are &lt;strong&gt;families&lt;/strong&gt; of probability distributions. An example of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Parametric_model&quot; rel=&quot;nofollow&quot;&gt;parametric model&lt;/a&gt;  would be a set of Gaussian distributions,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left\{\mathcal{N}_{\mu, \sigma^2} : \mu \in \mathbb{R}, \sigma^2 \in \left]0, \infty\right[\right\}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine the terminology is no different for the Weibull distribution. However, in practice, &quot;distribution&quot; and &quot;model&quot; are often used sloppily and interchangeably.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-30T16:03:17.887" Id="66031" LastActivityDate="2013-07-30T16:03:17.887" OwnerUserId="7733" ParentId="66017" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="66063" AnswerCount="2" Body="&lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Fixed_effects_estimator&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; for fixed effect model:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Consider the linear unobserved effects model for N observations and T time periods:&#10;  $$&#10;    y_{it} = X_{it}\mathbf{\beta}+\alpha_{i}+u_{it} \quad t=1,..,T, \quad i=1,...,N&#10;$$&#10;  where $y_{it}$ is the dependent variable observed for individual $i$ at time $t$, $X_{it}$ is the time-variant $1\times k$ regressor matrix, $\alpha_{i}$ is the unobserved time-invariant individual effect and $u_{it}$ is the error term. Unlike $X_{it}$, $\alpha_{i}$ cannot be observed by the econometrician. Common examples for time-invariant effects $\alpha_{i}$ are innate ability for individuals or historical and institutional factors for countries.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Unlike the Random effects (RE) model where the unobserved $\alpha_{i}$ is independent of $x_{it}$ for all $t=1,...,T$, the FE model &lt;strong&gt;allows $\alpha_{i}$ to be correlated with the regressor matrix $x_{it}$&lt;/strong&gt;. Strict exogeneity, however, is still required.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Since $\alpha_{i}$ is not observable, it cannot be directly controlled for. The FE model eliminates $\alpha_{i}$ by demeaning the variables using the within transformation:&#10;  $$&#10;    y_{it}-\overline{y_{i}}=\left(X_{it}-\overline{X_{i}}\right) \beta+ \left( \alpha_{i} - \overline{\alpha_{i}} \right ) + \left( u_{it}-\overline{u_{i}}\right) = \ddot{y_{it}}=\ddot{X_{it}} \beta+\ddot{u_{it}}&#10;$$&#10;  where $\overline{X_{i}}=\frac{1}{T}\sum\limits_{t=1}^{T}X_{it}$ and $\overline{u_{i}}=\frac{1}{T}\sum\limits_{t=1}^{T}u_{it}$. Since &lt;strong&gt;$\alpha_{i}$ is constant&lt;/strong&gt;, $\overline{\alpha_{i}}=\alpha_{i} $ and hence the effect is eliminated. The FE estimator $\hat{\beta}_{FE}$ is then obtained by an OLS regression of $\ddot{y}$ on $\ddot{X}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;On one hand, it says &quot;the FE model allows $\alpha_{i}$ to be correlated with the regressor matrix $x_{it}$&quot;, which I understand to be that both $\alpha_{i}$ and $x_{it}$ are random variables. On the other hand, it says &quot;$\alpha_{i}$ is constant&quot;, and my previous impression is that  $\alpha_{i}$ is nonrandom in fixed effect model, and random in random effect model. So I was wondering how to correctly understand &quot;fixed effect&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-30T17:41:43.733" Id="66042" LastActivityDate="2013-07-30T21:51:21.230" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;fixed-effects-model&gt;" Title="Is the fixed effect in a fixed effect model a random variable or not?" ViewCount="185" />
  <row Body="&lt;p&gt;The principal components you calculated from your test set are the axes of a two dimensional space into which you projected your training data. You need to project the test set into this space (defined by the principal components of your &lt;em&gt;training&lt;/em&gt; data) to calculate predictions in your model. I don't have weka handy so I can't poke around the specific PCA implementation there, but the basic idea is that you need project the test data into the same space you projected your training data into.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm guessing that the weka output includes a matrix of your eigenvectors. If we call this matrix $M$ and your training data $X_{train}$, you project your training data into this space via:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{train}^*=X_{train} M[,1:2]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Just do the same thing to project your test data onto your principal components: &lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{test}^*=X_{test} M[,1:2]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $M[,1:2]$ is the matrix produced when we strip off all but the first two columns of $M$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-30T19:31:32.970" Id="66050" LastActivityDate="2013-07-31T12:06:24.917" LastEditDate="2013-07-31T12:06:24.917" LastEditorUserId="22047" OwnerUserId="8451" ParentId="66048" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to figure out how SPSS is initializing the multiplicative Holt Winters exponential smoothing model using backcasting. Thankfully IBM roughly &lt;a href=&quot;http://pic.dhe.ibm.com/infocenter/spssstat/v21r0m0/topic/com.ibm.spss.statistics.help/alg_tsmodel_models_exsmooth_estimation_initialization.htm&quot; rel=&quot;nofollow&quot;&gt;described&lt;/a&gt; their way of doing so .. very roughly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming a seasonal period of 3 and 3 seasons (9 observations) for the sake of simplicity. As far as I understand they fit lines for all corresponding observations of all seasons with intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fitting observations 1,4,7 with intercept $µ_1$ and slope $β_1$ .. fitting observations 2,5,8 with intercept $µ_2$ and slope $β_2$ .. and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the initial trend is the average of all slopes. But how exactly are the seasonal indices computed? And isn't the intercept mostly the first observation on a regression line, is it?? (e.g. observation 1 for the first line)&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I guess it will help me a lot if I know more about how backcasting is performed in general. All I found in literature is that the time series is reversed and smoothed values are generated for the season before $t_1$. But how do I get the initial values used for forecasting when I only have compound backcasts?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-07-30T20:42:16.813" FavoriteCount="1" Id="66055" LastActivityDate="2014-02-23T21:38:37.913" LastEditDate="2013-07-31T21:21:16.280" LastEditorUserId="28647" OwnerUserId="28647" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;smoothing&gt;&lt;exponential&gt;" Title="Holt Winters initialization using backcasting like SPSS" ViewCount="365" />
  
  
  
  <row AcceptedAnswerId="66127" AnswerCount="2" Body="&lt;p&gt;I chanced on this article on wikipedia on &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayesian_search_theory&quot; rel=&quot;nofollow&quot;&gt;Bayesian search&lt;/a&gt;. In the mathematics section, it states how the posteriors are estimated. While I understand how $p^{'}$ is calculated, I can't seem to figure out how the $r^{'}$ are updated. How does the posterior on other cells on the grid change given a no-find on a given grid cell? If someone could give any pointers it would be helpful. Thanks &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-31T00:02:13.690" FavoriteCount="2" Id="66072" LastActivityDate="2013-07-31T12:38:02.330" OwnerUserId="13516" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;bayesian&gt;" Title="A question on Bayesian Search" ViewCount="98" />
  <row Body="&lt;p&gt;Dimensionality reduction does not &lt;em&gt;always&lt;/em&gt; lose information. In some cases, it is possible to re-represent the data in lower-dimensional spaces without discarding any information.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you have some data where each measured value is associated with two ordered covariates. For example, suppose you measured signal quality $Q$ (indicated by color white=good, black=bad) on a dense grid of $x$ and $y$ positions relative to some emitter. In that case, your data might look something like the left-hand plot [*1]:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0vXqr.png&quot; alt=&quot;radial averaging demo&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It is, at least superficially, a two dimensional piece of data: $Q(x,y)$. However, we might know &lt;em&gt;a priori&lt;/em&gt; (based on the underlying physics) or assume that the it depends only on the distance from the origin: r = $\sqrt{x^2 + y^2}$. (Some exploratory analysis might also lead you to this conclusion if even the underlying phenomenon isn't well understood). We could then rewrite our data as $Q(r)$ instead of $Q(x,y)$, which would effectively reduce the dimensionality down to a single dimension. Obviously, this is only lossless if the data is radially symmetric, but this is a reasonable assumption for many physical phenomena. &lt;/p&gt;&#10;&#10;&lt;p&gt;This transform $Q(x,y) \rightarrow Q(r)$ is non-linear (there's a square root and two squares!), so it is somewhat different from the sort of dimensionality reduction performed by PCA, but I think it's a nice example of how you can sometimes remove a dimension without losing any information. &lt;/p&gt;&#10;&#10;&lt;p&gt;For another example, suppose you perform a singular value decomposition on some data (SVD is a close cousin to--and the often the underlying guts of--principal components analysis). SVD takes your data matrix $M$ and factors it into three matrices such that $M = USV^{T}$. The columns of U and V are the left and right singular vectors, respectively, which form a set of orthonormal bases for $M$. The diagonal elements of $S$ (i.e., $S_{i,i})$ are singular values, which are effectively weights on the $i$th basis set formed by the corresponding columns of $U$ and $V$ (the rest of $S$ is zeros). By itself, this doesn't give you any dimensionality reduction (in fact, there are now 3 $NxN$ matrices instead of the single $NxN$ matrix you started with). However, sometimes some diagonal elements of $S$ are zero. This means that the corresponding bases in $U$ and $V$ aren't needed to reconstruct $M$, and so they can be dropped. For example, suppose the $Q(x,y)$ matrix above contains 10,000 elements (i.e., it's 100x100). When we perform an SVD on it, we find that only one pair of singular vectors has a non-zero value [*2], so we can re-represent the original matrix as the product of two 100 element vectors (200 coefficients, but you can actually do a bit better [*3]). &lt;/p&gt;&#10;&#10;&lt;p&gt;For some applications, we know (or at least assume) that the useful information is captured by principal components with high singular values (SVD) or loadings (PCA). In these cases, we might discard the singular vectors/bases/principal components with smaller loadings even if they are non-zero, on the theory that these contain annoying noise rather than a useful signal. I've occasionally seen people reject specific components based on their shape (e.g., it resembles a known source of additive noise) regardless of the loading. I am not sure if you would consider this a loss of information or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some neat results about the information-theoretic optimality of PCA. If your signal is Gaussian and corrupted with additive Gaussian noise, then PCA can maximize the mutual information between the signal and its dimensionality-reduced version (assuming the noise has a identity-like covariance structure). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;hr&gt;&#10;Footnotes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;This a cheesy and totally non-physical model. Sorry!&lt;/li&gt;&#10;&lt;li&gt;Due to floating point imprecision, some of these values will be not-quite-zero instead.&lt;/li&gt;&#10;&lt;li&gt;On further inspection, &lt;strong&gt;in this particular case&lt;/strong&gt;, the two singular vectors are the same AND symmetric about their center, so we could actually represent the entire matrix with only 50 coefficients. Note that the first step falls out of the SVD process automatically; the second requires some inspection/a leap of faith. (If you want to think about this in terms of PCA scores, the score matrix is just $US$ from the original SVD decomposition; similar arguments about zeros not contributing at all apply).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2013-07-31T01:14:33.703" Id="66076" LastActivityDate="2013-07-31T18:52:38.083" LastEditDate="2013-07-31T18:52:38.083" LastEditorUserId="7250" OwnerUserId="7250" ParentId="66060" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Longitude and latitude are angles which define points on a sphere so you should probably be looking at the &lt;a href=&quot;http://en.wikipedia.org/wiki/Great-circle_distance&quot; rel=&quot;nofollow&quot;&gt;Great Circle Distance&lt;/a&gt; or other geodesic distances between points rather than the Euclidean distance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also as has been mentioned, certain explicitly model-based clustering algorithms like mixture models and implicitly model-based ones like K-means, make &lt;a href=&quot;http://en.wikipedia.org/wiki/K-means_clustering#Discussion&quot; rel=&quot;nofollow&quot;&gt;assumptions&lt;/a&gt; about the shape and size of the clusters.&#10;In this situation are your expecting your data to fit an underlying model?&#10;If not then density-based methods which don't make assumptions about the shape/size of the clusters might be more appropriate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-31T09:27:03.387" Id="66102" LastActivityDate="2013-07-31T15:25:58.063" LastEditDate="2013-07-31T15:25:58.063" LastEditorUserId="13310" OwnerUserId="13310" ParentId="65411" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;This is a &lt;a href=&quot;http://quant.stackexchange.com/questions/8563/shrinkage-estimator-for-newey-west-covariance-matrix&quot;&gt;cross post&lt;/a&gt;.&#10;I would like to apply the Newey-West covariance estimator for portfolio optmization. Up to lag one it is given by&#10;$$&#10;\Sigma = \Sigma(0) + \frac12 \left (\Sigma(1) + \Sigma(1)^T \right),&#10;$$&#10;where $\Sigma(i)$ is the lag $i$ covariance matrix for $i=0,1$. Furthermore I like to use shrinkage estimators as implemented in the corpcor package for R. The identity matrix as shrinkage prior for $\Sigma(0)$ is plausible.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would you use as prior for $\Sigma(1)$ - the zero-matrix? Do you know an R implementation that allows to estimate lag-covariance matrices using shrinkage? There must be some basic difference as a lag-covariance matrix is not necessarily positive-definite (e.g. the zero-matrix). If I apply shrinkage to $\Sigma(0)$ and use the standard sample-estimator for $\Sigma(1)$ then it is not assured that $\Sigma$ is positive-definite.&lt;/p&gt;&#10;&#10;&lt;p&gt;The above definition is taken from:&lt;/p&gt;&#10;&#10;&lt;p&gt;Whitney K. Newey and Keneth D. West. A simple, positive semi-denite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3):703-708, 1987.&lt;/p&gt;&#10;&#10;&lt;p&gt;It can also be found &lt;a href=&quot;http://arxiv.org/pdf/1009.3638.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; in formula (1.9) on page 6.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-31T09:46:01.267" Id="66103" LastActivityDate="2013-08-05T14:38:53.250" OwnerUserId="12147" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;covariance&gt;&lt;lags&gt;&lt;neweywest&gt;" Title="Shrinkage Estimator for Newey-West Covariance Matrix" ViewCount="150" />
  
  <row Body="&lt;p&gt;As the question is still not answered, here are my 2ct:&lt;br&gt;&#10;I think here are two different topics mixed into this question:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;How can I calculate the sensitivity and specificity (or analogous measures) of a continuous diagnostic test in predicting a continuous outcome (e.g., blood pressure) without dichotomizing the outcome?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I take it that you want to measure the performance of the model. The model predicts continuous (metric) outcome from some kind of input (happens to be metric in your example as well, but that doesn't really matter here). This is a regression scenario, not a classification. So you better look for performance measures for regression models, sensitivity and specificity are &lt;em&gt;not&lt;/em&gt; what you are looking for*.&lt;br&gt;&#10;Some regression problems have a &quot;natural&quot; grouping into presence and absence of something, which gives a link to classification. For that you may have a bimodal distribution: lots of cases with absence, and a metric distribution of values for cases of presence. For example, think of a substance that contaminates some product. Many of the product samples will not contain the contaminant, but for those that do, a range of concentrations is observed. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, this is not the case  for your example of blood pressure (absence of  blood pressure is not a sensible concept here). I'd even guess that blood pressures come in a unimodal distribution. All that points to a regression problem without close link to classification. &lt;/p&gt;&#10;&#10;&lt;p&gt;* &lt;em&gt;With the caveat that both words are used in analytical chemistry for regression (calibration), but with a different meaning: there, the &lt;a href=&quot;http://goldbook.iupac.org/S05606.html&quot; rel=&quot;nofollow&quot;&gt;sensitivity&lt;/a&gt; is the slope of the calibration/regression function, and specific sometimes means that the method is completely &lt;a href=&quot;http://goldbook.iupac.org/S05564.html&quot; rel=&quot;nofollow&quot;&gt;selective&lt;/a&gt;, that is it is insensitive to other substances than the analyte, and no cross-sensitivities occur.&lt;/em&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://goldbook.iupac.org/&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;A. D. McNaught und A. Wilkinson, eds.: Compendium of Chemical Terminology (the&#10;“Gold Book”). Blackwell Scientific, 1997. ISBN: 0-9678550-9-8. DOI: doi:10.1351/&#10;goldbook. URL: http://goldbook.iupac.org/&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Analogues of sensitivity and specificity for continuous outcomes&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;On the other hand, if the underlying nature of the problem is a classification, you may nevertheless find yourself describing it better by a regression:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the regression describes a degree of belonging to the classes (as in fuzzy sets).&lt;/li&gt;&#10;&lt;li&gt;the regression models (posterior) probability of beloning to the classes (as in logistic &lt;em&gt;regression&lt;/em&gt;)&lt;/li&gt;&#10;&lt;li&gt;your cases can be described as mixtures of the pure classes (very close to &quot;normal&quot; regression, the contamination example above) &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For these cases it does make sense to extend the concepts behind sensitivity and specificity to &quot;continuous outcome classifiers&quot;. The basic idea is to weight each case according to its degree of belonging to the class in question. For sensitivity and specificity that refers to the reference label, for the predictive values to the predicted class memberships. It turns out that this leads to a very close link to regression-type performance measures.&lt;/p&gt;&#10;&#10;&lt;p&gt;We recently described this in &#10;&lt;a href=&quot;http://softclassval.r-forge.r-project.org/2013/2013-01-03-ChemomIntellLabSystTheorypaper.html&quot; rel=&quot;nofollow&quot;&gt;C. Beleites, R. Salzer and V. Sergo:&lt;br&gt;&#10;Validation of Soft Classification Models using Partial Class Memberships: An Extended Concept of Sensitivity &amp;amp; Co. applied to Grading of Astrocytoma Tissues&lt;br&gt;&#10;Chemom. Intell. Lab. Syst., 122 (2013), 12 - 22.&lt;/a&gt;&lt;br&gt;&#10;The link points to the home page of the R package implementing the proposed perfromance measures.&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, the blood pressure example IMHO is &lt;em&gt;not&lt;/em&gt; adequately described as classification problem. However, you may still want to read the paper - I think the formulation of the reference values there will make clear that blood pressure is not sensibly described in a way that is suitable for classification.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;(If you formulate a continuous degree of &quot;high blood pressure&quot; that would itself be a model, and a different one from the problem you describe.)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I had only a quick glance at the paper you linked, but if I understood correctly the authors use thresholds (dichotomize) for &lt;em&gt;both&lt;/em&gt; modeling strategies: for the continuous prediction is further processed: a prediction interval is calculated and compared to some threshold. In the end, they have a dichotomous prediction, and generate the ROC by varying the specification for the interval.&lt;br&gt;&#10;As you specify that you want to avoid this, the paper doesn't seem to be overly relevant.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-07-31T10:17:38.150" Id="66107" LastActivityDate="2013-07-31T18:58:38.680" LastEditDate="2013-07-31T18:58:38.680" LastEditorUserId="4598" OwnerUserId="4598" ParentId="49010" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="76112" AnswerCount="2" Body="&lt;p&gt;I have a cloud of points that can be clustered so that each set of points in the cluster can be fitted with an inverse function : $f(x) = cte / x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be an approach for clustering my dataset when discrimining criteria is fit to an inverse curve. I am on the computational side, I am looking for a simple fitting algorithm for a beautiful display with curves that are fitting the data thus demarcating several cluters of points. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-07-31T10:25:04.997" Id="66110" LastActivityDate="2013-11-10T10:56:49.743" OwnerUserId="10812" PostTypeId="1" Score="0" Tags="&lt;data-visualization&gt;&lt;clustering&gt;&lt;data-mining&gt;&lt;curve-fitting&gt;" Title="Clustering cloud fitting inverse functions" ViewCount="66" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a 2-way repeated measures design (3 x 2), and I'd like to get figure out how to calculate effect sizes (partial eta squared).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a matrix with data in it (called a) like so (repeated measures)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         A.a          A.b           B.a        B.b          C.a           C.b&#10;1        514.0479     483.4246      541.1342   516.4149     595.5404      588.8000&#10;2        569.0741     550.0809      569.7574   599.1509     621.4725      656.8136&#10;3        738.2037     660.3058      812.2970   735.8543     767.0683      738.7920&#10;4        627.1101     638.1338      641.2478   682.7028     694.3569      761.6241&#10;5        599.3417     637.2846      599.4951   632.5684     626.4102      677.2634&#10;6        655.1394     600.9598      729.3096   669.4189     728.8995      716.4605&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;idata = &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Caps    Lower&#10;       A       a&#10;       A       b&#10;       B       a&#10;       B       b&#10;       C       a&#10;       C       b&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I know how to do a repeated measures ANOVA with the car package (type 3 SS is standard in my field although I know that it results in a logical error.. if somebody wants to explain that to me like I'm 5 I would love to understand it):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(Anova(lm(a ~ 1),&#10;          idata=idata,type=3, &#10;          idesign=~Caps*Lower)),&#10;    multivariate=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think what I want to do is take this part of the summary print out: &lt;/p&gt;&#10;&#10;&lt;p&gt;Univariate Type III Repeated-Measures ANOVA Assuming Sphericity&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                     SS num Df Error SS den Df        F    Pr(&amp;gt;F)    &#10;(Intercept)     14920141     1   153687      5 485.4072 3.577e-06 ***&#10;Caps            33782        2     8770     10  19.2589  0.000372 ***&#10;Lower           195          1    13887      5   0.0703  0.801451    &#10;Caps:Lower      2481         2      907     10  13.6740  0.001376 ** &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And use it to calculate partial ETA squared. So, if I'm not mistaken, I need to take the SS from the first column and divide it by (itself + SS Error for that row) for each effect. Is this the correct way to go about it? If so, how do I do it? I can't figure out how to reference values from the summary print out.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" ClosedDate="2013-08-10T13:12:11.157" CommentCount="2" CreationDate="2013-07-31T11:48:19.760" Id="66120" LastActivityDate="2013-08-10T11:47:20.543" LastEditDate="2013-07-31T12:16:47.873" LastEditorUserId="88" OwnerUserId="28668" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;effect-size&gt;" Title="Partial $\eta^2$ for repeated measures ANOVA (with R's car package)" ViewCount="467" />
  
  <row Body="&lt;p&gt;The intercept is the predicted value of the dependent variable when all the independent variables are 0. It is rarely of interest, unless the IVs are centered or standardized. Since you haven't told us what any of your variables are, there's no way to say more than that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-07-31T13:35:47.690" Id="66140" LastActivityDate="2013-07-31T13:35:47.690" OwnerUserId="686" ParentId="66129" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there any tool that can calculate the AUC value from a ROC curve if I already have true positive, true negative, false positive, false negative values. &lt;/p&gt;&#10;" ClosedDate="2013-08-06T10:50:29.533" CommentCount="6" CreationDate="2013-07-31T17:19:27.040" Id="66172" LastActivityDate="2013-07-31T17:57:57.410" LastEditDate="2013-07-31T17:57:57.410" LastEditorUserId="7290" OwnerUserId="28681" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;mathematical-statistics&gt;&lt;roc&gt;&lt;auc&gt;" Title="Tool for calculating AUC Value" ViewCount="49" />
  
  
  
  <row AcceptedAnswerId="66223" AnswerCount="2" Body="&lt;p&gt;OpenCV provides an implementation of random forest named random trees and derived from a decision tree class. One parameter to train the random forest is the maximum depth, which in the provided examples is typically between 10 and 20. I learned that random forest is generally grown to its full depth and no pruning is done and, therefor, other random forest implementations do not provide a parameter of maximum depth. Why then, without consulting the source code, which might give the answer, does the OpenCV implementation provide this parameter and is it meaningful to limit the maximum depth in a random forest?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-01T05:24:51.287" Id="66209" LastActivityDate="2014-02-05T19:09:34.910" OwnerUserId="28586" PostTypeId="1" Score="0" Tags="&lt;random-forest&gt;" Title="OpenCV parameters of Random Trees" ViewCount="505" />
  <row AcceptedAnswerId="66224" AnswerCount="2" Body="&lt;p&gt;We simulate $E(\exp(z))$ by Monte Carlo method,where $z\sim{}N(0,1)$. For sample sizes $2^{16}$ and $2^{17}$, the variance errors are 0.00531 and 0.00364, respectively. The ratio of these two errors is approximately $1.5$. My questions are:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) What information can we learn from this ratio?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Why should the ratio approach $\sqrt 2$ as the sample size approaches infinity?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-08-01T07:31:53.180" Id="66214" LastActivityDate="2013-08-03T10:49:33.280" LastEditDate="2013-08-01T08:16:53.423" LastEditorUserId="28701" OwnerUserId="28701" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;monte-carlo&gt;" Title="A question on Monte Carlo" ViewCount="198" />
  <row Body="&lt;p&gt;OK, I think I found a possible mechanism that works. I'm not sure if this is Simpson's paradox though, some confirmation would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that there are two type of firms:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Firms &lt;strong&gt;HIGH&lt;/strong&gt; with a high earnings growth/return relation, i.e. a high beta.&lt;/li&gt;&#10;&lt;li&gt;Firms &lt;strong&gt;LOW&lt;/strong&gt; with a low, but still positive, earnings growth/return relation, i.e. a low beta.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;That is, in this economy every single firm has a positive earnings growth/return relation, as found by the cited study.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, assume for simplicitiy that there are only two states:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;State: &lt;strong&gt;HIGH&lt;/strong&gt; have a moderate positive earnings growth, but very large returns (due to the large beta), &lt;strong&gt;LOW&lt;/strong&gt; have zero earnings growth and therefore, zero returns.&lt;/li&gt;&#10;&lt;li&gt;State: &lt;strong&gt;LOW&lt;/strong&gt; have a large positive earnings growth, but only moderate positive returns (due to the small beta), &lt;strong&gt;HIGH&lt;/strong&gt; have zero earnings growth and therefore, zero returns.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Now aggregating the two states, one gets:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;State: Moderate mean earnings growth, large mean stock returns.&lt;/li&gt;&#10;&lt;li&gt;State: Large mean earnings growth, moderate mean stock returns.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Therefore, regression realized returns on earnings growth yields a negative beta. I think this explanation is better to understand when drawing each case in a diagram.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is certainly an extreme example and I'm still surprised that they find it in their study empirically (because my explanation basically needs a huge negative correlation between the two types of firms: precisely in those periods in which one type does great, the other has to do bad, and vice versa...not sure how realistic that is), but at least that is a simple mechanism that works. There is also a study by &lt;a href=&quot;http://mba.tuck.dartmouth.edu/pages/faculty/jon.lewellen/docs/Earnings.pdf&quot; rel=&quot;nofollow&quot;&gt;Kothari/Lewellen/Warner (2006, p. 539)&lt;/a&gt;: They write:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It is useful to note that a negative reaction to aggregate earnings is entirely consistent with a positive reaction to ﬁrm earnings (a result conﬁrmed in our data). The economic story is simple. Aggregate earnings ﬂuctuate with discount rates because both are tied to macroeconomic conditions, while ﬁrm earnings primarily reﬂect idiosyncratic cash-ﬂow news. As a result, the confounding effects of discount-rate changes show up only in aggregate returns. Put differently, cash-ﬂow news is largely idiosyncratic while discount-rate changes are common across ﬁrms. By a simple diversiﬁcation argument, discount-rate effects play a larger role at the aggregate level. In short, our evidence suggests that common variation in discount rates explains an important fraction of aggregate stock market movements.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Maybe some of you find this explanation helpful.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-01T08:21:53.473" Id="66217" LastActivityDate="2013-08-01T10:13:11.140" LastEditDate="2013-08-01T10:13:11.140" LastEditorUserId="28673" OwnerUserId="28673" ParentId="66138" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As pointed out by the commenters, this seems to be an instance of Simpson's paradox. The difference between the relationships at the aggregate and within-group level can really be as large as you want it to be (no correlation at one level, strong positive or negative correlation at the other, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure about the mechanism that could account for it in your case but I think this is easiest to understand by looking at some plots. I don't have time to create a fictional example tailored to your situation and I obviously don't have your data at hand but here is one I created some time ago:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hYaQ3.png&quot; alt=&quot;Simpson paradox&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As I said, this plot was generated in another context but let's imagine that points of the same color/shape represent measures from the same firm and the two variables are some sort of interesting quantitative characteristics. Within each firm, there is basically no relationship. At the aggregate level there is a perfect positive correlation. Pooling all the data and ignoring the structure of the data set, there seems to be a high positive correlation, driven by the aggregate level correlation and dampened by the (smaller) within-firm variance. Roughly, one interpretation for this example would be that the evolution of both variables are not related but that different firms have a different, stable, baseline level on each and that those are related.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since one of your variables seems to be a difference and the other a ratio, providing an intuitive interpretation is more difficult but graphically and mathematically pretty much everything is possible, the correlation and regression coefficients at both level are just two different things. I think that this is the key insight behind Simpson's paradox and the examples with dichotomous variables and the interpretations that go with it are just special cases.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-01T09:30:46.583" Id="66221" LastActivityDate="2013-08-01T12:31:22.713" LastEditDate="2013-08-01T12:31:22.713" LastEditorUserId="6029" OwnerUserId="6029" ParentId="66138" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Well an important question is how many samples do you need to half the sampling error.  essentially by the central limit theorem, the sample mean, has standard error sigma/sqrt(N), [where sigma is the &quot;true&quot; population standard deviation]&lt;/p&gt;&#10;&#10;&lt;p&gt;so all the ratio is telling you is that doubling the number of samples reduces the error by only 1/sqrt(2)...&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-01T09:31:59.027" Id="66222" LastActivityDate="2013-08-01T09:31:59.027" OwnerUserId="27556" ParentId="66214" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;there are lots of implementations that do provide max depth parameter. It is basically used as a method of reducing complexity of the tree classifier and therefore the variance of the estimator.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-01T09:38:18.123" Id="66223" LastActivityDate="2013-08-01T09:38:18.123" OwnerUserId="27556" ParentId="66209" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="66232" AnswerCount="1" Body="&lt;p&gt;Say I have a sequence of length $n$ of $k$ different elements, s.t. $n &amp;gt; k, k &amp;gt; 2$. I want to show this sequence is random, based on the number of runs. I found this &lt;a href=&quot;http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;amp;id=pdf_1&amp;amp;handle=euclid.aoms/1177731825&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; by AM Mood which explains how to find the expected values and variances of the number of runs of each type, but I'm unsure how to show that a sequence is random from this data. I'm new to hypothesis testing and statistics in general, so any help would be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-01T10:40:12.353" FavoriteCount="0" Id="66228" LastActivityDate="2013-08-01T11:35:24.737" LastEditDate="2013-08-01T11:35:24.737" LastEditorUserId="22047" OwnerUserId="28703" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;mathematical-statistics&gt;&lt;sequential-analysis&gt;" Title="Runs test for randomness for k elements" ViewCount="112" />
  <row Body="&lt;p&gt;When you are dealing with strongly imbalanced data, the &lt;a href=&quot;https://lirias.kuleuven.be/bitstream/123456789/295592/1/d.&quot; rel=&quot;nofollow&quot;&gt;Precision-Recall curve&lt;/a&gt; is a very good tool, better than its more common cousin the &lt;a href=&quot;http://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot; rel=&quot;nofollow&quot;&gt;ROC curve&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://lirias.kuleuven.be/bitstream/123456789/295592/1/d.&quot; rel=&quot;nofollow&quot;&gt;Davis et. al.&lt;/a&gt; have shown that an algorithm which optimizes the area under the&#10;ROC curve is not guaranteed to optimize the area under the PR curve.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-01T11:09:07.547" Id="66230" LastActivityDate="2013-08-01T11:17:09.457" LastEditDate="2013-08-01T11:17:09.457" LastEditorUserId="25433" OwnerUserId="25433" ParentId="66204" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="66252" AnswerCount="3" Body="&lt;p&gt;I have the following data frame&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;structure(list(Chi = structure(c(1L, 1L, 5L, 5L, 6L, 9L, 9L, &#10;12L, 13L, 14L, 14L, 16L, 16L, 19L, 19L, 20L, 20L, 23L, 24L, 24L, &#10;26L, 26L, 31L, 31L, 33L, 33L, 36L, 37L, 37L, 40L, 40L, 43L, 43L, &#10;44L, 44L, 45L, 46L, 47L, 47L, 48L, 48L, 52L, 52L, 54L, 54L, 55L, &#10;55L, 56L, 59L, 59L, 61L, 61L, 63L, 63L, 64L, 64L, 65L, 65L, 69L, &#10;69L, 70L, 70L, 71L, 71L, 72L, 72L, 75L, 75L, 76L, 76L, 77L, 77L, &#10;79L, 79L, 86L, 86L, 87L, 87L, 88L, 88L, 91L, 91L, 92L, 92L, 93L, &#10;95L, 96L, 96L, 97L, 97L, 98L, 98L, 99L, 99L, 100L, 100L, 101L, &#10;101L, 103L, 103L, 104L, 104L, 107L, 108L, 108L, 112L, 112L, 113L, &#10;116L, 116L, 117L, 120L, 125L, 125L, 127L, 127L, 129L, 129L, 130L, &#10;131L, 131L, 132L, 132L, 134L, 134L, 135L, 135L, 136L, 136L, 139L, &#10;141L, 141L, 143L, 144L, 144L, 145L, 145L, 146L, 150L, 150L, 151L, &#10;151L, 153L, 153L, 155L, 155L, 157L, 162L, 162L, 163L, 163L, 164L, &#10;164L, 167L, 167L, 168L, 169L, 169L, 171L, 171L, 172L, 172L, 174L, &#10;174L, 175L, 175L, 177L, 177L, 180L, 180L, 183L, 187L, 27L, 83L, &#10;83L, 165L, 165L, 85L, 85L, 156L, 156L, 17L, 17L, 123L, 123L, &#10;124L, 124L, 57L, 57L, 42L, 42L, 159L, 159L, 38L, 38L, 82L, 82L, &#10;41L, 41L, 142L, 142L), .Label = c(&quot;0106610856&quot;, &quot;0107470802&quot;, &#10;&quot;0108490513&quot;, &quot;0108590534&quot;, &quot;0109480651&quot;, &quot;0111290260&quot;, &quot;0111410339&quot;, &#10;&quot;0201390418&quot;, &quot;0207570604&quot;, &quot;0208360352&quot;, &quot;0212323105&quot;, &quot;0212380362&quot;, &#10;&quot;0301310432&quot;, &quot;0302705635&quot;, &quot;0303450495&quot;, &quot;0304260266&quot;, &quot;0304440574&quot;, &#10;&quot;0305280546&quot;, &quot;0305380338&quot;, &quot;0305381393&quot;, &quot;0305510576&quot;, &quot;0305542214&quot;, &#10;&quot;0308610733&quot;, &quot;0309370345&quot;, &quot;0309665035&quot;, &quot;0310380545&quot;, &quot;0403320259&quot;, &#10;&quot;0403360374&quot;, &quot;0404360343&quot;, &quot;0406270198&quot;, &quot;0501451137&quot;, &quot;0504460676&quot;, &#10;&quot;0511310366&quot;, &quot;0605270511&quot;, &quot;0605340560&quot;, &quot;0605410461&quot;, &quot;0605410585&quot;, &#10;&quot;0606260684&quot;, &quot;0606270353&quot;, &quot;0609360507&quot;, &quot;0702520535&quot;, &quot;0702570818&quot;, &#10;&quot;0705430421&quot;, &quot;0710380364&quot;, &quot;0801330378&quot;, &quot;0801430275&quot;, &quot;0802320430&quot;, &#10;&quot;0803510802&quot;, &quot;0805390383&quot;, &quot;0806560533&quot;, &quot;0809430460&quot;, &quot;0902380354&quot;, &#10;&quot;0904340252&quot;, &quot;0904370445&quot;, &quot;0906340403&quot;, &quot;0907380379&quot;, &quot;0909415420&quot;, &#10;&quot;0910300100&quot;, &quot;0911430253&quot;, &quot;1001270460&quot;, &quot;1001360389&quot;, &quot;1002455294&quot;, &#10;&quot;1005280487&quot;, &quot;1006330445&quot;, &quot;1009350447&quot;, &quot;1010375156&quot;, &quot;1011270447&quot;, &#10;&quot;1012350312&quot;, &quot;1012400441&quot;, &quot;1102570648&quot;, &quot;1105450589&quot;, &quot;1106230566&quot;, &#10;&quot;1106330587&quot;, &quot;1204530475&quot;, &quot;1206350342&quot;, &quot;1208330373&quot;, &quot;1209280345&quot;, &#10;&quot;1209400502&quot;, &quot;1209400561&quot;, &quot;1210380536&quot;, &quot;1302240455&quot;, &quot;1305751256&quot;, &#10;&quot;1306370353&quot;, &quot;1307260470&quot;, &quot;1310340250&quot;, &quot;1312430613&quot;, &quot;1312440597&quot;, &#10;&quot;1312690593&quot;, &quot;1404430512&quot;, &quot;1404530479&quot;, &quot;1405330376&quot;, &quot;1406310360&quot;, &#10;&quot;1406350419&quot;, &quot;1406430439&quot;, &quot;1408460602&quot;, &quot;1412360366&quot;, &quot;1502385236&quot;, &#10;&quot;1503370488&quot;, &quot;1503470628&quot;, &quot;1503660400&quot;, &quot;1506390447&quot;, &quot;1508340196&quot;, &#10;&quot;1510340688&quot;, &quot;1510440453&quot;, &quot;1603310622&quot;, &quot;1604440376&quot;, &quot;1606370014&quot;, &#10;&quot;1609650549&quot;, &quot;1610345304&quot;, &quot;1610345304x&quot;, &quot;1612300367&quot;, &quot;1702330397&quot;, &#10;&quot;1704330181&quot;, &quot;1706330316&quot;, &quot;1712560522&quot;, &quot;1802340270&quot;, &quot;1804310336&quot;, &#10;&quot;1808430417&quot;, &quot;1810400244&quot;, &quot;1902340299&quot;, &quot;1902610679&quot;, &quot;1905360355&quot;, &#10;&quot;1906320438&quot;, &quot;1906390525&quot;, &quot;1909310514&quot;, &quot;1912460408&quot;, &quot;2002440204&quot;, &#10;&quot;2004350288&quot;, &quot;2007350203&quot;, &quot;2009400364&quot;, &quot;2009460669&quot;, &quot;2011410428&quot;, &#10;&quot;2011500524&quot;, &quot;2103335236&quot;, &quot;2109370262&quot;, &quot;2112290355&quot;, &quot;2201330484&quot;, &#10;&quot;2201600686&quot;, &quot;2203290471&quot;, &quot;2203406259&quot;, &quot;2205430513&quot;, &quot;2207340473&quot;, &#10;&quot;2208340396&quot;, &quot;2303430410&quot;, &quot;2303530717&quot;, &quot;2308290390&quot;, &quot;2309420506&quot;, &#10;&quot;2310370398&quot;, &quot;2310370398.0&quot;, &quot;2312280310&quot;, &quot;2404436295&quot;, &quot;2406640663&quot;, &#10;&quot;2411420404&quot;, &quot;2501520858&quot;, &quot;2505330239&quot;, &quot;2505380376&quot;, &quot;2511320428&quot;, &#10;&quot;2511320436&quot;, &quot;2511360306&quot;, &quot;2601490470&quot;, &quot;2601520566&quot;, &quot;2608450598&quot;, &#10;&quot;2611400237&quot;, &quot;2701470625&quot;, &quot;2702230407&quot;, &quot;2702340342&quot;, &quot;2703470916&quot;, &#10;&quot;2704380538&quot;, &quot;2709250586&quot;, &quot;2712350545&quot;, &quot;2712541146&quot;, &quot;2805310438&quot;, &#10;&quot;2805350472&quot;, &quot;2807360475&quot;, &quot;2807480594&quot;, &quot;2809325316&quot;, &quot;2809470634&quot;, &#10;&quot;2902400411&quot;, &quot;2903350442&quot;, &quot;2905330376&quot;, &quot;2906450480&quot;, &quot;2910240363&quot;, &#10;&quot;3004510529&quot;, &quot;3007230195&quot;, &quot;3012410333&quot;, &quot;3107440299&quot;, &quot;3108350420&quot;&#10;), class = &quot;factor&quot;), Sex = structure(c(2L, 2L, 2L, 2L, 1L, 1L, &#10;1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, &#10;2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, &#10;2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, &#10;1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, &#10;1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, &#10;1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, &#10;2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, &#10;1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, &#10;2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, &#10;2L, 2L, 2L), .Label = c(&quot;F&quot;, &quot;M&quot;), class = &quot;factor&quot;), Age = c(50L, &#10;50L, 63L, 63L, 83L, 55L, 55L, 72L, 81L, 42L, 42L, 86L, 86L, 74L, &#10;74L, 74L, 74L, 50L, 74L, 74L, 73L, 73L, 67L, 67L, 79L, 79L, 71L, &#10;70L, 70L, 75L, 75L, 68L, 68L, 73L, 73L, 79L, 69L, 79L, 79L, 61L, &#10;61L, 74L, 74L, 74L, 74L, 77L, 77L, 73L, 68L, 68L, 76L, 76L, 84L, &#10;84L, 78L, 78L, 77L, 77L, 71L, 71L, 55L, 55L, 67L, 67L, 88L, 88L, &#10;77L, 77L, 78L, 78L, 84L, 84L, 71L, 71L, 69L, 69L, 67L, 67L, 41L, &#10;41L, 78L, 78L, 80L, 80L, 76L, 66L, 76L, 76L, 73L, 73L, 74L, 74L, &#10;64L, 64L, 46L, 46L, 72L, 72L, 78L, 78L, 67L, 67L, 74L, 47L, 47L, &#10;79L, 79L, 79L, 78L, 78L, 81L, 77L, 79L, 79L, 67L, 67L, 76L, 76L, &#10;70L, 64L, 64L, 70L, 70L, 79L, 79L, 74L, 74L, 82L, 82L, 83L, 69L, &#10;69L, 76L, 69L, 69L, 58L, 58L, 83L, 83L, 83L, 68L, 68L, 69L, 69L, &#10;79L, 79L, 79L, 66L, 66L, 70L, 70L, 65L, 65L, 65L, 65L, 72L, 87L, &#10;87L, 57L, 57L, 80L, 80L, 76L, 76L, 63L, 63L, 64L, 64L, 78L, 78L, &#10;60L, 76L, 80L, 75L, 75L, 90L, 90L, 78L, 78L, 74L, 74L, 69L, 69L, &#10;80L, 80L, 73L, 73L, 71L, 71L, 56L, 56L, 76L, 76L, 87L, 87L, 38L, &#10;38L, 61L, 61L, 78L, 78L), SBR = c(12.061, 11.447, 9.403, 9.136, &#10;9.747, 8.648, 7.934, 7.914, 9.349, 11.224, 10.433, 4.897, 5.823, &#10;8.683, 8.692, 13.018, 13.386, 7.817, 7.384, 7.518, 11.091, 11.028, &#10;8.372, 8.497, 10.751, 10.488, 4.347, 2.593, 2.203, 6.461, 7.272, &#10;4.581, 4.593, 10.31, 9.004, 10.362, 10.307, 9.266, 10.163, 9.24, &#10;8.732, 8.449, 7.823, 10.427, 10.669, 8.695, 8.729, 8.653, 12.299, &#10;12.158, 11.748, 11.19, 8.431, 8.717, 8.253, 8.412, 6.911, 6.805, &#10;9.468, 11.413, 6.603, 7.697, 7.762, 7.097, 10.607, 8.162, 5.419, &#10;5.575, 7.007, 6.974, 8.708, 8.419, 9.47, 8.42, 8.229, 8.027, &#10;5.294, 4.628, 11.475, 10.328, 7.905, 8.491, 10.724, 9.02, 9.095, &#10;5.754, 9.805, 7.332, 6.669, 5.118, 12.443, 11.972, 13.309, 13.906, &#10;14.963, 15.119, 6.465, 6.38, 6.949, 6.064, 6.541, 6.648, 3.542, &#10;11.148, 11.918, 9.743, 9.795, 6.103, 6.025, 3.917, 7.304, 7.628, &#10;8.092, 7.347, 9.051, 8.206, 10.697, 10.286, 4.564, 10.62, 9.84, &#10;9.105, 7.998, 6.437, 5.707, 6.949, 6.315, 6.165, 6.68, 8.86, &#10;8.326, 8.6, 7.776, 5.193, 5.456, 11.864, 11.381, 6.385, 10.972, &#10;9.87, 9.645, 7.738, 10.096, 9.667, 9.687, 8.255, 4.606, 8.738, &#10;8.519, 7.002, 6.288, 10.425, 10.303, 8.278, 8.342, 6.657, 6.111, &#10;5.928, 13.06, 12.747, 5.545, 5.845, 9.338, 9.534, 9.635, 8.716, &#10;7.765, 7.254, 7.517, 7.317, 7.335, 5.628, 4.864, 7.1, 7.02, 6.734, &#10;5.622, 7.167, 7.391, 6.443, 6.874, 8.373, 7.573, 5.701, 6.355, &#10;6.884, 6.296, 9.097, 9.645, 7.068, 7.252, 6, 5.794, 8.074, 9.267, &#10;12.584, 10.723, 9.39, 9.165, 9.635, 8.814), Diagnosis = structure(c(2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(&quot;A&quot;, &quot;N&quot;), class = &quot;factor&quot;), &#10;    fit = c(10.1654358296645, 10.1654358296645, 9.07109655193284, &#10;    9.07109655193284, 7.38749766311491, 9.74453610746002, 9.74453610746002, &#10;    8.31347705196477, 7.5558575519967, 10.8388753851917, 10.8388753851917, &#10;    7.13495782979222, 7.13495782979222, 8.14511716308298, 8.14511716308298, &#10;    8.14511716308298, 8.14511716308298, 10.1654358296645, 8.14511716308298, &#10;    8.14511716308298, 8.22929710752388, 8.22929710752388, 8.73437677416926, &#10;    8.73437677416926, 7.7242174408785, 7.7242174408785, 8.39765699640567, &#10;    8.48183694084657, 8.48183694084657, 8.06093721864208, 8.06093721864208, &#10;    8.65019682972836, 8.65019682972836, 8.22929710752388, 8.22929710752388, &#10;    7.7242174408785, 8.56601688528746, 7.7242174408785, 7.7242174408785, &#10;    9.23945644081464, 9.23945644081464, 8.14511716308298, 8.14511716308298, &#10;    8.14511716308298, 8.14511716308298, 7.89257732976029, 7.89257732976029, &#10;    8.22929710752388, 8.65019682972836, 8.65019682972836, 7.97675727420119, &#10;    7.97675727420119, 7.30331771867401, 7.30331771867401, 7.80839738531939, &#10;    7.80839738531939, 7.89257732976029, 7.89257732976029, 8.39765699640567, &#10;    8.39765699640567, 9.74453610746002, 9.74453610746002, 8.73437677416926, &#10;    8.73437677416926, 6.96659794091043, 6.96659794091043, 7.89257732976029, &#10;    7.89257732976029, 7.80839738531939, 7.80839738531939, 7.30331771867401, &#10;    7.30331771867401, 8.39765699640567, 8.39765699640567, 8.56601688528746, &#10;    8.56601688528746, 8.73437677416926, 8.73437677416926, 10.9230553296326, &#10;    10.9230553296326, 7.80839738531939, 7.80839738531939, 7.6400374964376, &#10;    7.6400374964376, 7.97675727420119, 8.81855671861015, 7.97675727420119, &#10;    7.97675727420119, 8.22929710752388, 8.22929710752388, 8.14511716308298, &#10;    8.14511716308298, 8.98691660749195, 8.98691660749195, 10.5021556074281, &#10;    10.5021556074281, 8.31347705196477, 8.31347705196477, 7.80839738531939, &#10;    7.80839738531939, 8.73437677416926, 8.73437677416926, 8.14511716308298, &#10;    10.4179756629872, 10.4179756629872, 7.7242174408785, 7.7242174408785, &#10;    7.7242174408785, 7.80839738531939, 7.80839738531939, 7.5558575519967, &#10;    7.89257732976029, 7.7242174408785, 7.7242174408785, 8.73437677416926, &#10;    8.73437677416926, 7.97675727420119, 7.97675727420119, 8.48183694084657, &#10;    8.98691660749195, 8.98691660749195, 8.48183694084657, 8.48183694084657, &#10;    7.7242174408785, 7.7242174408785, 8.14511716308298, 8.14511716308298, &#10;    7.47167760755581, 7.47167760755581, 7.38749766311491, 8.56601688528746, &#10;    8.56601688528746, 7.97675727420119, 8.56601688528746, 8.56601688528746, &#10;    9.49199627413733, 9.49199627413733, 7.38749766311491, 7.38749766311491, &#10;    7.38749766311491, 8.65019682972836, 8.65019682972836, 8.56601688528746, &#10;    8.56601688528746, 7.7242174408785, 7.7242174408785, 7.7242174408785, &#10;    8.81855671861015, 8.81855671861015, 8.48183694084657, 8.48183694084657, &#10;    8.90273666305105, 8.90273666305105, 8.90273666305105, 8.90273666305105, &#10;    8.31347705196477, 7.05077788535132, 7.05077788535132, 9.57617621857822, &#10;    9.57617621857822, 7.6400374964376, 7.6400374964376, 7.97675727420119, &#10;    7.97675727420119, 9.07109655193284, 9.07109655193284, 8.98691660749195, &#10;    8.98691660749195, 7.80839738531939, 7.80839738531939, 9.32363638525553, &#10;    7.97675727420119, 7.6400374964376, 8.06093721864208, 8.06093721864208, &#10;    6.79823805202863, 6.79823805202863, 7.80839738531939, 7.80839738531939, &#10;    8.14511716308298, 8.14511716308298, 8.56601688528746, 8.56601688528746, &#10;    7.6400374964376, 7.6400374964376, 8.22929710752388, 8.22929710752388, &#10;    8.39765699640567, 8.39765699640567, 9.66035616301912, 9.66035616301912, &#10;    7.97675727420119, 7.97675727420119, 7.05077788535132, 7.05077788535132, &#10;    11.1755951629553, 11.1755951629553, 9.23945644081464, 9.23945644081464, &#10;    7.80839738531939, 7.80839738531939), lwr = c(5.90999794584117, &#10;    5.90999794584117, 4.85411038352648, 4.85411038352648, 3.16383967274129, &#10;    5.5078318233643, 5.5078318233643, 4.10340735387365, 3.33646831235876, &#10;    6.54339594841324, 6.54339594841324, 2.90340734886211, 2.90340734886211, &#10;    3.93437871118446, 3.93437871118446, 3.93437871118446, 3.93437871118446, &#10;    5.90999794584117, 3.93437871118446, 3.93437871118446, 4.01899333351728, &#10;    4.01899333351728, 4.52246825860982, 4.52246825860982, 3.50829974380329, &#10;    3.50829974380329, 4.18762073879345, 4.27163348349591, 4.27163348349591, &#10;    3.84956354899072, 3.84956354899072, 4.43905717652239, 4.43905717652239, &#10;    4.01899333351728, 4.01899333351728, 3.50829974380329, 4.35544561188301, &#10;    3.50829974380329, 3.50829974380329, 5.0187327527967, 5.0187327527967, &#10;    3.93437871118446, 3.93437871118446, 3.93437871118446, 3.93437871118446, &#10;    3.67933199647321, 3.67933199647321, 4.01899333351728, 4.43905717652239, &#10;    4.43905717652239, 3.76454793766266, 3.76454793766266, 3.07722713717148, &#10;    3.07722713717148, 3.59391587315661, 3.59391587315661, 3.67933199647321, &#10;    3.67933199647321, 4.18762073879345, 4.18762073879345, 5.5078318233643, &#10;    5.5078318233643, 4.52246825860982, 4.52246825860982, 2.72879720476565, &#10;    2.72879720476565, 3.67933199647321, 3.67933199647321, 3.59391587315661, &#10;    3.59391587315661, 3.07722713717148, 3.07722713717148, 4.18762073879345, &#10;    4.18762073879345, 4.35544561188301, 4.35544561188301, 4.52246825860982, &#10;    4.52246825860982, 6.62171309179384, 6.62171309179384, 3.59391587315661, &#10;    3.59391587315661, 3.42248381273475, 3.42248381273475, 3.76454793766266, &#10;    4.60567896791123, 3.76454793766266, 3.76454793766266, 4.01899333351728, &#10;    4.01899333351728, 3.93437871118446, 3.93437871118446, 4.77149984958079, &#10;    4.77149984958079, 6.22823093408572, 6.22823093408572, 4.10340735387365, &#10;    4.10340735387365, 3.59391587315661, 3.59391587315661, 4.52246825860982, &#10;    4.52246825860982, 3.93437871118446, 6.14896197954192, 6.14896197954192, &#10;    3.50829974380329, 3.50829974380329, 3.50829974380329, 3.59391587315661, &#10;    3.59391587315661, 3.33646831235876, 3.67933199647321, 3.50829974380329, &#10;    3.50829974380329, 4.52246825860982, 4.52246825860982, 3.76454793766266, &#10;    3.76454793766266, 4.27163348349591, 4.77149984958079, 4.77149984958079, &#10;    4.27163348349591, 4.27163348349591, 3.50829974380329, 3.50829974380329, &#10;    3.93437871118446, 3.93437871118446, 3.25025350300496, 3.25025350300496, &#10;    3.16383967274129, 4.35544561188301, 4.35544561188301, 3.76454793766266, &#10;    4.35544561188301, 4.35544561188301, 5.26417374160422, 5.26417374160422, &#10;    3.16383967274129, 3.16383967274129, 3.16383967274129, 4.43905717652239, &#10;    4.43905717652239, 4.35544561188301, 4.35544561188301, 3.50829974380329, &#10;    3.50829974380329, 3.50829974380329, 4.60567896791123, 4.60567896791123, &#10;    4.27163348349591, 4.27163348349591, 4.68868944268447, 4.68868944268447, &#10;    4.68868944268447, 4.68868944268447, 4.10340735387365, 2.81620086292774, &#10;    2.81620086292774, 5.34559069482765, 5.34559069482765, 3.42248381273475, &#10;    3.42248381273475, 3.76454793766266, 3.76454793766266, 4.85411038352648, &#10;    4.85411038352648, 4.77149984958079, 4.77149984958079, 3.59391587315661, &#10;    3.59391587315661, 5.10074511800645, 3.76454793766266, 3.42248381273475, &#10;    3.84956354899072, 3.84956354899072, 2.55340019612735, 2.55340019612735, &#10;    3.59391587315661, 3.59391587315661, 3.93437871118446, 3.93437871118446, &#10;    4.35544561188301, 4.35544561188301, 3.42248381273475, 3.42248381273475, &#10;    4.01899333351728, 4.01899333351728, 4.18762073879345, 4.18762073879345, &#10;    5.42680991723477, 5.42680991723477, 3.76454793766266, 3.76454793766266, &#10;    2.81620086292774, 2.81620086292774, 6.85553891121225, 6.85553891121225, &#10;    5.0187327527967, 5.0187327527967, 3.59391587315661, 3.59391587315661&#10;    ), upr = c(14.4208737134878, 14.4208737134878, 13.2880827203392, &#10;    13.2880827203392, 11.6111556534885, 13.9812403915557, 13.9812403915557, &#10;    12.5235467500559, 11.7752467916346, 15.1343548219701, 15.1343548219701, &#10;    11.3665083107223, 11.3665083107223, 12.3558556149815, 12.3558556149815, &#10;    12.3558556149815, 12.3558556149815, 14.4208737134878, 12.3558556149815, &#10;    12.3558556149815, 12.4396008815305, 12.4396008815305, 12.9462852897287, &#10;    12.9462852897287, 11.9401351379537, 11.9401351379537, 12.6076932540179, &#10;    12.6920403981972, 12.6920403981972, 12.2723108882934, 12.2723108882934, &#10;    12.8613364829343, 12.8613364829343, 12.4396008815305, 12.4396008815305, &#10;    11.9401351379537, 12.7765881586919, 11.9401351379537, 11.9401351379537, &#10;    13.4601801288326, 13.4601801288326, 12.3558556149815, 12.3558556149815, &#10;    12.3558556149815, 12.3558556149815, 12.1058226630474, 12.1058226630474, &#10;    12.4396008815305, 12.8613364829343, 12.8613364829343, 12.1889666107397, &#10;    12.1889666107397, 11.5294083001765, 11.5294083001765, 12.0228788974822, &#10;    12.0228788974822, 12.1058226630474, 12.1058226630474, 12.6076932540179, &#10;    12.6076932540179, 13.9812403915557, 13.9812403915557, 12.9462852897287, &#10;    12.9462852897287, 11.2043986770552, 11.2043986770552, 12.1058226630474, &#10;    12.1058226630474, 12.0228788974822, 12.0228788974822, 11.5294083001765, &#10;    11.5294083001765, 12.6076932540179, 12.6076932540179, 12.7765881586919, &#10;    12.7765881586919, 12.9462852897287, 12.9462852897287, 15.2243975674713, &#10;    15.2243975674713, 12.0228788974822, 12.0228788974822, 11.8575911801404, &#10;    11.8575911801404, 12.1889666107397, 13.0314344693091, 12.1889666107397, &#10;    12.1889666107397, 12.4396008815305, 12.4396008815305, 12.3558556149815, &#10;    12.3558556149815, 13.2023333654031, 13.2023333654031, 14.7760802807705, &#10;    14.7760802807705, 12.5235467500559, 12.5235467500559, 12.0228788974822, &#10;    12.0228788974822, 12.9462852897287, 12.9462852897287, 12.3558556149815, &#10;    14.6869893464325, 14.6869893464325, 11.9401351379537, 11.9401351379537, &#10;    11.9401351379537, 12.0228788974822, 12.0228788974822, 11.7752467916346, &#10;    12.1058226630474, 11.9401351379537, 11.9401351379537, 12.9462852897287, &#10;    12.9462852897287, 12.1889666107397, 12.1889666107397, 12.6920403981972, &#10;    13.2023333654031, 13.2023333654031, 12.6920403981972, 12.6920403981972, &#10;    11.9401351379537, 11.9401351379537, 12.3558556149815, 12.3558556149815, &#10;    11.6931017121067, 11.6931017121067, 11.6111556534885, 12.7765881586919, &#10;    12.7765881586919, 12.1889666107397, 12.7765881586919, 12.7765881586919, &#10;    13.7198188066704, 13.7198188066704, 11.6111556534885, 11.6111556534885, &#10;    11.6111556534885, 12.8613364829343, 12.8613364829343, 12.7765881586919, &#10;    12.7765881586919, 11.9401351379537, 11.9401351379537, 11.9401351379537, &#10;    13.0314344693091, 13.0314344693091, 12.6920403981972, 12.6920403981972, &#10;    13.1167838834176, 13.1167838834176, 13.1167838834176, 13.1167838834176, &#10;    12.5235467500559, 11.2853549077749, 11.2853549077749, 13.8067617423288, &#10;    13.8067617423288, 11.8575911801404, 11.8575911801404, 12.1889666107397, &#10;    12.1889666107397, 13.2880827203392, 13.2880827203392, 13.2023333654031, &#10;    13.2023333654031, 12.0228788974822, 12.0228788974822, 13.5465276525046, &#10;    12.1889666107397, 11.8575911801404, 12.2723108882934, 12.2723108882934, &#10;    11.0430759079299, 11.0430759079299, 12.0228788974822, 12.0228788974822, &#10;    12.3558556149815, 12.3558556149815, 12.7765881586919, 12.7765881586919, &#10;    11.8575911801404, 11.8575911801404, 12.4396008815305, 12.4396008815305, &#10;    12.6076932540179, 12.6076932540179, 13.8939024088035, 13.8939024088035, &#10;    12.1889666107397, 12.1889666107397, 11.2853549077749, 11.2853549077749, &#10;    15.4956514146983, 15.4956514146983, 13.4601801288326, 13.4601801288326, &#10;    12.0228788974822, 12.0228788974822)), .Names = c(&quot;Chi&quot;, &quot;Sex&quot;, &#10;&quot;Age&quot;, &quot;SBR&quot;, &quot;Diagnosis&quot;, &quot;fit&quot;, &quot;lwr&quot;, &quot;upr&quot;), row.names = c(NA, &#10;201L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I plot SBR v Age for each Sex using &lt;code&gt;ggplot2&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(sbr_with_pred, aes(x=Age, y=SBR)) + geom_point(aes(col=Sex), &#10;                                                           shape=19, alpha=0.4) + &#10;            geom_smooth(aes(col=Sex),method = 'lm', se=FALSE,linetype=2) + &#10;            geom_ribbon(aes(y = fit, ymin = lwr, ymax = upr, fill = 'prediction'), &#10;                        linetype =2,alpha = 0.1) + &#10;            scale_fill_manual('Interval', values = c('blue')) + theme_bw() + &#10;            theme(legend.position = &quot;right&quot;) + &#10;            scale_y_continuous(limits = c(-3,15.5),breaks = c(0,5,10,15)) + &#10;            scale_color_manual(&quot;Sex&quot;, values = c('red','blue'))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives the following&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/j3DxP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I can get the equation of each regression fit easy enough&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(formula = SBR ~ Age, data = subset(sbr_with_pred, Sex == &quot;F&quot;))&#10;lm(formula = SBR ~ Age, data = subset(sbr_with_pred, Sex == &quot;M&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However how do I test whether or not they are significantly different (which they are not). I think analysis of covariance is the appropriate test but I do not know how to implement this in R&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-01T13:47:48.750" FavoriteCount="2" Id="66250" LastActivityDate="2013-08-01T14:28:44.870" LastEditDate="2013-08-01T14:18:26.423" LastEditorUserId="7290" OwnerUserId="28710" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;interaction&gt;&lt;ancova&gt;" Title="Compare 2 regression lines in R" ViewCount="143" />
  
  
  <row Body="&lt;p&gt;@Peter Flom is right.  You are always comparing levels with categorical IVs.  SPSS is simply presenting your results in a way that obscures that fact from you.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In this specific case, since each of your IV's has only two levels, the reported p-values are the ones you want.  (Note that if you had &gt;2 levels, you would want to use &lt;code&gt;anova()&lt;/code&gt; to get the appropriate p-value for the factor.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;An additional issue here is that you have an interaction term included in your model.  The rule is that when you have an interaction you don't interpret the main effects.  In your case the interaction term is not 'significant' by conventional standards, but it is close enough that I wouldn't want to interpret the main effects anyway.  What this means then is that the results for, say, &lt;code&gt;Animal_Number&lt;/code&gt; only pertain to the case where &lt;code&gt;Treatment&lt;/code&gt; is &lt;em&gt;control&lt;/em&gt;.  Basically, when there is an interaction, there is no such thing as 'the &lt;em&gt;pure&lt;/em&gt; effect of a variable'.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I also notice that the results from R and SPSS differ.  That is unsettling, but based on what's provided, I can't tell why this happened.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-01T14:28:10.750" Id="66261" LastActivityDate="2013-08-01T16:05:15.373" LastEditDate="2013-08-01T16:05:15.373" LastEditorUserId="7290" OwnerUserId="7290" ParentId="66253" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="66312" AnswerCount="1" Body="&lt;p&gt;I would like to sample from:&#10;$$&#10;p(\theta_2|x)=\int p(\theta_2|\theta_1,x) . p(\theta_1|x) . d\theta_1&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Several persons suggest me to use the following procedure to draw $(\theta_2^i)_{i=1:N}$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for i=1:N&#10;    draw $\theta_1^i$ from $p(\theta_1|x)$&#10;    draw $\theta_2^i$ from $p(\theta_2|\theta_1^i,x)$&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;this appears intuitively satifying but I do not see how to show that this procedure is valid. It appears to be linked to Rao-Blackwellisation but it is not clear to me. Thanks for your help.&lt;/p&gt;&#10;&#10;&lt;p&gt;(this question is linked to an older one that I posted &lt;a href=&quot;http://stats.stackexchange.com/questions/63427/sampling-from-marginal-using-integrated-conditional&quot;&gt;Sampling from marginal using integrated conditional&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-01T14:49:59.417" FavoriteCount="1" Id="66264" LastActivityDate="2013-08-02T01:10:54.453" OwnerUserId="14346" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;sampling&gt;" Title="Procedure to sample from an integrated marginal" ViewCount="37" />
  <row Body="&lt;h3&gt;Summary&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The generalization of least-squares regression to complex-valued variables is straightforward, consisting primarily of replacing matrix transposes by conjugate transposes in the usual matrix formulas.&lt;/strong&gt;  A complex-valued regression, though, corresponds to a complicated multivariate multiple regression whose solution would be much more difficult to obtain using standard (real variable) methods.  Thus, when the complex-valued model is meaningful, using complex arithmetic to obtain a solution is strongly recommended.  This answer also includes some suggested ways to display the data and present diagnostic plots of the fit.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For simplicity, let's discuss the case of ordinary (univariate) regression, which can be written&lt;/p&gt;&#10;&#10;&lt;p&gt;$$z_j = \beta_0 + \beta_1 w_j + \varepsilon_j.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have taken the liberty of naming the independent variable $W$ and the dependent variable $Z$, which is conventional (see, for instance, Lars Ahlfors, &lt;em&gt;Complex Analysis&lt;/em&gt;).  All that follows is straightforward to extend to the multiple regression setting.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Interpretation&lt;/h3&gt;&#10;&#10;&lt;p&gt;This model has an easily visualized geometric interpretation: multiplication by $\beta_1$ will &lt;em&gt;rescale&lt;/em&gt; $w_j$ by the modulus of $\beta_1$ and &lt;em&gt;rotate&lt;/em&gt; it around the origin by the argument of $\beta_1$.  Subsequently, adding $\beta_0$ translates the result by this amount.  The effect of $\varepsilon_j$ is to &quot;jitter&quot; that translation a little bit.  Thus, regressing the $z_j$ on the $w_j$ in this manner is an effort to understand the collection of 2D points $(z_j)$ as arising from a constellation of 2D points $(w_j)$ via such a transformation, allowing for some error in the process.  This is illustrated below with the figure titled &quot;Fit as a Transformation.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the rescaling and rotation are not just any linear transformation of the plane: they rule out skew transformations, for instance.  Thus &lt;em&gt;this model is not the same as a bivariate multiple regression with four parameters.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Ordinary Least Squares&lt;/h3&gt;&#10;&#10;&lt;p&gt;To connect the complex case with the real case, let's write &lt;/p&gt;&#10;&#10;&lt;p&gt;$z_j = x_j + i y_j$ for the values of the dependent variable and&lt;/p&gt;&#10;&#10;&lt;p&gt;$w_j = u_j + i v_j$ for the values of the independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, for the parameters write &lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_0 = \gamma_0 + i \delta_0$ and $\beta_1 = \gamma_1 +i \delta_1$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Every one of the new terms introduced is, of course, real, and $i^2 = -1$ is imaginary while $j=1, 2, \ldots, n$ indexes the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;OLS finds $\hat\beta_0$ and $\hat\beta_1$ that minimize the sum of squares of deviations,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{j=1}^n ||z_j - \left(\hat\beta_0 + \hat\beta_1 w_j\right)||^2&#10;= \sum_{j=1}^n \left(\bar z_j - \left(\bar{\hat\beta_0} + \bar{\hat\beta_1} \bar w_j\right)\right) \left(z_j - \left(\hat\beta_0 + \hat\beta_1 w_j\right)\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Formally this is identical to the usual matrix formulation: compare it to $\left(z - X\beta\right)'\left(z - X\beta\right).$  The only difference we find is that the transpose of the design matrix $X'$ is replaced by the &lt;em&gt;conjugate transpose&lt;/em&gt; $X^* = \bar X '$.  Consequently &lt;strong&gt;the formal matrix solution is&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat\beta = \left(X^*X\right)^{-1}X^* z.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;At the same time, to see what might be accomplished by casting this into a purely real-variable problem, we may write the OLS objective out in terms of the real components:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{j=1}^n \left(x_j-\gamma_0-\gamma_1u_j+\delta_1v_j\right)^2 &#10;+ \sum_{j=1}^n\left(y_j-\delta_0-\delta_1u_j-\gamma_1v_j\right)^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Evidently this represents two &lt;em&gt;linked&lt;/em&gt; real regressions: one of them regresses $x$ on $u$ and $v$, the other regresses $y$ on $u$ and $v$; and we require that the $v$ coefficient for $x$ be the negative of the $u$ coefficient for $y$ and the $u$ coefficient for $x$ equal the $v$ coefficient for $y$. Moreover, because the &lt;em&gt;total&lt;/em&gt; squares of residuals from the two regressions are to be minimized, it will usually not be the case that either set of coefficients gives the best estimate for $x$ or $y$ alone.  This is confirmed in the example below, which carries out the two real regressions separately and compares their solutions to the complex regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis makes it apparent that rewriting the complex regression in terms of the real parts (1) complicates the formulas, (2) obscures the simple geometric interpretation, and (3) would require a generalized multivariate multiple regression (with nontrivial correlations among the variables) to solve.  We can do better.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Example&lt;/h3&gt;&#10;&#10;&lt;p&gt;As an example, I take a grid of $w$ values at integral points near the origin in the complex plane.  To the transformed values $w\beta$ are added iid errors having a bivariate Gaussian distribution: in particular, the real and imaginary parts of the errors are not independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is difficult to draw the usual scatterplot of $(w_j, z_j)$ for complex variables, because it would consist of points in four dimensions.  Instead we can view the scatterplot matrix of their real and imaginary parts.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/a6xiV.png&quot; alt=&quot;Scatterplot matrix&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Ignore the fit for now and look at the top four rows and four left columns: these display the data.  The circular grid of $w$ is evident in the upper left; it has $81$ points.  The scatterplots of the components of $w$ against the components of $z$ show clear correlations.  Three of them have negative correlations; only the $y$ (the imaginary part of $z$) and $u$ (the real part of $w$) are positively correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;For these data, the true value of $\beta$ is $(-20 + 5i, -3/4 + 3/4\sqrt{3}i)$.  It represents an expansion by $3/2$ and a counterclockwise rotation of 120 degrees followed by translation of $20$ units to the left and $5$ units up.  I compute three fits: the complex least squares solution and two OLS solutions for $(x_j)$ and $(y_j)$ separately, for comparison.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fit            Intercept          Slope(s)&#10;True           -20    + 5 i       -0.75 + 1.30 i&#10;Complex        -20.02 + 5.01 i    -0.83 + 1.38 i&#10;Real only      -20.02             -0.75, -1.46&#10;Imaginary only          5.01       1.30, -0.92&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It will always be the case that the real-only intercept agrees with the real part of the complex intercept and the imaginary-only intercept agrees with the imaginary part fo the complex intercept.  It is apparent, though, that the real-only and imaginary-only slopes neither agree with the complex slope coefficients nor with each other, exactly as predicted.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's take a closer look at the results of the complex fit.  First, a plot of the residuals gives us an indication of their bivariate Gaussian distribution.  (The underlying distribution has marginal standard deviations of $2$ and a correlation of $0.8$.)  Then, we can plot the magnitudes of the residuals (represented by sizes of the circular symbols) and their arguments (represented by colors exactly as in the first plot) against the fitted values: this plot should look like a random distribution of sizes and colors, which it does.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/DNNbn.png&quot; alt=&quot;Residual plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, we can depict the fit in several ways.  The fit appeared in the last rows and columns of the scatterplot matrix (&lt;em&gt;q.v.&lt;/em&gt;) and may be worth a closer look at this point.  Below on the left the fits are plotted as open blue circles and arrows (representing the residuals) connect them to the data, shown as solid red circles.  On the right the $(w_j)$ are shown as open black circles filled in with colors corresponding to their arguments; these are connected by arrows to the corresponding values of $(z_j)$.  Recall that each arrow represents an expansion by $3/2$ around the origin, rotation by $120$ degrees, and translation by $(-20, 5)$, plus that bivariate Guassian error.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/e8BT8.png&quot; alt=&quot;Fit as transformation&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;These results, the plots, and the diagnostic plots all suggest that the complex regression formula works correctly and achieves something different than separate linear regressions of the real and imaginary parts of the variables.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Code&lt;/h3&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; code to create the data, fits, and plots appears below.  Note that the actual solution of $\hat\beta$ is obtained in a single line of code.  Additional work--but not too much of it--would be needed to obtain the usual least squares output: the variance-covariance matrix of the fit, standard errors, p-values, etc.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# Synthesize data.&#10;# (1) the independent variable `w`.&#10;#&#10;w.max &amp;lt;- 5 # Max extent of the independent values&#10;w &amp;lt;- expand.grid(seq(-w.max,w.max), seq(-w.max,w.max))&#10;w &amp;lt;- complex(real=w[[1]], imaginary=w[[2]])&#10;w &amp;lt;- w[Mod(w) &amp;lt;= w.max]&#10;n &amp;lt;- length(w)&#10;#&#10;# (2) the dependent variable `z`.&#10;#&#10;beta &amp;lt;- c(-20+5i, complex(argument=2*pi/3, modulus=3/2))&#10;sigma &amp;lt;- 2; rho &amp;lt;- 0.8 # Parameters of the error distribution&#10;library(MASS) #mvrnorm&#10;set.seed(17)&#10;e &amp;lt;- mvrnorm(n, c(0,0), matrix(c(1,rho,rho,1)*sigma^2, 2))&#10;e &amp;lt;- complex(real=e[,1], imaginary=e[,2])&#10;z &amp;lt;- as.vector((X &amp;lt;- cbind(rep(1,n), w)) %*% beta + e)&#10;#&#10;# Fit the models.&#10;#&#10;print(beta, digits=3)&#10;print(beta.hat &amp;lt;- solve(Conj(t(X)) %*% X, Conj(t(X)) %*% z), digits=3)&#10;print(beta.r &amp;lt;- coef(lm(Re(z) ~ Re(w) + Im(w))), digits=3)&#10;print(beta.i &amp;lt;- coef(lm(Im(z) ~ Re(w) + Im(w))), digits=3)&#10;#&#10;# Show some diagnostics.&#10;#&#10;par(mfrow=c(1,2))&#10;res &amp;lt;- as.vector(z - X %*% beta.hat)&#10;fit &amp;lt;- z - res&#10;s &amp;lt;- sqrt(Re(mean(Conj(res)*res)))&#10;col &amp;lt;- hsv((Arg(res)/pi + 1)/2, .8, .9)&#10;size &amp;lt;- Mod(res) / s&#10;plot(res, pch=16, cex=size, col=col, main=&quot;Residuals&quot;)&#10;plot(Re(fit), Im(fit), pch=16, cex = size, col=col,&#10;     main=&quot;Residuals vs. Fitted&quot;)&#10;&#10;plot(Re(c(z, fit)), Im(c(z, fit)), type=&quot;n&quot;,&#10;     main=&quot;Residuals as Fit --&amp;gt; Data&quot;, xlab=&quot;Real&quot;, ylab=&quot;Imaginary&quot;)&#10;points(Re(fit), Im(fit), col=&quot;Blue&quot;)&#10;points(Re(z), Im(z), pch=16, col=&quot;Red&quot;)&#10;arrows(Re(fit), Im(fit), Re(z), Im(z), col=&quot;Gray&quot;, length=0.1)&#10;&#10;col.w &amp;lt;-  hsv((Arg(w)/pi + 1)/2, .8, .9)&#10;plot(Re(c(w, z)), Im(c(w, z)), type=&quot;n&quot;,&#10;     main=&quot;Fit as a Transformation&quot;, xlab=&quot;Real&quot;, ylab=&quot;Imaginary&quot;)&#10;points(Re(w), Im(w), pch=16, col=col.w)&#10;points(Re(w), Im(w))&#10;points(Re(z), Im(z), pch=16, col=col.w)&#10;arrows(Re(w), Im(w), Re(z), Im(z), col=&quot;#00000030&quot;, length=0.1)&#10;#&#10;# Display the data.&#10;#&#10;par(mfrow=c(1,1))&#10;pairs(cbind(w.Re=Re(w), w.Im=Im(w), z.Re=Re(z), z.Im=Im(z),&#10;            fit.Re=Re(fit), fit.Im=Im(fit)), cex=1/2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="10" CreationDate="2013-08-01T15:08:17.430" Id="66268" LastActivityDate="2013-10-04T21:11:36.993" LastEditDate="2013-10-04T21:11:36.993" LastEditorUserId="919" OwnerUserId="919" ParentId="66088" PostTypeId="2" Score="16" />
  <row Body="&lt;p&gt;You can't really say much about this data without knowing where the data comes from. To forecast you need a model. All models come with assumptions and if you can't tell whether these assumptions are held then you won't be able to have any confidence in the predictions. For example, in linear regression, you &lt;em&gt;assume&lt;/em&gt; that you data represents a process of the form: $$ y_i = \beta x_i + e_i$$, the $e$ accounting for the data not exactly fitting the straight line. It so happens that in order to have any confidence in your predictions from this linear model, every $e$ must be independent from the other. Can you guarantee that? Also, the $e$'s must be normally distributed, does the problem make that assumption sensible? There are other assumptions as well, but I hope you now understand why you can't just give a statistician data and say &quot;please predict what happens next&quot;. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-01T15:13:46.640" Id="66270" LastActivityDate="2013-08-01T15:13:46.640" OwnerUserId="12031" ParentId="66265" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm setting up a logistic regression that models a probability $\mathbb{P}_t$ of certain events. The probability is changing over time and I want to add proportions of past observations that estimate this probability. Suppose I have a proportion $q = \frac{\text{#positives}}{\text{#total}}=\frac{b}{c}$ that estimates $\mathbb{P}_t$ if times are relatively close. How do I add it as a feature in a logistic regression model?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My guess&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the logistic regression model is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \log \frac{p}{1-p} = x \cdot \beta,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;1) my intuition says add $\log q$ and $\log(1-q)$. Then our logistic regression model would clearly encompass the baseline model of $p=q$. In the case where the coefficient of $\log q$ is $+1$ and the coefficient of $\log(1-q)$ is $-1$ and other coefficients in $\beta$ are $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Another option, remembering that $q=\frac{b}{c}$, is to express&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\log{q}&amp;amp;= \log{b}-\log{c}\\&#10;\log{1-q}&amp;amp;=\log \frac{c-b}{c}=\log{(c-b)} - \log{c}.&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Here I would be tempted to add $\log{(b + 1)}, \log{(c + 1)}, \log{(c - b + 1)}.$ as features.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-01T22:27:21.673" Id="66302" LastActivityDate="2013-08-06T00:35:16.400" LastEditDate="2013-08-02T21:08:31.987" LastEditorUserId="11554" OwnerUserId="11554" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;feature-construction&gt;" Title="Adding proportions as features to logistic regression" ViewCount="155" />
  
  <row Body="Interval censoring means the value of a data point is only known to lie w/i a given interval. The most common example is when data have been rounded, eg, a value of 5 implies the original value was in the interval [4.5, 5.5)." CommentCount="0" CreationDate="2013-08-02T03:21:28.747" Id="66321" LastActivityDate="2013-08-02T03:21:28.747" LastEditDate="2013-08-02T03:21:28.747" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="4" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;What are the lesser known but powerful probabilistic inference algorithms?&lt;/p&gt;&#10;&#10;&lt;p&gt;Most references about probabilistic graphical models describe popular inference methods like Variable Elimination and Junction Tree. But I think that there are a huge number of other important probabilistic inference algorithms out there. Every once in a while I would stumble upon a paper that describes a method that I didn't hear about before, take for example &lt;a href=&quot;http://www.cs.ubc.ca/~murphyk/Papers/ff_uai01.pdf&quot; rel=&quot;nofollow&quot;&gt;The Factored Frontier Algorithm for Approximate Inference in DBNs&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. please try to add one algorithm per answer, with a brief description or points to related papers, if possible.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-02T07:59:50.960" Id="66330" LastActivityDate="2013-08-02T08:07:09.147" LastEditDate="2013-08-02T08:07:09.147" LastEditorUserId="22047" OwnerUserId="20179" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;references&gt;&lt;inference&gt;&lt;graphical-model&gt;&lt;big-list&gt;" Title="Lesser known but powerful probabilistic inference algorithms" ViewCount="75" />
  <row AnswerCount="2" Body="&lt;p&gt;I calculate multiple linear regression in R with &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(var ~ VAR1+VAR2+VAR3+VAR4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Do you know how to calculate R-squared change for each variable &lt;code&gt;VAR1&lt;/code&gt;, &lt;code&gt;VAR2&lt;/code&gt;, &lt;code&gt;VAR3&lt;/code&gt; ?&#10;Thank you&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-02T08:47:22.053" FavoriteCount="1" Id="66335" LastActivityDate="2013-08-05T08:17:52.253" LastEditDate="2013-08-02T15:30:16.290" LastEditorUserId="14860" OwnerUserId="28170" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;r-squared&gt;" Title="R squared change multiple linear regression" ViewCount="742" />
  <row Body="&lt;p&gt;If each of these 40 participants only ever see one condition, then you have a traditional between-subject design and would use independent samples tests. The way you describe your data formatting problem in SPSS at the end suggests that this in fact your situation. &lt;/p&gt;&#10;&#10;&lt;p&gt;If each participant undergoes both conditions, then observations under each condition are not independent anymore. You should then use a paired sample test. Note that it could even be the case if the participants are different but you have some ways to match them (say you select them in pairs based on age, gender, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually, the relevant question is whether the data comes from the same participants or not and that's typically the way these tests are taught but there is a broader notion of “independence” lurking underneath. If the participants were situations or objects or anything else that your rater had to evaluate, the logic would remain the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, the fact that there is one rater in common does not introduce any dependence as it is constant across the design. It's a person instead of a piece of equipment but logically it is not much different than using the same room or the same pen for the whole experiment. Should the color of the pen have some sort of effect on the response of interest, you would not be able to generalize your results to other pens but statistical inferences about your manipulation are not threatened as such.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-02T09:19:17.973" Id="66340" LastActivityDate="2013-08-02T09:25:11.563" LastEditDate="2013-08-02T09:25:11.563" LastEditorUserId="6029" OwnerUserId="6029" ParentId="66333" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;What counts as a good answer would depend on the instructions you received, the learning goals and contents of the course associated with the exam (if any). In some courses, you are expected to document the whole procedure and do all relevant computations by hand. In others, just printing out the right output from a statistical package is enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;What this online calculator did is a statistical test. What you could add to your answer to demonstrate your understanding:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The name of the test and a succinct explanation of the reasons you think it is appropriate.&lt;/li&gt;&#10;&lt;li&gt;A list of the assumptions of the test and why they are reasonable.&lt;/li&gt;&#10;&lt;li&gt;A statement of the hypotheses being tested.&lt;/li&gt;&#10;&lt;li&gt;An explicit formula for the test statistic/computation of the test statistic.&lt;/li&gt;&#10;&lt;li&gt;The name and parameters of the relevant statistical distribution, if applicable.&lt;/li&gt;&#10;&lt;li&gt;A &lt;em&gt;p&lt;/em&gt;-value/critical value for the statistic, justifying your conclusion that the difference is or is not significant.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Also, your answers are obviously incorrect. I don't know how comfortable you are with these notions but you can realize that by thinking about the link between power and sample size, even without looking at any computation or specific test result.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-02T11:24:59.460" Id="66351" LastActivityDate="2013-09-27T08:07:07.960" LastEditDate="2013-09-27T08:07:07.960" LastEditorUserId="6029" OwnerUserId="6029" ParentId="66348" PostTypeId="2" Score="4" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Your interpretation of the effect of Decline is not correct : If Decline switches from 0 to 1, the % impact of Decline on R&amp;amp;D is 100[exp(-0.852) - 1]. You should read &lt;a href=&quot;http://davegiles.blogspot.fr/2011/03/dummies-for-dummies.html&quot; rel=&quot;nofollow&quot;&gt;Dave Gile's post&lt;/a&gt; for the interpretation of a dummy variable in a log-linear model. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You should also be careful with your interpretation since I would suspect an omitted variable bias. If you have an unobserved bad shock for the firm, it may affect simultaneously the decline and the R&amp;amp;D expenditures. In that case, you would have to find a set of instrumental variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2013-08-02T11:25:57.250" Id="66352" LastActivityDate="2013-08-02T11:59:03.637" LastEditDate="2013-08-02T11:59:03.637" LastEditorUserId="25965" OwnerUserId="25965" ParentId="66347" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;A beta distribution has the form $p(\theta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}$. A beta(1,1) distribution has parameters $(\alpha, \beta)=(1,1)$. (Unfortunately, this kind of statistical short-hand places a burden on the reader to know how the particular model is parameterized!)&lt;/p&gt;&#10;&#10;&lt;p&gt;The beta prior with a binomial likelihood (fixed number of trials with binary outcomes and fixed probabilities of success/failure) has the property of conjugacy, which allows the posterior (the product of the prior and the likelihood) to be written in closed form:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\begin{split}&#10;p(\theta|y) &amp;amp;= \frac{p(y|\theta)p(\theta)}{p(y)} \\&#10;~\\&#10;~\\&#10;&amp;amp;\propto\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}*\binom{n}{y}\theta^y(1-\theta)^{n-y} \\&#10;~\\&#10;~\\&#10;&amp;amp;\propto\theta^{\alpha-1}(1-\theta)^{\beta-1}*\theta^y(1-\theta)^{n-y} \\&#10;~\\&#10;&amp;amp;\propto\theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1} \\&#10;~\\&#10;&amp;amp;=\frac{\Gamma(\alpha+y-1)\Gamma(\beta+n-y-1)}{\Gamma(\alpha+\beta+n-1)}\theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1}&#10;\end{split}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;For the particular example in the text, the author is indicating that a beta(1,1) prior with data n=10 and y=8 produces a beta(1+8,1+2)=beta(9,3) posterior distribution on $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This closed-form expression is convenient, but by no means necessary. Multiplying probability densities can be done the same way as multiplying other mathematical expressions; the difficulties arrive since many products of densities are not as easily rewritten as the beta prior/binomial likelihood. Fortunately, this is where computers pick up the slack.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-02T12:56:20.880" Id="66358" LastActivityDate="2013-08-10T22:39:47.803" LastEditDate="2013-08-10T22:39:47.803" LastEditorUserId="7290" OwnerUserId="22311" ParentId="66315" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;You may want to look into John Myles White's &lt;a href=&quot;http://cran.r-project.org/web/packages/TextRegression/&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;TextRegression&lt;/code&gt; package&lt;/a&gt; for &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-02T14:41:53.223" Id="66367" LastActivityDate="2013-08-02T14:41:53.223" OwnerUserId="28051" ParentId="48445" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;I do not believe it is possible as of SAS 9.3.  Modeling procedures in SAS that do repeated measure modeling have a &lt;code&gt;REPEATED&lt;/code&gt; statement.  &lt;code&gt;PROC QUANTREG&lt;/code&gt; does not have this.  Nor does it have a &lt;code&gt;RANDOM&lt;/code&gt; statement.&lt;/p&gt;&#10;&#10;&lt;p&gt;I honestly don't think I've read of any implementation of quantile regression that facilitates the concept of random effects.  But I'd like to know about it!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-02T20:41:33.253" Id="66393" LastActivityDate="2013-08-02T20:41:33.253" OwnerUserId="8120" ParentId="66383" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you assume that the weaker players won't gang up on the stronger player (a very strong assumption!), then a reasonable model would be the following.  (I'm following the notation of the &quot;theory&quot; section of &lt;a href=&quot;http://en.wikipedia.org/wiki/Elo_rating_system&quot; rel=&quot;nofollow&quot;&gt;the Wikipedia article on ELO.&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;let $R_A, R_B, R_C$ be the ratings of the three players.&lt;/li&gt;&#10;&lt;li&gt;let $Q_A = 10^{R_A/400}$; define $Q_B, Q_C$ similarly.&lt;/li&gt;&#10;&lt;li&gt;the probability of $A$ winning is $Q_A/(Q_A + Q_B + Q_C)$ and similarly for the other two players.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;With the numbers you gave, $C$ has a probability of about $0.613$ of winning, and $A, B$ each have probability $0.194$ of winning.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This seems like the &quot;obvious&quot; generalization of the Elo math.  The most obvious problem, to me, is that I wouldn't know how to update these ratings after a multi-player game is played.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-02T23:09:28.623" Id="66398" LastActivityDate="2013-08-02T23:09:28.623" OwnerUserId="98" ParentId="63219" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;This is a question to reflect some very basic understanding of the logic of the chi-square for a planned introductory essay.       &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;[update] I've tried to improve the question with clearer examples and better focusing of the problems upon which I stumbled. It might still be a bit weak but I can't do better at the moment&lt;/em&gt;        &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm used to the chi-square as a measure for the deviance from an expected discrete distribution, where the expected frequencies are computed via the marginal frequencies of a crosstab over the empirical data of two categorical items. Let's discuss the 2x2-table here which has one degree of freedom. Assume the marginal row frequencies being [53,47] and the marginal column frequencies being [40,60]  giving the expected frequencies as &#10;$$    {\bf A}: \text{ expectation for cell-frequencies by equal ratios} \\&#10;\small \begin{array} {r|rr|r} &#10;&amp;amp;0&amp;amp;1&amp;amp;all\\ \hline&#10;0&amp;amp;21.2&amp;amp;18.8&amp;amp;40\\&#10;1&amp;amp;31.8&amp;amp;28.2&amp;amp;60\\ \hline&#10;all&amp;amp;53&amp;amp;47&amp;amp;100&#10;     \end{array}  $$&lt;/p&gt;&#10;&#10;&lt;p&gt;First I began to think what it means, that the empirical contingency table is bounded by the minimal entry of the marginal frequencies: in this case the cell [0,0] can vary between &lt;em&gt;0&lt;/em&gt; and &lt;em&gt;40&lt;/em&gt; only, so we have at most 41 possible outcomes depending on the possible frequencies in that cell. If we assume a normal random process, which generates that frequencies, the (expected) frequency in this cell should be centered around the mean of them &lt;em&gt;20&lt;/em&gt;:       &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ {\bf B}:  \text{ means of possible ranges for frequencies in cells}\\&#10;\small \begin{array} {r|rr|r} &#10;&amp;amp;0&amp;amp;1&amp;amp;all\\ \hline&#10;0&amp;amp;20&amp;amp;20&amp;amp;40\\&#10;1&amp;amp;33&amp;amp;27&amp;amp;60\\ \hline&#10;all&amp;amp;53&amp;amp;47&amp;amp;100&#10;     \end{array} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;This expectation in cell[0,0] is not equal to the expected frequency of 21.2 .&#10;First question: can the latter concept be brought/translated into the first (and standard) one? How could the difference (and relation or possible non-relation) between that two concepts be best explained?    &lt;/p&gt;&#10;&#10;&lt;p&gt;To answer this myself I went back one further step and asked, what does it mean at all to base the chi-square computation on the sample's marginal distribution if this distribution is itself subject of random...     &lt;/p&gt;&#10;&#10;&lt;p&gt;So I generated a &quot;population&quot; data set with &lt;em&gt;N=10000&lt;/em&gt; cases distributed exactly like our  table &lt;strong&gt;A&lt;/strong&gt; and took &lt;em&gt;1000&lt;/em&gt; random samples each with &lt;em&gt;n=100&lt;/em&gt;. Over the &lt;em&gt;1000&lt;/em&gt; samples I've got different marginal frequencies and consequently, each sample has different parameters for its expected frequencies and so for its possible chi-square.&lt;br&gt;&#10;The table of the deviances of the empirical marginal frequencies from the population marginal frequencies was &lt;/p&gt;&#10;&#10;&lt;p&gt;$$  {\bf C}: \text{ deviations of empirical marginal frequencies from population} \\&#10;\small  \begin{array} {r|rrrr} &#10;\text{dev } &amp;amp; \text{dev from}&amp;amp; \text{dev from}&amp;amp; \text{dev from}&amp;amp; \text{dev from}\\&#10;\text{value} &amp;amp; 53 &amp;amp; 47 &amp;amp; 40 &amp;amp; 60 \\ \hline&#10;-18&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;1\\    -15&amp;amp;1&amp;amp;1&amp;amp;1&amp;amp;1\\   -14&amp;amp;1&amp;amp;1&amp;amp;0&amp;amp;0\\   -13&amp;amp;2&amp;amp;4&amp;amp;1&amp;amp;1\\&#10;-12&amp;amp;2&amp;amp;6&amp;amp;5&amp;amp;4\\   -11&amp;amp;10&amp;amp;9&amp;amp;5&amp;amp;5\\    -10&amp;amp;8&amp;amp;12&amp;amp;7&amp;amp;14\\   -9&amp;amp;13&amp;amp;15&amp;amp;17&amp;amp;17\\&#10;-8&amp;amp;27&amp;amp;20&amp;amp;21&amp;amp;23\\   -7&amp;amp;34&amp;amp;38&amp;amp;26&amp;amp;31\\   -6&amp;amp;45&amp;amp;35&amp;amp;31&amp;amp;37\\   -5&amp;amp;53&amp;amp;43&amp;amp;62&amp;amp;50\\&#10;-4&amp;amp;46&amp;amp;46&amp;amp;50&amp;amp;49\\   -3&amp;amp;81&amp;amp;67&amp;amp;67&amp;amp;63\\   -2&amp;amp;55&amp;amp;83&amp;amp;84&amp;amp;67\\   -1&amp;amp;85&amp;amp;71&amp;amp;97&amp;amp;80\\&#10;0&amp;amp;86&amp;amp;86&amp;amp;83&amp;amp;83\\   1&amp;amp;71&amp;amp;85&amp;amp;80&amp;amp;97\\   2&amp;amp;83&amp;amp;55&amp;amp;67&amp;amp;84\\   3&amp;amp;67&amp;amp;81&amp;amp;63&amp;amp;67\\&#10;4&amp;amp;46&amp;amp;46&amp;amp;49&amp;amp;50\\   5&amp;amp;43&amp;amp;53&amp;amp;50&amp;amp;62\\    6&amp;amp;35&amp;amp;45&amp;amp;37&amp;amp;31\\   7&amp;amp;38&amp;amp;34&amp;amp;31&amp;amp;26\\&#10;8&amp;amp;20&amp;amp;27&amp;amp;23&amp;amp;21\\&#10;9&amp;amp;15&amp;amp;13&amp;amp;17&amp;amp;17\\10&amp;amp;12&amp;amp;8&amp;amp;14&amp;amp;7\\11&amp;amp;9&amp;amp;10&amp;amp;5&amp;amp;5\\12&amp;amp;6&amp;amp;2&amp;amp;4&amp;amp;5\\&#10;13&amp;amp;4&amp;amp;2&amp;amp;1&amp;amp;1\\14&amp;amp;1&amp;amp;1&amp;amp;0&amp;amp;0\\15&amp;amp;1&amp;amp;1&amp;amp;1&amp;amp;1\\18&amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;0&#10;     \end{array}&#10;  $$&#10;The table of occurrences of chi-square for those deviations of the empirical marginal frequencies from the population's is&#10;$$ {\bf D}: \text{ deviations of the empirical marginal frequencies} \\&#10;\text{ from that of the population }\\&#10; \text{(in terms of}\ \chi^2\ \text{values)}  \\  \begin{array} {r|rr} &#10;  &amp;amp; &amp;amp; \text{backwards} \\&#10;\chi^2 \text{-value} &amp;amp; \text{freq} &amp;amp; \text{cum freq} \\ \hline&#10;0.0&amp;amp;12.8&amp;amp;100.0\\&#10;0.5&amp;amp;22.0&amp;amp;87.2\\&#10;1.0&amp;amp;12.7&amp;amp;65.2\\&#10;1.5&amp;amp;11.2&amp;amp;52.5\\&#10;2.0&amp;amp;10.5&amp;amp;41.3\\&#10;2.5&amp;amp;7.8&amp;amp;30.8\\&#10;3.0&amp;amp;3.3&amp;amp;23.0\\&#10;3.5&amp;amp;4.8&amp;amp;19.7\\&#10;4.0&amp;amp;3.2&amp;amp;14.9\\&#10;4.5&amp;amp;2.5&amp;amp;11.7\\&#10;5.0&amp;amp;2.3&amp;amp;9.2\\&#10;5.5&amp;amp;1.0&amp;amp;6.9\\&#10;6.0&amp;amp;1.7&amp;amp;5.9\\&#10;6.5&amp;amp;1.2&amp;amp;4.2\\&#10;7.0&amp;amp;0.9&amp;amp;3.0\\&#10;7.5&amp;amp;0.6&amp;amp;2.1\\&#10;8.0&amp;amp;0.5&amp;amp;1.5\\&#10;9.0&amp;amp;0.1&amp;amp;1.0\\&#10;9.5&amp;amp;0.5&amp;amp;0.9\\&#10;10.0&amp;amp;0.1&amp;amp;0.4\\&#10;10.5&amp;amp;0.2&amp;amp;0.3\\&#10;17.0&amp;amp;0.1&amp;amp;0.1&#10;     \end{array}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm stumbling at the  simple fact that we compute the chi-square as deviation from an expected frequency - but where the expected frequency is based on an &lt;em&gt;empirical&lt;/em&gt; marginal frequency which is &lt;em&gt;itself&lt;/em&gt; subject of a random process - and for instance has some confidence-interval when I infer from the sample onto the population.&lt;br&gt;&#10;So - in reverse - having an empirical marginal distribution in our single empirical sample the conclusion to the population's marginal distribution is a matter of confidence intervals. &lt;em&gt;(Is here also a maximum-likelihood aspect lurking around anywhere ?)&lt;/em&gt;&lt;br&gt;&#10;This reminds me of the praxis, that we use the sample's variation as estimate of the population's variation-parameter, and do tests based on this assumption.    &lt;/p&gt;&#10;&#10;&lt;p&gt;Q: In the justification/formula for the chi-square-distribution as basis for the significance test - can we find some point, where the randomness of the marginal frequencies is reflected in the formulae?       &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-08-03T06:10:09.220" Id="66414" LastActivityDate="2013-08-03T10:50:30.623" LastEditDate="2013-08-03T10:50:30.623" LastEditorUserId="22047" OwnerUserId="1818" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;education&gt;" Title="Chi-square of a sample with respect to a known population's marginal distribution?" ViewCount="136" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I aim to discriminate between three populations by using a maximum likelihood classifier. The idea is that every population has a unique distribution $\hat{f_i} (x)$. The pdf  $\hat{f_i}(x)$  has been estimated on a set of training data. A new set of observations $(x_1,...x_p)$ will be assigned to the population for which &#10;$\log(q_i)+\sum_{j=1}^p \log(\hat{f_i}(x_j))$ is maximized. I want to investigate if the difference in the likelihoods are significant. How should I a perform a likelihood ratio test? I could of course perform the &quot;standard&quot; test in which I test $\lambda=\frac{L_0}{L_1} $  and then take $\chi^2= -2 \log \lambda$, but as I understand this test is used when there is a difference in the number of parameters between $L_1$ and $L_0$ . &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Let L0 be the maximum value of the likelihood when the parameters are restricted (and reduced in number) based on the assumption. Assume k parameters were lost (i.e., L0 has k less parameters than L1).&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;Or is this correct? Otherwise how should the test be performed?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-03T12:55:58.690" FavoriteCount="1" Id="66425" LastActivityDate="2013-08-04T10:55:43.970" LastEditDate="2013-08-04T10:55:43.970" LastEditorUserId="22047" OwnerUserId="28769" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;&lt;maximum-likelihood&gt;&lt;likelihood-ratio&gt;" Title="Likelihood-ratio test for different distributions" ViewCount="194" />
  <row Body="&lt;p&gt;I'm not sure about the predict method comment but a primary issue is related to generating easily interpretable variance measures, not variance measures per se. Bates isn't commenting in the first quote on whether you can do it, just what it means.&lt;/p&gt;&#10;&#10;&lt;p&gt;Take a simple multi-level model of a two level repeated measures design. Let's say you have the following data where each line is a subject:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4CW2k.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;code&gt;lmer&lt;/code&gt; the model could be expressed as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y ~ x + (1|subject)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You're predicting the y-value from x as a fixed effect (the difference between A and B); and the intercept a random effect**. Look carefully at the graph and note that while there is variability in the x effect for each subject (each line's slope) it's relatively small compared to the variability across subjects (the height of each line).&lt;/p&gt;&#10;&#10;&lt;p&gt;The model parses these two sets of variability and each one is meaningful. You can use the random effects to predict heights of lines and you can use fixed effects of x to predict slopes. You could even use the two combined to work our individual y-values. But what you can't do is really say anything meaningful &lt;em&gt;with respect to your model&lt;/em&gt; when you combine the variability of slopes and heights of lines together. You need to talk about the variability of your slopes and heights of lines separately. That's a &lt;em&gt;feature&lt;/em&gt; of the model, not a liability.&lt;/p&gt;&#10;&#10;&lt;p&gt;You will have a variability of the effect of x that's relatively easily estimated. You could say something about a confidence interval around that. But note that, this confidence interval is going to have small relation to the prediction of any particular y value because the y value is influenced by a combination of effect and subject variance that's different from the variability of the effect alone.&lt;/p&gt;&#10;&#10;&lt;p&gt;When Bates writes things like you've quoted I imagine he's often thinking of much more complex multi-level designs that this doesn't even approach. But even if you just consider this simple example you come down to wondering what kind of real meaning can be extracted from combining all of the variance measures together.&lt;/p&gt;&#10;&#10;&lt;p&gt;** I ignored the fixed effect of intercept for simplicity and just treat it as a random effect. You could extract similar conclusions from an even simpler model with a random and fixed intercept only but I think that it would be harder to convey. In that case, again, the fixed effect and random effect are parsed for a reason and mean different things and putting their variability back together for predicted values causes that variability to make little sense with respect to the model.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-08-03T13:50:33.593" Id="66428" LastActivityDate="2013-08-03T15:23:32.597" LastEditDate="2013-08-03T15:23:32.597" LastEditorUserId="601" OwnerUserId="601" ParentId="66421" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I see the quasi-poisson as a technical fix; it allows you to estimate as an additional parameter $\phi$, the dispersion parameter. In the Poisson $\phi = 1$ by definition. If your data are not as or more dispersed than that, the standard errors of the model coefficients are biased. By estimating $\hat{\phi}$ at the same time as estimating the other model coefficients, you can provide a correction to the model standard errors, and hence their test statistics and associated $p$-values. This is just a correction to the model assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The negative binomial is a more direct model for the overdispersion; that the data generating process is or can be approximated by a negative-binomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;The quasi-Poisson also introduces a whole pile of practical issues such as it not having a true likelihood hence the whole stack of useful things for model selection, like likelihood ratio test, AIC, etc... (I know there is something called QAIC, but R's &lt;code&gt;glm()&lt;/code&gt; for example won't give you it).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-03T16:00:03.687" Id="66437" LastActivityDate="2013-08-12T16:58:41.733" LastEditDate="2013-08-12T16:58:41.733" LastEditorUserId="1390" OwnerUserId="1390" ParentId="66412" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;Given the following models, as examples, built from regression techniques:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y = \beta_0 + \beta_1X + \epsilon$  &lt;/p&gt;&#10;&#10;&lt;p&gt;$Y = \beta_0 \times X^{\beta_1} + \epsilon$  &lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_0$ and $\beta_1$ are regression-coefficients (or parameters) on above equations (regression models).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-03T19:28:56.713" Id="66455" LastActivityDate="2013-08-03T22:29:09.187" LastEditDate="2013-08-03T22:29:09.187" LastEditorUserId="22468" OwnerUserId="22468" PostTypeId="5" Score="0" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering how to estimate the actual correlation function when I have some noisy samples of some space. Lets say, I have a space and the locations in the space are variables following a multivariate gaussian distribution. I generate some samples with the given autocorrelation function and add some noise independently to all of them. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now from the samples I find an empirical correlation function and then fit a model autocorrelation function to it. This model might has some nugget effects or might not represent the true autocorrelation function. How to deal with this issue?&lt;/p&gt;&#10;&#10;&lt;p&gt;For eg, I generated some data with exponential correlation function with range 10. If I add no noise and try to fit a model correlation function(exponential) based upon some samples of the field. I get almost the actual function. &lt;/p&gt;&#10;&#10;&lt;p&gt;I generated data with exponential correlation function with range 10. I then got some samples from this function. I got the empirical correlation function and then fit a model (exponential) to it to get almost the same params i.e range of 9.5&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I add some white gaussian noise to the correlation/covariance function(I assume std to be 1) and then I tried to fit a model correlation function to the empirical one, I got range of 5 and also nugget of 0.8. How to estimate the underlying function in this case?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-04T13:48:05.477" Id="66494" LastActivityDate="2013-08-04T16:28:24.913" LastEditDate="2013-08-04T16:28:24.913" LastEditorUserId="12329" OwnerUserId="11475" PostTypeId="1" Score="2" Tags="&lt;autocorrelation&gt;&lt;white-noise&gt;&lt;variogram&gt;" Title="Estimating the autocorrelation function with some noisy observations" ViewCount="163" />
  <row Body="&lt;p&gt;In the equation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{align*}&#10;p(\left. q=x \right| s,f)&#10;&amp;amp;= \frac{ {s+f \choose s} x^{s+\alpha-1}(1-x)^{f+\beta-1} / \operatorname{B}(\alpha,\beta) }{ \int_{y=0}^1 \left({s+f \choose s} y^{s+\alpha-1}(1-y)^{f+\beta-1} / \operatorname{B}(\alpha,\beta)\right) dy } \\&#10;&amp;amp;= \frac{ {s+f \choose s} x^{s+\alpha-1}(1-x)^{f+\beta-1} / \operatorname{B}(\alpha,\beta) }{ {s+f \choose s} \left( \int_{y=0}^1 y^{s+\alpha-1}(1-y)^{f+\beta-1} \, dy \right) / \operatorname{B}(\alpha \, \beta) } \\&#10;&amp;amp;= \frac{ x^{s+\alpha-1}(1-x)^{f+\beta-1} }{ \int_{y=0}^1 y^{s+\alpha-1}(1-y)^{f+\beta-1} \, dy }&#10;\end{align*} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;the denominator matches the form of the beta function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \operatorname{B}(x,y) = \int_0^1 t^{x-1}(1-t)^{y-1} \, dt $$&lt;/p&gt;&#10;&#10;&lt;p&gt;so the denominator becomes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\int_{y=0}^1 y^{s+\alpha-1}(1-y)^{f+\beta-1} \, dy = \operatorname{B}(s+\alpha,f+\beta)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the original equation becomes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p(\left. q=x \right| s,f)&#10;= \frac{ x^{s+\alpha-1}(1-x)^{f+\beta-1} }{ \operatorname{B}(s+\alpha,f+\beta) }&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-04T14:59:51.827" Id="66502" LastActivityDate="2013-08-04T15:18:42.383" LastEditDate="2013-08-04T15:18:42.383" LastEditorUserId="7025" OwnerUserId="7025" ParentId="66495" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a dataset with two sets of variables defining each sample. I am performing pairwise correlations between each variable in the first set with every variable in the second set. &lt;/p&gt;&#10;&#10;&lt;p&gt;I essentially have $n$ samples. For each sample, I have two sets of variables $[a_1, ..., a_{m1}]$ and $[b_1, ..., b_{m2}]$ that define that particular sample. Correlating all the variables in $a$ with those in $b$ gives me a final correlation matrix of size $m_1 \times m_2$. In addition, some of the variables within set $a$ may be correlated with each other and some of the variables within set $b$ may also be correlated with each other, but I am not assessing those correlations. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to figure out:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;if I need to correct for multiple comparisons, and  &lt;/li&gt;&#10;&lt;li&gt;if I do, what method I should use. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have tried Bonferroni, but with the large number of comparisons I get an extremely small adjusted p-value. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-04T16:51:14.420" FavoriteCount="2" Id="66509" LastActivityDate="2015-02-05T15:29:22.327" LastEditDate="2013-08-04T17:19:13.793" LastEditorUserId="7290" OwnerUserId="28806" PostTypeId="1" Score="4" Tags="&lt;correlation&gt;&lt;statistical-significance&gt;&lt;multiple-comparisons&gt;" Title="Pairwise Correlations--Multiple Comparison Correction" ViewCount="2656" />
  <row AcceptedAnswerId="66540" AnswerCount="2" Body="&lt;p&gt;I'm having trouble understanding how to interpret/explain the end result of dimensionality reduction via PCA. Namely, I've attempted to code up a simple example in R but can't really say what happened. I started with three vectors (x,y,z) where x and y are highly correlated and z is veeery loosely related to x. After finding the top 2 principal components and multiplying back by the original data, I have what I believe to be the &quot;final result&quot;... but the plot of the new 2-D data appears entirely uncorrelated. What am I seeing? What are the axes? I expected to see something that resembled the data on the x-y plane. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have plots of the mean-subtracted data in 3d and the reduced plot but not enough points to post them. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-04T18:29:42.110" Id="66518" LastActivityDate="2013-11-21T23:26:04.033" OwnerUserId="28808" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;dimensionality-reduction&gt;" Title="Interpreting plot of PCA results (from 3 to 2 dimensions)" ViewCount="818" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am generating some simulated data from a multivariate gaussian distribution with a covariance matrix sigma. To add some noise, I added an identity matrix to the covariance matrix which depicts gaussian white noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then drew some samples from it, calculated the pairwise semivariance as&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma(h)=\frac{1}{2N} \sum{|x_i-x_j|^2}$ where N is the total number of observations that are h distance apart.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I fitted a model variogram to it. I found out the nugget to be equal to 1 which makes sense because I added gaussian white noise of variance 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I then realized that it is the semivariance not variance so I should have got 1/2 i.e 0.5 as nugget isn't it(approx). I am plotting not the variogram but the semi variogram so I am a bit confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suggestions? I am a bit confused. People are using the terms semi variogram and variogram with interchangeability. However, semi variogram is half of variogram. And people are like fitting variogram models(exponential, spherical etc) to the empirical semivariogram.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I am confused. The graph of empirical semivariogram and variogram is definitely different. Variogram is double of that of semivariogram. So if I fit the model variorum, the sill param should be different as well.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-04T19:53:40.940" Id="66529" LastActivityDate="2013-08-04T20:10:48.877" LastEditDate="2013-08-04T20:10:48.877" LastEditorUserId="12329" OwnerUserId="12329" PostTypeId="1" Score="2" Tags="&lt;covariance&gt;&lt;autocorrelation&gt;&lt;exponential&gt;&lt;variogram&gt;" Title="Confusion related to estimation of nugget" ViewCount="29" />
  <row Body="&lt;p&gt;Take an unbiased coin. The random variable &quot;Heads&quot; is equal in distribution to random variable &quot;Tails&quot;.  The variable &quot;heads&quot; is equal almost surely to Not(Heads).&#10;2 different unbiased coins are also equal in distribution, but they are not equal almost surely... I do not know the value of other coin from knowing value of the first.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-04T20:14:53.997" Id="66530" LastActivityDate="2013-08-04T20:14:53.997" OwnerUserId="27556" ParentId="66522" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Apologies in advance for the tedious beginner question.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to translate a least-squares problem from a manual process (using Excel for matrix transposition and multiplication) to using the Python &lt;a href=&quot;http://statsmodels.sourceforge.net/devel/example_formulas.html#ols-regression-using-formulas&quot; rel=&quot;nofollow&quot;&gt;statsmodels&lt;/a&gt; package. In this case, I'm performing an affine transform from a set of observed coordinates  to a set of ground coordinates in eastings (&lt;em&gt;E&lt;/em&gt;) and northings (&lt;em&gt;N&lt;/em&gt;). I've used the following formula to form the &lt;strong&gt;A&lt;/strong&gt; (design) matrix:&#10;$$&#10;\begin{equation}&#10;    f_i(a_{0}, a_{1}, a_{2}) = a_{0} + a_{1}x_{i} + a_{2}y_{i} \\&#10;    f_i(b_{0}, b_{1}, b_{2}) = b_{0} + b_{1}x_{i} + b_{2}y_{i}&#10;\end{equation}&#10;$$&#10;Which gives me a matrix that looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{bmatrix}&#10;    1 &amp;amp; E_1 &amp;amp; N_1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\&#10;    1 &amp;amp; E_n &amp;amp; N_n &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\&#10;    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; E_1 &amp;amp; N_1 \\&#10;    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; E_n &amp;amp; N_n \\&#10;\end{bmatrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and a &lt;strong&gt;b&lt;/strong&gt; vector, which is all &lt;em&gt;x&lt;/em&gt; co-ordinates,  followed by all &lt;em&gt;y&lt;/em&gt; co-ordinates:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{bmatrix}&#10;x_1 \\ x_n \\ y_1 \\ y_n&#10;\end{bmatrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I also construct a square covariance matrix, &lt;strong&gt;W&lt;/strong&gt;, using the square of the standard errors of the eastings and northings, arranged in a diagonal. This is used as the weight matrix during the least-squares process (the standard errors are assumed to be independent)&lt;/p&gt;&#10;&#10;&lt;p&gt;I then calculate:  &lt;/p&gt;&#10;&#10;&lt;p&gt;$A^TWA$, $A^TWb$, and $(A^TWA)^{-1}$, then multiply $(A^TWA)^{-1}$ by $A^TWb$ to determine a vector &lt;strong&gt;x&lt;/strong&gt;, which contains values for $a_0, a_1, a_2$ and $b_0, b_1, b_2$ &#10;5. Multiply &lt;strong&gt;A&lt;/strong&gt; by &lt;strong&gt;x&lt;/strong&gt;, and subtract &lt;strong&gt;b&lt;/strong&gt; from the result, to determine a residuals vector, &lt;strong&gt;v&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can calculate the unit variance ($\sigma0$) by obtaining the square root of $\frac{v^TWv}{observations - unknowns}$, and multiplying it by the &lt;em&gt;a priori&lt;/em&gt; standard error of each co-ordinate, in order to assess the quality (&lt;em&gt;a posteriori&lt;/em&gt; standard error) of the transform. I can also calculate the standard error of my &lt;strong&gt;x&lt;/strong&gt; vector (the diagonal values of &lt;strong&gt;Cx&lt;/strong&gt;) by multiplying $(A^TWA)^{-1}$ by $\sigma0^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now that the tedious step-by-step manual explanation is out of the way, let's say I have Pandas DataFrames for &lt;strong&gt;A&lt;/strong&gt;, &lt;strong&gt;b&lt;/strong&gt; and &lt;strong&gt;W&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;In [124]: A_matrix&#10;Out[124]: &#10;&amp;lt;class 'pandas.core.frame.DataFrame'&amp;gt;&#10;Int64Index: 108 entries, 0 to 107&#10;Data columns (total 6 columns):&#10;As             108  non-null values&#10;Eastings_a     108  non-null values&#10;Northings_a    108  non-null values&#10;Bs             108  non-null values&#10;Eastings_b     108  non-null values&#10;Northings_b    108  non-null values&#10;dtypes: float64(4), int64(2)&#10;&#10;In [125]: b_vector&#10;Out[125]:&#10;&amp;lt;class 'pandas.core.frame.DataFrame'&amp;gt;&#10;Int64Index: 108 entries, 0 to 107&#10;Data columns (total 1 columns):&#10;coordinates    108  non-null values&#10;dtypes: float64(1)&#10;&#10;In [162]: Weight_matrix&#10;Out[162]: &#10;&amp;lt;class 'pandas.core.frame.DataFrame'&amp;gt;&#10;Int64Index: 108 entries, 0 to 107&#10;Columns: 108 entries, 0 to 107&#10;dtypes: float64(108)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do I use &lt;code&gt;statsmodels.ols_regression&lt;/code&gt; to easily calculate my residuals and $\sigma 0$?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-04T21:47:34.610" Id="66538" LastActivityDate="2013-08-04T22:19:59.543" LastEditDate="2013-08-04T22:19:59.543" LastEditorUserId="7080" OwnerUserId="7080" PostTypeId="1" Score="1" Tags="&lt;least-squares&gt;&lt;python&gt;" Title="Using Pandas and statsmodels for ordinary least squares" ViewCount="717" />
  <row Body="&lt;p&gt;This is a common rookie error when using RF models (I'll put my hand up as a previous perpetrator). The forest that you build using the training set will in many cases fit the training data almost perfectly (as you are finding) when considered in totality. However, as the algorithm builds the forest it remembers the out-of-bag (OOB) prediction error, which is its best guess of the generalization error.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you send the training data back into the predict method (as you are doing) you get this almost perfect prediction (which is wildly optimistic) instead of the correct OOB error. Don't do this. Instead, the trained Forest object should have remembered within it the OOB error. I am unfamiliar with the scikit-learn implementation but looking at the documentation &lt;a href=&quot;http://scikit-learn.org/0.13/modules/generated/sklearn.ensemble.RandomForestClassifier.html&quot;&gt;here&lt;/a&gt; it looks like you need to specify oob_score=True when calling the fit method, and then the generalization error will be stored as oob_score_ in the returned object. In the R package &quot;randomForest&quot;, calling the predict method with no arguments on the returned object will return the OOB prediction on the training set. That lets you define the error using some other measure. Sending the training set back into the predict method will give you a different result, as that will use all the trees. I don't know if the scikit-learn implementation will do this or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is a mistake to send the training data back into the predict method in order to test the accuracy. It's a very common mistake though, so don't worry.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-05T01:54:20.100" Id="66546" LastActivityDate="2013-08-05T02:00:11.387" LastEditDate="2013-08-05T02:00:11.387" LastEditorUserId="10354" OwnerUserId="10354" ParentId="66543" PostTypeId="2" Score="18" />
  
  
  
  <row Body="&lt;p&gt;I think you are looking for the &lt;a href=&quot;http://www.jstor.org/stable/1912352?origin=crossref&amp;amp;&quot; rel=&quot;nofollow&quot;&gt;Heckman&lt;/a&gt; model (sample selection model/Heckit model). It is a two stage estimator. In Stata, you use command &lt;code&gt;heckman&lt;/code&gt; for that.   &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-05T11:50:18.893" Id="66573" LastActivityDate="2013-08-05T11:50:18.893" OwnerUserId="14860" ParentId="66555" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;In the R package AER you will find the function &lt;code&gt;dispersiontest&lt;/code&gt;, which implements a &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/030440769090014K&quot;&gt;Test for Overdispersion&lt;/a&gt; by Cameron &amp;amp; Trivedi (1990). &lt;/p&gt;&#10;&#10;&lt;p&gt;It follows a simple idea: In a Poisson model, the mean is $E(Y)=\mu$ and the variance is $Var(Y)=\mu$ as well. They are equal. The test simply tests this assumption as a null hypothesis against an alternative where $Var(Y)=\mu + c * f(\mu)$ where the constant $c &amp;lt; 0$ means underdispersion and $c &amp;gt; 0$ means overdispersion. The function $f(.)$ is some monoton function (often linear or quadratic; the former is the default).The resulting test is equivalent to testing $H_0: c=0$ vs. $H_1: c \neq 0$ and the test statistic used is a $t$ statistic which is asymptotically standard normal under the null.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R&amp;gt; library(AER)&#10;R&amp;gt; data(RecreationDemand)&#10;R&amp;gt; rd &amp;lt;- glm(trips ~ ., data = RecreationDemand, family = poisson)&#10;R&amp;gt; dispersiontest(rd,trafo=1)&#10;&#10;Overdispersion test&#10;&#10;data:  rd&#10;z = 2.4116, p-value = 0.007941&#10;alternative hypothesis: true dispersion is greater than 0&#10;sample estimates:&#10;dispersion &#10;    5.5658 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here we clearly see that there is evidence of overdispersion (c is estimated to be 5.57) which speaks quite strongly against the assumption of equidispersion (i.e. c=0). &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if you not use &lt;code&gt;trafo=1&lt;/code&gt;, it will actually do a test of $H_0: c^*=1$ vs. $H_1: c^* \neq 1$ with $c^*=c+1$ which has of course the same result as the other test apart from the test statistic being shifted by one. The reason for this, though, is that the latter corresponds to the common parametrization in a quasi-Poisson model.     &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-05T15:43:18.473" Id="66593" LastActivityDate="2013-09-03T20:06:54.477" LastEditDate="2013-09-03T20:06:54.477" LastEditorUserId="8413" OwnerUserId="8413" ParentId="66586" PostTypeId="2" Score="10" />
  <row AcceptedAnswerId="66640" AnswerCount="1" Body="&lt;p&gt;I am using the golub data set from R with the labels.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;labelgb &amp;lt;- factor(c(rep(&quot;ALL&quot;,27),rep(&quot;AML&quot;,11)))&#10;&#10;names(golub) &amp;lt;- labelgb&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I need to use the empirical Bayes method in the limma package and calculate p-values for the golub dataset using the &lt;code&gt;eBayes()&lt;/code&gt; function.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit = lmFit(golub)&#10;e = eBayes(fit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the &lt;code&gt;lmFit&lt;/code&gt; function what does the design argument do and if its crucial for the golub dataset to supply the design argument?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-05T17:14:28.893" Id="66600" LastActivityDate="2013-08-06T09:47:07.547" LastEditDate="2013-08-05T17:52:49.363" LastEditorUserId="22468" OwnerUserId="28843" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;bayes&gt;" Title="eBayes() lmFit()" ViewCount="1236" />
  
  <row Body="&lt;p&gt;This is a rather hand-made approach, and I would really appreciate some comment on it, (and the criticizing ones are usually the most helpful). If I understand correctly, the OP calculates sample means $\bar x_j$, where each sample contains the previous sample +1 observation from a new r.v. Denote $F_j$ the distribution of each sample mean. Then we can write  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathcal{T} \overset{def}{=} \sum_{j=1}^n \left(1-F_j(c)\right) = n- \sum_{j=1}^n F_j(c)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a sample size $m$ after which the distribution of the sample mean is almost normal, denote it $\hat G$. Then we can write&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathcal{T} = n- \sum_{j=1}^m F_j(c)-\sum_{j=m+1}^n \hat G_j(c) &amp;lt; n-\sum_{j=m+1}^n \hat G_j(c)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Solving $\hat G_j(c)$ we obtain &#10;$$\hat G_j(c) = 1- \Phi\left(\frac{\sqrt j}{\sigma}(\mu-c)\right) $$&#10;where $\Phi$ is the standard normal cdf, $\sigma$ is the standard deviation of the i.i.d process, and $\mu$ is its mean. Inserting into the bound and re-arranging we get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathcal{T} &amp;lt; m+\sum_{j=m+1}^n \Phi\left(\frac{\sqrt j}{\sigma}(-a)\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this bound depends also on the variance of the process. Is this a better bound than the one presented in the question? This will depend crucially on how &quot;quickly&quot; the distribution of the sample mean becomes &quot;almost normal&quot;. To give a numerical example, &lt;em&gt;assume&lt;/em&gt; that  $m= 30$. Assume also that the random variables are uniform in $[0,1]$. Then $\sigma = \sqrt \frac{1}{12}$ and $\mu = \frac 12$. Consider a 10% deviation from the mean, i.e. set $a=0.05$. then : already for $n=34$ the bound I propose (which is meaningful for $n&amp;gt;30$) becomes tighter. For $n=100$ the Hoeffding bound is $78.5$ while the bound I propose is $36.2$. The Hoeffding bound converges to $\approx 199.5$ while the bound I propose to $\approx 38.5$ If you increase $a$ the discrepancy between the two bounds reduces but remains visible: for a 20% deviation, $a=0.1$, the Hoeffding bound converges to $49.5$ while the bound I propose converges to $30.5$ (i.e the sum of the normal cdfs contributes very little to the overall bound).&lt;br&gt;&#10;Somewhat more generally, we note that for $n\rightarrow \infty$ the Hoeffding bound converges to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$H_b\rightarrow \frac{1}{e^{2 a^2}-1} $$&#10;while my bound to &#10;$$A_b \rightarrow m$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since for small values of $a$ (which is rather the case of interest) $H_b$ becomes a  large number, there is still the case that $A_b$ may outperform it in tightness, even if the sample is such that the distribution of the sample mean converges slowly to the normal distribution.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-05T19:40:11.777" Id="66607" LastActivityDate="2013-09-06T01:13:42.107" LastEditDate="2013-09-06T01:13:42.107" LastEditorUserId="28746" OwnerUserId="28746" ParentId="64432" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="66617" AnswerCount="1" Body="&lt;p&gt;I would like to generate pairs of random numbers with certain correlation. However, the usual approach of using a linear combination of two normal variables is not valid here, because a linear combination of uniform variables is not any more an uniformly distributed variable. I need the two variables to be uniform.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea on how to generate pairs of uniform variables with a given correlation?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-05T20:18:24.930" FavoriteCount="3" Id="66610" LastActivityDate="2013-08-06T15:59:42.527" OwnerUserId="17115" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;random-generation&gt;&lt;uniform&gt;" Title="Generate pairs of random numbers uniformly distributed and correlated" ViewCount="2512" />
  <row Body="&lt;p&gt;I'm not aware of a universal method to generate correlated random variables with any given marginal distributions. So, I'll propose an ad hoc method to generate pairs of uniformly distributed random variables with a given (Pearson) correlation.&#10;Without loss of generality, I assume that the desired marginal distribution is standard uniform (i.e., the support is $[0, 1]$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The proposed approach relies on the following:&lt;br&gt;&#10;a) For standard uniform random variables $U_1$ and $U_2$ with respective distribution functions $F_1$ and $F_2$, we have $F_i(U_i) = U_i$, for $i = 1, 2$.&#10;Thus, by definition &lt;a href=&quot;http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;Spearman's rho&lt;/a&gt; is&#10;$$&#10;\rho_{\rm S}(U_1, U_2) = {\rm corr}(F_1(U_1), F_2(U_2)) = {\rm corr}(U_1, U_2) .&#10;$$&#10;So, Spearman's rho and Pearson's correlation coefficient are equal (sample versions might however differ).&lt;/p&gt;&#10;&#10;&lt;p&gt;b) If $X_1, X_2$ are random variables with continuous margins and &lt;a href=&quot;http://en.wikipedia.org/wiki/Copula_%28probability_theory%29#Gaussian_copula&quot; rel=&quot;nofollow&quot;&gt;Gaussian copula&lt;/a&gt; with (Pearson) correlation coefficient $\rho$, then Spearman's rho is&#10;$$&#10;\rho_{\rm S}(X_1, X_2) = \frac{6}{\pi} \arcsin \left(\frac{\rho}{2}\right) .&#10;$$&#10;This makes it easy to generate random variables that have a desired value of Spearman's rho.&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The approach is to generate data from the Gaussian copula with an appropriate correlation coefficient $\rho$ such that the Spearman's rho corresponds to the desired correlation for the uniform random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Simulation algorithm&lt;/strong&gt;&lt;br&gt;&#10;Let $r$ denote the desired level of correlation, and $n$ the number of pairs to be generated.&#10;The algorithm is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Compute $\rho = 2\sin (r \pi/6)$.&lt;/li&gt;&#10;&lt;li&gt;Generate a pair of random variables from the Gaussian copula (e.g., &lt;a href=&quot;http://stats.stackexchange.com/a/64988/27403&quot;&gt;with this approach&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;Repeat step 2 $n$ times.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br&gt;&#10;The following code is an example of implementation of this algorithm using R with a target correlation $r = 0.6$ and $n = 500$ pairs.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## Initialization and parameters &#10;set.seed(123)&#10;r &amp;lt;- 0.6                            # Target (Spearman) correlation&#10;n &amp;lt;- 500                            # Number of samples&#10;&#10;## Functions&#10;gen.gauss.cop &amp;lt;- function(r, n){&#10;    rho &amp;lt;- 2 * sin(r * pi/6)        # Pearson correlation&#10;    P &amp;lt;- toeplitz(c(1, rho))        # Correlation matrix&#10;    d &amp;lt;- nrow(P)                    # Dimension&#10;    ## Generate sample&#10;    U &amp;lt;- pnorm(matrix(rnorm(n*d), ncol = d) %*% chol(P))&#10;    return(U)&#10;}&#10;&#10;## Data generation and visualization&#10;U &amp;lt;- gen.gauss.cop(r = r, n = n)&#10;pairs(U, diag.panel = function(x){&#10;          h &amp;lt;- hist(x, plot = FALSE)&#10;          rect(head(h$breaks, -1), 0, tail(h$breaks, -1), h$counts/max(h$counts))})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the figure below, the diagonal plots show histograms of variables $U_1$ and $U_2$, and off-diagonal plots show scatter plots of $U_1$ and $U_2$. &#10;&lt;img src=&quot;http://i.stack.imgur.com/YycJL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;By constuction, the random variables have uniform margins and a correlation coefficient (close to) $r$. But due to the effect of sampling, the correlation coefficient of the simulated data is not exactly equal to $r$.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cor(U)[1, 2]&#10;# [1] 0.5337697&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Simulation study&lt;/strong&gt;&lt;br&gt;&#10;The following simulation study repeated for target correlation $r= -0.5, 0.1, 0.6$ suggests that the distribution of the correlation coefficient converges to the desired correlation as the sample size $n$ increases.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## Simulation&#10;set.seed(921)&#10;r &amp;lt;- 0.6                                                # Target correlation&#10;n &amp;lt;- c(10, 50, 100, 500, 1000, 5000); names(n) &amp;lt;- n     # Number of samples&#10;S &amp;lt;- 1000                                               # Number of simulations&#10;&#10;res &amp;lt;- sapply(n,&#10;              function(n, r, S){&#10;                   replicate(S, cor(gen.gauss.cop(r, n))[1, 2])&#10;               }, &#10;               r = r, S = S)&#10;boxplot(res, xlab = &quot;Sample size&quot;, ylab = &quot;Correlation&quot;)&#10;abline(h = r, col = &quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bV442.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/5fJR0.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/cdnhB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-08-05T23:48:22.057" Id="66617" LastActivityDate="2013-08-06T15:59:42.527" LastEditDate="2013-08-06T15:59:42.527" LastEditorUserId="27403" OwnerUserId="27403" ParentId="66610" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Are covariates considered moderator variables or control variables? To elaborate on my question, I'm conducting a research study which has:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;variables that are supposed to affect the dependent variable but that I don't consider in any analysis (e.g., daytime the experiment done)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;variables that are used as exclusion criteria, so that some subjects are removed (filtered) from the analysis (e.g., having medical condition)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;variables that are supposed to influence the dependent variable and which I therefore include as covariates in my model (e.g., a GLM or MANCOVA).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What would you call each of these three kinds of variables?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-08-06T03:36:53.400" FavoriteCount="1" Id="66622" LastActivityDate="2014-02-04T17:00:40.707" LastEditDate="2014-02-04T16:14:06.000" LastEditorUserId="7290" OwnerUserId="28637" PostTypeId="1" Score="1" Tags="&lt;experiment-design&gt;&lt;interaction&gt;&lt;terminology&gt;&lt;statistical-control&gt;&lt;covariate&gt;" Title="Covariates considered moderator or control variables?" ViewCount="2337" />
  <row AnswerCount="0" Body="&lt;p&gt;I have run models using negative binomial &lt;code&gt;glm.nb()&lt;/code&gt; in R and have 3 top models with really close AIC (&amp;lt;2). I was thinking about doing a model averaging using MuMIn package.&#10;My problem is to know how to handle my covariates. In fact my three models do not have the exact same set of variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;M1 &amp;lt;- Response_variable ~ Cov1+Cov2 &#10;M2 &amp;lt;- Response_variable ~ Cov2:Cov3&#10;M3 &amp;lt;- Response_variable ~ Cov1+Cov2+Cov3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I average coefficients if the covariates are not in all the models? Do I have to add &quot;0&quot; to my models?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I tried using summary((model.avg(candidate models)) and modavg.glm()&#10;  but neither of those commands seems to want to compute my estimates&#10;  because my models are negative binomial. Although when I read the&#10;  MuMIn's pdf it said that glm.nb was supported by model.avg. Is there&#10;  somthing else that I should write on the command?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="4" CreationDate="2013-08-06T04:43:42.717" Id="66624" LastActivityDate="2013-11-05T06:53:21.580" LastEditDate="2013-08-07T03:55:10.620" LastEditorUserId="11875" OwnerUserId="11875" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;model&gt;&lt;average&gt;" Title="Model averaging - models with different covariates" ViewCount="240" />
  <row AnswerCount="2" Body="&lt;p&gt;So, we were reviewing a manuscript where the research group was developing an anxiety survey for upper level biology students.  The survey had 25 items; was administered via paper and online formats (N = 200-250 each time) and reliability was &gt; 0.8 with either format.  Secondly, they got 4 components using PCA, loadings were fairly high (&gt; 0.7) and subscale reliability was pretty high too.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on their high reliability and loadings, the researchers were contemplating on whether items needed to be excluded from the next administration of the survey.  From what they had stated, there were no items with cross loadings, no items that consistently showed up without loadings and their 4 components were meaningful.  So, why would high reliability, high loadings, a 25 item survey and a decent item:case ratio bring up the question of dropping any items at all?  Is there justification for that decision?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-06T05:05:52.260" Id="66626" LastActivityDate="2013-08-06T08:11:12.650" LastEditDate="2013-08-06T07:52:09.717" LastEditorUserId="6029" OwnerUserId="28687" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;survey&gt;" Title="Is there a statistical justification for removing items from a scale with good reliability?" ViewCount="220" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using 10 fold cross validation using the &lt;code&gt;CVlm()&lt;/code&gt; function from the DAAG package. This is part of the result shown:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Predicted    2.47e-04 2.26e-04 -0.000359  0.000335&#10;cvpred      -7.88e-05 3.72e-06 -0.000597  0.000322    &#10;y            1.47e-03 2.21e-03 -0.004676 -0.001969&#10;CV residual  1.55e-03 2.21e-03 -0.004078 -0.002291&#10;&#10;Sum of squares = 0    Mean square = 0    n = 48 &#10;&#10;Overall (Sum over all 48 folds) &#10;      ms &#10;9.81e-06 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What is the difference between “Predicted” and “cvpred”? If I change the seed in &lt;code&gt;CVlm()&lt;/code&gt; cvpred changes but Predicted remains the same. Can someone tell me how Predicted is calculated?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-06T06:47:59.533" FavoriteCount="2" Id="66630" LastActivityDate="2013-08-06T08:00:11.457" LastEditDate="2013-08-06T08:00:11.457" LastEditorUserId="6029" OwnerUserId="28858" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;cross-validation&gt;&lt;lm&gt;" Title="How to interpret output of CVlm() in R?" ViewCount="420" />
  <row AnswerCount="0" Body="&lt;p&gt;There are 10 datasets, each contains around 2000 records with 21 features and a binary label.&lt;br&gt;&#10;There have been many attempts to clean the data and even showing that there were so many redundant, duplicate and almost identical records in the original version of dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;In all versions of cleaned data, more than 50% of original records are removed which half of them are marked as duplicate or almost identical,&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to show these duplicated records and their connection in a plot, so I was wondering if there is any method to find the closest records?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, using some distance relevancy method like Mahalanobis Method, Euclidean Distance, or even Nearest Neighbourhood can we find very close points (records) in the dataset? (by setting a cut-point value for the distance to mark values less than that as very/almost identical) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-06T07:18:33.207" FavoriteCount="1" Id="66632" LastActivityDate="2013-08-07T16:41:01.840" LastEditDate="2013-08-07T16:41:01.840" LastEditorUserId="18192" OwnerUserId="18192" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;distance-functions&gt;&lt;data-cleaning&gt;&lt;euclidean&gt;" Title="How to find almost identical records in dataset using an algorithm (for plotting purposes)?" ViewCount="70" />
  <row AcceptedAnswerId="66661" AnswerCount="1" Body="&lt;p&gt;I am learning Gibbs Sampling, in which there is a step named sampling from conditional distributions. I don't understand: &#10;1. where is the conditional distribution from? From a general case, how can I get the conditional distribution? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help will be greatly appreciated. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-06T09:35:04.370" FavoriteCount="0" Id="66639" LastActivityDate="2013-08-06T16:00:53.827" OwnerUserId="28859" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;conditional-probability&gt;&lt;gibbs&gt;" Title="How can I sample from the conditional distribution?" ViewCount="81" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to do an a priori analysis of minimum sample size with G*Power and I have two questions about this:&lt;/p&gt;&#10;&#10;&lt;p&gt;First, I will test an interaction effect for a moderator variable. I am not sure which statistical test to choose in G*Power as I am less interested in the overall $R^2$ and more interested in whether the interaction is significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, I will do a mediation analysis following the Baron &amp;amp; Kenny approach. Which statistical test do I have to choose here to calculate the minimum sample size? &lt;/p&gt;&#10;&#10;&lt;p&gt;I know about the G*Power tutorials, but I couldn't find anything about moderation- and mediation-analysis there. So I would be grateful if anybody could help me!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-06T09:54:17.173" Id="66642" LastActivityDate="2013-08-06T10:34:48.373" LastEditDate="2013-08-06T10:34:48.373" LastEditorUserId="6029" OwnerUserId="28862" PostTypeId="1" Score="2" Tags="&lt;interaction&gt;&lt;power-analysis&gt;&lt;mediation&gt;" Title="Power analysis for moderator effect and mediation analysis" ViewCount="295" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am doing my master's thesis and I must compare various forecasting techniques at different frequencies of datasets. I am using my universities dataset, the REDD dataset, UCI dataset and CER Ireland dataset for this purpose. The data I use is in seconds for a time span of a month and this gives &gt; 3 million records.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been trying to understand how to make good use of all this data but couldn't exactly get to a solution. I have read &lt;a href=&quot;http://stats.stackexchange.com/questions/29424/time-series-modeling-with-high-frequency-data&quot;&gt;Time series modeling with high-frequency data&lt;/a&gt;, but I don't understand and couldn't find resources how to apply it to my problem. I have tried reading several blogs and books to get an understanding of time series forecasting but most literature has examples with granularity only as low as hourly data. Some references I found were about high granularity data but only for a short period of time. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have read in Prof. Rob Hyndman's blog that practically the ARIMA model can only calculate till 200 autoregressive points and if my understanding is correct then for data with a frequency in seconds, I could achieve daily trends only with $3600*24 = 86400$ previous values? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure how I should deal with this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is how the data look (the y-axis is watts):&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/uKC7j.png&quot; alt=&quot;http://postimg.org/image/i0txbwz6h/&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-06T15:00:58.473" FavoriteCount="1" Id="66655" LastActivityDate="2013-08-06T15:49:51.023" LastEditDate="2013-08-06T15:49:51.023" LastEditorUserId="7290" OwnerUserId="28857" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;frequency&gt;" Title="Forecasting with large, high frequency dataset" ViewCount="190" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a k x n dataset where k equals the number of variables and n equals the number of observations per variable. I know these data are correlated and I would like to whiten them with the ordinary whitening transformation. Inconveniently, k outnumbers n by far, so that when estimating the k x k covariance matrix it will not be invertible. To get around this problem, I estimated the covariance matrix using optimal shrinkage, but the obtained covariance matrix is not suited for whitening anymore. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be grateful for ideas on how to whiten these data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-06T23:03:59.307" Id="66693" LastActivityDate="2013-08-07T09:10:32.203" LastEditDate="2013-08-07T09:10:32.203" LastEditorUserId="6029" OwnerUserId="26853" PostTypeId="1" Score="1" Tags="&lt;covariance&gt;&lt;matrix&gt;&lt;shrinkage&gt;" Title="Whitening a dataset with fewer observations than variables" ViewCount="52" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I want find statistical support that dependence is inversely proportional to power. To do so, I have &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;~260 cases,&lt;/li&gt;&#10;&lt;li&gt;with four questions about dependence, and&lt;/li&gt;&#10;&lt;li&gt;one question about power&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The questions about dependence are on a continuous scale, whereas the question about power only allows three ordered answers (I am powerful, equilibrium, the other one is powerful).&lt;/p&gt;&#10;&#10;&lt;p&gt;To support the inverse proportionality of power and dependence (in this application), is the right way to do a ordinal logistic regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have plotted grouped error bars for the data (&lt;code&gt;error.bars.by()&lt;/code&gt;) and they show the case I want to prove quite clearly; however I suppose I need the right figures on top of that as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any suggestions and advice.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I had a look at some threads on Cross validated (&lt;a href=&quot;http://stats.stackexchange.com/questions/52088/how-to-test-correlation-between-an-ordinal-and-a-continuous-variable&quot;&gt;1&lt;/a&gt;,&lt;a href=&quot;http://stats.stackexchange.com/questions/33413/continuous-dependent-variable-with-ordinal-independent-variable&quot;&gt;2&lt;/a&gt;,&lt;a href=&quot;http://stats.stackexchange.com/questions/52188/ordinal-dependent-variable-with-continuous-independent-variables&quot;&gt;3&lt;/a&gt;) about similar questions, and as far as I understand there is not a clear answer to how the above described &lt;em&gt;mutual&lt;/em&gt; inverse correlation/proportionality could be tested. Correct me if I'm wrong. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Thanks for answers and comments so far. As far as I understand ordinal logistic regression helps me to find support for a relation of power with dependence, if I use dependence as predictor and power as dependent variable. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-07T12:02:18.850" FavoriteCount="1" Id="66732" LastActivityDate="2013-08-29T14:51:51.573" LastEditDate="2013-08-08T15:05:13.027" LastEditorUserId="22047" OwnerUserId="28053" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;ordinal&gt;&lt;manova&gt;" Title="Ordinal logistic regression to find support for inverse proportionality of a continuous predictor on an ordinal dependent variable" ViewCount="156" />
  
  <row Body="&lt;p&gt;BI is obsolete term used as marketing umbrella for various companies (like IBM, Oracle or SAP) to create illusion that they are relevant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data Visualization (DV) is a more specific and modern term: DV allowing users to see, explore, drilldown and interact with their data. It is a huge variety of DV vendors, but 3 of them are way ahead of others (Spotfire/TIBCO, Tableau, Qlikview/Qliktech) if you do not wish to do much coding and focus on your data and business problems.&#10;However if you willing to do some own development, you can find some excellent SDKs on market (D3 as a leading example), which can enable you to do useful visualizations but it will require more of your time and reduce your overall productivity.&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case Data Visualization cannot provide a complete solution to your analytical problems and needs to be used together with analytical tools like R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-07T12:43:27.437" Id="66739" LastActivityDate="2013-08-07T13:57:47.337" LastEditDate="2013-08-07T13:57:47.337" LastEditorUserId="28704" OwnerUserId="28704" ParentId="66733" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Does the lag identification procedure have to be applied using the cross correlation? If not, I would recommend the function &lt;code&gt;VARselect()&lt;/code&gt; from the package &lt;a href=&quot;http://cran.r-project.org/web/packages/vars/index.html&quot; rel=&quot;nofollow&quot;&gt;vars&lt;/a&gt; in the R programming environment. This function suggests different optimal lags according to some information criteria (AIC, HQ, SC and FPE). Once you have chosen the lag, you can perform a Granger causality test using another function from the same package &lt;code&gt;causality()&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-08-07T15:55:03.420" Id="66756" LastActivityDate="2013-08-12T02:12:58.143" LastEditDate="2013-08-12T02:12:58.143" LastEditorUserId="7290" OwnerUserId="26092" ParentId="66726" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="66828" AnswerCount="1" Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I have a data set which looks like this:! &lt;a href=&quot;http://i.imgur.com/vrDpuQ5.png&quot; rel=&quot;nofollow&quot;&gt;http://i.imgur.com/vrDpuQ5.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Histogram: &lt;img src=&quot;http://i.stack.imgur.com/eUomJ.png&quot; alt=&quot;http://i.imgur.com/2U5QtQM.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are about 10000 entries in total the Outcomes(EventId#) can be either $1,2,3,\ldots,18,19$ so from 1-19.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Although it is highly debatable if the events are independent, let's assume for simplicity's sake that they are for now.  The reality is that for the smaller EventId's (1,2,3) are more likely to occur after each other and especially after EventId 19. Then again, let's ignore this for now.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am looking specifically for what methodology/formula/principles would allow me to calculate the probability of EventID/Number $n$ to occur between two numbers $x_1$ and $x_2$. Or as can be seen in the first picture of the data set, the probability of the EventId/Number &quot;17&quot; occuring betwen two number/EventId &quot;18&quot;. The amount of numbers between $x_1$ and $x_2$ and whether they are located before or after the number $n$, is irrelevant to us, as long as they do not exceed the numbers $n$ and $x_1$ and $x_2$. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So throughout the entire data set we are looking for the probability of events / subseries(?) like these:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2,15,3,6,7,8,2,17,16,2,13,6,7,2,3,1,6,15,   &#10; **18,3,6,1,2,9,10,17,14,1,6,16,4,14,18**,2,3,5,1,2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or this:   &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;5,8,2,3,11,15,1,**18,2,3,17,3,11,6,14,16,2,1,6,18**,&#10; 2,1,1,9,8,1,13,11,2,7,9,1,2,3,7,2,1,7&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So how would I approach solving this problem.  What is the solution I am looking for?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried looking at almost everything the internet would offer me as a solution, cumulative distribution functions, empirical distribution functions, multivariate distribution, exponential distribution, etc., but from my understanding is that most of these things really just deal with a number/EventID occurring that is between number $x_1$ and Number $x_2$. Which is fine but it doesn't necessarily deal with for example: EventId 18 happening, and then waiting until EventId 17 happens and then waiting again until we have EventId 18. The same obviously could apply to EventId 5 happening then waiting for EventId 10 and then waiting for EventId 15 all the while not allowing any numbers/EventId in between to be higher than 10 and 15.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now assuming I can have / expand this data set into a much much larger data set and I have much more data available, sooner or later the obvious logical conclusion is in fact that EventId 1 2 and 3 tend to follow EventId 19 and 18 more often than for the other EventId's.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I would have to do more analysis on this using Markov chains or some other tool I assume which is why I said for the moment being, let's just assume that the EventId's are independent and random.&lt;/p&gt;&#10;&#10;&lt;p&gt;So yeah, I'm pretty much stuck, if anyone has an idea how to approach this problem then I'd appreciate it, quite a lot.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-07T16:46:06.053" Id="66766" LastActivityDate="2013-08-08T13:12:31.533" LastEditDate="2013-08-07T17:05:53.983" LastEditorUserId="22047" OwnerUserId="28910" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;conditional-probability&gt;" Title="Probability of an event occurring between two events?" ViewCount="94" />
  
  <row Body="&lt;p&gt;You can use the CSTABULATE command in the CS module to get the population proportion and standard error estimates. Use these to complete proportion tests either manually or in excel&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-07T20:10:47.680" Id="66786" LastActivityDate="2013-08-07T20:10:47.680" OwnerUserId="28920" ParentId="61401" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/1269394&quot;&gt;Shao and Sitter 1996&lt;/a&gt; demonstrate that the right approach is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Take a bootstrap sample, &lt;em&gt;respecting the dependencies in the data&lt;/em&gt; (see below);&lt;/li&gt;&#10;&lt;li&gt;Run &lt;strong&gt;one&lt;/strong&gt; imputation on this sample, estimating the imputation model and producing one model + noise replicate;&lt;/li&gt;&#10;&lt;li&gt;Run a complete case analysis on this;&lt;/li&gt;&#10;&lt;li&gt;Repeat 1-3 $B$ times;&lt;/li&gt;&#10;&lt;li&gt;Combine using the bootstrap rules (&lt;strong&gt;not&lt;/strong&gt; the Rubin rules).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$B$ must be bootstrap-large, not the Rubin-large... 5 hundred rather than 5. The biggest issue that comes up with complex survey data which are in the focus of Shao &amp;amp; Sitter's paper is that there are non-trivial dependencies and indepedencies present in complex survey data. By design, observations between strata are independent, and imputation that borrow strength across the whole data set violate that independence. By design, observations within the same PSU are correlated. Both of these effects need to be addressed by the bootstrap scheme. For complex surveys, this needs to be the &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/582039&quot;&gt;complex survey bootstrap&lt;/a&gt;. For time-series, this needs to be the &lt;a href=&quot;http://dx.doi.org/10.1214/aos/1176347265&quot;&gt;block bootstrap&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The process proposed by orizon (as &lt;a href=&quot;http://stats.stackexchange.com/a/56140/5739&quot;&gt;clarified by Stef&lt;/a&gt;) may be right, and I have been rolling it in my head for some while in the past couple of years, but never had the chance to really review it for statistical soundness.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-08T13:54:51.890" Id="66845" LastActivityDate="2013-08-08T13:54:51.890" OwnerUserId="5739" ParentId="56136" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="67508" AnswerCount="2" Body="&lt;p&gt;I want to calculate the standard errors of a fitted hyperbolic distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my notation the density is given by&#10;\begin{align*}&#10;H(l;\alpha,\beta,\mu,\delta)&amp;amp;=\frac{\sqrt{\alpha^2-\beta^2}}{2\alpha \delta K_1 (\delta\sqrt{\alpha^2-\beta^2})} exp\left(-\alpha\sqrt{\delta^2+(l-\mu)^2}+\beta(l-\mu)\right)&#10;\end{align*}&#10;I am using the HyperbolicDistr package in R. I estimate the parameters via the following command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hyperbFit(mydata,hessian=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me a wrong parameterization. I change it into my desired parameterization with the &lt;code&gt;hyperbChangePars(from=1,to=2,c(mu,delta,pi,zeta))&lt;/code&gt; command. Then I want to have the standard errors of my estimates, I can get it for the wrong parameterization with the &lt;code&gt;summary&lt;/code&gt; command. But this gives me the standard errors for the other parameterization. According to this &lt;a href=&quot;http://stats.stackexchange.com/questions/55714/standard-errors-of-hyperbfit&quot;&gt;thread&lt;/a&gt; I have to use the delta-method (I do &lt;em&gt;not&lt;/em&gt; want to use bootstrap or cross-validation or so).&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;hyperbFit&lt;/code&gt; code is &lt;a href=&quot;https://github.com/sjp/GeneralizedHyperbolic/blob/master/R/hyperbFit.R&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. And the &lt;code&gt;hyperbChangePars&lt;/code&gt; is &lt;a href=&quot;https://github.com/sjp/GeneralizedHyperbolic/blob/master/R/hyperbChangePars.R&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Therefore I know, that $\mu$ and $\delta$ stay the same. Therefore also the standard errors are the same, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;For transforming $\pi$ and $\zeta$ into $\alpha$ and $\beta$ I need the relationship between them. According to the code this is done as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;alpha &amp;lt;- zeta * sqrt(1 + hyperbPi^2) / delta&#10;beta &amp;lt;- zeta * hyperbPi / delta&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;So how do I have to code the delta-method to get the desired standard errors?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I am using these &lt;a href=&quot;http://uploadeasy.net/upload/ygvdc.rar&quot; rel=&quot;nofollow&quot;&gt;data&lt;/a&gt;.&#10;I first perform the delta-method according to this thread.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# fit the distribution&#10;&#10;hyperbfitdb&amp;lt;-hyperbFit(mydata,hessian=TRUE)&#10;hyperbChangePars(from=1,to=2,hyperbfitdb$Theta)&#10;summary(hyperbfitdb)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;summary(hyperbfitdb)&lt;/code&gt; gives the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Data:      mydata &#10;Parameter estimates:&#10;        pi           zeta         delta           mu    &#10;    0.0007014     1.3779503     0.0186331    -0.0001352 &#10;  ( 0.0938886)  ( 0.9795029)  ( 0.0101284)  ( 0.0035774)&#10;Likelihood:         615.992 &#10;Method:             Nelder-Mead &#10;Convergence code:   0 &#10;Iterations:         315 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and &lt;code&gt;hyperbChangePars(from=1,to=2,hyperbfitdb$Theta)&lt;/code&gt; gives the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   alpha.zeta     beta.zeta   delta.delta         mu.mu &#10;73.9516898823  0.0518715378  0.0186331187 -0.0001352342 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;now I define the variables in the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pi&amp;lt;-0.0007014 &#10;lzeta&amp;lt;-log(1.3779503)&#10;ldelta&amp;lt;-log(0.0186331)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I now run the code (second edit) and get the following result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; se.alpha&#10;         [,1]&#10;[1,] 13.18457&#10;&amp;gt; se.beta&#10;        [,1]&#10;[1,] 6.94268&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is this correct? I am wondering about the following: If I use a bootstrap-algorithm in the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;B = 1000 # number of bootstraps&#10;&#10;alpha&amp;lt;-NA&#10;beta&amp;lt;-NA&#10;delta&amp;lt;-NA&#10;mu&amp;lt;-NA&#10;&#10;&#10;# Bootstrap&#10;for(i in 1:B){&#10;  print(i)&#10;  subsample = sample(mydata,rep=T)&#10;  hyperboot &amp;lt;- hyperbFit(subsample,hessian=FALSE)&#10;  hyperboottransfparam&amp;lt;- hyperbChangePars(from=1,to=2,hyperboot$Theta)&#10;  alpha[i]    = hyperboottransfparam[1]&#10;  beta[i]    = hyperboottransfparam[2]&#10;  delta[i] = hyperboottransfparam[3]&#10;  mu[i] = hyperboottransfparam[4]&#10;&#10;}&#10;# hist(beta,breaks=100,xlim=c(-200,200))&#10;sd(alpha)&#10;sd(beta)&#10;sd(delta)&#10;sd(mu)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get &lt;code&gt;119.6&lt;/code&gt; for &lt;code&gt;sd(alpha)&lt;/code&gt; and &lt;code&gt;35.85&lt;/code&gt; for &lt;code&gt;sd(beta)&lt;/code&gt;. The results are very different? Is there a mistake or what is the problem here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-08T16:25:17.090" FavoriteCount="2" Id="66861" LastActivityDate="2013-08-18T09:46:03.370" LastEditDate="2013-08-18T09:46:03.370" LastEditorUserId="25675" OwnerUserId="25675" PostTypeId="1" Score="8" Tags="&lt;distributions&gt;&lt;standard-error&gt;&lt;fitting&gt;&lt;delta-method&gt;" Title="Standard errors of hyperbolic distribution estimates using delta-method?" ViewCount="261" />
  
  
  <row AcceptedAnswerId="66959" AnswerCount="1" Body="&lt;p&gt;I have been trying to quantify the heterogeneity of a given variable. I'd like to say that the distribution in calcite concentrations, for example, is more heterogeneous in one rock formation than another. &lt;/p&gt;&#10;&#10;&lt;p&gt;So far I've graphed and compared the variable distributions with overlain histograms and box plots for multiple variables, and by and large this is fine, but it takes time on the part of the reader to interpret the plots. I've been looking for more direct comparisons, say bar charts comparing an appropriate measure of sample variability in a formation e.g. the standard deviation, interquartile range, coefficient of variation, mean or median absolute deviation etc. But for various reasons I'm not satisfied with these approaches.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't help but think there must be a neater, succinct way of quantifying heterogeneity/variability&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Reply To Comments:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm investigating the residual error between some predicted and observed values. The residual errors for rock A are tightly distributed and for rock B they are widely non-normally distributed.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to tie the variability observed in the error, with variability observed in other rock properties. Both rocks A and B are variable in different respects, and I’m trying to highlight the variability that might matter. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I’ve been looking at ways of characterising the variability with a single value, say plotting all the IQR’s for each variable next to each other on a bar chart for example, where one might infer that rock A is generally more variable than rock B. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problems is this: variables like porosity are generally always normally and tightly distributed. Permeability by contrast is always non-normal, heavily skewed, varies over 6 or 7 orders of magnitude and typically follows a power trend. For permeability I’m not happy that an IQR really captures the variability, and secondly IQRs for different variables don’t graph well next to each other given the difference in magnitude of the parent unit. I’ve looked at normalised measures of variability, but these fudge the values with mean or medians that I don’t feel reflect the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps there is no ideal solution for such varied distribution styles and I should just present their distributions….?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-08T19:12:08.243" Id="66872" LastActivityDate="2013-08-09T17:16:56.667" LastEditDate="2013-08-09T09:50:52.920" LastEditorUserId="19931" OwnerUserId="19931" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;variance&gt;&lt;standard-deviation&gt;&lt;heteroscedasticity&gt;" Title="Quantifying heterogeneity" ViewCount="107" />
  <row Body="&lt;p&gt;You suggest you're happy with assuming approximate normality. Then it's straightforward - a linear combination of multivariate normal variables is itself normal. The mean and variance follow from elementary properties of mean and variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $X\sim N(\mathbf{\mu},\Sigma)$ then $a'X \sim N(a'\mathbf{\mu},a'\Sigma a)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of a sum, $a = \bf 1$. That is, the sum is normal, where the mean of the sum is the sum of the means and the variance of the sum is the sum of all variances + twice the sum of all pairwise covariances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, if the series are no too dependent and are iid, or independent and none of them have too large a variance relative to all the others, the CLT definitely applies, so if there are enough terms, you should have that $\sqrt n \bar x$ is normal. Note that $\sqrt n \bar x =  \frac{X_1+\dots+X_n}{\sqrt n} $.&lt;/p&gt;&#10;&#10;&lt;p&gt;From there, if you think the $n$ is big enough to use the normal approximation at some point, you can back out one for the sum as well (the CLT - being about limits - doesn't apply to an unscaled sum, but the quality of the approximation of the cdf at some specific $n$ carries over to the sum). However the sample sizes required for these to kick in may be quite large.&lt;/p&gt;&#10;&#10;&lt;p&gt;Beyond that, &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem#Beyond_the_classical_framework&quot; rel=&quot;nofollow&quot;&gt;convergence to normality of appropriately scaled sums&lt;/a&gt; can occur in a wide variety of situations. Again, sample sizes may need to be large.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-09T01:22:39.117" Id="66887" LastActivityDate="2013-08-09T05:45:50.587" LastEditDate="2013-08-09T05:45:50.587" LastEditorUserId="805" OwnerUserId="805" ParentId="66886" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;What you are asking for is a simultaneous plot of the survival function for one process and the cumulative incidence function (= 1- S(t)) for the competing process. The 'cmprsk' R package should be able to do the plots, but since the usual mode is to display both process as the cumulative incidence, you will need to do some work to transform the data so that one is S(t) and the other is H(t).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-09T06:38:50.183" Id="66901" LastActivityDate="2013-08-09T06:38:50.183" OwnerUserId="2129" ParentId="34610" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I agree with @Error404 about the assumption, esp. because the one other person who rated bubble '9' gave hull a 10. If you wanted to get fancy you could do multiple imputation; if you want to keep it simple, just drop that person. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-09T10:53:10.293" Id="66917" LastActivityDate="2013-08-09T10:53:10.293" OwnerUserId="686" ParentId="66910" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="72454" AnswerCount="1" Body="&lt;p&gt;As part of my thesis, I'm proving (or attempting to prove...) a few asymptotic results. Because these results depend on the condition number, I'd like to have some idea about the typical sizes of a condition numbers that crop up in social science research. That way, I can give some guidance about how large the sample size has to be before we reach the happy land of asymptopia.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be happy for any guidance. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My very specific&lt;/strong&gt; setup is as follows. For the standard Generalized Least Squares (GLS) model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y = X\beta + e \quad \quad \quad e \sim N(0, V\sigma^2) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $V$ is assumed to be known and positive definite, we define &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ X^- = (X^\top X)^{-1} X^\top \quad \quad \quad U = (I-XX^-)V$$&#10;and the condition number $\kappa$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \kappa = \frac{ \lambda_{\text{max}} }{ \lambda_{\text{min}} } $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $\lambda_\star$ values are the maximum and minimum eigenvalues of the matrix $U$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have pointers to references for the sizes of condition numbers in social science research? I don't even know where to look. Any pointers for either &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;OLS estimators (used incorrectly in a GLS context as posed above)&lt;/li&gt;&#10;&lt;li&gt;GLS estimators (correctly analyzed) &lt;/li&gt;&#10;&lt;li&gt;REML/ML estimators where $V$ is&#10;estimated and then conditioned upon, or &lt;/li&gt;&#10;&lt;li&gt;OLS fixed effect only models&#10;where $V$ is the identity matrix&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;would be most welcome!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-09T13:48:43.087" Id="66938" LastActivityDate="2013-10-10T14:09:23.710" LastEditDate="2013-08-09T14:00:16.667" LastEditorUserId="5507" OwnerUserId="5507" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;linear-model&gt;&lt;multicollinearity&gt;" Title="What are typically encountered condition numbers in social science?" ViewCount="66" />
  
  
  <row Body="&lt;p&gt;You might find this useful:&#10;&lt;a href=&quot;http://www.mathworks.com/help/stats/gmdistribution.fit.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/help/stats/gmdistribution.fit.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MU1 = [1 2];&#10;SIGMA1 = [2 0; 0 .5];&#10;MU2 = [-3 -5];&#10;SIGMA2 = [1 0; 0 1];&#10;X = [mvnrnd(MU1,SIGMA1,1000);mvnrnd(MU2,SIGMA2,1000)];&#10;&#10;scatter(X(:,1),X(:,2),10,'.')&#10;hold on&#10;options = statset('Display','final');&#10;obj = gmdistribution.fit(X,2,'Options',options);&#10;h = ezcontour(@(x,y)pdf(obj,[x y]),[-8 6],[-8 6]);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now these covariances are only diagonal.  You can fix that by changing numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;gmdistribution.fit&quot; is where you get the contour values.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here is a plot that this would create:&#10;&lt;img src=&quot;http://i.stack.imgur.com/H1yoG.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you can see that it creates two multivariate gaussian distributions.  You only need to create one.&lt;/p&gt;&#10;&#10;&lt;p&gt;The function that generates the data is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mvnrnd(MU1,SIGMA1,1000)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the &lt;a href=&quot;http://www.mathworks.com/help/stats/mvnrnd.html&quot; rel=&quot;nofollow&quot;&gt;mvnrnd, or mv-n-rnd&lt;/a&gt; is the multivariate normal random number generator.  Its inputs are the multivariate mean, covariance matrix, and desired sample count.  The output is an array of numbers of dimension informed by mu1.&lt;/p&gt;&#10;&#10;&lt;p&gt;The example covariance that you provided was 5 dimensional.  Here is something &quot;hackable&quot; that you should be able to convert for your own non-nefarious purposes.&lt;/p&gt;&#10;&#10;&lt;p&gt;%this creates a 5-dimensional mean&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MU1 = rand(1,5)+[1,2,3,4,5];  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this creates a 5x5 covariance, diagonally dominant&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;SIGMA1 = 23*rand(5,5)+eye(5);  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this is for consistent notation.  previously it stacked multiple multivariate gaussians.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X = [mvnrnd(MU1,SIGMA1,1000)];  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this plots the dots&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;scatter(X(:,1),X(:,2),10,'.');  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this lets multiple plots overlay&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hold on;  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this is a parm of the gm fit function.  It says &quot;display final result&quot;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;options = statset('Display','final'); &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this fits a gaussian to the data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;obj = gmdistribution.fit(X,1,'Options',options); &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;%this plots the contours of the fitted gaussian over the domain of the data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;h = ezcontour(@(x,y)pdf(obj,[x y]),[min(X(:,1)) max(X(:,1))],...&#10;                                   [min(X(:,2)) max(X(:,2))]); &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Best of luck.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-09T18:59:35.857" Id="66967" LastActivityDate="2013-08-10T14:53:22.913" LastEditDate="2013-08-10T14:53:22.913" LastEditorUserId="22452" OwnerUserId="22452" ParentId="66313" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Fixed factors are factors for which the only levels you want the model to apply are the ones you are observing.  Random factors are those for which you may want to generalize to levels other than those you are inserting into your model.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You will want &lt;em&gt;gender&lt;/em&gt; to be a &lt;strong&gt;fixed&lt;/strong&gt; effect, as there are only two genders (or at least main genders).&lt;/li&gt;&#10;&lt;li&gt;If &lt;em&gt;age&lt;/em&gt; were an IV (independent variable), it would almost certainly be a random effect, since you would almost certainly want to generalize to more than just the ages in your data.  I just mention that as an example of a random effect IV, though, as in &lt;em&gt;your&lt;/em&gt; model it is a DV (dependent variable).&lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;kill type&lt;/em&gt; I do not know enough about.  Is there a variety of kill types wider than those you are processing, or are you looking at the full set (or at least the full set you would ever want to consider)?  If it's the full set, it should be a fixed.  If not, it should be random. -- Edit:  I am leaving the reasoning here for future readers, but based on your comment &quot;The kill type is all I will observe, the full range of all kill types.&quot;, it &lt;em&gt;sounds&lt;/em&gt; like a &lt;strong&gt;fixed&lt;/strong&gt; effect.  Of course, I am assuming that when you say &quot;the full range of all kill types&quot;, you mean the full range that exist or the full range you would ever want to generalize to, and &lt;em&gt;not just&lt;/em&gt; the full range you see in your data.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As for the unbalanced Analysis of Variance, according to another SE question (), you can use the Anova function from library car with parameter type=&quot;III&quot;, and it will use the Type III sum of squares, which seems to be more appropriate for the unbalanced case.  (Function anova (or summary.aov) uses the type I (or sequential) sum of squares, which seems to be less appropriate for the unbalanced case, and &quot;aov&quot; documentation states &quot;aov is designed for balanced designs, and the results can be hard to interpret without balance&quot;.)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-10T02:23:13.910" Id="66987" LastActivityDate="2013-08-12T12:09:35.230" LastEditDate="2013-08-12T12:09:35.230" LastEditorUserId="27142" OwnerUserId="27142" ParentId="66986" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;This is more of a further question on a previous topic, so my apologies if there was some way to link them that I haven't found. It is essentially the same problem as &lt;a href=&quot;http://stats.stackexchange.com/questions/56182/how-to-back-calculate-change-from-baseline-from-a-p-value-for-a-paired-t-test&quot;&gt;How to back-calculate change from baseline from a p-value for a paired t test&lt;/a&gt; . I.e. I need to calculate the standard deviation of the change. However in the study I have the two samples are indepedent, and therefore the authors have already calculated a p-value using the two-sided Wilcoxon rank sum test. Specifically:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Sample 1: n=86, mean=0.58, sd = +/- 0.12 &lt;/p&gt;&#10;  &#10;  &lt;p&gt;Sample 2: n=69, mean = 0.41 sd = +/- 0.108 &lt;/p&gt;&#10;  &#10;  &lt;p&gt;The authors have used a two-sided Wilcoxon rank sum test to get a p-value of 0.51.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is is possible to impute the standard deviation of the change? This is necessary for a meta-analysis I am writing. &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks for any pointers.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-10T09:07:20.867" Id="66999" LastActivityDate="2014-09-23T02:17:04.557" LastEditDate="2013-08-10T11:35:56.850" LastEditorUserId="22468" OwnerUserId="28987" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;t-test&gt;&lt;standard-deviation&gt;&lt;meta-analysis&gt;&lt;mann-whitney-u-test&gt;" Title="Imputing standard deviations for changes from baseline" ViewCount="407" />
  
  
  
  <row Body="&lt;p&gt;Sounds like you're trying to fit a logistic growth curve of the type used in biological modeling, where you're fitting a population growth rate variable $r$.  A very simple model is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;    r_i = r_{i-1}(1 - n / k) &#10;$$&#10;where $n$ is current population and $k$ some theoretical maximum population.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A binomial model in &lt;code&gt;glm&lt;/code&gt; is more appropriate for binary data, so you'll probably need another technique. I'm not that expert in R, but did a quick scan and this question's answer points you toward an appropriate R function using &lt;code&gt;nls()&lt;/code&gt;: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/47802/&quot;&gt;What&amp;#39;s the most pain-free way to fit logistic growth curves in R?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding other curves, you could also try linear regression using curvilinear models, where you include powers of your independent variable(s), e.g. $$ y = b_0 + b_1X + b_2X^2.$$ You always have to be careful with these, however, not to extrapolate results beyond observed data, because the curves ultimately bend/curve according to their inherent mathematical form - which can even happen within the ranges of your observed data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;As for evaluating results, each technique will usually have its own suite (and literature) of model-fit diagnostics, so that will be very case-specific.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-10T18:20:56.920" Id="67022" LastActivityDate="2013-08-11T00:02:47.653" LastEditDate="2013-08-11T00:02:47.653" LastEditorUserId="22047" OwnerUserId="14845" ParentId="67014" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="67026" AnswerCount="1" Body="&lt;p&gt;I have a problem with calculating the variance of an exponential distribution. In my formulary there are these formulas for exponential distributions:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(X)=\frac{1}{\lambda}$&#10;$V(X)=\frac{1}{\lambda^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $E(X)$ is the expected value and $V(X)$ the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a distribution function on the form $F_X(x)=C_1(1-e^{-\lambda_1x})+C_2(1-e^{-\lambda_2x})$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $C$s and $\lambda$s are constants.&lt;/p&gt;&#10;&#10;&lt;p&gt;I calculated $E(X)$ in the following way:&#10;$E(X)=C_1*\frac{1}{\lambda_1}+C_2*\frac{1}{\lambda_2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;But when I try to calculate the variance using the formula above for V(X) and the same method as I used when calculating $E(X)$ I don't get the right answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;I get the right answer if I use $V(X)=E(X^2)-(E(X))^2$,&lt;/p&gt;&#10;&#10;&lt;p&gt;but both should generate the same answer since $E(X^2)=\frac{2}{\lambda^2}$ (from partial integration) which gives $V(X)=\frac{2}{\lambda^2}-(\frac{1}{\lambda})^2=\frac{1}{\lambda^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain this please?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-10T20:00:28.297" Id="67025" LastActivityDate="2013-08-10T20:36:07.920" LastEditDate="2013-08-10T20:25:02.420" LastEditorUserId="26338" OwnerUserId="28909" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;exponential&gt;" Title="Calculate Variance for Exponential Distribution" ViewCount="678" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Note: If you want to skip to the summary of my 3 main questions, they are at the very bottom.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am attempting to produce a binary logistic regressional model for determining whether or not the data I am given is correct.  I have a particular column of data that is giving me quite a bit of trouble.  The column of data is very skewed.  It looks like a very, very sharp exponential decay.  It is used in conjunction with other indepedent variables that are linear.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case I am wanting to know when a zip code I am given from a certain data source with X categorical input is correct with at least 90% confidence (ideally higher).  I already know ~25% of the zip codes are correct - as I've compared them to data I know to be true.  Keep in mind this is before scrubbing as I'll explain shortly. After scrubbing ~40% of zip codes are true.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the future when I have no true data to compare the returned dataset with, I'll know how many zip codes are correct, but I won't know which ones to pick without figuring this (among a number of others problems) out.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am given a lot of other data within the dataset I am returned as well that I've considered trying to somehow proportionate (or do something) with to correct for this problem, but I feel like doing so is the wrong action.  This would then cause dependence between my supposedly independent variables - and would thus be pointless.  This is correct?...right? Actually, barring a counterexample that generally seems obvious to me.  I am somewhat new to statistics, but I am learning as quickly as possible so forgive me for any false assumptions, etc. I make here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have software that has allowed me to apply nearly every non-normalized data transformation technique you can think of to this - but the AD p-value always comes back with a value much less than .05.  About 10+ transformations were attempted.  I list them below the dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am pretty sure I am going to have to go the non-parametric route, which I generally understand - except for one key part:&#10;Everything I can find about non-parametric is about &quot;tests.&quot;  Like the Mann Whitley test, etc.  I can't find anywhere or way I can actually transform the data non-parametrically.  &lt;strong&gt;I can't figure out what to do with the result of my non-parametric &quot;tests&quot;.  What do you do with them?  What is the action taken from the results of the test?&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;The only thing I can see that I could do here is straight-up using the data from this table, figuring out a proportion based upon how likely it is correlated with being correct for each number, assuming that number is categorical rather than ordinal at first (and definitely not continuous), and then sorting that list of proportions in a ordered fashion according to the likelyhood they dictate that the zip code given is the correct one. I'd come up with a list of ordinal variables I could use.  The huge problem is this doesn't account for standard deviations, number of times the original variable was listed, ect.. That is bad, so pending further enhancement of suggestions that strategy is out the window.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Another problem: Blanks&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that you are not supposed to delete blanks as blanks mean something if they are there - but even after inquiring the source of the returned data I get, the only answers I receive are simply &quot;we just didn't have the information.&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I had no idea what to do there and considered trying to use regressional models in an attempt to fill in the blanks. However, since 95% of the rows where a blank appeared in one of the columns equated to the given zip code being wrong ~98% of the time anyway...I did the unthinkable:&lt;/p&gt;&#10;&#10;&lt;p&gt;I just decided to delete them and add this &quot;scrubbing&quot; into my formula, which would essentially make it a simple algorithm.  I assumed as long as did the same scrubbing technique everytime, the eventual formulaic model I came up with would work the same as future data would equivalently be scrubbed of blanks.  This is assuming the way the source of data is derived remains the same every time.  If anyone has a better idea, suggestion, or correction for this, let me know.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the data.  I hope the list isn't too long - and yes it always begins at 1 and the highest value it ever reaches is 999.  It's beautifully skewed at both ends.  This has been scrubbed of rows of data that contain blanks.  The blanks do not only appear in the Accuracy Radius column though - this happens only about .02% (so yes .0002) of the time.  The blanks could appear in a variety of columns as well - all indicating the zip code returned is incorrect ~98% of the time as previously stated.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I realize this may potentially affect the result I get when attempting to transform this specific column of data in some way, but if I perform the said scrubbing every time as I mentioned I theorize it's fine.  If anyone would like the data with blanks, let me know.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the data and below it are my summarized questions.  Obviously column four would just be the sum of the previous two columns - it's the number of times that specific accuracy radius number was listed.  I just listed it for convenience.  I hope I formatted it correctly - it was quite difficult to do.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;Accuracy Radius (km)    #Zip=Correct    #Zip=Incorrect      #Rows AccR is Listed                                                        &#10;        1               1041                    327                 1368&#10;        2               764                     727                 1491&#10;        3               681                     1070                1751&#10;        4               527                     760                 1287&#10;        5               339                     605                 944&#10;        6               248                     405                 653&#10;        7               157                     320                 477&#10;        8               108                     245                 353&#10;        9               87                      174                 261&#10;        10              70                      124                 194&#10;        11              78                      105                 183&#10;        12              47                      103                 150&#10;        13              41                      73                  114&#10;        14              46                      56                  102&#10;        15              37                      59                  96&#10;        16              23                      50                  73&#10;        17              31                      61                  92&#10;        18              15                      47                  62&#10;        19              27                      36                  63&#10;        20              25                      41                  66&#10;        21              19                      30                  49&#10;        22              15                      43                  58&#10;        23              18                      52                  70&#10;        24              9                       47                  56&#10;        25              10                      24                  34&#10;        26              11                      30                  41&#10;        27              7                       32                  39&#10;        28              12                      25                  37&#10;        29              16                      32                  48&#10;        30              10                      31                  41&#10;        31              9                       23                  32&#10;        32              4                       25                  29&#10;        33              11                      24                  35&#10;        34              10                      18                  28&#10;        35              3                       21                  24&#10;        36              7                       20                  27&#10;        37              12                      17                  29&#10;        38              6                       9                   15&#10;        39              5                       13                  18&#10;        40              5                       18                  23&#10;        41              2                       19                  21&#10;        42              5                       24                  29&#10;        43              2                       16                  18&#10;        44              3                       12                  15&#10;        45              7                       12                  19&#10;        46              2                       21                  23&#10;        47              1                       9                   10&#10;        48              2                       15                  17&#10;        49              7                       12                  19&#10;        50              5                       11                  16&#10;        51              7                       13                  20&#10;        52              3                       15                  18&#10;        53              1                       12                  13&#10;        54              4                       6                   10&#10;        55              3                       12                  15&#10;        56              4                       13                  17&#10;        57              4                       5                   9&#10;        58              5                       12                  17&#10;        59              4                       10                  14&#10;        60              4                       6                   10&#10;        62              2                       7                   9&#10;        63              1                       8                   9&#10;        64              3                       9                   12&#10;        65              0                       6                   6&#10;        66              2                       5                   7&#10;        67              2                       13                  15&#10;        68              2                       14                  16&#10;        69              1                       4                   5&#10;        70              2                       7                   9&#10;        71              2                       12                  14&#10;        72              1                       8                   9&#10;        73              0                       9                   9&#10;        74              1                       9                   10&#10;        75              3                       2                   5&#10;        76              1                       8                   9&#10;        77              1                       8                   9&#10;        79              5                       8                   13&#10;        80              2                       5                   7&#10;        81              5                       4                   9&#10;        82              0                       12                  12&#10;        83              0                       5                   5&#10;        84              3                       5                   8&#10;        85              0                       5                   5&#10;        86              2                       3                   5&#10;        87              9                       8                   17&#10;        88              0                       8                   8&#10;        89              1                       8                   9&#10;        90              2                       8                   10&#10;        91              0                       10                  10&#10;        92              0                       4                   4&#10;        93              0                       10                  10&#10;        94              2                       5                   7&#10;        95              2                       6                   8&#10;        96              1                       5                   6&#10;        97              2                       7                   9&#10;        98              0                       9                   9&#10;        99              1                       7                   8&#10;        100             2                       11                  13&#10;        102             0                       3                   3&#10;        103             2                       5                   7&#10;        104             1                       7                   8&#10;        105             0                       10                  10&#10;        106             0                       5                   5&#10;        108             1                       6                   7&#10;        110             2                       4                   6&#10;        111             1                       9                   10&#10;        114             2                       3                   5&#10;        115             0                       4                   4&#10;        116             0                       5                   5&#10;        117             2                       3                   5&#10;        119             1                       1                   2&#10;        120             1                       3                   4&#10;        122             2                       5                   7&#10;        125             1                       4                   5&#10;        126             0                       2                   2&#10;        127             0                       4                   4&#10;        128             0                       1                   1&#10;        129             1                       2                   3&#10;        130             0                       3                   3&#10;        131             0                       3                   3&#10;        132             1                       3                   4&#10;        133             0                       6                   6&#10;        134             0                       5                   5&#10;        137             0                       3                   3&#10;        139             0                       3                   3&#10;        141             2                       8                   10&#10;        143             0                       3                   3&#10;        145             2                       2                   4&#10;        146             0                       4                   4&#10;        149             1                       1                   2&#10;        150             0                       7                   7&#10;        152             0                       6                   6&#10;        153             0                       6                   6&#10;        156             1                       4                   5&#10;        157             1                       3                   4&#10;        160             1                       3                   4&#10;        161             0                       17                  17&#10;        163             0                       1                   1&#10;        165             0                       1                   1&#10;        166             0                       5                   5&#10;        170             0                       6                   6&#10;        171             0                       4                   4&#10;        173             1                       13                  14&#10;        178             0                       4                   4&#10;        180             0                       18                  18&#10;        184             0                       15                  15&#10;        185             0                       6                   6&#10;        186             0                       1                   1&#10;        187             0                       2                   2&#10;        193             0                       2                   2&#10;        197             0                       4                   4&#10;        198             1                       1                   2&#10;        200             0                       7                   7&#10;        201             0                       1                   1&#10;        202             1                       0                   1&#10;        203             1                       0                   1&#10;        206             0                       1                   1&#10;        209             0                       1                   1&#10;        213             0                       4                   4&#10;        220             0                       2                   2&#10;        221             0                       3                   3&#10;        225             0                       2                   2&#10;        227             1                       1                   2&#10;        229             0                       5                   5&#10;        231             3                       4                   7&#10;        232             1                       2                   3&#10;        234             0                       2                   2&#10;        235             0                       5                   5&#10;        237             0                       3                   3&#10;        242             0                       3                   3&#10;        245             0                       1                   1&#10;        255             0                       2                   2&#10;        260             0                       3                   3&#10;        264             0                       5                   5&#10;        266             0                       1                   1&#10;        269             1                       1                   2&#10;        274             1                       0                   1&#10;        282             1                       0                   1&#10;        286             0                       2                   2&#10;        287             0                       3                   3&#10;        295             0                       1                   1&#10;        298             0                       2                   2&#10;        300             1                       1                   2&#10;        336             0                       2                   2&#10;        360             0                       2                   2&#10;        364             0                       1                   1&#10;        368             1                       0                   1&#10;        370             1                       0                   1&#10;        375             1                       0                   1&#10;        393             0                       1                   1&#10;        415             0                       1                   1&#10;        428             0                       1                   1&#10;        432             0                       3                   3&#10;        452             0                       1                   1&#10;        454             0                       1                   1&#10;        485             1                       0                   1&#10;        496             0                       1                   1&#10;        500             1                       0                   1&#10;        506             0                       1                   1&#10;        534             1                       30                  31&#10;        567             1                       0                   1&#10;        572             0                       1                   1&#10;        574             0                       1                   1&#10;        605             0                       2                   2&#10;        607             0                       2                   2&#10;        684             0                       1                   1&#10;        698             0                       6                   6&#10;        708             0                       1                   1&#10;        714             0                       2                   2&#10;        727             0                       1                   1&#10;        732             1                       0                   1&#10;        764             0                       1                   1&#10;        809             0                       1                   1&#10;        828             0                       1                   1&#10;        837             0                       1                   1&#10;        883             0                       3                   3&#10;        970             0                       1                   1&#10;        999             1                       33                  34&#10;&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;br&gt;&#10;&lt;strong&gt;1)  I'm quite positive I can't transform this into normalized data.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have performed, in order from most normalized to least normalized (according to my software):  Box-Cox, Loglogistic (2P), Johnson (Type SU Unbounded), Lognormal (2P), Beta 4P, Weibull (2P), Gamma (2P), Weibull (3P), Logistic (2P), Largest Extreme Value (2P), Loglogistic (3P), Lognormal (2P), Normal, Gamma (3P), Exponential (1P), Exponential (2p), Half-Normal, (1P), and Half-Normal (1P) transformations.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Regardless, the AD p-value in all of these cases was less than .05 and usually much, much less.  From what I've learned, this is bad and means the transformation cannot be applied.&lt;/p&gt;&#10;&#10;&lt;p&gt;I may as well give a shout out to the software add-in I used in Excel for being able to do all of these transformations for me at once and give me the results in graph and numeric form: SigmaXL - whom I'm not affiliated with at all nor do I know if they did all these correctly as I have not heard of 75% of them.  I'm just assuming they know what they're doing as they seem credible and have pretty helpful &amp;amp; lengthy videos.  The videos do leave alot of holes unfilled despite their usefulness though - as you may be able to tell from my question(s).&#10;&lt;br&gt;&#10;I also tried some manual transformations on my own like proportions involving subgroups or taking the ln of each value - these didn't help - or I did them incorrectly. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Any suggestions here?  Am I completely wrong and I actually could transform this into normalized data?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2) Non-parametric &quot;transformation&quot;?, &quot;ranking&quot;?, or what?  And what do I do with those darn tests everyone speaks of.  Testing is great, but what ACTION do I take?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Reading the above question #1, it seems apparent I'm going to have to do some sort of non-parametric &quot;transformation&quot; or  &quot;ranking.&quot;  The problem is, as I stated earlier, all I can find are non-parametric &quot;tests.&quot;  I can't seem to find any way to transform the data using a non-parametric technique, nor do I know what to do with the values from some of the tests I've run like Mann Whitley.  It will say the test is significant and then give me a Mann Whitley value, but I can't figure out what that means for instance, find out what to do with it through research on the net, and thus how to practically apply it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I even supposed to try to fit non-parametric transformed data (if its even possible to transform non-parametrically) with normalized parametric data into a binary logistic regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What do I do here?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3)  Blanks&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As I stated before, I decided to, as part of my process, scrub out the blanks as they indicated 95% of the time the zip code was wrong anyway.  I do realize this may screw up how I transform certain data columns, BUT I actually do want everything to fit into and be calibrated towards a blank free model anyway.  I expect and hope for disagreements/suggestions against this.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Thoughts and Opinions Welcome&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As a side note:  No need to question the validity of the data I am comparing my returned data from x categorical input to.  For the sake of complexity, just assume it is correct.&lt;/p&gt;&#10;" ClosedDate="2014-03-19T09:28:07.467" CommentCount="13" CreationDate="2013-08-11T05:30:53.403" FavoriteCount="1" Id="67044" LastActivityDate="2013-08-11T06:05:46.710" LastEditDate="2013-08-11T06:05:46.710" LastEditorUserId="28928" OwnerUserId="28928" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;nonparametric&gt;&lt;normalization&gt;&lt;mann-whitney-u-test&gt;&lt;nonparametric-bayes&gt;" Title="Un-transformable Seemingly Continuous Data? If not, What to Do With Non-Parametric Test Results - What is the Next Action?" ViewCount="120" />
  
  
  
  <row Body="&lt;p&gt;Am I correct that you shuffled the cards for all subjects?  If not, then simply test the average number of seconds among subjects with shuffled piles versus the average number of seconds among subjects with non-shuffled ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise, however, I think you're asking two questions here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question #1) &lt;em&gt;If the distribution of the ten cards is disproportionately in the first two stacks, will that affect the number of seconds it takes for a subject to identify all ten cards?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To test this hypothesis, perhaps use a Kolmogorov-Smirnov test?  You can compare the average distribution of subjects who answered in under 130 seconds to subjects who answered in 130+ seconds. $\quad$ I doubt you'll find a significant result in your data - but I agree that a lopsided draw might affect the number of seconds: you probably won't find anything significant just because your sample is so small.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question #2) &lt;em&gt;When you randomly shuffled the decks, are the ten cards randomly distributed into the ten piles?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes,  and I think - for what you're doing - this is the more important question.  In other words, you can assume from the start that each subject had the same chance of getting a lopsided distribution... so question #1 doesn't really matter.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-11T13:47:26.547" Id="67061" LastActivityDate="2014-03-19T08:55:11.227" LastEditDate="2014-03-19T08:55:11.227" LastEditorUserId="805" OwnerUserId="16939" ParentId="67056" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;Experiment 1: mice of genotype1 compared to their wild type littermates,&lt;br&gt;&#10;Experiment 2: mice of genotype2 compared to their wild type littermates. &lt;/p&gt;&#10;&#10;&lt;p&gt;(Each experiment in its own right is pretty straightforward: To determine whether the experimental treatment (=the genotype) has a significant effect on the variable measured, do a two-sample t-test.)&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is the following:&#10;Experiment 1 and 2 were conducted in an independent way from each other. They measure the same variable and have different experimental groups (mice of two different genotypes), with their respective wild type littermates serving as control group (the wild type littermates of both experiments have the same genotype, but are from different cages &amp;amp; different generation of mice and cannot, for various reasons, be considered identical).&lt;/p&gt;&#10;&#10;&lt;p&gt;Another confounding problem is that experiment 1 and 2 were conducted and analyzed by 2 different observers, which is yet another reason to consider them independent from each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I would like to do is compare the mice of genotype1 with the mice of genotype2.&lt;/strong&gt; I could basically just go ahead and do that with the data I have, but because of the reasons outlined above and because it's considered good practice in mouse experiments to always only compare littermates (in an effort to reduce inter-subject variability), this is not feasible.&lt;/p&gt;&#10;&#10;&lt;p&gt;So probably the only way to achieve this comparison (somewhat dirtily) is to somehow &lt;em&gt;normalize the data&lt;/em&gt;. Unfortunately, there is no inherent association or pairing between subjects in the experimental and control group, so the only strategy I can think of is to subtract from the measured variable of each experimental subject the control group mean and divide by the control group SD. I would then compare the normalized data using a independent two-sample t-test (after testing for equal variances).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am skeptical whether this approach is legitimate and would greatly appreciate any comment or clarifying question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-11T14:20:41.860" Id="67064" LastActivityDate="2014-05-14T20:48:16.460" LastEditDate="2013-08-11T14:32:56.423" LastEditorUserId="29026" OwnerUserId="29026" PostTypeId="1" Score="4" Tags="&lt;t-test&gt;&lt;multiple-comparisons&gt;&lt;normalization&gt;" Title="How can two different experiments be compared when they have different controls?" ViewCount="389" />
  <row Body="&lt;p&gt;Possible duplicate: &lt;a href=&quot;http://stats.stackexchange.com/questions/55714/standard-errors-of-hyperbfit&quot;&gt;Standard errors of hyperbFit?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I could bet some accounts belong to the same person ...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-11T15:05:12.780" Id="67070" LastActivityDate="2013-08-11T15:05:12.780" OwnerUserId="29028" ParentId="66861" PostTypeId="2" Score="-2" />
  
  
  <row Body="&lt;p&gt;It depends on what kind of survival model you use and what you mean by &quot;using just the coefficients from the model&quot;. You may need to integrate the instantaneous hazard to get the cumulative hazard, but that should be straightforward given that your time periods are equal and I assume you know the value of covariates for the all relevant times (if you do not have a full history for the individual you are trying to predict, then I cannot see how you would calculate cumulative hazards). So assuming you have the full history for the individual in question, the question becomes if you know the instantaneous hazard for all times.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a parametric survival model (e.g. a Weibull proportional hazards model), having the parameters for the baseline (e.g. scale, shape) and the covariates (regression coefficients) is sufficient to calculate everything you want. Since you said that you have time-varying covariates, I suspect that you have a semi-parametric model, such as a Cox proportional hazards model, thus getting the baseline hazard out is slightly more work, but perfectly doable. It is my understanding that using the standard Cox model with time-varying covariates allows you to continue to use all the standard methods with it, so there should be no problem that methods do not work as expected. There is some discussion of getting the hazard out of a Cox model here: &lt;a href=&quot;http://stats.stackexchange.com/questions/46532/cox-baseline-hazard&quot;&gt;Cox Baseline Hazard&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you can give me a reproducible example of what kind of model you want, I can check and attempt to give you a code example.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-11T16:41:22.990" Id="67076" LastActivityDate="2013-08-11T19:08:20.673" LastEditDate="2013-08-11T19:08:20.673" LastEditorUserId="29020" OwnerUserId="29020" ParentId="67071" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Given the limited data you have to work with you may only be able to address this question by incorporating additional assumptions (or data?) regarding the process behind these incidence rates, then doing some manual modeling.  Any statistical technique you use will be implicitly making such assumptions for you under the hood, so better to call those out and structure your analysis around them.    &lt;/p&gt;&#10;&#10;&lt;p&gt;You have observations of discrete incidence counts.  The forms of some common discrete distributions encode the following assumptions: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Poisson: variance is equal to the mean&lt;/li&gt;&#10;&lt;li&gt;Binomial:  variance is smaller than the mean&lt;/li&gt;&#10;&lt;li&gt;Negative Binomial: variance is greater than the mean&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You've already started down this road by ruling out a poisson model for the underlying process, saying variance = mean is not reasonable.   If the process is a contagion model then it might be very reasonable to assume variance is greater than mean, so a negative binomial distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;The next question is fitting the parameters of the selected model and then making your comparisons.  You could approach this in a couple of ways: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Empirically with your four data points - calculate mean and variance, then fit to distribution with old-fashioned algebra using distribution's mean and variance formulas. (You may need to standardize your data, for same reasons you'd use an offset in the glm.) Then calculate the probabilities of all 4 data points (and perhaps different combinations of 3) using the fitted model(s); lower probability suggests the process generating the incidence rates are not equivalent.  &lt;/li&gt;&#10;&lt;li&gt;Use data from existing literature/research to fit the model; then test the probability of your incidence data occurring under that model.  Poor fit for one of the data points could suggest that it's incidence rate deviates from the standard process in some way (or a poor model of course if most or all do not fit well).  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;i&gt;Such results are hardly conclusive&lt;/i&gt; (nothing can be with only this data imho), but just as importantly it can inform dialogue and further research into the process you're modeling.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-11T17:23:21.697" Id="67081" LastActivityDate="2013-08-11T17:23:21.697" OwnerUserId="14845" ParentId="66755" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I used conditional probability to find the probability that &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          A    not A&#10;    B    27        6&#10;not B    12&#10;&#10;          C    not C&#10;    B    29        4&#10;not B    50&#10;&#10;          C    not C&#10;    A    36        3&#10;not A    43&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;P(A|B) &amp;amp;=.81  \\&#10;~\\&#10;P(B|A) &amp;amp;=.69  \\&#10;~\\&#10;P(B|C) &amp;amp;=.82  \\&#10;~\\&#10;P(C|B) &amp;amp;=.36  \\&#10;~\\&#10;P(A|C) &amp;amp;=.92  \\&#10;~\\&#10;P(C|A) &amp;amp;=.45&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;These numbers seem too high to be not statistically significant. Is there any inference I can draw from this information that would be judged to be significant?&lt;/p&gt;&#10;&#10;&lt;p&gt;These numbers are drawn from the interaction of a ligand with a protein. Each number in my tables represents a unique contact between the instance and the protein. There are non unique duplicates of the connections sometimes as many as 23 per atom. I do not consider them because they are confounding. B is the standard for which 33 atoms are interact with the protein. B is a substandard and has 39 contact points. A is another standard to a lesser degree it is sometimes rejected. C made from A to excrete A and C is always rejected. With these probabilities I plan to identify novel ligands which significantly overlap with B and A and the like rejecting those who are similar to C. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-11T20:41:00.277" FavoriteCount="1" Id="67090" LastActivityDate="2013-08-12T05:21:39.437" LastEditDate="2013-08-12T05:21:39.437" LastEditorUserId="11279" OwnerUserId="11279" PostTypeId="1" Score="-1" Tags="&lt;self-study&gt;&lt;statistical-significance&gt;&lt;conditional-probability&gt;" Title="Statistical significance of conditional probabilties" ViewCount="148" />
  <row AnswerCount="1" Body="&lt;p&gt;If you are running logistic regression on data, and the prior probability of the event is 50%, and there is no preference for sensitivity or specificity... &lt;/p&gt;&#10;&#10;&lt;p&gt;When would you still want to use a cutoff? Can you please give an example to help me, a beginner, understand?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-11T20:50:27.880" FavoriteCount="1" Id="67091" LastActivityDate="2013-08-12T12:17:37.350" LastEditDate="2013-08-12T12:17:37.350" LastEditorUserId="88" OwnerUserId="17801" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;threshold&gt;" Title="Logistic regression - cutoff" ViewCount="201" />
  <row Body="&lt;p&gt;The chi-distribution is the square root of a chi-square distribution. That is, it arises as the square root of a sum of squares of independent standard normal random variables. It has a degrees of freedom parameter, as with the chi-square. &lt;/p&gt;&#10;&#10;&lt;p&gt;The absolute value of a standard normal is an example of a chi-distribution (it has a chi(1) distribution), as is the Rayleigh(1) distribution (chi(2)) and the Maxwell distribution (normalized molecular speeds, which are chi(3)).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-12T01:45:01.723" Id="67109" LastActivityDate="2013-08-12T05:16:22.560" LastEditDate="2013-08-12T05:16:22.560" LastEditorUserId="805" OwnerUserId="805" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;A &lt;a href=&quot;http://en.wikipedia.org/wiki/Metric_%28mathematics%29&quot; rel=&quot;nofollow&quot;&gt;metric&lt;/a&gt; is a function that outputs a distance between 2 elements of a set.  To meet the definition of a metric, a distance function must fulfill the following criteria:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;There is no distance between an element and itself: $d(x_i,x_i)=0$.  &lt;/li&gt;&#10;&lt;li&gt;If the distance between two elements is $0$, those elements are equivalent: $d(x_i,x_j)=0\implies x_i=x_j$.  &lt;/li&gt;&#10;&lt;li&gt;All distances are non-negative: $d(x_i,x_j)\ge0$.  &lt;/li&gt;&#10;&lt;li&gt;The distance between two elements is the same in either direction: $d(x_i,x_j)=d(x_j,x_i)$.  &lt;/li&gt;&#10;&lt;li&gt;The distance between two elements is less than or equal to the sum of the distances between those elements and a third: $d(x_i,x_j)\le d(x_i,x_k)+d(x_j,x_k)$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-08-12T03:30:28.613" Id="67117" LastActivityDate="2013-08-12T03:30:28.613" LastEditDate="2013-08-12T03:30:28.613" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;I think one way you can test neural networks like the Restricted Boltzman Machine (RBM) is to apply them to new sets of data, or more commonly a subset of the total data set, and see how they perform. You can also test the model with cross validation. &#10;I have used a popular open source machine learning (ML) software toolkit that is simple to install and use, and it allows you to easily test many types of ML algorithms separately with a number of the model's regression statistics (&lt;a href=&quot;http://www.cs.waikato.ac.nz/ml/weka/&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.waikato.ac.nz/ml/weka/&lt;/a&gt;). That might give you a quick analysis without much intervention except the basics of running the program, although depending it may not be worth it if it will be difficult to format your data (although you can usually google &quot;csv to arff convert online&quot; for example for one-step formatting).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-12T04:11:14.627" Id="67119" LastActivityDate="2013-08-12T04:11:14.627" OwnerUserId="29044" ParentId="41029" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I am estimating a linear probability model with a hierarchical structure.  I am bootstrapping standard errors to test the significance of the coefficients in order to double check the parametric estimates of the standard errors. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the bootstrapping process I am finding that the coefficients are all positive (which I expected) but there are a few extreme positive outliers. The outliers make the standard errors huge and subsequent t-statistics small. This is misleading because an empirical p-value has a value under 0.01. &lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What is the best way to convey that this coefficient is statistically different from zero? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What would be the best way to report these results? Normally in a LaTex table I report coefficients plus standard errors. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-08-12T04:27:03.483" Id="67121" LastActivityDate="2013-08-12T05:01:42.973" LastEditDate="2013-08-12T05:01:42.973" LastEditorUserId="16974" OwnerUserId="24450" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;bootstrap&gt;&lt;skewness&gt;" Title="Best way to convey highly skewed data" ViewCount="107" />
  <row Body="&lt;p&gt;A really interesting example I personally like is taken from the book Freakonomics by Steven D. Levitt and Stephen J. Dubner. There is a chapter in the book that discusses correlation vs. causality. Correlation between two statistical variables does not necessarily imply that these variables are statistically dependent, but a mistake along these lines was made by experts. Quoting from the book:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;A tricky beast, Polio was extremely difficult for researchers to pin down.  They couldn’t figure out how it was passed or when/how it expressed itself.  We have a tendency to remember this time as one in which Polio was ‘epidemic’ when, in fact, it was not affecting large swaths of the population (compared with the more common measles, for example).  The reason it was seen as epidemic was because it was so frightening.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What researchers DID manage to determine in their studies was that Polio infection rates went UP in the Summer.  They also saw that ICE CREAM CONSUMPTION went up in the Summer.  And so they concluded that ice cream consumption led to Polio and for a time ice cream was demonized. &quot;&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-08-12T06:50:40.703" Id="67126" LastActivityDate="2013-08-12T06:50:40.703" OwnerUserId="27495" ParentId="67124" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I think the best example of this may likely be the controversy around hormone replacement therapy and cardiovascular risk - large cohort epidemiological studies seem to suggest a protective effect and health policy and physician recommendations were made on this information.&lt;/p&gt;&#10;&#10;&lt;p&gt;Follow-up RCTs then seem to show that there's actually an &lt;em&gt;increased&lt;/em&gt; risk of myocardial infarction in women placed on HRT.&lt;/p&gt;&#10;&#10;&lt;p&gt;This goes back and forth for a bit, and has been used as one of the canonical cases to attack epidemiology as a field, but a recent &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3731075/&quot; rel=&quot;nofollow&quot;&gt;re-analysis by Hernan&lt;/a&gt; seems to propose that the two studies actually don't have discordant results if you make sure you ask the same question.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-12T06:58:14.523" Id="67127" LastActivityDate="2013-08-12T12:21:41.680" LastEditDate="2013-08-12T12:21:41.680" LastEditorUserId="21054" OwnerUserId="5836" ParentId="67124" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="67129" AnswerCount="1" Body="&lt;p&gt;I'm trying to fashion survey experiments for which the demographic composition of the samples needs to be as close as possible to a 'golden' composition. For concreteness, let's say the 'golden' composition is &lt;code&gt;14% middle class, 35% upper-middle class, and 51% upper class&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a whole bunch of samples, each with slightly different compositions. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;S1 = 10% middle class, 41% upper-middle class, 49% upper class&#10;S2 = 33% middle class, 33% upper-middle class, 34% upper class&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm trying to find a good statistical measure to describe how close the distribution in each sample is to the ideal, 'golden' composition. Is there a simple metric here that I could use?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-12T07:21:53.490" Id="67128" LastActivityDate="2013-08-12T09:06:45.943" LastEditDate="2013-08-12T08:20:50.800" LastEditorUserId="3277" OwnerUserId="29051" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;survey&gt;&lt;methodology&gt;" Title="Measuring how close a given sample distribution is to an ideal distribution" ViewCount="60" />
  <row AcceptedAnswerId="67135" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Update: First part answered. Directly jump to the second part for those wanting to answer!&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First Part&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to have a better understanding of what is a likelihood function and what is AIC.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my current understanding: If we toss a coin 11 times and get 5 heads and 6 tails, the likelihood function is&#10;$$L(p) = P(data|p) = p^5(1-p)^6$$&#10;p being the hyopthesis of the function which is the probability to get head.&lt;/p&gt;&#10;&#10;&lt;p&gt;the Maximum Likelihood is the value of p that satisfies this equation: $$\frac{dL(p)}{dp} = 5p^4(1-p)^6-6p^5(1-p)^5=0$$ Therefore, $\hat p=\frac{5}{11}$&lt;/p&gt;&#10;&#10;&lt;p&gt;The AIC is given by: $$AIC=2k-2ln(L)$$&#10;k being the number of parameters (one in our example).&#10;Therefore:&#10;$$AIC=2*1-2*ln(0^5*(1-0)^6) = 2-2*ln(L)=2$$&#10;I guess I missunderstand something! Is there a mistake? Where is it?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second Part&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am also wondering how does this AIC calculations apply when performing regressions. For example we have this simple set of data (writing in R format):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;respond_variable = c(1,2.2,3.1,4)&#10;predictor_variable = c(1,2,6,10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We perform a linear and a quadratic model on them. How is the AIC calculated?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-12T08:43:18.527" Id="67131" LastActivityDate="2013-08-13T09:45:14.130" LastEditDate="2013-08-12T10:36:32.140" LastEditorUserId="24097" OwnerUserId="24097" PostTypeId="1" Score="3" Tags="&lt;mathematical-statistics&gt;&lt;maximum-likelihood&gt;&lt;aic&gt;&lt;likelihood-function&gt;" Title="AIC and likelihood function applied to a dataset" ViewCount="205" />
  <row AnswerCount="0" Body="&lt;p&gt;How bad is this for my model? &lt;/p&gt;&#10;&#10;&lt;p&gt;I've used ML (maximum likelihood) with promax rotation and Kaiser on a dataset ($n&amp;gt;300$) to extract underlying factors to build a model. Eigenvalue selection was reset from $.1$ to $.5$, so as to focus on cases (questions) that explain the most covariance according to underlying factors. I hope my explanation makes sense, as I`m new to this. Anyways, 9 clean factors emerged, with only this one troubling number, in the pattern matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tested reliability of the factors with ANOVA and reliability was excellent (most factors above $.900$). &lt;/p&gt;&#10;&#10;&lt;p&gt;I`ve already used the information to refine and collect data with a questionnaire, but this one number is still troubling me. Does it invalidate the model? Is it a serious flaw in my model? What are the implications of a case number in the pattern matrix being $&amp;gt;1.0$?&lt;/p&gt;&#10;&#10;&lt;p&gt;None of the communalities were $&amp;gt;1.0$, so I don`t think this is a Heywood case.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is potentially a serious problem, as it might necessitate me redoing the model and adding a year to my studies - the questionnaire is a longitudinal one, taking one academic year. Also, I am interested in publishing the resulting questionnaire to aid in others` related research, but if this one number is a serious error, then publication is out of the question. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would greatly appreciate some advice regarding this problem.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-12T08:49:24.050" FavoriteCount="0" Id="67132" LastActivityDate="2013-08-12T09:27:25.323" LastEditDate="2013-08-12T09:27:25.323" LastEditorUserId="29052" OwnerUserId="29052" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;modeling&gt;&lt;maximum-likelihood&gt;&lt;factor-analysis&gt;" Title="In maximum likelihood, pattern matrix, one case $&gt;1.0$" ViewCount="142" />
  <row Body="&lt;p&gt;The actual date /time/channel is an observation/transaction. A time series is a bucketing of transactions. For each type of advert,I would take the time of the advert and bucket them into hours to create a number of possibly &quot;causal&quot; time series. The impact of an advert may depend on the hour of the day or the day of the week or whether or not it is on a holiday or even “nearly” on a holiday.Since you only have 10 days it is not feasible to try and compute daily/holiday effects. The cross-correlation between each of these discrete advert time series can be computed (descriptive statistic) but shouldn’t be as your predictor varaibles are discrete counts and your subsription data may be autocorrelated.  I would create 23 predictor series reflecting hour of the day and include these as well as the advert time series computed above into an ARMAX model.Care should be taken to identify and deal with any subscription readings that reflected either Pulses, Level Shifts, Time Trends or Seasonal Pulses as these would be assignable to omitted variables that you had not controlled for. Hope this helps.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-12T10:17:19.130" Id="67137" LastActivityDate="2013-08-12T10:17:19.130" OwnerUserId="3382" ParentId="67134" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It's not clear why you think example 2 has 'more significant variation'. In fact, Example 1 has a larger coefficient of variation (CV); i.e. &#10;\begin{gather*}&#10;CV(Example1)=0.44721/0.2=2.24,\\&#10;CV(Example2)=402.49/280=1.44.&#10;\end{gather*}&#10;So while example 2 has larger $scale$, it does not really show more $variation$. And as you showed with the deviation scores, both examples have equally anomalous final values when scaled. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-12T12:26:07.157" Id="67144" LastActivityDate="2013-08-12T12:26:07.157" OwnerUserId="26357" ParentId="67142" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Beyond considering the SEM discussion in comment from @NickCox, I'd test different codings of your age variable before ruling out its inclusion.  I'm assuming you've transformed to categorical since the age relationship will be non-linear (very young and old people are more likely to be infected, everyone in between less so).  If you have enough data perhaps you could test as a continuous covariate with a quadratic age variable model; otherwise explore how you bin the ages into different sub groups to make sure that's not confounding the variable's influence. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-12T16:30:49.707" Id="67181" LastActivityDate="2013-08-12T16:30:49.707" OwnerUserId="14845" ParentId="67161" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="67235" AnswerCount="1" Body="&lt;p&gt;I understand that &lt;a href=&quot;http://en.wikipedia.org/wiki/Multinomial_test&quot; rel=&quot;nofollow&quot;&gt;multinomial tests&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_test&quot; rel=&quot;nofollow&quot;&gt;binomial tests&lt;/a&gt; are both likelihood ratio tests for testing if a sample has a categorical or Bernoulli distribution. &#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/G-test&quot; rel=&quot;nofollow&quot;&gt;G tests&lt;/a&gt; are also likelihood ratio tests for testing if a sample has a specific distribution.&#10;so my question is: do multinomial and binomial tests belong to G tests? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-12T20:55:35.903" FavoriteCount="1" Id="67200" LastActivityDate="2013-08-13T08:14:11.287" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;" Title="Do multinomial and binomial tests belong to G tests?" ViewCount="70" />
  
  <row Body="&lt;p&gt;Suppose you and I are coaching track teams. Our athletes come from the same school, are similar ages, and the same gender (i.e., they're drawn from the same population), but I claim to have discovered a Revolutionary New Training System that will make my team members run much faster than yours. How can I convince you that it really does work?&lt;/p&gt;&#10;&#10;&lt;p&gt;We have a race. &lt;/p&gt;&#10;&#10;&lt;p&gt;Afterward, I sit down and compute the average time for the members of my team and the average time for the members of yours. I'll claim victory if the mean time for my athletes is not only faster than the mean for yours, but the difference is also large compared to the &quot;scatter&quot;, or standard deviation, of our results. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;hr&gt;&#10;This is essentially a [$t$-test][1]. We're assuming that the data arises from distributions with specific parameters, in this case a mean and standard deviation. The test estimates those parameters and compares one of them (the mean). It is, consequently, called a &lt;em&gt;parametric&lt;/em&gt; test, since we are comparing these parameters.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&quot;But Matt&quot;, you complain, &quot;this isn't quite fair. Our teams are pretty similar, but you--due to pure chance--ended up with the fastest runner in the district. He's not in the same league as everyone else; he's practically a freak of Nature. He finished 3 minutes before the next-fastest finisher, which reduces your average time a lot, but the rest of the competitors are pretty evenly mixed. Let's look at the finish order instead. If your method really works, the earlier finishers should mostly be from your team, but if it doesn't the finish order should be pretty random. This doesn't give undue weight to your super-star!&quot;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;This method is essentially the [Mann-Whitney U Test][2] (also called the Wilcoxon Rank Sum Test, Manning-Whitney-Wilcoxon Test, and several other permutations besides!). Note that unlike the $t$-test, we're not assuming that the data comes from specific distributions, nor are we computing any parameters for them. Instead, we're comparing the relative ranks of the data points directly. &lt;/p&gt;&#10;&#10;&lt;p&gt;That's the major distinction--parametric tests model things with distributions and compare the parameters of these distributions; non-parametric tests....don't and operate more directly on the data. As with parametric tests, non-parametric test statistics are also constructed so that the $p$ values are uniformly distributed on [0,1] under the null hypothesis and clustered towards 0 in the presence of an effect. You would report and interpret them just like the results of a parametric test. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure about the relative popularity of parametric and non-parametric methods. Some non-parametric methods (e.g., histograms!) are in nearly universal use; others might be over or under-used. I suspect that the Mann-Whitney U Test ought to be used more, rather than less frequently. It's about as efficient as a $t$-test on normally distributed data and actually does better than the $t$-test on sufficiently non-normal data. It's also fairly robust to outliers. Plus, you can use it on ordinal data too (e.g., finish order rather than just finish time), which makes it more broadly applicable than a $t$-test.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-13T00:54:44.877" Id="67215" LastActivityDate="2013-08-13T00:54:44.877" OwnerUserId="7250" ParentId="67204" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;You can apply the same transformation to the centroids that you applied to your data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;Dimension reduction to 2D via PCA is just a matrix multiplication.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may want to refresh your knowledge of PCA.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-13T07:25:46.703" Id="67232" LastActivityDate="2013-08-13T07:25:46.703" OwnerUserId="7828" ParentId="67208" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I'm assuming here that your output variable is categorical, though that may not be the case. Typically though, when I've seen HMM's used, the number of states is known in advance rather than selected through tuning. Usually they correspond to some well-understood variable that happens to not be observed. But that doesn't mean you can't experiment with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The danger in using BIC (and AIC) though is that the &lt;em&gt;k&lt;/em&gt; value for the number of free parameters in the model increases quadratically with the number of states because you have the transition probability matrix with Px(P-1) parameters (for P states) and the output probabilities for each category of the output given each state. So if the AIC and BIC are being calculated properly, the &lt;em&gt;k&lt;/em&gt; should be going up fast.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have enough data, I would recommend a softer method of tuning the number of states like testing on a holdout sample. You might also want to just look at the likelihood statistic and visually see at what point it plateaus. Also if your data is large, keep in mind that this will push the BIC to a smaller model. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-13T08:02:04.533" Id="67234" LastActivityDate="2013-08-13T08:02:04.533" OwnerUserId="29087" ParentId="65285" PostTypeId="2" Score="7" />
  
  
  
  
  
  <row Body="&lt;p&gt;I suggest a concordance correlation between $y$ and $x$. This measures agreement between $y$ and $x$, not correlation. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Concordance_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Concordance_correlation_coefficient&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-13T15:38:47.433" Id="67279" LastActivityDate="2013-08-13T15:38:47.433" OwnerUserId="22047" ParentId="67278" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;For a simulation it's crucial to be &lt;em&gt;correct&lt;/em&gt; as well as fast.&lt;/strong&gt;  Both these objectives suggest writing code that targets core capabilities of the programming environment as well as code that is as short and simple as possible, because simplicity lends clarity and clarity promotes correctness.  Here is my attempt to achieve both in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# Simulate one play with a deck of `n` distinct cards in `k` suits.&#10;#&#10;sim &amp;lt;- function(n=13, k=4) {&#10;  deck &amp;lt;- sample(rep(1:n, k)) # Shuffle the deck&#10;  deck &amp;lt;- c(deck, 1:n)        # Add sentinels to terminate the loop&#10;  k &amp;lt;- 0                      # Count the cards searched for&#10;  for (j in 1:n) {&#10;    k &amp;lt;- k+1                          # Count this card&#10;    deck &amp;lt;- deck[-(1:match(j, deck))] # Deal cards until `j` is found&#10;    if (length(deck) &amp;lt; n) break       # Stop when sentinels are reached&#10;  }&#10;  return(k)                   # Return the number of cards searched&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Applying this &lt;em&gt;in a reproducible way&lt;/em&gt; can be done with the &lt;code&gt;replicate&lt;/code&gt; function after setting the random number seed, as in&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; set.seed(17);  system.time(d &amp;lt;- replicate(10^5, sim(13, 4)))&#10;   user  system elapsed &#10;   5.46    0.00    5.46&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That's slow, but fast enough to conduct fairly lengthy (and therefore precise) simulations repeatedly without waiting.  There are several ways we can exhibit the result.  Let's start with its mean:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; n &amp;lt;- length(d)&#10;&amp;gt; mean(d)&#10;[1] 5.83488&#10;&#10;&amp;gt; sd(d) / sqrt(n)&#10;[1] 0.005978956&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The latter is the standard error: we expect the simulated mean to be within two or three SEs of the true value.  &lt;strong&gt;That places the true expectation somewhere between $5.817$ and $5.853$&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;We might also want to see a tabulation of the frequencies (and &lt;em&gt;their&lt;/em&gt; standard errors). The following code prettifies the tabulation a little:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;u &amp;lt;- table(d)&#10;u.se &amp;lt;- sqrt(u/n * (1-u/n)) / sqrt(n)&#10;cards &amp;lt;- c(&quot;A&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;T&quot;, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;)&#10;dimnames(u) &amp;lt;- list(sapply(dimnames(u), function(x) cards[as.integer(x)]))&#10;print(rbind(frequency=u/n, SE=u.se), digits=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is the output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                2       3      4      5      6      7       8       9       T       J       Q       K&#10;frequency 0.01453 0.07795 0.1637 0.2104 0.1995 0.1509 0.09534 0.04995 0.02249 0.01009 0.00345 0.00173&#10;SE        0.00038 0.00085 0.0012 0.0013 0.0013 0.0011 0.00093 0.00069 0.00047 0.00032 0.00019 0.00013&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How can we know the simulation is even correct?&lt;/strong&gt; One way is to test it exhaustively for smaller problems.  For that reason this code was written to attack a small generalization of the problem, replacing $13$ distinct cards with &lt;code&gt;n&lt;/code&gt; and $4$ suits with &lt;code&gt;k&lt;/code&gt;.  However, for the testing it is important to be able to feed the code a deck in a predetermined order.  Let's write a slightly different interface to the same algorithm:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;draw &amp;lt;- function(deck) {&#10;  n &amp;lt;- length(sentinels &amp;lt;- sort(unique(deck)))&#10;  deck &amp;lt;- c(deck, sentinels)&#10;  k &amp;lt;- 0&#10;  for (j in sentinels) {&#10;    k &amp;lt;- k+1&#10;    deck &amp;lt;- deck[-(1:match(j, deck))]&#10;    if (length(deck) &amp;lt; n) break&#10;  }&#10;  return(k)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(It is possible to use &lt;code&gt;draw&lt;/code&gt; in place of &lt;code&gt;sim&lt;/code&gt; everywhere, but the extra work done at the beginning of &lt;code&gt;draw&lt;/code&gt; makes it twice as slow as &lt;code&gt;sim&lt;/code&gt;.)&lt;/p&gt;&#10;&#10;&lt;p&gt;We can use this by applying it to &lt;em&gt;every&lt;/em&gt; distinct shuffle of a given deck.  Since the purpose here is just a few one-off tests, efficiency in generating those shuffles is unimportant.  Here is a quick brute-force way:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;n &amp;lt;- 4 # Distinct cards&#10;k &amp;lt;- 2 # Number of suits&#10;d &amp;lt;- expand.grid(lapply(1:(n*k), function(i) 1:n))&#10;e &amp;lt;- apply(d, 1, function(x) var(tabulate(x))==0)&#10;g &amp;lt;- apply(d, 1, function(x) length(unique(x))==n)&#10;d &amp;lt;- d[e &amp;amp; g,]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now &lt;code&gt;d&lt;/code&gt; is a data frame whose rows contain all the shuffles.  Apply &lt;code&gt;draw&lt;/code&gt; to each row and count the results:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;d$result &amp;lt;- apply(as.matrix(d), 1, draw)&#10;    (counts &amp;lt;- table(d$result))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output (which we will use in a formal test momentarily) is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   2    3    4 &#10; 420  784 1316 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(The value of $420$ is easy to understand, by the way: we would still be working on card $2$ if and only if all the twos preceded all the aces.  The chance of this happening (with two suits) is $1/\binom{2+2}{2} = 1/6$.  Out of the $2520$ distinct shuffles, $2520/6 = 420$ have this property.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;We can test the output with a chi-squared test.&lt;/strong&gt;  To this end I apply &lt;code&gt;sim&lt;/code&gt; $10,000$ times to this case of $n = 4$ distinct cards in $k = 2$ suits:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;set.seed(17)&#10;&amp;gt;d.sim &amp;lt;- replicate(10^4, sim(n, k))&#10;&amp;gt;print((rbind(table(d.sim) / length(d.sim), counts / dim(d)[1])), digits=3)&#10;&#10;         2     3     4&#10;[1,] 0.168 0.312 0.520&#10;[2,] 0.167 0.311 0.522&#10;&#10;&amp;gt; chisq.test(table(d.sim), p=counts / dim(d)[1])&#10;&#10;    Chi-squared test for given probabilities&#10;&#10;data:  table(d.sim) &#10;X-squared = 0.2129, df = 2, p-value = 0.899&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Because $p$ is so high, we find no significant difference between what &lt;code&gt;sim&lt;/code&gt; says and the values computed by exhaustive enumeration.  Repeating this exercise for some other (small) values of $n$ and $k$ produces comparable results, giving us ample reason to trust &lt;code&gt;sim&lt;/code&gt; when applied to $n=13$ and $k=4$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, &lt;strong&gt;a two-sample chi-squared test&lt;/strong&gt; will compare the output of &lt;code&gt;sim&lt;/code&gt; to the output reported in another answer:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;y &amp;lt;- c(1660,8414,16973,21495,20021,14549,8957,4546,2087,828,313,109)&#10;&amp;gt;chisq.test(cbind(u, y))&#10;&#10;data:  cbind(u, y) &#10;X-squared = 142.2489, df = 11, p-value &amp;lt; 2.2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The enormous chi-squared statistic produces a p-value that is essentially zero: &lt;strong&gt;without a doubt, &lt;code&gt;sim&lt;/code&gt; disagrees with the other answer.&lt;/strong&gt;  There are two possible resolutions of the disagreement: one (or both!) of these answers is incorrect or they implement different interpretations of the question.  For instance, I have interpreted &quot;after the deck runs out&quot; to mean &lt;em&gt;after&lt;/em&gt; observing the last card and, if allowable, updating the &quot;number you will be on&quot; before terminating the procedure.  Conceivably that last step was not meant to be taken.  Perhaps some such subtle difference of interpretation will explain the disagreement, at which point we can modify the question to make it clearer what is being asked.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-13T17:11:11.193" Id="67292" LastActivityDate="2013-08-13T17:41:32.673" LastEditDate="2013-08-13T17:41:32.673" LastEditorUserId="919" OwnerUserId="919" ParentId="67179" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;p&gt;This sounds like a candidate for a log-transform of the diff variable. That way you'll be effectively measuring the percentage change in the variable. The losses will be transformed to negative values and the gains will be positive. If you felt that the percentage change was still impacted by the initial value, you could also use that as an offset term or as a predictor in  your model. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-13T18:29:25.897" Id="67307" LastActivityDate="2013-08-13T18:29:25.897" OwnerUserId="29087" ParentId="67300" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a Box-Cox regression where the explanatory variables are almost all dummy variables. If I want to see if there is multicollinearity among them, what would be an appropriate test? Do variance inflation factor (VIF) tests work here? What about the Pearson correlation coefficient matrix? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-13T18:33:03.427" Id="67309" LastActivityDate="2013-08-16T22:30:21.963" LastEditDate="2013-08-13T18:47:46.733" LastEditorUserId="7290" OwnerUserId="29117" PostTypeId="1" Score="3" Tags="&lt;categorical-data&gt;&lt;multicollinearity&gt;" Title="How to test for multicollinearity among dummy explanatory variables?" ViewCount="1435" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to analyse a dataset with at minimum 50 explanatory variables coded as 0 and 1 for presence/absence and a binary response variable (case/control). The goal is to see how the variables can predict the separation between case and control.&lt;/p&gt;&#10;&#10;&lt;p&gt;As there are more variables than observations I applied a partial least square discriminant analysis (PLS-DA) using the package mixOmics in R. However, when I want to test the significance of the analysis with PLSDA.test (package RVAideMemoire) I get a lot of warnings :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1: In pls(X, ind.mat, ncomp = ncomp, mode = &quot;regression&quot;,  ... :&#10; Zero- or near-zero variance predictors. &#10; Reset predictors matrix to not near-zero variance predictors.&#10; See $nzv for problematic predictors.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I guess the problem with near-zero variance results from the 0/1 coding of the predictor variables. I tried to convert the variables to factors, but this doesn't help. Is there a different analysis more suitable? How can I deal with presence/ absence variables as predictors?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-13T18:51:39.597" Id="67312" LastActivityDate="2014-10-08T19:50:00.507" LastEditDate="2014-10-08T19:50:00.507" LastEditorUserId="7290" OwnerUserId="29119" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;binary&gt;&lt;predictor&gt;&lt;pls&gt;&lt;discriminant&gt;" Title="PLS-DA with binary predictors in R (package mixOmics)" ViewCount="479" />
  
  <row Body="&lt;p&gt;I am guessing you are planning to perform an anova.&lt;/p&gt;&#10;&#10;&lt;p&gt;Determining whether you have a large enough sample size depends not only on the number within each group, but also on their expected means, standard deviations, and the power you choose.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far you have given us:&lt;/p&gt;&#10;&#10;&lt;p&gt;$n_1=46, n_2=40, n_3=29, n_4=37$,&lt;/p&gt;&#10;&#10;&lt;p&gt;but previous studies/analyses should inform your expectations about the group means and standard deviations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, .80 is commonly used as a threshold for power. While your alpha (e.g. $.05$) can be thought of as Power is the threshold for true positives.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many tools (readily available through a Google search) that can help you with power and sample size calculation. Good luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-13T18:58:46.343" Id="67315" LastActivityDate="2013-08-13T18:58:46.343" OwnerUserId="29109" ParentId="67311" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have four models, each one is run with MCMC on a large number phylogenetic trees. Each resulting model has an associated log posterior probability (sum of the log likelihood and log prior). Since I need to average the models weighting by their posteriors (bayesian model averaging) I need the original posterior probabilities. How can I standardize them from 0 to 1? Should I normalize among the four model for each tree?&#10;Also consider that my log posteriors are negative and very low (&amp;lt;-700), then when I calculate their exponential I get 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-13T19:30:12.610" Id="67319" LastActivityDate="2013-08-13T19:30:12.610" OwnerUserId="28049" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;mcmc&gt;&lt;normalization&gt;&lt;model&gt;&lt;posterior&gt;" Title="Transform log posterior probabilities for Bayesian model averaging" ViewCount="175" />
  
  
  <row Body="&lt;p&gt;I find that it helps to think in events rather than proportions to get the general scale needed, then go to more precise power calculation. For rare events, the sampling error is related to the square root of the number of events. So if your group b has a proportion of 0.007, that's 700 expected events in a sample of 100,000 cases, with a sampling error of around 25 events. So it seems that you shouldn't be so far away from adequate power as your output from the pwr package suggests; a proportion of 0.008 in a sample of 100,000 cases has 800 expected events.&lt;/p&gt;&#10;&#10;&lt;p&gt;Double-check that the input to the program you used in the pwr package is correct. I don't use it, but it seems that there is a specific definition of &quot;effect size&quot; in the ES.h() program in that package. Using that formula for proportions of 0.007 and 0.008 gives me an &quot;effect size&quot; of 0.011, not the simple proportion difference of 0.001 you seemed to have specified in calling the program.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can't get away from the need for large numbers of cases with low proportions, but things might not be quite so bad for your present application as you fear.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-13T20:01:37.963" Id="67325" LastActivityDate="2013-08-13T20:01:37.963" OwnerUserId="28500" ParentId="67273" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Radford Neal has done a good bit of work in this area that might interest you, including some direct work in equating Bayesian graphical models with neural networks. (His PhD thesis was apparently on this specific topic.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not familiar enough with this work to provide an intelligent summary, but I wanted to give you the pointer in case you find it helpful : &lt;a href=&quot;http://www.cs.toronto.edu/~radford/res-neural.html&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.toronto.edu/~radford/res-neural.html&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-13T20:05:09.890" Id="67327" LastActivityDate="2013-08-13T20:05:09.890" OwnerUserId="28988" ParentId="58381" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;Proc Similarity in SAS can cluster time series.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-14T01:55:51.763" Id="67361" LastActivityDate="2013-08-14T01:55:51.763" OwnerUserId="29137" ParentId="66976" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="67398" AnswerCount="2" Body="&lt;p&gt;I seem not to find this in any textbooks. So I post these questions.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is monthly data better than weekly data for forecasting?&lt;/li&gt;&#10;&lt;li&gt;Can there be seasonality in weekly data? Most software/methods don't  seem to find seasonality in forecasting data.&lt;/li&gt;&#10;&lt;li&gt;Is there a way to aggregate weekly data to monthly data?&lt;/li&gt;&#10;&lt;li&gt;How do methods like ARIMA/Exponential Smoothing handle seasonality with weekly data?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-08-14T02:02:50.217" FavoriteCount="1" Id="67362" LastActivityDate="2014-03-04T23:59:20.230" LastEditDate="2013-08-14T11:20:50.887" LastEditorUserId="22047" OwnerUserId="29137" PostTypeId="1" Score="1" Tags="&lt;forecasting&gt;&lt;arima&gt;&lt;methodology&gt;" Title="Shall I use weekly or monthly data for forecast?" ViewCount="1032" />
  
  <row Body="&lt;p&gt;I'll assume that each iPad had a priori the same chance of having mulfunctions (i.e. you tested them on comparable amount of apps and time). If my assumption does not hold, then you simply cannot make inference: maybe all iPads are buggy, but only the first three were actually used and properly tested (and the rest maybe never left their original manufacturer packaging)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, the data has Poisson distribution, and it seems that you want to predict if the number of failures has anything to do with some predictor. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_regression&quot; rel=&quot;nofollow&quot;&gt;Poisson regression&lt;/a&gt; exactly answers this question. Please mind, that you will need to supply an independent variable that tries to predict the failures.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to calculate some sort of significance, please be aware, that the data set is too small for safe out-of-the-box usage of any classical statistical tools. You should at least do bootstraping/permutation testing of statistical significance of fitted regression parameters. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or even better, do Bayesian extension analysis (i.e. Bayesian Poisson regression) which has completely different view on how to draw conclusion from empirical data, and which is proven to work also on extremely small data sets.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-14T07:05:02.963" Id="67374" LastActivityDate="2013-08-14T07:05:02.963" OwnerUserId="10069" ParentId="67291" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;As was already mentioned, in hypothesis testing you are actually investigating the null hypothesis, usually in the hope that you can reject it. In addition to the other answers I would like to propose a somewhat different approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;Generally speaking, if you have some sort of a theory about what might be going on in your data, you could do a confirmatory analysis (like &lt;a href=&quot;http://en.wikipedia.org/wiki/Confirmatory_factor_analysis&quot; rel=&quot;nofollow&quot;&gt;confirmatory factor analysis&lt;/a&gt; as just one example). In order to do so you would need a model. You can then see how well your model fits the data. This approach would also allow for testing different models against each other. The nice thing with Big Data is that it allows you to actually do these model tests. In contrast, in psychology e.g. it is often not really possible to do so, because the sample sizes tend to be too small for these kinds of methods. &lt;/p&gt;&#10;&#10;&lt;p&gt;I realize that typically with Big Data, an exploratory approach is used, because there is no theory, yet. Also, since I don't know what exactly you are interested in, this might not really be an option.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-14T08:44:14.707" Id="67378" LastActivityDate="2013-08-14T08:44:14.707" OwnerUserId="27099" ParentId="67320" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Welcome to the site. You seem to have a few things confused. &lt;/p&gt;&#10;&#10;&lt;p&gt;Low power means that you could make a type II error. A type II error is failing to reject the null when it is, in fact, false. You can only make a type II error when you a) Fail to reject the null and b) Are incorrect. So, it's not that there is a 94% chance that your failure to reject is incorrect; it's a 94% chance IF the right decision was to reject. What if the decision is correct? Then it's no error at all. &lt;/p&gt;&#10;&#10;&lt;p&gt;Next, you have a $\chi ^2$ value of 0.25. The expected value of $\chi ^2$ under the null is the df. Your value is less than the df, it won't be significant. In fact, your data is less associated than random data would be. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-14T11:09:12.547" Id="67389" LastActivityDate="2013-08-14T11:09:12.547" OwnerUserId="686" ParentId="67386" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You have 15 values here, and there are going to be different judgements here on what is sensible with such a small dataset, but here are some personal opinions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Starting with the graphics:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The first histogram can be decoded as showing bins with frequency 1, 2 or 3. Showing a density scale is not wrong in any absolute sense, but I'd want histograms for sample sizes this small to show frequencies directly. That's a matter of clear and simple communication rather than statistical logic. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A density function being estimated here from 15 points using kernel estimation also falls into the category of not wrong, but likely to be regarded by technical readers as a real stretch with so few data. In any case, there is never a single possible kernel density estimate, even if a program produced one for you, so you need to worry more about the effects of different kernel choices, or not use the method. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;To the point here, the kernel density you show based on the raw data is unimodal, and really doesn't match the story you have of two regimes. On the other hand, that based on sampling with replacement is bimodal. The two really should be telling the same story! My guess is that you are relying on program default choices, which produces the anomaly. But there can't be more information in the bulked out data than you have originally. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;By far the easiest display to think about is the dot plot, strip plot or rug &#10;in the last graph above. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Mixture modeling here is also a real stretch with this sample size. How to interpret your graph is a little unclear as I'd expect two normal density curves and a combined one. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I realise that these are (mostly) side-issues for you. My answer to your main question is that unless you have independent evidence for uniform or mixed normal distributions, any such assumption is arbitrary. You would need to spend space and time discussing either assumption. Bulking out the data by sampling with replacement is the least bad option if you really need to get much bigger samples. But it still seems dubious: bootstrapping what you have seems a much better way of exploring uncertainty. &lt;/p&gt;&#10;&#10;&lt;p&gt;All that said, there is some sampling process that led to these 15 values, but that is not discussed here beyond an admission that there is bias. As you know, none of your solutions removes that bias, not there is any obvious alternative.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-14T13:00:57.077" Id="67399" LastActivityDate="2013-08-14T13:00:57.077" OwnerUserId="22047" ParentId="67396" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;It seems to me that some quite different questions are bundled together here. In the thread &lt;a href=&quot;http://stats.stackexchange.com/questions/67017/results-of-bootstrap-reliable&quot;&gt;Results of bootstrap reliable?&lt;/a&gt; I asked, in effect, for an explanation of what you mean by  &quot;reliable&quot; and that request still stands. But I will subdivide the questions as I see them. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Are these results surprising?&lt;/strong&gt; Without your data we cannot check, but viscerally, they are completely unsurprising to me. Fitting univariate distributions with several parameters is often very tricky, even with carefully chosen parameterisation and even if the data are a good candidate for the distribution being fitted. The territory here, with an example of financial returns data, is long-tailed, possibly skewed distributions where theoretical people have a great deal of fun thinking of new distributions but the results often disappoint people new to the field, because often no particular distribution is quite right for a particular dataset. The occurrence of outliers remains something to look out for. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Are the results trustworthy?&lt;/strong&gt; Much of the point of bootstrapping I take as being to give a honest picture of the uncertainty possible with a given procedure and a given dataset, although bootstrapping like anything else has its limitations. In another thread, a member commented &quot;I do not like the bootstrap method ... because it gave me too much different results&quot;. I recognise the feeling, but this is very much like not watching the news on television on the grounds that it is usually bad. So, without paradox, you can usually trust the bootstrap to signal if a procedure is untrustworthy in the sense that results can be very unstable. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;How could the results be used?&lt;/strong&gt; With such asymmetric distributions, you are quite right that estimates $\pm$ multiple of SEs would be poor choices for confidence intervals, but the answer is immediate: use percentile-based confidence intervals, or if needed and desired something more subtle as covered by any bootstrap text. This should come out of your software, but I am not familiar with the R functions you use. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A note of caution in this case: Financial returns I understand to be time series, but bootstrapping strict sense is based on an assumption of independent observations. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-14T18:30:58.417" Id="67413" LastActivityDate="2013-08-14T18:30:58.417" OwnerUserId="22047" ParentId="67411" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm dealing with 3D data that are the trajectory of a point over time. I would like to have an indication of how much it is &quot;spread&quot; in space and I thought about using the volume of the 95% confidence ellipsoid as they do for stabilometric measures in 2D with the displacement of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Center_of_pressure_%28fluid_mechanics%29&quot;&gt;Center of Pressure&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So I have $x,y,z$ over time and with PCA I find the three eigenvalues and eigenvectors and I can plot the ellipsoid. But then I get stuck with the confidence region...I know that probably it is trivial but I don't understand which is the coefficient that I have to multiply the axis lengths for in order to obtain the volume that I need. Can I treat my data as a trivariate distribution? I know that in the &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/4705-errorellipse&quot;&gt;errorellipse function&lt;/a&gt; for MATLAB, for example, they use the chi-squared distribution to calculate the coefficient, but I don't understand why.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-14T20:16:00.323" Id="67422" LastActivityDate="2013-08-14T21:17:48.287" LastEditDate="2013-08-14T20:44:23.367" LastEditorUserId="7290" OwnerUserId="29164" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;confidence-interval&gt;&lt;pca&gt;&lt;chi-squared&gt;&lt;spatial&gt;" Title="Volume of the 95% confidence ellipsoid" ViewCount="511" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a technology acceptance model where I assume that 10 independent variables (each latent variable is composed of three items, based on a 1-5 likert scale) should influence the actual use (dependent variable) of a contactless payment system. &#10;Only the dependent variable is composed of two items, the first one measure the frequency on a 1-5 likert scale (1.never - 2. few times per year - 3. few times per month - 4. few times per week - 5. every day) while the second one measure the number of uses (so it is an open ended question, linked to the previous item). &lt;/p&gt;&#10;&#10;&lt;p&gt;I need to conduct a SEM analysis so I should start with an exploratory factor analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;My sample is composed of 350 cases and I've found that for the two items measuring actual use the distributions are strictly not normal. In particular, only 48% declared to use the system and usage ranged from 1 to 730 per user over a period of one year.  The first item has a Skewness of 1,535 and a Kurtosis of 2,180 and the second item has a Skewness of 6,934 and a Kurtosis of 54,817.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I would like to ask if it is possible to conduct the analysis. Should I first standardize all the variables, because of the different scales of measure, and then transform both the two actual use variables to normalize the distributions?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much, it is for my thesis and I have big big troubles&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-15T09:02:49.200" Id="67464" LastActivityDate="2013-08-15T11:55:49.640" OwnerUserId="29174" PostTypeId="1" Score="1" Tags="&lt;factor-analysis&gt;" Title="Exploratory factor analysis and latent variable measuring actual use" ViewCount="208" />
  
  <row AnswerCount="2" Body="&lt;p&gt;This question may appear to be quite odd, but the following explanation should make it a little more comprehensible. &lt;/p&gt;&#10;&#10;&lt;p&gt;I work on the analysis of population trends of birds. In our team, we work with a simple trend index, that is calculated as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\rho_t - \rho_{t+1}}{\rho_t + \rho_{t+1}} = \frac{(\mbox{density at time }t) - (\mbox{density at time }t+1)}{(\mbox{density at time t })+ (\mbox{density at time }t+1)}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;When the population size does not change, the index value is $0$. When the population decreases, the index is negative; when the species goes extinct, the index value is $-1$. An increasing population yields positive index values, with the extreme of $1$ when the species colonises a formerly uninhabited area. We calculate this index for a large number of monitoring plots. For abundant species the index values of all plots together yield a nice, more or less bell-shaped curve that is convenient for further analysis. For less abundant species, however, the number of plots where it disappeared, and the number of newly colonised plots can be large leading to a three-modal curve with modes at $-1$, $0$ and $1$. The rarer the species, the higher get the peaks at $-1$ (extinction) and $+1$ (colonisation) and the flatter gets the bump in the middle. Analyses of such data (e.g. in various types of regression analysis) are difficult since I am not aware of a statistical distribution that can describe them.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that this is not a very precise question that can easily be answered, but I'd be thankful for any advice how to treat data like this or how to calculate a more &quot;user-friendly&quot; trend index for two points in time.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-15T15:47:51.963" Id="67495" LastActivityDate="2013-08-15T22:16:50.443" LastEditDate="2013-08-15T22:16:50.443" LastEditorUserId="25936" OwnerUserId="7417" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;" Title="How do I handle a normal distribution with peaks at each end?" ViewCount="75" />
  
  <row Body="&lt;p&gt;Yes, it has a conjugate prior in the exponential family. Consider the three parameter family&#10;$$&#10;\pi(\alpha, \beta \mid a, b, p) &#10;\propto \left\{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\right\}^p &#10;    \exp\left(a\alpha + b\beta \right).&#10;$$&#10;For some values of $(a, b, p)$ this is integrable, although I haven't quite figured out which (I believe $p \ge 0$ and $a &amp;lt; 0, b &amp;lt; 0$ should work - $p = 0$ corresponds to independent exponential distributions so that definitely works, and the conjugate update involves incrementing $p$ so this suggest $p &amp;gt; 0$ works as well). &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem, and at least part of the reason no one uses it, is that &#10;$$&#10;\int_0^\infty \int_0^\infty \left\{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\right\}^p &#10;    \exp\left(a\alpha + b\beta \right) = ?&#10;$$&#10;i.e. the normalizing constant doesn't have a cloed form. &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-08-15T18:55:40.713" Id="67511" LastActivityDate="2013-08-17T23:19:28.520" LastEditDate="2013-08-17T23:19:28.520" LastEditorUserId="5339" OwnerUserId="5339" ParentId="67443" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;That isn't the MAP estimate for the Beta prior. That is the posterior expected value, $E\{p\mid X\}$. The posterior distribution under the Beta(1, 1) prior is the Beta(2, 1); recall that the mode of the $p \sim \mbox{Beta}(\alpha, \beta)$ distribution is&#10;$$&#10;\mbox{mode}(p) = \frac{\alpha - 1}{\alpha+\beta -2}&#10;$$&#10;but &#10;$$&#10;E(p) = \frac{\alpha}{\alpha+\beta}.&#10;$$&#10;Hence the mode of the Beta(2, 1) is $\frac{1}{1} = 1$, the same as the MLE.&lt;/p&gt;&#10;&#10;&lt;p&gt;The MAP and MLE coincide when a flat prior is used, but it should be rememebred that this only occurs for parameters which have flat priors - for example, the induced prior on $\log\{p/(1-p)\}$ is not flat and so the MLE of this quantity is not the same as the MAP estimate even when $p \sim \mathcal U(0, 1)$. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-15T21:37:45.917" Id="67520" LastActivityDate="2013-08-15T21:37:45.917" OwnerUserId="5339" ParentId="67515" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;I would guess &lt;code&gt;lr.eta&lt;/code&gt; is the linear predictor&amp;mdash;the logit&amp;mdash;from the fitted model, as $\eta$ is a commonly used symbol for it; or, if not, the probability from the fitted model. You can check the code in &lt;code&gt;ROC&lt;/code&gt;. In any case you'll be able to calculate it from the model coefficients for any number of predictors. (Note that it won't be a cut-off for each predictor separately, but a function of all predictors.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Your first sentence should say (as evidenced by the graphs) that you're looking for where the &lt;em&gt;sum&lt;/em&gt; of sensitivity &amp;amp; specificity is maximized. But why is this &quot;optimal&quot;? Does a false positive result have the same import as a false negative result? See &lt;a href=&quot;http://stats.stackexchange.com/questions/14153/based-only-on-these-sensitivity-and-specificity-values-what-is-the-best-decisio/14178#14178&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-16T11:02:05.287" Id="67562" LastActivityDate="2014-02-05T09:17:27.740" LastEditDate="2014-02-05T09:17:27.740" LastEditorUserId="17230" OwnerUserId="17230" ParentId="67560" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;Depends on how you wish to use those predictions: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;if you want the predicted value of .1 to mean &quot;.1&quot; and not &quot;something between .05 and 1.5&quot;, then I would just use simple correlation. That way you take seriously that .1 means .1 and any deviation from that value is a real deviation. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you want to use the predicted value in the second interpretation, then I would categorize the observed values. In that interpretation, any observed value within the bracket does not represent a deviation. I would than use any measure of association for a cross tabulation of (ordered) categorical variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-08-16T13:10:41.627" Id="67574" LastActivityDate="2013-08-16T13:10:41.627" OwnerUserId="23853" ParentId="67570" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;Seconded to @Jen 's answer. In fact the 5th line in the result of &lt;code&gt;summary(hyperbfitalv)&lt;/code&gt; are SE's. They are indeed the square root of the diagonal elements of inverse-hessian &lt;code&gt;solve(hyperbfitalv$hessian)&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; sqrt(1.365591e-6)#for pi&#10;0.0011685850418347824&#10;&amp;gt;&amp;gt;&amp;gt; sqrt(5.113433e-3)#for mu&#10;0.071508272248740568&#10;&amp;gt;&amp;gt;&amp;gt; sqrt(1.5261031428)*0.002035#for delta&#10;0.0025139483860139073&#10;&amp;gt;&amp;gt;&amp;gt; sqrt(1.6617499980)*0.204827#for zeta&#10;0.26404019669949413&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that lZeta and lDelta are in fact log(Zeta) and log(Delta). Cheers!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-16T18:40:22.387" Id="67603" LastActivityDate="2013-08-16T18:40:22.387" OwnerUserId="23790" ParentId="67595" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;A friend of mine approached me to help her to interpret her multinomial logistic regression model.  They had measured people as 1 of 2 states at 2 time periods.  So, each person can have 1 of 4 configurations: start at state 1, end at state 1; start at state 1, end at state 2; start state 2, end state 1; start state 2, end state2.  They had performed an analysis where the outcome had 4 levels corresponding to each of these configurations.  This struck me as a somewhat unnatural way to do this.  I feel that this is a situation similar to if you had a continuous outcome, measured at 2 time points.  If you were interested in the change from baseline, you can model the end value and adjust for the baseline and other covariates.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this translate to logistic regression?  Can you model the end state, and adjust for the beginning state with other covariates?  I couldn't think of a good reason why this would not work, but I also had this problem.  Is this better than modeling the outcome as 4 possible categories?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-16T20:50:39.197" Id="67612" LastActivityDate="2015-01-12T04:38:25.327" OwnerUserId="26802" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;modeling&gt;&lt;interpretation&gt;&lt;longitudinal&gt;" Title="Pre-post Logistic regression" ViewCount="119" />
  
  
  <row Body="&lt;p&gt;I think adaptive in this context just means the reestimation on a rolling basis. So the parameter should not change until there is a change point. Then the true parameter increases and stays constant after it decreases again because of the second change point. The estimated parameter is evaluated compared to the true parameter: How fast does it get the change point? How fast does it &lt;em&gt;adapt&lt;/em&gt; to the new environment?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-17T07:53:38.020" Id="67622" LastActivityDate="2013-08-17T07:53:38.020" OwnerUserId="14390" ParentId="63217" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="67627" AnswerCount="1" Body="&lt;p&gt;I have scoured around, reading posts on Cross Validated (&lt;a href=&quot;http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models#30909&quot;&gt;Difference between logit and probit models&lt;/a&gt;) and also looking at references including Dobson and McCullagh and Nelder, e.g. &lt;a href=&quot;http://www.statsci.org/glm/books.html&quot; rel=&quot;nofollow&quot;&gt;http://www.statsci.org/glm/books.html&lt;/a&gt; so I am aware that this topic is well trodden. Nevertheless, I am trying to articulate and formalize my understanding of GLM and while several posts have helped me with that, I am conscious of gaps and the possibility of an unsound foundation of my understanding. &lt;/p&gt;&#10;&#10;&lt;p&gt;In simple linear regression we have some set of observations $(x_i, y_i)$ pairs and treat $y_i$ as a realization of a random variable, $Y_i$, distributed as $Y_i \sim N(\mu_i, \sigma^2)$. The means ($\mu_i$) depend on the predictor but the variance is constant. We model $\mu_i = \beta_0 + (\beta_1 \times x_i)$ (or matrix equivalent ) which I believe is the same as saying $Y_i = \mu_i + \epsilon_i$ with $\epsilon_i \sim N(0,\sigma^2)$. I am not sure but I think a correct way to state $\mu$ is $\mu_i = E[Y_i|X=x]$. Can someone confirm that?&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, we might transform the response to achieve linearity (maybe taking logs) and in that case we are modelling $log(Y_i) = \alpha_0 + (\alpha_1 \times x_i) + \epsilon_i$ where $\epsilon_i$ now is assumed to have a log normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;We generalize by decomposing the model into:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A structural component ($\beta_0 + (\beta_1 \times x_i)$)&lt;/li&gt;&#10;&lt;li&gt;A link g(.)&lt;/li&gt;&#10;&lt;li&gt;A response distribution (or random component) (member of the exponential family - Guassian, bin,gamma etc)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Let's say that the observations are assumed to come from a distribution in the exponential family and to keep things simple assume that it is the Gaussian distribution. Again, the expected value comes out as $E[Y_i|X=x] = \mu_i$, but does so from the first derivative of the b(theta) term in the exponential family form (&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0412317605&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Generalized-Edition-Monographs-Statistics-Probability/dp/0412317605&lt;/a&gt;) (pg 29). In the case where we assume a binomial distribution, the expected value comes out at np but it still comes from the first derivative of the b(theta) term when the distribution is expressed in the exponential family form. Note, I don't fully appreciate or understand the derivation of the mean or variance at this stage, perhaps someone could provide a layman's explanation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of modelling the mean as was done for simple linear regression, we now model a transformation of the mean so instead of saying $\mu_i = \beta_0 + (\beta_1 \times x_i)$ we are saying $g(\mu_i) = \eta_i = \beta_0 + (\beta_1 \times x_i)$ where g is some link function (invertible and differentiable). I think this is a key (yet still somewhat confusing to me) distinction between SLR and GLM. In SLR we transform the response ($y_i$) and model that, in GLM we transform the expected value ($\mu_i$ in the case of the Gaussian example) and model that. Another way of saying it is that in the SLR case we model $E[g(Y_i|X=x)] = \beta_0 + (\beta_1 \times x_i)$ but in the GLM world we are modelling $\eta_i = g[E(Y_i|X=x)] = \beta_0 + (\beta_1 \times x_i)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is around verifying my understanding and statement of the essence of the foundations of GLM and the differences between GLM and the traditional linear model. Thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-17T10:47:44.940" FavoriteCount="1" Id="67626" LastActivityDate="2013-08-17T11:57:14.290" OwnerUserId="24865" PostTypeId="1" Score="2" Tags="&lt;generalized-linear-model&gt;" Title="Understanding of GLM" ViewCount="308" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;1. The problem&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have some measurements of a variable $y_t$, where $t=1,2,..,n$, for which I have a distribution $f_{y_t}(y_t)$ obtained via MCMC, which for simplicity I'll assume is a gaussian of mean $\mu_t$ and variance $\sigma_t^2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a physical model for those observations, say $g(t)$, but the residuals $r_t = \mu_t-g(t)$ appear to be correlated; in particular, I have physical reasons to think that an $AR(1)$ process will suffice to take into account the correlation, and I plan on obtaining the coefficients of the fit via MCMC, for which I need &lt;strong&gt;the likelihood&lt;/strong&gt;. I think the solution is rather simple, but I'm not pretty sure (it seems so simple, that I think I'm missing something).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2. Deriving the likelihood&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A zero-mean $AR(1)$ process can be written as:&#10;$$X_t = \phi X_{t-1}+\varepsilon_t,\ \ \ (1)$$&#10;where I'll assume $\varepsilon_t\sim N(0,\sigma_w^2)$. The parameters to be estimated are, therefore, $\theta = \{\phi,\sigma_w^2\}$ (in my case, I also have to add the parameters of the model $g(t)$, but that's not the problem). What I observe, however, is the variable&#10;$$R_t = X_t+\eta_t,\ \ \ (2)$$&#10;where I'm assuming $\eta_t\sim N(0,\sigma_t^2)$, and the $\sigma_t^2$ are known (the measurement errors). Because $X_t$ is a gaussian process, $R_t$ is also. In particular, I know that&#10;$$X_1 \sim N(0,\sigma_w^2/[1-\phi^2]),$$&#10;therefore,&#10;$$R_1 \sim N(0,\sigma_w^2/[1-\phi^2]+\sigma_t^2).$$&#10;The next challenge is to obtain $R_t|R_{t-1}$ for $t\neq 1$. To derive the distribution of this random variable, note that, using eq. $(2)$ I can write&#10;$$X_{t-1} = R_{t-1}-\eta_{t-1}.\ \ \ (3)$$&#10;Using eq. $(2)$, and using the definition of eq. $(1)$, I can write,&#10;$$R_{t} = X_t+\eta_t = \phi X_{t-1}+\varepsilon_{t}+\eta_t.$$&#10;Using eq. $(3)$ in this last expression, then, I obtain,&#10;$$R_{t} = \phi (R_{t-1}-\eta_{t-1})+\varepsilon_{t}+\eta_t,$$&#10;thus,&#10;$$R_t|R_{t-1} = \phi (r_{t-1}-\eta_{t-1})+\varepsilon_{t}+\eta_t,$$&#10;and, therefore,&#10;$$R_t|R_{t-1} \sim N(\phi r_{t-1},\sigma_w^2+\sigma_t^2-\phi^2\sigma^2_{t-1}).$$ Finally, I can write the likelihood function as&#10;$$L(\theta) = f_{R_1}(R_1=r_1) \prod_{t=2}^{n} f_{R_{t}|R_{t-1}}(R_t=r_t|R_{t-1}=r_{t-1}),$$&#10;where the $f(\cdot)$ are the distributions of the variables that I just defined, .i.e., defining $\sigma'^2 = \sigma_w^2/[1-\phi^2]+\sigma_t^2,$&#10;$$f_{R_1}(R_1=r_1) = \frac{1}{\sqrt{2\pi \sigma'^2}}\text{exp}\left(-\frac{r_1^2}{2\sigma'^2}\right),$$&#10;and defining $\sigma^2(t) = \sigma_w^2+\sigma_t^2-\phi^2\sigma^2_{t-1}$,&#10;$$f_{R_{t}|R_{t-1}}(R_t=r_t|R_{t-1}=r_{t-1})=\frac{1}{\sqrt{2\pi \sigma^2(t)}}\text{exp}\left(-\frac{(r_t-\phi r_{t-1})^2}{2\sigma^2(t)}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3. Questions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;strong&gt;Is my derivation ok?&lt;/strong&gt; I don't have any resources to compare other than simulations (which seem to agree), and I'm not a statistician!&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Are there any derivation of this kind of things in the literature for $MA(1)$ proccesses or $ARMA(1,1)$ proccesses?&lt;/strong&gt; A study for $ARMA(p,q)$ proccesses in general that could be particularized to this case would be nice.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2013-08-17T16:11:43.970" FavoriteCount="1" Id="67634" LastActivityDate="2015-02-06T22:15:08.443" LastEditDate="2013-08-20T21:42:39.977" LastEditorUserId="9174" OwnerUserId="9174" PostTypeId="1" Score="10" Tags="&lt;time-series&gt;&lt;mcmc&gt;&lt;autoregressive&gt;&lt;likelihood-function&gt;" Title="AR(1) process with heteroscedastic measurement errors" ViewCount="397" />
  
  
  <row Body="&lt;p&gt;Why use a toy matrix?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just get some small (maybe toy) data set, and compute the similarity matrix for it, and verify it against the original data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-17T18:46:18.943" Id="67646" LastActivityDate="2013-08-17T18:46:18.943" OwnerUserId="7828" ParentId="67635" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;For that first iterate? That shouldn't matter, since in effect it just affects your starting point - as long as subsequently you sample correctly, everything should still work as intended - the convergence still works and so on. (It's useless but should be harmless.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If after that first time through, your scheme is doing one of these:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1)&#10; $\theta_1^{(i+1)} \sim f(\theta_1 | \theta_2^{(i)}, \text{Data})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\,\,\theta_2^{(i+1)} \sim f(\theta_2 | \theta_1^{(i+1)}, \text{Data})$&lt;/p&gt;&#10;&#10;&lt;p&gt;OR&lt;/p&gt;&#10;&#10;&lt;p&gt;(2)&#10;$\theta_2^{(i+1)} \sim f(\theta_2 | \theta_1^{(i)}, \text{Data})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\,\,\theta_1^{(i+1)} \sim f(\theta_1 | \theta_2^{(i+1)}, \text{Data})$&lt;/p&gt;&#10;&#10;&lt;p&gt;then you have a Gibbs sampler and from the convergence of the Markov Chain to its stationary distribution, you'll in the long run be sampling from $f(\theta_1 , \theta_2| \text{Data})$. It shouldn't matter that you did something nonstandard on the first step.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-17T21:34:58.067" Id="67652" LastActivityDate="2013-08-18T00:52:42.220" LastEditDate="2013-08-18T00:52:42.220" LastEditorUserId="805" OwnerUserId="805" ParentId="67651" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One very famous paper that uses simple OLS that any 1st year economics undergraduate can understand and recreate is: &lt;/p&gt;&#10;&#10;&lt;p&gt;Mankiw, N. Gregory, David Romer, and David N. Weil (1992), &#10;    “A Contribution to the Empirics of Economic Growth.” &#10;    &lt;em&gt;The Quarterly Journal of Economics&lt;/em&gt; 107: 407-437. &lt;/p&gt;&#10;&#10;&lt;p&gt;It's a great early contribution to the growth literature, and a very easy read! It uses data from the &lt;a href=&quot;https://pwt.sas.upenn.edu/php_site/pwt_index.php&quot; rel=&quot;nofollow&quot;&gt;Summer and Heston World Tables&lt;/a&gt; which are freely available!&lt;/p&gt;&#10;&#10;&lt;p&gt;A Google will reveal .pdf copies of this paper in various places. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, let me know if you're not too into macro/growth and want something different. I can try to think up some cooler papers that only use OLS.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-17T23:35:13.387" Id="67655" LastActivityDate="2013-08-18T08:08:23.443" LastEditDate="2013-08-18T08:08:23.443" LastEditorUserId="22047" OwnerUserId="29093" ParentId="67632" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a software which takes input as investment and gives the output as return on a particular stock. Now profit metric $x_i$ is defined as the ratio of return $g_i$ to maximum possible return $g_{max}$ at $i^{th}$ period of time. &lt;/p&gt;&#10;&#10;&lt;p&gt;so $x_i = g_i / g_{max}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$g_{max}$ is constant for any period but varies from stock to stock. &#10;Return for a particular stock at $i_{th}$ period is $g_i$ which depends on the investment $s_i$ for that period.&#10;The relation between $g_i$ and $s_i$ varies from one period to other period.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;question&lt;/strong&gt; is how to decide $s_i$ based on $s_{i-1}$ and $g_{i-1}$ in order to maximize $x_i$ for a particular stock . Is it possible to build an online learning mechanism to decide the value of $s_i$ based on previous data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;[note : $ 0 &amp;lt; g_i \leq g_{max}$ , $ s_{min} \leq s_i \leq s_{max}$ , $i_{th}$ time period is fixed ,in this case it's six months ]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-18T03:43:47.783" FavoriteCount="2" Id="67660" LastActivityDate="2013-08-28T06:23:46.990" LastEditDate="2013-08-18T04:26:16.933" LastEditorUserId="29258" OwnerUserId="29258" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;stochastic-processes&gt;" Title="Online learning to maximize profit" ViewCount="85" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;em&gt;&lt;strong&gt;Introduction&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm working with heart rate data. IBI (InterBeat Interval) is defined as the time period between any two consecutive heart beat and is usually measured in millisecond. I have followed a subject for 6 days and using a device, I have all his IBI measures. My dataset has about 450,000 IBI measure. The device I'm using to measure IBI's is not 100% accurate. Therefore, I may get IBI measures that our out of range and are not consistent with previous and future measures. Consider the data of: &lt;code&gt;1500,580, 590, 570, 580, 1450, 560, 590, ...&lt;/code&gt; for sure, 1450 is an outlier.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Question&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to find a reasonable way to:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Detect outliers (like 1500, 1450 in the example above)&lt;/li&gt;&#10;&lt;li&gt;Impute a reasonable value for detected outliers&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;What I have done so far&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is to fit a cubic natural spline to the data. Then, any point outside of the 95% confidence interval around the N.S. fit, is considered as outlier. I can then impute for the outliers by using the fitted value from the N.S. fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Issue&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the data above, the outliers &lt;code&gt;1500&lt;/code&gt; and &lt;code&gt;1450&lt;/code&gt; have substantial effect on the N.S fit. They might even cause the true values right next to them (580, 590, ...) being marked as outlier. This means that even those values are valid, they need to be imputed with fitted value which in case are lot larger than those valid values.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;How can I solve the issue above?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it reasonable to first fit an initial N.S. fit and mark all outliers. Then, fit a secondary N.S. (natural spline) fit with all outliers excluded from the dataset and use this secondary fit to impute for outliers?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would you have any better idea?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot for your help. I believe this message can initiate a very good discussion on outlier detection/imputation methods.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-18T18:56:12.447" FavoriteCount="3" Id="67685" LastActivityDate="2013-08-18T21:48:18.137" OwnerUserId="8006" PostTypeId="1" Score="2" Tags="&lt;outliers&gt;&lt;data-imputation&gt;&lt;splines&gt;" Title="Outlier detection/imputation - discussion" ViewCount="207" />
  
  
  
  
  
  
  <row AcceptedAnswerId="67770" AnswerCount="2" Body="&lt;p&gt;I am working with a binary predictive model for data that belongs to A and B. The learning sample that I am using contains 6000 row that belongs to group A and 1000 row that belongs to group B. I would like to make my learning sample equal in number for both variables (i.e. 1000 row that belongs to A and 1000 row that belongs to B). A random sampling technique might be very biased when selecting 1000 out of 6000 rows. What would be the best way to pick these 1000 samples from group A in a way that assures this is not a biased sampling? &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, I would like to have a sample of 1000 rows that represent as closely as possible the 6000 rows. What technique would do this best?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-19T13:58:39.900" Id="67755" LastActivityDate="2013-11-20T13:55:30.440" LastEditDate="2013-08-19T14:15:29.837" LastEditorUserId="28940" OwnerUserId="28940" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;sampling&gt;&lt;data-mining&gt;&lt;prediction&gt;&lt;bias&gt;" Title="Perfect sampling from a huge dataset" ViewCount="233" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;Feel free to critique my overall approach instead of answering my question directly.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to look at bivariate relationships among a binary outcome and multiple predictor variables before conducting multiple regression. EDIT: [The data are multiply imputed] and the categorical predictors have been dummy coded. Therefore, for each nominal categorical variable in the original data, there are k-1 dummy variables, with the omitted category serving as the reference group.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seemed to me that there was no meaningful &quot;bivariate&quot; relationship between the outcome and a single [dummy] variable from a collection of related variables (this would change the comparison category to &quot;everything else,&quot; which could not be the case in a multiple regression). For this reason, I did not use bivariate correlations. Instead, I did a series of &quot;bivariate&quot; logistic regressions including in a single regression all k-1 dummies of a variable. So, like: outcome = race_black race_other (race_white omitted). In any case, the reference category cannot be examined directly since it was omitted in the multiple imputation process and has missing values.&lt;/p&gt;&#10;&#10;&lt;p&gt;I figured if any coefficient of a binary dummy were significant, I would include the whole variable (group of k-1 dummies) in the omnibus regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;After laboriously compiling a table of the results of these &quot;bivariate&quot; regressions without including the intercept, it occurred to me that (MAYBE?) the intercept is the &quot;dummy&quot; for the omitted category. If it is significant, is it in fact indicating that the omitted group is different from the mean on the outcome? If that's true, should I include the variable in my omnibus regression even if none of the explicit category variables are significant predictors?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm worried that I am thinking wrongly about the comparison groups and meanings of significant coefficients.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-19T14:35:35.023" FavoriteCount="1" Id="67762" LastActivityDate="2013-08-20T10:23:07.860" LastEditDate="2013-08-19T14:45:50.910" LastEditorUserId="25759" OwnerUserId="25759" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;categorical-data&gt;" Title="Interpretation of logistic regression intercept with one dummy coded categorical variable" ViewCount="848" />
  
  <row Body="&lt;p&gt;My answer is going to be based on Whuber's comment from above. Like Whuber said, by default, &lt;code&gt;sample&lt;/code&gt; should be sampling with equal probability.  However, if you specify it yourself using the &lt;code&gt;prob&lt;/code&gt; option, the two methods do not return the same answer. However, the difference between the two is systematic. In fact, it turns out (if you set the random seed) the sample will be exactly the same minus one. That is, if you use the &lt;code&gt;prob&lt;/code&gt; option then you should need to only subtract 1 from your samples to get back what sample would have returned had you not used the &lt;code&gt;prob&lt;/code&gt; option.  Here is some very short code illustrating that point.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N = 100&#10;n = length(50:90)&#10;&#10;set.seed(1)&#10;random.sample1 = sample(50:90, N, replace=TRUE, prob=rep(1/n, times=n))&#10;&#10;set.seed(1)&#10;random.sample2 = sample(50:90, N, replace=TRUE)&#10;&#10;plot(density(random.sample1),col=&quot;blue&quot;)&#10;lines(density(random.sample2),col=&quot;red&quot;)&#10;&#10;summary(random.sample1)&#10;summary(random.sample2)&#10;&#10;random.sample1&#10;random.sample2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which yields&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(random.sample1)&#10;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. &#10;  50.00   63.00   70.00   71.27   82.00   90.00 &#10;&amp;gt; summary(random.sample2)&#10;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. &#10;  50.00   62.75   69.50   70.68   81.00   90.00 &#10;&amp;gt; &#10;&amp;gt; random.sample1&#10;  [1] 61 66 74 88 59 87 89 78 76 53 59 58 79 66 82 71 80 50 66 82 89 59 77 56 61&#10; [26] 66 51 66 86 64 70 75 71 58 84 78 83 55 80 67 84 77 83 73 72 83 51 70 81 79&#10; [51] 70 86 68 61 53 55 63 72 78 67 88 63 69 64 77 61 70 82 54 86 64 85 65 64 70&#10; [76] 87 86 66 82 90 68 80 67 64 82 59 80 55 61 56 60 53 77 86 82 83 69 67 84 75&#10;&amp;gt; random.sample2&#10;  [1] 60 65 73 87 58 86 88 77 75 52 58 57 78 65 81 70 79 90 65 81 88 58 76 55 60&#10; [26] 65 50 65 85 63 69 74 70 57 83 77 82 54 79 66 83 76 82 72 71 82 50 69 80 78&#10; [51] 69 85 67 60 52 54 62 71 77 66 87 62 68 63 76 60 69 81 53 85 63 84 64 63 69&#10; [76] 86 85 65 81 89 67 79 66 63 81 58 79 54 60 55 59 52 76 85 81 82 68 66 83 74&#10;&amp;gt; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So as you can see, the two sample are the same with one being shifted by minus 1.  Why this occurs I have not figured out by my guess is that by default the &lt;code&gt;sample&lt;/code&gt; command assigns the equal probabilities differently then the user would. Also, a disclaimer, the above idea works in the case when the sample is an integer, however, when I ran the same code sampling from numbers with decimals, I could not get the above results to hold.  Most likely it must have to do something with the comment: &quot;The optional prob argument can be used to give a vector of weights for obtaining the elements of the vector being sampled. They need not sum to one, but they should be non-negative and not all zero.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Pviis.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h1&gt;Update:&lt;/h1&gt;&#10;&#10;&lt;p&gt;So after digging a bit deeper we see that the &lt;code&gt;sample&lt;/code&gt; command actually relies on the command &lt;code&gt;sample.int&lt;/code&gt;.  However, withing &lt;code&gt;sample.int&lt;/code&gt;, if you do not specify the &lt;code&gt;prob&lt;/code&gt; option then the command calls &lt;code&gt;.Internal(sample2())&lt;/code&gt; which I have not figured out how to see inside of.  However, if someone know how to see what the function &lt;code&gt;sample2&lt;/code&gt; is doing, then we will have our answer as to how they specify the &lt;code&gt;prob&lt;/code&gt; option when not explicitly given.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sample&#10;function (x, size, replace = FALSE, prob = NULL) &#10;{&#10;    if (length(x) == 1L &amp;amp;&amp;amp; is.numeric(x) &amp;amp;&amp;amp; x &amp;gt;= 1) {&#10;        if (missing(size)) &#10;            size &amp;lt;- x&#10;        sample.int(x, size, replace, prob)&#10;    }&#10;    else {&#10;        if (missing(size)) &#10;            size &amp;lt;- length(x)&#10;        x[sample.int(length(x), size, replace, prob)]&#10;    }&#10;}&#10;&amp;lt;bytecode: 0x000000000ff22210&amp;gt;&#10;&amp;lt;environment: namespace:base&amp;gt;&#10;&#10;&amp;gt; sample.int&#10;function (n, size = n, replace = FALSE, prob = NULL) &#10;{&#10;    if (!replace &amp;amp;&amp;amp; is.null(prob) &amp;amp;&amp;amp; n &amp;gt; 1e+07 &amp;amp;&amp;amp; size &amp;lt;= n/2) &#10;        .Internal(sample2(n, size))&#10;    else .Internal(sample(n, size, replace, prob))&#10;}&#10;&amp;lt;bytecode: 0x000000000ffa3478&amp;gt;&#10;&amp;lt;environment: namespace:base&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-08-19T16:06:09.470" Id="67773" LastActivityDate="2013-08-19T16:17:19.690" LastEditDate="2013-08-19T16:17:19.690" LastEditorDisplayName="user25658" OwnerDisplayName="user25658" ParentId="67750" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This should probably go as a comment but I cannot add it there because of low reputation. However, I think it can also partly serve as an answer to the OP's question.&lt;/p&gt;&#10;&#10;&lt;p&gt;As @NickCox alluded to, you should tackle this piecemeal. Read through the theory, work with a small number of variables and try to develop an intuitive sense of what is happening, although to develop this intuitive understanding will take some time, it will be worth it when you are trying to interpret the results of your principal component analysis.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you look at the link @NickCox suggested and scroll down to whuber's answer you will find a good geometric explanation for what eigenvectors and eigenvalues mean and will give you a visual feel of what you are looking at. I will add to that answer - if you collapse his ellipsoid into a two dimensional space i.e look at only the first two principal components, you will get an ellipse with one major and one minor axis. The major axis is the &quot;direction&quot; which explains most of the variance of your data. It is represented by the first eigenvector and its length is the first eigenvalue (when you arrange the eigenvalues in descending order). It is &lt;em&gt;also&lt;/em&gt; the &lt;em&gt;linear combination&lt;/em&gt; of all the variables in your dataset, more specifically it is that linear combination which &lt;em&gt;most explains&lt;/em&gt; the variance in your data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, going back to your data you should look into textbooks on multivariate methods for methods of analyzing the covariance structure of your dataset. I am afraid I cannot point to anything specific but I think any intermediate level book would do. Additionally, I remember reading about applied principal component analysis from the book &lt;em&gt;Statistical Methods in the Atmospheric Sciences&lt;/em&gt; by Daniel Wilks. If you get a hold of this book, you will find that it gives a good overview of the methodology in Chapter 11 (atmospheric scientists call PCA as Empirical Orthogonal Functions and it is the same thing). The author works with multi-variate data so it is structurally similar to what you have.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, in addition to clustering and PCA you can also look into Canonical Correlation Analysis (CCA). It is also used in reducing the dimensionality of one's dataset but it looks at the relationship &lt;em&gt;between&lt;/em&gt; pairs of data. I cannot say whether it would be applicable in your case.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-19T18:39:56.533" Id="67784" LastActivityDate="2013-08-19T18:39:56.533" OwnerUserId="10088" ParentId="67748" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="67831" AnswerCount="1" Body="&lt;p&gt;I am interested in modelling human activities using sensor data with HMMs and would like to incorporate prior knowledge during inference. The normal procedure is to model K different activities with K separate HMMs. To test an unknown sequence, compute its likelihood from each of the HMMs and the HMM with maximum value is assigned as the class label. This is all done under the assumption that priors over HMM are uniform. &lt;/p&gt;&#10;&#10;&lt;p&gt;A problem can arise when one of the class is rare or unusual and its prior probability may be very low in comparison to other classes and therefore the uniform priors may not be a good assumption.  Therefore, I am interested in posterior probability and not just the likelihood to capture the combined effect. My observations are continuous (features extracted from sensor) and not discrete values. My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Can inference be done using a bayesian network type approach that include multiplication of prior with likelihood? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In my case the prior will be the count of activities available per HMM. Can that be estimated using a dirichlet prior to avoid zero-count problem for rare class (assuming I approximate an HMM for a rare class). Does that make sense?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The multivariate observation data is approximated using single gaussian (and not mixtures), in that case likelihood will be gaussian?, can it be mixed with dirichlet prior to compute posterior probability? or the likelihood is still multinomial as it represents K different outcomes from K different HMMs?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Sorry if I have mixed with some of the basic concepts, I am new and I seek guidance to move further.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-19T20:19:00.563" FavoriteCount="0" Id="67793" LastActivityDate="2013-10-19T09:54:32.537" LastEditDate="2013-10-19T09:54:32.537" LastEditorUserId="88" OwnerUserId="29314" PostTypeId="1" Score="1" Tags="&lt;maximum-likelihood&gt;&lt;hidden-markov-model&gt;&lt;posterior&gt;&lt;dirichlet-distribution&gt;&lt;conjugate-prior&gt;" Title="How to use prior probability in inferencing from HMM for activity recognition?" ViewCount="187" />
  
  <row Body="" CommentCount="0" CreationDate="2013-08-20T01:23:41.033" Id="67808" LastActivityDate="2013-08-20T01:23:41.033" LastEditDate="2013-08-20T01:23:41.033" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="" CommentCount="0" CreationDate="2013-08-20T01:36:11.383" Id="67810" LastActivityDate="2013-08-20T01:36:11.383" LastEditDate="2013-08-20T01:36:11.383" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row Body="The Intra-class Correlation Coefficient measures the degree to which a categorical variable accounts for variability in a continuous variable. It is often used to index the degree of clustering in a dataset." CommentCount="0" CreationDate="2013-08-20T01:57:20.723" Id="67822" LastActivityDate="2013-11-17T19:15:25.573" LastEditDate="2013-11-17T19:15:25.573" LastEditorUserId="7290" OwnerUserId="21599" PostTypeId="4" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The problem that we have is as follows. We have close to 60 discrete random variables each of which shall take on an average of 5 categorical values. We have developed a Bayesian network representation using our domain knowledge. We have the data for these 50 discrete random variables and how they interplay with each other using some logs.&lt;/p&gt;&#10;&#10;&lt;p&gt;We are not able to compile/infer the conditional probability distribution and marginal probability distribution for this Bayesian network from the data/spreadsheet. The software libraries (gRain, bnlearn in CRAN) give up.&lt;/p&gt;&#10;&#10;&lt;p&gt;As of now, we are trying to solve the problem by introducing some latent variables and by exploiting some local structure inherent in the problem. We are not successful so far. Any generic suggestions in terms of algorithms, tools to model, infer and solve problems of this scale shall be very useful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-20T08:59:39.713" FavoriteCount="1" Id="67838" LastActivityDate="2014-04-03T06:27:29.673" LastEditDate="2014-04-03T06:27:29.673" LastEditorUserId="32036" OwnerUserId="29330" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;bayesian&gt;&lt;algorithms&gt;&lt;cdf&gt;&lt;latent-variable&gt;" Title="Problems in scale Bayesian network mode using R" ViewCount="186" />
  <row AcceptedAnswerId="67891" AnswerCount="1" Body="&lt;p&gt;I have collected data on gas fluxes from plots of soil subjected to 5 different treatments (&quot;D2&quot;, &quot;K2&quot;, &quot;M&quot;, &quot;N&quot;, and &quot;O2&quot;), which also possessed variable clay contents.  The experiment was laid out in a randomized complete block design, with 4 replications.  Within each plot, two separate measurements of flux were performed.  The resulting data.frame resembles the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;block   treatment   subsample       flux        clay&#10;1           D2          1           112068.6003 14.8&#10;1           D2          2           129223.1641 14.8&#10;1           K2          1           256712.4712 15.5&#10;1           K2          2           113343.9756 15.5&#10;1           M2          1           85794.47834 16.4&#10;1           M2          2           -33620.6990 16.4&#10;1           N           1           70283.98133 18.2&#10;1           N           2           49569.84621 18.2&#10;1           O2          1           100553.1116 13.4&#10;1           O2          2           38885.99674 13.4&#10;2           D2          1           96968.58451 15.8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to build a linear mixed effect model that takes account of this subsampling, and have come up with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;flux.lme &amp;lt;- lme(flux ~ treatment + block, random = ~1|subsample)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which produces an ANOVA table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; anova(flux.lme)&#10;               numDF denDF   F-value p-value&#10;(Intercept)        1    26 158.15781  &amp;lt;.0001&#10;treatment          4    26   8.88691  0.0001&#10;clay               1    26   1.72640  0.2003&#10;block              3    26   1.59188  0.2153&#10;treatment:clay     4    26   1.73011  0.1736&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This output seems a bit strange to me, as the denominator degrees of freedom should not be 26, which is taking each repeated sampling as independent experimental unit. It should instead be based on the number of “main plots”, which is 20. In my case the denominator d.f. should be 20-1-3-1-4-4=7. Is is possible to instruct the lme() function to use this value?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-20T09:55:48.763" FavoriteCount="1" Id="67840" LastActivityDate="2013-08-20T17:45:00.697" LastEditDate="2013-08-20T15:14:32.357" LastEditorUserId="29331" OwnerUserId="29331" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;lme&gt;" Title="Representing repeated measurements within sample plots in linear mixed effects model in R" ViewCount="446" />
  
  <row Body="&lt;p&gt;Since Thierry Silbermann has already answered the first part of the question, I will&#10;confine myself to the second part.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $A_k$ denote the event that $\tau = k$.  Then, $A_2, A_3, A_4, \ldots$ are&#10;&lt;em&gt;disjoint&lt;/em&gt; or &lt;em&gt;mutually exclusive&lt;/em&gt; events. Then, the third axiom of probability&#10;theory tells us that the event &lt;/p&gt;&#10;&#10;&lt;p&gt;$$B = \{\tau ~\text{has finite value}\} = A_2 \cup A_3 \cup A_4 \cup \cdots$$&#10;has probability&#10;$$P(B) = P(A_2 \cup A_3 \cup A_4 \cup \cdots) = P(A_2) + P(A_3) + P(A_4) + \cdots$$&#10;where the sum on the right is really&#10;$$\begin{align}&#10;\lim_{k \to \infty} \bigr[P(A_2) + P(A_3) + P(A_4) + \cdots + P(A_k)\bigr]&#10;&amp;amp;= \lim_{k \to \infty} \bigr[0.5 + (0.5)^2 + \cdots + (0.5)^{k-1}\bigr]\\&#10;&amp;amp;= \lim_{k \to \infty} (0.5)\times \frac{1-(0.5)^{k-1}}{1-0.5}\\&#10;&amp;amp;= \lim_{k \to \infty} 1-(0.5)^{k-1}\\&#10;&amp;amp;= 1.&#10;\end{align}$$&#10;Note that we have used the formula for a geometric series in the&#10;calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $P(B) = 1$, &#10;the &lt;em&gt;complementary&lt;/em&gt; event $B^c$ that the system stays in State $4$ forever&#10;and never returns to State $1$ thus has probability $0$.  Note that $B^c$&#10;is not necessarily the empty or impossible event $\emptyset$; that is,&#10;it is not necessary to deny the &lt;em&gt;logical possibility&lt;/em&gt; that the system never ever&#10;returns to State $1$.  It is just that&#10;the probability model assigns a probability of&#10;$0$ to such an occurrence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this is stats.SE, it is worth&#10;considering that none of us will ever be in a position to verify &#10;and cross-validate that $B^c$ actually occurred.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-20T14:06:40.953" Id="67864" LastActivityDate="2013-08-20T14:06:40.953" OwnerUserId="6633" ParentId="48185" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Another option is the lasso, which can efficiently select a subset of important predictors from large collection.  This can be implemented in the &lt;code&gt;glmnet&lt;/code&gt; package in R.  See &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/lasso.html&quot; rel=&quot;nofollow&quot;&gt;http://www-stat.stanford.edu/~tibs/lasso.html&lt;/a&gt; for more information.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-20T15:07:10.600" Id="67877" LastActivityDate="2013-08-20T15:07:10.600" OwnerUserId="28916" ParentId="67867" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The censoring process needs to be independent of the survival process. I would advocate the second method of censoring at 5 years, as this ensures that the censoring time is independent of the survival process and any relevant covariates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also note that time A and time B may be influenced by the same covariates (e.g. disease severity, but also year due to treatment protocol changes), so using B-A as the time of interest may be problematic. A classic example of this problem is the change in breast cancer mortality due to earlier detection.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-20T15:07:33.057" Id="67878" LastActivityDate="2013-08-20T15:07:33.057" OwnerUserId="29020" ParentId="67871" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="88338" AnswerCount="1" Body="&lt;p&gt;I am reading conflicting references regarding the &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher_consistency&quot; rel=&quot;nofollow&quot;&gt;Fisher consistency&lt;/a&gt; of the sample variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$s^2 = \frac{\sum_i^n (x_i - \bar{x})^2}{n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone explain me how to proof whether $s^2$ is Fisher consistent or not? The &lt;a href=&quot;http://en.wikipedia.org/wiki/Talk%3aFisher_consistency&quot; rel=&quot;nofollow&quot;&gt;discussion page&lt;/a&gt; on Wikipedia was inconclusive as well.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-20T15:20:32.693" Id="67879" LastActivityDate="2014-03-01T01:57:03.473" LastEditDate="2013-08-20T19:04:37.270" LastEditorUserId="28090" OwnerUserId="28090" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;&lt;variance&gt;&lt;robust&gt;&lt;fisher&gt;&lt;consistency&gt;" Title="Sample variance Fisher consistency" ViewCount="98" />
  
  <row AnswerCount="0" Body="&lt;p&gt;My clinical study aim is looking at treatment effect on two outcomes. Each subject underwent 3 measurements at 3 different time points at baseline, 6 and 12 months after initiated treatment. We measure two dependent variables at each time point.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to perform correlation analysis to see if there is any correlation between the two dependent variables in each subject using baseline variable for each subject as reference.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering if Spearman's rank correlation is appropriate for this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-20T19:19:42.063" Id="67898" LastActivityDate="2013-08-20T19:39:58.300" LastEditDate="2013-08-20T19:39:58.300" LastEditorUserId="22047" OwnerUserId="29354" PostTypeId="1" Score="1" Tags="&lt;longitudinal&gt;&lt;rank-correlation&gt;" Title="longitudinal correlation in SPSS" ViewCount="67" />
  
  
  
  
  
  <row Body="&lt;p&gt;Here's some pseudocode to do it. Of course, it depends on the error structure you choose. You don't need the stats models to do it, because Scipy has an minimizer built-in. The minimizer probably doesn't give you CIs though, like mle2 will. There may be another minimizer that will profile your parameters, but I don't know of one on the top of my head.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, here you go&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from scipy import stats&#10;import numpy as np&#10;from scipy.optimize import minimize&#10;import pylab as py&#10;&#10;ydata = np.array([0.1,0.15,0.2,0.3,0.7,0.8,0.9, 0.9, 0.95])&#10;xdata = np.array(range(0,len(ydata),1))&#10;&#10;def sigmoid(params):&#10;    k = params[0]&#10;    x0 = params[1]   &#10;    sd = params[2]&#10;&#10;    yPred = 1 / (1+ np.exp(-k*(xdata-x0)))&#10;&#10;    # Calculate negative log likelihood&#10;    LL = -np.sum( stats.norm.logpdf(ydata, loc=yPred, scale=sd ) )&#10;&#10;    return(LL)&#10;&#10;&#10;initParams = [1, 1, 1]&#10;&#10;results = minimize(sigmoid, initParams, method='Nelder-Mead')&#10;print results.x&#10;&#10;estParms = results.x&#10;yOut = yPred = 1 / (1+ np.exp(-estParms[0]*(xdata-estParms[1])))&#10;&#10;py.clf()&#10;py.plot(xdata,ydata, 'go')&#10;py.plot(xdata, yOut)&#10;py.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me the following:&#10;&lt;img src=&quot;http://i.stack.imgur.com/bZcHL.png&quot; alt=&quot;MLfit&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-21T04:02:03.887" Id="67933" LastActivityDate="2013-08-21T04:02:03.887" OwnerUserId="27861" ParentId="66199" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Two words: Sample Size...A power analysis is a must. By including a competent statistician on your team from the get-go, you will likely save yourself a great deal of frustration when you are writing the results and discussion sections of your manuscript or report. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is all too common for a principal investigator to collect data prior to consulting with a statistician with the expectation of a &quot;predictive model&quot; or a &quot;causal relationship&quot; from a sample of less than 30 subjects. Had the PI consulted with a statistician prior to collecting data, the statistician would have been able to inform the PI, after appropriate analyses, to collect more data/subjects or to restructure the goals of their analysis plan/project.&lt;/p&gt;&#10;" CommentCount="14" CommunityOwnedDate="2013-08-23T14:43:23.057" CreationDate="2013-08-21T05:10:29.503" Id="67939" LastActivityDate="2013-08-21T05:10:29.503" OwnerUserId="29068" ParentId="67936" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;I believe what Fisher meant in his famous quote goes beyond saying &quot;We will do a full factorial design for our study&quot; or another design approach. Consulting a statistician when planning the experiment means thinking about every aspect of the problem in an intelligent way, including the research objective, what variables are relevant, how to collect them, data management, pitfalls, intermediate assessment of how the experiment is going and much more. Often, I find it is important to see every aspect of the proposed experiment hand-on to really understand where the difficulties lie.&lt;/p&gt;&#10;&#10;&lt;p&gt;My experience is mainly from medical applications. Some of the issues I have encountered that could have been prevented by consulting a statistician beforehand:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Insufficient sample size is, of course, number one on this list. Often, data from previous studies would have been available and it would have been easy to give a reasonable estimate of the sample size needed. In these cases, the only recourse is often to do a purely descriptive analysis of the data and promise further research in the paper (not publishing is usually not an option after doctors invested valuable time).&lt;/li&gt;&#10;&lt;li&gt;Execution of the experiments is left to convenience and chance instead of design. An example I am currently working on has measurements collected over time. The measurement times, measurement frequency and end of monitoring period all vary wildly between individuals. Increasing the number of measurements per individual and fixing the measurement dates and end of monitoring period would have been fairly little extra work (in this case) and would have been very beneficial to the study.&lt;/li&gt;&#10;&lt;li&gt;Poor control of nuisance factors that could have easily been controlled. E.g. measurements were sometimes performed on the day of sample collection and sometimes later, leaving the possibility that the sample has degraded.&lt;/li&gt;&#10;&lt;li&gt;Poor data management, including my personal favourite &quot;I rounded the data before putting it into the computer, because the machine is inaccurate in its measurements&quot;. Often, relevant data is just not collected and it is impossible to get it after the fact.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Often, problems with a study go even further back, to the initial conception of the research:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Data is sometimes collected without a clear objective and just the assumption that it will be useful somehow. Producing hypotheses and &quot;significant results&quot; is left to the statistician.&lt;/li&gt;&#10;&lt;li&gt;And the opposite: data is scraped together with the aim of proving a specific point that the PI has in his head, irrespective of the data and what can actually be proved with it. This time, the statistician is just supposed to put his stamp of significance on pre-written conclusions without the conclusions getting adjusted in the face of the data.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So far, this mainly sounds like the statistician suffers and maybe scientific integrity suffers when the PI tries to push conclusions not supported by the data (always a fun discussion). But the experimental team suffers as well, because they do unnecessary extra work (while not doing necessary work) during the experimental phase and need to spend much more time in discussion with their statistician after the fact, because they did not get their advice before. And of course, the final paper will be worse, will have fewer conclusions (and more &quot;conjectures&quot;) and will likely not make it into that high-impact journal the PI wanted.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2013-08-23T14:43:23.057" CreationDate="2013-08-21T10:47:27.240" Id="67961" LastActivityDate="2013-08-23T13:14:43.797" LastEditDate="2013-08-23T13:14:43.797" LastEditorUserId="17230" OwnerUserId="29020" ParentId="67936" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;I can't see that ordering the levels by frequency creates an ordinal variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Shrinkage is necessary to deal with this problem, either by using penalized maximum likelihood estimation (e.g., R &lt;code&gt;rms&lt;/code&gt; package's &lt;code&gt;ols&lt;/code&gt; and &lt;code&gt;lrm&lt;/code&gt; functions for quadratic (ridge) L2 penalty) or using random effects.  You can get predictions for individual levels easily using penalized maximum likelihood estimation, or by using BLUPS in the mixed effects modeling context.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-21T11:49:03.493" Id="67969" LastActivityDate="2013-08-21T11:49:03.493" OwnerUserId="4253" ParentId="67938" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;A reasonably efficient algorithm exists,&lt;/strong&gt; deriving from these observations:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;A random shuffle of $N+k$ cards can be generated by randomly shuffling $N$ cards and then &lt;strong&gt;randomly interspersing&lt;/strong&gt; the remaining $k$ cards within them.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;By shuffling only the aces, and then (applying the first observation) interspersing the twos, then the threes, and so on, &lt;strong&gt;this problem can be viewed as a chain of thirteen steps.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We need to keep track of more than the value of the card we are seeking. When doing this, &lt;em&gt;we do not need to account for the position of the mark relative to all the cards,&lt;/em&gt; but only its position relative to cards of equal or smaller value.  Let's imagine placing a mark on the first ace, and then marking the first two found after it, and so on.  (If at any stage the deck runs out without displaying the card we are currently seeking, we will leave all cards unmarked.)  Let the &quot;place&quot; of each mark (when it exists) be the number of cards of equal or lower value that were dealt when the mark was made (including the marked card itself).  &lt;strong&gt;The places contain all the essential information.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The place after the $i^\text{th}$ mark is made is a random number.  For a given deck, the sequence of these places forms a stochastic process.  It in fact is a Markov process (with variable transition matrix).  &lt;strong&gt;An exact answer can therefore be calculated from twelve matrix multiplications.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Using these ideas, this machine obtains a value of $5.8325885529019965$ (computing in double precision floating point) in $1/9$ second.  This approximation of the exact value $\frac{1982600579265894785026945331968939023522542569}{339917784579447928182134345929899510000000000}$ is accurate to all digits shown.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Generating random shuffles of a deck&lt;/h3&gt;&#10;&#10;&lt;p&gt;It is actually clearer conceptually and no more complicated mathematically to consider a &quot;deck&quot; (aka &lt;em&gt;multiset&lt;/em&gt;) of $N = k_1+k_2+\cdots+k_m$ cards of which there are $k_1$ of the lowest denomination, $k_2$ of the next lowest, and so on.  (The question as asked concerns the deck determined by the $13$-vector $(4,4,\ldots,4)$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;A &quot;random shuffle&quot; of $N$ cards is one permutation taken uniformly and randomly from the $N! = N\times(N-1)\times\cdots\times 2\times 1$ permutations of the $N$ cards.  These shuffles fall into groups of equivalent configurations because permuting the $k_1$ &quot;aces&quot; among themselves changes nothing, permuting the $k_2$ &quot;twos&quot; among themselves also changes nothing, and so on.  Therefore each group of permutations that look identical when the suits of the cards are ignored contains $k_1!\times k_2!\times \cdots \times k_m!$ permutations.  These groups, whose number therefore is given by the &lt;em&gt;multinomial coefficient&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\binom{N}{k_1,k_2,\ldots,k_m} = \frac{N!}{k_1!k_2!\cdots k_m!},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;are called &quot;combinations&quot; of the deck.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is another way to count the combinations.  The first $k_1$ cards can form only $k_1!/k_1! = 1$ combination.  They leave $k_1+1$ &quot;slots&quot; between and around them in which the next $k_2$ cards can be placed.  We could indicate this with a diagram where &quot;$*$&quot; designates one of the $k_1$ cards and &quot;$\_$&quot; designates a slot that can hold between $0$ and $k_2$ additional cards:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\underbrace{\_*\_*\_\cdots\_*\_}_{k_1\text{ stars}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;When $k_2$ additional cards are interspersed, the pattern of stars and new cards partitions the $k_1+k_2$ cards into two subsets.  The number of distinct such subsets is $\binom{k_1+k_2}{k_1,k_2} = \frac{(k_1+k_2)!}{k_1!k_2!}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Repeating this procedure with $k_3$ &quot;threes,&quot; we find there are $\binom{(k_1+k_2)+k_3}{k_1+k_2,k_3}= \frac{(k_1+k_2+k_3)!}{(k_1+k_2)!k_3!}$ ways to intersperse them among the first $k_1+k_2$ cards.  Therefore the total number of distinct ways to arrange the first $k_1+k_2+k_3$ cards in this manner equals &lt;/p&gt;&#10;&#10;&lt;p&gt;$$1\times\frac{(k_1+k_2)!}{k_1!k_2!}\times\frac{(k_1+k_2+k_3)!}{(k_1+k_2)!k_3!} = \frac{(k_1+k_2+k_3)!}{k_1!k_2!k_3!}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;After finishing the last $k_n$ cards and continuing to multiply these telescoping fractions, we find that the number of distinct combinations obtained equals the total number of combinations as previously counted, $\binom{N}{k_1,k_2,\ldots,k_m}$.  Therefore we have overlooked no combinations.  That means &lt;em&gt;this sequential process of shuffling the cards correctly captures the probabilities of each combination,&lt;/em&gt; assuming that at each stage each possible distinct way of interspersing the new cards among the old is taken with uniformly equal probability.&lt;/p&gt;&#10;&#10;&lt;h3&gt;The place process&lt;/h3&gt;&#10;&#10;&lt;p&gt;Initially, there are $k_1$ aces and obviously the very first is marked.  At later stages there are $n = k_1 + k_2 + \cdots + k_{j-1}$ cards, the place (if a marked card exists) equals $p$ (some value from $1$ through $n$), and we are about to intersperse $k=k_j$ cards around them.  We can visualize this with a diagram like&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\underbrace{\_*\_*\_\cdots\_*\_}_{p-1\text{ stars}}\odot\underbrace{\_*\_\cdots\_*\_}_{n-p\text{ stars}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where &quot;$\odot$&quot; designates the currently marked symbol. &lt;em&gt;Conditional&lt;/em&gt; on this value of the place $p$, we wish to find the probability that the next place will equal $q$ (some value from $1$ through $n+k$; by the rules of the game, the next place must come after $p$, whence $q\ge p+1$).  &lt;strong&gt;If we can find how many ways there are to intersperse the $k$ new cards in the blanks so that the next place equals $q$,&lt;/strong&gt; then we can divide by the total number of ways to intersperse these cards (equal to $\binom{n+k}{k}$, as we have seen) to obtain the &lt;em&gt;transition probability&lt;/em&gt; that the place changes from $p$ to $q$.  (There will also be a transition probability for the place to disappear altogether when none of the new cards follow the marked card, but there is no need to compute this explicitly.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's update the diagram to reflect this situation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\underbrace{\_*\_*\_\cdots\_*\_}_{p-1\text{ stars}}\odot\underbrace{**\cdots*}_{s\text{ stars}}\ \vert\ \underbrace{\_*\_\cdots\_*\_}_{n-p-s\text{ stars}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The vertical bar &quot;$\vert$&quot; shows where the first new card occurs after the marked card: &lt;em&gt;no&lt;/em&gt; new cards may therefore appear between the $\odot$ and the $\vert$ (and therefore no slots are shown in that interval).  We do not know how many stars there are in this interval, so I have just called it $s$ (which may be zero)  The unknown $s$ will disappear once we find the relationship between it and $q$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose, then, we intersperse $j$ new cards around the stars before the $\odot$ and then--&lt;em&gt;independently of that&lt;/em&gt;--we intersperse the remaining $k-j-1$ new cards around the stars after the $\vert$.  There are &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\tau_{n,k}(s,p) = \binom{(p-1)+j}{j}\binom{(n-p-s) + (k-j)-1}{k-j-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;ways to do this.  Notice, though--this is the trickiest part of the analysis--that the place of $\vert$ equals $p+s+j+1$ because &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There are $p$ &quot;old&quot; cards at or before the mark.&lt;/li&gt;&#10;&lt;li&gt;There are $s$ old cards after the mark but before $\vert$.&lt;/li&gt;&#10;&lt;li&gt;There are $j$ new cards before the mark.&lt;/li&gt;&#10;&lt;li&gt;There is the new card represented by $\vert$ itself.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thus, $\tau_{n,k}(s,p)$ gives us information about the transition from place $p$ to place $q=p+s+j+1$.  When we track this information carefully for all possible values of $s$, and &lt;em&gt;sum&lt;/em&gt; over all these (disjoint) possibilities, we obtain the conditional probability of place $q$ following place $p$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\Pr}_{n,k}(q|p) = \left(\sum_j \binom{p-1+j}{j}\binom{n+k-q}{k-j-1}\right) / \binom{n+k}{k}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the sum starts at $j=\max(0, q-(n+1))$ and ends at $j=\min(k-1, q-(p+1)$.  (The variable length of this sum suggests there is unlikely to be a closed formula for it as a function of $n, k, q,$ and $p$, except in special cases.)&lt;/p&gt;&#10;&#10;&lt;h3&gt;The algorithm&lt;/h3&gt;&#10;&#10;&lt;p&gt;Initially there is probability $1$ that the place will be $1$ and probability $0$ it will have any other possible value in $2, 3, \ldots, k_1$.  This can be represented by a vector $p_1 = (1, 0, \ldots, 0)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;After interspersing the next $k_2$ cards, the vector $p_1$ is updated to $p_2$ by multiplying it (on the left) by the transition matrix $(\Pr_{k_1,k_2}(q|p), 1\le p\le k_1, 1\le q\le k_2)$.  This is repeated until all $k_1+k_2+\cdots+k_m$ cards have been placed.  At each stage $j$, the sum of the entries in the probability vector $p_j$ is the chance that &lt;em&gt;some&lt;/em&gt; card has been marked.  Whatever remains to make the value equal to $1$ therefore is the chance that &lt;em&gt;no&lt;/em&gt; card is left marked after step $j$.  The successive differences in these values therefore give us the probability that we could not find a card of type $j$ to mark: that is the probability distribution of the value of the card we were looking for when the deck runs out at the end of the game.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Implementation&lt;/h3&gt;&#10;&#10;&lt;p&gt;The following &lt;code&gt;R&lt;/code&gt; code implements the algorithm.  It parallels the preceding discussion.  First, calculation of the transition probabilities is performed by &lt;code&gt;t.matrix&lt;/code&gt; (without normalization with the division by $\binom{n+k}{k}$, making it easier to track the calculations when testing the code):&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;t.matrix &amp;lt;- function(q, p, n, k) {&#10;  j &amp;lt;- max(0, q-(n+1)):min(k-1, q-(p+1))&#10;  return (sum(choose(p-1+j,j) * choose(n+k-q, k-1-j))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is used by &lt;code&gt;transition&lt;/code&gt; to update $p_{j-1}$ to $p_j$.  It calculates the transition matrix and performs the multiplication.  It also takes care of computing the initial vector $p_1$ if the argument &lt;code&gt;p&lt;/code&gt; is an empty vector:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# `p` is the place distribution: p[i] is the chance the place is `i`.&#10;#&#10;transition &amp;lt;- function(p, k) {&#10;  n &amp;lt;- length(p)&#10;  if (n==0) {&#10;    q &amp;lt;- c(1, rep(0, k-1))&#10;  } else {&#10;    #&#10;    # Construct the transition matrix.&#10;    #&#10;    t.mat &amp;lt;- matrix(0, nrow=n, ncol=(n+k))&#10;    #dimnames(t.mat) &amp;lt;- list(p=1:n, q=1:(n+k))&#10;    for (i in 1:n) {&#10;      t.mat[i, ] &amp;lt;- c(rep(0, i), sapply((i+1):(n+k), &#10;                                        function(q) t.matrix(q, i, n, k)))&#10;    }&#10;    #&#10;    # Normalize and apply the transition matrix.&#10;    #&#10;    q &amp;lt;- as.vector(p %*% t.mat / choose(n+k, k))&#10;  }&#10;  names(q) &amp;lt;- 1:(n+k)&#10;  return (q)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can now easily compute the non-mark probabilities at each stage for any deck:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# `k` is an array giving the numbers of each card in order;&#10;# e.g., k = rep(4, 13) for a standard deck.&#10;#&#10;# NB: the *complements* of the p-vectors are output.&#10;#&#10;game &amp;lt;- function(k) {&#10;  p &amp;lt;- numeric(0)&#10;  q &amp;lt;- sapply(k, function(i) 1 - sum(p &amp;lt;&amp;lt;- transition(p, i)))&#10;  names(q) &amp;lt;- names(k)&#10;  return (q)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here they are for the standard deck:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;k &amp;lt;- rep(4, 13)&#10;names(k) &amp;lt;- c(&quot;A&quot;, 2:9, &quot;T&quot;, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;)&#10;(g &amp;lt;- game(k))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         A          2          3          4          5          6          7          8          9          T          J          Q          K &#10;0.00000000 0.01428571 0.09232323 0.25595013 0.46786622 0.66819134 0.81821790 0.91160622 0.96146102 0.98479430 0.99452614 0.99818922 0.99944610&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;According to the rules, if a king was marked then we would not look for any further cards: this means the value of $0.9994461$ has to be increased to $1$.  Upon doing so, the differences give the distribution of the &quot;number you will be on when the deck runs out&quot;:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; g[13] &amp;lt;- 1; diff(g)&#10;          2           3           4           5           6           7           8           9           T           J           Q           K &#10;0.014285714 0.078037518 0.163626897 0.211916093 0.200325120 0.150026562 0.093388313 0.049854807 0.023333275 0.009731843 0.003663077 0.001810781&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Compare this to the output I report in a separate answer describing a Monte-Carlo simulation: they appear to be the same, up to expected amounts of random variation.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The expected value is immediate:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; sum(diff(g) * 2:13)&#10;[1] 5.832589&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All told, this required only a dozen lines or so of executable code.  I have checked it against hand calculations for small values of $k$ (up to $3$).  Thus, if any discrepancy becomes apparent between the code and the preceding analysis of the problem, &lt;em&gt;trust the code&lt;/em&gt; (because the analysis may have typographical errors).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Remarks&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Relationships to other sequences&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When there is one of each card, the distribution is a sequence of reciprocals of whole numbers:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; 1/diff(game(rep(1,10)))&#10;[1]      2      3      8     30    144    840   5760  45360 403200&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The value at place $i$ is $i! + (i-1)!$ (starting at place $i=1$).  This is &lt;a href=&quot;http://oeis.org/A001048&quot; rel=&quot;nofollow&quot;&gt;sequence A001048&lt;/a&gt; in the Online Encyclopedia of Integer Sequences.  Accordingly, we might hope for a closed formula for the decks with constant $k_i$ (the &quot;suited&quot; decks) that would generalize this sequence, which itself has some profound meanings.  (For instance, it counts sizes of the largest conjugacy classes in permutation groups and is also related to &lt;a href=&quot;http://oeis.org/A036039&quot; rel=&quot;nofollow&quot;&gt;trinomial coefficients&lt;/a&gt;.)  (Unfortunately, the reciprocals in the generalization for $k\gt 1$ are not usually integers.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The game as a stochastic process&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Our analysis makes it clear that the initial $i$ coefficients of the vectors $p_j$, $j\ge i$, are constant.  For example, let's track the output of &lt;code&gt;game&lt;/code&gt; as it processes each group of cards:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sapply(1:13, function(i) game(rep(4,i)))&#10;&#10;[[1]]&#10;[1] 0&#10;&#10;[[2]]&#10;[1] 0.00000000 0.01428571&#10;&#10;[[3]]&#10;[1] 0.00000000 0.01428571 0.09232323&#10;&#10;[[4]]&#10;[1] 0.00000000 0.01428571 0.09232323 0.25595013&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[[13]]&#10; [1] 0.00000000 0.01428571 0.09232323 0.25595013 0.46786622 0.66819134 0.81821790 0.91160622 0.96146102 0.98479430 0.99452614 0.99818922 0.99944610&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For instance, the second value of the &lt;em&gt;final&lt;/em&gt; vector (describing the results with a full deck of 52 cards) already appeared after the second group was processed (and equals $1/\binom{8}{4}=1/70$).  Thus, &lt;em&gt;if you want information only about the marks up through the $j^\text{th}$ card value, you only have to perform the calculation for a deck of $k_1+k_2+\cdots+k_j$ cards.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the chance of not marking a card of value $j$ is getting quickly close to $1$ as $j$ increases, after $13$ types of cards in four suits we have almost reached a limiting value for the expectation.  Indeed, the limiting value is approximately $5.833355$ (computed for a deck of $4 \times 32$ cards, at which point double precision rounding error prevents going any further). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Timing&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the algorithm applied to the $m$-vector $(k,k, \ldots, k)$, we see its timing should be proportional to $k^2$ and--using a crude upper bound--not any worse than proportional to $m^3$.  By timing all calculations for $k=1$ through $7$ and $n=10$ through $30$, and analyzing only those taking relatively long times ($1/2$ second or longer), I estimate the computation time is approximately $O(k^2 n^{2.9})$, supporting this upper-bound assessment.&lt;/p&gt;&#10;&#10;&lt;p&gt;One use of these asymptotics is to project calculation times for larger problems.  For instance, seeing that the case $k=4, n=30$ takes about $1.31$ seconds, we would estimate that the (very interesting) case $k=1, n=100$ would take about $1.31(1/4)^2(100/30)^{2.9}\approx 2.7$ seconds. (It actually takes $2.87$ seconds.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-21T15:38:38.777" Id="67991" LastActivityDate="2013-08-25T15:39:33.530" LastEditDate="2013-08-25T15:39:33.530" LastEditorUserId="919" OwnerUserId="919" ParentId="67179" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working on some GWAS (Genome-Wide Association Studies) now. A genome scan was done for all the SNPs, with first 3 principal components adjusted (PCs are used for adjusting ethnicity effect) and the QQ plot looks fine (most p-values lay on the diagonal line with a few signals off the line). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I adjusted for clinical covariates (treatment, clinical stage) and rerun the genome scan, the QQ plot looks weird. Usually, when you adjust for covariates, you expect to see the p-values lay under the diagonal line (or fewer hits than unadjusted model). In my model, after adjusting for clinical covariates, the QQ plot is way off the diagonal line and almost all the SNPs are false positives. I don't know how to explain this. One important thing is that some clinical covariates are very very significant in the clinical model (the model with no SNP information), and the p-value goes to 10E-11. I have 250 samples and 750K SNPs in my dataset.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-08-21T18:08:27.903" FavoriteCount="4" Id="68005" LastActivityDate="2014-04-02T00:00:15.980" LastEditDate="2013-08-22T13:58:00.027" LastEditorUserId="88" OwnerUserId="29403" PostTypeId="1" Score="4" Tags="&lt;p-value&gt;&lt;genetics&gt;&lt;gwas&gt;" Title="Inflated p-value after adjusting for covariates, in GWAS" ViewCount="650" />
  <row AcceptedAnswerId="68022" AnswerCount="2" Body="&lt;p&gt;I am not sure what the right keywords would be for this but I would like to know if it is possible to apply functions to random variables. I think it may make sense in terms of expected value but I would appreciate any information on a more formal or rigorous approach to this concept.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example if there was a random variable X and a function f(x)=2*x and E[X] = 2 would E[f(X)] = 4 ? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-21T20:18:28.143" Id="68017" LastActivityDate="2013-08-22T03:00:58.620" OwnerUserId="29404" PostTypeId="1" Score="7" Tags="&lt;random-variable&gt;&lt;function&gt;" Title="Function of random variables" ViewCount="237" />
  <row AnswerCount="2" Body="&lt;p&gt;This is a question about the rhetoric of describing analysis done using a public data set or any other pre-existing data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here is the hypothetical situation:&lt;/strong&gt; A researcher reports that they have a hypothesis. To test this, they take a sample, n, of individuals that meets study criteria from a database of N participants. They run a single, a priori, test of this hypothesis on the sample, and report that the hypothesized group difference, or correlation, or whatever, is greater than 0, p&amp;lt;.05. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Is this exploratory or is it a fine example of hypothesis testing?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because any single reported analysis of an existing data set might be one of many interrogations, any such report should be framed as exploratory rather than hypothesis testing. In other words, presenting an analysis of an existing data set using the rhetoric of a single sample hypothesis test seems to be misleading the reader to over-interpret the results. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I can also see an argument being made for the researcher being given the benefit of the doubt as is similarly the case in all non-pre-registered studies. In other words, can we assume that the researcher did just test this one hypothesis on this single subset of the existing database? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-22T00:10:45.133" FavoriteCount="1" Id="68032" LastActivityDate="2013-08-22T05:21:44.220" OwnerUserId="16294" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;multiple-comparisons&gt;&lt;eda&gt;" Title="Is analysis of existing data always exploratory, or can it be used for hypothesis testing?" ViewCount="142" />
  
  <row AcceptedAnswerId="86118" AnswerCount="1" Body="&lt;p&gt;Here is loop of actions I do very often in my machine learning research:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Run an experiment with a certain configuration (a certain classifier with certain settings, etc.).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Look at the log file. Try to think what configuration to change in order to improve the results.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Rename the log file of the experiment, so that I remember what configuration it came from. For example, from &quot;experiment.log&quot; to &quot;experiment.bayesian.log&quot; or &quot;experiment.decisiontree.log&quot;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Change the configuration and return to step 1.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This loop has many technical actions, and many opportunities to make a mistake. For example: I renamed the log, but didn't include one of the configuration params because I thought it is not significant. Then the results proved that it is significant, and I have to go over all the logs and remane them. For example, rename &quot;experiment.bayesian.withoutboosting.log&quot;, and the new log file to &quot;experiment.bayesian.withboosting.log&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I wonder if there is a system that automates this process? &lt;/p&gt;&#10;&#10;&lt;p&gt;Preferrably, I am looking for a generic tool, that does not depend on the programming language I use for my classifiers. Maybe a shell script, that changes a configuration file, runs a program, extracts the results from the log, and keeps them in a table.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know of anything like this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-22T09:21:14.810" FavoriteCount="1" Id="68048" LastActivityDate="2014-03-03T20:44:02.253" OwnerUserId="10760" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;experiment-design&gt;&lt;software&gt;" Title="Managing the experiment loop - experiment organization tool" ViewCount="59" />
  
  <row AnswerCount="1" Body="&lt;p&gt;This is probably a really dumb question but I can't find the answer anywhere.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been given a bunch of data and asked to calculate proportions between two variables. The variables are both counts (this is social science stuff).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that there are a number of zeroes dotted althrough my dataset which of course present a problem when calculating proportions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way that I can transform the data to get rid of the zeros? I'm not going to analyze it, so I don't need it to satisfy any specific assumptions. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-08-22T11:21:19.577" Id="68053" LastActivityDate="2013-08-22T14:10:01.200" LastEditDate="2013-08-22T14:10:01.200" LastEditorUserId="22047" OwnerUserId="29423" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;count-data&gt;" Title="Alternatives to ratios of counts if denominators can be zero" ViewCount="708" />
  <row Body="&lt;p&gt;When reporting ordered or graded scales, working with simple descriptive summaries like&lt;/p&gt;&#10;&#10;&lt;p&gt;% improved $−$ % deteriorated&lt;/p&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;p&gt;% ranking as good $−$ % ranking as bad&lt;/p&gt;&#10;&#10;&lt;p&gt;is sometimes helpful. In such summaries, omitting any neutral or middle category is common (but not essential). Clearly, such a measure gives the preponderance of two tails: if everybody improved, we get $100$, and, if everybody got worse, we get $−100$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In political terms, an election could be imagined in which there are votes “for” and “against” from these two categories, and from that context, these measures may be described as plurality measures. (Is there a better general term, or any term that is standard in some field, for particular examples of such measures?) Whatever the terminology, such measures are discussed in Tukey (1977, pp.498–502), Zeisel (1985, pp.75–77), and Wilkinson (2005, pp.57–58).&lt;/p&gt;&#10;&#10;&lt;p&gt;Naturally, the percent formulation is not compulsory, and you could just as easily — in fact, a little more easily — work with proportions or fractions with results ranging from $1$ to $−1$. In either case, using a difference is natural whenever thinking is in terms of the percent or proportion scale being used. Also, a ratio such as&lt;/p&gt;&#10;&#10;&lt;p&gt;% ranking as good / % ranking as bad&lt;/p&gt;&#10;&#10;&lt;p&gt;may be less desirable with small denominators. Either the result may be unstable, or, if the denominators are ever 0, it may be indeterminate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let us illustrate both points with the idea of looking at gender roles across a set of activities, and&lt;/p&gt;&#10;&#10;&lt;p&gt;% who are female $−$ % who are male&lt;/p&gt;&#10;&#10;&lt;p&gt;as a way of summarizing data on who does what. If, in a village, 21 women and zero men do laundry, four men and 11 women fetch water, and 14 men and zero women take care of cows, then neither the male–female ratio nor the female–male ratio can be used throughout to summarize the balance of the sexes. Whenever zero is a denominator, the ratio is indeterminate. Even if no zeros are present, we should worry about sensitivity. However, the measure above is one which is always practical. &lt;/p&gt;&#10;&#10;&lt;p&gt;All that said, it should be evident that the raw frequencies remain important and should be reported, or at least easily accessible. &quot;1/3 of the cats showed improvement, 1/3 deterioration, but the other cat ran away&quot; has an equivalent here too.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The details should be simple in your favourite software, but for Stata details see Cox (2007) on which this is based. &lt;/p&gt;&#10;&#10;&lt;p&gt;Cox, N.J. 2007. How do I calculate measures such as percent improved minus percent deteriorated? &lt;a href=&quot;http://www.stata.com/support/faqs/data-management/plurality-measures/&quot; rel=&quot;nofollow&quot;&gt;http://www.stata.com/support/faqs/data-management/plurality-measures/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Tukey, J. W. 1977.&#10;&lt;em&gt;Exploratory Data Analysis.&lt;/em&gt; Reading, MA: Addison–Wesley.&lt;/p&gt;&#10;&#10;&lt;p&gt;Wilkinson, L. 2005.&#10;&lt;em&gt;The Grammar of Graphics.&lt;/em&gt; 2nd ed. New York: Springer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Zeisel, H. 1985.&#10;&lt;em&gt;Say It with Figures.&lt;/em&gt; 6th ed. New York: Harper &amp;amp; Row.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-22T11:57:27.660" Id="68056" LastActivityDate="2013-08-22T12:38:26.243" LastEditDate="2013-08-22T12:38:26.243" LastEditorUserId="22047" OwnerUserId="22047" ParentId="68053" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working with biological sequence data where each position in the sequence has an associated continuous value.  I'm ignoring the sequence content so the data is very similar to a time series with measurements at discrete timepoints -- all values are equally spaced.  I would like to be able to detect whether high values tend to cluster together (occur in runs) so I applied the Wald-Wolfowitz runs test for non-random placement of values &gt;1.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some issues with that approach:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Wald-Wolfowitz works on binary data so I have to binarise the continuous values I have (everything larger than 1 becomes 1 and the rest is 0).  Ideally I would like to be able to detect features such as runs of similar values (let's say 10 values of 0.5 in a row) as well.  I would imagine there are some methods that would operate on continuous values (e.g. based on autocorrelation) but couldn't find any.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;While I get a measure of clustering (the test p-value), I don't know which parts are actually clustered or how many clusters there are. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I would also like to extend this approach to 3D (mapping of sites on the protein structure) and  the test doesn't support multiple dimensions, either.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I was wondering if there are more sophisticated statistical approaches that I could apply?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-08-22T13:11:07.127" FavoriteCount="1" Id="68064" LastActivityDate="2013-08-28T17:20:21.380" LastEditDate="2013-08-28T17:20:21.380" LastEditorUserId="9811" OwnerUserId="9811" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;clustering&gt;" Title="Detecting statistically significant clustering of continuous values" ViewCount="263" />
  <row AcceptedAnswerId="76413" AnswerCount="1" Body="&lt;p&gt;I have a dataset with about 35,000 individuals described by around 15 categorical variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to study the independence / correlation between these 15 categorical variables. My first idea was to, for each pair of variables, create a contingency table and calculate the $\chi^2$. Then, study the overall difference in the statistic. However, because the population is so large, $\chi^2$ is always significant. I'm having difficulty interpreting and comparing the results for each pair of variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, I can summarize my question as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;For large datasets, when I know $\chi^2$ will almost always be significant, is there an alternative test that will give more reasonable results?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have two ideas, as well&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I was thinking of taking many bootstrap samples of say 1K individuals. On each sample calculate the correlation, then average over all the bootstrap samples. The average should be a good representation of the overall sample, but I &lt;em&gt;feel&lt;/em&gt; like I'm somehow cheating.&lt;/li&gt;&#10;&lt;li&gt;Can I simply compare the &lt;em&gt;magnitudes&lt;/em&gt; of the $\chi^2$ test between the different pairs of variables? The degrees of freedom are different (the categories are of different sizes), which leads me to think this won't make sense.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2013-08-22T14:20:37.097" FavoriteCount="0" Id="68071" LastActivityDate="2013-11-13T18:11:47.577" LastEditDate="2013-11-13T18:11:47.577" LastEditorUserId="29433" OwnerUserId="29433" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;correlation&gt;&lt;categorical-data&gt;&lt;chi-squared&gt;&lt;large-data&gt;" Title="Correlation or independence on contingency table for large N" ViewCount="323" />
  
  
  <row Body="&lt;p&gt;The calculation you refer to uses the normal distribution to approximate the binomial distribution. They integrate over the range of the binomial distribution - that's where the upper limit of the integral comes from. For a truly normal distribution one would indeed integrate to infinity.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-22T15:41:56.520" Id="68084" LastActivityDate="2013-08-22T15:41:56.520" OwnerUserId="279" ParentId="68082" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Yudi Pawitan writes in his book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0199671222&quot;&gt;In All Likelihood&lt;/a&gt; that the second derivative of the log-likelihood evaluated at the maximum likelihood estimates (MLE) is the &lt;em&gt;observed Fisher information&lt;/em&gt; (see also &lt;a href=&quot;http://www.public.iastate.edu/~mervyn/stat580/Notes/s09mle.pdf&quot;&gt;this document&lt;/a&gt;, page 2). This is exactly what most optimization algorithms like &lt;code&gt;optim&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt; return: the Hessian evaluated at the MLE. When the &lt;em&gt;negative&lt;/em&gt; log-likelihood is minimized, the negative Hessian is returned. As you correctly point out, the estimated standard errors of the MLE are the square roots of the diagonal elements of the inverse of the observed Fisher information matrix. In other words: The square roots of the diagonal elements of the inverse of the Hessian (or the negative Hessian) are the estimated standard errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The negative Hessian evaluated at the MLE is the same as the observed Fisher information matrix evaluated at the MLE.&lt;/li&gt;&#10;&lt;li&gt;Regarding your main question: &lt;strong&gt;No, it's not correct&lt;/strong&gt; that the&#10;observed Fisher information can be found by inverting the (negative)&#10;Hessian.&lt;/li&gt;&#10;&lt;li&gt;Regarding your second question: The inverse of the (negative) Hessian is an estimator of the asymptotic covariance matrix. Hence, the square roots of the diagonal elements of covariance matrix are estimators of the standard errors.&lt;/li&gt;&#10;&lt;li&gt;I think the second document you link to got it wrong.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Formally&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $l(\theta)$ be a log-likelihood function. The &lt;em&gt;Fisher information matrix&lt;/em&gt; $\mathbf{I}(\theta)$ is a symmetrical $(p\times p)$ matrix containing the entries:&#10;$$&#10;\mathbf{I}(\theta)=-\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}l(\theta),~~~~ 1\leq i, j\leq p&#10;$$&#10;The &lt;em&gt;observed Fisher information matrix&lt;/em&gt; is simply $I(\hat{\theta}_{\mathrm{ML}})$, the information matrix evaluated at the maximum likelihood estimates (MLE). The Hessian is defined as:&#10;$$&#10;\mathbf{H}(\theta)=\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}l(\theta),~~~~ 1\leq i, j\leq p&#10;$$&#10;It is nothing else but the matrix of second derivatives of the likelihood function with respect to the parameters. It follows that if you minimize the &lt;em&gt;negative&lt;/em&gt; log-likelihood, the returned Hessian is the equivalent of the observed Fisher information matrix whereas in the case that you maximize the log-likelihood, then the &lt;em&gt;negative&lt;/em&gt; Hessian is the observed information matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, the inverse of the Fisher information matrix is an estimator of the asymptotic covariance matrix:&#10;$$&#10;\mathrm{Var}(\hat{\theta}_{\mathrm{ML}})=[\mathbf{I}(\hat{\theta}_{\mathrm{ML}})]^{-1}&#10;$$&#10;The standard errors are then the square roots of the diagonal elements of the covariance matrix.&#10;For the asymptotic distribution of a maximum likelihood parameter, we can write&#10;$$&#10;\hat{\theta}_{\mathrm{ML}}\stackrel{a}{\sim}\mathcal{N}\left(\theta_{0}, [\mathbf{I}(\hat{\theta}_{\mathrm{ML}})]^{-1}\right)&#10;$$&#10;where $\theta_{0}$ denotes the true parameter value. Hence, the estimated standard error of the maximum likelihood estimates is given by:&#10;$$&#10;\mathrm{SE}(\hat{\theta}_{\mathrm{ML}})=\frac{1}{\sqrt{\mathbf{I}(\hat{\theta}_{\mathrm{ML}})}}&#10;$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-22T16:46:51.303" Id="68095" LastActivityDate="2014-08-07T21:29:29.040" LastEditDate="2014-08-07T21:29:29.040" LastEditorUserId="21054" OwnerUserId="21054" ParentId="68080" PostTypeId="2" Score="13" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'd like to pose the following question which for some reason is proving to be unclear to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume we have the Normal distribution, mean 0, sd 1. Let's say we take 1000 samples from it; call them $X_1, X_2, X_3,..., X_{1000}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, draw a random variable $Y$ (from the same Normal distribution mean 0, sd 1), and compute its percentile score with respect to the entire set of values in $X$. Is this percentile score uniformly distributed between 0 and 1?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-08-22T19:20:15.353" Id="68111" LastActivityDate="2013-08-22T20:02:57.787" LastEditDate="2013-08-22T20:02:57.787" LastEditorUserId="29454" OwnerUserId="29454" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;quantiles&gt;&lt;uniform&gt;" Title="Should percentiles of one set of samples from a distribution wrt another set be uniformly distributed?" ViewCount="89" />
  
  <row Body="&lt;p&gt;It seems that $P(z_{t},\ldots,z_1 ; A)$ should be read as &quot;the probability of $z_{t},\ldots,z_1$, for a given value of parameters $A$&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a difference in notation because $A$ is not a random variable but a fixed parameter. If $A$ were a random variable, you'd use $P(X|A)$ instead of $P(X; A)$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-23T09:27:25.083" Id="68153" LastActivityDate="2013-08-23T09:27:25.083" OwnerUserId="5795" ParentId="68150" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have this confusion related to implementing linear regression with normalization. Let's say I have a training set &lt;code&gt;trainX&lt;/code&gt; and &lt;code&gt;trainY&lt;/code&gt;, and test set &lt;code&gt;testX&lt;/code&gt; and &lt;code&gt;testY&lt;/code&gt;. For the training set I take the mean and standard deviation of &lt;code&gt;trainX&lt;/code&gt;, use it to transform the &lt;code&gt;trainX&lt;/code&gt; data to have a  mean center and unit standard deviation. I do the same for &lt;code&gt;trainY&lt;/code&gt;. Now I run a ridge regression for training. For cross validation, I use 10 fold and then get some coefficients optimal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now when I use these coefficients on the test sets &lt;code&gt;testX&lt;/code&gt; and &lt;code&gt;testY&lt;/code&gt;, I need to mean center and give unit standard deviation to both &lt;code&gt;testX&lt;/code&gt; and &lt;code&gt;testY&lt;/code&gt; using the mean and standard deviation I got from training data sets. I apply the coefficients to predict &lt;code&gt;Y&lt;/code&gt;. To these predicted &lt;code&gt;Y&lt;/code&gt; values, do I again need to add the mean and standard deviation used before to get the actual &lt;code&gt;Y&lt;/code&gt; values? Is this the way to go?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-23T21:25:38.033" Id="68202" LastActivityDate="2013-08-24T06:55:28.857" LastEditDate="2013-08-24T06:55:28.857" LastEditorUserId="22047" OwnerUserId="12329" PostTypeId="1" Score="1" Tags="&lt;cross-validation&gt;&lt;standardization&gt;&lt;ridge-regression&gt;" Title="Implementing linear regression with standardization" ViewCount="137" />
  
  
  
  
  <row AcceptedAnswerId="68263" AnswerCount="2" Body="&lt;p&gt;I am running an A/B test, and the following are samples of how many questions a user has answered. I am trying to figure out which test is better (A or B), and how confident we are that it is better.  For example, in Sample A, first user answered 4 questions, next two users answered 5 questions.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I know how to calculate statistical significance for A/B tests, but am not sure how to do it for a range of numbers.  I am trying to get users to answers the maximum questions possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry for my incorrect lingo but i'm not a stat guy, so i'm not sure what you refer to these terms.  Essentially, I would like to figure out it if my A/B test, A sample or B sample yields a higher number.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sample A :&#10;4, 5, 5, 9, 11, 14, 15, 15, 16, 19, 27, 30, 31, 32, 58, 65, 67, 79,&#10;98, 99, 100, 103, 106, 204, 232, 341, 354, 359, 360&#10;Med: 58&#10;Avg: 98&#10;&#10;Sample B:&#10;1, 3, 4, 4, 4, 5, 9, 11, 12, 15, 19, 28, 37, 48, 50, 54, 59, 72, 74,&#10;78, 80, 81, 89, 91, 99, 101, 103, 103, 104, 120, 121, 174, 203&#10;Med: 59&#10;Avg: 62&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2013-08-24T16:07:38.517" Id="68247" LastActivityDate="2014-04-10T04:50:23.883" LastEditDate="2014-04-10T04:50:23.883" LastEditorUserId="9007" OwnerUserId="11799" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;" Title="How would I calculate statistical significance for a range?" ViewCount="177" />
  <row Body="&lt;p&gt;I believe the ellipse drawn is just done by hand, but it is supposed to illustrate something like the &lt;a href=&quot;http://www.cs.princeton.edu/courses/archive/spr10/cos226/demo/ah/ConvexHull.html&quot; rel=&quot;nofollow&quot;&gt;convex hull&lt;/a&gt; (or &lt;a href=&quot;http://mathworld.wolfram.com/ConvexHull.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) of the data support.  There are multiple ways of deciding where exactly the data support lies.  For instance, you could just find the &quot;outermost&quot; points in your data cloud and connect them with lines, or you could have an algorithm that attempts to &quot;smoothly&quot; join the outer points, ignoring any points that the more naive algorithm would see as &quot;outer&quot; but which are too far interior to produce a nice smoothed shape.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Example in R&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Example from ?mvrnorm&#10;library(MASS)&#10;Sigma &amp;lt;- matrix(c(10,3,3,2),2,2)&#10;Sigma&#10;set.seed(1)&#10;dat &amp;lt;- mvrnorm(n=1000, rep(0, 2), Sigma)&#10;plot( dat )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/k4EYA.png&quot; alt=&quot;data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Find convex hull and plot: http://stats.stackexchange.com/questions/11919/convex-hull-in-r#11921&#10;library(grDevices) # load grDevices package&#10;con.hull.pos &amp;lt;- chull(dat)&#10;con.hull &amp;lt;- rbind(dat[con.hull.pos,],dat[con.hull.pos[1],])&#10;plot(dat) # plot data&#10;lines(con.hull,col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZuZ1F.png&quot; alt=&quot;data with hull&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea here is that the underlying function outside of where you have data could continue as you expect--or it could verge off elsewhere.  So at $(-5,4)$, for example, the relationship between $(X1,X2)$ and $f(X1,X2)$ might not be what you're expecting, and you're extrapolating despite being within the range of both X1 and X2.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-24T16:18:58.307" Id="68248" LastActivityDate="2013-08-24T19:03:50.940" LastEditDate="2013-08-24T19:03:50.940" LastEditorUserId="3488" OwnerUserId="3488" ParentId="68245" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about the package &lt;strong&gt;leaps&lt;/strong&gt; which I am using for model selection. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to compare 4 different selection methods: forward, backward, stepwise and best subset. I used the code below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(leaps)&#10;forward &amp;lt;- regsubsets(Response ~.,data = mydata, method = &quot;forward&quot;, nbest=1)  &#10;backward &amp;lt;- regsubsets(Response ~.,data = mydata, method = &quot;backward&quot;, nbest=1)&#10;stepwise &amp;lt;- regsubsets(Response ~., data = mydata, method = &quot;seqrep&quot;, nbest=1)&#10;best subset &amp;lt;- regsubsets(Response ~.,data = mydata, method = &quot;exhaustive&quot;, nbest=1)&#10;# adjusted R2&#10;opt = par (mfrow =c(2,2))&#10;plot(forward, scale = &quot;adjr2&quot;, main = &quot;Forward Selection&quot;)&#10;plot(backward, scale = &quot;adjr2&quot;, main = &quot;Backward Selection&quot;)&#10;plot(stepwise, scale = &quot;adjr2&quot;, main = &quot;Stepwise selection&quot;)&#10;plot(best subset, scale = &quot;adjr2&quot;, main = &quot;Best subset selection&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Using these commands I obtained figures below: &#10;&lt;img src=&quot;http://i.stack.imgur.com/KlDWC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering why figure A and D are similar to each other (and also figure B and C). I would expect different algorithms to select models in a different way. For instance models selected with forward selection method should be chosen based on the significance level/ AIC value. On the other hand models selected with best subset selection method should be chosen based on the sample statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am also wondering why forward selection does not choose one variable at the time adding it to the existing model? &lt;/p&gt;&#10;&#10;&lt;p&gt;Also Fig B shows that backward selection starts with eight variables in the model. Why it does not start with all the variables and excludes one at the time?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very grateful for these clarifications. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-24T16:40:39.457" Id="68250" LastActivityDate="2013-08-24T16:52:20.063" LastEditDate="2013-08-24T16:52:20.063" LastEditorUserId="25814" OwnerUserId="25814" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;multiple-regression&gt;&lt;multivariate-analysis&gt;&lt;programming&gt;" Title="Selection of regressors" ViewCount="246" />
  
  <row Body="&lt;p&gt;The last review of SCA and AUTOBOX was done in 1995 (see &lt;a href=&quot;http://www.autobox.com/cms/index.php/products/review&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). We had approached Prof. Len Tashman the editor of Foresight for a head-to-head review of AUTOBOX vs SAS but SAS's representative replied in the negative in July 2010: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;ARIMA modeling is an interesting subject, but not one I would want to&#10;  focus on, or one I think of as central for modern forecasting methods.&#10;  Even with the scope of the comparison explicitly limited to ARIMA&#10;  functionality, there is still an imbalance in an Autobox versus SAS&#10;  comparison. Autobox is a niche product that is all about automatic&#10;  ARIMA modeling. On the other hand, SAS/ETS offers a broad range of&#10;  tools for econometrics and time series modeling—our PROC ARIMA is just&#10;  one of twenty-seven ETS procedures. For forecasting, our primary&#10;  offering is SAS Forecast Server, not SAS/ETS. In turn, SAS/ETS and&#10;  Forecast Server are just two of the dozens of offerings in the SAS&#10;  product line. So even if the article is specifically limited to ARIMA,&#10;  I’m afraid a “SAS” versus Autobox comparison would still come across a&#10;  little like comparing a Notepad alternative to “Microsoft”. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;To correct the above: AUTOBOX is and always has been about automatic and non-automatic modelling of both ARIMA and Transfer Functions. In this way experts can use their expertise much like SCA and SAS or at their option use the expert heuristics within AUTOBOX as a productivity aid.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a fairly recent review of AUTOBOX in 2010 (see &lt;a href=&quot;http://viewer.zmags.com/publication/9d4dc62a#/9d4dc62a/66&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) that was very thorough.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am one of the developers of AUTOBOX. In case you need more help please feel free to contact me and/or pose additional questions.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-24T21:20:44.520" Id="68258" LastActivityDate="2013-08-24T22:47:36.913" LastEditDate="2013-08-24T22:47:36.913" LastEditorUserId="3382" OwnerUserId="3382" ParentId="68253" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;Let's say you wanted to map the intersections between n different online communities. For example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;80% of CrossValidated users also have a Stack Overflow account&lt;/li&gt;&#10;&lt;li&gt;50% of Hacker News users also have a Reddit account&lt;/li&gt;&#10;&lt;li&gt;etc.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What would be a good way to visualize this? Any examples of existing diagrams?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've seen &lt;a href=&quot;http://stats.stackexchange.com/questions/6234/visualizing-the-intersections-of-many-sets&quot;&gt;this question&lt;/a&gt;, which provides an answer. But I'm curious to know if there are other visualization techniques that would specifically lend themselves to comparing the relationships between communities. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-25T03:40:36.287" Id="68269" LastActivityDate="2013-08-25T03:40:36.287" OwnerUserId="29520" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;" Title="Visualizing intersections between online communities" ViewCount="38" />
  <row Body="&lt;p&gt;Is there only 1 trend line ? Probably not . You have outlying points below the &quot;visual trend line at the bottom&quot; . How will these be &quot;ignored&quot; so as to capture the dominant floor trend line that the eye sees and not be influenced by them. To detect tehm and reduce their influence one would want to simultaneously detect BOTH trend line(s) and pulses that are inconsistent with the trend(s). If you can reduce your xy observations as you said and then post the reduced set , I might take a shot at this using the only piece of commercially avaialable software that I know that deals with trend detection while considering ARIMA structure and pulses. As far as I know nothing free is available and of course the heuristics to duplicate the human eye are not disclosable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-25T12:23:27.580" Id="68282" LastActivityDate="2013-08-25T12:23:27.580" OwnerUserId="3382" ParentId="68279" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Loading matrix is the adjustment matrix ($α$  matrix). The elements of $α$ determine the speed of adjustment to the long-run equilibrium. &lt;/p&gt;&#10;&#10;&lt;p&gt;Please see p.4 of this &lt;a href=&quot;http://www.uh.edu/~bsorense/coint.pdf&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; to understand the relationship between adjustment matrix and cointegrating vector.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-08-25T15:59:30.417" Id="68293" LastActivityDate="2013-08-25T18:52:07.343" LastEditDate="2013-08-25T18:52:07.343" LastEditorUserId="14860" OwnerUserId="14860" ParentId="68264" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;Say, for example, we have a $Y \sim \mathrm{Uniform}(0, 0.25)$, then $P(Y = y) = 4$ for all $Y \in [0, 0.25]$.&#10;My question is: what is the statistical significance/meaning of the $4$?&lt;/p&gt;&#10;" ClosedDate="2013-08-25T19:13:01.840" CommentCount="2" CreationDate="2013-08-25T18:28:53.800" Id="68299" LastActivityDate="2013-08-25T18:57:57.623" LastEditDate="2013-08-25T18:57:57.623" LastEditorUserId="21054" OwnerUserId="29531" PostTypeId="1" Score="2" Tags="&lt;pdf&gt;" Title="What is the significance of the PDF's value at a certain point?" ViewCount="59" />
  
  
  
  
  <row Body="&lt;p&gt;I believe canocial correlation is a technique that can be used to approach this kind of problem. Canonical correlation is a general multivariate form that measures the strength of the relationship between two SETS of variables - one set of predictor variables and a set of response variables. The objective of canonical correlation is to find a simultaneous relationship between linear combinations of the original variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, see:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;help(cancor)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'll also note that I there are multivariate normality assumptions associated with the technique require testing. Structural Equation Modeling and GLM are also options. For an overview of canonical correlation, see:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/65692/how-to-visualize-what-canonical-correlation-analysis-does-in-comparison-to-what&quot;&gt;How to visualize what canonical correlation analysis does (in comparison to what principal component analysis does)?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-26T07:08:10.127" Id="68322" LastActivityDate="2013-08-26T07:08:10.127" OwnerUserId="24865" ParentId="68313" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The TF part of the TF-IDF approach is supposed to use &lt;strong&gt;relative term frequencies&lt;/strong&gt;, which equals normalizing the data by document length.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is probably as good as you can do to counter this problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Topic clustering isn't trivial. There is much more to language than topics. So the clustering algorithm may well end up clustering text by e.g. ethnicity or gender or educational status of the author: these may all be different reasons for people using different words and language.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without supervision, there is by no means a guarantee that you will have &lt;em&gt;topic&lt;/em&gt; clusters.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-26T07:32:59.197" Id="68324" LastActivityDate="2013-08-26T07:32:59.197" OwnerUserId="7828" ParentId="68289" PostTypeId="2" Score="1" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Ties are numbers that are repeating in sample. You may try &lt;code&gt;anyDuplicated(p)&lt;/code&gt; to find whether there are ties. There is uncertainty with quantiles when ties are present. However, if their number is not high, there is no reason to worry about them.&lt;/li&gt;&#10;&lt;li&gt;In two-sample KS test you are testing whether two empirical distributions come from the same theoretical distribution. In one-sample KS test you are testing whether given empirical distribution come from given theoretical. Here is an example. Let we have one empirical distribution  positively skewed, and second - the same but negatively skewed. Two sample KS test would search whether both samples can be drawn from the third- &lt;em&gt;theoretical&lt;/em&gt; distribution, which has zero skewness in this example. And p-value can be &gt;0.05. While performing one-sample test will search whether first distribution can be drawn from the second, which unlikely can be. And thus p falls below 0.05.&lt;/li&gt;&#10;&lt;li&gt;Because you tested not whether the sample can be drawn from uniform distribution, but from distribution p that is only similar to uniform.&lt;/li&gt;&#10;&lt;li&gt;If &lt;code&gt;length(duplicated(p))&lt;/code&gt; is small being compared with &lt;code&gt;length(p)&lt;/code&gt; than you should not worry about it.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;And finally: if you want to test whether some empirical distribution comes from uniform, you should use one-sample KS test in this notation:&#10;    &lt;code&gt;ks.test(p,&quot;punif&quot;)&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-26T12:54:18.827" Id="68340" LastActivityDate="2013-08-26T13:19:42.837" LastEditDate="2013-08-26T13:19:42.837" LastEditorUserId="805" OwnerUserId="8165" ParentId="68333" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="71120" AnswerCount="1" Body="&lt;p&gt;I’m modelling tuberculosis (TB) case rates at the neighborhood level and trying to identify risk factors that are associated with higher rates. I would like to identify communities with below average case rates given their risk factor profile with the idea that these would be the best places for new case-finding. It’s a bit circular I realize to use the same units I’m identifying risk factors with to then identify areas with expected higher rates, but my thinking is that I could identify the communities which are at the lower end of the distribution for each risk factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;My plan then would be to construct a Poisson model with significant risk factors, plug in each neighborhood and calculate the ‘expected rate’. Then calculate the difference by subtracting ‘observed’ from ‘expected’ rates, and then identify the communities with the highest difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this approach legitimate? Or is this using the model in a way it isn’t meant to be used?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-26T13:18:52.020" Id="68342" LastActivityDate="2013-09-26T12:57:55.227" LastEditDate="2013-08-26T23:58:09.613" LastEditorUserId="805" OwnerUserId="29547" PostTypeId="1" Score="3" Tags="&lt;modeling&gt;&lt;poisson&gt;" Title="Identifying below-average units in a Poisson model" ViewCount="57" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;The adjusted $R^2$ is not shown when a regression with robust standard errors is calculated in Stata.&#10;This is surprising to me since the value of the $R^2$ is unaffected in regressions with robust standard errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any statistical reason for not quoting the adjusted $R^2$ when using robust standard errors in regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthemore if I add more variables  the F test disappears.(e.g. with 9 variables it shows but not with 13), Is this for the same reason? How can we report such results and deal with this issue? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-26T15:28:07.480" Id="68354" LastActivityDate="2013-08-28T12:39:51.037" LastEditDate="2013-08-28T12:39:51.037" LastEditorUserId="26267" OwnerUserId="26267" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;stata&gt;&lt;robust&gt;&lt;r-squared&gt;&lt;f-test&gt;" Title="Adjusted $R^2$ &amp; F test are not shown in regression with robust standard errors in Stata" ViewCount="1317" />
  <row AnswerCount="0" Body="&lt;p&gt;A coworker and I are trying to analyze agreement between two measurement methods.  I apologize in advance for needing some extra explanation due to the fact I'm an engineer whose statistics background is mostly geared toward the relationship between signal-to-noise ratio and bit error rates, and other analysis of random processes.&lt;/p&gt;&#10;&#10;&lt;p&gt;For method comparison, it is natural to create a Bland-Altman plot (and we've done so).  However our data has some additional characteristics that Bland-Altman style analysis doesn't account for.  Furthermore, we're trying to compare our results to an earlier study that published a correlation coefficient resulting from mixed-effect analysis, unfortunately this publication didn't say whether they were reporting Pearson correlation coefficient or Intra-class correlation (maybe there are others too?).&lt;/p&gt;&#10;&#10;&lt;p&gt;The characteristics of our data set are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Multiple test subjects&lt;/li&gt;&#10;&lt;li&gt;Multiple observation instants for each test subject, sequentially ordered and equally spaced in time&lt;/li&gt;&#10;&lt;li&gt;The subjects are time varying, but receiving treatment so that the time dependent changes are not monotonic&lt;/li&gt;&#10;&lt;li&gt;At each observation instant, one measurement is made using each methods&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A statistician here at our university warned us that a simple paired analysis wasn't appropriate because there's a subject-specific effect, and pointed us to mixed-effects analysis but couldn't help further.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read several articles on mixed-effect analysis, but most of them are a comparison of groups, rather than a group of comparisons, if that makes sense.  The information I found on intra-class correlation said it treats the measurements within the class interchangeably, and that seems suspect here.&lt;/p&gt;&#10;&#10;&lt;p&gt;This article uses mixed-effect analysis for method comparison, but has repeated measurements instead of a series of time-separated measurements.  It also doesn't cover correlation coefficients on grouped data.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.degruyter.com/view/j/ijb.2008.4.1/ijb.2008.4.1.1107/ijb.2008.4.1.1107.xml&quot; rel=&quot;nofollow&quot;&gt;Statistical Models for Assessing Agreement in Method Comparison Studies with Replicate Measurements&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Here's what I've done so far, using R:&lt;/p&gt;&#10;&#10;&lt;p&gt;Load the data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;- read.table(filename, header=TRUE, sep=&quot;,&quot;);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Convert variables to cases, adding factors (is it correct to create a factor for the encounter, since the time indicators are independent for each test subject?):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;reshape&quot;)&#10;mdata &amp;lt;- within(melt(data, variable_name=&quot;method&quot;, id=c(&quot;subject&quot;, &quot;time&quot;)), {&#10;  subject &amp;lt;- factor(subject)&#10;  time &amp;lt;- factor(interaction(subject, time))&#10;  method &amp;lt;- factor(method)&#10;})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Run linear mixed-effects model.  I've chosen an autocorrelation structure for the random subject/time covariance matrices, because of the nice periodic measurements.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;lm2 &amp;lt;- lme(value ~ method, random = list( ~1|subject, ~1|time ), corr = corAR1(), data = mdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd like to know whether I've assigned the right factors to fixed and random effects.  Also, from my research I guess there should be a random effect on method*subject, but it shouldn't have AR(1) structure and I don't know how to give different structure to different random effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I did calculate a correlation coefficient, using intra-class correlation and encounter as the class to get measurements paired properly.  But I don't think this is using the subject grouping, and as I said earlier, I don't feel like treating class members interchangeably is right.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(psychometric)&#10;r2 &amp;lt;- ICC1.lme(value, time, mdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What would you do differently?  It seems like the &lt;code&gt;lmer&lt;/code&gt; function was a bit easier to describe random effect nested groups, but I didn't find a way to control the correlation structure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-26T16:51:12.580" FavoriteCount="1" Id="68363" LastActivityDate="2013-08-26T16:56:42.010" LastEditDate="2013-08-26T16:56:42.010" LastEditorUserId="29552" OwnerUserId="29552" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;mixed-effect&gt;&lt;paired-data&gt;&lt;method-comparison&gt;" Title="mixed effect model for method comparison of time series of paired measurements" ViewCount="479" />
  
  
  <row Body="&lt;p&gt;Seems you are asking for intuition.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;In a mixture, this is enough to find clusters of co-occurring words. &#10;This means that the distribution over vocabulary i.e. topics will sum to one. So it is sensible that the co-occuring words come in same topic and fewer words in same topic increases the probability of words occuring in the topic as it should sum to one.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In LDA, the Dirichlet on the topic proportions can encourage sparsity&#10;As topics are sampled from an exchangable dirichilet so all topics are sampled uniformly. But if alpha in dirichlet is less than 1 than by the definition of dirichilet distribution you can see that $\theta^{(\alpha - 1)}$ is $(fraction)^{(-ve)}$ which will be high so the peaks in the simplex will be high at the corners i.e. sparsity as the topics are sampled from that dirichilet.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-08-27T05:02:23.890" Id="68398" LastActivityDate="2014-02-11T01:28:06.963" LastEditDate="2014-02-11T01:28:06.963" LastEditorUserId="29568" OwnerUserId="29568" ParentId="25820" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The Royal Statistical Society in the UK offers the Graduate Diploma in Statistics, which is at the level of a good Bachelor's degree. A syllabus, reading list, &amp;amp; past papers are available from their &lt;a href=&quot;http://www.rss.org.uk&quot;&gt;website&lt;/a&gt;. I've known mathematicians use it to get up to speed in Statistics. Taking the exams (officially, or in the comfort of your own study) could be a useful way to measure when you're there.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2013-08-27T09:21:35.953" CreationDate="2013-08-27T09:21:35.953" Id="68409" LastActivityDate="2013-08-27T09:21:35.953" OwnerUserId="17230" ParentId="6538" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two 5-point Likert questions x and y. I want to test whether they are associated using the chi-squared test of independence.&#10;I summarized the responses into the following frequency table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      x     y &#10;1     3    39&#10;2     7    76&#10;3    38    65 &#10;4    97    30 &#10;5    75     8 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm not really sure if the table is appropriate for verifying whether there is an association between x and y. Anyway, calculating chi-squared leads to $X^2 =$ 184.7218, with $p &amp;lt; $2.2e-16.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, when I calculate the gamma correlation coefficient for x and y, where both variables are lists of all responses (containing a Likert item from {1,2,3,4,5}), I get for the gamma coefficient -0.02189, and a $p$ value of approx. 0.781.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the high $p$-value of the gamma coefficient, there is probably no association between these two variables ($p&amp;gt;$0.05). Chi-squared states the opposite. What is the right interpretation here? I'm using R for calculations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I'm not experienced with statistics, I would like to know whether this is plausible, or I am doing/understanding something wrong?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-08-27T10:17:29.337" Id="68416" LastActivityDate="2013-08-27T10:27:07.757" LastEditDate="2013-08-27T10:27:07.757" LastEditorUserId="22047" OwnerUserId="29578" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;chi-squared&gt;&lt;gamma-distribution&gt;&lt;frequency&gt;&lt;coefficient&gt;" Title="Chi-squared test vs gamma correlation coefficent" ViewCount="207" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to obtain the integral:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{{\mathbb R}^p} \frac{1}{(2\pi)^{\frac{n}{2}}\vert\Sigma\vert^{\frac{1}{2}}}\exp\left[-\frac{1}{2}({\bf y} - {\bf X}{\beta})^{\top} \Sigma^{-1}({\bf y} - {\bf X}{\beta})\right] d\beta,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where ${\bf y}$ is an $n\times 1$ vector, $\beta$ is a $p\times 1$ vector, $\Sigma$ is a symmetric positive definite matrix and ${\bf X}$ is an $n\times p$ matrix. This expression is very common in Bayesian linear regression but I do not know the trick used to separate ${\bf X}$ from $\beta$ in order to obtain the integral by using its resemblance to the multivariate normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would appreciate any guidelines on this. Thanks.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-08-27T10:21:41.133" FavoriteCount="0" Id="68417" LastActivityDate="2013-08-27T11:35:24.787" LastEditDate="2013-08-27T11:35:24.787" LastEditorUserId="29579" OwnerUserId="29579" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;bayesian&gt;&lt;normal-distribution&gt;&lt;integral&gt;" Title="Integration step in Bayesian linear regression" ViewCount="142" />
  <row AcceptedAnswerId="68435" AnswerCount="1" Body="&lt;p&gt;I am new to the &lt;code&gt;glmnet&lt;/code&gt; package, and I am still unsure of how to interpret the results. Could anyone please help me read the following trace plot?&lt;/p&gt;&#10;&#10;&lt;p&gt;The graph was obtaining by running the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(glmnet)&#10;return &amp;lt;- matrix(ret.ff.zoo[which(index(ret.ff.zoo)==beta.df$date[2]), ])&#10;data   &amp;lt;- matrix(unlist(beta.df[which(beta.df$date==beta.df$date[2]), ][ ,-1]), &#10;                 ncol=num.factors)&#10;model  &amp;lt;- cv.glmnet(data, return, standardize=TRUE)&#10;&#10;op &amp;lt;- par(mfrow=c(1, 2))&#10;plot(model$glmnet.fit, &quot;norm&quot;,   label=TRUE)&#10;plot(model$glmnet.fit, &quot;lambda&quot;, label=TRUE)&#10;par(op)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/a5n0b.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-27T13:20:51.657" FavoriteCount="2" Id="68431" LastActivityDate="2013-12-06T04:56:10.353" LastEditDate="2013-12-06T04:56:10.353" LastEditorUserId="7290" OwnerUserId="29183" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;interpretation&gt;&lt;lasso&gt;&lt;glmnet&gt;" Title="Interpretting LASSO variable trace plots" ViewCount="2506" />
  <row Body="&lt;p&gt;Suppose two predictors have a strong effect on the response but are highly correlated in the sample from which you build your model.  If you drop one from the model it won't predict well for samples from similar populations in which the predictors aren't highly correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to improve the precision of your coefficient estimates in the presence of multicollinearity you have to introduce a little bias, off-setting it by a larger reduction in variance. One way is by removing predictors entirely&amp;mdash;with LASSO, or, in the old days, stepwise methods&amp;mdash;, which is setting their coefficient estimates to zero. Another is by biasing all of the estimates a bit&amp;mdash;with ridge regression, or, in the old days, regressing on the first few principal components. A drawback of the former is that it's very unsafe if the model will be used to predict responses for predictor patterns away from those that occurred in the original sample, as predictors tend to get excluded just because they're not much use &lt;em&gt;together with other, nearly collinear, predictors&lt;/em&gt;. (Not that extrapolation is ever completely safe.) The elastic net is a mixture of the two, as @user12436 explains, &amp;amp; tends to keep groups of correlated predictors in the model.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-27T14:39:12.547" Id="68443" LastActivityDate="2013-08-28T09:38:45.283" LastEditDate="2013-08-28T09:38:45.283" LastEditorUserId="17230" OwnerUserId="17230" ParentId="68436" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I suggest using nonlinear regression to fit one model to all your data. What is the point of picking an arbitrary volume and fitting one model to volumes less than that and another model to larger volumes? Is there any reason, beyond the look of the figure, for using 5 as a sharp threshold? Do you really believe that after a particular volume threshold, the ideal curve is linear? Isn't it more likely that it approaches horizontal as volume increases, but is never quite linear?&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, the selection of analysis tool has to depend on what scientific questions you are trying to answer and  your prior knowledge of the system. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-27T16:06:34.553" Id="68461" LastActivityDate="2013-08-27T16:06:34.553" OwnerUserId="25" ParentId="68454" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;One thing to do would be to define a &quot;metric z-score&quot;, i.e. for each item score $x_i$&#10;$$&#10;z_i = \frac{ x_i - \bar{x} }{ s }&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\bar{x}$ and $s$ are the average and empirical standard deviation of the full set of item scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then your ratings could correspond to cutoffs of the $z$ scores, say &amp;lt; -3, -3 to -1, -1 to 1, 1 to 3, and &gt; 3. &lt;/p&gt;&#10;&#10;&lt;p&gt;This avoids the &quot;equal groupings&quot; problem of percentiles, and also gives a fixed interpretation to the ratings even as the underlying metric changes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-27T17:09:14.450" Id="68471" LastActivityDate="2013-08-27T17:09:14.450" OwnerUserId="29507" ParentId="68108" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There is a &lt;code&gt;sklearn&lt;/code&gt; library in python, which (among others) implements Ordinary Least Squares Linear Regression&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html&quot; rel=&quot;nofollow&quot;&gt;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample usage:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from sklearn import  linear_model&#10;&#10;#creating a regression object&#10;regr = linear_model.LinearRegression()&#10;&#10;#runnin OLS on your data, assuming that you already have arrays x and y&#10;regr.fit( x, y )&#10;&#10;#displaying coefficients matrix&#10;print regr.coef_&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-08-27T19:11:53.157" Id="68485" LastActivityDate="2013-08-27T19:11:53.157" OwnerUserId="28903" ParentId="68475" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Copied my answer across from Stack Overflow:&lt;/p&gt;&#10;&#10;&lt;p&gt;I am going to assume that you understand what is meant by minimising loss. Otherwise you should look into a Statistics course.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Prior probabilities&lt;/code&gt; are useful in a branch of probability theory and statistics known as &lt;code&gt;Bayesian Statistics&lt;/code&gt;. Simply put, the &quot;prior probabilities of each class&quot; are how likely you expect each class to be in the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;A reasonable starting point for choosing your priors is to take a frequency based prior. If you have a validation set of data (where you know the true class for each of your samples) you can simply count the number of times each class appears, and divide it by the number of samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other cases you may have more informed knowledge about how probable each class is in your data (maybe your own past experience, or from literature), in which case you should input that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-28T01:07:05.853" Id="68507" LastActivityDate="2013-08-28T01:07:05.853" OwnerUserId="29611" ParentId="68498" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;I would say that there might not be any particular or only one measure which you should take into account.&lt;/p&gt;&#10;&#10;&lt;p&gt;Last time when I did probabilistic classification I had a R package ROCR and explicit cost values for the False Positives and False Negatives. &lt;/p&gt;&#10;&#10;&lt;p&gt;I considered all cutoff-points from 0 to 1 and used many measures such as expected cost when selecting this cutoff - point. Of course I had already AUC measure for the general measure of classifying accuracy. But for me this was not the only possibility.&lt;/p&gt;&#10;&#10;&lt;p&gt;Values for the FP and FN cases must come outside your particular model, maybe these are provided by some subject matter expert?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example in customer churn analysis it might be more expensive to incorrectly infer that customer is not churning but also that it will be expensive to give a general reduction in prices for services without accurary to target these to correct groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;-Analyst&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-28T09:26:00.003" Id="68529" LastActivityDate="2013-08-28T09:26:00.003" OwnerUserId="28732" ParentId="67027" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Likelihood ratio test can be used where you test &quot;smaller&quot; model versus &quot;larger&quot;. If test statistic is significant it means that residual/deviance variation will increase if smaller model is used.&lt;/p&gt;&#10;&#10;&lt;p&gt;-Analyst &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-28T13:18:47.083" Id="68550" LastActivityDate="2013-08-28T13:18:47.083" OwnerUserId="28732" ParentId="38578" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;if you are beginner looking for predictive analytics then:&#10;1. first of all you will need a training dataset to see if the model you have constructed using the code above is a good fit. R offers variety of tools for that but I prefer Rattle for a quick glimpse.&#10;2. Once you tune the model that fits your training dataset, then you can predict the output every time you get a new values for AGE, STEROID, GPA.&lt;/p&gt;&#10;&#10;&lt;p&gt;if you are not beginner, the put some more detailed description of the problem and avoid administrators/users from removing the topic ;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-28T13:30:05.413" Id="68552" LastActivityDate="2013-08-28T13:30:05.413" OwnerUserId="21982" ParentId="68510" PostTypeId="2" Score="-1" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like a meaningful and concise explanation for what it means exactly, when someone says, &quot;We built a statistical model of all our images&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I overheard this, (and keep overhearing that phrase), but I am not sure how/what that entails exactly. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;EDIT: Context:&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem set here is as follows. From a UAV, video imagery of corn fields is collected en-masse. The problem is to be able to tell when a pest (usually a small-medium sized rodent/mammal) is moving. Usually they just took the difference between consecutive images. However in this case, many false alarms happened because leaves might sway in the wind, small changes occur that are not related to the rodent moving, etc. So to this end, they said &quot;Instead of the difference between frames method, we instead build a robust statistical model of the images we have&quot;. So what does that mean exactly?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the context. Thank you.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-08-28T14:56:10.743" FavoriteCount="1" Id="68558" LastActivityDate="2013-08-29T20:48:51.147" LastEditDate="2013-08-28T21:11:02.717" LastEditorUserId="27158" OwnerUserId="27158" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;normal-distribution&gt;&lt;modeling&gt;&lt;terminology&gt;&lt;expectation-maximization&gt;" Title="What does it mean exactly, to &quot;build a statistical model&quot; of, say, a series of images?" ViewCount="270" />
  
  <row AnswerCount="4" Body="&lt;p&gt;Consider the data below. (Dependent Variable: Human Development Index (HDI))&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           Coefficient   Std. Error   t-ratio   p-value&#10;const        -0.260523      0.129316     2.0146    0.05693  *&#10;LITRATE       0.0031374    0.000965695   3.2488    0.00384  ***&#10;LIFEXP        0.0051052    0.00268771    1.8995    0.07133  *&#10;GINI          0.00598775   0.00175813    3.4057    0.00266  ***&#10;&#10;R-squared           0.826974&#10;Adjusted R-squared  0.802256&#10;S.D. dependent var  0.053285&#10;S.E. of regression  0.023695&#10;Sum squared resid   0.011791&#10;Durbin-Watson       0.618921&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do you interpret these results? Given the independent variables are significant yet it result to a small coefficient, but the R-squared is high?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-08-28T17:34:16.070" Id="68588" LastActivityDate="2014-01-09T00:04:26.070" LastEditDate="2013-08-28T17:41:35.130" LastEditorUserId="27581" OwnerUserId="29648" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;interpretation&gt;&lt;r-squared&gt;&lt;coefficient&gt;" Title="How do you interpret a low coefficient yet statistically significant with a high R-squared?" ViewCount="2956" />
  
  
  <row Body="" CommentCount="0" CreationDate="2013-08-28T21:50:27.097" Id="68620" LastActivityDate="2013-08-28T21:50:27.097" LastEditDate="2013-08-28T21:50:27.097" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="" CommentCount="0" CreationDate="2013-08-28T22:09:07.653" Id="68626" LastActivityDate="2013-08-28T22:09:07.653" LastEditDate="2013-08-28T22:09:07.653" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="Latent variables refer to variables that cannot be directly observed. These variable are defined in terms of observable variables." CommentCount="0" CreationDate="2013-08-28T22:09:07.653" Id="68627" LastActivityDate="2013-08-28T22:34:52.500" LastEditDate="2013-08-28T22:34:52.500" LastEditorUserId="686" OwnerUserId="27581" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;It would help if you could answer @whuber's question.  However, in general, this question cannot be answered.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The reason is that variables are almost always on &lt;a href=&quot;http://en.wiktionary.org/wiki/incommensurable&quot; rel=&quot;nofollow&quot;&gt;incommensurate&lt;/a&gt; scales.  Consider the typical study that involves human subjects (as in medical research or the social and behavioral sciences), what covariates do we typically have?  It is quite common to include $\text{sex}$, $\text{age}$, $\text{weight}$, $\text{height}$, etc.  Now ask yourself: How many years makes you male?  How many centimeters equals a kilogram?  Etc.  These cannot be equated.  However, the ability of knowledge of one variable to help you differentiate amongst possible values of another variable is a function &lt;strong&gt;not only&lt;/strong&gt; of the slope of the relationship between the two variables in question, &lt;strong&gt;but also&lt;/strong&gt; of how spread out those values are&lt;sup&gt;1&lt;/sup&gt;.  That is, you cannot elide the issue of the incommensurable units with which your variables are measured.  Thus, @StocSim's answer, while commonly believed, will not work: the significance of your variables (their p-values, t statistics, etc.) is related to how spread out those variables are in your dataset and there is no &lt;em&gt;absolute&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt; way to determine whether the range of your $\text{heights}$ is comparable to the range of your $\text{ages}$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;1 See my answer here: &lt;a href=&quot;http://stats.stackexchange.com/questions/68588//68625#68625&quot;&gt;how-do-you-interpret-a-low-coefficient-yet-statistically-significant-with-a-high&lt;/a&gt;, for more along these lines.&lt;br&gt;&#10;2 It may be possible to equate variables by recourse to something else, e.g., how much it would cost to increase the amount of different variables.  Note, however, that this will change over time with changes in technology, the market, etc.  &lt;/sub&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-28T22:24:36.350" Id="68628" LastActivityDate="2013-08-28T22:29:56.247" LastEditDate="2013-08-28T22:29:56.247" LastEditorUserId="7290" OwnerUserId="7290" ParentId="52385" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to sample a uniformly random point in a polygon... &lt;/p&gt;&#10;&#10;&lt;p&gt;If sample a large number they'd be equally likely to fall into two regions if they have the same area.&lt;/p&gt;&#10;&#10;&lt;p&gt;This would be quite simple if it were a square since I would take two random numbers in [0,1] as my coordinates.&lt;/p&gt;&#10;&#10;&lt;p&gt;The shape I have is a regular polygon, but I'd like it to work for any convex polygon.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle&quot;&gt;http://stackoverflow.com/questions/3058150/how-to-find-a-random-point-in-a-quadrangle&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-08-28T23:56:10.567" Id="68634" LastActivityDate="2013-08-29T00:18:53.520" OwnerUserId="16529" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;monte-carlo&gt;&lt;geometry&gt;" Title="random sampling in a polygon" ViewCount="109" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;Central Limit Theorem&lt;/a&gt; says, &quot;given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So this can be used in Hypothesis Testing, that even a distribution $M(\mu, \sigma)$ is not a normal distribution, if $x \sim M(\mu, \sigma)$, the mean of $n$ samples, $\bar x$, shall follow normal distribution $N(\mu, \sigma / \sqrt{n})$. The parameter $\mu$ has a way of hypothesis testing.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, how about the Variance?&lt;/p&gt;&#10;&#10;&lt;p&gt;For normal distribution $N(\mu, \sigma)$, the $\sigma$ of $n$ samples follows $\chi^2(n)$ distribution, based on this one can set up the hypothesis testing.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the distribution $M(\mu, \sigma)$ is not normal, is there any similar way to set up a hypothesis testing for the Var (or standard deviation)?&lt;/p&gt;&#10;&#10;&lt;p&gt;What I can think of now is find the 5% and 95% percentile from the distribution, and change this to a binomial test. But this sounds a bit loose?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-29T04:00:50.930" Id="68641" LastActivityDate="2013-08-29T04:00:50.930" OwnerUserId="22571" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;variance&gt;&lt;central-limit-theorem&gt;" Title="Is there anything similar to Central Limit Theorem for Variance for Hypothesis Testing?" ViewCount="65" />
  <row AnswerCount="3" Body="&lt;p&gt;I need to derive analytic expressions for the autocovariance function $\gamma\left(k\right)$ of an ARMA(2,1) process denoted by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_t=\phi_1y_{t-1}+\phi_2y_{t-2}+\theta_1\epsilon_{t-1}+\epsilon_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I know that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma\left(k\right) = \mathrm{E}\left[y_t,y_{t-k}\right]$&lt;/p&gt;&#10;&#10;&lt;p&gt;so I can write:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma\left(k\right) = \phi_1 \mathrm{E}\left[y_{t-1}y_{t-k}\right]+\phi_2 \mathrm{E}\left[y_{t-2}y_{t-k}\right]+\theta_1 \mathrm{E}\left[\epsilon_{t-1}y_{t-k}\right]+\mathrm{E}\left[\epsilon_{t}y_{t-k}\right]$&lt;/p&gt;&#10;&#10;&lt;p&gt;then, to derive the analytic version of the autocovariance function, I need to substitute values of $k$ - 0, 1, 2 ... until I get a recursion that is valid for all $k$ greater than some integer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I substitute $k=0$ and work this through to get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\gamma \left(0\right) = \mathrm{E}\left[y_t,y_t\right]&#10; =  \phi_1 \mathrm{E}\left[y_{t-1}y_t\right] + \phi_2 \mathrm{E}\left[y_{t-2}y_t\right]+\theta_1 \mathrm{E}\left[\epsilon_{t-1}y_t\right]+\mathrm{E}\left[\epsilon_ty_t\right]\\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;now I can simplify the first two of these terms, and then substitute for $y_t$ as before:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\gamma\left(0\right) = \phi_1 \gamma\left(1\right) + \phi_2 \gamma\left(2\right)\\&#10;+ \theta_1 \mathrm{E}\left[\epsilon_{t-1} \left(\phi_1 y_{t-1} +\phi_2 y_{t-2} +\theta_1 \epsilon_{t-1} + \epsilon_t \right)\right]\\&#10;+ \mathrm{E}\left[\epsilon_t \left(\phi_1 y_{t-1} +\phi_2 y_{t-2} +\theta_1 \epsilon_{t-1} + \epsilon_t \right)\right]&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;then I multiply out the eight terms, which are:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;+\theta_1\phi_1\mathrm{E}\left[\epsilon_{t-1}y_{t-1}\right]\\&#10;+\theta_1\phi_2\mathrm{E}\left[\epsilon_{t-1}y_{t-2}\right]\\&#10;+\theta_1^2\mathrm{E}\left[\left(\epsilon_{t-1}\right)^2\right]=\theta_1^2\sigma_{\epsilon}^2\\&#10;+\theta_1\mathrm{E}\left[\epsilon_{t-1}\epsilon_{t}\right]=\theta_1\mathrm{E}\left[\epsilon_{t-1}\right]\mathrm{E}\left[\epsilon_{t}\right]=0\\&#10;+\phi_1\mathrm{E}\left[\epsilon_{t}y_{t-1}\right]\\&#10;+\phi_2\mathrm{E}\left[\epsilon_{t}y_{t-2}\right]\\&#10;+\theta_1\mathrm{E}\left[\epsilon_t\epsilon_{t-1}\right] = \theta_1\mathrm{E}\left[\epsilon_{t}\right]\mathrm{E}\left[\epsilon_{t-1}\right]=0 \\&#10;+\mathrm{E}\left[\left(\epsilon_t\right)^2\right] = \sigma_{\epsilon}^2&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I am left needing to resolve the four remaining terms.  I want to use the same logic for lines 1, 2, 5 and 6 as I used on lines 4 and 7 - for example for line 1:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta_1\phi_1\mathrm{E}\left[\epsilon_{t-1}y_{t-1}\right] = \theta_1\phi_1\mathrm{E}\left[\epsilon_{t-1}\right]\mathrm{E}\left[y_{t-1}\right] = 0$ because $\mathrm{E}\left[\epsilon_{t-1}\right]=0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly for lines 2, 5 and 6.  But I have a model solution that suggests the expression for $\gamma\left(0\right)$ simplifies to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma\left(0\right) = \phi_1\gamma\left(1\right)+\phi_2\gamma\left(2\right) +\theta_1\left(\phi_1+\theta_1\right)\sigma_{\epsilon}^2+\sigma_{\epsilon}^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;This suggests my simplification as described above would miss the term with the coefficient $\phi_1$ - which under my logic should be 0. Is my logic at fault, or is the model solution I found incorrect?&lt;/p&gt;&#10;&#10;&lt;p&gt;The worked solution also suggest that &quot;analogously&quot; $\gamma\left(1\right)$ can be found as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma\left(1\right) = \phi_1\gamma\left(0\right)+\phi_2\gamma\left(1\right) + \theta_1\sigma_{\epsilon}^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;and for $k&amp;gt;1$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma\left(k\right) = \phi_1\gamma\left(k-1\right)+\phi_2\left(k-2\right)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope the question is clear.  Any assistance will be much appreciated. Thank you in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a question related to my research, and is not in preparation for any exam or coursework.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-29T05:20:05.220" FavoriteCount="1" Id="68644" LastActivityDate="2013-08-29T13:09:20.080" LastEditDate="2013-08-29T07:39:48.123" LastEditorUserId="21167" OwnerUserId="21167" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;covariance&gt;&lt;autocorrelation&gt;&lt;arma&gt;" Title="Autocovariance of an ARMA(2,1) process - derivation of analytical model for $\gamma( k)$" ViewCount="2470" />
  <row Body="&lt;p&gt;It is not by accident. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.5540&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; you shall find a brief a very nice review on conjugate priors. Concretely, it mentions that if there exist a set of of sufficient statistics of fixed dimension for the given likelihood function, then you can construct a conjugate prior for it. Having a set of &lt;a href=&quot;http://en.wikipedia.org/wiki/Sufficient_statistic&quot; rel=&quot;nofollow&quot;&gt;sufficient statistics&lt;/a&gt; means that you can factorize the likelihood in a form that let you estimate the parameters in an computational efficient way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides that, having conjugate priors it is not only computationally convenient. It also provides smoothing and allows to work with very little samples or no previous samples, which is necessary for problems like decision making, in cases where you have very little evidence.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-29T08:03:50.503" Id="68652" LastActivityDate="2013-08-29T08:03:50.503" OwnerUserId="17908" ParentId="59363" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There has been some work on adapting deep learning methods for sequential data. A lot of this work has focused on developing &quot;modules&quot; which can be stacked in a way analogous to stacking restricted boltzmann machines (RBMs) or autoencoders to form a deep neural network. I'll highlight a few below:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~hinton/absps/fcrbm_icml.pdf&quot;&gt;Conditional RBMs&lt;/a&gt;: Probably one of the most successful applications of deep learning for time series. Taylor develops a RBM like model that adds temporal interactions between visible units and apply it to modeling motion capture data. Essentially you end up with something like a linear dynamical system with some non-linearity added by the hidden units.&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf&quot;&gt;Temporal RBMs&lt;/a&gt;: In his thesis (section 3) Ilya Sutskever develops several RBM like models with temporal interactions between units. He also presents some interesting results showing training recurrent neural networks with SGD can perform as well or better than more complex methods, like Martens' Hessian-free algorithm, using good initialization and a slightly modified equation for momentum.&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.socher.org/index.php/Main/ParsingNaturalScenesAndNaturalLanguageWithRecursiveNeuralNetworks&quot;&gt;Recursive Autoencoders&lt;/a&gt;: Lastly I'll mention the work of Richard Socher on using recursive autoencoders for parsing. Although this isn't time series, it is definitely related.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2013-08-29T13:46:45.903" Id="68677" LastActivityDate="2013-08-29T13:46:45.903" OwnerUserId="6248" ParentId="68662" PostTypeId="2" Score="13" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a dataset with mostly financial variables (120 features, 4k examples) which are mostly highly correlated and very noisy (technical indicators, for example) so I would like to select about max 20-30 for later use with model training (binary classification - increase / decrease).&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking about using random forests for feature ranking. Is it a good idea to use them recursively? For example, let's say in the first round I drop the worst 20%, second too and so on until I get the desired number of features.  Should I use cross-validation with RF? (It's intuitive for me not to use CV because that's pretty much what RF does already.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Also if I go with random forests should I use them as classifiers for the binary or regressor for the actual increase / decrease to get feature importances?&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, the models I would like to try after feature selection are: SVM, neural nets, locally weighted regressions, and random forest. I'm mainly working in Python.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-29T17:15:59.797" FavoriteCount="8" Id="68692" LastActivityDate="2014-01-28T12:05:11.157" LastEditDate="2013-08-29T17:52:35.770" LastEditorUserId="7290" OwnerUserId="29688" PostTypeId="1" Score="7" Tags="&lt;feature-selection&gt;&lt;random-forest&gt;&lt;python&gt;" Title="Feature selection with Random Forests" ViewCount="4171" />
  <row Body="&lt;p&gt;The book we used in my Statistical Theory classes was &lt;em&gt;Probability and Statistical Inference&lt;/em&gt;, by Nitis Mukhopadhyay.  It definitely starts at the beginning with &quot;Notions of Probability&quot; and the classic coin toss, but then quickly moves into statistics and statistical distributions and moments and such.  It definitely seems mathematically-oriented to me, I pretty much never crack it when working on applied statistics projects.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-29T19:05:28.977" Id="68701" LastActivityDate="2013-08-29T19:05:28.977" OwnerUserId="29350" ParentId="68601" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I need to estimate baseline hazard function $\lambda_0(t)$ in a time dependent Cox model&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lambda(t) = \lambda_0(t) \exp(Z(t)'\beta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;While I took Survival course, I remember that the direct derivative of cumulative hazard function ($\lambda_0(t) dt = d\Lambda_0(t)$) would not be a good estimator because Breslow estimator gives a step function.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, is there any function in R that I could use directly ? Or any reference on this topic ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if it is worth to open another question, so I just add some background why baseline hazard function is important for me. The formula below estimates the probability that the survival time for one subject is larger than another,. Under a Cox model setting, baseline hazard function $\lambda_0(t)$ is required.   &lt;/p&gt;&#10;&#10;&lt;p&gt;$P(T_1 &amp;gt; T_2 ) = - \int_0^\infty S_1(t) dS_2(t) = - \int_0^\infty S_1(t)S_2(t)\lambda_2(t)dt $ &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-30T01:08:18.947" FavoriteCount="1" Id="68737" LastActivityDate="2013-08-30T08:52:22.723" LastEditDate="2013-08-30T01:23:52.277" LastEditorUserId="29363" OwnerUserId="29363" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;survival&gt;&lt;cox-model&gt;" Title="How to estimate baseline hazard function in Cox Model with R" ViewCount="2814" />
  <row Body="&lt;p&gt;You second last paragraph is the correct answer. As you say, this is the estimate that uses the whole ensemble, but never uses any data that was used to construct the trees making the individual predictions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T05:04:33.570" Id="68741" LastActivityDate="2013-08-30T05:04:33.570" OwnerUserId="10354" ParentId="68740" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I suppose in the next advice that you use some method for which R squared is not usefull.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that you use GLM with maximum likelihood estimation. Then you could use likelihood ratio test as a variable selection  method (or more properly as a test for model-size). First you must have a baseline model, for example one which do not have any covariates or only minimal amount of those. You can call that H0 hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next you formulate an alternate hypothesis in a way that model size is larger. You can call this H1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next likelihood ratio test is done, first you calculate values for the loglikelihood. LR test statistic is actually distributed as a Chisq with k+p-k degrees of freedom, large statistic means that H0 must be rejected. Value of test statistic is following:&lt;/p&gt;&#10;&#10;&lt;p&gt;-2LogLik(H0)+2logLik(H1)&lt;/p&gt;&#10;&#10;&lt;p&gt;p is number of extra parameters in the more larger model.&lt;/p&gt;&#10;&#10;&lt;p&gt;And there is also AIC, BIC and HQIC information criterias which are all based on log likelihood but penalize from too large model. So they prefer simplicity over more complex models.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is also generalized method of moments estimation (GMM/GEE) for which there are different tests available in the same spirit as for MLE. If I remember correctly there is a Hansens test which tests for model-size and test statistic is distributed as a Chisq. This comes from the fact that parameters are asymptotically normally distributed and their quadratic transformation follows Chisq - distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T06:20:53.463" Id="68743" LastActivityDate="2013-08-30T06:20:53.463" OwnerUserId="28732" ParentId="68137" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;As @ocram and @gung rightly point out, if your correlations vary greatly, a different statistical procedure may be more appropriate. That said, for your case I would suggest two possible ways to estimate a required sample size or achieved power using G*Power:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;strong&gt;Use the most conservative estimate&lt;/strong&gt;. A null correlation among repeated measures will yield a sample size that is equivalent to that of a between subject design divided by the number of groups: A between subjects comparison of your three groups ($f = .25$, $α = .05$, $1-β = .95$) would require a total sample size of $n = 252$. A within subjects comparison with an assumed correlation among repeated measures of $r = 0$ requires a total sample size of $n = 84$. Since each measurement is repeated in all three groups your effective sample size is $84\times3 = 252$. Thus, you could simply use 0 as an estimate for your correlation. However, that will most likely be an overly conservative estimate. Using the lowest correlation coefficient found in your data, would give you a more adequate but still conservative estimate of the required sample size. I am uncertain how you need to enter a negative correlation coefficient into G*Power. Entering a negative value will yield a much larger required sample size, which doesn't readily make sense to me, but I'm not terribly knowledgable concerning these calculations.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Use the mean correlation&lt;/strong&gt;. A less conservative estimate of the correlation among repeated measures is the mean correlation found in your data. Note, however, correlation coefficients are not normally distributed. &lt;em&gt;The formulas you suggest are, therefore, not appropriate to determine the mean correlation&lt;/em&gt;. You need to perform a &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Use_the_Fisher_transformation&quot; rel=&quot;nofollow&quot;&gt;Fisher-transformation&lt;/a&gt; before averaging and afterwards retransform the mean to yield the correct mean correlation. But, I think, if the correlations you enter into the mean vary greatly, this approach may lead to sample size estimates that are too liberal. Comparing the results to those resulting from the most conservative estimate may be helpful.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Since it appears you are using &lt;code&gt;R&lt;/code&gt; to conduct your analysis here is a function that will do the calculations for you:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rep.m.cor &amp;lt;- function(x, measure, formula, type = &quot;min&quot;) {&#10;  require(&quot;reshape2&quot;)&#10;&#10;  fisher.z &amp;lt;- function(r) {&#10;    return(0.5 * log((1+r)/(1-r)))&#10;  }&#10;&#10;  inv.fisher.z &amp;lt;- function(z) {&#10;    return((exp(2*z) - 1)/(exp(2*z) + 1))&#10;  }&#10;&#10;  melt.data &amp;lt;- melt(x, measure.vars = measure, na.rm = FALSE)&#10;  wide.data &amp;lt;- dcast(melt.data, formula = formula, mean, value.var = &quot;value&quot;)&#10;&#10;  correlations &amp;lt;- cor(wide.data[, -1])&#10;  correlations &amp;lt;- correlations[upper.tri(correlations)]&#10;&#10;  if(type == &quot;mean&quot;) {&#10;    m.correlations &amp;lt;- inv.fisher.z(mean(fisher.z(correlations)))&#10;  } else if(type == &quot;min&quot;) {&#10;    m.correlations &amp;lt;- inv.fisher.z(min(fisher.z(correlations)))&#10;  } else {&#10;    stop(&quot;Type must be either 'min' or 'mean'.&quot;)&#10;  }&#10;&#10;  return(m.correlations)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;x = A &lt;code&gt;data.frame&lt;/code&gt; resulting from aggregation, for example &lt;code&gt;aggregate(measure ~ subject * factor1 * factor2, data, mean)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;measure = A string providing the name of the measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;formula = A formula giving the factor for which the correlation should be calculated, for example &lt;code&gt;subject ~ factor1&lt;/code&gt;. It is also possible to determine the correlation for the interaction of two or more factors (&lt;code&gt;subject ~ factor1  + factor2&lt;/code&gt;; yes, it needs to be a &quot;+&quot;). &lt;em&gt;Note that GPower can be used to perform power analyses for up to two repeated measures factors as long as one of them has only two levels. To do so, enter the larger number of factor levels into the field &quot;Number of measurements&quot; and multiply the effect size $f$ by $\sqrt{2}$ (2 corresponding two the number of levels of the other factor). If both factor have more than two levels GPower will underestimate the required sample size!&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;type = A string naming the estimation procedure for the correlation among repeated measures. &lt;code&gt;&quot;min&quot;&lt;/code&gt; corresponds to the above option 1, &lt;code&gt;&quot;mean&quot;&lt;/code&gt; corresponds to the above option 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope this helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T10:02:53.333" Id="68754" LastActivityDate="2013-09-03T15:04:34.617" LastEditDate="2013-09-03T15:04:34.617" LastEditorUserId="14646" OwnerUserId="14646" ParentId="44134" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You are referring to a very hard problem of finding the best possible metric. It is a hard problem even for the unimodal data, the multimodal case you are referring to is a great challenge. There are basically three possibilities:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;use some primitive metric, like Euclidean distance, treating everything as numbers (you can convert categorical values to some values as well). This will yield rather poor results, but is the simplest possibility and gives you time for analysis and optimization of the rest of the system.&lt;/li&gt;&#10;&lt;li&gt;perform deep analysis of your data and/or find an expert able to design a good metric. This is the most hard to do, but would yield the best results (assuming that you have access to &quot;real expert&quot;).&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;add additional abstraction layer to your problem and treat finding this metric as an optimization problem on its own. There are numerous studies showing how one can find good multi-modal metrics for any kind of data by formalizing it as an optimization problem and applying one of many known mathematical solvers. Some examples of such studies would be:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~pengtaox/ijcai2013paper.pdf&quot; rel=&quot;nofollow&quot;&gt;Multi Modal Distance Metric Learning&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cosmal.ucsd.edu/~gert/papers/mcfee11a.pdf&quot; rel=&quot;nofollow&quot;&gt;Learning Multi-modal Similarity&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T10:12:13.693" Id="68756" LastActivityDate="2013-09-01T18:07:24.740" LastEditDate="2013-09-01T18:07:24.740" LastEditorUserId="7290" OwnerUserId="28903" ParentId="68736" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have collected data that represent stimulation levels in three different time points ($T_0$, $T_1$ and $T_2$) using three different stimulants ($S_1$, $S_2$ and $S_3$) and a control ($C$). I have tested all three stimulants with three different subjects ($J_1$, $J_2$ and $J_3$) and for each subject and stimulant/ control I have three numbers that represent stimulation levels in $T_0$, $T_1$ and $T_2$ time points.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I test the hypothesis that a certain stimulant shows significant higher level than the control? How can I test the hypothesis that all three stimulants shows a significant/ insignificant difference?&lt;/p&gt;&#10;&#10;&lt;p&gt;The data is not normally distributed so I would have to use non-parametric tests. I am lost to whether it should be paired/unpaired or to use a regression or something else. Can anybody help?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T10:16:15.957" Id="68758" LastActivityDate="2013-08-30T13:41:26.823" LastEditDate="2013-08-30T13:41:26.823" LastEditorUserId="29710" OwnerUserId="29710" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;repeated-measures&gt;&lt;nonparametric&gt;" Title="How do I compare stimulation levels between different stimulants with time points?" ViewCount="53" />
  
  
  
  
  <row Body="&lt;p&gt;In the face of an apparent paradox, &lt;strong&gt;it helps to resort to definitions and first principles.&lt;/strong&gt;  These are applied below to&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Describe the random variables $X,$ $Y,$ and $Z$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Compute the expectations needed for the correlation coefficient calculation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Compute the correlation coefficient itself.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It is all simple and straightforward--precisely those characteristics of small problems like this that help illuminate basic concepts and procedures which otherwise may be so tricky and counterintuitive.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;By definition, a &lt;em&gt;random variable&lt;/em&gt; (such as $X$, $Y$, or $Z$) is a real-valued measurable function of a probability space $(\Omega, \mathfrak{S}, p)$.  In more colloquial language (as explained at &lt;a href=&quot;http://stats.stackexchange.com/questions/50/what-is-meant-by-a-random-variable/54894#54894&quot;&gt;What is meant by a &amp;quot;random variable&amp;quot;?&lt;/a&gt;), $\Omega$ is a box, its elements $\omega \in \Omega$ are tickets (slips of paper), $\mathfrak S$ stipulates what events can be given probabilities, and $p$ tells us the proportions of each kind of ticket in the box.  A &quot;measurable function&quot; merely is a consistent way of writing numbers on the tickets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the setting involves two distinct random variables $(X,Y)$ we begin by &lt;strong&gt;writing these values on the tickets&lt;/strong&gt;.  There are $3\times 3 = 9$ possibilities, small enough to tabulate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{array}{ccccccc}&#10; X &amp;amp; Y &amp;amp; Z &amp;amp; XZ &amp;amp; X^2 &amp;amp; Z^2 &amp;amp; p\\&#10; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; p_0 q_0 \\&#10; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; p_0 q_1 \\&#10; 0 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; p_0 q_2 \\&#10; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; p_1 q_0 \\&#10; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; p_1 q_1 \\&#10; 1 &amp;amp; 2 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; p_1 q_2 \\&#10; 2 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 4 &amp;amp; 1 &amp;amp; p_2 q_0 \\&#10; 2 &amp;amp; 1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 4 &amp;amp; 1 &amp;amp; p_2 q_1 \\&#10; 2 &amp;amp; 2 &amp;amp; 1 &amp;amp; 2 &amp;amp; 4 &amp;amp; 1 &amp;amp; p_2 q_2 \\&#10;\end{array}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Each row (beneath the header) gives information about one kind of the nine tickets.  The $X$ column shows the value of $X$ written on each ticket and the $Y$ column shows the value of $Y$.  &lt;strong&gt;The rest is deduced from the information given:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$Z$ is computed according to its definition in terms of $X$ and $Y$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$XZ$ is the product of $X$ and $Z$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$X^2$ and $Z^2$ are the squares of $X$ and $Z$, respectively.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Those latter three columns were computed in anticipation they would be needed for calculating the correlation coefficient of $X$ and $Z$.  The final column, $p$, computes the proportions of each kind of ticket &lt;em&gt;assuming $X$ and $Y$ are independent&lt;/em&gt;.  This means the proportion of tickets where $(X,Y) = (i,j)$ is the product of the probabilities $\Pr(X=i) = p_i$ and $\Pr(Y=j) = q_j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;By definition, &lt;strong&gt;the expectation of a random variable is its average in the box,&lt;/strong&gt; weighted according to the proportions.  Thus the six expectations (simplified using the axiom of total probability $p_0+p_1+p_2=1=q_0+q_1+q_2$ to eliminate $p_0$ and $q_0$) are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;\mathbb{E}[X] &amp;amp;= p_1 q_0+2 p_2 q_0+p_1 q_1+2 p_2 q_1+p_1 q_2+2 p_2 q_2 &amp;amp; = p_1 + 2p_2 \\&#10;\mathbb{E}[Y] &amp;amp;=p_1 q_0+2 p_2 q_0+p_1 q_1+2 p_2 q_1+p_1 q_2+2 p_2 q_2 &amp;amp; = q_1 + 2q_2 \\&#10;\mathbb{E}[Z] &amp;amp;=p_1 q_0+p_2 q_0+p_1 q_1+p_2 q_1+p_1 q_2+p_2 q_2 &amp;amp;= p_1 q_1-p_2 \left(q_2-1\right)+q_2 \\&#10;\mathbb{E}[XZ] &amp;amp;=p_1 q_0+2 p_2 q_0+p_1 q_1+2 p_2 q_1+p_1 q_2+2 p_2 q_2 &amp;amp;= p_1 \left(q_1+q_2\right)+2 p_2 \\&#10;\mathbb{E}[X^2] &amp;amp;=p_1 q_0+4 p_2 q_0+p_1 q_1+4 p_2 q_1+p_1 q_2+4 p_2 q_2&amp;amp;= p_1+4 p_2 \\&#10;\mathbb{E}[Z^2] &amp;amp;=p_1 q_0+p_2 q_0+p_1 q_1+p_2 q_1+p_1 q_2+p_2 q_2 &amp;amp;= p_1 q_1-p_2 \left(q_2-1\right)+q_2.&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The correlation coefficient is &lt;em&gt;defined&lt;/em&gt; as&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\rho_{X,Z} = \frac{\mathbb{E}[XZ] - \mathbb{E}[X]\mathbb{E}[Z]}{\sqrt{\mathbb{E}[X^2] - \mathbb{E}[X]^2}\sqrt{\mathbb{E}[Z^2] - \mathbb{E}[Z]^2}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The rest is arithmetic (shown below for completeness).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Plugging in the preceding values produces&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{-\left(p_1+2 p_2\right) \left(p_2 \left(-\left(q_2-1\right)\right)+p_1 q_1+q_2\right)+p_1 \left(q_1+q_2\right)+2 p_2}{\sqrt{\left(-\left(p_1+2 p_2\right){}^2+p_1+4 p_2\right) \left(-\left(p_2 \left(-\left(q_2-1\right)\right)+p_1 q_1+q_2\right){}^2-p_2 \left(q_2-1\right)+p_1 q_1+q_2\right)}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for the correlation coefficient.  It can range anywhere from $0$ (approached in the limit as $p_1=p_2=1/2$ and $q_1=q_2=1/2-e^2$ for which it equals $\frac{e}{\sqrt{1-e^2}} \to 0$ as $e\to 0$) through $1$ (set $p_1=0, p_2=1/2, q_1=0, q_2=0$ for instance).  Setting (as in the code of the question) $p_1 =q_1= 0.18, p_2 =q_2= 0.01$ gives $\rho_{X,Z} = 0.46308$ and $p_1= q_1 = 0.32, p_2=q_2 = 0.04$ gives $\rho_{X,Z} = 0.564433.$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-30T15:07:41.227" Id="68782" LastActivityDate="2013-08-30T15:07:41.227" OwnerUserId="919" ParentId="68599" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="68806" AnswerCount="1" Body="&lt;p&gt;Since the population variance can never be estimated, will we always have a t-distribution instead of normal?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-30T18:31:22.723" Id="68799" LastActivityDate="2013-08-30T19:53:19.837" OwnerUserId="12005" PostTypeId="1" Score="-3" Tags="&lt;normal-distribution&gt;&lt;t-distribution&gt;" Title="Can we never have a normal distribution?" ViewCount="94" />
  <row AnswerCount="0" Body="&lt;p&gt;I've been asked by a reviewer on a manuscript to provide plots of a model fit for a binomial lme which is specified as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;EVENT_OCCURRED ~ (IV1*IV2) + IV3 + (1|PPT) + (1|RANDOM1) + (1|RANDOM2) + (1|RANDOM3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where &lt;code&gt;EVENT_OCCURRED&lt;/code&gt; is a binary value (0/1), &lt;code&gt;IV1&lt;/code&gt;, &lt;code&gt;IV2&lt;/code&gt; and &lt;code&gt;IV3&lt;/code&gt; are independent variables, &lt;code&gt;PPT&lt;/code&gt; is the participant identifier (it's a behavioral experiment), and &lt;code&gt;RANDOM1&lt;/code&gt;, &lt;code&gt;RANDOM2&lt;/code&gt; and &lt;code&gt;RANDOM3&lt;/code&gt; are random effects. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the version of the manuscript that I submitted, I included mean-average data that visualised the three IVs and the interaction, without any kind of line fitting the results. Before submitting it, I'd asked a colleague who seemed to think that plotting a model with a bunch of random effects and three independent variables with an interaction wouldn't really make much sense. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, now I've been asked to do it specifically, I would like to check:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is there any way that I could plot the fitted versus actual values for this model?  &lt;/li&gt;&#10;&lt;li&gt;Would it involve generating a large number of plots - for example, breaking up the plots and fits by the participant identifier, and for the other random factors? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-08-30T20:04:41.780" Id="68808" LastActivityDate="2013-09-02T18:46:23.430" LastEditDate="2013-09-02T18:46:23.430" LastEditorUserId="7290" OwnerUserId="28365" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;binomial&gt;&lt;lme&gt;&lt;glmm&gt;" Title="Plotting fit for binomial lme" ViewCount="146" />
  <row Body="&lt;p&gt;If the item is a box containing fifty-one \$20 bills, then most people will buy it. Bump the price up to \$1040, and most people will not buy it.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can repeat the experiment with a price of \$1005, \$995, \$1010, \$990, but each time, you'll usually get 100% yes's. Any model built off these measurements will fail to correctly predict that at $1040, you'll get 100% no's. This illustrates that the information you get at even multiple prices may not be useful for predicting some other price.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-08-30T23:03:21.183" Id="68818" LastActivityDate="2013-08-30T23:03:21.183" OwnerUserId="2221" ParentId="68813" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think there's a problem where you say you &quot;observe the two coins are independent.&quot;  You never &lt;em&gt;observe&lt;/em&gt; probabilistic independence, per se; it is always a property of the events / random variables under consideration, which are &lt;em&gt;constructed&lt;/em&gt; by the modeller to represent some (physical or otherwise) phenomenon.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the physical phenomenon here is: there are two fair coins, and tossing either one of them does not affect the outcome of the other.  They're 'independent' in a nontechnical sense, if you want to use that word.&lt;/p&gt;&#10;&#10;&lt;p&gt;A probabilistic model that we can build to represent this physical phenomenon mathematically is: &lt;strong&gt;define&lt;/strong&gt; two independent, Bernoulli(1/2)-distributed random variables $X_{0}$ and $X_{1}$.  I.e.,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;X_{0}, X_{1} \overset{iid}\sim \text{bernoulli}(1/2) \\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here I mean 'independent' in the technical, probabilistic sense.  That gives us stuff like&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(\{X_{0} = 1\}\cap\{X_{1} = 1\} = P(X_{0} = 1)P(X_{1}=1)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and we can get the joint distribution by enumerating all the combinations of $X_{0}$ and $X_{1}$, as you've done.  Nothing circular to be found.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T23:17:40.330" Id="68819" LastActivityDate="2013-08-31T02:43:25.063" LastEditDate="2013-08-31T02:43:25.063" LastEditorUserId="7706" OwnerUserId="7706" ParentId="68752" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The marginal distribution refers to the probability distribution of a subset of variables contained in a joint distribution. It is obtained by &quot;summing over&quot; all outcomes of the other variables in the joint distribution in case of discrete variables, and &quot;integrating over&quot; all outcomes of the other variable in case of continuous variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, if $P(x_1,x_2,\ldots,x_n)$ represents a discrete joint distribution, the marginal distribution of $x_i$ is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(x_i) = \sum_{x_1,x_2,\ldots,x_{i-1},x_{i+1},\ldots x_n} P(x_1,x_2,\ldots,x_n)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The summation is over all possible outcomes of the indicated variables. Similarly, for the case of a continuous joint distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(x_i) = \int \int \ldots \int P(x_1,x_2,\ldots,x_n) dx_1 dx_2 \ldots dx_{i-1} dx_{x+1} \ldots dx_{n}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-30T23:46:17.430" Id="68823" LastActivityDate="2013-08-31T08:27:36.783" LastEditDate="2013-08-31T08:27:36.783" LastEditorUserId="27581" OwnerUserId="27581" PostTypeId="5" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a collection of Poisson processes each with an unknown $\lambda$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to estimate $\lambda$ for each process.&lt;/p&gt;&#10;&#10;&lt;p&gt;for each process I could take either the total number of event over the total time or the inverse of the mean of the waiting times.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that the dataset is quite small, with many of the processes having less than 5 total events, is is better to use the estimate based on the waiting times or the total count?&lt;/p&gt;&#10;&#10;&lt;p&gt;The replies to &lt;a href=&quot;http://stats.stackexchange.com/questions/31138/can-i-estimate-the-parameter-of-a-poisson-arrival-process-from-a-low-incidence-o&quot;&gt;this question&lt;/a&gt; suggest that the count over the time is the best estimator, but no mention of the waiting times.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-08-31T02:50:13.393" Id="68828" LastActivityDate="2014-06-20T06:59:58.537" LastEditDate="2013-08-31T08:32:55.333" LastEditorUserId="22047" OwnerUserId="20009" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;poisson&gt;&lt;poisson-process&gt;" Title="Low intensity Poisson estimation" ViewCount="118" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am not very familiar with mixed models --- I have been reviewing the various tutorials on the web but still am not sure how to specify my model, even as a starting point for comparison. I have a relatively simple design:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;two non-random groups (normal/obese) -- randomized within each group to two conditions (control/treatment). And the DV measured at pre and post-test.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So I am interested in looking at differences between pre and post, for the treatment vs. control and also between the two groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using SPSS -- this is the syntax I've come up with so far.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MIXED Score BY Group Treatment Time&#10;/CRITERIA=CIN(95) MXITER(200) MXSTEP(10) SCORING(5) SINGULAR(0.000000000001) HCONVERGE(0,&#10;ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)&#10;/FIXED=Group Treatment Group*Treatment Time Group*Time Treatment*Time Group*Treatment*Time | SSTYPE(3)&#10;/METHOD=REML&#10;/PRINT=G R SOLUTION TESTCOV&#10;/RANDOM=INTERCEPT | SUBJECT(ID) COVTYPE(VC)&#10;/REPEATED=Time | SUBJECT(ID) COVTYPE(UN)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Some of my questions --&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;I have specified a random intercept for subject ID, but do I need to account somehow that these are random effects from TWO separate populations (normal weight and obese)? If so, how would I do this?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It seems that I could estimate the time effect using either the &lt;code&gt;REPEATED&lt;/code&gt; or &lt;code&gt;RANDOM&lt;/code&gt; command (by adding time to the &lt;code&gt;RANDOM&lt;/code&gt;). I can't figure out the difference of what that means conceptually and how to decide which fits my data or design better.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is it correct to list &lt;code&gt;TIME&lt;/code&gt; as a fixed factor? I have also seen in one of the books I'm looking at that variables have been coded as 0/1 indicators and entered as covariates. I have no idea why this was done -- I can't seem to find an explanation. However, If I do this, it changes my fixed effects (even with all the same effects and interactions specified). And changes on whether the pre or post is specified as 0/1. So how does mixed models treat factors vs covariates differently? What could be the advantage of treating a variable as a covariate rather than a factor? I noted that although the tests of fixed effects changed, the estimates using &lt;code&gt;EMMEANS&lt;/code&gt; or &lt;code&gt;TEST&lt;/code&gt; remained the same, which I also thought was weird.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Does one need to account for baseline group differences, and if so how would this be done? Since my data is in long format I no longer have a pre-test measurement that I can put in as a covariate in post-test scores. Sorry if these are dumb questions -- I am much more used to ANCOVA and GLM. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-08-31T04:10:27.443" Id="68830" LastActivityDate="2013-08-31T07:17:37.330" LastEditDate="2013-08-31T07:17:37.330" LastEditorUserId="930" OwnerUserId="29362" PostTypeId="1" Score="0" Tags="&lt;mixed-model&gt;&lt;spss&gt;&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;" Title="Random effects in mixed models" ViewCount="1349" />
  
  <row Body="&lt;p&gt;my 2 cents:&lt;/p&gt;&#10;&#10;&lt;p&gt;First, locally, if $\text{pdf}_s(s)$ is the probability density function of a random variable $s$, and let's do transform that $s = s(t)$, one would have $$\text{pdf}_t(t) = \text{pdf}_s(s(t)) \frac{ds(t)}{dt} ...  (1)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's look at $x \rightarrow \sin{x}$ transform.&lt;/p&gt;&#10;&#10;&lt;p&gt;Locally, one could replace (1) as $t \rightarrow x$, $s \rightarrow  \sin{x}$, and have&#10;$$\text{pdf}_x(x) = \text{pdf}_{\sin{x}} (\sin{x}) \cos{x}$$&#10;, or $$\text{pdf}_y(y) = \text{pdf}_x(x) / \cos{x}$$, where $y=\sin{x}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As this is only a &quot;local&quot; equation, for $y=\sin(x)$, the $\text{pdf}_y(y)$ shall be a infinite sum of all the points of $x+2k\pi$, $k \in Z$. So one have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{pdf}_y(y) = \Sigma_{k\in Z} \text{pdf}_x(x + 2k\pi) / \cos{x}... (2)$$, where $y=\sin{x} $.&lt;/p&gt;&#10;&#10;&lt;p&gt;However here is a catch: at $x=(k+\frac{1}{2})\pi$, $k \in Z$, $\sin{x} = \pm1$, $\frac{d\sin{x}}{dx} = \cos{x} = 0$, natually $\text{pdf}_y(y) \rightarrow \infty$. This explains in your first diagram, at points $\sin{x}=\pm1$, the pdf curve in red spikes up.&lt;/p&gt;&#10;&#10;&lt;p&gt;Things become complex when one try to do a transform $x, y \rightarrow z$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's put it as $z = f(x,y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's define inverse function of $z$ on $y$, given $x$ as $g(x, z) = y$. This means $f(x, g(x,z)) = z$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $g(x,z)$ exists, we would have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{pdf}_z(z) = \int_{-\infty}^{+\infty} \text{pdf}_{x,y} (x, g(x,z) ) \frac{\partial g(x,z)}{\partial z} dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So there's no simple results for $z=x*y$, $z=x+y$ or $z=sin(x+y+3)$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-08-31T12:00:52.327" Id="68846" LastActivityDate="2013-08-31T12:47:06.157" LastEditDate="2013-08-31T12:47:06.157" LastEditorUserId="22571" OwnerUserId="22571" ParentId="8591" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In the case of  count data where my variance is 6.34 and mean is 5.44, which would be a better fit to the data - Poisson or negative binomial?  And what will be the assumptions ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-08-31T14:56:33.737" Id="68856" LastActivityDate="2013-08-31T18:09:10.583" OwnerUserId="29757" PostTypeId="1" Score="0" Tags="&lt;poisson&gt;" Title="Poisson vs Negative Binomial" ViewCount="120" />
  
  <row Body="&lt;p&gt;The first question is one that has no generally agreed upon answer. My own view is like yours, but others have argued that a population can be viewed as a sample from a &quot;super-population&quot; where the exact nature of a super-population varies depending on context: E.g. a census of all the people living in a building could be viewed as a sample from all the people living in similar buildings; a census of the population of the USA (not that one could ever be truly complete) could be viewed as a sample from a super-population of Americans who might one day exist (or something like that). I think this is often an excuse to get to use p-values; many scientists in substantive fields aren't comfortable if they haven't got a p-value. (But that is &lt;em&gt;my&lt;/em&gt; view).&lt;/p&gt;&#10;&#10;&lt;p&gt;The second question seems a bit odd to answer in a general way. When do you get a sample that is (say) even more than half of the population?&lt;/p&gt;&#10;&#10;&lt;p&gt;A bigger problem will be bias. Going back to the US Census, the problem isn't simply that it misses people, but that the people it misses are not a random sample of the total population; so, even if the census gets answers from (to pick a number) 95% of all people, if those 5% remaining are quite different, then results will be biased. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-01T11:54:12.963" Id="68901" LastActivityDate="2013-09-01T11:54:12.963" OwnerUserId="686" ParentId="68886" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The density (or probability density function, &quot;PDF&quot;) of a continuous random variable $X$ is a function $x\to f_X(x)$.  Its defining property is that for all numbers $a\le b$, the probability of the event $a\lt X \le b$ is obtained by integrating $f_X$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(a \lt X \leq b) = \int^{b}_{a}f_X(x)dx.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability density function can be obtained by taking the derivative of the distribution function $F_X(x) = \Pr(X \le x)$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-01T11:59:23.177" Id="68902" LastActivityDate="2013-10-07T14:16:51.233" LastEditDate="2013-10-07T14:16:51.233" LastEditorUserId="919" OwnerUserId="26338" PostTypeId="5" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;Dividing the cable in sections is not the only possibility.&lt;/p&gt;&#10;&#10;&lt;p&gt;A natural approach for this problem is through a stochastic point&#10;process indexed by the abscissa $x$ along the cable's path which can&#10;be used as a 'time'. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_process&quot; rel=&quot;nofollow&quot;&gt;Poisson Process&lt;/a&gt; is a good&#10;candidate: the breakage rate is given by the the &lt;em&gt;intensity&lt;/em&gt; (or&#10;&lt;em&gt;rate&lt;/em&gt;) $\lambda(x)$ of the process, which here is the probability of&#10;break by unit of cable length around $x$. This intensity can be&#10;constant or piecewise constant and can be related then to some area&#10;predictors as cited by Robert Jones. It can also be related if needed&#10;to continuous predictors such as the cable depth.  For a fixed period&#10;of time, say a year, a PP can describe the locations of the breaks.&#10;The intensity function $\lambda(x)$ is not unlike a probability&#10;density, and will have to be integrated with respect to $x$ for most&#10;computations.&lt;/p&gt;&#10;&#10;&lt;p&gt;A more sophisticated version would use a spatio-temporal point&#10;process with intensity $\lambda(x,\,t)$ related to the time $t$ and &#10;to the abscissa $x$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-01T13:48:18.580" Id="68914" LastActivityDate="2013-09-01T13:48:18.580" OwnerUserId="10479" ParentId="68525" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Assuming your inferential focus is the treatment-control contrast, the solution you propose is a multicentre parallel randomised controlled trial design, where the centres are the communities, and there is no clear need to account for measurement times.&lt;/li&gt;&#10;&lt;li&gt;A particular feature of the design you propose is that all participants from one cluster are accrued at the same time. Another feature is that the accrual times are pre-set and randomised. This step is not necessary for bias control, since period-specific effects will be equally balanced between the two arms. The randomisation of the accrual times may however help control bias under more complex time-community-intervention interactions.&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In all communities your primary endpoint is at three months (from randomisation). There is no need to account for the fact that endpoints are measured at different calendar times, similarly to a single-center parallel RCT design that may accrue participants over several years. The power implications of your design are those of a multicentre parallel RCT.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;(This addresses a statement in your post.) The clustered stepped wedge design does not require a measurement at each time interval for every participant. Every participant would usually be assessed at baseline, and then at any later time point set for a primary or secondary endpoint. The related analytical feature of the stepped wedge design is that the time period at which the participant is recruited into the study enters the analysis as a fixed or random effect (see Hussey &amp;amp; Hughes, 2007, section 3.1 &amp;amp; Discussion). This is to alleviate potential confounding with time, as might typically occur with a simple pre-post design. Such confounding will not arise in your design.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2013-09-02T05:41:46.203" Id="68948" LastActivityDate="2013-09-02T05:41:46.203" OwnerUserId="29796" ParentId="67999" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You are correct about not wanting to make inferences about individual differences based on the group aggregates (you are avoiding &lt;a href=&quot;http://en.wikipedia.org/wiki/Ecological_fallacy&quot; rel=&quot;nofollow&quot;&gt;ecological fallacy&lt;/a&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, there are no problems with aggregating individual data to generate group means (&quot;summing the total score for each group and then dividing to provide a score for each group&quot;). Means could not be calculated if we didn't do this with individual data. Good questions to ask though!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-02T06:03:00.830" Id="68950" LastActivityDate="2013-09-02T15:44:01.813" LastEditDate="2013-09-02T15:44:01.813" LastEditorUserId="29068" OwnerUserId="29068" ParentId="68949" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="Population average model assesses the association between X and Y based on averaging over the random effects, rather than the one for an individual subject (subject specific model)." CommentCount="0" CreationDate="2013-09-02T14:41:40.670" Id="68981" LastActivityDate="2013-09-02T17:12:46.983" LastEditDate="2013-09-02T17:12:46.983" LastEditorUserId="21599" OwnerUserId="21599" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;As I expected, no one probably uses MI to perform variable selection, and thus I will stick with other methods such as variable clustering, random forest, PCA, correlation assessment among potential predictor variables, and other methods in lieu of using MI to deal with high dimensionality. &lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, MI is likely not that useful for continuous variables, which encompass a large percentage of the candidate predictor variables in a typical dataset that I work with, so I am likely much better off without making using of MI as another variable selection technique even if it is a worthwhile approach to do so with continuous potential predictor variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;PLEASE correct me if I am wrong, as I would enjoy exploring a new technique (at least to me) of avoiding the use of stepwise techniques to reduce the number of candidate variables in my logistic regression models!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-02T14:42:44.170" Id="68982" LastActivityDate="2013-09-02T14:51:57.073" LastEditDate="2013-09-02T14:51:57.073" LastEditorUserId="29068" OwnerUserId="29068" ParentId="68129" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of distributions that are sampled. How do I measure their similarity between themselves or as a group/set?&lt;/p&gt;&#10;&#10;&lt;p&gt;So, given a finite domain for a sampled function$F_j(x)$, with $x_i \in {1,\ldots,N}$ and $j \in {1, \ldots, J}$. I would like a measure $\cdot(F_{1,J})$ for how similar the distributions are, or test of whether whether they are statistically close enough, eg. a test producing some p-value. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-09-02T16:16:06.843" Id="68989" LastActivityDate="2013-09-02T17:58:37.210" OwnerUserId="1098" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;sampling&gt;&lt;p-value&gt;&lt;sample&gt;&lt;similarities&gt;" Title="What methods exist to measure how similar a set of sampled distributions are to each other?" ViewCount="97" />
  
  <row Body="&lt;p&gt;Your acf of the seasonally differenced series strongly suggests the need for a regular difference. What follows is the acf of the doubly differenced series ( your series reularly differenced) &lt;img src=&quot;http://i.stack.imgur.com/z3YMi.jpg&quot; alt=&quot;enter image description here&quot;&gt; . THis acf suggests an autoregressive seasonal factor as the partial acf of lag 12 and 24 are apparently significant although since the seasonal coefficient will be between -.4 and +.4 it reall won't matter if you call it a seasnal ma of lag12. When models are tried including an ma(1) statistical significance for the ma(1) is rejected and is thusly not included. A good final model should include some indicators for anonmalous data .&lt;img src=&quot;http://i.stack.imgur.com/rAZXY.jpg&quot; alt=&quot;enter image description here&quot;&gt; , So in conlusion your preferred model is in your terms (0,1,0)(1,1,0) OR (0,1,0)(0,1,1). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-02T17:42:58.767" Id="68997" LastActivityDate="2013-09-02T17:42:58.767" OwnerUserId="3382" ParentId="68966" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There appears to be a difference in terminology (as so often is the case with a discipline used in so many areas), so I'm not 100% sure, but I think they're referring to &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_density_estimation&quot; rel=&quot;nofollow&quot;&gt;kernel density estimation&lt;/a&gt;&lt;/em&gt;, with a Gaussian kernel, but performed on binned data.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Edit: if someone familiar with how the term &quot;Gaussian smearing&quot; is used in physics - and how it would apply in relation to a histogram - could chime in, that would be good]&lt;/p&gt;&#10;&#10;&lt;p&gt;In that case, each observation is replaced with a scaled density (or more generally, a kernel, which needn't necessarily be positive), which has been scaled to area $\frac{1}{n}$, centered on the observation. &lt;/p&gt;&#10;&#10;&lt;p&gt;The scale (the standard deviation in the case of a Gaussian) of the kernel is a tunable smoothing parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Presumably the intent is to place observations at bin-centers before smoothing (this discreteness leads to a need for larger kernel bandwidth to achieve a given amount of smoothness, though if the bins are small relative to the default bandwidth the effect is very small); another alternative would be to spread the observations throughout their bins, though I suspect this isn't the intent of the request in your case.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the unbinned case, here's an illustration of the idea. Below are 7 observations&lt;br&gt;&#10;(5.996 22.687  8.868 15.883 14.727  5.896  9.397) marked with &quot;+&quot; symbols (there are two almost coincident at the far left), with one point (14.727) marked in bold red. The little red density there is the gaussian kernel of bandwidth 2.5; there are gray kernels placed above each of the other points. The green dashed curve is the kernel density estimate, which came from summing up the densities at a fine grid of points and joining to give a smooth curve. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y4CwN.png&quot; alt=&quot;kernel density estimate&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The bandwidth I used in my illustration is a little small. There are automatic methods for choosing the bandwidth that usually do pretty well. I did my calculations in R; like many other stats packages, it comes with a function to automate the whole procedure, so I could do the following to get a plot like the green dashed one:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a = c(5.996, 22.687, 8.868, 15.883, 14.727, 5.896, 9.397)&#10;plot(density(a,bw=2.5))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Below is the kernel density estimate generated by the default bandwidth in the same program (i.e. by using &lt;code&gt;density(a)&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WVWye.png&quot; alt=&quot;default bandwidth&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you see, it's wider, and achieves a smoother looking result. Of course, 7 observations is unrealistic; here's a histogram and kernel density estimate for 200 observations from a gamma(10,1):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/y0WA8.png&quot; alt=&quot;hist and kde for gamma(10) r.v., n = 200&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you click on the &lt;code&gt;kde&lt;/code&gt; tag that your post has (I believe it was added in an edit), you'll see many posts on the topic here. There's also a &lt;code&gt;kernel&lt;/code&gt; tag by the look of it, which mostly seems to be doing the same job (though &lt;code&gt;kernel&lt;/code&gt; can refer to other things than density estimation), and that has more hits still.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-02T22:51:47.617" Id="69016" LastActivityDate="2013-09-05T07:21:31.513" LastEditDate="2013-09-05T07:21:31.513" LastEditorUserId="805" OwnerUserId="805" ParentId="68999" PostTypeId="2" Score="11" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have monthly returns data going back to 1991 and I'm trying to work out if the data has a tendency to mean revert over time.  In order to work this out I've used the Augmented Dickey Fuller test on GRETL - an example of my output is here:&#10;&lt;img src=&quot;http://i.stack.imgur.com/ifOUO.jpg&quot; alt=&quot;output&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've also run checks on regression models without a trend, and without constant (this form had the lowest p value). I'm now struggling to understand what this output means.  I know that the stars refer to how significant the coefficients are, but what does everything else mean. I'm guessing the constant refers to the mean it trends back to? How does the time term  fit into this?  What impact do the lags have?  How could I plot this regression output on the graph of monthly returns?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry, I know this is all pretty basic stuff but I'm really struggling.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-03T15:00:36.767" Id="69070" LastActivityDate="2013-12-02T21:49:58.393" LastEditDate="2013-12-02T21:49:58.393" LastEditorUserId="88" OwnerUserId="29860" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;stationarity&gt;&lt;augmented-dickey-fuller&gt;" Title="Explaining augmented Dickey-Fuller regression output" ViewCount="404" />
  <row AnswerCount="0" Body="&lt;p&gt;In a book chapter I am reading&lt;sup&gt;1&lt;/sup&gt; the authors compare the fit of the relationship between biodiversity and ecosystem function using both a log-linear model and a Michaelis-Menten model.  The authors refer to the Michaelis-Menten model as &quot;saturating&quot; and the log-linear model as non-saturating.  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;...authors of the publications used in [Meta-analysis 1] did not distinguish between non-linear curves that do (Michaelis-Menten) and do not saturate (log-linear).  In [Meta-analysis 2] this distinction could be made and showed that the average R&lt;sup&gt;2&lt;/sup&gt; value was 0.690 and 0.682 for the saturating Michaelis-Menten and log-linear relationship, respectively.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My initial investigation into these models (via Wikipedia etc...) appears to show very similar behavior in that they both approach an asymptote with increasing X-values. Furthermore, I am not sure what the authors are referring to with respect to the saturating and non-saturating descriptions because both models appear to saturate. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is, how should I interpret the relationship differently if it fits one of these models versus the other?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;NOTE: I have looked up the models and while I can somewhat understand each, I lack the training to relate them mathematically.  However, I am looking for a non-mathematical comparison of what fitting one or the other would indicate about the relationship they describe.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Schmidt, B. et al. 2008. &quot;&lt;a href=&quot;http://www.zora.uzh.ch/25528/1/Schmid_etal_Naeem09_V.pdf&quot; rel=&quot;nofollow&quot;&gt;Consequences of species loss for ecosystem functioning: meta-analyses of data from biodiversity experiments&lt;/a&gt;&quot; in Naeem et al. eds. 2008. Biodiversity, Ecosystem Functioning &amp;amp; Human Wellbeing: an ecological and economic perspective. Oxford University Press.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="12" CreationDate="2013-09-03T18:40:41.390" Id="69092" LastActivityDate="2013-09-04T00:55:41.493" LastEditDate="2013-09-04T00:55:41.493" LastEditorUserId="805" OwnerUserId="4048" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;nonlinear-regression&gt;" Title="What is the qualitative difference between a Michaelis-Menten model and a log-linear model?" ViewCount="114" />
  <row AcceptedAnswerId="69095" AnswerCount="1" Body="&lt;p&gt;My test recorded the number of times a variable reached a threshold value when a predetermined event occurred. Then, the test recorded the number of times the variable surpassed the threshold value by 1 level when the same event occurred. The test progressed in this manner with the highest observation at level 22. The resulting frequency distribution follows (sample size was 9,970 observations):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;0:3468 &amp;lt;= the threshold level&lt;/li&gt;&#10;&lt;li&gt;1:2163 &amp;lt;= level 1&lt;/li&gt;&#10;&lt;li&gt;2:1408&lt;/li&gt;&#10;&lt;li&gt;3:1005&lt;/li&gt;&#10;&lt;li&gt;4:641&lt;/li&gt;&#10;&lt;li&gt;5:487&lt;/li&gt;&#10;&lt;li&gt;6:266&lt;/li&gt;&#10;&lt;li&gt;7:196&lt;/li&gt;&#10;&lt;li&gt;8:135&lt;/li&gt;&#10;&lt;li&gt;9:71&lt;/li&gt;&#10;&lt;li&gt;10:49&lt;/li&gt;&#10;&lt;li&gt;11:30&lt;/li&gt;&#10;&lt;li&gt;12:16&lt;/li&gt;&#10;&lt;li&gt;13:6&lt;/li&gt;&#10;&lt;li&gt;14:10&lt;/li&gt;&#10;&lt;li&gt;15:8&lt;/li&gt;&#10;&lt;li&gt;16:3&lt;/li&gt;&#10;&lt;li&gt;17:4&lt;/li&gt;&#10;&lt;li&gt;18:0&lt;/li&gt;&#10;&lt;li&gt;19:1&lt;/li&gt;&#10;&lt;li&gt;20:1&lt;/li&gt;&#10;&lt;li&gt;21:1&lt;/li&gt;&#10;&lt;li&gt;22:1 &amp;lt;= level 22&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Note that the frequency of Level 1 observations do not include the frequency of Level 0 (the threshold) observations. Finally, in order for the variable to reach subsequently higher levels, it must pass through lower levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;During runtime, given that the variable has reached the threshold level(0), what's the probability that the variable will continue to level 1? to level 10?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-03T18:53:55.590" Id="69094" LastActivityDate="2013-09-03T19:07:54.780" OwnerUserId="29869" PostTypeId="1" Score="1" Tags="&lt;conditional-probability&gt;&lt;frequentist&gt;&lt;frequency&gt;" Title="Given a frequency distribution, calculate probability of A given B" ViewCount="334" />
  <row AnswerCount="1" Body="&lt;p&gt;I have some concerns related to the use of &lt;code&gt;nntool&lt;/code&gt; in MATLAB toolbox. Following links like &lt;a href=&quot;http://stackoverflow.com/q/11807184/395857&quot;&gt;this&lt;/a&gt;, I have found that &lt;code&gt;nntool&lt;/code&gt; by default normalizes the inputs to the range &lt;code&gt;[-1 1]&lt;/code&gt;. So I am a bit concerned, I created a neural network with &lt;code&gt;tansig&lt;/code&gt; activation in the first layer and &lt;code&gt;logsig&lt;/code&gt; activation in the output layer. I manually normalized the outputs to the range of &lt;code&gt;[0 1]&lt;/code&gt; in the data and fed it to &lt;code&gt;nntool&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now my question is does &lt;code&gt;nntool&lt;/code&gt; further normalizes it to the range &lt;code&gt;[-1 1]&lt;/code&gt;. If it does then it is not correct, the output of &lt;code&gt;logsig&lt;/code&gt; cannot be in the range of &lt;code&gt;[-1 1]&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using the &lt;code&gt;newfit&lt;/code&gt; function like this &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;load house_dataset;&#10;net = newfit(houseInputs,houseTargets,20);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-03T20:14:18.757" Id="69101" LastActivityDate="2013-10-04T14:39:05.613" LastEditDate="2013-10-04T14:39:05.613" LastEditorUserId="930" OwnerUserId="12329" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;matlab&gt;&lt;neural-networks&gt;" Title="Concerns related to neural network matlab toolbox" ViewCount="353" />
  
  
  
  
  <row AcceptedAnswerId="69159" AnswerCount="2" Body="&lt;p&gt;I'm doing principal component analysis on my dataset and my professor told me that I should normalize the data before doing the analysis. My question is: &quot;Why?&quot; :) &lt;/p&gt;&#10;&#10;&lt;p&gt;What would happen If I did PCA without normalization? Why do we normalize data in general? Could someone give clear and intuitive example which would demonstrate the consequences of not normalizing the data before analysis etc.?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help =) &lt;/p&gt;&#10;" ClosedDate="2013-09-04T17:51:16.447" CommentCount="6" CreationDate="2013-09-04T08:12:31.617" FavoriteCount="3" Id="69157" LastActivityDate="2013-09-04T14:14:48.123" OwnerUserId="18528" PostTypeId="1" Score="6" Tags="&lt;pca&gt;&lt;normalization&gt;" Title="Why do we need to normalize data before analysis" ViewCount="11999" />
  <row Body="&lt;p&gt;I think you should definitely look into more metrics than just AUC and accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Accuracy (together with sensitivity and specificity) is a very simple but biased metric which forces you to look at the absolute prediction result and does not open for assertion of class probabilities or ranking. It also does not take the population into account which invites to misinterpretation as a model giving a 95% accuracy on a population with 95% chance of being correct at random isn't really a good model, even if the accuracy is high. &lt;/p&gt;&#10;&#10;&lt;p&gt;AUC is a good metric for asserting model accuracy that is independent of population class probabilities. It will, however not tell you anything about how good the probability estimates actually are. You could get a high AUC but still have very skewed probability estimates. This metric more discriminating than accuracy and will definitely give you better models when used in combination with some proper scoring rule, e.g Brier score as mentioned in another post. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can get a more formal proof here, although this paper is quite theoretical: &lt;a href=&quot;http://cling.csd.uwo.ca/papers/ijcai03.pdf&quot; rel=&quot;nofollow&quot;&gt;AUC: a Statistically Consistent and more Discriminating Measure than Accuracy&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are however a bunch of good metrics available.&#10;&lt;a href=&quot;http://www-stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf&quot; rel=&quot;nofollow&quot;&gt;Loss Functions for Binary Class Probability Estimation&#10;and Classiﬁcation: Structure and Applications&lt;/a&gt; is a good paper investigaing proper scoring rules such as the Brier score. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another interesting paper with metrics for assertion of model performance is &lt;a href=&quot;http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf&quot; rel=&quot;nofollow&quot;&gt;Evaluation: from precision, recall and F-measure to ROC, &#10;informedness, markedness &amp;amp; correlation&lt;/a&gt; taking up other good performance metrics such as informedness. &lt;/p&gt;&#10;&#10;&lt;p&gt;To summarize I would recommend looking at AUC/Gini and Brier score to assert you model performance, but depending on the goal with your model other metrics might suit your problem better.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-04T08:13:51.847" Id="69158" LastActivityDate="2013-09-04T08:13:51.847" OwnerUserId="17481" ParentId="58756" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using stochastic gradient descent for minimizing an objective function of the following form&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/b2km3.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;with such an R script&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(rootSolve)&#10;&#10;zz &amp;lt;- c(86.02,86.85,87.48,88.69,88.82,87.46,87.4,87.6,&#10;        88.38,86.91,88.02,89.4,89.96,90.52,91.07,91.03,&#10;        90.14,90.1,91.11,91.24,90.7,91.76,92.52,92.8,&#10;        92.19,91.79,91.53,92.38,92.08,92.09,90.47,91.46,&#10;        90.79,89.65,90.23,91.45,91.12,114.67,114.97,115.64,&#10;        116.54,117.33,117.56,119.08,120.35,121.03,119.31,&#10;        121.06,122.5,122.26,122.06,122.37,122.65,122.24,&#10;        121.87,124,123.01,122.76,123.42,124.78,125.94,124.93,&#10;        125.4,125.45,126.43,125.49,125.11,122.75,124.09,122.96,&#10;        121.57,120.59,121.7,122.69)&#10;zz &amp;lt;- matrix(zz,ncol=2)&#10;nu &amp;lt;- 0.000002076897&#10;lambda &amp;lt;- 2&#10;m &amp;lt;- 5&#10;xprev &amp;lt;- c(0.5471319,0.4390035)&#10;yy &amp;lt;- NULL&#10;g &amp;lt;- function(x,y1,y2) sum(y1*x[1]-y2*x[2])^2-lambda*sum((y1*x[1]-y2*x[2])^2)&#10;for(i in m:nrow(zz)){&#10;    y &amp;lt;- zz[(i-m):i,]&#10;    xprev &amp;lt;- xprev - nu*gradient(g,xprev,y1=y[,1],y2=y[,2])&#10;    yy &amp;lt;- rbind(yy,xprev)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Usually it works fine and found parameters are in the reasonable bounds, but sometimes they start to explode for some reason. For example here I used about 1300 data points during the in-sample period to learn the model (nu,lambda,m) and got the following x's values dynamics (which is normal)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Q81TF.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;but after about 600 data points during out-of-sample period where data didn't undergo any significant changes (as I can visually detect it) x's values started to explode&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tPNJI.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;for this particular data set&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0t9fw.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please help me to understand what can be the reason of such an explosion and maybe how to struggle with it.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-09-04T08:53:06.303" Id="69161" LastActivityDate="2015-02-16T14:10:22.500" LastEditDate="2015-02-16T14:10:22.500" LastEditorUserId="53618" OwnerUserId="29890" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;machine-learning&gt;" Title="Explosion in stochastic gradient descent" ViewCount="121" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is it possible to compute the Jensen-Shannon divergence between a discrete and a continuous probability distribution, e.g. between a standard normal distribution and a distribution taking the values 1,2,3 each with probability 1/3? Or are there any other divergences which can be used as a similarity measure of discrete and continuous distributions.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-04T09:47:12.497" Id="69164" LastActivityDate="2013-09-04T09:47:12.497" OwnerUserId="18530" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;entropy&gt;&lt;distance&gt;&lt;similarities&gt;" Title="Computing Jensen-Shannon Divergence between discrete and continuous distribution" ViewCount="155" />
  
  
  
  <row AcceptedAnswerId="69186" AnswerCount="1" Body="&lt;p&gt;Assume that $ X_1 , ..., X_n $&#10;is an i.i.d. sample from the population described by a density function $f_x (x,\theta) $ . &lt;/p&gt;&#10;&#10;&lt;p&gt;Is the sample sufficient for $ \theta $ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know it is a trivial question, but I am in doubt!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-04T12:18:23.820" Id="69184" LastActivityDate="2013-09-04T15:29:23.230" OwnerUserId="29696" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;&lt;inference&gt;" Title="Is the sample sufficient?" ViewCount="75" />
  <row Body="&lt;p&gt;Are you talking about sufficient statistics? If so, then you might be asking whether the identity function is a sufficient statistic. But note that the question is somehow ill-posed (read last paragraph).&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer to this question is trivially yes: A statistic is sufficient if it provides at least as much information about a parameter as every other statistic you could compute from it. It basically means you throw nothing important away. The identity function throws nothing away, so it also doesn't throw anything important away.&lt;/p&gt;&#10;&#10;&lt;p&gt;But note that a statistic is a value that is obtained by applying some function to a sample. You asked whether a sample is sufficient, but a sample is not a value computed from a sample, so the question is somehow invalid. For example &lt;em&gt;sample mean&lt;/em&gt; is a statistic and can be computed for a given sample. Sample mean is a sufficient statistic for the mean of a normal distribution. So given some sample $X_1,...,X_n$ from a normal distribution, it is enough to look at the sample mean when you are interested in the distribution mean. Of course the whole sample contains the information of the sample mean, so keeping the whole sample is also sufficient.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-04T12:34:03.400" Id="69186" LastActivityDate="2013-09-04T15:29:23.230" LastEditDate="2013-09-04T15:29:23.230" LastEditorUserId="3293" OwnerUserId="3293" ParentId="69184" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I would suggest the following approach:&lt;/p&gt;&#10;&#10;&lt;p&gt;First run unpaired t tests on each of the 50 proteins. Record the P value from each comparison, but ignore any statements about statistical significance. &lt;/p&gt;&#10;&#10;&lt;p&gt;You next will divide the 50 proteins into two groups, those where you suspect the difference is real (&quot;discoveries&quot;) and those where you suspect the difference is a coincidence. To decide where to draw the line, you need to choose the maximum false discovery rate (FDR) you feel comfortable with. This is the probability that a protein that you think shows a real difference is in fact a coincidence of random sampling, a false discovery. &lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have decided on your desired maximum FDR (a fraction between 0 and 1, usually abbreviated Q), you can easily divide your 50 proteins into &quot;discoveries&quot; and &quot;not discoveries&quot; using the method of &lt;a href=&quot;http://engr.case.edu/ray_soumya/mlrg/controlling_fdr_benjamini95.pdf&quot; rel=&quot;nofollow&quot;&gt;Benjamini and Hochberg&lt;/a&gt;. Briefly, the method first sorts the P values from high to low. The comparison with the largest P value is considered to be a &quot;discovery&quot; if it is less than Q. The second largest is a discovery if the P value is less than Q*(24/25), ... and the comparison with the smallest P value is a discovery if it is less than Q/25. (Why 25? Because there are 25 comparisons in your example). Once you find a comparison to be a &quot;discovery&quot;, also call all other comparisons with smaller P values to be discoveries (without doing more calculations). &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the term &quot;significant&quot; is not used with this FDR method.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.graphpad.com/prism&quot; rel=&quot;nofollow&quot;&gt;GraphPad Prism 6&lt;/a&gt; can do all this with its multiple t test analysis. But so can lots of programs. It also wouldn't be hard to do it all with Excel. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-04T14:24:05.287" Id="69201" LastActivityDate="2013-09-04T14:24:05.287" OwnerUserId="25" ParentId="24400" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="69234" AnswerCount="1" Body="&lt;p&gt;Imaginary example, but should work well for the purpose of this question:&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm doing a simple linear regression (or a correlation, with 1 IV and 1 DV) for 2 variables, for example the number of steps in the staircase vs reported satisfaction of achieving the top by participants.&#10;Let's say I'm taking 4 different staircases into account: 3-steps; 5-steps; 10-steps; and 50-steps high. The point is, this isn't a categorical, but a ratio scale (ignore the other problems with this sample of staircases, such as far-from-normal distribution, etc).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, let's say I have 10 participants climbing these staircases in random order each, and reporting their satisfaction in a number from 1 to 7.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 2 ways to analyse this data. One would be to average responses for each staircase. So say we get the results:&lt;/p&gt;&#10;&#10;&lt;p&gt;3-steps:  1.6&lt;/p&gt;&#10;&#10;&lt;p&gt;5-steps:  2.3&lt;/p&gt;&#10;&#10;&lt;p&gt;10-steps: 3.4&lt;/p&gt;&#10;&#10;&lt;p&gt;50-steps: 6.1&lt;/p&gt;&#10;&#10;&lt;p&gt;Our scatterplot would be very simple:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3C6qH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But maybe averaging the results is unnecessary and kills the variation in the data? We could leave the raw scores as they were and then plot them, like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bk1Z2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How we approach it also affects Person's r (or Spearman's rho since we had non-normal distribution of stair number). Obviously degrees of freedom (and therefore p values) also change.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a one-and-always-true definite answer to how this should be done?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-04T15:46:05.593" Id="69204" LastActivityDate="2013-09-04T20:05:41.920" OwnerUserId="29915" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="Correlating two variables at different levels" ViewCount="42" />
  
  <row Body="&lt;p&gt;In my opinion you need to make a series of nested models. If the goal is to identify influential variables - then compare models sequentially with likelyhood ratio test (or other appropriate test). If the goal is to obtain predictive model - then compare cross-validation statistics, or make test set and evaluate predictivity with it.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-04T17:03:38.410" Id="69216" LastActivityDate="2013-09-04T17:03:38.410" OwnerUserId="8165" ParentId="69208" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Answered by @whuber.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;According to the legend for that graphic, &quot;simple coincidence&quot; is&#10;  synonymous with &quot;expected from pure chance.&quot; The latter might mean&#10;  that if we assume the answers to all three questions are independent&#10;  and if a random man's answers are matched to a random woman's answers,&#10;  then they should match 32%/3.7 of the time.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CommunityOwnedDate="2013-09-04T21:17:02.627" CreationDate="2013-09-04T21:17:02.627" Id="69238" LastActivityDate="2013-09-04T21:17:02.627" OwnerUserId="29617" ParentId="10249" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The problem with what you intend to quantify, is that you would need to record hashtags in pre-defined and recurring time-points, otherwise you will not be able to draw valid inferences.&#10;For example, if the second #goodMood was recorded at September 3, 2013 &lt;strong&gt;11&lt;/strong&gt;:00 AM, but no record of any hashtag was made at September 3, 2013 &lt;strong&gt;9&lt;/strong&gt;:00 AM, then you would not be able to say, &quot;hmm, not much clumpiness here&quot; (the scientifically recognizable word is &quot;clustering&quot;) - because you could not say whether #goodMood has already set in at 9:00 AM, or not...  &lt;/p&gt;&#10;&#10;&lt;p&gt;...except if there is some (actually observed) rule that says that &quot;I record a hashtag the moment I feel it sets in&quot;. In such a case, you could easily create a data sample with time intervals (#goodmod set in 2 hours after exercise, 3 hours after exercise, 1 hour after exercise etc)- and then go on and calculate their mean, variance, and other statistics of interest. This would give you for example the average time interval between exercise and #goodMood -an absolute figure, which could then be compared to other central tendency statistics from other pairs, and create a ranking.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-04T22:55:49.877" Id="69247" LastActivityDate="2013-09-04T22:55:49.877" OwnerUserId="28746" ParentId="69192" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I've seen examples of constructing multivariate distribution with univariate marginals coupled together via a normal copula (see &lt;a href=&quot;http://cran.cermin.lipi.go.id/web/packages/copula/copula.pdf&quot; rel=&quot;nofollow&quot;&gt; Mvdc &lt;/a&gt; function from &lt;code&gt;copula&lt;/code&gt; package in R). However I was wondering whether I could simulate density with multivariate marginals via a normal copula? Thank you!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Just to clarify how I attempted to solve this problem without copula:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $vec(X_i^T) \sim N_{n \times p} (0, \Sigma_i \otimes I_p ), \, i = 1, \dots, 3$, and $\Psi ( 3x3)$ is the design covariance matrix for the samples. Let L be a $3\times 3$ matrix such that $L L^T = \Psi$, then $Y = X L $, where $X = (vec(X_1^T) \cdots vec(X_3^T))$ would be the desired correlated sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;However my concern is that this implies $Y$ follows $N_{np,3} (0, V, \Psi)$, where $V$ is the common row covariance matrix. This contradicts with my original setup. &lt;/p&gt;&#10;&#10;&lt;p&gt;Am I wrong about this? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-05T03:29:12.407" Id="69258" LastActivityDate="2013-09-05T23:41:38.683" LastEditDate="2013-09-05T23:41:38.683" LastEditorUserId="28640" OwnerUserId="28640" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;copula&gt;&lt;random-matrix&gt;" Title="Generate correlated multivariate normal samples with copula" ViewCount="169" />
  <row Body="&lt;p&gt;You seem to have gone on a convoluted route to asking for how to assess independent count data. You've got 100 independent items.  There are 40 males that clicked, 10 males that didn't click, 10 non-males that clicked, and 40 non-males that didn't click. You can easily construct what is called a contingency table (below) from those data and do a $\chi^2$ (chi-square) test for independence. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         male non-male&#10;click    40   10&#10;no click 10   40&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Searching for the chi-square test on the internet will show you the formulas, logic, and even online calculators that can solve the problem.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-05T07:08:48.853" Id="69266" LastActivityDate="2013-09-05T07:08:48.853" OwnerUserId="601" ParentId="69259" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Well, not a scaling, but you (implicitly) have &lt;code&gt;centered = TRUE&lt;/code&gt;, so you need to undo the effect of centering after the rotation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot (x, asp = 1, col = 3)&#10;transformed &amp;lt;- pcX$x %*% t (pcX$rotation)&#10;transformed &amp;lt;- scale (transformed, center = -pcX$center, scale = FALSE)&#10;points (transformed, col = 2, pch = 19, cex = 0.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;reconstructs your original data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you want to use only PC 1, you need to multiply score 1 (no score 2 involved!) with the inverse (for PCA = transpose) of loading 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot (x, asp = 1, col = 3, pch = 19, cex = 0.5)&#10;transformed  &amp;lt;- pcX$x [,1] %*% t (pcX$rotation [1,]&#10;transformed &amp;lt;- scale (transformed), center = -pcX$center, scale = FALSE)&#10;points (transformed, col = 2, pch = 19, cex = 0.5)&#10;segments (x [,1],x [,2], transformed [,1], transformed [,2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HRCsn.png&quot; alt=&quot;PCA projection&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the segments will only show an orthogonal projection if the plot has &lt;code&gt;asp = 1&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-05T07:56:40.727" Id="69268" LastActivityDate="2013-09-05T07:56:40.727" OwnerUserId="4598" ParentId="69251" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Asymptotic theory is concerned with the properties of estimators and test statistics in large samples which are assumed to tend towards infinity in size. This allows to obtain complicated estimators and tests which would not be available in small samples. Note that asymptotic theory is only an approximation with small samples, and it is not always a good approximation. Examples of asymptotic properties of estimators are consistency, regularity or their asymptotic distribution. Frequently used concepts in asymptotic theory include the weak and strong law of large numbers, the central limit theorem, certain classes of expansions (e.g. Taylor, Edgeworth, von Mises) and the Delta method.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-05T15:10:55.473" Id="69300" LastActivityDate="2013-09-05T15:24:37.180" LastEditDate="2013-09-05T15:24:37.180" LastEditorUserId="7290" OwnerUserId="26338" PostTypeId="5" Score="0" />
  <row Body="Asymptotic theory studies the properties of estimators and test statistics when the sample size approaches infinity." CommentCount="0" CreationDate="2013-09-05T15:10:55.473" Id="69301" LastActivityDate="2013-09-05T15:25:22.357" LastEditDate="2013-09-05T15:25:22.357" LastEditorUserId="7290" OwnerUserId="26338" PostTypeId="4" Score="0" />
  
  
  
  <row Body="&lt;p&gt;There's nothing wrong with three charts, each answering one of your questions, over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The JIRA chart you cite is a specialization of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Cumulative_flow_diagram&quot; rel=&quot;nofollow&quot;&gt;Cumulative Flow Diagram&lt;/a&gt;, for which you should be able to find a lot of information about. &lt;/p&gt;&#10;&#10;&lt;p&gt;The danger of such area charts is that while the critical measures are represented as horizontal and vertical distances, our perception emphasizes the shortest distance between the curves. A famous example is discussed at &lt;a href=&quot;http://blog.bissantz.com/vis-a-vis&quot; rel=&quot;nofollow&quot;&gt;blog.bissantz.com/vis-a-vis&lt;/a&gt;, including a look at two curves together compared with their vertical distance:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Dn76h.png&quot; alt=&quot;enter image description here&quot;&gt;   Red - Blue = Green: &lt;img src=&quot;http://i.stack.imgur.com/qnyD0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the middle section the red and blue lines seem to have a constant separation, but there is a significant spike in the vertical distance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-05T16:53:30.210" Id="69312" LastActivityDate="2013-09-05T16:53:30.210" OwnerUserId="1191" ParentId="69245" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The requester is probably referring to a kernel density smoother as Glen_b notes, but &quot;smearing&quot; is evocative of the &lt;em&gt;shadowgram&lt;/em&gt; featured in the book &lt;a href=&quot;http://www.visualstats.org/&quot;&gt;Visual Statistics&lt;/a&gt;. To address the issue of choosing the right bin or kernel width, the shadowgram is a overlay of many different width choices.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/OPciB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-05T19:45:49.030" Id="69320" LastActivityDate="2013-09-05T19:45:49.030" OwnerUserId="1191" ParentId="68999" PostTypeId="2" Score="5" />
  
  
  
  <row AcceptedAnswerId="69355" AnswerCount="1" Body="&lt;p&gt;Ljung Box test tells that the following time series is white noise (&lt;code&gt;p=0.9746845&lt;/code&gt; for the current run). How could this be?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x=rep(10,1000)&#10;x[500]=-10&#10;Box.test(x,type=&quot;Ljung-Box&quot;)$p.value&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6VSlm.jpg&quot; alt=&quot;The time series&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-09-06T07:47:18.277" FavoriteCount="0" Id="69352" LastActivityDate="2015-02-19T13:23:34.917" LastEditDate="2015-02-19T13:23:34.917" LastEditorUserId="17230" OwnerUserId="17748" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;hypothesis-testing&gt;&lt;white-noise&gt;" Title="Strange results of Ljung-Box test (for white noise process)" ViewCount="422" />
  <row Body="&lt;p&gt;Like other hypothesis tests, the Ljung&amp;ndash;Box test doesn't tell us the null hypothesis is true, or even likely to be true; but calibrates a test statistic measuring departure from the null hypothesis in a direction of interest by telling us how probable it is that it would exceed (or equal) the value observed under hypothetical repetitions of the experiment or study, &lt;em&gt;if&lt;/em&gt; the null hypothesis were true. The L&amp;ndash;B test statistic uses the sum of squared sample autocorrelations up to a given lag to investigate autocorrelation in the time series, given a null hypothesis of no autocorrelation at any lag.  In your case the spike at $x=500$ gets averaged out in the sample autocorrelations &amp;amp; hardly affects the test statistic at all (check the auto-correlation plot with &lt;code&gt;acf&lt;/code&gt;). This time series isn't white noise because its mean is far from zero (do a $t$-test if you like). (It's also not Gaussian noise, clearly.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The moral of the story is that there's no single test for &lt;em&gt;every&lt;/em&gt; way the null hypothesis can be wrong. Use an outlier test when you want to test for outliers, a location test to test for zero mean, &amp;amp; the L&amp;ndash;B test to test for non-zero auto-correlations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-06T08:35:13.873" Id="69355" LastActivityDate="2013-09-07T11:21:16.907" LastEditDate="2013-09-07T11:21:16.907" LastEditorUserId="17230" OwnerUserId="17230" ParentId="69352" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="69362" AnswerCount="1" Body="&lt;p&gt;In bivariate linear regression is there a direct relationship between sample size $n$, coefficient of determination $r^2$ and $\sigma_\beta$ (the standard error of coefficient $\beta$)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Assume data have been normalized so both target and predictor variable have $\sigma=1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Putting the question another way, does $\sigma_\beta$ tell me something different to $r^2$ or are they measures of the same thing?  Or, is it possible to have a strong, certain but unreliable link between two variables (large $\beta$, small $\sigma_\beta$, but small $r^2$)?&lt;/p&gt;&#10;&#10;&lt;p&gt;(In multiple regression this doesn't apply as even with high $r^2$, $\sigma_\beta$ can indicate uncertainty as to which of the multiple predictors is causing the response).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Just got this out of my software (without standardized data):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;regression coeff 0.023&#10;stderr of coeff 0.0046&#10;p=0.000002&#10;&#10;n=131&#10;multiple r2=0.17&#10;adjusted r2=0.16&#10;&#10;predictor std=22.5&#10;target std=2.24&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Standardized coefficient is presumably &lt;/p&gt;&#10;&#10;&lt;p&gt;$0.023*22.5/2.24 = 0.23$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If standardized coefficient is the same as correlation, then &lt;/p&gt;&#10;&#10;&lt;p&gt;$r^2 = 0.23^2 = 0.053$&lt;/p&gt;&#10;&#10;&lt;p&gt;...not the same as the software gave.  What am I doing wrong?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-09-06T10:18:44.777" Id="69360" LastActivityDate="2013-09-06T11:49:01.830" LastEditDate="2013-09-06T11:28:30.680" LastEditorUserId="8515" OwnerUserId="8515" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="In bivariate linear regression is there a direct relationship between $n$, $r^2$ and coefficient error?" ViewCount="135" />
  <row AcceptedAnswerId="69376" AnswerCount="2" Body="&lt;p&gt;I have a set of predictors in a linear regression, as well as three control variables. The issue here is that one of my variables of interest is only statistically significant if the control variables are included in the final model. However, the control variables themselves are not statistically significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is how the multicollinearity of all my variables look like (including control variables):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; &amp;gt; vif(lm(return ~ EQ + EFF + SIZE + MOM + MSCR + UMP, data = as.data.frame(port.df)))&#10;       EQ      EFF     SIZE      MOM     MSCR      UMP &#10; 3.687171 3.481672 2.781901 1.064312 1.438596 1.003408&#10;&#10; &amp;gt; vif(lm(return ~ EQ + MOM + MSCR, data = as.data.frame(port.df)))&#10;       EQ      MOM     MSCR &#10; 1.359992 1.048142 1.412658 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My variables of interest are &lt;strong&gt;EQ, MOM and MSCR&lt;/strong&gt;, and the control variables are &lt;em&gt;EFF, SIZE and UMP&lt;/em&gt;. EQ is only significant if the three control var are included, and becomes insignificant when they are not:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Here are the coefficients (1rst row) and t-stats (2nd row) when control variables are included (notice that EQ is statistically significant)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       intercept           EQ          EFF        SIZE         MOM       MSCR          UMP&#10;[1,] 0.005206246 -0.006310531 0.0001229055 0.004125551 0.007738259 0.00473377 5.838596e-06&#10;[2,] 1.866628909 -1.746583234 0.0388823612 1.178460997 2.145062820 2.08131100 1.994863e-01&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Now, here is the result of the regression when the control variables are excluded (notice that EQ is NOT statistically significant anymore)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       intercept           EQ         MOM       MSCR&#10;[1,] 0.007313402 -0.002111833 0.007128606 0.00668364&#10;[2,] 2.652662996 -0.595391117 2.036985378 2.80177366&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The problem is that when I include my control variables, all my variables of interest are significant, but my control variables are not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which variables should I include in my final model? How should I structure my final model then, given the fact that the model will be used for forecasting?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you,&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2013-09-06T13:51:22.027" FavoriteCount="1" Id="69371" LastActivityDate="2013-09-17T17:16:20.380" LastEditDate="2013-09-06T13:59:32.343" LastEditorUserId="29183" OwnerUserId="29183" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;regression&gt;&lt;statistical-significance&gt;&lt;factor-analysis&gt;&lt;multicollinearity&gt;" Title="Should control variables be included in model if statistically insignificant?" ViewCount="961" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a pretty standard situation of a study in which repeated measurements are taken from the same individuals. There are two factors: &quot;Group&quot; (with 25 individuals in each of two groups) and &quot;Day&quot; (time is treated here as a categorical variable). To keep things simple, let's consider only two time points, Day 1 and Day 2. When working in R, the data would look as follows (ID - subjects' IDs; Group - labels for the groups; Day - factor indicating the day of sampling, with 2 levels; BW - body weight, kg):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; dat&#10;     ID Group   Day       BW&#10; 1   ID1     A Day 1 2333.231&#10; 2   ID2     A Day 1 2615.744&#10; 3   ID3     A Day 1 2282.484&#10; 4   ID4     A Day 1 2796.806&#10; 5   ID5     A Day 1 2262.759&#10; 6   ID6     A Day 1 2520.216&#10; 7   ID7     A Day 1 2606.598&#10; 8   ID8     A Day 1 2617.347&#10; 9   ID9     A Day 1 2439.651&#10; 10 ID10     A Day 1 2515.900&#10; 11 ID11     B Day 1 2692.253&#10; 12 ID12     B Day 1 2208.707&#10; 13 ID13     B Day 1 2343.652&#10; 14 ID14     B Day 1 2564.080&#10; 15 ID15     B Day 1 2411.044&#10; 16 ID16     B Day 1 2774.001&#10; 17 ID17     B Day 1 2634.651&#10; 18 ID18     B Day 1 2514.433&#10; 19 ID19     B Day 1 2198.449&#10; 20 ID20     B Day 1 2505.220&#10; 21  ID1     A Day 2 2314.214&#10; 22  ID2     A Day 2 2302.396&#10; 23  ID3     A Day 2 2319.029&#10; 24  ID4     A Day 2 2533.612&#10; 25  ID5     A Day 2 2290.300&#10; 26  ID6     A Day 2 2168.727&#10; 27  ID7     A Day 2 2466.597&#10; 28  ID8     A Day 2 2223.379&#10; 29  ID9     A Day 2 2441.762&#10; 30 ID10     A Day 2 2288.917&#10; 31 ID11     B Day 2 1984.846&#10; 32 ID12     B Day 2 2702.819&#10; 33 ID13     B Day 2 2793.834&#10; 34 ID14     B Day 2 2563.337&#10; 35 ID15     B Day 2 2666.664&#10; 36 ID16     B Day 2 2399.159&#10; 37 ID17     B Day 2 2586.255&#10; 38 ID18     B Day 2 2193.912&#10; 39 ID19     B Day 2 2797.592&#10; 40 ID20     B Day 2 3043.074&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is a graphical representation of these data (data points coming from the same subject are connected with dashed lines to make it easier to understand the structure of this dataset):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RCUZH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to test the effects of Group and Day, I could fit a mixed-effects model using e.g. the nlme package for R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Fit the model:&#10;M &amp;lt;- lme(BW ~ Day * Group, random = ~ 1 | ID, data = dat)&#10;&#10;# check the significance of effects:&#10;anova(M)&#10;            numDF denDF  F-value p-value&#10;(Intercept)     1    18 5564.085  &amp;lt;.0001&#10;Day             1    18    0.326  0.5753&#10;Group           1    18    2.849  0.1087&#10;Day:Group       1    18    3.631  0.0728&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thus, according to the fitted mixed-effects model (which was adequate for these data - diagnostics were run but are not presented here), neither of the examined factors (Day and Group) are affecting the response variable; also, there is no interaction between the two factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the type of analysis that I would do for such a dataset if I were asked to. However, in my organisation many people have no idea about the mixed-effects models. What they would typically do is applying a bunch of t-tests (or similar tests) to detect the effect of the &quot;Group&quot; on each of the sampling dates. For example, for the data shown above one would conduct a t-test for Day 1 and another t-test for Day 2, getting the following results:&lt;/p&gt;&#10;&#10;&lt;p&gt;Day 1: P = 0.271&lt;/p&gt;&#10;&#10;&lt;p&gt;Day 2: P &amp;lt; 0.001&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, they would claim that there was a significant Group effect on Day 2. I tried to explain that this result would not be correct because of the presence of correlation in data, which originates from the repeated measurements made on the same subjects. However, a colleague of mine asked a question that I could not answer easily. He said: &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Ok, the observations are correlated, I get that. But for now, forget about the fact that we have data from Day 1 and suppose that there are data only from Day 2. Observations in Group A and Group B are independent from each other, and so we are allowed to apply to a t-test or something similar. When we do apply a t-test [&lt;em&gt;as shown above&lt;/em&gt;], we get a significant Group effect. How should we then treat this result?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;And this is exactly the point were I got stuck. Indeed, if one has only the information from Day 2 and does a simple t-test, one gets a very different (and, in principle, justified) conclusion than the one obtained with the mixed effects model. &lt;strong&gt;Which method of analysis is to trust then? Is the Group effect real?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I feel like I am missing some important piece for justification of the use of mixed model. Any hint would be highly appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-06T14:22:12.967" FavoriteCount="1" Id="69375" LastActivityDate="2013-09-06T17:32:39.397" OwnerUserId="23854" PostTypeId="1" Score="3" Tags="&lt;repeated-measures&gt;&lt;multiple-comparisons&gt;&lt;mixed-effect&gt;" Title="A mixed-effects model for repeated measurements vs multiple time point-wise comparisons with a simpler test" ViewCount="491" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to calculate the z-score for a distribution for which I know the odds-ratio, and the p-value. I also know the upper and lower 95% confidence limits for the odds-ratio. An example with two-data points is shown here.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;OR       OR_95    OR_95U   p-value&#10;0.997804 0.970573 1.025798 0.876215&#10;1.039562 1.010116 1.069866 0.00815&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-09-06T18:41:11.137" Id="69401" LastActivityDate="2013-09-06T19:08:39.413" LastEditDate="2013-09-06T19:08:39.413" LastEditorUserId="7290" OwnerUserId="30004" PostTypeId="1" Score="3" Tags="&lt;genetics&gt;&lt;odds-ratio&gt;" Title="Calculate z-score from odds-ratio" ViewCount="753" />
  
  <row Body="&lt;p&gt;By way of summary of the comments, so that this question has an answer (if Alecos would like to present a summary I'll happily delete this and upvote it instead, as long as the question ends up with an answer) --&lt;/p&gt;&#10;&#10;&lt;p&gt;As Alecos points out, the AR(1) process for $\phi=0.9$ is definitely Gaussian.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since it's stationary, observations from it should all have the same Gaussian distribution; the OP is right to expect that it should, since it's undeniably so. As Alecos says, the pen-and-paper result should suggest there's a problem with either the code or the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are in fact several such problems:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) The Shapiro Wilk test assumes independence. We don't have it so the test doesn't apply. The OP notes that the test doesn't seem to have problems even with moderately large values of the parameter, and that's not surprising - because of the way the test works, it should be fairly robust to mild dependence. The Shapiro-Wilk doesn't look at the dependence in consecutive values. The most noticeable effect of the dependence on the distribution of the order statistics will be to increase their variance, but the Shapiro Wilk won't notice that at all. There will be a tendency for the tails to wander more from the straight line than with an independent series and eventually that sort of deviation will become detectable.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) The code doesn't have any warmup period. The mean and variance of the early values won't correspond to the mean and variance of the AR(1) process (so we don't have independence OR identically distributed values).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-07T02:22:06.700" Id="69423" LastActivityDate="2013-09-08T03:53:12.417" LastEditDate="2013-09-08T03:53:12.417" LastEditorUserId="805" OwnerUserId="805" ParentId="69419" PostTypeId="2" Score="5" />
  
  
  
  
  
  <row Body="&lt;p&gt;No, they are not comparable. Because you are fitting your &lt;code&gt;ets&lt;/code&gt; to two different data sets, one with and the other one without BoxCox transformation. By default, when you don't provide the argument of lambda, it is considered as NULL and will be ignored. You cannot compare the AIC ( or AICc) values, whenever you change your data set. A very good reference here is &lt;a href=&quot;http://www.mun.ca/biology/quant/ModelSelectionMultimodelInference.pdf&quot; rel=&quot;nofollow&quot;&gt;Model Selection and Multimodel Inference Second Edition&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-07T23:02:17.067" Id="69474" LastActivityDate="2013-09-07T23:02:17.067" OwnerUserId="13138" ParentId="69203" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;The question is : &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;You have found the following ages (in years) of 6 bears. Those bears&#10;  were randomly selected from the 28 bears at your local zoo:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;46,1,20,16,12,2&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Based on your sample, what is the average age of the bears? What is&#10;  the standard deviation? You may round your answers to the nearest&#10;  tenth.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;the problem is , while I determine the deviation the average divied by (n-1). In this case divided by 5. but why? when will we divide by n and when it by (n-1)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.khanacademy.org/math/probability/descriptive-statistics/variance_std_deviation/e/standard_deviation&quot; rel=&quot;nofollow&quot;&gt;Source:&lt;/a&gt;&lt;/p&gt;&#10;" ClosedDate="2013-09-08T01:11:29.900" CommentCount="3" CreationDate="2013-09-07T23:21:09.100" Id="69476" LastActivityDate="2013-09-07T23:37:59.250" OwnerUserId="30037" PostTypeId="1" Score="0" Tags="&lt;standard-deviation&gt;&lt;average&gt;" Title="standard deviation problem" ViewCount="28" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to determine if there are differences in the neck circumferences (dependent variable) among three groups of patients (categorized into one of 3 skeleto-facial pattern=independent variable) taking into consideration their age groups (3 age groups=another independent variable), gender (another independent variable), BMI category (4 categories=another independent variable), and risk for sleep apnea (possible risk or no risk=another independent variable).  I'm not sure if these are really independent variables or co-variables.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-08T04:09:56.500" Id="69485" LastActivityDate="2013-09-08T04:09:56.500" OwnerUserId="30041" PostTypeId="1" Score="0" Tags="&lt;categorical-data&gt;" Title="which stat method should I use?" ViewCount="31" />
  <row Body="&lt;p&gt;Yes, that strategy - shifting back, then estimating in the usual way - should work fine (with the proviso that you then need to shift the resulting fitted distribution back again if you want to write it as a shifted Poisson on the original data).&lt;/p&gt;&#10;&#10;&lt;p&gt;An example in R, using &lt;code&gt;fitdistr&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; v &amp;lt;- rpois(110,3.2)+1  # make up some data&#10; library(MASS)&#10; vfit &amp;lt;- fitdistr(v-1,&quot;poisson&quot;)&#10; vfit&#10;    lambda  &#10;  3.1909091 &#10; (0.1703181)&#10; obf &amp;lt;- as.vector(table(v)/sum(table(v)))&#10; plot(1:8,obf,type=&quot;h&quot;,lwd=2,xlim=c(0,10),ylim=c(0,0.25))&#10; lines((1:10)+.05,dpois(0:9,3.191),col=2,type=&quot;h&quot;,lwd=2)&#10; abline(h=0,col=8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6Q8TN.png&quot; alt=&quot;shifted Poisson, observed and fitted&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-08T04:44:59.367" Id="69486" LastActivityDate="2013-09-08T04:56:17.843" LastEditDate="2013-09-08T04:56:17.843" LastEditorUserId="805" OwnerUserId="805" ParentId="69483" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The &lt;code&gt;family&lt;/code&gt; describes the response. Its what makes a glm different to a standard plain linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a linear model your responses (y-values) are considered to be Normally-distributed values with mean aX+b and variance sigma-squared. In a GLM of another family the responses are thought to come from another distribution - such as poisson, or binomial, via a transformation of the aX+b bit. If your response values are small numbers of counts (classic example: number of cars past a post every minute on a quiet road) then the linear model is inappropriate because it might end up predicting a negative number of cars... So you use a poisson regression, where the distribution is non-negative.&lt;/p&gt;&#10;&#10;&lt;p&gt;This affects how you think about residuals, and the GLM code in R can return various types of residuals - see help(residuals.glm) for details. For example the size of the residuals in your plot, together with you telling us that the data is in the thousands, makes me think these are 'response' residuals, and not scaled in the way some of the other residuals are.&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't think the blue line in your plot is the 'distribution of your residuals' do you? You should maybe have a look at the histogram or a boxplot or quantiles of your residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fitted vs resid plots like you've given us tell is some things: for example if you see a strong pattern of high, low, high residuals then that's saying there's some more curviness in your data that your model hasn't found (because your residuals aren't independent); if your residuals are all small at low fitted values and then large at high fitted values (but all balanced around zero) then your variance isn't constant and you should think carefully about a transformation of some kind.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-08T07:45:37.697" Id="69491" LastActivityDate="2013-09-08T07:45:37.697" OwnerUserId="1549" ParentId="69472" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to implement Pettitt test in R following papers like this &lt;a href=&quot;http://www.ias.ac.in/jess/forthcoming/JESS-D-13-00049.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt; (pp. 5 &amp;amp; 6), or this &lt;a href=&quot;http://www.igu.in/17-3/paper-2.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;. But, I'm misunderstanding something, because having tested it with some data, I think that output is not correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pettitt &amp;lt;- function(x, alpha=0.99) {&#10;# Pettitt AN. 1979 A non-parametric approach to the change point detection.&#10;# x is a vector&#10;# alpha, integer, level of significance&#10;x &amp;lt;- na.omit(x)&#10;o &amp;lt;- rank(x)&#10;s &amp;lt;- c()&#10;L &amp;lt;- length(x)&#10;for (i in 1:(L-1)) {&#10;      s &amp;lt;- c(s, 2*(colSums(as.matrix(o[1:i]))) - (i*(L+1)) )&#10;}&#10;vc &amp;lt;- sqrt((-1) * log(alpha) * (L^3 + L^2)/6)&#10;output &amp;lt;- list(abs(s), vc)&#10;return(output)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Testing with &lt;code&gt;larain&lt;/code&gt; and &lt;code&gt;tempdub&lt;/code&gt; dataset from &lt;code&gt;TSA package&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(TSA)&#10;data(larain)&#10;data(tempdub)&#10;pettitt(larain)&#10;[[1]]&#10;  [1]  78 118 180  76  30  30 144  90 124 148 224 334 314 298 362 444 356 334&#10; [19] 300 302 194 121  83  55  45  57  25  95 175 195 193 287 181 231 175 213&#10; [37] 301 331 421 345 392 322 282 354 372 274 194 130 188 248 175  97  85 153&#10; [55] 105 171 181 189 245 297 401 375 449 557 467 551 594 576 602 490 406 354&#10; [73] 262 266 362 248 244 214 208 200 247 147  89  13   9  15  97   5   9  83&#10; [91]   3  95 123  63  31  12  44   6  48  34  72 108 208 164 170 282 214 148&#10;[109] 202 140 104   6 102  86&#10;&#10;[[2]]&#10;[1] 50.69224&#10;&#10;&amp;gt; max(pettitt(larain)[[1]])&#10;[1] 602&#10;&#10;pettitt(tempdub)&#10;[[1]]&#10;  [1]  83 161 226 235 164  60  80 169 220 219 188  74  57 177 266 281 228 147&#10; [19]  19  82 125 140 102  41 100 197 235 254 233 141   1  97 144 153 112  26&#10; [37]  73 206 255 258 235 137  28  49  98 101  46  29 149 252 281 274 247 160&#10; [55]  43  70 115 126  79  22 157 248 317 328 287 224  96  27  86  79  27  82&#10; [73] 225 348 407 406 351 256 125  10  58  77  32  61 200 314 381 386 353 216&#10; [91] 124  40  35  70  35  36 173 302 365 386 321 242 131  10  51  38  19 146&#10;[109] 241 319 342 359 330 223  89  45 113 144 111   2 123 228 280 275 250 177&#10;[127]  34  50  89 102  59  22 131 248 334 359 302 198  73  46  83 100  73&#10;&#10;[[2]]&#10;[1] 70.96777&#10;&#10;&amp;gt; max(pettitt(tempdub)[[1]])&#10;[1] 407&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I don't know if I lost something in pettitt test or there are error in my code.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-09-08T13:37:22.690" FavoriteCount="1" Id="69501" LastActivityDate="2014-06-27T07:00:51.743" LastEditDate="2013-09-08T14:04:35.343" LastEditorUserId="7290" OwnerUserId="30050" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;heteroscedasticity&gt;" Title="Implementing Pettitt test in R" ViewCount="1056" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose there is any distribution whose parameter values are known. So how to simulate data in this case?&lt;/p&gt;&#10;" ClosedDate="2013-09-09T11:57:03.970" CommentCount="3" CreationDate="2013-09-09T07:19:51.850" Id="69535" LastActivityDate="2013-09-09T09:40:31.323" LastEditDate="2013-09-09T09:40:31.323" LastEditorUserId="27581" OwnerUserId="30076" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;matlab&gt;&lt;computational-statistics&gt;" Title="How to simulate data for those distributions which are not defined in MATLAB" ViewCount="78" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a sampling distribution generated by computing the maximum across many samples. I'd now like to generate an estimate for what the true maximum parameter is within the population I sampled from. How can I do this? I thought about the bootstrap, but it didn't work well in estimating the maximum. Can anyone suggest a method to estimate the maximum from my sampling distribution?  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-09T21:00:28.260" FavoriteCount="1" Id="69599" LastActivityDate="2013-09-09T21:15:21.043" LastEditDate="2013-09-09T21:09:44.887" LastEditorUserId="13090" OwnerUserId="13090" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;nonparametric&gt;&lt;resampling&gt;" Title="Suitable method to estimate confidence intervals for an extreme order statistic" ViewCount="91" />
  <row Body="&lt;p&gt;Here is some R code that validates your formulas given above:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Generate psuedo data&#10;Y = rnorm(10)&#10;X = matrix(c(rep(1,10),rnorm(10)),ncol=2)&#10;&#10;#Calculate X times beta hat&#10;XB1 = X%*%solve(t(X)%*%X)%*%t(X)%*%Y&#10;&#10;#Make sure X = UDV'&#10;svd(X)$u%*%diag(svd(X)$d)%*%t(svd(X)$v)&#10;&#10;&amp;gt; svd(X)$u%*%diag(svd(X)$d)%*%t(svd(X)$v)&#10;      [,1]        [,2]&#10; [1,]    1 -0.20283033&#10; [2,]    1 -0.85846798&#10; [3,]    1  0.07970559&#10; [4,]    1 -0.28254373&#10; [5,]    1  0.39261439&#10; [6,]    1 -0.31559482&#10; [7,]    1  0.20561526&#10; [8,]    1  0.55152336&#10; [9,]    1 -0.69396930&#10;[10,]    1 -1.21970880&#10;&amp;gt; X&#10;      [,1]        [,2]&#10; [1,]    1 -0.20283033&#10; [2,]    1 -0.85846798&#10; [3,]    1  0.07970559&#10; [4,]    1 -0.28254373&#10; [5,]    1  0.39261439&#10; [6,]    1 -0.31559482&#10; [7,]    1  0.20561526&#10; [8,]    1  0.55152336&#10; [9,]    1 -0.69396930&#10;[10,]    1 -1.21970880&#10;&#10;&#10;#Calculate UU'Y&#10;U = svd(X)$u&#10;XB2 = U%*%t(U)%*%Y&#10;&#10;#Check to see if they return the same thing&#10;cbind(XB1,XB2)&#10;&#10;&amp;gt; cbind(XB1,XB2)&#10;            [,1]       [,2]&#10; [1,] -0.4644321 -0.4644321&#10; [2,] -0.7215807 -0.7215807&#10; [3,] -0.3536183 -0.3536183&#10; [4,] -0.4956966 -0.4956966&#10; [5,] -0.2308919 -0.2308919&#10; [6,] -0.5086596 -0.5086596&#10; [7,] -0.3042351 -0.3042351&#10; [8,] -0.1685660 -0.1685660&#10; [9,] -0.6570624 -0.6570624&#10;[10,] -0.8632634 -0.8632634&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So as you can see from the output above, for sure one decomposition of $X$ is $X=UDV^T$.  Likewise, calculating $UU^TY$ is equivalent to calculating $X\hat\beta$ where $\hat\beta=(X^TX)^{-1}X^TY$.  So this solution really just pertains to validating your second question about whether or not what you are doing is correct.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-09T22:23:54.130" Id="69608" LastActivityDate="2013-09-09T22:31:30.743" LastEditDate="2013-09-09T22:31:30.743" LastEditorDisplayName="user25658" OwnerDisplayName="user25658" ParentId="69605" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Basically you need to add an exra grid on top of your typical learning stack. Since you didn't specify the algorithm of choice or any properies of p, I can't really be more specific. The most basic version would look something like this though:&lt;/p&gt;&#10;&#10;&lt;p&gt;For all values of p:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Apply p to the images to obtain feature vectors&lt;/li&gt;&#10;&lt;li&gt;Apply the learning procedure using these feature vectors&lt;/li&gt;&#10;&lt;li&gt;Remember the performance for this choice&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Afterwards, look at the value of p that gave you the best performance and use this for subsequent learning. Remember to use a held out part of your dataset for testing.&lt;/p&gt;&#10;&#10;&lt;p&gt;For more information, look at 'cross validation'.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-09T23:18:35.073" Id="69610" LastActivityDate="2013-09-09T23:18:35.073" OwnerUserId="12752" ParentId="68001" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;A dynamic panel model might make sense if you have a eye-for-an-eye retaliation model for homicides. For example, if the homicide rate was largely driven by gangs feuds, the murders at time $t$ might well be a function of the deaths at $t-1$, or other lags.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am going to answer your questions out of order. Suppose the DGP is &#10;\begin{equation}&#10;y_{it}=\delta y_{it-1}+x_{it}^{\prime}\beta+\mu_{i}+v_{it},&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where the errors $\mu$ and $v$ are independent of each other and among themselves. You're interested in conducting the test of whether $\delta = 0$ (question 2).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you use OLS, it's easy to see that $y_{it-1}$ and the first part of the error are correlated, which renders OLS biased and inconsistent, even when there's no serial correlation in $v$. We need something more complicated to do the test. &lt;/p&gt;&#10;&#10;&lt;p&gt;The next thing you might try is the fixed effects estimator with the within transformation, where you transform the data by subtracting each unit's average $y$, $\bar y_{i}$, from each observation. This wipes out $\mu$, but this estimator suffers from &lt;a href=&quot;http://links.jstor.org/sici?sici=0012-9682%28198111%2949:6%3C1417%3aBIDMWF%3E2.0.CO;2-N&amp;amp;origin=repec&quot;&gt;Nickell bias&lt;/a&gt;, which bias does not go away as the number of observations $N$ grows, so it is inconsistent for large $N$ and small $T$ panels. However, as $T$ grows, you get consistency of $\delta$ and $\beta$. &lt;a href=&quot;http://faculty.smu.edu/millimet/classes/eco6375/papers/judson%20owen%201999.pdf&quot;&gt;Judson and Owen (1999)&lt;/a&gt; do some simulations with $N=20,100$ and $T=5,10,20,30$ and found the bias to be increasing in $\delta$ and decreasing in $T$. However, even for $T=30$, the bias could be as much as $20\%$ of the true coefficient value. That's bad news bears! So depending on the dimensions of you panel, you may want to avoid the within FE estimator. If $\delta &amp;gt; 0$, the bias is negative, so the persistence of $y$ is underestimated. If the regressors are correlated with the lag, the $\beta$ will also be biased.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another simple FE approach is to first-difference the data to remove the fixed effect, and use $y_{it-2}$ to instrument for $\Delta y_{it-1} = y_{it-1}-y_{it-2}$. You also use $x_{it}-x_{it-1}$ as an instrument for itself. &lt;a href=&quot;http://www.jstor.org/discover/10.2307/2287517?uid=3739560&amp;amp;uid=2129&amp;amp;uid=2134&amp;amp;uid=2&amp;amp;uid=70&amp;amp;uid=4&amp;amp;uid=3739256&amp;amp;sid=21102622796277&quot;&gt;Anderson and Hsiao (1981)&lt;/a&gt; is the canonical reference. This estimator is consistent (as long as the explanatory $X$s are pre-determined and the &lt;em&gt;original&lt;/em&gt; error terms are not serially correlated), but not fully efficient since it does not use all the available moment conditions and does not use the fact that the error term is now differenced. This would probably be my first choice. If you think that $v$ follow an AR(1) process, can use third and fourth lags of $y$ instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.jstor.org/stable/2297968&quot;&gt;Arellano and Bond (1991)&lt;/a&gt; derive a more efficient generalized method of moments (GMM) estimator, which has been extended since, relaxing some of the assumptions. Chapter 8 of Baltagi's &lt;a href=&quot;http://books.google.com/books?id=oQdx_70Xmy0C&amp;amp;lpg=PA13&amp;amp;ots=xi8g6F3lRA&amp;amp;dq=baltagi%20panel%20data&amp;amp;lr&amp;amp;pg=PP1#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;panel book&lt;/a&gt; is a good survey of this literature, though it does not deal with lag selection as far as I can tell. This is state of the art 'metrics, but more technically demanding. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the &lt;a href=&quot;http://cran.r-project.org/web/packages/plm/vignettes/plm.pdf&quot;&gt;&lt;code&gt;plm&lt;/code&gt; package&lt;/a&gt; in R has some of these built in. Dynamic panel models have been in Stata &lt;a href=&quot;http://www.stata.com/stata10/dpd.html&quot;&gt;since version 10&lt;/a&gt;, and SAS has the &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_panel_sect035.htm&quot;&gt;GMM version&lt;/a&gt; at least. None of these are count data models, but that may not be a big deal depending on your data. However, here's &lt;a href=&quot;http://www.stata.com/meeting/mexico10/mex10sug_trivedi.pdf&quot;&gt;one example&lt;/a&gt; of a GMM dynamic Poisson panel model in Stata.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer to your first question is more speculative. If you leave out the lagged $y$ and first difference, I believe that $\beta$ can still be estimated consistently, though less precisely since the variance is now larger. If that is the parameter you care about, that may be acceptable. What you loose is that you cannot say whether there were a lot of homicides in area X because they were lots last month or because area X has a propensity for violence. You give up the ability to distinguish between state dependence and unobserved heterogeneity (question 1).    &lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2013-09-16T16:31:03.990" CreationDate="2013-09-10T01:26:48.947" Id="69614" LastActivityDate="2013-09-16T16:31:03.990" LastEditDate="2013-09-16T16:31:03.990" LastEditorUserId="7071" OwnerUserId="7071" ParentId="69570" PostTypeId="2" Score="9" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset in which I measure the response variable of a number of individual animals. I used this response variable as the dependent variable in a linear mixed model, for which I have 5 explanatory variables. Two of these explanatory variables are nested if they are entered in the same model, as they refer to the individual animal, and the research site they are situated on. So it would be site(individual). If I do not nest them but enter them separately, the program runs into calculation errors, and throws one of the two variables out. I use SPSS. &#10;I then created a whole number of different models, with all the different combinations of explanatory variables, and ranked them according to their AIC. I then found out that a number of models had exactly the same AIC. These were the models which, all else being equal, contained either just individual as an explanatory variable, or site(individual). If the model, all else being equal, only contained site, the AIC was different.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now my questions are: a) do these results make sense? I cannot think of what I am doing wrong, but I am a bit worried about the exact same value for the AIC.. &#10;b) can I compare models this way - comparing nested variables with un-nested ones?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-10T01:44:38.287" FavoriteCount="0" Id="69616" LastActivityDate="2013-09-10T02:20:19.690" LastEditDate="2013-09-10T02:20:19.690" LastEditorUserId="21599" OwnerUserId="30102" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;spss&gt;&lt;aic&gt;" Title="Mixed linear models with exactly the same AIC?" ViewCount="107" />
  <row Body="&lt;p&gt;Your variable is defined as &#10;$$X_{t} = e^{\alpha t + \beta} e^{z_{t}} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Say you have a sample $S_n$ of $n$ observations of past values of the variable and you want to forecast period $n+1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then&#10;$$E\Big (X_{n+1}\mid S_n \Big ) = E\Big (e^{\alpha (n+1) + \beta} e^{z_{n+1}}\mid S_n\Big) = E\Big (e^{\alpha (n+1) + \beta}\mid S_n\Big) E\left(e^{z_{n+1}}\right)$$ $$= E\Big (e^{\alpha (n+1) + \beta}\mid S_n\Big)e^{\sigma^2/2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;...since $z_t$ is Gaussian white noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;adjusted forecast with an empirical correction factor&quot;, uses rather confusing if not incorrect notation, ignores various biases, and approximates the above by &#10;$$E\Big (e^{\alpha (n+1) + \beta}\mid S_n\Big) \approx e^{\hat \alpha (n+1) + \hat \beta} = e^{\hat{\log x_{n+1}}} $$&#10;and &#10;$$ e^{\sigma^2/2} = E\left(e^{z_{n+1}}\right) \approx \frac {1}{n}\sum_{t=1}^{n} e^{\hat z_t}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and so &#10;$$ \widehat E\Big (X_{n+1}\mid S_n \Big )= e^{\hat{\log x_{n+1}}}\frac {1}{n}\sum_{t=1}^{n} e^{\hat z_t} $$&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-10T03:08:51.470" Id="69621" LastActivityDate="2013-09-10T03:08:51.470" OwnerUserId="28746" ParentId="69613" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Say, I have collected 50 comments. For each comment, I am planning to get 10 ratings from non-expert raters for the following criteria: &quot;interesting vs. not interesting&quot;. Based on the plurality of those ratings, I will determine if a comment is &quot;interesting&quot; or not. For example, 6 out of 10 raters rated comment #1 as &quot;interesting&quot;, I assume comment #1 is interesting&lt;/p&gt;&#10;&#10;&lt;p&gt;The thing that complicates the matter a bit is that I also have experts (the ones who are trained/versed in giving such ratings) rate those comments in a similar fashion as mentioned above.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: I would also like to know if I can use non-experts (less costly) to rate the comments instead of paying more for experts. What kind of statistical/mathematical calculation can I do to find that out? Is there a calculation that tells us how much of two sets of ratings overlap/agree?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-10T05:52:20.977" Id="69632" LastActivityDate="2013-09-10T05:52:20.977" OwnerUserId="26150" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;statistical-significance&gt;&lt;summary-statistics&gt;&lt;inter-rater&gt;" Title="What is the analysis techniques for comparing two sets of ratings" ViewCount="73" />
  <row AnswerCount="0" Body="&lt;p&gt;How outliers can effect the correlation results (Spearman and Pearson)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two series of data. The source of the data are different but both are temperature that are measured by different instruments with different accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please possibly give answers with reference.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-10T06:53:08.823" Id="69635" LastActivityDate="2013-09-10T06:53:08.823" OwnerUserId="17833" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;autocorrelation&gt;&lt;pearson&gt;&lt;spearman&gt;" Title="How outliers can effect the correlation results (Spearman and Pearson)?" ViewCount="483" />
  <row AnswerCount="1" Body="&lt;p&gt;I am currently during my final year of schooling. I am curious to know (and I want to put this in my Math assignment), what is better 1-tailed Hypothesis Testing, where you see if a claim is justifiable, or is it a confidence interval, we only deal with 95% ones. &lt;/p&gt;&#10;&#10;&lt;p&gt;I personally am torn between the two and sometimes can't even tell the difference. &lt;/p&gt;&#10;&#10;&lt;p&gt;Confidence intervals use sample data and create an interval on that and then you see how different the claim is based on the sample. But you get a range of values so you can test more claims more easily.&lt;/p&gt;&#10;&#10;&lt;p&gt;With Hypothesis testing, you are pretty much doing the same, seeing how unusual a claim is based on the sample using a Z-Test. &lt;/p&gt;&#10;&#10;&lt;p&gt;The context of my assignment is the 2013 election in Australia. We are testing out claims made by political parties against the data collected by polls.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-10T11:09:32.827" Id="69648" LastActivityDate="2013-09-10T14:13:05.533" LastEditDate="2013-09-10T11:16:18.863" LastEditorUserId="22047" OwnerUserId="30122" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;confidence-interval&gt;&lt;binomial&gt;" Title="Which is better? Hypothesis Testing or Confidence Interval" ViewCount="158" />
  <row AnswerCount="0" Body="&lt;p&gt;I’m fairly new to scientific significance testing and getting into the topic more and more.&lt;br&gt;&#10;I have a multifactorial dataset with Respiration rate as response variable and Temperature and CO2 partial pressure as factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data seem to be normally distributed, but I believe to see heterogeneity of variance.&lt;br&gt;&#10;As the residuals of the factor Temperature have different variances over the treatment levels.&lt;br&gt;&#10;When I run a two-way ANOVA on the dataset, Temperature and the interaction are significant, CO2 is not.&lt;/p&gt;&#10;&#10;&lt;p&gt;To deal with the heterogeneity, a colleague advised me to use a GLS model and weights on the factor that shows heterogeneity (in this case Temperature).&#10;If I do that, all terms become significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if this approach is reasonable, as I don’t see an effect of CO2 in the raw data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here the levene output:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;leveneTest(RR,factor(Cal$Temp), center = mean)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;p= 0.06131&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;leveneTest(RR, factor(Cal$pCO2), center = mean)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;p= 0.2059&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;leveneTest(RR, interaction(factor(Cal$pCO2), factor(Cal$Temp)), center = mean)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;p= 0.01829&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope the given information are enough to imagine my problem.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-10T11:33:32.067" Id="69649" LastActivityDate="2013-09-10T12:46:58.157" LastEditDate="2013-09-10T12:46:58.157" LastEditorUserId="22468" OwnerUserId="30123" PostTypeId="1" Score="1" Tags="&lt;heteroscedasticity&gt;&lt;gls&gt;" Title="Using GLS and weights to solve heterogeneity of variances?" ViewCount="202" />
  <row AnswerCount="1" Body="&lt;p&gt;In trying to implement Mixture Markov Model, (see question &lt;a href=&quot;http://stats.stackexchange.com/questions/69563/can-log-likelihood-function-calculated-value-m-step-be-smaller-after-1-em-iter/69584#69584&quot;&gt;here&lt;/a&gt;), I have extreme cases ( e.g. 0's in the Transition Probability Matrix). I have approached this with replacing 0 with 1e-17. However, I believe that such approximation might be breaking the algorithm, because in consequent steps I start seeing even smaller values by huge  magnitute ( e.g. conditional posteriors in E-step are sometimes of magnitude 1e-321). This results from the likelihood formula, which multiplies the probabilities taken to powers of the number of occurrences of such transitions in each sequence. &lt;/p&gt;&#10;&#10;&lt;p&gt;With Markov mixture model I am trying to cluster a number of sequences, some of which might be very long (up to several hundred transitions). This also creates some extremalities (such as very small transition probabilities for some states for some sequences, if not 0 then 1e-20 or something similar). &lt;/p&gt;&#10;&#10;&lt;p&gt;Since I already make assumption that 1e-17 is an approximation of 0, any values that are less than this, would be meaningless. However, working with 0's is impossible because MAP log-likelihood value would be -Inf ( =log(0)). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an approximation technique that is used for such cases? What is the best way to approximate 0's? I am working in &lt;code&gt;R&lt;/code&gt; and there is a limit to &lt;code&gt;R&lt;/code&gt;'s precision (somewhere around 1e-400..). I am really at a loss. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-10T12:15:40.090" FavoriteCount="0" Id="69655" LastActivityDate="2013-10-10T14:25:21.853" OwnerUserId="14556" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixture&gt;&lt;likelihood&gt;&lt;expectation-maximization&gt;&lt;markov-chain&gt;" Title="How to approximate 0 in transition probability matrix without loss of generality?" ViewCount="84" />
  <row Body="&lt;p&gt;Lots of lab papers, especially the instrument testing experiments, apply such x on y regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;They argue that from the data collection in the experiment, the y conditions are controlled, and get x from the instrument reading (introducing some error in it). This is the original physical model of the experiment, so the x~y+error is more suitable.&lt;/p&gt;&#10;&#10;&lt;p&gt;To minimize the experiment error, sometimes, y being controlled on the same condition, then x is measured for several times (or repeated experiment). This procedure may help you to understand the logic behind them and find x~y+error more clearly.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-10T13:22:35.567" Id="69660" LastActivityDate="2013-09-10T13:22:35.567" OwnerUserId="29187" ParentId="69646" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="69777" AnswerCount="1" Body="&lt;p&gt;I want to compare two ​GLMs with binomial dependent variables. The results are: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; m1 &amp;lt;- glm(symptoms ~ 1,         data=data2)&#10; m2 &amp;lt;- glm(symptoms ~ phq_index, data=data2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The model test gives the following results: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;​ anova(m1, m2)​&#10;         no AIC    logLik   LR.stat df  Pr(&amp;gt;Chisq)   &#10; m1      1  4473.9 -2236.0                        &#10; m2      9  4187.3 -2084.7  302.62  8   &amp;lt; 2.2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;​I am used to comparing these kinds of models using chi-squared values, a chi-squared difference, and a chi-squared difference test. Since all other models in the paper are compared this way, and since I'd like to report them in a table together: why exactly is this model test different from my other model tests in which I get chi-squared values and difference tests? Can I obtain chi-squared values from this test? &lt;/p&gt;&#10;&#10;&lt;p&gt;Results from other model comparisons (e.g., GLMER), look like this: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    #Df AIC     BIC     logLik  Chisq   Chi     DF diff Pr(&amp;gt;Chisq)&#10;m3  13  11288   11393   -5630.9 392.16          &#10;m4  21  11212   11382   -5584.9 92.02   300.14  8       0.001&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-09-10T13:52:32.857" Id="69664" LastActivityDate="2013-09-11T19:03:23.643" LastEditDate="2013-09-11T17:51:03.993" LastEditorUserId="7290" OwnerUserId="14731" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;chi-squared&gt;" Title="Comparing nested GLMs via chi-squared and loglikelihood" ViewCount="2118" />
  <row AcceptedAnswerId="69710" AnswerCount="1" Body="&lt;p&gt;Let's say I have one stochastic process $X$ which represents the production of something. I make some improvements over the production line and I hopefully end up with a different stochastic process, $Y$, with mean greater than $X$ and less variance. Let's suppose each process was run for the same time.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I want to quantify the improvement (I) using their expectations ratio ($I = \frac{\mathrm{E}[Y]}{\mathrm{E}[X]}$), what would be the 95% confidence interval of I?&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I just treat them as independent Gaussian variables and calculate the improvement ($I$) variance with the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance#Product_of_independent_variables&quot; rel=&quot;nofollow&quot;&gt;variance product formula&lt;/a&gt;? Since $X$ and $Y$ are variations of the same production line, I don't know if they can be considered independent. Also, I'm not sure if they can be considered Gaussian.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-10T14:12:09.647" FavoriteCount="1" Id="69669" LastActivityDate="2013-09-10T21:43:19.970" LastEditDate="2013-09-10T14:20:48.517" LastEditorUserId="7290" OwnerUserId="30125" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;stochastic-processes&gt;" Title="Confidence interval for the expectation of two stochastic processes" ViewCount="133" />
  
  <row AcceptedAnswerId="69940" AnswerCount="1" Body="&lt;p&gt;For normal-distributed interval/ratio data, we can apply linear mixed effect model for analysing longitudinal data, where each subject is measured multiple times. How about dichotomy (binomial) data?&lt;/p&gt;&#10;&#10;&lt;p&gt;My experiment is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two conditions A and B, and M is an interaction technique that will be used by the users for a task in the experiment on a VOLUNTARY base. (it means the task can be accomplished with or without M, and we are interested to know whether or not they use it). The experiment lasts for 4 weeks. Each week we experimented only ONCE for each participant. The task in each week is the same for each participant, but is different across different weeks. So in total each participant were tested on the same set of 4 tasks. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the first 2 weeks, all 5 subjects were exposed to condition A, we observe if a subject has used M or not. So M is a dichotomy or binary variable which is either YES or NO. In the last 2 weeks, all 5 subjects were exposed to condition B, we again observe if a subject has used M or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data is something like follows&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subject   week   condition   useTechnique &#10;   1       1         A           YES&#10;   2       1         A           NO&#10;   3       1         A           YES&#10;   .........................................&#10;   .........................................&#10;   .........................................&#10;   3       4         B           NO&#10;   4       4         B           YES&#10;   5       4         B           YES&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The research question is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Does the change of condition has an effect on whether or not a user used the interaction technique M?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Normally we use chi-square test or mcnemar test for dichotomy data. The former one deals with unpaired and unmatched groups and the latter one deals with paired groups. McNemar's test is often used when we asked whether the participants liked/used a device before and after the experiment. This is essentially what I want to test, but unfortunately my experiment introduced random effect factors, because each subject is tested two twice under each condition.Furthermore, the task used in each week was different.&lt;/p&gt;&#10;&#10;&lt;p&gt;What test I should use for my research question for such data and how? I am using R, can anyone give some suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-10T15:56:11.463" Id="69681" LastActivityDate="2013-09-13T11:00:50.620" OwnerUserId="27813" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;chi-squared&gt;&lt;binomial&gt;&lt;multilevel-analysis&gt;&lt;mixed-effect&gt;" Title="Mixed-effect for Dichotomy/binomial variables?" ViewCount="79" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have sample data that I expect to contain values from at least several Poisson distributions (set around various lambda values).  Some of these lambda values are nicely spaced, leading to what are visually obvious distinct distributions. My goal is to set confidence intervals (bins) around the most believable of these distributions so I can parse the data in a defensible way.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So I'm thinking about using the conditional Chi-squared dispersion test once I have each lambda estimated... But I'm wondering what would be the best way to choose these values for lambda other than just simple visual inspection, which is where I'm stuck right now. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-09-11T00:24:57.943" Id="69719" LastActivityDate="2013-09-16T18:50:19.387" LastEditDate="2013-09-11T18:53:43.323" LastEditorUserId="30147" OwnerUserId="30147" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;maximum-likelihood&gt;&lt;poisson&gt;&lt;binning&gt;" Title="MLEs of Poisson lambda values" ViewCount="153" />
  <row Body="&lt;p&gt;Vowpal Wabbit, a very fast machine learning program focused on online gradient descent learning, can be used with Hadoop:&#10;&lt;a href=&quot;http://arxiv.org/abs/1110.4198&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/abs/1110.4198&lt;/a&gt;&#10;Though, I've never used it this way. If I understand it correctly, it really only uses Hadoop for reliability and providing the data to the Vowpal Wabbit processes. It uses something like MPI's AllReduce to do most of the communication.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-11T02:48:56.890" Id="69728" LastActivityDate="2013-09-11T02:48:56.890" OwnerUserId="12306" ParentId="69699" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The chi-square test $(n-1)s^2/\sigma^2$ of the hypothesis that the variance is $\sigma^2$ can be either one- or two-tailed in exactly the same sense that the t-test $(m-\mu)\sqrt{n}/s$ of the hypothesis that the mean is $\mu$ can be either one- or two-tailed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-11T03:46:58.830" Id="69729" LastActivityDate="2013-09-11T03:46:58.830" OwnerUserId="20776" ParentId="22347" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Clearly, the results from Google and My Search are not independent (since you have the &quot;query&quot; factor). Consider a simpler example: instead of calculating &quot;precision&quot;, take the number of hits. Don't you think that these numbers will be very well correlated between the two search engines? After all, a query like &quot;dog&quot; will return many more hits than &quot;multilinearity&quot; in &lt;em&gt;both&lt;/em&gt; engines.&lt;/p&gt;&#10;&#10;&lt;p&gt;Try to look at this from this point of view: instead of calculating the precision of the two machines, for each query you calculate the &lt;em&gt;difference&lt;/em&gt; in precision. Instead of having two columns, your statistic is now a single variable and the hypothesis that you wish to check is whether this variable is significantly different from 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is &lt;em&gt;exactly&lt;/em&gt; a situation to use a paired test.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-11T06:56:32.423" Id="69732" LastActivityDate="2013-09-11T06:56:32.423" OwnerUserId="14803" ParentId="69726" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;In order to make feature selection with unlabeled data, what about clustering+ANOVA to study which variables are more important? With the clustering you learn the distribution from the data, and using ANOVA will let you know which variables are more interesting for your study.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, you may use ANOVA on all your data to see which variables are able to explain your variance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-11T10:51:49.920" Id="69741" LastActivityDate="2013-09-11T10:51:49.920" OwnerUserId="29943" ParentId="39105" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It seems to me that your second equation will only be true if $\phi$ is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_map&quot;&gt;linear mapping&lt;/a&gt; (and hence $K$ is a linear kernel).  As the Gaussian kernel is non-linear, the equality will not hold (except perhaps in the limit as $\sigma$ goes to zero).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-11T15:31:35.890" Id="69762" LastActivityDate="2013-09-11T15:31:35.890" OwnerUserId="887" ParentId="69759" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="69788" AnswerCount="3" Body="&lt;p&gt;I was wondering if there is anything you can do when you have a regression problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{cases}&#10;   Y_t = \beta_1x_t + \beta_0 + \varepsilon_t \\&#10; \left(\varepsilon_1,\ldots,\varepsilon_n\right)\sim\mathcal{N}_n\left(\mathbf{0},D \right) \\&#10; 1\leq t \leq n&#10;\end{cases}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and you know $D$, the covariance matrix of the errors. I know  weighted least squares when $D$ is diagonal. But what happens if it isn't? &#10;Is there any well established estimator for $(\beta_0,\beta_1)$ ?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-11T18:16:00.980" Id="69785" LastActivityDate="2014-04-19T23:07:18.710" LastEditDate="2013-09-11T18:26:58.370" LastEditorUserId="919" OwnerUserId="21521" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;estimation&gt;&lt;least-squares&gt;&lt;generalized-least-squares&gt;" Title="How to do regression with known correlations among the errors?" ViewCount="222" />
  <row Body="&lt;p&gt;Probably simply no, you cannot split the pieces and treat both as Poisson, as the two individual pieces:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;em&gt;&lt;del&gt;Do not sum to 1 so are not valid PMFs&lt;/del&gt;&lt;/em&gt; (may be neither relevant nor true)&lt;/li&gt;&#10;&lt;li&gt;Are dependent on each other through $\pi$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For further reading, perhaps see the Eisenberger paper mentioned in the comments on the question and &lt;a href=&quot;http://ac.els-cdn.com/S0895717707001100/1-s2.0-S0895717707001100-main.pdf?_tid=a7b42358-1b19-11e3-931c-00000aacb35e&amp;amp;acdnat=1378928455_91883668c5d05f19869ecbca1e24f859&quot; rel=&quot;nofollow&quot;&gt;(Wang et al 2006)&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Edit&lt;/h3&gt;&#10;&#10;&lt;p&gt;@whuber's answer is far better, as usual.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Reference:&lt;/p&gt;&#10;&#10;&lt;p&gt;Kui Wang, Kelvin K.W. Yau, Andy H. Lee, Geoffrey J. McLachlan, Two-component Poisson mixture regression modelling of count data with bivariate random effects, Mathematical and Computer Modelling, Volume 46, Issues 11–12, December 2007, Pages 1468-1476, ISSN 0895-7177, &lt;a href=&quot;http://dx.doi.org/10.1016/j.mcm.2007.02.003&quot; rel=&quot;nofollow&quot;&gt;http://dx.doi.org/10.1016/j.mcm.2007.02.003&lt;/a&gt;.&#10;(&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0895717707001100&quot; rel=&quot;nofollow&quot;&gt;http://www.sciencedirect.com/science/article/pii/S0895717707001100&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-11T19:41:17.127" Id="69789" LastActivityDate="2013-09-11T21:03:05.717" LastEditDate="2013-09-11T21:03:05.717" LastEditorUserId="29617" OwnerUserId="29617" ParentId="69773" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Consider the following m regression equation system:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$r^i = X^i \beta^i + \epsilon^i \;\;\; \text{for}  \;i=1,2,3,..,T$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $r^i$ is a $(T\times 1)$ vector of the T observations of the dependent variable, $X^i$ is a $(T\times k)$ matrix of independent variables, $\beta^i$ is a $(k\times1)$ vector of the regression coefficients and $\epsilon^i$ is the vector of errors for the $T$ observations of the $i^{th}$ regression&lt;/p&gt;&#10;&#10;&lt;p&gt;If the above is an SUR model (seemingly unrelated regressions), does it make sense to compute the BIC (Bayesian Information Criterion) for the model?&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, how would that be computed? Should the dependent variables be stacked into one vector, in order to get only one regression equation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be appreciated! Thank you!!!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-11T19:44:33.800" FavoriteCount="1" Id="69791" LastActivityDate="2013-09-11T19:52:14.647" LastEditDate="2013-09-11T19:52:14.647" LastEditorDisplayName="user25658" OwnerUserId="29183" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;bic&gt;" Title="Computing BIC for SUR model" ViewCount="39" />
  <row AcceptedAnswerId="69827" AnswerCount="2" Body="&lt;p&gt;I am working with PCA to do dimensionality reduction to a set of data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 1600 data points with 36 variables and I want to have a matrix with a new data set with 6 principal components. I managed to do it in matlab, so I have:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xtrain (normalized) &amp;lt;1600x36&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and I write:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;coeff=pca(xtrain,'NumComponents',6) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives me a &amp;lt;36x6&gt; matrix&lt;/p&gt;&#10;&#10;&lt;p&gt;So far so good. Now I want to know how to reverse the process. If I'm given a data set of  how do I use the coeff matrix to pass it to a &quot;n x 36&quot; representation?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to do this in matlab but a simple explanation can help me.&lt;/p&gt;&#10;" ClosedDate="2015-02-15T01:32:16.953" CommentCount="4" CreationDate="2013-09-11T23:48:40.990" Id="69802" LastActivityDate="2013-09-12T13:02:38.230" LastEditDate="2013-09-12T05:54:03.780" LastEditorUserId="3277" OwnerUserId="30187" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;matlab&gt;&lt;matrix&gt;" Title="Reverse PCA with nonsquare matrix" ViewCount="621" />
  
  <row Body="&lt;p&gt;I would suggest watching this YouTube video &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=9v2d-9VfEK4&quot; rel=&quot;nofollow&quot;&gt;http://www.youtube.com/watch?v=9v2d-9VfEK4&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;in order to understand exactly how to use a random number table to answer your question.  The video show you how to select random numbers between 0 and 58 but you can follow the same routine to sample your random values. &lt;/p&gt;&#10;&#10;&lt;p&gt;I assume when you say a population of 10,000 you mean that your random values can be any value in the range 0  - 10,000 (correct me if I am wrong) and so you would pick a random starting spot in the your table and starting going down the rows and moving over one column at a time as you finish all the rows until you reach your desired sample size of 40 random values.&lt;/p&gt;&#10;&#10;&lt;h1&gt;Edit:&lt;/h1&gt;&#10;&#10;&lt;p&gt;So based on the table you provided above now, you could use columns 1 - 5 and work your way down accepting numbers (using all 5 columns) that fell in the range 0 - 10,000.  Once you are down with all the rows you would move over and then use columns 2 - 6 and proceed in similar fashion. However, if you exclude 10,000 then you could more efficiently use columns 1 - 4 and when done with those move over to 2 - 5 and so on. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-12T00:53:11.460" Id="69805" LastActivityDate="2013-09-12T01:39:17.677" LastEditDate="2013-09-12T01:39:17.677" LastEditorDisplayName="user25658" OwnerDisplayName="user25658" ParentId="69803" PostTypeId="2" Score="0" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset, for example, of success factors (columns) noted by several subjects (rows). Basically, when a subject notes a factor, its marked as a yes or no. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to do an analysis to find factors that seem to be related or appear together. correlation analysis. I'm thinking of giving the yes/no values either 1 or 0 respectively and performing covariance on that with excel.&#10;Is this a valid approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;Will appreciate any comments on this&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-12T15:51:22.753" Id="69870" LastActivityDate="2013-09-12T18:50:43.490" LastEditDate="2013-09-12T17:19:24.203" LastEditorDisplayName="user25658" OwnerUserId="30222" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;" Title="Correlation with Count Yes/No" ViewCount="55" />
  
  
  <row Body="&lt;p&gt;As you only have two classifiers there only few options that come to my mind:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;min/max value&lt;/li&gt;&#10;&lt;li&gt;Logical operators: AND, OR, XOR, NOR and the like.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;However my suggestion is twofold:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;First, I will focus not on the decision but on the score associated to that decision. If you use scores (probabilities, likelihood) you may use other ways of combining information (geometric mean, harmonic mean and so on).&lt;/li&gt;&#10;&lt;li&gt;Secondly, in case you are not interested on this former option, I would put my attention on those cases where both classifier disagree, analyze which of the former options (logical operators, min/max value) produce better performance than single classifiers and finally create a decisor-maker based on all these combinations.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I wish it works.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-12T21:35:23.210" Id="69903" LastActivityDate="2013-09-12T21:35:23.210" OwnerUserId="29943" ParentId="58204" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;A kind of PCA applied to cagetorical data is MCA (Multiple Correspondence analysis) that let you detect underlying structures in a data set.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-13T06:53:55.890" Id="69923" LastActivityDate="2013-09-13T06:53:55.890" OwnerUserId="29943" ParentId="31292" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="70309" AnswerCount="1" Body="&lt;p&gt;Is it possible to compare variable importance, obtained for different models, built on different datasets with simular variables, using caret package? The property are different, but close, and the variables used for modelling are the same. For example, one can calculate importance for pls and random forest model, then scale them to obtain values from 0 to 1. &#10;Would it be valid to compare such values for two different Y? Is there any way to compare two different statistical models, built on two different datasets in terms of variable influence?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-13T10:54:01.763" Id="69938" LastActivityDate="2013-09-24T18:15:59.023" LastEditDate="2013-09-24T18:15:59.023" LastEditorUserId="28707" OwnerUserId="28707" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;importance&gt;" Title="Variable Importance for different methods" ViewCount="87" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a question regarding quantile regression.  Supposing that I have 10000 observations with one response variable and several predictor variables in a dataset collected each year over several years.  I run a multivariate quantile regression at 90% percentile.  I want to compare the observed 90th percentile response value for each year (a single value) to the 90th percentile predicted response value for that year (a single value) based on the multivariate quantile regression model.  How can I do that?  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-13T16:09:54.727" FavoriteCount="0" Id="69963" LastActivityDate="2014-09-04T06:48:39.653" LastEditDate="2014-03-17T09:17:34.447" LastEditorUserId="26338" OwnerUserId="30269" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;multiple-regression&gt;&lt;quantile-regression&gt;" Title="Quantile regression" ViewCount="343" />
  
  <row Body="&lt;p&gt;I am confused.  You've done the prediction already?  And for each year you are comparing the 90% quantile of the predicted with the 90% quantile of the observed?&#10;This is just a linear regression but sounds like all that you really want to see is how well they agree.  In which case calculate the mean square error:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;MSE(x,y) = \dfrac{1}{N}\sum_{i=1}^N (x-y)^2&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Some other metric might also be possible such as the absolute difference.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-09-13T20:16:40.327" Id="69979" LastActivityDate="2014-09-04T06:48:39.653" LastEditDate="2014-09-04T06:48:39.653" LastEditorUserId="13310" OwnerUserId="13310" ParentId="69963" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The first thing to understand is that the assumption of normality when using the t-test can never be &lt;em&gt;proven&lt;/em&gt;.  One can only say that the data look approximately normal, and if this is the case, the t-test may be appropriate. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, even though the exclusion of negative values is technically a violation of the normality assumption, it might not be an important violation if the distribution of differences appears to be bell shaped.  Of course, a bell shape for absolute values can happen only if the distribution of differences is centered pretty far enough away from zero so that the truncation point at zero is not obvious.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-13T21:27:54.367" Id="69988" LastActivityDate="2013-09-13T21:27:54.367" OwnerUserId="27765" ParentId="69972" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;As mentioned above all the examples in the book are from SAS, JMP, and Minitab. The data files, however, are available online as text files so importing them into R isn't a problem (I've done it with a few of the example myself). There's no reason why you can't just replicate the examples in R using just the lm, aov, and Anova (from the car package) functions. Or you can follow the guidelines in the book to do things the long way round (decomposing the sums of squares individually by computing consecutive lm model fits and summing the squared residuals, for example). &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-01-15T15:28:23.077" CreationDate="2013-09-14T11:00:46.790" Id="70014" LastActivityDate="2013-09-14T11:00:46.790" OwnerUserId="24982" ParentId="64406" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You might want to look at &lt;a href=&quot;http://stats.stackexchange.com/questions/19103/how-to-statistically-compare-two-time-series/19334#19334&quot;&gt;How to statistically compare two time series?&lt;/a&gt; as it treats the question of discriminating between time series. Essentially a common model is used to estimate parameters both globally AND individually. The error sums of squares are then compared to determine if a significan reduction is obtained by individual estimation as compared to global estimation. Upon concluding that there are benefits from individual estimation one needs to identify which ones are different from the &quot;others&quot; . This is akin to finding which means are different from other means.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-14T22:42:02.200" Id="70044" LastActivityDate="2013-09-15T14:28:01.550" LastEditDate="2013-09-15T14:28:01.550" LastEditorUserId="3382" OwnerUserId="3382" ParentId="70008" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The basic idea of regression may be the 'cause and effect' or 'independent and dependent'. The normal practice of placing independent variable in the X axis and dependent variable in the Y axis, is represented by Y= mX+c. Whether the the slope is to be called as m (X on Y) or (Y on X) and the regression as: (X on Y) or (Y on X). It is handled in both ways, which is not good and needs to be clarified.&#10;Modellers frequently use Scatter Plots, to judge whether Simulated Series matches Observed Series; and use of regression line is unavoidable. here is no causative clause. Going by this necessity, the mute question posed by the thread stands.&#10;Or simply put, please clarify how to call the normal regression analysis: X on Y; or Y on X?, going beyond the causative answer.&#10;It is not an answer to the main thread; but a parallel question.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-15T11:54:47.703" Id="70062" LastActivityDate="2014-07-11T07:26:52.833" LastEditDate="2014-07-11T07:26:52.833" LastEditorUserId="-1" OwnerUserId="30311" ParentId="22718" PostTypeId="2" Score="-5" />
  <row Body="&lt;p&gt;Finish 10.000 MC runs and then start computing your confidence intervals. Compute e.g. the median value, which divides your probability distribution (PD) into two parts, where each part corresponds to 50% probability or area of your PD. Integrate your PD from -infinity to the z value covering 2.5% area of your PD, and integrate from zero to the z value covering 97.5% area of your PD. In total you then have the z values, which enclose 95% of your PD. This is denoted the 95 percentile. You can then compute something like a standard deviation by: z1=z975-median and z2=median-z025. Maybe have a look at &quot;confidence.pro&quot; (IDL language).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-15T19:51:46.827" Id="70087" LastActivityDate="2013-09-15T19:59:40.653" LastEditDate="2013-09-15T19:59:40.653" LastEditorUserId="30321" OwnerUserId="30321" ParentId="68767" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="70090" AnswerCount="1" Body="&lt;p&gt;I believe I read today a phrase which went something like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If a distribution has a mean and a variance ...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So I guess that means some distributions do not have means or variances?&lt;/p&gt;&#10;&#10;&lt;p&gt;I fiend that a bit difficult to understand - for example:&lt;/p&gt;&#10;&#10;&lt;p&gt;I could get data which is said to follow such a mean-less and variance-less distribution and as an estimation compute the mean and variance of such a data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;could someone help shed some light on what I am missing?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-15T21:32:23.760" Id="70088" LastActivityDate="2013-09-15T21:38:35.327" OwnerUserId="14163" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;variance&gt;&lt;mean&gt;" Title="When does a distribution not have a mean or a variance?" ViewCount="734" />
  
  <row Body="&lt;p&gt;the basic difference b/w Interpolation and regression is as follows:&#10;Interpolation:suppose there are n points (eg:10 data points),in interpolation we will fit the &lt;strong&gt;curve passing through all the data points&lt;/strong&gt; (i.e here 10 data points) with a degree of the polynomial (no.of data points -1; i.e here it is 9).where as in regression not all the data points only a set of them needed for curve fitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;generally the order of the Interpolation &amp;amp; regression will be (1,2 or 3) if the order is more than 3 ,more oscillations will be seen in the curve.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-16T06:42:11.263" Id="70109" LastActivityDate="2013-09-16T06:42:11.263" OwnerUserId="30339" ParentId="33659" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I do not know what you mean exactly by 'trajectory'. But what I understand is a one-dimensional vector with numeric values.&lt;/p&gt;&#10;&#10;&lt;p&gt;My suggestion here is to use Dynamic Time Warping (henceforth, DTW), a method able to 'align' two one-dimensional signals. Then, if you want to measure belongness, DTW provides you with several metrics to state to what extent two signals are similar.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, you can perform clustering based on the metrics provided by DTW instead of using the standard metrics involved whether in Fuzzy C-means or $k$-means.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-16T07:29:47.750" Id="70112" LastActivityDate="2013-09-16T07:29:47.750" OwnerUserId="29943" ParentId="70106" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I would say that these three group you indicated are indeed only two groups:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Statistics&lt;/li&gt;&#10;&lt;li&gt;Machine learning, artificial intelligence and pattern recognition.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;All the branches related to signal filtering are based on two aspects: feature extraction (wavelets, Gabor and Fourier) which belongs to pattern recognition and Discrete Fourier Transformation which belongs to hard mathematics. In fact, digital filtering is more close to a engineering side as it try to solve this pattern recognition problem by means of simple and low computational cost algorithms. But essentially, it is machine learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, Filtering, Wavelets, Gabor and Fourier are extensively used in image processing being the core of artificial vision.&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference exists between statistics and machine learning.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-09-16T07:47:55.790" CreationDate="2013-09-16T07:47:55.790" Id="70114" LastActivityDate="2013-09-16T07:47:55.790" OwnerUserId="29943" ParentId="14571" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The standard loss function for kernel ridge regression is:&#10;$||Y-K\beta||_2^2 + \lambda\beta^T K\beta$.&#10;The equation in your question is missing the kernel matrix K in the $L_2$ error term.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In practice, the Gaussian (a.k.a. RBF) and polynomial kernels are popular choices and could be a good starting point. However, the choice of kernel generally depends on the problem at hand. Sometimes it may be helpful to think of the kernel as a similarity metric for the input data vectors. You may need to experiment with different kernels to make an appropriate choice for the specific dataset.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Yes, in addition to $\lambda$, you will need to determine the kernel parameters through cross-validation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-09-16T11:17:36.120" Id="70127" LastActivityDate="2013-09-16T12:49:25.480" LastEditDate="2013-09-16T12:49:25.480" LastEditorUserId="7290" OwnerUserId="28627" ParentId="61023" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You probably want to do exploratory (or possibly confirmatory) factor analysis to check that all items can be scored in terms of your two theorised scales. This could be combined with reliability analysis. You would then score your two scales and then obtain the correlation between the two scales.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a &lt;a href=&quot;http://jeromyanglim.blogspot.com.au/2009/10/scale-construction-item-reversal-scale.html&quot; rel=&quot;nofollow&quot;&gt;tutorial which describes this process using SPSS including scoring the scales&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you wanted to study latent variables and the measurement model of the test you could use structural equation modelling software. If you were in the world of SPSS, this would involve using Amos.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-16T11:36:53.557" Id="70129" LastActivityDate="2013-09-26T05:22:14.717" LastEditDate="2013-09-26T05:22:14.717" LastEditorUserId="183" OwnerUserId="183" ParentId="67837" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="70226" AnswerCount="1" Body="&lt;p&gt;I'm quite new to this StackExchange, only been a lurker till now, but my StackOverflow fellows have said you'd be the best people to ask about this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, enough introduction. I'm using the weighted k-Nearest-Neighbours algorithm. My original data set has 37 features. I've looked into using PCA to reduce dimensionality, and I'm going to follow this method. &lt;/p&gt;&#10;&#10;&lt;p&gt;For simplicity's sake, let us assume that two of the new features created account for 90% of the variance and I'm only going to use these two new features. Let us call them feature 1 and feature 2 ($f_1$, $f_2$). Let us say that $f_1$ accounts for 60% of the variance and $f_2%$ accounts for 30% of the variance. I know wish to select the weights ($w_1 , w_2$) for these two features. My initial intuition is that we could correlate the variance accounted for with the weight of the feature. Therefore, I would use a weight combination of $w_1 = 0.6$ and $w_2 = 0.3$ in my k-Nearest-Neighbours algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am well aware that there is much literature suggesting the best way to select weights would be to use a lattice type of method where we select different combinations of weights and then follow through with combination that yields the best results. I was just wondering if the intuition of weights being related to total variance accounted for was. Also, as my dataset actually requires 11 features to be used to account for 90% of the variance, I'd like to have a starting point for determining combinations of weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; When using PCA as a precursor for kNN, is it possible to base the weights of features in k-NN on the total variance said features accounts for in the data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry if there are any formatting errors or if I'm breaking any protocols. Let me know if I have, and I will update the post.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-16T12:32:42.360" FavoriteCount="1" Id="70134" LastActivityDate="2013-09-17T09:17:52.590" LastEditDate="2013-09-16T12:49:40.797" LastEditorUserId="6029" OwnerUserId="30348" PostTypeId="1" Score="4" Tags="&lt;classification&gt;&lt;pca&gt;&lt;k-nearest-neighbour&gt;&lt;weighted-mean&gt;" Title="Using the eigenvalues from PCA in k-nearest-neighbours" ViewCount="385" />
  <row Body="&lt;p&gt;It depends on what, exactly, you're interested in. All of your proposals can make sense given different research aims. Population density is also measuring the area of the territory that the children are in, which might not be relevant to your research question -- unless, e.g., you think there are some interesting urban vs. rural effects -- so you could inadvertently introduce some weird biases to your analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same arguments go for children per adult or children per total population. &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, is there any reason that you can't just compare the number of children directly?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-16T12:49:55.357" Id="70136" LastActivityDate="2013-09-16T12:49:55.357" OwnerUserId="22311" ParentId="70135" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;@HarveyMotulsky is right, you can use the Mann-Whitney U-test with unequal sample sizes.  Note however, that your statistical power (i.e., the ability to detect a difference that really is there) will diminish as the group sizes become more unequal.  For an example, I have a simulation (actually of a t-test, but the principle is the same) that demonstrates this &lt;a href=&quot;http://stats.stackexchange.com/questions/31326/how-should-one-interpret-the-comparison-of-means-from-different-sample-sizes/31330#31330&quot;&gt;here&lt;/a&gt;.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-16T15:02:33.297" Id="70145" LastActivityDate="2013-09-16T15:02:33.297" OwnerUserId="7290" ParentId="40342" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;This problem is special, in the following way.  Label the $2n-1$ non-chair women with the numbers $1, 2, \ldots, 2n-1$ and label the $2n$ men with the numbers $1, 2, \ldots, 2n$.  Any all-female committee is determined by the $n$ labels of its members. Corresponding to such a committee are &lt;em&gt;two&lt;/em&gt; distinct all-male committees: the one of men having those labels and the one of the men &lt;em&gt;not&lt;/em&gt; having those labels.  Furthermore, exactly one of those all-male committees includes the man with label $2n$: call this the &quot;second&quot; committee and let the other choice of all-male committee be the &quot;first&quot; committee.&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice that any two distinct &quot;first&quot; committees correspond to distinct all-female committees, that any two distinct &quot;second&quot; committees correspond to distinct all-female committees, and no &quot;first&quot; committee can also be a &quot;second&quot; committee or conversely.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an illustration, consider the case $n=2$.  The $\binom{2n-1}{n}=3$ all-female committees and their $\binom{2n}{n}=6$ all-male counterparts are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign {&#10;\text{Women} &amp;amp;  &amp;amp; \text{First men}; &amp;amp; \text{Second men}\\&#10;\{1,2\} &amp;amp;\to &amp;amp;\{1,2\}; &amp;amp; \{3,4\}\\&#10;\{1,3\} &amp;amp;\to &amp;amp;\{1,3\}; &amp;amp; \{2,4\}\\&#10;\{2,3\} &amp;amp;\to &amp;amp;\{2,3\}; &amp;amp; \{1,4\}.&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, there are exactly two possible all-male committees for each all-female committee.  If in fact all members of the committee are of the same sex, then it must be exactly twice as likely that they are male than that they are female.  &lt;strong&gt;The answer does not depend on $n$!&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2013-09-16T16:00:13.513" Id="70151" LastActivityDate="2013-09-16T16:00:13.513" OwnerUserId="919" ParentId="70108" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;There is a distinction which sometimes doesn't get enough attention, namely &lt;strong&gt;hypothesis generation vs. hypothesis testing&lt;/strong&gt;, or exploratory analysis vs. hypothesis testing. You are allowed all the dirty tricks in the world to come up with your idea / hypothesis. But when you later test it, you must ruthlessly kill your darlings.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm a biologist working with high throughput data all the time, and yes, I do this &quot;slicing and dicing&quot; quite often. Most of the cases the experiment performed was not carefully designed; or maybe those who planned it did not account for all possible results. Or the general attitude when planning was &quot;let's see what's in there&quot;. We end up with expensive, valuable and in themselves &lt;em&gt;interesting&lt;/em&gt; data sets that I then turn around and around to come up with a story. &lt;/p&gt;&#10;&#10;&lt;p&gt;But then, it is only a story (possible bedtime). After you have selected a couple of interesting angles -- and here is the crucial point -- you must test it not only with independent data sets or independent samples, but preferably with an independent &lt;em&gt;approach&lt;/em&gt;,  an independent experimental system.&lt;/p&gt;&#10;&#10;&lt;p&gt;The importance of this last thing -- an independent experimental setup, not only independent set of measurements or samples -- is often underestimated. However, when we test 30,000 variables for significant difference, it often happens that while similar (but different) samples from the same cohort and analysed with the same method will not reject the hypothesis we based on the previous set. But then we turn to another type of experiment and another cohort, and our findings turn out to be the result of a methodological bias or are limited in their applicability.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is why we often need several papers by several independent researchers to really accept a hypothesis or a model.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I think such data torturing is fine, as long as you keep this distinction in mind and remember what you are doing, at what stage of scientific process you are. You can use moon phases or redefine 2+2 as long as you have an &lt;em&gt;independent&lt;/em&gt; validation of the data. To put it on a picture:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sQNcc.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, there are those who order a microarray to round up a paper after several experiments have been done and no story emerged, with the hope that the high throughput analysis shows something. Or they are confused about the whole hypothesis testing vs. generation thing.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-16T16:21:45.060" Id="70152" LastActivityDate="2013-09-16T16:30:27.567" LastEditDate="2013-09-16T16:30:27.567" LastEditorUserId="14803" OwnerUserId="14803" ParentId="70150" PostTypeId="2" Score="18" />
  <row Body="&lt;p&gt;Sometimes the things you see as &quot;data torture&quot; aren't really. It's not always clear beforehand exactly what you're going to do with data to give what you believe are the genuine results of the experiment until you see it.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, with reaction time data for a decision task, you often want to reject times that aren't about the decision (i.e., when they are going so fast they are obviously just guessing and not making a decision). You can plot accuracy of the decision against RT to see where the guessing is generally occurring. But until you've tested that particular paradigm you have no way of knowing where the cutoffs are (in time, not accuracy). To some observers such a procedure looks like torturing the data but as long as it doesn't have anything directly to do with the hypothesis tests (you're not adjusting it based on tests) then it's not torturing the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data snooping during an experiment is ok as long as it's done the right way. It's probably unethical to stick your experiment in a black box and only do the analysis when the planned number of subjects have been run. Sometimes it's hard to tell that there are issues with the experiment until you look at data and you should look at some as soon as possible. Data peeking is strongly disparaged because it's equated to seeing if p &amp;lt; 0.05 and deciding to continue. But there are lots of criteria by which you can decide to continue collecting that do not do anything harmful to your error rates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say you want to make sure that your variance estimate is within a known likely range. Small samples can have pretty far out variance estimates so you collect extra data until you know the sample is more representative. In the following simulation I expect the variance in each condition to be 1. I'm going to do something really crazy and sample each group independently for 10 samples and then add subjects until variance is close to 1.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y &amp;lt;- replicate(1000, {&#10;    y1 &amp;lt;- rnorm(10)&#10;    while(var(y1) &amp;lt; 0.9 | var(y1) &amp;gt; 1.1) y1 &amp;lt;- c(y1, rnorm(1))&#10;    y2 &amp;lt;- rnorm(10)&#10;    while(var(y2) &amp;lt; 0.9 | var(y2) &amp;gt; 1.1) y2 &amp;lt;- c(y2, rnorm(1))&#10;    c( t.test(y1, y2, var.equal = TRUE)$p.value, length(y1), length(y2) )&#10;    })&#10;range(Y[2,]) #range of N's in group 1&#10;[1]   10 1173&#10;range(Y[3,]) #range of N's in group 2&#10;[1]   10 1283&#10;sum(Y[1,] &amp;lt; 0.05) / ncol(Y)&#10;[1] 0.045&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, I've just gone bonkers with the sampling and making my variances close to expected and I still don't affect alpha much (it's a little under 0.05). A few more constraints like the N's must be equal in each group and can't be more than 30 and alpha is pretty much right on 0.05. But what about SE? What if I instead tried to make the SE a given value? That's actually a really interesting idea because I'm in turn setting the width of CI in advance (but not the location).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;se &amp;lt;- function(x) sqrt(var(x) / length(x))&#10;Y &amp;lt;- replicate(1000, {&#10;        y1 &amp;lt;- rnorm(10)&#10;        y2 &amp;lt;- rnorm(10)&#10;        while(se(y1) &amp;gt; 0.2 | se(y2) &amp;gt; 0.2) {&#10;            y1 &amp;lt;- c(y1, rnorm(1)); y2 &amp;lt;- c(y2, rnorm(1))&#10;        }&#10;        c( t.test(y1, y2, var.equal = TRUE)$p.value, length(y1) )&#10;        })&#10;range(Y[2,]) #range of N's in group 1 and 2 (they're equal now)&#10;[1] 10 46&#10;sum(Y[1,] &amp;lt; 0.05) / ncol(Y)&#10;[1] 0.053&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Again, alpha changed a small amount even though I've allowed N's to roam up to 46 from the original 10 based on data snooping. More importantly, the SE's all fall in a narrow range in each of the experiments. It's easy to make a small alpha adjustment to fix that if it's a concern. The point is that some data snooping does little to no harm and can even bring benefits.&lt;/p&gt;&#10;&#10;&lt;p&gt;(BTW, what I'm showing isn't some magic bullet. You don't actually reduce the number of subjects in the long run doing this because power for the varying N's simulation is about the same as for a simulation of the average N's)&lt;/p&gt;&#10;&#10;&lt;p&gt;None of the above contradicts the recent literature on adding subjects after an experiment started. In those studies they looked at simulations where you added subjects after doing a hypothesis test in order to get the p-value lower. That's still bad and can extraordinarily inflate alpha. Furthermore, I really like January and Peter Flom's answers. I just wanted to point out that looking at data while you're collecting it, and even changing a planned N while collecting, are not necessarily bad things.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-09-16T16:57:46.577" Id="70156" LastActivityDate="2013-11-08T05:10:08.197" LastEditDate="2013-11-08T05:10:08.197" LastEditorUserId="601" OwnerUserId="601" ParentId="70150" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;With the interaction, the main effect of &quot;during&quot; (or anything else, but this is the variable that you asked about) is only accurate for the case where the other variable in the interaction is 0: That is, where &quot;season&quot; is not &quot;winter&quot;, &quot;pre-breeding&quot; or &quot;migration&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the parameter estimates for the interaction terms are negative (thus canceling some of the positive main effect) and one of them is large.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would also be concerned about the singularities..&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-16T17:42:53.763" Id="70160" LastActivityDate="2013-09-16T17:42:53.763" OwnerUserId="686" ParentId="70158" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;In Bayesian terms, you have some observable property $X$ and a parameter $\Theta$. The joint distribution for $X,\Theta$ is specified, but factored as the conditional distribution of $X\mid \Theta$ and the prior distribution of $\Theta$. A statistic $T$ is sufficient for this model if and only if the posterior distribution of $\Theta\mid X$ is the same as that of $\Theta\mid T(X)$, &lt;strong&gt;for every prior distribution of&lt;/strong&gt; $\Theta$. In words, your updated opinion about $\Theta$ after knowing the value of $X$ is the same as your updated opinion about $\Theta$ after knowing the value of $T(X)$, &lt;strong&gt;whatever prior opinion you have about&lt;/strong&gt; $\Theta$. Keep in mind that sufficiency is a model dependent concept.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-16T18:36:09.283" Id="70167" LastActivityDate="2013-09-16T18:41:43.817" LastEditDate="2013-09-16T18:41:43.817" LastEditorUserId="9394" OwnerUserId="9394" ParentId="44063" PostTypeId="2" Score="5" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am attempting to monitor the performance of a software package in order to identify when a change to the code base introduces a performance regression (a slow down in the code). My assumption is that if the performance of the data is unchanged, I'm essentially collecting samples from the same population, and I should be able to use a wilcoxon signed-rank test to test my null hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;To test out this process, I collected 100 samples of performance data for a test application, each containing 1000 observations. For each sample, I compared it to each of the other samples using Python's &lt;code&gt;scipy.stats.wilcoxon&lt;/code&gt; function, for example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sample    Sample    Wilcoxon test    p-value   &#10;0         1         228976           0.019&#10;0         2         227054           0.011&#10;0         3         237746           0.171&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then, for each sample, I counted the number of comparisons for that sample for which the p-value was less than my target threshold (0.05). I would have expected ~5% of the samples to be less than this threshold, however in my observations this was typically closer to ~30%, for example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sample    # comparison p-values &amp;lt; 0.05&#10;0         28&#10;1         36&#10;2         40&#10;3         25&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This seems to imply that over 30% of the samples are showing a statistically significant performance difference, even though the performance should be essentially the same (the same code is executed in each instance). I'm attempting to understand what in my experiment could lead to that sort of result, and find out if there is a way to correct it. Or alternatively, if there is a more effective way to determine if a slow down as observed in a software performance test is significant.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-16T20:26:25.170" FavoriteCount="2" Id="70183" LastActivityDate="2013-09-19T15:08:09.897" LastEditDate="2013-09-17T00:01:51.553" LastEditorUserId="30355" OwnerUserId="30355" PostTypeId="1" Score="5" Tags="&lt;hypothesis-testing&gt;&lt;wilcoxon&gt;" Title="Evaluating software performance to identify regressions resulting from source code changes" ViewCount="170" />
  
  <row Body="&lt;p&gt;The specific answer is no, this is not how &lt;code&gt;lme()&lt;/code&gt; works. &lt;code&gt;corAR1()&lt;/code&gt; will estimate a single additional parameter $\phi$ which is the AR(1) coefficient applied &lt;em&gt;within&lt;/em&gt; any specified nesting via argument &lt;code&gt;form&lt;/code&gt;. In other words, the same AR(1) is then assumed for, in your case, each subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same applies to &lt;code&gt;corARMA()&lt;/code&gt;; a single estimate of of the AR and MA parameters is made, across all levels of any grouping factor. Hence if you specify an ARMA(2,1), the same ARMA(2,1) is assumed to operate within the levels of the grouping factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could investigate this for yourself by fitting a model with an AR correlation structure for residuals nested within subject &lt;code&gt;corAR1(form = time | subject)&lt;/code&gt; and note the change in model degrees of freedom compared with a model fitted without the &lt;code&gt;corAR1()&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, when you specify say &lt;code&gt;corAR1(form = time | subject)&lt;/code&gt;, you are asking &lt;code&gt;lme()&lt;/code&gt; to estimate a single parameter $\phi$ which is then applied within the groups defined by &lt;code&gt;subject&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; that there is &lt;strong&gt;zero&lt;/strong&gt; correlation between groups (&lt;code&gt;subject&lt;/code&gt;s).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-16T22:33:20.323" Id="70193" LastActivityDate="2013-09-16T23:30:21.863" LastEditDate="2013-09-16T23:30:21.863" LastEditorDisplayName="user25658" OwnerUserId="1390" ParentId="70188" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This is really a cultural problem of unbalanced thinking, where publication bias leads to the favouring of positive results and our competitive nature requires editors and researchers to be seen to be producing results of interest that are novel or contentious, for example, in the sense of rebutting someone else's results.  In medical research there is considerable progress being made to redress this problem by the compulsory registration of trials and publication of results with records of abandoned trials to also be made public. I understand that since publication in journals for unsuccessful research may not be practicable, there are plans to keep a publicly available database of them.  Unusual results that can not be replicated are not necessarily a result of misdemeanour, as with perhaps 50,000 (a guess) researchers worldwide doing several experiments a year, some pretty unusual results are to be expected from time to time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using different methods is not necessarily a solution either.  For example, what chemist would mix reagents in different ways in different conditions and expect the same results as a matter of course?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-16T23:54:35.847" Id="70198" LastActivityDate="2013-09-16T23:54:35.847" OwnerUserId="19815" ParentId="70150" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I've been a member of CrossValidated for a little over two years now, and the site is set as my homepage on both my work and home machines. The answers I've gotten from this site were invaluable while I was working on my PhD, and some of the questions - and the answers to them - have helped spark some new thoughts in terms of data analysis and visualization in my own work.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to help give back a little.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've attempted, in my answers (though like Peter, my top ranked answer is to a joke :/ ) to provide patient, approachable answers to the &lt;em&gt;doing&lt;/em&gt; of statistics and data analysis, where my expertise lies (the mathematical details are best left to others - a good moderator knows their limits).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your consideration, and good luck to all the others who have put their name in the hat.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2013-09-17T07:11:06.100" CreationDate="2013-09-17T07:11:06.100" Id="70216" LastActivityDate="2013-09-17T07:11:06.100" LastEditDate="2013-09-17T07:11:06.100" LastEditorUserId="5836" OwnerUserId="5836" PostTypeId="6" Score="0" />
  
  <row Body="&lt;p&gt;Always use the scores the way they come out of the classifier. If you take absolute values you are basically changing the classifier's ranking and you will obtain an erroneous ROC curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;The values of scores across classifiers are entirely irrelevant: the only thing that matters to plot ROC curves is the ranking produced by each classifier, which is based on the scores. In ROC analysis you only compare rankings, irrelevant of what the scores may have been.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you insist on transforming scores (which is useless), whatever transformation you do &lt;em&gt;must&lt;/em&gt; keep the original ranking intact. For example, you could scale the scores with a positive constant or add an arbitrary (but constant!) value to them.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-17T09:10:29.953" Id="70225" LastActivityDate="2013-09-17T09:18:00.033" LastEditDate="2013-09-17T09:18:00.033" LastEditorUserId="25433" OwnerUserId="25433" ParentId="70223" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="70254" AnswerCount="1" Body="&lt;p&gt;I have been asked by a client to write a program in R which calculates the &quot;binomial dispersion test.&quot; I have not found anything for this, but I have found that you can calculate the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/Binomial.html&quot; rel=&quot;nofollow&quot;&gt;binomial distribution&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are they the same thing? I suspect they are.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-17T13:51:45.160" Id="70251" LastActivityDate="2013-09-18T14:28:13.470" LastEditDate="2013-09-17T13:58:58.903" LastEditorUserId="88" OwnerUserId="27026" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;binomial&gt;" Title="Is binomial dispersion the same as binomial distribution?" ViewCount="137" />
  
  <row Body="&lt;p&gt;You should be dividing the $p$-value (0.034) by two, not the $t$-value. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-17T13:55:10.830" Id="70253" LastActivityDate="2013-09-17T13:57:22.573" LastEditDate="2013-09-17T13:57:22.573" LastEditorUserId="21054" OwnerUserId="3262" ParentId="70248" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="70263" AnswerCount="1" Body="&lt;p&gt;I am working with a set of historical data that I did not collect myself. I cannot add to this data set in any way. For a 350-year period I have many thousands of data points, each associated with one of ~50 time points. Some of the time points have hundreds of data points associated with them, so I'm sure that those time points can be used in the linear regression. However, others have very few, even just one.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, how can I determine a cut-off for the time points that have enough data (and should be included in my regression) and the time points that do not have enough data (and should be excluded)? I'd like to do this in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, perhaps this is relevant - I'm interested in the minimum for each time point, not the mean. Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-17T14:42:48.813" Id="70261" LastActivityDate="2013-09-17T15:14:06.020" OwnerUserId="29787" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;" Title="Establish minimum sample size for linear regression with historical data" ViewCount="59" />
  
  
  
  <row Body="&lt;p&gt;You can't get $P(x)$ for a continuous distribution. It is 0. To get $P(a \leq x \leq b)$ you would compute the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{a}^{b} f(x) \ dx $$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-17T18:19:49.877" Id="70295" LastActivityDate="2013-09-17T18:19:49.877" OwnerUserId="30402" ParentId="70293" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have data that are generated like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;An argument $i$ comes to $c_i \in \mathbb{N}_0$ conclusions, where $i \in N,|N|=n$.&lt;/li&gt;&#10;&lt;li&gt;A respondent assigns $f_{ij} \in \mathbb{N}_0$ fallacies to conclusion $j$ of argument $i$.&lt;/li&gt;&#10;&lt;li&gt;I divide the total number of fallacies by the number of conclusions to obtain an invalidity score $(\sum_j f_{ij})/c_i$.&lt;/li&gt;&#10;&lt;li&gt;Both $f_{ij}$ and $c_i$ are random variables.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What is a good probability distribution to describe this data model? Here are the ones I've considered:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A gamma distribution.&lt;/li&gt;&#10;&lt;li&gt;A continuous extension of the Poisson distribution (e.g. &lt;a href=&quot;http://cmscience.blogspot.com/2008/04/mt-6-poisson-and-gamma-distribution.html&quot; rel=&quot;nofollow&quot;&gt;see here&lt;/a&gt;).&lt;/li&gt;&#10;&lt;li&gt;?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="13" CreationDate="2013-09-18T09:16:01.970" FavoriteCount="1" Id="70344" LastActivityDate="2013-09-19T10:21:59.227" LastEditDate="2013-09-19T10:21:59.227" LastEditorUserId="7616" OwnerUserId="7616" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;" Title="What is the right probability distribution for this ratio?" ViewCount="109" />
  
  
  
  
  <row Body="&lt;p&gt;You have an error:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Ee^{pX}=\int_{-\infty}^{\infty}e^{px}f_X(x)dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note the limits of integration, you had them wrong. This way you can differentiate under integral. Of course there are conditions, when you can do this exactly, but you can assume that they are met. &lt;/p&gt;&#10;&#10;&lt;p&gt;The fact that you are integrating over infinite interval is a complication, but still everything depends on the properties of $exp$ and $f_X$. I.e. as long as the integral is differentiable function and some additional smoothness properties (probably), you can differentiate inside the integral.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-18T11:31:47.310" Id="70351" LastActivityDate="2013-09-18T11:31:47.310" OwnerUserId="2116" ParentId="70348" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;This question doesn't really have anything to do with MLM as it stands now. You're just predicting &lt;code&gt;grade&lt;/code&gt; with &lt;code&gt;class&lt;/code&gt; and &lt;code&gt;sex&lt;/code&gt;. You just just want to look up data imputation. There are loads of methods there. When you've read a bit on that, if you have trouble, come back here and ask another question.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should keep in mind that the best methods of data imputation will not have a fixed value to stick into your missing data slots; rather they will define the probably distribution from which the value should come from.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Old answer in case you really do have some random grouping variables you haven't identified&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's not just that each data point belongs to some group in MLM but that the group they belong to is a random variable. Simple between subject designs have each individual in some group, just like your male / female &lt;code&gt;sex&lt;/code&gt; variable. But &lt;code&gt;class&lt;/code&gt; is a variable that you define in an MLM as a random variable. You don't have predictions for the values in each class, nor do you typically particularly care about them. The classes are a random sample of possible classes for which you will care more about the estimates of the variance than any particular class. The goal is to generalize across classes. Your &lt;code&gt;sex&lt;/code&gt; variable is clearly not a random sample of sexes and you don't plan to generalize one value across the many sexes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without knowing more about your design it's hard to say whether your other categorical variables are more like &lt;code&gt;class&lt;/code&gt; or &lt;code&gt;sex&lt;/code&gt; but I'm guessing that they're the latter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, your would simply model this with &lt;code&gt;sex&lt;/code&gt; as a fixed effect and &lt;code&gt;class&lt;/code&gt; as a random grouping variable that your fixed effects are nested within. In &lt;code&gt;lme4&lt;/code&gt; package in R the model would look something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer( grade ~ sex + (sex|class), data = dat )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That's &lt;code&gt;grade&lt;/code&gt; predicted by &lt;code&gt;sex&lt;/code&gt; and also both &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;intercept&lt;/code&gt; grouped by &lt;code&gt;class&lt;/code&gt; as random effects. I've allowed &lt;code&gt;sex&lt;/code&gt; to both be fixed and randomly vary by &lt;code&gt;class&lt;/code&gt; (but that doesn't make it the random variable).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a variety of papers that will teach you basics of MLM. If you can just read basic linear model syntax and don't want to see any matrix modelling thrown up then you might find the early sections of Barr et al (2013) very educational.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you construct a model like the above you can get missing values filled in by taking predicted values from the model. After you get that far you should search for methods of getting predicted values from your model to fill in your missing data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirma- tory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language&lt;/em&gt;, &lt;em&gt;68&lt;/em&gt;, 255-278. doi: dx.doi.org/10.1016/j.jml.2012.11.001&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-18T12:30:31.417" Id="70360" LastActivityDate="2013-09-18T12:54:05.083" LastEditDate="2013-09-18T12:54:05.083" LastEditorUserId="601" OwnerUserId="601" ParentId="70347" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;A group of raters (about 20) will be watching a series of videos and will be classifying them into 4 categories. I will be running a Fleiss' kappa to measure the agreement. How does one compute for the sample size to arrive at 0.8 power, 0.05 alpha? Also, will that sample size be the number of videos to be evaluated?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-18T14:32:33.643" Id="70374" LastActivityDate="2015-02-06T21:13:45.937" LastEditDate="2013-09-18T15:33:24.587" LastEditorUserId="16380" OwnerUserId="16380" PostTypeId="1" Score="2" Tags="&lt;sample-size&gt;&lt;kappa&gt;" Title="Sample size needed for Fleiss' Kappa?" ViewCount="328" />
  <row Body="&lt;p&gt;You may also want to look at &lt;em&gt;The R Book&lt;/em&gt; by Crawley (&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470973927&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/The-Book-Michael-J-Crawley/dp/0470973927&lt;/a&gt;), which I find to be a great toolbox-type resource for R modeling. It runs through high-level implementations of many common modeling techniques using simple datasets, and will frequently reuse the same datasets to show how different models compare.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is not completely theory-free and probably a little long if your topic is just one part of a larger class, but the points at which he applies theory are usually used to show how to derive the answer in R outside of the modeling functions, which I found useful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-18T15:18:41.230" Id="70378" LastActivityDate="2013-09-18T15:18:41.230" OwnerUserId="30435" ParentId="70345" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="70384" AnswerCount="1" Body="&lt;p&gt;After using &lt;code&gt;ezANOVA&lt;/code&gt; as my primary way of specifying mixed ANOVAs, I've hit a stumbling block when it come to adding a covariate to the model. I am using an ANCOVA in order to determine if there is a developmental trajectory in my data; namely, I need to be able to see the F-statistic and p-values for interactions with the covariate (see p.466 onwards &lt;a href=&quot;http://www.psyc.bbk.ac.uk/research/DNL/personalpages/annaz_etal_2009.pdf&quot; rel=&quot;nofollow&quot; title=&quot;here&quot;&gt;here&lt;/a&gt; if you want an example).&lt;/p&gt;&#10;&#10;&lt;p&gt;Using &lt;code&gt;ezANOVA&lt;/code&gt;, I can include covariates but the output does not show the F-statistic and p-values for interactions with the covariate - the main effect of the covariate is also not tested using this method.&lt;/p&gt;&#10;&#10;&lt;p&gt;My &lt;code&gt;ezANOVA&lt;/code&gt; model is as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov.model&amp;lt;-ezANOVA(&#10;      data=textureView.child.outliersRemoved&#10;      , dv=.(x)&#10;      , wid=.(ID)&#10;      , within=.(Texture,View)&#10;      , between=.(TNOGroup)&#10;      , between_covariates=.(Age)&#10;      , type=3&#10;      , return_aov=TRUE&#10;      )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Another option is to use &lt;code&gt;lm&lt;/code&gt; or &lt;code&gt;Anova&lt;/code&gt;, but I don't know how to specify the error terms properly for either and I'm limited because I want to use Type-III sums of squares (&lt;code&gt;drop1&lt;/code&gt; doesn't work in the cases where I've tried to use the &lt;code&gt;aov&lt;/code&gt; wrapper for &lt;code&gt;lm&lt;/code&gt;; it fails while reporting &lt;code&gt;'Error in formula.default(object, env = baseenv()) : invalid formula'&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I've heard about using the &lt;code&gt;nlme&lt;/code&gt; package to specify my ANCOVA as a mixed model instead, but I don't know where to begin here (despite spending a while reading about it).&lt;/p&gt;&#10;&#10;&lt;p&gt;To give a summary, I'm trying to do a 2 (between; TNOGroup) x 2 (within, Texture) x2 (within, View) mixed ANCOVA, with age as a covariate. I want to use Type-III sums of squares, and see the F-statistic and p-values for interactions with the covariate, as well as for the main effect of the covariate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice on the best way to do this would be much appreciated. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-18T15:20:03.397" Id="70379" LastActivityDate="2013-09-18T16:18:55.637" OwnerUserId="13554" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;ancova&gt;&lt;lm&gt;&lt;nlme&gt;" Title="Best way to specify a mixed ANCOVA in R?" ViewCount="438" />
  <row AcceptedAnswerId="70400" AnswerCount="1" Body="&lt;p&gt;Given $X$ is a continuous random variable whose density is symmetric about a point $a$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Show that $V=X-a$ and $U=a-X$ have same distribution.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;$$F_U(u) = P(U \leq u) = P(X-a \leq u) = F_X(a+u)$$   and similarly $$F_W(w) = 1 - F_X(a-w)  \longrightarrow f_U(w) = f_X(a+w) = f_X(a-w)$$ by symmetry. Therefore,  $f_U(u) = f_X(a+u)$  by changing variable $w$ to $u$, which shows $f_U(u)=f_W(u)$.  Is this solution right? Thanks! &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-18T17:20:55.837" Id="70394" LastActivityDate="2013-09-20T23:36:02.790" LastEditDate="2013-09-20T23:36:02.790" LastEditorUserId="4856" OwnerUserId="30438" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;moments&gt;&lt;cdf&gt;" Title="Show that the random variable $V=X-a$ and $U=a-X$ have same distribution?" ViewCount="161" />
  
  
  
  
  <row AcceptedAnswerId="70419" AnswerCount="1" Body="&lt;p&gt;I have done linear regression and plotted the data, the regression line and also the confidence interval (for 95% confidence).  However it seems that most of the data points fall outside the confidence interval.  So how am I supposed to interpret the confidence interval.  It cannot be &lt;em&gt;I am 95% confident that the data point will be this close to the regression line&lt;/em&gt; since a lot more than 5% of the data points do not fall in that area.  So what does it mean then?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-09-18T18:36:53.587" FavoriteCount="2" Id="70410" LastActivityDate="2015-02-28T01:03:46.570" LastEditDate="2013-09-18T19:05:59.420" LastEditorUserId="1390" OwnerUserId="28524" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;confidence-interval&gt;" Title="Confidence intervals for regression interpretation" ViewCount="754" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Given $X_t=\sin(2\pi Ut)$ with $U$ uniformly distributed over $(0,1)$ and $t$ integer,&#10;how can I prove that $X_t$ is weakly stationary? (I didn't manage to calculate the correlation because I didn't have the joint distribution of $(X_t,X_s)$.)&#10;How can I prove that $X_t$ is not strictly stationary?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-09-18T19:01:04.280" Id="70414" LastActivityDate="2013-10-07T09:45:18.407" LastEditDate="2013-10-07T09:45:18.407" LastEditorUserId="2116" OwnerUserId="30445" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;self-study&gt;" Title="How to demonstrate that this process is weakly stationary?" ViewCount="876" />
  <row AnswerCount="3" Body="&lt;p&gt;I have data on 44 firms that have all been ranked by an expert. The &quot;best&quot; firm has rank 1, the second best has rank 2, ..., the last one has rank 44. &#10;I have a bunch of explanatory variables and would like to explain the rank of the firm on the basis of these variables. My inclination is to use a regression model, but am concerned about the fact that the dependent variable is limited, it can only be a positive discrete number. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have thought about ordinal regression, but that seems impossible since I would have as many categories as I have observations. &lt;/p&gt;&#10;&#10;&lt;p&gt;What regression models would be possible? (preferably to be run in R)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-18T19:30:10.233" Id="70417" LastActivityDate="2013-10-19T12:39:35.157" LastEditDate="2013-10-19T09:15:25.653" LastEditorUserId="22047" OwnerUserId="30446" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;ordered-variables&gt;" Title="Regression with rank order as dependent variable" ViewCount="519" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose that I have a regression model&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;Y = X\beta_X + Z_1 \beta_{Z_1} + Z_2 \beta_{Z_2} + \varepsilon&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $X$ are some controls, and $Z_1$ and $Z_2$ are two sets of dummy ($\{0,1\}$-valued) variables. I assume that $Z_2$ does not depend upon $Z_1$, but $Z_1$ might depend upon $Z_2$ in the sense that the probability that an element of $Z_1$ might be affected by the value of $Z_2$. I would like to decompose the information (variation) in $Z_2$ into a component that affects $Y$ only through $Z_1$, and a residual that affects $Y$ directly. &lt;/p&gt;&#10;&#10;&lt;p&gt;If $Z_1$ were continuous, I could do something like regress (each coordinate of) $Z_1$ onto $Z_2$, then regress $Y$ on the fitted values. The natural analogue of this, say fitting a &lt;em&gt;logistic&lt;/em&gt; (or similar) regression model for $Z_1$ gives up linearity, losing interpretability, and takes much longer to compute.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would appreciate any insight on literature addressing this or similar problems.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-18T23:57:15.740" FavoriteCount="1" Id="70434" LastActivityDate="2013-12-18T08:43:47.657" LastEditDate="2013-09-19T00:25:48.360" LastEditorUserId="30454" OwnerUserId="30454" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;categorical-data&gt;&lt;references&gt;&lt;mediation&gt;" Title="A regression decomposition for dummy variables" ViewCount="121" />
  
  <row Body="&lt;p&gt;I think that if this is a homework question then you are supposed to say so?&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, lots of shrinkage methods will perform feature selection for you at the same time. Start with the Lasso (&lt;a href=&quot;http://www-stat.stanford.edu/~tibs/lasso.html&quot; rel=&quot;nofollow&quot;&gt;http://www-stat.stanford.edu/~tibs/lasso.html&lt;/a&gt;), which you seem to be on the right track towards (for orthogonal features it will select features on the weights, exactly as you say).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-19T00:35:31.427" Id="70438" LastActivityDate="2013-09-19T00:35:31.427" OwnerUserId="30454" ParentId="70422" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Let's say I have a collection of objects.  Each object has a set of predictive attributes where the attribute may be true or false.  Each attribute when true predicts &quot;success&quot; with a known probability, and these probabilities are not necessarily the same for different attributes.  What is the probability of success for an object with more than one true attribute?  For example, if being male has a 25% chance of success, and being under 35 has a 50% chance of success, what are the chances of success for a male under 35, assuming the two attributes are independent in terms of success?&lt;/p&gt;&#10;&#10;&lt;p&gt;Second question.  Let's say I know the z-value for attribute A, and the z-value for attribute B.  What would I expect the z-value to be for (A and B), assuming A and B are independent?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-19T01:08:32.673" Id="70442" LastActivityDate="2014-01-26T15:42:14.057" LastEditDate="2013-09-26T07:32:46.240" LastEditorUserId="12602" OwnerUserId="12602" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;z-test&gt;" Title="What is the probability of success when two independent predictors co-occur?" ViewCount="170" />
  <row Body="&lt;p&gt;In the approach you write for the case where the r.v.'s were continuous, you would regress $Z_1$ on $Z_2$ and use the fitted values $\hat Z_1$ in the $Y$-regression. But $\hat Z_1 = \hat E(Z_1\mid Z_2)$. So what you need is the conditional expectation function of $Z_1$ given $Z_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.wisc.edu/~wahba/ftp1/tr1170.pdf&quot; rel=&quot;nofollow&quot;&gt;This paper has a nice exposition for the bivariate Bernoulli distribution&lt;/a&gt; (as an introduction to the multivariate case).&lt;/p&gt;&#10;&#10;&lt;p&gt;Denoting  the joint probability $P(Z_1 = 0, Z_2=0)=p(0,0) $ and analogously for the other three possible combinations, the conditional density of $Z_1\mid Z_2$ is (eq. 2.11 of paper)&#10;$$P(Z_1=z_1\mid Z_2=z_2) = \left(\frac {p(1,z_2)}{p(1,z_2)+p(0,z_2)}\right)^{z_1}\left(\frac {p(0,z_2)}{p(1,z_2)+p(0,z_2)}\right)^{1-z_1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then the conditional expectation function we are looking for is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(Z_1\mid Z_2=z_2) = \frac {p(1,z_2)}{p(1,z_2)+p(0,z_2)} \equiv Z_1^* $$&lt;/p&gt;&#10;&#10;&lt;p&gt;You will then have to approximate the joint probabilities, creating a standard $2 \times 2$ contingency table of relative empirical frequencies -from which actually you can also test your dependence suspicion: theoretically, $Z_1$ and $Z_2$ will be independent if and only if $p(0,0)\cdot p(1,1) = p(0,1)\cdot p(1,0)$ and you can test this by a &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher exact test&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that independence is rejected, then the contigency table will give you the estimated probabilities that you need and you will obtain the regressor&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat Z_1^* = \frac {\hat p(1,z_2)}{\hat p(1,z_2)+\hat p(0,z_2)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;..which will produce a series of numbers, as for each observation $j$ of the sample we will use the ratio that corresponds to the value $z_{2j}$ of $Z_2$, i.e. if $z_{2j}=1$ we will have &#10;$$\hat z_{1j}^* = \frac {\hat p(1,1)}{\hat p(1,1)+\hat p(0,1)}$$&#10;etc. $\hat Z_1^*$ is still a binary variable, but not taking the $\{0,1\}$ values any more.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-19T02:26:18.170" Id="70448" LastActivityDate="2013-09-19T02:26:18.170" OwnerUserId="28746" ParentId="70434" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;So I understand that your DV is numerical and your 3 IVs are categorical (3 levels). Boxplots is a good choice. You will have 9 boxplots, 3 for each IV.&lt;/p&gt;&#10;&#10;&lt;p&gt;Plot each IV separately. On the y axis will always be the DV (uranium). On the x-axis will the the IVs. For example, temp low, temp med, temp high. Do this for all 3 IVs. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to look at the interaction between the IVs, plots will be more complicating (as will be your analysis). There's no easy way. You're just going to have to divide up the data into 6 when looking at 2 IVs, and 9 when looking at all 3 at once, and make boxplots for each. I don't suggest you do this. Given your skill level and because it is for a class, looking at one IV at a time is probably good enough.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-19T06:24:58.197" Id="70456" LastActivityDate="2013-09-19T06:24:58.197" OwnerUserId="30460" ParentId="70447" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I'd appreciate ideas about how to estimate the standardized mean difference (SMD) for two independent samples from each sample's size and middle three quartiles (i.e., median and 25th and 75th percentiles).  This problem's context is a meta-analysis whose focal effect size is a post-intervention SMD between treatment and control groups on a continuous outcome variable: Many studies report both samples' sizes, means, and variances or equivalent results (e.g., from a &lt;em&gt;t&lt;/em&gt; test), from which a SMD is readily estimated, but some studies report other results instead (e.g., sizes and quartiles).  Below I state the problem more precisely, give an example, and offer a few remarks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we're interested in the following SMD on a continuous dependent variable $Y$ for Groups 1 and 2: $\delta = (\mu_1 - \mu_2) / \sigma$, where $\mu_j = \mathrm{E}(Y_j)$ and $\sigma^2 = \mathrm{Var}(Y_j)$ for $j = 1, 2$ are the groups' means and (common) variance.  For convenience let's assume normality for each group, so $Y_j \sim \mathcal{N}(\mu_j, \sigma^2)$.  Now consider a study that reports for one simple random sample from each group the size, $n_j$, and estimates of the three middle quartiles, $q_{1j}$, $q_{2j}$, and $q_{3j}$.  How can we estimate $\delta$ from these results?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EXAMPLE:&lt;/strong&gt; The following randomized study is from a meta-analysis of interventions to improve medication adherence.  In short, Group 1's treatment sample experienced a multi-component intervention, Group 2's control sample received no attention beyond enrollment and data collection, and the outcome variable was a pharmacy refill measure on which higher scores represent better adherence.  The samples' sizes and quartiles are as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Group 1: $n_1$ = 80, $q_{11}$ = 58, $q_{21}$ = 85, $q_{31}$ = 174&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Group 2: $n_2$ = 46, $q_{12}$ = 31, $q_{22}$ = 79, $q_{32}$ = 158&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The intervention seems to have improved adherence, in that all three quartiles are higher for the treatment group.  Both groups also exhibit evidence of positive skewness.  Answers to this question could use these results to demonstrate any proposed method for estimating $\delta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;REMARKS:&lt;/strong&gt; Below are several related issues that may be worth considering, in no particular order.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R1.&lt;/strong&gt; Typically no details are reported about how exactly quartiles were computed.  In principle one could obtain this information -- and perhaps the original subject-level data -- from the study's author(s), but in practice that's often not feasible.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R2.&lt;/strong&gt; For now I'd be content with a &quot;reasonable&quot; estimator of $\delta$.  I'm less interested in properties of the estimator (e.g., bias, consistency, efficiency) or in finding an optimal estimator (e.g., UMVUE), but those would be interesting to consider eventually.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R3.&lt;/strong&gt; A potential complication is duplicate quartiles within a sample (e.g., $q_{11} = q_{21}$) or between samples (e.g., $q_{21} = q_{22}$), especially when reported results are rounded severely.  We can ignore this for now, but it arises in practice.  For instance, another study from the same meta-analysis as the above example reported all six quartiles as 3.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R4.&lt;/strong&gt; One ad hoc method I've considered involves obtaining several estimates of $\delta$ and combining them in a way that accounts for their variances and correlations (e.g., generalized least-squares using estimated precision matrix).  For instance, we could use Group 1's results to estimate $\sigma$ and $\mu_1$ as follows, where $\Phi^{-1}(p)$ is the standard normal inverse CDF (i.e., quantile function) at $p$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat\sigma = (q_{21} - q_{11}) / [\Phi^{-1}(1/2) - \Phi^{-1}(1/4)]$ .&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat\mu_1 = q_{11} - \Phi^{-1}(1/4)\hat\sigma$ .&lt;/p&gt;&#10;&#10;&lt;p&gt;We could use other pairs of quartiles for these estimates, do the same for Group 2, and estimate $\delta$ using various combinations of estimates from Groups 1 and 2 (e.g., mean difference and pooled variance).  Note that $\delta$ estimates from different combinations of quartiles may differ in sign.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R5.&lt;/strong&gt; Some meta-analytic techniques require a sampling variance for the estimate of $\delta$, such as precision-weighted methods that use weighted least-squares.  For now I'm willing to obtain this variance by resampling or simulation, but a closed-form expression would be convenient.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R6.&lt;/strong&gt; I can imagine numerous useful variants on or extensions of this problem.  For example, suppose a study reports for at least one group different or more quartiles (e.g., $q_{0j}$ [minimum] or $q_{4j}$ [maximum]), different or more quantiles (e.g., 10th and 90th percentiles), or other results (e.g., sample mean).  Another idea is to consider monotonic transformation of the quartiles, say $X_j = f(Y_j)$, so that $X_1$ and $X_2$ better satisfy normality and homoscedasticity.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-19T06:30:45.127" Id="70457" LastActivityDate="2013-09-19T19:05:12.753" OwnerUserId="25224" PostTypeId="1" Score="2" Tags="&lt;estimation&gt;&lt;meta-analysis&gt;&lt;effect-size&gt;&lt;normality&gt;&lt;quantiles&gt;" Title="How to estimate a standardized mean difference from two samples' quartiles?" ViewCount="273" />
  <row Body="&lt;p&gt;Honestly, once you go through the basics, I'd go with a learn-as-you-go approach. When you do a study that requires a certain analysis, then go find a place to learn how to do it. This way, you don't have to worry about forgetting how to code things plus you'll be more motivated to learn about the specific topic. At least that has worked for me so far.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-19T07:09:27.700" Id="70462" LastActivityDate="2013-09-19T07:09:27.700" OwnerUserId="30460" ParentId="70401" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;If the sample of 1000 is a random sample of the larger population &lt;em&gt;and&lt;/em&gt; you developed the algorithm on a training set and then used a test set of the sample of 1000 to get the confidence interval and you didn't do anything else against the &quot;rules&quot; then the confidence interval ought to be the same in the wider population as it is in the test set of the sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps different groups use words differently, but usually the word &quot;population&quot; in the second line of your question would be &quot;sample&quot;, and usually the prediction rate on the wider population is what we are really interested in. After all, with the sample, we &lt;em&gt;know&lt;/em&gt; what the right results are. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-19T10:21:50.480" Id="70471" LastActivityDate="2013-09-19T10:21:50.480" OwnerUserId="686" ParentId="70470" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="70486" AnswerCount="1" Body="&lt;p&gt;Let's say I have an histogram of some continuous data with unknown and arbitrary pdf. I don't have access to the original raw data. So I don't know that 3 of my data points are 21, 25 and 27, all I know is that 3 of my data points are in the bin 20-30. How accurate is it to compute the moments of this pdf from the binned data? Can I estimate the error I'm making?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-19T12:33:53.373" Id="70479" LastActivityDate="2013-09-19T13:15:12.180" OwnerUserId="16329" PostTypeId="1" Score="2" Tags="&lt;pdf&gt;&lt;histogram&gt;" Title="Accuracy of computing moments from an histogram" ViewCount="105" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two time series of daily returns on two stock indices (S&amp;amp;P 500 and BOVESPA)&#10;that I would like to estimate the portfolio value at risk (VaR) for. Since these are &#10;indices from two different regions they do not naturally line up smoothly as their &#10;business days are unequal due to some country specific holidays.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example would be on the 4th of July, when the return of the S&amp;amp;P would be 0 and the BOVESPA a positive or negative number. Including this sample in the empirical returns distribution would include a 0 return which never happened. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I am interested in the portfolio VaR I would prefer returns on days where both&#10;markets were open so as to catch the implicit correlations in the returns of the indices. &#10;However, this would mean I will remove 250 paired observations (over a 20 year period) where either one of them have a zero value. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any better ways to handle the data?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-19T12:45:31.363" Id="70480" LastActivityDate="2013-09-19T14:24:37.273" LastEditDate="2013-09-19T12:49:31.867" LastEditorUserId="6029" OwnerUserId="30470" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;missing-data&gt;" Title="Handling missing data (holidays) in multiple time series (historical simulation VaR)" ViewCount="279" />
  <row Body="&lt;p&gt;See &lt;em&gt;Experimental uncertainty estimation and statistics for data having interval uncertainty&lt;/em&gt; (&lt;a href=&quot;http://prod.sandia.gov/techlib/access-control.cgi/2007/070939.pdf&quot;&gt;Ferson et al. 2007&lt;/a&gt;). Calculating the uncertainty in the mean is easy, just compute the mean of the data assuming all points are at the lowest value of the interval and the mean assuming all points are at the highest value of the interval. The true mean has to lie somewhere in this interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;Variance is a bit harder. But the referenced publication gives some algorithms to estimate it, and fortunately for binned histogram data it is a case of what Ferson et. al (2007) refer to as &lt;em&gt;no-nesting&lt;/em&gt; type data.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are various other measures that are easy to calculate by only examining the end points of the intervals (like the CDF). Ferson et. al (2007) refer to such measures as stochastic dominant (wording taken from Charles Manski).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-19T13:15:12.180" Id="70486" LastActivityDate="2013-09-19T13:15:12.180" OwnerUserId="1036" ParentId="70479" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;A relatively unknown but very useful nonparametric substitute for two-way ANOVA with replication (must be balanced ANOVA) is the Schierer-Ray-Hare test. It is an extension of the Kruskal-Wallis test. Do it this way:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Replace each data observation with its overall rank (lowest number is ranked 1 and tied observations are all given the average rank)&lt;/li&gt;&#10;&lt;li&gt;Run the two-way ANOVA as usual with the ranks instead of the actual data values.&lt;/li&gt;&#10;&lt;li&gt;Discard the MS, F, and p value terms in the ANOVA output.&lt;/li&gt;&#10;&lt;li&gt;Sum SS for SS factors, SS interaction, and SS error. Divide this sum by df total. The result is MS total.&lt;/li&gt;&#10;&lt;li&gt;The test statistic, H, for each factor and interaction equals its SS / MS total&lt;/li&gt;&#10;&lt;li&gt;The Excel formula for the p value for each is:  &lt;code&gt;CHIDIST(H, df)&lt;/code&gt;. The df is the usual df for each factor and interaction. The Excel output provides these df figures.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The Schierer-Ray-Hare test is a lot less powerful that regular two-way ANOVA. The p values are usually around twice as large on the SRH test as those generated by two-factor ANOVA.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-19T16:46:20.383" Id="70510" LastActivityDate="2013-09-19T17:03:43.987" LastEditDate="2013-09-19T17:03:43.987" LastEditorUserId="7290" OwnerUserId="30476" ParentId="58425" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;would anyone please tell me the differences among the three methods? &#10;also i have a data in which the dependent variable is a count variable. is it classified as a level 1 variable? one of my classmates tried using HLM, and he clustered the DV into groups. Im not really familiar with hierarchical modeling. so i would really appreciate if someone could help me with this one? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-19T17:06:51.630" Id="70515" LastActivityDate="2013-09-19T17:06:51.630" OwnerUserId="30478" PostTypeId="1" Score="1" Tags="&lt;multilevel-analysis&gt;&lt;hierarchical-bayesian&gt;" Title="HLM vs linear mixed models vs random effects model" ViewCount="297" />
  
  
  
  <row Body="&lt;p&gt;The easiest way to determine skill level would be to simply add up the number of questions correct for each student on the very first attempt. Unless you have a clear reason why attempt count is important, it is vague whether number of attempts would represent skill level. Plus, item difficulty is taken into account even if you disregard it; students who get the difficult questions right will simply get more points. This is the way most tests are assessed. &lt;/p&gt;&#10;&#10;&lt;p&gt;You should try calculating the reliability too. look up kuder-richardson 20. The idea is that students who get high overall scores should be the ones that are more likely to answer the difficult questions (ones that not many get correct) correct, and vice versa. If the reliability is low, it means your test was poor at discriminating students who have good/bad knowledge, which is the whole point of the test. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-19T22:35:03.910" Id="70549" LastActivityDate="2013-09-19T22:35:03.910" OwnerUserId="30460" ParentId="70532" PostTypeId="2" Score="3" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have a question in which it asks to verify whether if the Uniform distribution (${\rm Uniform}(a,b)$) is normalized. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;For one, what does it mean for any distribution to be normalized? &lt;/li&gt;&#10;&lt;li&gt;And two, how do we go about verifying whether a distribution is normalized or not?  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I understand by computing &#10;$$&#10;\frac{X-\text{mean}}{\text{sd}}&#10;$$ &#10;we get normalized &lt;em&gt;data&lt;/em&gt;, but here it's asking to verify whether a &lt;em&gt;distribution&lt;/em&gt; is normalized or not.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-19T23:39:31.987" FavoriteCount="4" Id="70553" LastActivityDate="2013-09-23T20:01:01.307" LastEditDate="2013-09-20T00:20:17.463" LastEditorUserId="7290" OwnerUserId="30499" PostTypeId="1" Score="9" Tags="&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;data-transformation&gt;&lt;terminology&gt;" Title="How to verify a distribution is normalized?" ViewCount="4277" />
  
  <row AcceptedAnswerId="70607" AnswerCount="2" Body="&lt;p&gt;I have scoured lots of help sites and am still confused about how to specify more complicated nested terms in a mixed model as well. I am also confused as the use of &lt;code&gt;:&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; in specifying interactions and nesting with random factors using &lt;code&gt;lmer()&lt;/code&gt; in the &lt;code&gt;lme4&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the purpose of this question, let's assume I have accurately portrayed my data with this standard statistical model:&#10;$$&#10;Y_{ijk} = u + \text{station}_i + \text{tow}_{j(i)} + \text{day}_k + (\text{station}\times \text{day})_{ik} + (\text{tow}\times\text{day})_{j(i)k}&#10;$$&#10;&lt;code&gt;station&lt;/code&gt; is fixed, &lt;code&gt;tow&lt;/code&gt; and &lt;code&gt;day&lt;/code&gt; are random.  &lt;code&gt;Tow&lt;/code&gt; is (implicitly) nested within &lt;code&gt;station&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, I'm hoping that my model includes Station(i,fixed), Tow(j,random,implicitly nested within Station), Day(k,random), and interaction between Tow and Day, and the interaction between Day and Station.  I have consulted with a statistician to create my model and at this time believe it to be representative of my data, but will also add in a description of my data for those who are interested at the bottom of my post so as not to clutter.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far what I've been able to piece together is the following in &lt;code&gt;lmer&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(y ~ station + (1|station:tow) + (1|Day) + (1|station:day) + (1|tow:day), &#10;     data=my.data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does this accurately depict my statistical model? Any suggestions for how to improve my code if it does not read correctly?  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I've bolded the specific terms I'm having difficulty specifying in  my lmer formula&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;#1. tow nested within station when tow is random and station is fixed&lt;/strong&gt;&lt;br&gt;&#10;I'm confused, however about differentiating between nested and interaction terms that are random using &lt;code&gt;:&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt; .  In my above example, I have &lt;code&gt;(1|station:tow)&lt;/code&gt; in which I'm hoping reads tow nested within station.  I've read conflicting comments on various sites whether or not I should be using &lt;code&gt;:&lt;/code&gt; or &lt;code&gt;/&lt;/code&gt; here within the random &lt;code&gt;(1|...)&lt;/code&gt; format of &lt;code&gt;lmer&lt;/code&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;#2. The interaction between station and day when station is fixed and day is random&lt;/strong&gt;&lt;br&gt;&#10;I then have &lt;code&gt;(1|station:day)&lt;/code&gt; but this time I'm hoping it reads the interaction between station and day.  It seems like I could use station*day to account for the individual effects of station and day as well as their interaction (rather than including each of the three terms separately as I do above), but I don't see how to specify this when one is fixed and the other is random.  Would &lt;code&gt;station*(1|day)&lt;/code&gt; do that?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;#3. The interaction between tow and day (both random) when tow is nested in station (fixed)&lt;/strong&gt;&#10;Then lastly, I have &lt;code&gt;(1|tow:day)&lt;/code&gt; which I'm hoping reads the interaction of &lt;code&gt;tow&lt;/code&gt; and &lt;code&gt;day&lt;/code&gt;, but I'm wondering if I need to specify again that tow is nested (implicitly) in station?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am new to both &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;lmer&lt;/code&gt; and statistical modeling and greatly appreciate the trouble of thorough explanations in any responses to my questions if possible.$$$$&lt;/p&gt;&#10;&#10;&lt;p&gt;More details on my data:  I am asking whether concentrations of plankton vary across a physical front in the nearshore ocean.  I have three stations, inshore, within, and offshore of this front.  Station is thus fixed.  At each station, I take three replicate plankton tows (from which I sort, count, and get a concentration in terms of # of bugs per meter cubed of water).  Tow is random: in three tows I hope to account for the general variability in plankton at that particular station. Tow is intrinsically nested in station as each tow does not have a unique ID (123,123,123 is the ID for tows at each station).  I then did this on multiple, independent days with a new front that had formed.  I think I can think of Day as a blocking factor? Day is random as repeating this on multiple independent front days is attempting to capture variability from day to day and be representative of all days where this front is present.  I want to know about the interaction terms to see if Tows change in variability from day to day and if stations always yield similar data or does it depend on the day?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Again, thank you for your time and help, I appreciate it!&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2013-09-20T00:34:23.250" FavoriteCount="6" Id="70556" LastActivityDate="2013-09-22T17:55:47.487" LastEditDate="2013-09-20T17:25:43.237" LastEditorUserId="30501" OwnerUserId="30501" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;" Title="Have I correctly specified my model in lmer?" ViewCount="1384" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have conducted a study where I employed a two-way repeated measures design to investigate whether subjects respond to a main treatment effect. What I'm mainly interested in is whether subjects subsequently compensate for the treatment effect by returning to a control level, or whether the initial differences persist for the duration of the treatment (time x treatment interaction).&lt;/p&gt;&#10;&#10;&lt;p&gt;My factors:&lt;/p&gt;&#10;&#10;&lt;p&gt;Treatment: Between-subjects factor (3 levels)&lt;/p&gt;&#10;&#10;&lt;p&gt;Exposure Time: Within-subjects repeated measure (4 levels - pre-exposure values not included)&lt;/p&gt;&#10;&#10;&lt;p&gt;Treatment x Time Interaction: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/w867m.jpg&quot; alt=&quot;Response over time to treatment effect (3 levels). Pre-test (time zero) values were not included in the ANOVA&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Error bars = Standard deviation&lt;/p&gt;&#10;&#10;&lt;p&gt;My ANOVA results:&#10;Sphericity was taken into account: p = 0.25. Adjusted values told the same story.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both main effects, 'Treatment' and 'Time' were significant.&#10;Treatment effect had a large effect size (0.4). (sensu Cohen)&#10;Time, isn't really that interesting on its own, had a small effect size (0.08)&#10;The interaction was not significant (p = 0.3) and had a tiny effect size (0.04).&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem:&#10;The graph appears to show that one of the treatment levels interacts with exposure time (fig. d). (ie the lines are not all parallel from day 4 - 14)&#10;The ANOVA results however suggest that that the difference between treatments persists (the lines remain parallel).&#10;I have looked at pairwise comparisons within levels of the main effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question:&#10;I am tempted to included post-hoc comparisons to compare treatment levels at each time interval to show that the difference between groups disappears on day 11. Does this defeat the purpose of running an ANOVA, which clearly tells me that I should just state that there was a main effect (ie the treatment groups remained different in fig d). I therefore shouldn't really fish around for significant differences.&lt;/p&gt;&#10;&#10;&lt;p&gt;The literature that I'm comparing my work to employs ANOVA, but then also puts pairwise comparisons all over the place. I'm a fan of effect sizes, rather than p-values, so feel justified to just highlight the main treatment effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;My main conclusion could be very different depending on whether I go with the ANOVA (ie no interaction -&gt; no recovery) or whether I include pairwise comparisons at each time point (partial recovery).&lt;/p&gt;&#10;&#10;&lt;p&gt;Side note: Mixed effects models in lme gave the same result. I stuck with ANOVA as my audience is more likely to relate to it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for the feedback.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-20T08:34:02.020" FavoriteCount="1" Id="70574" LastActivityDate="2015-02-26T05:06:46.060" LastEditDate="2013-09-20T08:50:58.077" LastEditorUserId="21985" OwnerUserId="21985" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;repeated-measures&gt;&lt;post-hoc&gt;&lt;mixed&gt;" Title="Interpreting Two-way repeated measures ANOVA results: Post-hoc tests allowed without significant interaction?" ViewCount="2376" />
  <row AcceptedAnswerId="70578" AnswerCount="1" Body="&lt;p&gt;I have modeled a distribution, $f$, over a r.v. $x \in \mathbb{R}^3$. At inference a set of measuring points, $X$, of the r.v. variables show up. I want to form a distribution over this sample set so that I can sample from it so that more probable points will show up more often.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My idea was to evaluate $f$ for each point in the set and then normalize it to form a discrete distribution over the points. Since $f \approx 0$ I want to use the log likelihood. However, the $\log$ is a nonlinear transform and normalizing it means calculating: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\log \sum_i f(x_i)$, &lt;/p&gt;&#10;&#10;&lt;p&gt;which is bound to give numerical errors due to the fact that $f(x_i)\approx 0$. So the best I can do is a log sum exp calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: is there a better way of doing this? And is this a completely wrong approach in terms of modeling probability distributions?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-20T09:00:29.070" Id="70575" LastActivityDate="2013-09-20T09:47:57.767" OwnerUserId="4976" PostTypeId="1" Score="1" Tags="&lt;normalization&gt;&lt;likelihood&gt;" Title="Treating the log-likelihood as a probability distribution and normalizing?" ViewCount="531" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;A clinical trial examines the effect of two treatments upon lung function.  Patients in each group contribute data at baseline and at the end of the trial.  Baseline data are provided for each group and then end of treatment data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QLyge.jpg&quot; alt=&quot;Data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to calculate the mean change (SD) over the time of the trial for each drug so that I can use the data in a meta-analysis - how should I approach this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-20T15:14:39.757" Id="70591" LastActivityDate="2013-09-20T15:25:46.583" LastEditDate="2013-09-20T15:25:46.583" LastEditorUserId="8960" OwnerUserId="7421" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;meta-analysis&gt;&lt;group-differences&gt;&lt;clinical-trials&gt;" Title="Estimating absolute difference between two conditions" ViewCount="39" />
  <row Body="&lt;p&gt;In this type of situation one often assumes that the errors in the phasors are &lt;a href=&quot;http://en.wikipedia.org/wiki/Complex_normal_distribution&quot;&gt;complex circular Gaussian&lt;/a&gt; distributed; this model comes up since one assumes that the observed errors are the accumulation of a larger number of smaller errors, and then using the central limit theorem.  The key assumptions to watch for are&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the noise is purely additive and independent of the signal,&lt;/li&gt;&#10;&lt;li&gt;the noise is zero mean in the complex plane, and&lt;/li&gt;&#10;&lt;li&gt;that the mechanisms generating the errors/fluctuations are independent of phase, this is what makes complex distribution circular, and&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;An important class of error that does not conform to these assumptions is when&#10;there is some unknown phase error induced by the system (e.g. from random variations in the thickness or index of refraction in some component that the &#10;light is passing through).&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you make this assumption,  the effects of the additive noise are a 2-dimensional random walk in the complex plane.  So if $Z_{true}$ is the complex phasor that you'd expect in the absence of noise, then&#10;the difference between what you observe and the noise free result $Z_{obs}-Z_{true}$ will be a complex circular distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if you measure the intensity (magnitude squared) of the result you'll have to describe this by a &lt;a href=&quot;http://en.wikipedia.org/wiki/Rice_distribution&quot;&gt;Rice distribution&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-20T15:36:03.250" Id="70592" LastActivityDate="2013-09-20T15:36:03.250" OwnerUserId="16859" ParentId="70587" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;A recent paper on several choices for the prior on $\nu$ in the following link:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://bayesian.org/sites/default/files/BA/forthcoming/BA854.pdf&quot; rel=&quot;nofollow&quot;&gt;http://bayesian.org/sites/default/files/BA/forthcoming/BA854.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-20T16:56:03.310" Id="70597" LastActivityDate="2013-09-20T16:56:03.310" OwnerUserId="30518" ParentId="6492" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="70621" AnswerCount="2" Body="&lt;p&gt;I conducted a glm.nb by &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm1&amp;lt;-glm.nb(x~factor(group))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;with group being a categorial and x being a metrical variable. When I try to get the summary of the results, I get slightly different results, depending on if I use &lt;code&gt;summary()&lt;/code&gt; or &lt;code&gt;summary.glm&lt;/code&gt;. &lt;code&gt;summary(glm1)&lt;/code&gt; gives me &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    ...&#10;Coefficients:&#10;                    Estimate Std. Error z value Pr(&amp;gt;|z|)  &#10;    (Intercept)       0.1044     0.1519   0.687   0.4921  &#10;    factor(gruppe)2   0.1580     0.2117   0.746   0.4555  &#10;    factor(gruppe)3   0.3531     0.2085   1.693   0.0904 .&#10;    ---&#10;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;    (Dispersion parameter for Negative Binomial(0.7109) family taken to be 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;whereas summary.glm(glm1) gives me&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    ...&#10;Coefficients:&#10;                    Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;    (Intercept)       0.1044     0.1481   0.705   0.4817  &#10;    factor(gruppe)2   0.1580     0.2065   0.765   0.4447  &#10;    factor(gruppe)3   0.3531     0.2033   1.737   0.0835 .&#10;    ---&#10;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;    (Dispersion parameter for Negative Binomial(0.7109) family taken to be 0.9509067)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I understand the meaning of the dispersion parameter, but not of the line &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;(Dispersion parameter for Negative Binomial(0.7109) family taken to be 0.9509067)&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the handbook it says, it would be the estimated dispersion, but it seems to be a bad estimate, as 0.95 is not close to 0.7109, or is the estimated dispersion something different than the estimated dispersion parameter?&#10;I guess, I have to set the dispersion in the &lt;code&gt;summary.nb(x, dispersion=)&lt;/code&gt; to something, but I'm not sure, if I have to set the dispersion to 1 (which will yield the same result as &lt;code&gt;summary()&lt;/code&gt; or if I should insert an estimate of the dispersion parameter, in this case leading to &lt;code&gt;summary.nb(glm1, dispersion=0.7109)&lt;/code&gt; or something else? Or am I fine with just using the &lt;code&gt;summary(glm1)&lt;/code&gt;? I would apprecciate your help!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-09-20T21:34:00.867" FavoriteCount="2" Id="70619" LastActivityDate="2013-09-20T22:25:35.813" OwnerUserId="29355" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;negative-binomial&gt;" Title="dispersion in summary.glm()" ViewCount="1417" />
  <row AnswerCount="0" Body="&lt;p&gt;Is it possible to perform a power analysis for the Kruskal-Wallis and Mann-Whitney U test? If yes, are there any R packages/functions that perform it?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-21T07:34:40.163" FavoriteCount="1" Id="70643" LastActivityDate="2013-09-21T09:10:11.753" LastEditDate="2013-09-21T09:10:11.753" LastEditorUserId="21054" OwnerUserId="6547" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;nonparametric&gt;&lt;power-analysis&gt;&lt;kruskal-wallis&gt;" Title="Power analysis for Kruskal-Wallis or Mann-Whitney U test using R?" ViewCount="1110" />
  <row AnswerCount="1" Body="&lt;p&gt;I am having issues in using random forests in MATLAB. I have features of size 2000 and around 4000 data points. I am trying to learn how to compute random forests in MATLAB using the library &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/31036-random-forest&quot; rel=&quot;nofollow&quot;&gt;Random Forest&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I guess it's too slow to use this method. I have set the number of trees to 500 and &lt;code&gt;mtry&lt;/code&gt; to 720 and it is taking ages. Is it natural to take this long? How long does it normally take?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-21T18:52:17.587" FavoriteCount="1" Id="70658" LastActivityDate="2014-04-02T19:09:15.947" LastEditDate="2013-09-21T19:21:02.983" LastEditorUserId="21054" OwnerUserId="12329" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;matlab&gt;&lt;random-forest&gt;" Title="Using random forest in MATLAB" ViewCount="4597" />
  <row Body="&lt;p&gt;Some good ecology books based in Bayesian statistics are:&lt;/p&gt;&#10;&#10;&lt;p&gt;Kery, M.  2010. &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0123786053&quot; rel=&quot;nofollow&quot;&gt;Introduction to WinBUGS for Ecologists: Bayesian approach to regression, ANOVA, mixed models and related analyses&lt;/a&gt;.  Academic Press.&lt;/p&gt;&#10;&#10;&lt;p&gt;Kery, M., and M. Schaub.  2011.  &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0123870208&quot; rel=&quot;nofollow&quot;&gt;Bayesian Population Analysis using WinBUGS: A hierarchical perspective&lt;/a&gt;.  Academic Press.&lt;/p&gt;&#10;&#10;&lt;p&gt;Royle, J.A. and R.M. Dorazio. 2008. &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0123740975&quot; rel=&quot;nofollow&quot;&gt;Hierarchical Modeling and Inference in Ecology: The Analysis of Data from Populations, Metapopulations, and Communities&lt;/a&gt;. Academic Press&lt;/p&gt;&#10;&#10;&lt;p&gt;I also find Zuur et al. (2009) very useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Zuur, A., E. N. Ieno, N. Walker, A. A. Saveliey, and G. M. Smith.  &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387874577&quot; rel=&quot;nofollow&quot;&gt;Mixed Effects Models and Extensions in Ecology with R&lt;/a&gt;.  Springer.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-22T04:58:27.980" Id="70680" LastActivityDate="2013-09-22T04:58:27.980" OwnerUserId="12318" ParentId="70526" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The book is correct. &lt;/p&gt;&#10;&#10;&lt;p&gt;The following refers to the GNP equation where GNP consists of lag of GNP and M1 consists of lag of M1. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;, , GNP&#10;&#10;           GNP          M1&#10;1  1.271812104 -0.03383385&#10;2 -0.004229937  0.06353801&#10;3 -0.267154022 -0.02858942&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;GNP_{t} = 1.27 GNP_{t-1} -0.033 M1_{t-1} - 0.004 GNP_{t-2} + 0.063 M1_{t-2} - 0.2671 GNP_{t-3} - 0.028 M1_{t-3}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;The following refers to the M1 equation where GNP consists of lag of GNP and M1 consists of lag of M1. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;, , M1&#10;&#10;         GNP         M1&#10;1  1.1674655  1.5876695&#10;2 -0.6941813 -0.4838919&#10;3 -0.5103451 -0.1294549&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;M1_{t} = - 1.167 GNP_{t-1} + 1.58 M1_{t-1} - 0.694 GNP_{t-2} - 0.48M1_{t-2} - 0.510 GNP_{t-3} - 0.12 M1_{t-3}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;If you arrange this is matrix form, then you will be able to get the same as in the book. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-22T14:32:55.020" Id="70698" LastActivityDate="2013-09-22T14:38:59.687" LastEditDate="2013-09-22T14:38:59.687" LastEditorUserId="14860" OwnerUserId="14860" ParentId="70675" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using RandomForest regressor on my data and I could see that the oob score was obtained to be 0.83. I am not sure how it came out to be like this. I mean my targets are high values in the range of 10^7. So if it's MSE then it should have been much higher. I don't understand what 0.83 signify here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using python's RandomForestRegressor of the sklearn toolkit.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do&lt;/p&gt;&#10;&#10;&lt;p&gt;model = RandomForestRegressor(max_depth=7, n_estimators=100, oob_score=True, n_jobs=-1)&#10;model.fit(trainX, trainY )&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I see model.oob_score_ and I get values like 0.83809026152005295&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-22T16:57:10.550" FavoriteCount="1" Id="70704" LastActivityDate="2014-01-29T16:30:39.863" LastEditDate="2013-09-23T16:14:34.650" LastEditorUserId="12329" OwnerUserId="12329" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;random-forest&gt;" Title="Interpreting out of bag error estimate for RandomForestRegressor" ViewCount="1044" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose I have a sorted list of 30 numbers with display exponential trend (a few numbers are fairly large, and a lot of them are relatively small). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I am selecting a random index (a realization from DISCRETE-UNIFORM(0, 29)) and remove that entry from the list.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, my goal is that the next number I remove from the list should be similar in magnitude to the last draw (thus introducing positive correlation between consecutive draws).&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, once the list shrinks to 1, there is only one option left, irrespective or the correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be nice if this function could take in a correlation coefficient R that the caller can change from 0 (completely independent) to 1 (always pick the next closest value).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-22T20:24:16.687" Id="70714" LastActivityDate="2013-09-23T13:18:54.837" OwnerUserId="26629" PostTypeId="1" Score="-1" Tags="&lt;correlation&gt;" Title="What is an easy algorithm to introduce positive correlation?" ViewCount="39" />
  <row Body="&lt;p&gt;You probably can be served by the &lt;code&gt;CI.Rsq&lt;/code&gt; function in the psychometric package. The equation is provided in the help. An alternative would be bootstrapping the CI.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-22T21:39:42.300" Id="70718" LastActivityDate="2013-09-22T21:39:42.300" OwnerUserId="601" ParentId="70716" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I found the error, so I guess I'll answer my own question.  Apparently the intercept term is not part of the coefficient vector, its a separate thing, so that has to be added onto the linear combination.  So here is the correct code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for i in data:&#10;        print i[0]*coeffs[0]+i[1]*coeffs[1]+i[2]*coeffs[2]+i[3]*coeffs[3]+i[4]*coeffs[4]+regr.intercept_&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T00:16:33.483" Id="70725" LastActivityDate="2013-09-23T00:16:33.483" OwnerUserId="28524" ParentId="70720" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It is ordinal because the spacing of the categories is not the same all the way across. Some have a range of .5, others .25, and option 5 can be infinite.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if the categories are equally spaced, you can consider them continuous. For example, categories 1-5,6-10,11-15,16-20,21-25, and 26-30. The range for each category is 5, so it meets the assumption of a continuous variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that ranges of 5's seem big, but think about it this way. Imagine a variable where most of the values range from 0 to 1. The researcher measures this variable to the first decimal place. Therefore, all the possible values are 0,.1,.2,.3 ... .9,1.0. The researcher could have measured it to the second decimal place, but he didn't, and this made the measurements less accurate. It may look like it is a variable with 11 categories. However, the variable is definitely still continuous. &lt;/p&gt;&#10;&#10;&lt;p&gt;Again, as long as your categories are the same distance apart, the variable is continuous. It probably is not a very great continuous variable, but this is only because of poor measurement methods. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T01:30:45.860" Id="70729" LastActivityDate="2013-09-23T01:30:45.860" OwnerUserId="30460" ParentId="57696" PostTypeId="2" Score="0" />
  <row AnswerCount="3" Body="&lt;p&gt;I believe that some variables can be interval/ratio or ordinal based on the context. I'd like to know people's opinion on this. Let me explain:&lt;/p&gt;&#10;&#10;&lt;p&gt;Class variable with categories Freshman, Sophomore, Junior, Senior. Seems like an ordinal variable right? Well, a student's class standing is determined by credits. If you are looking at a population where students with 0-29, 30-59, 60-89, 90-119, 120+ credits are Freshman, Sophomore, Junior, Senior, and graduated, respectively, then the categories are equally spaced apart (by 30 credits). Then, you may be able to consider this class variable as a ratio variable that was simply poorly measured (It's like measuring people's height in feet without decimals or inches). &lt;/p&gt;&#10;&#10;&lt;p&gt;Do you think this is valid?&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2013-09-29T18:56:54.327" CreationDate="2013-09-23T02:27:55.127" Id="70735" LastActivityDate="2013-09-23T04:02:25.753" LastEditDate="2013-09-23T03:50:08.100" LastEditorUserId="30460" OwnerUserId="30460" PostTypeId="1" Score="1" Tags="&lt;ordinal&gt;&lt;measurement&gt;" Title="Can some variables be ordinal or interval/ratio, depending on the context?" ViewCount="79" />
  
  <row Body="&lt;p&gt;Aren't we mixing terms here?  Continuous should be contrasted with discrete.  Ordinal should be contrasted to nominal, interval, and ratio.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2013-09-29T18:56:54.327" CreationDate="2013-09-23T03:28:52.557" Id="70740" LastActivityDate="2013-09-23T03:28:52.557" OwnerUserId="3919" ParentId="70735" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="70754" AnswerCount="3" Body="&lt;p&gt;I am reading an article which shows Hazard Ratios for continuous variables, but I'm not sure how to interpret the given values. &lt;/p&gt;&#10;&#10;&lt;p&gt;My current understanding of hazard ratios is that the number represents the relative likelihood of [event] given some condition. E.g: if the hazard ratio for death from lung cancer given smoking (a binary event) is 2, then smokers were twice as likely to die in the monitored time period than non-smokers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking on wikipedia, the interpretation for continuous variables is that the hazard ratio applies to a unit of difference. This makes sense to me for ordinal variables (e.g number of cigarettes smoked a day), but I don't know how to apply this concept to continuous variables (e.g. grams of nicotine smoked a day?)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T03:58:50.470" Id="70741" LastActivityDate="2013-09-24T15:35:33.370" OwnerUserId="29611" PostTypeId="1" Score="5" Tags="&lt;continuous-data&gt;&lt;hazard&gt;" Title="How to interpret a hazard ratio from a continuous variable -- unit of difference?" ViewCount="2126" />
  <row Body="&lt;p&gt;It's always been my impression that if you can make the case that the ordinal variable acts &lt;em&gt;as if&lt;/em&gt; continuous, where the valued differences between groups are of known, comparable magnitudes, it should be fine. I've seen people argue that you can even do so if they're not of equal distances from each other, but I don't like that argument because the interpretation of model coefficients can't really apply well if there are different theoretical distances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise, I think it can be useful, especially when what you're trying to ascertain in the direction of a relationship, or you're trying to compare magnitudes of a relationship across subgroups or clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;(And as a shameless plug, I asked a question yesterday or so about how we might be able to test if deviations from interval distances are significant enough to rule out the option for treating an ordinal variable as interval: &lt;a href=&quot;http://stats.stackexchange.com/questions/70672/&quot;&gt;Testing if treating a categorical variable as continuous is okay&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-09-29T18:56:54.327" CreationDate="2013-09-23T04:02:25.753" Id="70742" LastActivityDate="2013-09-23T04:02:25.753" OwnerUserId="21971" ParentId="70735" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="70958" AnswerCount="2" Body="&lt;p&gt;I know R is a very powerful tool for data mining and predictive model building.  However, I'm finding it difficult to extract an equation from any sort of modeling that I do.  For example, using something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- lm(y~var1+var2+var3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For data mining, I can use something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;analysis &amp;lt;- rpart(y~var1+var2+var3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;to understand my data.  However, it would be nice if R had the capability of displaying the model it uses, whether it is linear or something much more complicated.  There is a program called Eureka from Nutonian that is able to take data and try and fit an equation to it.  Is R  able to do this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-23T04:24:35.677" Id="70743" LastActivityDate="2013-09-24T22:16:18.197" OwnerUserId="26756" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-mining&gt;&lt;predictive-models&gt;" Title="Detect equation from data" ViewCount="124" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using &lt;code&gt;glmmLasso&lt;/code&gt; for variable selection. In my case, &lt;code&gt;n&lt;/code&gt; is slightly less than &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt; are bioclimatic variables for different time periods, so are highly correlated. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I choose the right values for the arguments: &lt;code&gt;lambda&lt;/code&gt; and &lt;code&gt;control&lt;/code&gt;. I have tried with different values of &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;maxIter&lt;/code&gt; (even to 10,000) and &lt;code&gt;control&lt;/code&gt; (&lt;code&gt;start&lt;/code&gt; and &lt;code&gt;steps&lt;/code&gt;). But, the algorithm never converges. The p.values of all the selected variables are 0 and that makes me wonder if the non-convergence is the cause. What factor could help achieve convergence in a reasonable number of iterations? &lt;/p&gt;&#10;&#10;&lt;p&gt;Among the variables selected, since p.values are all 0 (the rest of the variables are NA), can I estimate the relative variable importance based on the &lt;code&gt;StdErr&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, the &lt;code&gt;lambda&lt;/code&gt; resulting in the lowest &lt;code&gt;BIC&lt;/code&gt; values selects too many variables as being significant. Would it be OK in this case,(as I am only doing a rough variable selection for modelling) to not worry about the BIC, but choose the lambda that gives me a reasonable number of variables that also make sense given my data.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the &lt;code&gt;start&lt;/code&gt; arguments in &lt;code&gt;control&lt;/code&gt;, anyway? All zeros are not acceptable, and so are vectors below a certain length. What do the values and vector length depend on? Apologies, if that shows ignorance of the mathematics behind mixed-models...&lt;/p&gt;&#10;&#10;&lt;p&gt;For details on the data, please refer to &lt;a href=&quot;http://stats.stackexchange.com/questions/70227/variable-selection-using-mixed-models-lme4&quot;&gt;Variable selection using mixed-models (lme4)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I went through the &lt;code&gt;GMMBoost&lt;/code&gt;, also by Groll, but did not find something to guide me in this case. Has anyone used glmmLasso for analysis and faced similar situations? Glad to hear any suggestions&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T07:52:45.723" FavoriteCount="1" Id="70753" LastActivityDate="2015-03-01T05:23:38.473" LastEditDate="2013-09-23T17:33:58.730" LastEditorUserId="28472" OwnerUserId="28472" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lasso&gt;&lt;glmm&gt;" Title="Correct estimation of arguments for glmmLasso function" ViewCount="235" />
  <row Body="&lt;p&gt;There is a big difference between representing and fitting a model.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you want to display/represent the variable analysis you just make plot(analysis) in R and then you will see a representation of your model.&lt;/li&gt;&#10;&lt;li&gt;If you want a explicit formula for your model you must (1) know (or assume) a specific model for your data (linear, quadratic, etc...) (2) fit your parameters model. Nonetheless there are very few problems that can be expressed as a formula, unless it is linear/quadratic.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The idea of machine learning is to provide a solution which makes possible to predict without the necessity of using an explicit formula.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-23T08:45:41.173" Id="70755" LastActivityDate="2013-09-23T08:45:41.173" OwnerUserId="29943" ParentId="70743" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="71002" AnswerCount="1" Body="&lt;p&gt;I am looking at a time series which has no obvious trend, but seems to have an intercept a little above zero. The results I get for &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/urca/html/ur.df.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;ur.df&lt;/code&gt;&lt;/a&gt; function in R is the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logprice_df &amp;lt;- ur.df(test3, lags = 1, type= 'trend')&#10;summary(logprice_df)&#10;&#10;############################################### &#10;# Augmented Dickey-Fuller Test Unit Root Test # &#10;############################################### &#10;&#10;Test regression trend &#10;&#10;&#10;Call:&#10;lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-0.50614 -0.04394  0.00134  0.03859  0.64408 &#10;&#10;Coefficients:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  1.841e-02  8.268e-03   2.226  0.02626 *  &#10;z.lag.1     -1.573e-02  5.635e-03  -2.791  0.00537 ** &#10;tt           9.234e-06  1.080e-05   0.855  0.39272    &#10;z.diff.lag   1.411e-01  3.364e-02   4.195 3.01e-05 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 0.07512 on 865 degrees of freedom&#10;Multiple R-squared: 0.02651,    Adjusted R-squared: 0.02314 &#10;F-statistic: 7.852 on 3 and 865 DF,  p-value: 3.572e-05 &#10;&#10;&#10;Value of test-statistic is: -2.791 2.6012 3.8997 &#10;&#10;Critical values for test statistics: &#10;      1pct  5pct 10pct&#10;tau3 -3.96 -3.41 -3.12&#10;phi2  6.09  4.68  4.03&#10;phi3  8.27  6.25  5.34&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem is that I do not understand how one shall interpret these values. What is &lt;code&gt;F-statistic&lt;/code&gt;? And what is the probabilities below &lt;code&gt;Pr(&amp;gt;|t|)&lt;/code&gt; in the first table, and also &lt;code&gt;Value of test-statistic&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;I really appreciate any help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T09:59:12.650" Id="70764" LastActivityDate="2013-09-25T10:59:25.160" LastEditDate="2013-09-23T10:19:12.260" LastEditorUserId="21054" OwnerUserId="30589" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;augmented-dickey-fuller&gt;" Title="How to interpret augmented Dickey-Fuller unit root test in R" ViewCount="3336" />
  <row Body="&lt;p&gt;The survival function at time $t$ for an exponential distribution with true mean $\mu$ is&#10;$$&#10;S(t) = e^{-t/\mu} \quad , \quad t &amp;gt; 0.&#10;$$&#10;According to&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_distribution#Parameter_estimation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Exponential_distribution#Parameter_estimation&lt;/a&gt;&#10;one over the sample mean is the maximum likelihood estimator of $1/\mu$, so that&#10;$$&#10;\hat S(t) = e^{-t /\hat \mu}&#10;$$&#10;serves as a natural estimator of $S$. A different way would be to drop the parametric assumption and use 1 minus the empirical distribution function instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;For both approaches, stratification by gender or other factors works as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T11:03:06.213" Id="70773" LastActivityDate="2014-04-23T19:01:28.467" LastEditDate="2014-04-23T19:01:28.467" LastEditorUserId="30351" OwnerUserId="30351" ParentId="52493" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you have available the data set of the predicted values (and not just the graph), then you could apply &quot;reverse-engineering&quot; by trial-and-error, i.e. try to &quot;estimate&quot; a function from this data, treating them as random although they are not, by applying standard time-series approaches. In principle you would eventually find the &lt;em&gt;exact&lt;/em&gt; forecasting equation.&lt;/p&gt;&#10;&#10;&lt;p&gt;But generally speaking, the fact that for a specific interval (the circled values) the forecasting function appears to behave as a &quot;~linear function with structural shifts&quot; can not, on its own, help &quot;guess&quot; the function, because, in other parts of the graph the function exhibits different behavior.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T12:30:05.370" Id="70778" LastActivityDate="2013-09-23T12:30:05.370" OwnerUserId="28746" ParentId="70768" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The general one-line formula to linearly rescale data values having observed &lt;strong&gt;min&lt;/strong&gt; and &lt;strong&gt;max&lt;/strong&gt; into a new arbitrary range &lt;strong&gt;min'&lt;/strong&gt; to &lt;strong&gt;max'&lt;/strong&gt; is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  newvalue= (max'-min')/(max-min)*(value-max)+max'&#10;  or&#10;  newvalue= (max'-min')/(max-min)*(value-min)+min'.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2013-09-23T16:13:43.403" Id="70808" LastActivityDate="2013-09-23T16:13:43.403" OwnerUserId="3277" ParentId="70801" PostTypeId="2" Score="4" />
  <row AnswerCount="3" Body="&lt;p&gt;I am looking for a thesis topic for my Applied Economics degree where I can use Data Mining/Machine Learning for some kind of prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you guys have any good ideas about what I could do?&lt;/p&gt;&#10;&#10;&lt;p&gt;My first thought was credit scoring, but that seems a little boring considering the number of papers out there and the lack of available data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't want to write about stock market forecasting because that's what I did for my mathematician thesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good topic would be predicting some sort of an economical indicator but I don't really have any good ideas so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" ClosedDate="2014-06-03T16:20:51.873" CommentCount="3" CommunityOwnedDate="2014-05-13T22:54:37.970" CreationDate="2013-09-23T16:38:49.997" FavoriteCount="3" Id="70811" LastActivityDate="2014-06-03T15:44:53.440" OwnerUserId="29688" PostTypeId="1" Score="7" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;application&gt;" Title="Applications of Data Mining in economics" ViewCount="853" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using the nls procedure in R to fit a logistic growth model. In their SSlogis function, José Pinheiro and Douglas Bates chose the formulation&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Asym / (1 + exp((xmid-input) / scal))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for their model. As I am fairly inexperienced with the numerical properties of such models, I wonder:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Can somebody explain why the authors chose this formulation instead of possible alternatives? In particular, ecologists seem to prefer a model with initial population, carrying capacity and growth rate. Does the formulation above have favourable numerical properties?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It seems that when the model is misspecified and the data are actually fairly linear with time, this formulation often fails to converge. Could such a problem be avoided?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is parameter orthogonality a key concern here or are other aspects of the model more important?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is it trivial to extend this model to allow a flexible intercept? Would the following model provide sensible numerical properties?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Intercept + (Asym - Intercept ) / (1 + exp((xmid-input) / scal))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am, of course, open for alternatives as long as it allows for some flexibility in intercept, location where 50% of the growth has been achieved and asymptote.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-09-23T17:33:31.410" FavoriteCount="1" Id="70821" LastActivityDate="2013-10-01T11:15:04.833" LastEditDate="2013-10-01T11:15:04.833" LastEditorUserId="29020" OwnerUserId="29020" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;logistic&gt;&lt;nls&gt;&lt;numerics&gt;&lt;growth-model&gt;" Title="Numerical properties of the logistic growth model for non-linear regression" ViewCount="112" />
  <row Body="&lt;p&gt;You are right to be skeptical of this approach. &lt;strong&gt;The Taylor series method does not work in general,&lt;/strong&gt; although the heuristic contains a kernel of truth. To summarize the technical discussion below,&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Strong concentration&lt;/strong&gt; implies that the Taylor series method works for nice functions&lt;/li&gt;&#10;&lt;li&gt;Things can and will go &lt;strong&gt;dramatically wrong&lt;/strong&gt; for heavy-tailed distributions or not-so-nice functions&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As Alecos's answer indicates, this suggests that the Taylor-series method should be scrapped if your data might have heavy tails. (Finance professionals, I'm looking at you.) &lt;/p&gt;&#10;&#10;&lt;p&gt;As Elvis noted, key problem is that &lt;strong&gt;the variance does not control higher moments&lt;/strong&gt;.  To see why, let's simplify your question as much as possible to get to the main idea.  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Suppose&lt;/strong&gt; we have a sequence of random variables $X_n$ with $\sigma(X_n)\to 0$ as $n\to \infty$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can we guarantee that $\mathbb{E}[|X_n-\mu|^3] = o(\sigma^2(X_n))$ as $n\to \infty?$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Since &lt;a href=&quot;https://en.wikipedia.org/wiki/Heavy-tailed_distribution#Common_heavy-tailed_distributions&quot;&gt;there are&lt;/a&gt; random variables with finite second moments and infinite third moments, the answer is emphatically &lt;strong&gt;no&lt;/strong&gt;.  Therefore, in general, &lt;strong&gt;the Taylor series method fails even for 3rd degree polynomials&lt;/strong&gt;. Iterating this argument shows you cannot expect the Taylor series method to provide accurate results, even for polynomials, unless &lt;strong&gt;all&lt;/strong&gt; moments of your random variable are well controlled.&lt;/p&gt;&#10;&#10;&lt;p&gt;What, then, are we to do? Certainly the method works for &lt;em&gt;bounded&lt;/em&gt; random variables whose support converges to a point, but this class is far too small to be interesting.  Suppose instead that the sequence $X_n$ comes from some &lt;strong&gt;highly concentrated&lt;/strong&gt; family that satisfies (say)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{P}\left\{ |X_n-\mu|&amp;gt; t\right\} \le \mathrm{e}^{- C n t^2} \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for every $t&amp;gt;0$ and some $C&amp;gt;0$. Such random variables are surprisingly common. For example when $X_n$ is the empirical mean&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ X_n := \frac{1}{n} \sum_{i=1}^n Y_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;of nice random variables $Y_i$ (e.g., iid and bounded), various &lt;a href=&quot;https://en.wikipedia.org/wiki/Hoeffding%27s_inequality&quot;&gt;concentration inequalities&lt;/a&gt; imply that $X_n$ satisfies (1). A standard argument (see p. 10 &lt;a href=&quot;http://arxiv.org/pdf/1011.3027.pdf&quot;&gt;here&lt;/a&gt;) bounds the $p$th moments for such random variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathbb{E}[|X_n-\mu|^p] \le \left(\frac{p}{2 C n}\right)^{p/2}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, for any &quot;sufficiently nice&quot; analytic function $f$ (see below), we can bound the error $\mathcal{E}_m$ on the $m$-term Taylor series approximation using the triangle inequality&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathcal{E}_m:=\left|\mathbb{E}[f(X_n)] - \sum_{p=0}^m \frac{f^{(p)}(\mu)}{p!} \mathbb{E}(X_n-\mu)^p\right|\le \tfrac{1}{(2 C n)^{(m+1)/2}} \sum_{p=m+1}^\infty |f^{(p)}(\mu)| \frac{p^{p/2}}{p!}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;when $n&amp;gt;C/2$.  Since Stirling's approximation gives $p! \approx p^{p-1/2}$, the error of the truncated Taylor series satisfies&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathcal{E}_m = O(n^{-(m+1)/2}) \text{ as } n\to \infty\quad \text{whenever} \quad \sum_{p=0}^\infty p^{(1-p)/2 }|f^{(p)}(\mu)| &amp;lt; \infty \tag{2}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, when $X_n$ is strongly concentrated and $f$ is sufficiently nice, the Taylor series approximation is indeed accurate. The inequality appearing in (2) implies that $f^{(p)}(\mu)/p! = O(p^{-p/2})$, so that in particular our condition requires that $f$ is &lt;a href=&quot;https://en.wikipedia.org/wiki/Entire_function#Properties&quot;&gt;entire&lt;/a&gt;.  This makes sense because (1) does not impose any boundedness assumptions on $X_n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's see what can go wrong when $f$ is has a singularity (following whuber's comment). Suppose that we choose $f(x)=1/x$.  If we take $X_n$ from the $\mathrm{Normal}(1,1/n)$ distribution truncated between zero and two, then $X_n$ is sufficiently concentrated but $\mathbb{E}[f(X_n)] = \infty$ for every $n$.  In other words, we have a &lt;em&gt;highly concentrated, bounded random variable&lt;/em&gt;, and still the Taylor series method fails when the function has just one singularity.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A few words on rigor.&lt;/strong&gt;  I find it nicer to present the condition appearing in (2) as &lt;em&gt;derived&lt;/em&gt; rather than a &lt;em&gt;deus ex machina&lt;/em&gt; that's required in a rigorous theorem/proof format.  In order to make the argument completely rigorous, first note that the right-hand side in (2)  implies that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}[|f(X_n)|] \le \sum_{i=0}^\infty \frac{|f^{(p)}(\mu)|}{p!} \mathbb{E}[|X_n-\mu|^p]&amp;lt; \infty$$&lt;/p&gt;&#10;&#10;&lt;p&gt;by the growth rate of subgaussian moments from above.  Thus, Fubini's theorem provides&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathbb{E}[f(X_n)] = \sum_{i=0}^\infty \frac{f^{(p)}(\mu)}{p!} \mathbb{E}[(X_n-\mu)^p]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The rest of the proof proceeds as above.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-09-23T17:42:54.137" Id="70822" LastActivityDate="2013-09-24T03:58:55.667" LastEditDate="2013-09-24T03:58:55.667" LastEditorUserId="30577" OwnerUserId="30577" ParentId="70490" PostTypeId="2" Score="17" />
  <row Body="&lt;p&gt;A covariate is just another independent variable which is metric. In ANOVA you can control for the influence of that variable by adding it to the factors (usually nominal variables).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-23T17:56:40.303" Id="70825" LastActivityDate="2013-09-23T18:00:41.953" LastEditDate="2013-09-23T18:00:41.953" LastEditorUserId="7290" OwnerUserId="30612" ParentId="70824" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Because you want to produce a &quot;sample&quot; that is representative of the type of distribution that you wish to compare with your real data and its expected distribution as well, for example, uniform, normal, exponential, and a whole host of others.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-23T18:13:00.110" Id="70827" LastActivityDate="2013-09-23T18:13:00.110" OwnerUserId="19815" ParentId="70799" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;After consulting the TA, what the question was asking was whether if &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ &#10;\int_{-\infty}^{\infty}f(x)dx=1&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $f(x)$ in this case is the density of the uniform(a,b).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-23T20:01:01.307" Id="70834" LastActivityDate="2013-09-23T20:01:01.307" OwnerUserId="30499" ParentId="70553" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;You might be interesting in taking a look at the &lt;a href=&quot;http://en.wikipedia.org/wiki/Vuong%27s_closeness_test&quot; rel=&quot;nofollow&quot;&gt;Vuong's test&lt;/a&gt; and the subsequent literature. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T00:07:07.793" Id="70851" LastActivityDate="2013-09-24T00:07:07.793" OwnerUserId="6828" ParentId="70828" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="71247" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Update: I got the JAGS model running and this eliminates the distracting part of my question. It's really about the proper preparation of data for dinterval() and inits.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't find a concrete example of preparing right-censored survival data for use in JAGS when starting with a vector of times and a vector of censored status. The examples I've seen all have a constant censoring time.&lt;/p&gt;&#10;&#10;&lt;p&gt;My two questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the correct way to prepare dinterval(t[i], c[i]) when you are starting with a vector of event times and a matching vector of censored/failed status?&lt;/li&gt;&#10;&lt;li&gt;What is the reason for supplying a initial value vector of event times where &lt;em&gt;uncensored&lt;/em&gt; times are set to NA and &lt;em&gt;censored&lt;/em&gt; times are set to a value &gt; than the observed censoring time? I'm really struggling to understand that.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update: I think I have the answer to (2). Apparently JAGS complains if you try to overwrite values in the init vector, but if the vector is initialized to NA it works.&lt;/strong&gt;   &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that my understanding of these requirements comes from looking at existing online examples. If the requirements are unfounded I'd love to learn that.&lt;/p&gt;&#10;&#10;&lt;p&gt;I created a short test using the aml data set from the survival package which does a survival fit then tries to fit a Weibull distribution using JAGS and add those points for comparison. Originally I thought you had to pass zeros in for the unscensored times in c[i] for dinterval(t[i], c[i]), but that causes an error: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Error in node is.censored[1]&#10;  Observed node inconsistent with observed parents at initialization.&#10;  Try setting appropriate initial values.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here is my sample R script. Some lines seem to be indented too much, not sure why. I converted all tabs to spaces and it looks clean in my edit window:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Right-censoring test&#10;require(survival)&#10;require(runjags)&#10;graphics.off()&#10;rm(fit, S, results)&#10;&#10;# SURVIVAL FIT AND PLOT&#10;fit &amp;lt;- survfit(Surv(aml$time, aml$status) ~ 1, conf.type = &quot;log-log&quot;)&#10;plot(fit, xlab = &quot;time&quot;, ylab = &quot;survival&quot;, main = &quot;AML survival&quot;)&#10;&#10;# JAGS WEIBULL FIT WITH CENSORING&#10;# Model uses three different versions of observed times.&#10;# t.orig is the original set of observed times including censored times.&#10;# t.mod has the censored times set to NA.&#10;# t.cens is the set I have questions about. Currently the same as t.orig&#10;# but is that the proper choice?&#10;#&#10;model &amp;lt;- &quot;model {&#10;    for(i in 1:N) {&#10;        is.censored[i] ~ dinterval(t.mod[i], t.cens[i])&#10;        t.mod[i] ~ dweib(a, b)&#10;        S[i] &amp;lt;- 1 - pweib(t.orig[i], a, b)&#10;    }&#10;    a ~ dgamma(3.5, 2)&#10;    b ~ dgamma(1.5, 100)&#10;}&quot;&#10;&#10;censored &amp;lt;- !aml$status&#10;    t.orig &amp;lt;- aml$time&#10;t.mod &amp;lt;- aml$time&#10;    t.mod[censored] &amp;lt;- NA&#10;    t.cens &amp;lt;- aml$time&#10;# Online examples seem to indicate special treatment of the c[i]&#10;# passed to dinterval(t[i], c[i]), changing the uncensored times.&#10;#t.cens[!censored] &amp;lt;- 0 # THIS CAUSES AN ERROR&#10;&#10;data &amp;lt;- dump.format(list(&#10;    t.orig = t.orig,&#10;    t.mod = t.mod,&#10;    N = length(t.mod),&#10;    t.cens = t.cens,&#10;    is.censored = as.numeric(censored)  &#10;))&#10;&#10;# Apparently you must provide initial values of the time vector&#10;# with uncensored times of NA and censored times &amp;gt; the observed time.&#10;# I don't understand this requirement at all.&#10;t.init &amp;lt;- rep(NA, length(t.mod))&#10;t.init[censored] &amp;lt;- t.cens[censored] + 1&#10;&#10;inits &amp;lt;- c(&#10;    dump.format(list(a = 1.1, b = 3.0e-4, t.mod = t.init)),&#10;    dump.format(list(a = 2.3, b = 0.02, t.mod = t.init))&#10;)&#10;results = run.jags(model, monitor=c(&quot;S&quot;, &quot;a&quot;, &quot;b&quot;), data=data, inits=inits)&#10;S &amp;lt;- print(results, vars=&quot;S&quot;)&#10;points(aml$time, S[,4], col=&quot;blue&quot;, pch=19)&#10;    segments(aml$time, S[,1], aml$time, S[,3], col=&quot;blue&quot;, lwd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks for taking the time to look at this.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-24T02:07:47.990" FavoriteCount="2" Id="70858" LastActivityDate="2013-11-01T22:57:05.090" LastEditDate="2013-11-01T22:57:05.090" LastEditorUserId="11756" OwnerUserId="30633" PostTypeId="1" Score="3" Tags="&lt;survival&gt;&lt;mcmc&gt;&lt;jags&gt;&lt;censoring&gt;&lt;weibull&gt;" Title="Right-censored survival fit with JAGS" ViewCount="484" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a longitudinal data set and I used Amelia to create 5 imputed sets. Then I use the &lt;code&gt;NLME&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; to do growth modelling on each of the imputed data sets separately. I am looking for a feasible way to pool the results together for reporting (including, coefficients, variance decomposition, and model comparison).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T06:27:51.740" Id="70874" LastActivityDate="2013-09-24T06:54:27.827" LastEditDate="2013-09-24T06:54:27.827" LastEditorDisplayName="user25658" OwnerUserId="30644" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;data-imputation&gt;&lt;nlme&gt;" Title="Pool results of 5 Amelia imputed datasets after NLME growth modelling" ViewCount="90" />
  <row Body="&lt;p&gt;First, wilcoxon test in &lt;code&gt;scipy.stats&lt;/code&gt; does NOT use $W$ as the test statics, it instead uses $T$ as defined in Siegel's popular book: Non-parametric statistics for the behavioral sciences. And yes, as @whuber correctly pointed out, once you know $T$ and sample size, $W$ is also defined (@whuber, strictly speaking, not quite, one also need to know how 0 differences are handled).&lt;/p&gt;&#10;&#10;&lt;p&gt;Only can only know how the test is implemented by reading the source code. For &lt;code&gt;scipy&lt;/code&gt;, Wilcoxon test can be found in &lt;code&gt;your_python_package_folde/scipy/stats/morestats.py&lt;/code&gt;. Compare to &lt;code&gt;R&lt;/code&gt;'s &lt;code&gt;wilcox.test&lt;/code&gt;, it is very simple. Go over the code, and you will see that it is equivalent to having &lt;code&gt;correct=FALSE, exact=FALSE, paired=TRUE&lt;/code&gt; flags on in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Python:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scipy import stats&#10;&amp;gt;&amp;gt;&amp;gt; x1=[48,  7, 12, 11, 62, 93, 79, 53, 28, 49, 74, 59, 57, 62, 22,  8, 30, 11,  2, 47]&#10;&amp;gt;&amp;gt;&amp;gt; x2=[20, 13, 41, 61, 93, 11, 28, 61, 26, 91, 95,  5, 80, 45, 88, 99, 50, 96, 69, 93]&#10;&amp;gt;&amp;gt;&amp;gt; stats.wilcoxon(x1, x2) # T and p value, two-sided&#10;(60.0, 0.092963126712486244)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x1&amp;lt;-c(48,  7, 12, 11, 62, 93, 79, 53, 28, 49, 74, 59, 57, 62, 22,  8, 30, 11,  2, 47)&#10;&amp;gt; x2&amp;lt;-c(20, 13, 41, 61, 93, 11, 28, 61, 26, 91, 95,  5, 80, 45, 88, 99, 50, 96, 69, 93)&#10;&amp;gt; wilcox.test(x1,x2,correct=FALSE,exact=FALSE,paired=TRUE)&#10;&#10;    Wilcoxon signed rank test&#10;&#10;data:  x1 and x2 &#10;V = 60, p-value = 0.09296&#10;alternative hypothesis: true location shift is not equal to 0 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T06:50:45.643" Id="70876" LastActivityDate="2013-09-24T06:50:45.643" OwnerUserId="23790" ParentId="62110" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What you really need is &lt;a href=&quot;http://opencv.org/&quot; rel=&quot;nofollow&quot;&gt;OpenCV &lt;/a&gt;. OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. It has C++, C, Python and Java interfaces and supports Windows, Linux, Android and Mac OS.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may find there lots of solutions for computer vision recognition problems.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T07:37:59.823" Id="70878" LastActivityDate="2013-09-24T07:37:59.823" OwnerUserId="29943" ParentId="70868" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In fact, this is an easy one. I just had a look at the proof of the NP lemma, in order to understand better what's going on. And the solution was suddenly clear. We have $\mathbb R^n$ and we consider two measures on it, $m_0$ and $m_a$, induced by $H_0$ and $H_a$, respectively. Our RR should be a subset that is as small as possible under $m_0$ (so as to minimize the type I error) while at the same time being as large as possible under $m_a$ (in order to maximize the power). Now since in this case the supports of the two measures are two hypercubes, one subset of the other, namely $[0,\theta_a]^n\subset [0,\theta_0]^n$, we simply choose for the RR the hypecube $[0,t]^n$, where $t=\min\{\theta_0\sqrt[n]{\alpha},\theta_a\}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T08:13:32.107" Id="70883" LastActivityDate="2013-09-24T08:13:32.107" OwnerUserId="30628" ParentId="70847" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There are three common causes for the zero degrees of freedom:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Saturated model. These are usually spotted from NAs is &lt;code&gt;summary(model)&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;Factor level combinations with zero observations.These are spotted as zero dfs in &lt;code&gt;Anova(model)&lt;/code&gt; for the interaction effects, but the main effect would still have, e.g., one df.&lt;/li&gt;&#10;&lt;li&gt;Perfectly collinear explanatory variables. These are spotted from the zero dfs for the interaction effects in presense of the main effects in &lt;code&gt;Anova(model)&lt;/code&gt;, or if only main effects are included the model, some of the main effect could have zero dfs.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Your case falls under the third cause. Judging from the specification of the models 1 and 2, some of your explanatory variables you have added to model 2 (ethnicity, gender, age, enrollment, Pell, TANF, GPA, onlineExp) are collinear with level and type.&lt;/p&gt;&#10;&#10;&lt;p&gt;For finding out the problematic variables, consult the answer on &lt;a href=&quot;http://stackoverflow.com/questions/3042117/screening-multicollinearity-in-a-regression-model&quot;&gt;Stackoverflow&lt;/a&gt;. Briefly, command &lt;code&gt;model.matrix(model)&lt;/code&gt; gives out, well, the model matrix for your model. Check the output! Following the advice George Dontas gave on the linked thread, you can then find out which ones are the problematic predictors. Small eigen values from the following command indicate problems:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;eigen(cov(model.matrix(model)[,-1]))$values&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For a very small R example, try the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;y&amp;lt;-rnorm(9)&#10;x1&amp;lt;-c(0,0,0,1,1,1,2,2,2)&#10;x2&amp;lt;-c(0,1,2,0,1,2,0,1,2)&#10;x3&amp;lt;-c(1,1,1,2,2,2,3,3,3)&#10;fit1&amp;lt;-lm(y~x1+x2)&#10;fit2&amp;lt;-lm(y~x1+x2+x3)&#10;fit3&amp;lt;-lm(y~x1+x2+x3+x1*x3)&#10;library(car)&#10;Anova(fit1)&#10;Anova(fit2) # main effects only&#10;Anova(fit3) # interactions, also&#10;e&amp;lt;-eigen(cov(model.matrix(fit2)[,-1]))$values&#10;names(e)&amp;lt;-colnames(model.matrix(fit2)[,-1]) # x3 creates the problem, the value is very low&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Another possibility is to add the new variables one by one, and see when the problem shows itself. You can then try to rectify the situation, e.g., by not adding the problematic variables to the model. Sometimes combining the variables in a suitable way helps, also.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T10:18:45.237" Id="70892" LastActivityDate="2013-09-24T10:18:45.237" OwnerUserId="26206" ParentId="70660" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Camera setup: We have setup some stereo cameras in a living apartment. That is, the indoor environment is monitored. With the stereo cameras, wide lens (3.5mm) are used to cover a big volume.&lt;/p&gt;&#10;&#10;&lt;p&gt;The height from the floor is around 2.8 meters. The objects (such as mug, bottle, telephone) are at least 3 meters away. For instance, an object (75mmx102mm), which located 3 meters away from the camera, is represented by 15x20px in the camera image. Thus, the images are getting smaller in the far field.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have around 30 different objects to be recognized.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not use the depth information, because it is not so accurate. I just use RGB values from a single camera. Image resolution is 1360 x 1024 pixels.&lt;/p&gt;&#10;&#10;&lt;p&gt;Approaches: 1. Point detectors/descriptors, matches (some models in the database, and check the match one by one) 2. Bag of visual worlds + SVM classification (5 object categories)&lt;/p&gt;&#10;&#10;&lt;p&gt;I had experiences with Haar-cascade but I never tried for my current issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;What methods/approaches should I try to investigate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-24T10:54:57.513" Id="70898" LastActivityDate="2013-09-24T10:54:57.513" OwnerUserId="30661" PostTypeId="1" Score="1" Tags="&lt;pattern-recognition&gt;" Title="Recognition of household objects ( image processing, pattern recognition ) using wide lens?" ViewCount="45" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'd appreciate any insights or references to research regarding the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you have a discrete metric space with a probability distribution on it. also, suppose that I'm given a number $k$. My problem is finding the highest probability ball. &lt;/p&gt;&#10;&#10;&lt;p&gt;A &lt;strong&gt;ball&lt;/strong&gt; is the set of all points whose distance from a given point is less than or equal to $k$, and the &quot;probability of the ball&quot; is just the sum of the probabilities of this set of points.&lt;/p&gt;&#10;&#10;&lt;p&gt;The continuous case (less interesting in my case, but potentially helpful) would be to have a probability distribution on $R^n$, an finding the n-ball with the highest integral when integrating the density function on the $n$-ball.&lt;/p&gt;&#10;&#10;&lt;p&gt;This problem is an abstraction of a practical problem I try to solve. the solution of special case of $k=0$ (e.g. in which case a &quot;ball&quot; is just a single point) is just the mode of the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The general case arises for example in the following prediction problem: suppose you try to &quot;predict&quot; $n$ independent binary events. So every prediction is a binary $n$-tuple (with a probability distribution imposed on that space, based, for example, on assumptions, previous data, etc.). Now, suppose you are allowed to have a few mistakes in your prediction, and you try to find a prediction that maximizes the probability of &quot;success&quot; (that is, not making too many mistakes). The solution is exactly an n dimensional &quot;ball&quot; in the n-tuple space with the hamming metric.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea, insight, or reference to relevant research would be much appreciated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-24T12:58:15.233" FavoriteCount="1" Id="70906" LastActivityDate="2013-09-24T21:29:51.170" LastEditDate="2013-09-24T13:20:33.543" LastEditorUserId="8960" OwnerUserId="25986" PostTypeId="1" Score="6" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;prediction&gt;&lt;metric&gt;" Title="Finding the highest probability balls" ViewCount="84" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;If we have $n &amp;gt; 1000$ units to sample from, how can we estimate the number of subsets of a certain size, since the number is too much for &lt;code&gt;R&lt;/code&gt; to handle?&lt;/p&gt;&#10;&#10;&lt;p&gt;What I mean is, if I can't use &lt;code&gt;choose(1000, 10)&lt;/code&gt; to get the total number of subsets of size $10$, is there a way to estimate it?&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2013-09-24T23:10:31.833" Id="70960" LastActivityDate="2013-09-25T15:25:14.553" LastEditDate="2013-09-25T15:25:14.553" LastEditorUserId="919" OwnerUserId="19848" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;sampling&gt;&lt;arithmetic&gt;&lt;subset&gt;" Title="Estimating the total number of sample subsets of size 10 from a population of size 1000" ViewCount="71" />
  <row Body="&lt;p&gt;Welcome to CV!&lt;/p&gt;&#10;&#10;&lt;p&gt;I cannot answer on which one is preferred (they are actually really close so I don't think it matters much), but generally, major statistical software packages use Satterthwaite's method. &lt;code&gt;SPSS&lt;/code&gt; and &lt;code&gt;SAS&lt;/code&gt; both use it. In &lt;code&gt;Stata&lt;/code&gt;, some commands like &lt;code&gt;ttest&lt;/code&gt; would allow user to specify Welch's method, but Satterthwaite's is still the default.&lt;/p&gt;&#10;&#10;&lt;p&gt;And in literature, I have mostly seen Satterthwaite's formula being cited. Time to time it's referred to as Satterthwaite-Welch's degrees of freedom, but the formula cited is Satterthwaite's. I guess having published it one year earlier did matter.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-24T23:58:49.860" Id="70962" LastActivityDate="2013-09-25T00:22:36.983" LastEditDate="2013-09-25T00:22:36.983" LastEditorUserId="13047" OwnerUserId="13047" ParentId="70959" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="70983" AnswerCount="1" Body="&lt;p&gt;I've been experimenting with the rgl library to draw 3d plots of some linear models. The package works really nicely, but I have a question on the math behind how one draws a plane (using planes3d) that shows the 'fit' (predicted values ) of the linear model, for a model with 2 parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The R package documentation says that the arguments -- a, b, c, and d -- of a call to planes3d must be chosen so they are the coordinates of the normal  to the plane (given by the model). And there is an example on page 33 of this document: &lt;a href=&quot;http://cran.r-project.org/web/packages/rgl/rgl.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/rgl/rgl.pdf&lt;/a&gt;, which I have slightly modified to use the data from the mtcars package (see R code below).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the listing I create the model from the mtcars data at line 3, then I plot the data points at line 6. Next I plot the plane in lines 9 - 15. The a,b,c,d coordinates are chosen per instructions in the documentation.   In particular,'c' is set to be -1, which is currently just a 'magic' value to me. I don't understand why -1 is chosen for 'c'.&lt;/p&gt;&#10;&#10;&lt;p&gt;But it works out. In line 18 - 26 I define vectors for the normal to the plane, and for a point lying within the plane.  I take the dot product of these two and add the intercept (offset) and I get zero. It all works out as expected. But I am hoping someone can explain the theory behind &gt;why&amp;lt; it works out.  &lt;/p&gt;&#10;&#10;&lt;p&gt;R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1  library(rgl)&#10; 2  attach (mtcars)&#10; 3  model &amp;lt;- lm(mpg ~  cyl + disp)&#10; 4  &#10; 5  # Plot data points &#10; 6  plot3d(cyl, disp, mpg, type=&quot;s&quot;, col=&quot;red&quot;, size=1) &#10; 7  &#10; 8  &#10; 9  # Plot the plane of points that make up the predicted values of the model&#10;10  coefs &amp;lt;- coef(model)&#10;11  a &amp;lt;- coefs[&quot;cyl&quot;]&#10;12  b &amp;lt;- coefs[&quot;disp&quot;]&#10;13  c &amp;lt;- -1&#10;14  d &amp;lt;- coefs[&quot;(Intercept)&quot;]&#10;15  planes3d(a, b, c, d, alpha=0.5)&#10;16  &#10;17  &#10;18  # define a normal vector to the plane&#10;19  #&#10;20  normal = c(a,b,c)&#10;21  &#10;22  # define a vector within the plane&#10;23  #&#10;24  newdata &amp;lt;- data.frame(cyl = 5, disp = 200)&#10;25  predicted = predict(model,  newdata)&#10;26  pointOnPlane = c(5, 200, predicted)&#10;27  &#10;28  &#10;29  &#10;30  # Verify that the dot products of the normal and the vector that defines pointOnPlane  is 0 &#10;31  # (after we subtract the offset 'd', which came from  the intercept  of the model). &#10;32  &#10;33  # This will print as 'TRUE'&#10;34  sum(pointOnPlane * normal)  + d   == 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;epilogue:  Got a great answer from Andre Silva, below. Thanks, Andre !  makes sense now ;^)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-25T01:30:03.813" Id="70967" LastActivityDate="2013-09-26T20:33:56.157" LastEditDate="2013-09-26T20:33:56.157" LastEditorUserId="30687" OwnerUserId="30687" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;scatterplot&gt;&lt;package&gt;" Title="Mathematics behind how the normal to the plane is derived from a linear model when plotting with planes3d" ViewCount="199" />
  
  <row AcceptedAnswerId="70998" AnswerCount="1" Body="&lt;p&gt;I use &lt;code&gt;escalc()&lt;/code&gt; function from &lt;code&gt;metafor&lt;/code&gt; package to calculate various effect sizes or outcome measures (and the corresponding sampling variances) that are commonly used in meta-analyses.&lt;/p&gt;&#10;&#10;&lt;p&gt;In most of articles are tables of &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;standard deviation&lt;/em&gt; which can be easily used by &lt;code&gt;escalc()&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# for example:&#10;# group A == mean=7; sd=1.8; n=13&#10;# group B == mean=3.5; sd=3; n=179&#10;escalc(m1i=7, sd1i=1.8, n1i=13, m2i=3.5, sd2i=3, n2i=179, measure=&quot;MD&quot;)&#10;      yi     vi&#10;1 3.5000 0.2995&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...unfortunately, in some articles are tables consisting of &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;confidence intervals&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there any way how to compute effect size by using confidence intervals instead of standard deviation?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# for example&#10;# group A == mean=19.25; CI=17.1-20.1; n=28&#10;# group B == mean=8;     CI=6.8-9.2;   n=72&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; or if not from CI than maybe from &lt;em&gt;range&lt;/em&gt; (probably impossible).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-25T08:16:25.383" FavoriteCount="2" Id="70993" LastActivityDate="2013-09-25T08:53:54.527" OwnerUserId="28218" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;meta-analysis&gt;" Title="How to calculate effect sizes from mean and CI" ViewCount="149" />
  <row Body="&lt;p&gt;It would not be too hard for simple analyses like a $t$-test, as long as you know the sample size. The range of a confidence interval is $2\cdot t\cdot \mathrm{SD}/\sqrt{N}$, which is also the margin of error times 2. If the $t$-value is not reported, it can be found with the appropriate df (from sample size) and alpha value. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm sure similar things can be done in more complicating tests, but but I'm not quite sure how off the top of my head. It might get messy dealing with df. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-25T08:51:41.313" Id="70998" LastActivityDate="2013-09-25T08:53:54.527" LastEditDate="2013-09-25T08:53:54.527" LastEditorUserId="21054" OwnerUserId="30460" ParentId="70993" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working on merging records from several databases that cover the same entities, but share no reliably deterministic fields, leaving us with fields such as name and address to resolve identity.  In reading about this problem I came across the Fellegi-Sunter statistical method for resolving record linkage.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't tell from my reading, however, exactly how the U probability should be determined.  I know that it is the likelihood of two &quot;randomly&quot; paired records matching on a given field, but it's the &quot;random&quot; part that I'm struggling with.  I'm working with a quantity of records that makes it impossible to compare all record pairings.  For the actual comparison stage, I'll be &quot;blocking&quot; the records using zip code, but when generating U probabilities, this would seem to contradict the &quot;randomly paired&quot; idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it acceptable to compare records that fall in the same &quot;block&quot; for purposes of generating the U probabilities, or is there some other method, such as a sample of truly random pairings, that I should be using?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-25T12:10:37.790" Id="71012" LastActivityDate="2013-09-25T12:10:37.790" OwnerUserId="30703" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;data-mining&gt;" Title="Generating M/U Probabilities in Fellegi-Sunter Record Linkage" ViewCount="90" />
  
  <row Body="&lt;p&gt;Depending on what you mean by linear (as asked by @Macro), you could do a polynomial regression. I'm not familiar with SPSS, but you could create a variable called x2 that is simply x squared, then include that term in a multiple regression model. &lt;/p&gt;&#10;&#10;&lt;p&gt;A quick search showed that in SPSS you don't even have to create an additional variable, but could check an option to have SPSS model a quadratic term: &lt;a href=&quot;http://www.ucdenver.edu/academics/colleges/nursing/Documents/PDF/NonlinearExampleHowTo.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.ucdenver.edu/academics/colleges/nursing/Documents/PDF/NonlinearExampleHowTo.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-25T15:14:20.267" Id="71027" LastActivityDate="2013-09-25T15:14:20.267" OwnerUserId="30682" ParentId="71022" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have been working with data and building models on data. I have developed models in regression using cubic and power series. It works fine for variables with one dependent and one independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;What kind of technique is to be adopted if I have to find an equation or model for say, D depends on C, C changes for a set of B, which changes for different A.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can carry our bi-variate correlation to find significance. But, that is not something I am interested in. I have data, but wondering as to how to fit a model in those lines. I have tried building models with regression, line fitting and get a relationship between D and C, or D and B. Good fits with varying constants, but no relationship between the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to combine and build a model altogether?&lt;/p&gt;&#10;&#10;&lt;p&gt;What is that kind of an analysis called?&lt;/p&gt;&#10;&#10;&lt;p&gt;How to go about it? Any softwares that would help me do it? I have been using SPSS, Minitab and R.&lt;/p&gt;&#10;" ClosedDate="2013-09-27T17:55:40.660" CommentCount="2" CreationDate="2013-09-25T09:09:16.613" Id="71046" LastActivityDate="2013-09-25T17:21:13.500" OwnerDisplayName="VR17" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Mathematical model building with dependent and independent variables" ViewCount="41" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have read and heard in &lt;a href=&quot;http://en.wikipedia.org/wiki/Deep_learning&quot; rel=&quot;nofollow&quot;&gt;several&lt;/a&gt; &lt;a href=&quot;http://www.ipam.ucla.edu/publications/gss2012/gss2012_10596.pdf&quot; rel=&quot;nofollow&quot;&gt;places&lt;/a&gt; that Deep Learning Networks take considerably longer to train than, say, support vector/kernel machines, random forests or boosting methods, but they can give better performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, what is fundamentally different about deep learning networks in relation to other learning methods that explains this difference? What is it known about their training complexity? &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, are DNN discriminative (in contrast to the other methods)? It looks like they could be considered generative since Wikipedia mentions: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Once sufficiently many layers have been learned the deep architecture&#10;  may be used as a generative model by reproducing the data when&#10;  sampling down the model (an &quot;ancestral pass&quot;) from the top level&#10;  feature activations.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I have also read on Wikipedia that DNNs can be pre-trained in an &lt;strong&gt;unsupervised manner&lt;/strong&gt;. Is this a notable difference in relation to other methods. Is this pre-training step that common? But most importantly, &lt;strong&gt;why do DNNs take so long to train in comparison to other methods&lt;/strong&gt;? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-25T19:12:47.140" FavoriteCount="5" Id="71054" LastActivityDate="2013-09-30T14:52:39.497" LastEditDate="2013-09-30T14:52:39.497" LastEditorUserId="27838" OwnerUserId="27838" PostTypeId="1" Score="4" Tags="&lt;intuition&gt;&lt;deep-learning&gt;" Title="Deep Learning Networks: Fundamental differences" ViewCount="229" />
  <row Body="&lt;p&gt;If you want to know more about the meaning and calculation of the F test when used as a criterion for the analysis of variance (ANOVA) with examples in Excel, I recommend this series of four articles.  The final formula is able to take into account the size of alpha, the number of degrees of freedom for the F ratio's numerator and denominator, and the noncentrality parameter.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The Concept of Statistical Power - &lt;a href=&quot;http://www.quepublishing.com/articles/article.aspx?p=2036566&quot; rel=&quot;nofollow&quot;&gt;http://www.quepublishing.com/articles/article.aspx?p=2036566&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;The Statistical Power of t-Tests - &lt;a href=&quot;http://www.quepublishing.com/articles/article.aspx?p=2036565&quot; rel=&quot;nofollow&quot;&gt;http://www.quepublishing.com/articles/article.aspx?p=2036565&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;The Noncentrality Parameter in the F Distribution - &lt;a href=&quot;http://www.quepublishing.com/articles/article.aspx?p=2036567&quot; rel=&quot;nofollow&quot;&gt;http://www.quepublishing.com/articles/article.aspx?p=2036567&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Calculating the Power of the F Test - &lt;a href=&quot;http://www.quepublishing.com/articles/article.aspx?p=2036568&quot; rel=&quot;nofollow&quot;&gt;http://www.quepublishing.com/articles/article.aspx?p=2036568&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-09-25T22:18:57.200" Id="71063" LastActivityDate="2013-09-25T22:18:57.200" OwnerUserId="27301" ParentId="55550" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="71089" AnswerCount="1" Body="&lt;p&gt;And yet another for the long list of degrees-of-freedom questions!&lt;/p&gt;&#10;&#10;&lt;p&gt;Given an i.i.d. sample $x_1,..., x_n$ from an arbitrary real-valued distribution with expectation $\mu$, the sample mean can be written as&#10;$$\bar x = \sum_i \frac{1}{n} x_i.$$&#10;One can say that the degrees of freedom for $\bar x$ is $n-1$, as there are $n$ data points and one parameter being estimated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose that some twisted statistician decides that the weights $1/n$ are not good enough, and replaces $\bar x$ with &#10;$$\bar x_w = \sum_i w_i x_i,$$&#10;where $w = (w_1, ..., w_n)$ is a pre-determined (i.e., not related to the data) vector of nonnegative weights with $\sum_i w_i = 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the degrees of freedom for $\bar x_w$?  References to versions of this problem in peer-reviewed work would be of particular interest.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-09-26T03:42:20.873" FavoriteCount="1" Id="71081" LastActivityDate="2013-09-28T14:15:41.637" OwnerUserId="27765" PostTypeId="1" Score="3" Tags="&lt;degrees-of-freedom&gt;&lt;weighted-mean&gt;" Title="Degrees of freedom for a weighted average" ViewCount="529" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;By &quot;generalized&quot;, I mean the circumstance where I have more than 2 periods of time and 2 treatment groups whereas the typical  D-I-D model involes just 2 periods, 1 treatment group and 1 control group. Moreover, an individual may jump between these treatment groups, say some observations may fall in treatment group A in period t but falls in treatment group B or control group in period t+1.&#10;    Are there any methods or models particularly that can solve the problem like this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-26T12:09:37.993" FavoriteCount="2" Id="71109" LastActivityDate="2014-11-22T10:01:58.957" LastEditDate="2013-09-26T12:21:05.453" LastEditorUserId="2116" OwnerUserId="30743" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;panel-data&gt;&lt;difference-in-difference&gt;" Title="Generalized difference-in-difference model in panel data" ViewCount="215" />
  <row Body="&lt;p&gt;Sounds like a &lt;a href=&quot;http://en.wikipedia.org/wiki/Power_transform&quot; rel=&quot;nofollow&quot;&gt;Box-Cox&lt;/a&gt; transformation. I think the real challenge is that the distributions in real data are often much more eratic than the pre-defined family of distributions that this type of method use. &lt;/p&gt;&#10;&#10;&lt;p&gt;One common problem are spikes. For example, imagine you have a survey containing the number of hours a week that the respondent works and the occupational status. For the first variable you would expect in most Western countries a huge spike at 40 hours a week. For the second variable you would expect to see spikes for common occupations like teachers, but for older cohorts you might expect a spike at farmers. If you think about it it is pretty self-evident, but the problem is that computers don't think&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-26T12:31:39.603" Id="71114" LastActivityDate="2013-09-26T12:31:39.603" OwnerUserId="23853" ParentId="71102" PostTypeId="2" Score="-1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;If I have a time series of Xt observations. I convert them to returns by:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Rt = Ln(Pt) - Ln(Pt-1).&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I then calculate the 20 period moving average and subtract from the Return to find the de-trended return DRt:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DRt = Rt- MA(Rt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Lastly I calculate the 20 period Moving Average of the de-trended return and calculate the standard deviation SD using:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;SDt = SQRT(SUM(MA(DRt)-DRt)^2/20)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So basically I am doing a differencing and de-trending on time series, and then finding the standard deviation. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is how do I convert back to the original scale the value MA(DRt) + SDt? I cant seem to get the transformations back correctly. I have done this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;SDt + MA(DRt) + MA(Rt) = Rt,&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;exp(Rt) = Yt - Xt,&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where Yt is the value of SDt +MA(DRt) in terms of the original time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;therefore,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Yt = exp(Rt) + Xt&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or in other words&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Yt = exp(SDt + MA(DRt) + MA(Rt)) + Xt&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Pretty sure this is not correct!&lt;/p&gt;&#10;&#10;&lt;p&gt;Also is there a better math function toolkit I can use to write out more clearly what I am doing on stackexchange?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-26T13:00:19.560" Id="71121" LastActivityDate="2013-09-26T13:00:19.560" OwnerUserId="30341" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;standard-deviation&gt;" Title="converting back to raw/original scale from time series tranformations and standard deviation" ViewCount="52" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a dataset in which I have measured a variable (&lt;code&gt;diff&lt;/code&gt;) on 38 participants (&lt;code&gt;sub&lt;/code&gt;) in two different conditions (&lt;code&gt;cond.lag&lt;/code&gt;). Now I am interested in whether or not the slope of the &lt;code&gt;position&lt;/code&gt; of the item differs between the two conditions. That is I am interested in whether or not there is an interaction between &lt;code&gt;cond.lag&lt;/code&gt; and &lt;code&gt;position&lt;/code&gt; (which I center on 0 prior).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(lme4)&#10;options(contrasts=c('contr.sum', 'contr.poly'))&#10;&#10;# read data&#10;dat &amp;lt;- read.table(&quot;http://pastebin.com/raw.php?i=MmNQigRv&quot;, colClasses = c(NA, rep(&quot;factor&quot;, 2), rep(&quot;numeric&quot;, 2)))&#10;# center position&#10;dat$pos.centered &amp;lt;- scale(dat$position, scale = FALSE)&#10;&#10;# fit the model&#10;m1 &amp;lt;- lmer(diff ~ cond.lag * pos.centered + (cond.lag * pos.centered|sub), dat)&#10;print(summary(m1), corr = FALSE)&#10;## [...]&#10;## Fixed effects:&#10;##                          Estimate Std. Error t value&#10;## (Intercept)             0.0819639  0.0121378   6.753&#10;## cond.lag1              -0.0122033  0.0155427  -0.785&#10;## pos.centered            0.0031775  0.0006429   4.942&#10;## cond.lag1:pos.centered -0.0011495  0.0011351  -1.013&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Although it appears as if I don't find an interaction (only a main effect of the slope of position), I am unsure of wether or not this model makes sense as I find a very unusual (I expect heavy tailed) qq plot:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qqnorm(resid(m1))&#10;qqline(resid(m1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QEWoj.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Does this model make any sense albeit the seemingly bad behaved residuals?&lt;/li&gt;&#10;&lt;li&gt;What can I do to remove those heavy tails or obtain a better model?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-09-26T17:59:27.130" FavoriteCount="1" Id="71172" LastActivityDate="2013-09-27T13:44:14.407" OwnerUserId="442" PostTypeId="1" Score="5" Tags="&lt;mixed-model&gt;&lt;residuals&gt;&lt;lmer&gt;&lt;lme4&gt;&lt;qq-plot&gt;" Title="How to deal with very s-shaped qq plot in a linear mixed model (lme4 1.0.4)?" ViewCount="1202" />
  <row Body="&lt;p&gt;There are a few different things that could be going on here:&lt;/p&gt;&#10;&#10;&lt;p&gt;One, the multicolinearity cannot be assessed based only on bivariate correlations.  Consider the case where x1, x2, and x3 are all generated as independent normal random vanriables and x4 is the sum of x1, x2, and x3.  There is definite colinearity with x4, but the bivariate colinearities are not strong (I did a quick example and the correlations ranged from 0.55 to 0.63).  So you should use a better measure of colinearity than just bivariate correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Two, it may be that x1, x2, and x3 explain quite a bit of the variation in the response variable and are independent of x4, so that without x1-x3 the effect size of x4 is small compared to the residual variation, but when you include x1-x3 the residual variation is much smaller and the effect size of x4 stays about the same, but is now large relative to the residual variation and therefore significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are probably other explanations as well.  You should really explore your data and all the relationships, then also consider the science behind the data, which model to use should be the one that makes the most sense scientifically.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-26T18:03:33.263" Id="71173" LastActivityDate="2013-09-26T18:03:33.263" OwnerUserId="4505" ParentId="71149" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;You need modifications to the bootstrap (.632, .632+) only because the original research used a discontinuous improper scoring rule (proportion classified correctly).  For other accuracy scores the ordinary optimism bootstrap tends to work fine.  For more information see &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/RmS#Studies_of_Methods_Used_in_the_T&quot; rel=&quot;nofollow&quot;&gt;http://biostat.mc.vanderbilt.edu/RmS#Studies_of_Methods_Used_in_the_T&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Improper scoring rules mislead you on the choice of features and their weights.  In other words, everything that can go wrong will go wrong.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-26T20:32:36.637" Id="71189" LastActivityDate="2013-09-26T20:32:36.637" OwnerUserId="4253" ParentId="71184" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="71204" AnswerCount="3" Body="&lt;p&gt;The data I have are the ages male or female won an award (79 data points, ages, for each gender). When constructing a freq table I used to find the number of classes,&#10;$$ n = 79 $$&#10;$$ \frac{\log(n)}{\log(2)} +1 \approx 8 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;to find the class width for each gender&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{max - min}{8} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;The freq tables look like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Female&#10;Values      Frequency   Rel Freq.&#10;1. 21 – 28   20          25%&#10;2. 29 – 36   31          39%&#10;3. 37 – 44   17          22%&#10;4. 45 – 52   4           5%&#10;5. 53 – 60   2           3%&#10;6. 61 – 68   3           4%&#10;7. 69 – 76   1           1%&#10;8. 77 – 84   1           1%&#10;&#10;Male&#10;Values      Frequency   Rel Freq.&#10;1. 29 – 34   10          13%&#10;2. 35 – 40   20          25%&#10;3. 41 – 46   24          30%&#10;4. 47 – 52   13          17%&#10;5. 53 – 58   6           8%&#10;6. 59 – 64   5           6%&#10;7. 65 – 70   0           0%&#10;8. 71 – 76   1           1%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The histograms end up looking like this (STATDISK),&#10;&lt;img src=&quot;http://www.xdcclan.com/images/histograms.jpg&quot; alt=&quot;Histograms&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Using a class start with the lowest data point in each data set, the histograms&#10;appear uneven with one starting at 0 and ending at 100 and the other starting at 20 and ending at 80&lt;/p&gt;&#10;&#10;&lt;p&gt;The histograms need the same amount of classes, but wouldn't it be better if I did at values as,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Values      Freq&#10;20 - 24     #&#10;25 - 30     #&#10;35 - 39     #&#10;etc&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;to get a better histogram to compare with? or does this not matter?&lt;/p&gt;&#10;&#10;&lt;p&gt;Do the # of classes only have to be the same? Do the histograms both have to start at the same point (0 or 20)? Are these histograms sufficient/correct? what if I said it is mandatory to use the formulas above to find the # of classes and class width? Should I use the same class start for each in the histograms?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-26T22:30:27.900" FavoriteCount="1" Id="71201" LastActivityDate="2014-04-25T09:44:10.013" LastEditDate="2013-09-26T22:43:14.737" LastEditorUserId="88" OwnerUserId="30770" PostTypeId="1" Score="3" Tags="&lt;histogram&gt;&lt;frequency&gt;" Title="Comparing histograms" ViewCount="1665" />
  <row Body="&lt;p&gt;From the documentation for that function.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;args : tuple, sequence, optional&#10;    Distribution parameters, used if `rvs` or `cdf` are strings.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So in the case of the uniform distribution, the args are the bounds for the uniform. E.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;stats.uniform.rvs(1, 5, size=10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;See if that helps.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Returns&#10;-------&#10;D : float&#10;    KS test statistic, either D, D+ or D-.&#10;p-value :  float&#10;    One-tailed or two-tailed p-value.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd read the docs for more information.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-27T03:32:40.370" Id="71213" LastActivityDate="2013-09-27T03:32:40.370" OwnerUserId="6828" ParentId="71144" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="71251" AnswerCount="3" Body="&lt;p&gt;Can I compare AIC values of a linear function with a non-linear function? Because I get totally different results. One is 4000 other the 6000000. Estimation is done on the same data setvariables.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-27T10:52:28.547" Id="71231" LastActivityDate="2013-09-27T16:31:47.467" LastEditDate="2013-09-27T10:59:11.603" LastEditorUserId="88" OwnerUserId="29335" PostTypeId="1" Score="2" Tags="&lt;aic&gt;" Title="Can I compare AIC values of a linear function with a non-linear function?" ViewCount="442" />
  
  <row Body="&lt;p&gt;To keep things really simple, you could consider using a simple mean/standard deviation inspired ratio, a bit like a z-score?&lt;/p&gt;&#10;&#10;&lt;p&gt;If you assume that the counts for two days, $X_1$ and $X_2$ are Poisson random samples with $\lambda_1$ and $\lambda_2$ respectively, then the change in word count follows a Skellam distribution, with mean $\lambda_2-\lambda_1$ and variance $\lambda_2+\lambda_1$&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking simple point estimates, I think it would therefore be reasonable to construct:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{Score} = \frac{X_2 - X_1}{\sqrt{X_2+X_1}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So in your example,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{Score_{dog}} = \frac{45}{\sqrt{135}} = 3.87$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{Score_{cat}} = \frac{2}{\sqrt{6}} = 0.816$&lt;/p&gt;&#10;&#10;&lt;p&gt;You could consider more difficult inferences if you have a strong idea what your really want to detect, but based on your description I think the above will be nice and simple and capture roughly the behaviour you want.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-27T12:41:37.807" Id="71241" LastActivityDate="2013-09-27T12:41:37.807" OwnerUserId="19879" ParentId="71222" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have created an &lt;code&gt;lme&lt;/code&gt; model using the same predictors both with and without a specified correlation structure based upon distance between the points (lat/long). &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that I am able to compare the AIC/BIC between different correlation structures (corSpher vs. corExp vs. corGaus), but am unsure if I can directly compare the AIC/BIC values of a model with no correlation (&lt;code&gt;lme(y~x1+x2, random=~x1|A, data = K)&lt;/code&gt;), and one with a correlation  (&lt;code&gt;lme(y~x1+x2, random=~x1|A), corr = corGaus(form=~long+lat), data = K)&lt;/code&gt;) structure?&lt;/p&gt;&#10;&#10;&lt;p&gt;If I am not able to compare the AIC/BIC, are there any recommend methods in comparing the fit/best model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice would be greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-09-27T12:49:15.610" Id="71243" LastActivityDate="2013-09-27T13:35:58.490" LastEditDate="2013-09-27T13:35:58.490" LastEditorUserId="442" OwnerUserId="30232" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;autocorrelation&gt;&lt;spatial&gt;&lt;mixed-effect&gt;" Title="Comparings BIC of lme models with and without a correlation function" ViewCount="164" />
  <row Body="&lt;p&gt;I was asked to re-post this answer here from my comment at &lt;a href=&quot;http://doingbayesiandataanalysis.blogspot.com/2012/01/complete-example-of-right-censoring-in.html&quot; rel=&quot;nofollow&quot;&gt;http://doingbayesiandataanalysis.blogspot.com/2012/01/complete-example-of-right-censoring-in.html&lt;/a&gt;&#10;The specifics of this answer relate to the model in that comment, but the concepts apply to the topic here.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The core of the JAGS model for censored data is this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;isCensored[i] ~ dinterval( y[i] , censorLimitVec[i] )&#10;y[i] ~ dnorm( mu , tau )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The key to understanding what JAGS is doing is that JAGS automatically imputes a random value for any variable that is not specified as a constant in the data. Thus, when &lt;code&gt;y[i]&lt;/code&gt; is &lt;code&gt;NA&lt;/code&gt; (i.e., a missing value, not a constant), then JAGS imputes a random value for it.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what value should it generate?&lt;/p&gt;&#10;&#10;&lt;p&gt;The second line of the model, above, says that &lt;code&gt;y[i]&lt;/code&gt; should be randomly generated from a normal distribution with mean mu and precision tau.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the first line of the model, above, puts another constraint on the randomly generated value of &lt;code&gt;y[i]&lt;/code&gt;. That line says that whatever value of &lt;code&gt;y[i]&lt;/code&gt; is randomly generated, it must fall on the side of &lt;code&gt;censorLimitVec[i]&lt;/code&gt; dictated by the value of &lt;code&gt;isCensored[i]&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;To understand this part, let's unpack the &lt;code&gt;dinterval()&lt;/code&gt; distribution. Suppose that &lt;code&gt;censorLimitVec&lt;/code&gt; has 3 values in it, not just 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;censorLimitVec = c(10,20,30)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then randomly generated values from &lt;code&gt;dinterval(y,c(10,20,30))&lt;/code&gt; will be either 0, 1, 2, or 3 depending on whether $y&amp;lt;10$, $10 &amp;lt; y &amp;lt; 20$, $20&amp;lt;y&amp;lt;30$, or $30&amp;lt;y$. So, if $y=15$, &lt;code&gt;dinterval(y,c(10,20,30))&lt;/code&gt; has output of $1$ with 100% probability. The trick is this: We instead specify the output of &lt;code&gt;dinterval&lt;/code&gt;, and impute a random value of &lt;code&gt;y&lt;/code&gt; that could produce it. Thus, if we say&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 ~ dinterval(y,c(10,20,30))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then &lt;code&gt;y&lt;/code&gt; is imputed as a random value between 10 and 20.&lt;/p&gt;&#10;&#10;&lt;p&gt;Putting the two model statements together,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 ~ dinterval( y , censorLimit )&#10;&#10;y ~ dnorm( mu , tau )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;means that &lt;code&gt;y&lt;/code&gt; comes from a normal density and &lt;code&gt;y&lt;/code&gt; must fall above the &lt;code&gt;censorLimit&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that helps!!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-27T13:18:44.767" Id="71247" LastActivityDate="2013-09-27T13:47:13.550" LastEditDate="2013-09-27T13:47:13.550" LastEditorUserId="442" OwnerUserId="30790" ParentId="70858" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;On general cases like these are what AIC and BIC are for, comparing different models that cannot be compared with likelihood ratio tests alone (or there are to many comparisons or so). However, in case of mixed models some caveats are to add.&lt;/p&gt;&#10;&#10;&lt;p&gt;Copying some relevant parts from the &lt;a href=&quot;http://glmm.wikidot.com/faq&quot; rel=&quot;nofollow&quot;&gt;R-SIG-mixed faq&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;h3&gt;How can I test whether a random effect is significant?&lt;/h3&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;perhaps you shouldn't (if the random effect is part of the experimental design, this procedure may be considered 'sacrificial pseudoreplication' (Hurlburt 1984); using stepwise approaches to eliminate non-significant terms in order to squeeze more significance out of the remaining terms is dangerous in any case)&lt;br&gt;&#10;  [...]&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;  &#10;  &lt;h3&gt;Can I use AIC for mixed models? How do I count the number of degrees of freedom for a random effect?&lt;/h3&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;Yes, with caution.&lt;br&gt;&#10;  [...]&lt;/li&gt;&#10;  &lt;li&gt;in cases when testing a variance parameter, AIC may be subject to the same kinds of boundary effects as likelihood ratio test p-values (i.e., AICs may be conservative/overfit slightly when the nested parameter value is on the boundary of the feasible space). [...]&lt;/li&gt;&#10;  &lt;li&gt;AIC also inherits the primary problem of likelihood ratio tests in the GLMM context — that is, that LRTs are asymptotic tests. [...]&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;More can be found there and I vaguely remember reading something on this issue in Pinheiro &amp;amp; Bates.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-27T13:33:51.027" Id="71250" LastActivityDate="2013-09-27T13:33:51.027" OwnerUserId="442" ParentId="71243" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;whuber is right that, technically, statistical inference will not be accurate if randomization was not used. However, in practice, random sampling is often impossible, so it is common (and inevitable and unfortunate) practice that people use inference on non-random samples and generalize the results to the entire population from which the sample was drawn.&lt;/p&gt;&#10;&#10;&lt;p&gt;The actual conclusions that you can confidently make is to simply describe the sample without generalizing the results to others in the population. For example, let's say group 1 had a higher average score on a certain test than group 2. You can conclude that, among the children you measured, group 1 scored higher on the test than group 2. In this case, you are only comparing the 5 children in group1 with the 5 in group 2, and inference is not used (i.e., p-vaues and confidence intervals would not make sense). Simply calculate descriptive statistics such as mean, median, population standard deviation, etc. to describe your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Keep in mind that you can still run various tests or calculate effect sizes without using inference. For example, you can calculate Cohen's d between group 1 and 2 on a test. You can use ANCOVA and find the mean difference of test score between the groups while controlling for the effect of age and gender. I think some people do not realize that things like ANOVA or multiple regression can be used for descriptive statistics.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-27T16:17:14.923" Id="71274" LastActivityDate="2013-09-27T18:20:01.077" LastEditDate="2013-09-27T18:20:01.077" LastEditorUserId="30460" OwnerUserId="30460" ParentId="71268" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to estimate the kernel density for number of days a child is sick. Around 73% of children report not being sick, i.e. zero. How do I estimate a kernel density for this censored variable using Stata?&lt;/p&gt;&#10;" ClosedDate="2013-10-01T21:38:12.240" CommentCount="2" CreationDate="2013-09-27T18:38:53.347" Id="71282" LastActivityDate="2013-09-27T21:09:36.857" LastEditDate="2013-09-27T21:09:36.857" LastEditorUserId="22047" OwnerUserId="30800" PostTypeId="1" Score="0" Tags="&lt;nonparametric&gt;&lt;kernel&gt;" Title="Kernel density estimation for a variable with lots of zeros" ViewCount="46" />
  <row AnswerCount="0" Body="&lt;p&gt;I have bootstrapped a linear model in PROC GLM.  Now I want to construct CI's around the parameter estimates.  For binary variables, it's straightforward: drop the reference category where the parameter estimates are zero.  However, for each replicate, the interaction of the binary term and a quadratic term produces two parameter estimates:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          b2*a     Est            Std err&#10;b2*a       0      -0.0312886      0.02925361&#10;&#10;b2*a       1      -0.0339503      0.03308580&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It's not clear to me how to add up the replicates to obtain the mean in this case.  Normally I use PROC UNIVARIATE and capture the percentile points, but only when there is one value per replicate.  I'm not sure what to do.  Can someone enlighten me?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-27T19:31:39.493" Id="71287" LastActivityDate="2013-09-27T19:31:39.493" OwnerUserId="28835" PostTypeId="1" Score="2" Tags="&lt;bootstrap&gt;&lt;linear-model&gt;" Title="Bootstrap quadratic term" ViewCount="26" />
  <row AnswerCount="0" Body="&lt;p&gt;I need to compare the goodness of fit of several averaged logistic regression models by calculating the deviance explained. I'm using the &lt;code&gt;MuMIn&lt;/code&gt; package in R to average many logistic regression models into a single averaged model. I ultimately want to compare the explanatory power of several averaged models, in part by using the deviance explained.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Does deviance explained apply to averaged models as a strong measure of the goodness of fit?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How does one calculate the deviance explained (calculated as the null deviance less the residual deviance as a proportion of the null deviance) from the averaged model output from the &lt;code&gt;model.avg()&lt;/code&gt; command in &lt;code&gt;MuMIn&lt;/code&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;Examining the structure of the averaged model object indicates that the null and residual deviances are calculated on each individual model that contributes to the averaged model, but I'm not sure how to extract them and then calculate the overall deviance explained by the averaged model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-09-27T20:05:48.220" Id="71292" LastActivityDate="2013-09-27T20:38:38.380" LastEditDate="2013-09-27T20:38:38.380" LastEditorUserId="7290" OwnerUserId="30805" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;multiple-regression&gt;&lt;goodness-of-fit&gt;&lt;deviance&gt;" Title="Calculate goodness-of-fit (with deviance) to compare averaged models?" ViewCount="180" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have data set that includes:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;multiple animals normalised heart rate&lt;/li&gt;&#10;&lt;li&gt;2 trials for each animal&lt;/li&gt;&#10;&lt;li&gt;3 time points per trial&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I want to analyse this data by comparing mean heart rate at each time point across trials... So, I have no between subject factor and only within subject factors... This is apparently a two way within-subjects design however I have no idea how to run this in SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry, I should have been more specific....&lt;/p&gt;&#10;&#10;&lt;p&gt;Animals were tested over two trials. I have taken raw heart rate and normalised it at 0.05 second intervals against the heart rate at the commencement of the each trial. From this, and based on previous literature, I have chosen 3 discrete time points in which to measure heart rate differentials between trials, 1 time point prior to the trial commencement, and two after (2 seconds prior, 2 second after and 7 seconds after).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking to see if trial one, at 2 second after commencement differs from all other time points in both sessions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this help?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-28T03:17:15.447" Id="71311" LastActivityDate="2015-02-10T02:08:34.527" LastEditDate="2013-09-28T11:03:33.747" LastEditorUserId="930" OwnerUserId="30813" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;" Title="Repeated measures with no between subject factors" ViewCount="112" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm using SPSS for building a neural network model. In the model summary there is a measure called &quot;Relative Error&quot;. What's the formula for it? Is it related to sum of squares error?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-28T06:46:42.700" Id="71315" LastActivityDate="2013-09-28T11:16:53.550" LastEditDate="2013-09-28T07:09:12.860" LastEditorUserId="16365" OwnerUserId="16365" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;neural-networks&gt;&lt;error&gt;" Title="What's &quot;Relative Error&quot; in a neural network model?" ViewCount="236" />
  
  <row Body="&lt;p&gt;It sounds like you are facing a semi-supervised classification problem. Based on your problem statement you have a positive set (second data set) and an unlabeled set (first data set). The first data set is unlabeled because it can contain both users that clicked the ad and users that didn't.&lt;/p&gt;&#10;&#10;&lt;p&gt;This kind of problem is a twist on traditional supervised classification. It poses a bunch of additional challenges, the main one being model selection, since the usual measures like accuracy are not useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you look up techniques to learn from positive and unlabeled data (often called &lt;em&gt;PU learning&lt;/em&gt;), you will probably find several useful ideas. The key idea that is common in most PU learning approaches is to weigh positive training instances (significantly) higher than unlabeled ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to determine which features are relevant, you may consider approaches like semi-supervised logistic regression. I consider &lt;a href=&quot;http://cseweb.ucsd.edu/~elkan/posonly.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Learning Classiﬁers from&#10;Only Positive and Unlabeled Data&lt;/em&gt; by Elkan and Noto&lt;/a&gt; to be a very good reference on the subject.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-09-28T08:12:57.570" Id="71318" LastActivityDate="2013-09-28T09:07:08.007" LastEditDate="2013-09-28T09:07:08.007" LastEditorUserId="25433" OwnerUserId="25433" ParentId="71286" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="71367" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/71302/power-of-a-mann-whitney-test-compared-to-a-t-test&quot;&gt;In a previous question&lt;/a&gt;, I asked about comparing the power of a t test to a Mann Whitney test under different situations. One of the answers pointed out that the worst that the Mann-Whitney can ever perform relative to the t-test is that it would require 1/0.864x as much data to give the same power as the t test so long as the data sets being compared were from the same distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I guess I am confused about how power can be compared between these two tests given that they test different null hypotheses. For example, if I estimate the power for a t-test at 0.9 using effect size equal to a 20% difference between means, then that makes sense for a t test. But a Mann Whitney test does not test for differences between means. If the distributions were the same it would test for differences between medians. Am I right in thinking that the Mann Whitney test would require 1/0.864x as much data to detect a 20% difference in medians with a power of 0.9?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-28T22:25:01.253" FavoriteCount="1" Id="71356" LastActivityDate="2013-09-29T10:10:37.183" LastEditDate="2013-09-28T23:48:14.330" LastEditorUserId="6029" OwnerUserId="22022" PostTypeId="1" Score="4" Tags="&lt;anova&gt;&lt;power-analysis&gt;&lt;power&gt;&lt;mann-whitney-u-test&gt;" Title="How is power for t test and Mann Whitney comparable when null hypotheses differ" ViewCount="111" />
  
  
  
  
  
  <row AcceptedAnswerId="71403" AnswerCount="1" Body="&lt;p&gt;Consider the multivariate normal distribution $X = [X_1, X_2, X_3]^T$ with&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mu = \begin{pmatrix} -3 \\ 1 \\ 4\end{pmatrix} \quad \quad \Sigma = \begin{pmatrix} 4 &amp;amp; -1 &amp;amp; 0\\ -1 &amp;amp; 5 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 2\end{pmatrix}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to find out, if the following random variables are independent:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$(X_1, X_2)$ and $X_3$&lt;/li&gt;&#10;&lt;li&gt;$X_1 - X_2$ and $X_1 + X_2 - X_3$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;How can I determine that? I see that e.g. $X_1$ and $X_3$ have covariance $= 0$. Normally, this doesn't imply, that they are independent, but (as I've learned from Wikipedia) since here  $X_1$ and $X_2$ are normally and jointly distributed, it does in fact apply. But how do I cope with the sums and minuses? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-09-29T13:44:25.137" Id="71394" LastActivityDate="2013-09-30T20:30:14.703" LastEditDate="2013-09-30T20:30:14.703" LastEditorUserId="30846" OwnerUserId="30846" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;&lt;independence&gt;" Title="Independence of multivariate normal distribution" ViewCount="240" />
  
  <row Body="Please do not use this tag. It is a duplicate. Use [multidimensional-scaling] instead. " CommentCount="0" CreationDate="2013-09-29T14:34:52.523" Id="71402" LastActivityDate="2014-04-26T02:07:26.997" LastEditDate="2014-04-26T02:07:26.997" LastEditorUserId="7290" OwnerUserId="686" PostTypeId="4" Score="0" />
  <row AcceptedAnswerId="71633" AnswerCount="1" Body="&lt;p&gt;I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y &gt; 0, then E(y) is gamma distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to set this up in BUGS/JAGS, because I've seen these models &lt;a href=&quot;http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags&quot;&gt;worked before for poisson-distributions&lt;/a&gt;. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# For the ones trick&#10;C &amp;lt;- 10000&#10;&#10;# for every observation&#10;for(i in 1:N){&#10;    # log-likelihood of the observation from the gamma likelihood&#10;    LogPos[i] &amp;lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])&#10;    #likelihood&#10;    Lpos[i] &amp;lt;- exp(LogPos[i])&#10;&#10;    # redefine the shape and rate parameters as a function of the mean and sd&#10;    shape[i] &amp;lt;- pow(mu[i], 2) / pow(sd, 2)&#10;    scale[i] &amp;lt;- mu[i] / pow(sd, 2)&#10;&#10;    # mu is a function of MTD: use the inverse link&#10;    #mu[i] &amp;lt;- 1/eta[i]&#10;    mu[i] &amp;lt;- beta0 + beta1*MTD[i]&#10;&#10;&#10;    # zero-inflated part, where w[i] is the probability of being zero&#10;    w[i] &amp;lt;- exp(zeta[i]) / (1 + exp(zeta[i]))&#10;    zeta[i] &amp;lt;- gamma0 + gamma1*MPD[i]&#10;&#10;    # ones trick&#10;    p[i] &amp;lt;- Lpos[i] / C&#10;    ones[i] ~ dbern(p[i])&#10;&#10;    # Full likelihood&#10;    Lik[i] &amp;lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]&#10;  } &#10;&#10;# PRIORS&#10;beta0 ~ dnorm(0, 0.001)&#10;beta1 ~ dnorm(0, 0.001)&#10;&#10;gamma0 ~ dnorm(0, 0.001)&#10;gamma1 ~ dnorm(0, 0.001)&#10;&#10;sd ~ dunif(0, 100)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Has anyone set a model up like this or have any advice on how to set it up correctly?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried a new set of code that's similar, but slight different. I still have not gotten it to work&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model{&#10;&#10;  # For the ones trick&#10;  C &amp;lt;- 10000&#10;&#10;  # for every observation&#10;  for(i in 1:N){&#10;&#10;    # make a dummy variable that is 0 if y is &amp;lt; 0.0001 and 1 if y &amp;gt; 0.0001. This is essentially a presence&#10;    # absence dummy variable&#10;    z[i] &amp;lt;- step(y[i] - 0.0001)&#10;&#10;    # define the logistic regression model, where w is the probability of occurance.&#10;    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function&#10;    w[i] &amp;lt;- exp(zeta[i]) / (1 + exp(zeta[i]))&#10;    zeta[i] &amp;lt;- gamma0 + gamma1*MPD[i]&#10;&#10;    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu&#10;    mu[i] &amp;lt;- exp(eta[i])&#10;    eta[i] &amp;lt;- beta0 + beta1*MTD[i]&#10;&#10;    # redefine the mu and sd of the continuous part into the shape and scale parameters&#10;    shape[i] &amp;lt;- pow(mu[i], 2) / pow(sd, 2)&#10;    scale[i] &amp;lt;- mu[i] / pow(sd, 2)&#10;&#10;    # for readability, define the log-likelihood of the gamma here&#10;    logGamma[i] &amp;lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])&#10;&#10;    # define the total likelihood, where the likelihood is (1 - w) if y &amp;lt; 0.0001 (z = 0) or&#10;    # the likelihood is w * gammalik if y &amp;gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be&#10;    # 0 and the second bit 1. Use 1 - z, which is 0 if y &amp;gt; 0.0001 and 1 if y &amp;lt; 0.0001&#10;    logLik[i] &amp;lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )&#10;&#10;    # Use the ones trick&#10;    p[i] &amp;lt;- logLik[i] / C&#10;    ones[i] ~ dbern(p[i])&#10;  } &#10;&#10;  # PRIORS&#10;  beta0 ~ dnorm(0, 0.001)&#10;  beta1 ~ dnorm(0, 0.001)&#10;&#10;  gamma0 ~ dnorm(0, 0.001)&#10;  gamma1 ~ dnorm(0, 0.001)&#10;&#10;  sd ~ dgamma(1, 2)&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE 2&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logLik[i] &amp;lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-09-29T19:16:28.253" FavoriteCount="1" Id="71414" LastActivityDate="2013-10-01T23:02:36.020" LastEditDate="2013-09-30T05:03:24.433" LastEditorUserId="27861" OwnerUserId="27861" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;bayesian&gt;&lt;jags&gt;" Title="Specify a Zero-inflated (Hurdle) Gamma Model in JAGS/BUGS" ViewCount="1053" />
  <row AcceptedAnswerId="71422" AnswerCount="1" Body="&lt;p&gt;Weekly's demand of a product has a probability function&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Demand, x   Probability (fx)&#10;------------------------&#10;0           0,1&#10;1           025&#10;2           0,4&#10;3           0,15&#10;4           0,10&#10;5 or more   0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have difficult to find out the &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;expected value,  &lt;/li&gt;&#10;&lt;li&gt;variance and  &lt;/li&gt;&#10;&lt;li&gt;standard deviation  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;for the weekly demand.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where should I start?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-29T21:19:11.820" Id="71420" LastActivityDate="2013-09-29T21:41:37.637" OwnerUserId="17322" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;variance&gt;&lt;standard-deviation&gt;&lt;expected-value&gt;" Title="Demand with different Measurement" ViewCount="26" />
  
  
  
  <row Body="&lt;p&gt;How about doing your linear regression, then take the residual by subtracting your regression line from the data, and look at its standard deviation (SD) or median absolute deviation (MAD)? (The SD part may be basically equivalent to the R^2, I'm not sure of the math.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is the &quot;one to five or six&quot; part. That's not much data there, but of course the data you have is what you have. (If you had a lot more data, you could consider decomposition methods on the residual to look at the power in higher and lower frequencies.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought of things like the KL-divergence or something like Alecos mentioned, but they're for differences in distributions not differences in time series, so I'm not sure they're applicable.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-09-30T12:34:58.543" Id="71469" LastActivityDate="2013-09-30T12:44:45.400" LastEditDate="2013-09-30T12:44:45.400" LastEditorUserId="1764" OwnerUserId="1764" ParentId="71455" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="71518" AnswerCount="1" Body="&lt;p&gt;How does one tell if a dataset is missing data at random? I've been reading up on how to impute missing values, and was wondering what techniques can be used to tell if data is really missing at random or systematically.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-09-30T20:17:05.723" FavoriteCount="1" Id="71514" LastActivityDate="2013-09-30T21:20:45.670" OwnerUserId="20709" PostTypeId="1" Score="4" Tags="&lt;missing-data&gt;&lt;data-imputation&gt;" Title="Missing data at random" ViewCount="105" />
  <row AcceptedAnswerId="71522" AnswerCount="1" Body="&lt;p&gt;Recently I’ve been working on a logistic model and I´m having some difficulties evaluating the results. I am still learning all this, so I apologise in advance for the mistakes. I would still appreciate any insight though.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, my model is a binomial logit. My explanatory variables are: 1 categorical variable with 15 categories, 1 dichotomous variable, and 2 continuous variables. My N is large +8000&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to model the decision of Firms to invest. Dependent variable in Investment (yes/no), the 15 level variables are different obstacles for investments reported by managers. The rest of the variables are controls for sales, credits and used capacity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below are my results, using the rms package in R.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  Model Likelihood     Discrimination    Rank Discrim.    &#10;                         Ratio Test            Indexes          Indexes       &#10;Obs          8035    LR chi2     399.83    R2       0.067    C       0.632    &#10; 1           5306    d.f.            17    g        0.544    Dxy     0.264    &#10; 2           2729    Pr(&amp;gt; chi2) &amp;lt;0.0001    gr       1.723    gamma   0.266    &#10;max |deriv| 6e-09                          gp       0.119    tau-a   0.118    &#10;                                           Brier    0.213                     &#10;&#10;          Coef    S.E.   Wald Z Pr(&amp;gt;|Z|)&#10;Intercept -0.9501 0.1141 -8.33  &amp;lt;0.0001 &#10;x1=10     -0.4929 0.1000 -4.93  &amp;lt;0.0001 &#10;x1=11     -0.5735 0.1057 -5.43  &amp;lt;0.0001 &#10;x1=12     -0.0748 0.0806 -0.93  0.3536  &#10;x1=13     -0.3894 0.1318 -2.96  0.0031  &#10;x1=14     -0.2788 0.0953 -2.92  0.0035  &#10;x1=15     -0.7672 0.2302 -3.33  0.0009  &#10;x1=2      -0.5360 0.2668 -2.01  0.0446  &#10;x1=3      -0.3258 0.1548 -2.10  0.0353  &#10;x1=4      -0.4092 0.1319 -3.10  0.0019  &#10;x1=5      -0.5152 0.2304 -2.24  0.0254  &#10;x1=6      -0.2897 0.1538 -1.88  0.0596  &#10;x1=7      -0.6216 0.1768 -3.52  0.0004  &#10;x1=8      -0.5861 0.1202 -4.88  &amp;lt;0.0001 &#10;x1=9      -0.5522 0.1078 -5.13  &amp;lt;0.0001 &#10;d2         0.0000 0.0000 -0.64  0.5206  &#10;f1        -0.0088 0.0011 -8.19  &amp;lt;0.0001 &#10;k8         0.7348 0.0499 14.74  &amp;lt;0.0001 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Basically I want to assess the regression in two ways, a) how well the model fits the data and b) how well is the model in predicting the outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;for a) (goodness of fit):&#10;Deviance tests based on Chi-squared are not appropriate in this case because the number of unique covariates approximates N, so we cannot assume a X2 distribution. &lt;strong&gt;Is this interpretation correct?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I can see the covariates using the epiR package.&lt;/p&gt;&#10;&#10;&lt;p&gt;require(epiR)&#10;logit.cp &amp;lt;- epi.cp(logit.df[-1]))&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id n x1   d2 f1 k8&#10; 1 1 13 2030 56  1&#10; 2 1 14  445 51  0&#10; 3 1 12 1359 51  1&#10; 4 1  1 1163 39  0&#10; 5 1  7  547 62  0&#10; 6 1  5 3721 62  1&#10;...&#10;7446&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have also read that the Hosmer-Lemeshow gof test is outdated, as it divides the data by 10 in order to run the test, which is rather arbitrary.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead I use the le Cessie–van Houwelingen–Copas–Hosmer test, implemented in the rms package. I not sure exactly how this test is performed, I have not read the papers about it yet.&#10;In any case, the results are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sum of squared errors     Expected value|H0                    SD                     Z &#10;         1711.6449914          1712.2031888             0.5670868            -0.9843245 &#10;                    P &#10;            0.3249560 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;P is large, so there is no sufficient evidence to say that my model doesn´t fit. Great! However....&lt;/p&gt;&#10;&#10;&lt;p&gt;b) When checking the predictive capacity of the model, I draw ROC curve and get the following results:&lt;/p&gt;&#10;&#10;&lt;p&gt;AUC is 0.6320586&#10;&lt;img src=&quot;http://i.stack.imgur.com/EzYWC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That doesn’t look very good.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, to sum up my questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1- Are the tests I run appropriate to check my model? What other test could I consider?&lt;/p&gt;&#10;&#10;&lt;p&gt;2- Do you find the model useful at all, or would you dismiss it based on the relatively poor ROC analysis results?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-09-30T20:58:50.167" FavoriteCount="6" Id="71517" LastActivityDate="2013-09-30T22:33:12.477" OwnerUserId="30777" PostTypeId="1" Score="7" Tags="&lt;logistic&gt;&lt;goodness-of-fit&gt;&lt;roc&gt;" Title="Evaluating a logistic regression" ViewCount="1441" />
  
  
  
  <row Body="&lt;p&gt;Predictive accuracy and AUC are quite limited in certain aspects. Try the &lt;a href=&quot;http://www.csse.monash.edu.au/~korb/pubs/ai02.pdf&quot; rel=&quot;nofollow&quot;&gt;Bayesian Information Reward&lt;/a&gt; (BIR), which addresses all your bullet points.&lt;/p&gt;&#10;&#10;&lt;p&gt;The intuition of BIR is as follows: a bettor is rewarded not just for identifying the ultimate winners and losers (0's and 1's), but more importantly for identifying the appropriate odds. Furthermore, it goes a step ahead and compares all predictions with the prior probabilities. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say you have a list of 10 Arsenal games with possible outcomes: &lt;code&gt;Win&lt;/code&gt; or &lt;code&gt;Lose&lt;/code&gt;. The formula for binary classification rewarding per game is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gI3No.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where, &lt;code&gt;p&lt;/code&gt; is your model's prediction for a particular football game, and &lt;code&gt;p'&lt;/code&gt; is the prior probability of Arsenal winning the game. As you can see, you treat the correct and incorrect classifications differently. For instance, if I know beforehand that &lt;code&gt;p'=0.6&lt;/code&gt;, and my predictor model produced &lt;code&gt;p =0.6&lt;/code&gt;, it is rewarded 0 since it is not conveying any new information.&lt;/p&gt;&#10;&#10;&lt;p&gt;BIR is not limited to binary classifications but is generalised for multinomial classification problems as well. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-01T10:07:32.457" Id="71556" LastActivityDate="2013-10-01T10:18:08.077" LastEditDate="2013-10-01T10:18:08.077" LastEditorUserId="28740" OwnerUserId="28740" ParentId="48419" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="71562" AnswerCount="1" Body="&lt;p&gt;I'm reading about MDL principle and my problem is on a book called: &quot;Guide to intelligent data analysis&quot;, 2nd edition page 106.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have added here a picture of the page I'm having trouble with. Could someone decode what the author is saying in the highlighted area:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sQzQ1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems abstract and confusing...I get confused when he says that the original data is contained in the decompression rule itself?!...huh?... and because of that I don't understand the rest of it :/&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-01T10:34:17.303" Id="71560" LastActivityDate="2013-10-01T10:59:30.473" OwnerUserId="18528" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;model-selection&gt;" Title="Help understanding an explanation about minimum description length principle" ViewCount="46" />
  <row AnswerCount="3" Body="&lt;p&gt;Is there a minimum item for questionnaire? Some researchers said that the total items for teenagers ranging from 80-110 items&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-01T11:06:51.563" Id="71564" LastActivityDate="2013-10-01T14:28:13.057" LastEditDate="2013-10-01T13:29:30.577" LastEditorUserId="919" OwnerUserId="30918" PostTypeId="1" Score="-1" Tags="&lt;survey&gt;&lt;reliability&gt;&lt;power-analysis&gt;&lt;rule-of-thumb&gt;" Title="Minimum items for questionnaire" ViewCount="380" />
  <row Body="&lt;p&gt;Welcome to the site.&lt;/p&gt;&#10;&#10;&lt;p&gt;No, there is no minimum number of items for a questionnaire. &lt;/p&gt;&#10;&#10;&lt;p&gt;Quite often, &lt;em&gt;shorter&lt;/em&gt; questionnaires are better as they can get more responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many many questionnaires have far less than 80 items.&lt;/p&gt;&#10;&#10;&lt;p&gt;And, finally what is &quot;total items for teenagers&quot; and who are these researchers?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-01T11:19:41.347" Id="71568" LastActivityDate="2013-10-01T11:19:41.347" OwnerUserId="686" ParentId="71564" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="71619" AnswerCount="1" Body="&lt;p&gt;The autocovariance is defined as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\gamma(t,s) = Cov(X_{t}, X_{s})=E[(X_{t}-\mu_{t})(X_{s}-\mu_{s})]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;When we have a stationary process the only thing that matters is the lag between the variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\gamma_{k} = Cov(X_{t}, X_{t-k})=E[(X_{t}-\mu)(X_{t-k}-\mu)]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the expectation means that we are summing over all possible values of the random variable $X$. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\gamma_{k}=\int\cdot\cdot\cdot\int(X_{t}-\mu)(X_{t-k}-\mu)f(X_{t},...,X_{t-k})dX_{t},...,dX_{t-k}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given this definition of $\gamma_{k}$, how can we derive the sample autocovariance:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\gamma_{k} = \frac{1}{N}\sum_{t=0}^{N}(X_{t}-\mu)(X_{t-k}-\mu)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;in which we are summing over $t$ instead of $X$. I think this is related to an ergodicity assuption, but I haven't found a very detailed explanation about it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another question: Is this definition of sample autocovariance correct  only when the process is stationary?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-01T16:46:40.833" Id="71601" LastActivityDate="2013-10-01T19:50:21.217" OwnerUserId="2676" PostTypeId="1" Score="1" Tags="&lt;covariance&gt;&lt;autocorrelation&gt;&lt;ergodic&gt;" Title="Derivation of sample autocovariance" ViewCount="101" />
  <row Body="&lt;p&gt;&lt;strong&gt;There is a simple efficient solution.&lt;/strong&gt;  It uses the ideas common to all rank-sum tests, such as the Wilcoxon tests.  This answer derives the solution and provides an &lt;code&gt;R&lt;/code&gt; implementation.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The code in the question simulates data that have a vanishingly small chance of exhibiting any ties at all between a sample of group $a$ and a sample of group $b$, so &lt;strong&gt;let's assume there exist no ties.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let there be $m$ elements in $a$ (and they &lt;em&gt;can&lt;/em&gt; have ties among each other) and $n$ elements in $b$ (which also may have ties). Let $A$ be the random variable modeled by drawing one element randomly and uniformly from group $A$ and similarly let $B$ be the random variable for one draw from group $B$.  &lt;strong&gt;The desired value (as I interpret the question) is the chance that $A$ exceeds $B$.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice that the test of whether a realization of $A$ exceeds one of $B$ is a simple comparison. Thus, the problem is unchanged if we replace all elements of $a$ and $b$ by their &lt;em&gt;indexes&lt;/em&gt; when the two sets are sorted in increasing order.  These indexes are their &lt;em&gt;ranks&lt;/em&gt;, provided that ties are resolved in some arbitrary manner (that is, do &lt;em&gt;not&lt;/em&gt; average the ranks of any groups of ties).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For example,&lt;/strong&gt; let $a = (0,2)$ and $b = (1,1,3)$.  Sorting the combined two (multi)sets gives the sequence $(0,1,1,2,3)$.  The indexes of the values coming from $a$ are $1$ and $4$ while the indexes of the values from $b$ are $2, 3,$ and $5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Compute the chance&lt;/strong&gt; $\Pr(A\gt B)$ by summing over the possible values of $A$, each of which has the probability $1/m$.  Let the ranks of these values be $r_1 \lt r_2 \lt \cdots \lt r_m$.  Suppose $r_i$ is picked.  Then the chance, conditional on this selection, that $B$ has a &lt;em&gt;smaller&lt;/em&gt; value equals the number of smaller values in $b$ divided by $n$.  The number of smaller values &lt;em&gt;altogether&lt;/em&gt; in both $a$ and $b$ is, by definition, $r_i-1$, but we know exactly $i-1$ of them (namely, $r_1, r_2, \ldots, r_{i-1}$) are in $a$.  Thus&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(A \gt B | A = r_i) = \frac{1}{n}\left(r_i-1 - (i-1)\right) = \frac{1}{n}\left(r_i-i\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;entailing&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(A\gt B) = \sum_{i=1}^m \Pr(A\gt B | A=r_i)\Pr(A=r_i) = \frac{1}{mn}\sum_i \left(r_i-i\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In the example,&lt;/strong&gt; $\Pr(A\gt B) = \frac{1}{2\times 3}((1-1) + (4-2)) = \frac{2}{6}$ and (reversing the roles of $a$ and $b$ as a quick check) $\Pr(B\gt A) = \frac{1}{3\times 2}((2-1) + (3-2) + (5-3)) = \frac{4}{6} = 1 - \frac{2}{6}$ as one would expect.&lt;/p&gt;&#10;&#10;&lt;p&gt;This calculation (when implemented as a general-purpose algorithm) requires sorting all $m+n$ values to find their ranks and then summing either $m$ or $n$ values (for efficiency, one would pick whichever is smaller).  Therefore the computational burden is $O((m+n)\log(m+n)),$ and can be reduced to $O(\min(m\log(n), n\log(m)))$ when the larger of $a$ and $b$ is already sorted.  That's pretty efficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;When there are ties&lt;/strong&gt; between elements of $a$ and $b$, the idea to reduce the question to rank sums and then compute a sum over conditional probabilities still works, but the calculations of the conditional probabilities get more complicated.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;R Code&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; will have trouble with calculations that overflow its integer data type.  The following solution handles that possibility.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;prob &amp;lt;- function(a, b, ...) {&#10;  # Returns chance that a random sample of `a` will exceed one of `b`&#10;  # (assuming no ties between elements of `a` and `b`)&#10;  # Optional args are passed to `rank` to control handling of NAs and&#10;  # how to resolve any ties.&#10;  m &amp;lt;- length(a); n &amp;lt;- length(b)&#10;  if (m &amp;lt; n) {&#10;    r &amp;lt;- rank(c(a,b), ...)[1:m] - 1:m&#10;  } else {&#10;    r &amp;lt;- rank(c(a,b), ...)[(m+1):(m+n)] - 1:n&#10;  }&#10;  s &amp;lt;- ifelse ((n+m)^2 &amp;gt; 2^31, sum(as.double(r)), sum(r)) / (as.double(m)*n)&#10;  return (ifelse(m &amp;lt; n, s, 1-s))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To emulate the data in the question, let's simulate sets of normally distributed values.  When the elements of $a$ come from a normal distribution with mean $\mu$ and standard deviation $\sigma$ and those of $b$ come from a normal distribution with mean $\nu$ and SD $\sigma$, we may analytically compute that $\Pr(A\gt B)$ (&lt;em&gt;prior&lt;/em&gt; to simulating the elements) equals $\Phi(\frac{\mu-\nu}{\sqrt{2}})$ where $\Phi$ is the cumulative standard normal distribution function.  This enables us to test &lt;code&gt;prob&lt;/code&gt;, as in the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(17)&#10;m &amp;lt;- 10^6; n &amp;lt;- 10^4&#10;mu.a &amp;lt;- 0; mu.b &amp;lt;- -2&#10;a &amp;lt;- rnorm(m, mu.a)&#10;b &amp;lt;- rnorm(n, mu.b)&#10;system.time(print(prob(a,b), digits=5))&#10;system.time(print(1 - prob(b,a), digits=5))&#10;print(pnorm((mu.a - mu.b)/sqrt(2)), digits=5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; system.time(print(prob(a,b), digits=5))&#10;[1] 0.92124&#10;   user  system elapsed &#10;   0.51    0.00    0.51 &#10;&amp;gt; system.time(print(1 - prob(b,a), digits=5))&#10;[1] 0.92124&#10;   user  system elapsed &#10;   0.51    0.00    0.52 &#10;&amp;gt; print(pnorm((mu.a - mu.b)/sqrt(2)), digits=5)&#10;[1] 0.92135&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It shows that the computation time does not depend on the order in which &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are provided to &lt;code&gt;prob&lt;/code&gt;.  The computation time of $1/2$ second is reasonably quick (for over one million numbers).  The close agreement of $0.92124$ and $0.92135$ is evidence in favor of the correctness of this solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;This solution can easily be iterated over groups using the usual &lt;code&gt;R&lt;/code&gt; idioms for looping.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-01T17:46:16.603" Id="71611" LastActivityDate="2013-10-01T23:07:54.603" LastEditDate="2013-10-01T23:07:54.603" LastEditorUserId="919" OwnerUserId="919" ParentId="71531" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="71703" AnswerCount="1" Body="&lt;p&gt;Suppose I have two vectors, &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt;, from which I can calculate the angle between these two vectors as a measure of their &quot;distance&quot;, using the arccos function, say. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v1 = c(100,200, 500,600)    &#10;v2 = c( 50, 30,  10,5)      &#10;v3 = c( 10,  7,  30,40) &#10;&#10;# pairwise angles&#10;acos( as.numeric((v1 %*% v2) / (norm_vec(v1) * norm_vec(v2))) ) * 180 / pi  # 66.8017&#10;acos( as.numeric((v2 %*% v3) / (norm_vec(v2) * norm_vec(v3))) ) * 180 / pi  # 66.67337&#10;acos( as.numeric((v1 %*% v3) / (norm_vec(v1) * norm_vec(v3))) ) * 180 / pi  # 8.061138&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This kind of measure will give similar distances (angles) &lt;strong&gt;regardless of the magnitude of the elements in the vectors&lt;/strong&gt;. For instance, the distance between &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt; is 66.80, and the distance between &lt;code&gt;v2&lt;/code&gt; and &lt;code&gt;v3&lt;/code&gt; is similarly 66.67, but clearly the &lt;strong&gt;magnitude&lt;/strong&gt; of &lt;code&gt;v2&lt;/code&gt; and &lt;code&gt;v3&lt;/code&gt; are more similar than &lt;code&gt;v1&lt;/code&gt;, so I am thinking of a measure that will also take the &quot;magnitude&quot; into account when calculating the dissimilarity. In other words, dist(v1, v2) should be greater than dist(v2,v3), but this result is obtained by &lt;strong&gt;still using the pairwise angle idea&lt;/strong&gt;. Thanks! &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you all for your replies! For the equivalence of Euclidean and angle distances, I use the same vectors as above to calculate the Euclidean distances:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Euclidean distance&#10;ed &amp;lt;- function(x1,x2)&#10;  sqrt(sum((x1 - x2) ^ 2))&#10;ed(v1,v2)  &#10;# 790.9014&#10;ed(v2,v3)  &#10;# 61.26989  --&amp;gt; imply v2, v3 are &quot;closer&quot;&#10;ed(v1,v3)  &#10;# 761.4782  --&amp;gt; imply v1, v3 are &quot;far apart&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, &lt;code&gt;v1, v2&lt;/code&gt; and &lt;code&gt;v1, v3&lt;/code&gt; have similar magnitude of distances in Euclidean case, but for arccosine, they are different (66.8 vs. 8.06). Is there anything particular that is revealed by the angle distance but not the Euclidean distance? I think the &lt;strong&gt;orientation&lt;/strong&gt; information is emphasized in the angle distance.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-10-01T18:58:04.417" Id="71614" LastActivityDate="2013-10-02T17:15:55.060" LastEditDate="2013-10-02T16:04:06.833" LastEditorUserId="14156" OwnerUserId="14156" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;distance&gt;" Title="distance measure of angles between two vectors, taking magnitude into account" ViewCount="501" />
  
  
  
  <row Body="&lt;p&gt;Effects &lt;em&gt;should&lt;/em&gt; be different across experiments, and so should variances. That's the nature of sampling. What you have is just different samples being different. There's no way to know which estimate of variance is closer to true value, or even guess at it with the information you've given and equal N's in the samples. So, while you'd like it to be the smaller one, that may not be correct. More than likely the average variance is best.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your general tactic here of searching for an effect may eventually bare fruit. You may combine all of the subjects into one experiment, run a few more, drop an outlier here or there, look for various analysis techniques, and voila, a significant effect. Maybe you won't do all of that but I'm trying to point out that you're thinking about it wrong. Use the data you have to make your best determination about the truth of the matter, not show an effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;An important thing to keep in mind is that an unstated assumption about any statistical test is that you're performing it because you want to know the answer to the test, not because you've previously done other tests and failed to find what you would like to find. So now, because you've already done the test, the rate of Type I error is no longer what you set it to be, alpha. You're increasing the probability of finding an effect whether there is one or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, you could do something that's not a test. You could construct a confidence interval of the effect through a mega-analysis (just combine all of the data) and report that as a higher quality estimate of the effect than either experiment had alone. You will have to concede that what you've done is post hoc and describe the tests that you did do already. But this is probably the best way to report what you've done so far.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-01T23:36:48.007" Id="71637" LastActivityDate="2014-01-05T18:31:24.640" LastEditDate="2014-01-05T18:31:24.640" LastEditorUserId="601" OwnerUserId="601" ParentId="71626" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;I originally answered this question incorrectly, but I will leave my answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The sample variance (with replacement) is defined to be a unbiased estimator of the variance. Therefore $E(s^2)=\sigma^2$. &#10;There are many examples online. Here is a good reference:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://pascencio.cos.ucf.edu/classes/Methods/Proof%20that%20Sample%20Variance%20is%20Unbiased.pdf&quot; rel=&quot;nofollow&quot;&gt;http://pascencio.cos.ucf.edu/classes/Methods/Proof%20that%20Sample%20Variance%20is%20Unbiased.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On your related note, I don't follow your notation. Please see the following reference for a clear proof on the variance of the difference of two random variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sp.uconn.edu/~st100is1/pdf/Ch11les.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.sp.uconn.edu/~st100is1/pdf/Ch11les.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-10-02T01:04:04.643" Id="71642" LastActivityDate="2013-10-04T14:41:42.203" LastEditDate="2013-10-04T14:41:42.203" LastEditorUserId="5053" OwnerUserId="5053" ParentId="71639" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;There won't be any advantage to bootstrapping the SE in your example because you have a very large sample size. The distribution of means of that sample size is going to be normal, not skewed, because of the central limit theorem [CLT] (try &lt;code&gt;hist(skewLeftbootData)&lt;/code&gt;). Even if it were skewed the SE is going to be so small because of N that the SE is not going to be appreciably skewed anyway. Then you're using for proof the backward calculation of an SD based on the SE of the bootstrap distribution calculated through conventional means. Even if the bootstrap distribution were skewed you've just tossed out one of the reasons you might do bootstrap in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bootstrapping would be more compelling if you had substantially smaller sample (say 12) and calculated your SE as the middle 67% of the bootstrapped data by cutoffs of the sorted bootstrap distribution. Then you would see that that is a different estimate than an SE calculated from the conventional SD. You also wouldn't then calculate a bootstrapped SD based on the cut offs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-02T02:00:33.590" Id="71649" LastActivityDate="2013-10-02T04:38:49.910" LastEditDate="2013-10-02T04:38:49.910" LastEditorUserId="601" OwnerUserId="601" ParentId="71638" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="71786" AnswerCount="1" Body="&lt;p&gt;I've read that if two time series, $Y_t$ and $X_t$, are trend stationary, then regressing $Y_t$ on $X_t$ results in a spurious regression &lt;em&gt;because of an omitted time trend variable&lt;/em&gt;. Let $Y_t = \delta_0 + \delta_1t + u_t$ and $X_t = \gamma_0 + \gamma_1t + v_t$. I want to show that $Y_t$ is a linear function of $X_t$, a deterministic time trend and an error term. Can somebody please provide a mathematical proof of this? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T02:05:52.090" FavoriteCount="2" Id="71650" LastActivityDate="2013-10-03T10:59:57.207" OwnerUserId="30192" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;econometrics&gt;" Title="Spurious correlation" ViewCount="150" />
  <row AcceptedAnswerId="71656" AnswerCount="2" Body="&lt;p&gt;I have the following exercise question that I would like insight on:&lt;/p&gt;&#10;&#10;&lt;p&gt;A legal case is being formulated by a union of secretaries who claim that they are being paid unfairly low wages by their employer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The 64 secretaries in the firm have an average annual salary of \$38300 with a SD of \$840.&#10;The average salary for all secretaries in the city in which the firm is located is \$42100. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can you support the claim of the secretaries by statistical arguments? If so, carefully state these arguments and the assumptions underlying them.&lt;/p&gt;&#10;&#10;&lt;p&gt;So would it make sense to make a confidence interval and see if the average salary for all secretaries fall outside the range? Would that work even though this wasn't a random sample?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T03:11:36.900" Id="71652" LastActivityDate="2013-10-02T05:57:52.197" LastEditDate="2013-10-02T03:17:23.883" LastEditorUserId="805" OwnerUserId="19848" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;sampling&gt;" Title="Generalizing results from a sample" ViewCount="88" />
  <row Body="&lt;p&gt;It's a bit of an odd kind of homework question. I hate it when they have no relation to real life. In a real lawsuit you'd just claim that all, or x% of these secretaries are below average. That's sufficient. Some claim about statistical significance is pointless.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, you could generate a CI around the sample. It looks like they're assuming the 41.2 is a parameter, not a sample estimate, and therefore itself has a CI of 0. So, you just need to check if the city value is outside the sample CI.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T05:57:52.197" Id="71662" LastActivityDate="2013-10-02T05:57:52.197" OwnerUserId="601" ParentId="71652" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The basic idea sounds OK to me, but you would need to specify what kind of uncertainty you try to quantify with your test. In the usual case of statistical testing the uncertainty is supposed to be due to random sampling from a population. In your case you could be trying to quantify the randomness that belongs to a Monte Carlo experiment. This is a good idea, but it is non-standard and thus needs to be discussed carefully. Also note that the distribution of $p$-values can legitimately deviate from a standard continuous uniform distribution even if the null hypothesis is true, e.g. in a one-sided test: &lt;a href=&quot;http://stats.stackexchange.com/questions/63786&quot;&gt;Is there a sample distribution so that the distribution of p-value is skewed towards 1?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To give a very meta answer: the K-S test does not perform too well, as you can see by looking at the distribution of $p$-values as you can see in the following simulation using Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set seed 12345&#10;clear all&#10;set more off&#10;&#10;program define sim&#10;    drop _all&#10;    set obs 100&#10;    gen x = runiform()&#10;    ksmirnov x = x&#10;end&#10;simulate p=r(p) p_cor=r(p_cor), reps(20000) : sim&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A K-S test on the resulting $p$-values results in the conclusion that the null hypothesis that the distribution of $p$-values is uniformly distributed is rejected at the 5% level:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. ksmirnov p = p&#10;&#10;One-sample Kolmogorov-Smirnov test against theoretical distribution&#10;           p&#10;&#10; Smaller group       D       P-value  Corrected&#10; ----------------------------------------------&#10; p:                  0.0001    0.999&#10; Cumulative:        -0.0221    0.000&#10; Combined K-S:       0.0221    0.000      0.000&#10;&#10;Note: ties exist in dataset;&#10;      there are 19982 unique values out of 20000 observations.&#10;&#10;. ksmirnov p_cor = p_cor&#10;&#10;One-sample Kolmogorov-Smirnov test against theoretical distribution&#10;           p_cor&#10;&#10; Smaller group       D       P-value  Corrected&#10; ----------------------------------------------&#10; p_cor:              0.0315    0.000&#10; Cumulative:        -0.0010    0.961&#10; Combined K-S:       0.0315    0.000      0.000&#10;&#10;Note: ties exist in dataset;&#10;      there are 19986 unique values out of 20000 observations.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For displaying the results graphically I like this graph: it shows on the y-axis the difference between the empirical estimate of the Cumulative Distribution Function (CDF) and the theoretical (continuous standard uniform) distribution. On the x-axis is the nominal p-value. The logic behind this graph is that for $p$-values in a simulation study in which the null hypothesis is true, the empirical CDF is an empirical estimate of the $p$-value. The empirical CDF gives for each nominal $p$-value an estimate of the probability of drawing a sample which deviates at least as much from the null hypothesis as the current sample (i.e. has a nominal $p$-value less than or equal to the current nominal $p$-value) if the null hypothesis is true. So negative values on the y-axis means that the emprical estimates of the $p$-value are less than the nominal $p$-values and positive values on the y-axis say that the empirical estimates of the $p$-values are larger than the nominal $p$-values.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;label var p `&quot;&quot;standard&quot; &quot;p-value&quot;&quot;'&#10;label var p_cor `&quot;&quot;corrected&quot; &quot;p-value&quot;&quot;'&#10;&#10;simpplot p p_cor, overall reps(20000) ///&#10;    scheme(s2color) ylab(,angle(horizontal))   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4kJDQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T12:04:04.137" Id="71685" LastActivityDate="2013-10-02T12:24:05.360" LastEditDate="2013-10-02T12:24:05.360" LastEditorUserId="23853" OwnerUserId="23853" ParentId="71676" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;In your problem, linear model with small data set, it seems that they are quite similar, and statistical linear model may be more suitable than data mining.&lt;/p&gt;&#10;&#10;&lt;p&gt;Their particular emphases are slightly different. Generally speaking, statistics is more based on probability, distribution and hypothesis tests. To get some theoretical proof results, statistics has more assumptions and tends to a simpler model form. On the other hand, data mining is more focus on optimizing the result performance of data set. Not emphasizing the mathematical assumptions and proof too much, it tends to use complex structure and model averaging. Usually, no model can guarantee always more suitable (or say outperforming) than the other models in data mining consistently by mathematical proof (before try to apply to the data set). Different models and different parameters will be tried to the data set and compared by performance (like cross-validation). So we can see lots of hypothesis test to validate the statistical models while data mining will usually only compare about the testing error (prediction performance).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the linear model, we can see there are several classic assumptions. And we use hypothesis and residuals plot etc to validate the model, which is suitable for small data. For big data, it is difficult for us to read in a mess plot and some hypothesis are not convincing. For example, Shapiro-Wilk Normality Test will nearly always reject Normality because that statistic is function of N.&lt;/p&gt;&#10;&#10;&lt;p&gt;With Big data and computational machine learning, not only more variables but also more complex structure even the same variables can we use, like splines, penalized regression, etc. Moreover, we can use bootstrapping to get a larger sample, and do model averaging. In these cases, the classic statistic hypothesis test or model selection like AIC/BIC are not valid, because we can not even strictly find the likelihood or parameters based on different kinds of models. So data mining is more focus on how to tune the model to get a good performance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T14:25:32.500" Id="71695" LastActivityDate="2013-10-02T14:25:32.500" OwnerUserId="29187" ParentId="71674" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I will mainly focus on your first three questions. The short answers are: (1) you need to compare the effect of the IV on the DV for each time period but (2) only comparing the magnitudes can lead to wrong conclusions, and (3) there are many ways of doing that but no consensus on which one is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below I describe why you cannot simply compare coefficient magnitudes and point you to some solutions that have been thought of so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to Allison (1999), unlike OLS, logistic regression coefficients are affected by unobserved heterogeneity even when such heterogeneity is not related to the variable of interest.&lt;/p&gt;&#10;&#10;&lt;p&gt;When you fit a logistic regression like:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1)$$&#10;\ln\bigg(\frac{1}{1-p_i}\bigg) = \beta_{0} + \beta_{1}x_{1i}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You are in fact fitting an equation predicting the value of a latent variable $y^*$ that represents the underlying propensity of each observation to assume the value $1$ in the binary dependent variable, what happens if $y^*$ is above a certain threshold. The equation for that is (Williams, 2009):&lt;/p&gt;&#10;&#10;&lt;p&gt;(2)$$&#10;y^* =\alpha_{0} + \alpha_{1}x_{1i} + \sigma \varepsilon&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The term $\varepsilon$ is assumed to be independent from the other terms and to follow a logistic distribution – or a normal distribution in the case of probit and a log-logistic distribution in case of complementary log-log and a cauchy distribution in the case of cauchit.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to Williams (2009), the $\alpha$ coefficients in equation 2 are related to the $\beta$ coefficients in equation 1 through:&lt;/p&gt;&#10;&#10;&lt;p&gt;(3)$$&#10;\beta_{j} = \frac{\alpha_{j}}{\sigma}\;\;j=1,...,J.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In equations 2 and 3, $\sigma$ is the scaling factor of the unobserved variation, and we can see that the size of the estimated $\beta$ coefficients depends on $\sigma$, which is not observed. Based on that, Allison (1999), Williams (2009), and Mood (2009), among others, claim that you cannot naively compare coefficients between logistic models estimated for different groups, countries or periods. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is because comparisons may yield incorrect conclusions if the unobserved variation differs between groups, countries or periods. Both comparisons using different models and using interaction terms within the same model suffer from this problem. Besides logit, this also applies to its cousins probit, clog-log, cauchit and, by extension, to discrete time hazard models estimated using these link functions. Ordered logit models are also affected by it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Williams (2009) argues that the solution is to model the unobserved variation through a heterogeneous choice model (a.k.a., a location-scale model), and provides a Stata add on called &lt;code&gt;oglm&lt;/code&gt;  for that (Williams 2010). In R, heterogeneous choice models can be fit with the &lt;code&gt;hetprob()&lt;/code&gt; function of the &lt;code&gt;glmx&lt;/code&gt; package, which is available through RForge, but not through CRAN. Both programs are very easy to use. Lastly, Williams (2009) mentions SPSS's &lt;code&gt;PLUM&lt;/code&gt; routine for fitting these models, but I have never used it and cannot comment in how easy it is to use.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there is at least &lt;a href=&quot;http://www.nuffield.ox.ac.uk/politics/papers/2005/Keele%20Park%20HetChoice%20050315.pdf&quot; rel=&quot;nofollow&quot;&gt;one working paper&lt;/a&gt; out there showing that comparisons using heterogeneous choice models can be even more biased if the variance equation is misspecified or there is measurement error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mood (2010) lists other solutions that do not involve modelling the variance, but use comparisons of predicted probability changes. &lt;/p&gt;&#10;&#10;&lt;p&gt;Apparently it is an issue that is not settled and I often see papers in conferences of my field (Sociology) coming up with different solutions for it. I would advise you to look at what people in your field do and then decide how to deal with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Allison, P. D. (1999). &lt;a href=&quot;http://smr.sagepub.com/content/28/2/186.abstract&quot; rel=&quot;nofollow&quot;&gt;Comparing Logit and Probit Coefficients Across Groups.&lt;/a&gt; Sociological Methods &amp;amp; Research, 28(2), 186–208.&lt;/li&gt;&#10;&lt;li&gt;Mood, C. (2010). &lt;a href=&quot;http://esr.oxfordjournals.org/content/26/1/67&quot; rel=&quot;nofollow&quot;&gt;Logistic Regression: Why We Cannot Do What We Think We Can Do, and What We Can Do About It.&lt;/a&gt; European Sociological Review, 26(1), 67–82.&lt;/li&gt;&#10;&lt;li&gt;Williams, R. (2009). &lt;a href=&quot;http://smr.sagepub.com/content/37/4/531.abstract&quot; rel=&quot;nofollow&quot;&gt;Using Heterogeneous Choice Models to Compare Logit and Probit Coefficients Across Groups.&lt;/a&gt; Sociological Methods &amp;amp; Research, 37(4), 531–559.&lt;/li&gt;&#10;&lt;li&gt;Williams, R. (2010). &lt;a href=&quot;http://www.stata-journal.com/article.html?article=st0208&quot; rel=&quot;nofollow&quot;&gt;Fitting heterogeneous choice models with oglm.&lt;/a&gt; The Stata Journal, 10(4), 540–567.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T14:33:15.807" Id="71696" LastActivityDate="2013-10-03T13:44:26.573" LastEditDate="2013-10-03T13:44:26.573" LastEditorUserId="29707" OwnerUserId="29707" ParentId="8718" PostTypeId="2" Score="6" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to estimate a specific point in a bimodal distributions, It's the point that I indicated in my picture with the A.&#10;&lt;img src=&quot;http://i.stack.imgur.com/YQhXE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As example we can generate a mixture of guassian and then try to determinate the point A(x,f(x)).&lt;/p&gt;&#10;&#10;&lt;p&gt;This problem is I think a problem of optimization and estimation. My question is, though, how would I go about that in detail? Maybe I'm just missing a good tutorial somehow.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-02T21:08:42.127" FavoriteCount="0" Id="71735" LastActivityDate="2013-10-02T21:08:42.127" OwnerUserId="29386" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;distributions&gt;&lt;estimation&gt;&lt;optimization&gt;&lt;density&gt;" Title="Estimating of point in the density of bimodal distribution" ViewCount="135" />
  <row Body="&lt;p&gt;You can solve your second equation to give $\pi(1)=\frac{a}{1-p} \pi(0)$, then your third to give $\pi(2)=\frac{ap}{(1-p)^2} \pi(0)$, then your first to give $\pi(x)=\frac{a}{p}\left(\frac{p}{1-p}\right)^x \pi(0)$ for $0 \lt x \lt n$, and finally your fourth to give $\pi(n)=\frac{p}{b} \pi(n-1) = \frac{a}{b}\left(\frac{p}{1-p}\right)^{n-1} \pi(0)$. Your fifth equation is not independent of the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can now add up the terms, noting the geometric progression in the middle, and set the sum equal to $1$ to solve for $\pi(0)$ and thus find all the values of $\pi(x)$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-02T21:14:43.973" Id="71736" LastActivityDate="2013-10-02T21:14:43.973" OwnerUserId="2958" ParentId="71726" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here is a simple approach. Generate $X \sim \mathrm{Unif} [0,{2 \over 3}].$ Let $$Y = \begin{cases} X+{1/3} \ , &amp;amp; \text{if} \ X \le {1/3} \\ X-{1/3} \ , &amp;amp; \text{if} \ {1/3} \lt X \le {2/3}  \end{cases}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Z = 1 - X - Y.$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Then it's not too hard to show that $X,Y,$ and $Z$ are identically distributed with $\mathrm{Unif} [0,{2 \over 3}]$ distributions. So each has mean and median of ${1 \over 3}$ and has a mode there as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, all pairwise correlations are $ -{1 \over   2},$ and only one call to a uniform random generator is needed to get the three variates.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-02T22:36:46.397" Id="71745" LastActivityDate="2013-10-02T22:36:46.397" OwnerUserId="24073" ParentId="59096" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;In Excel for a project I'm trying to model the density, distribution and survival function ($1-F(X)$) and I can't get the density to sum to one and I can't get the distribution to go to one. For the parameters I'm using $\alpha=3$ and $\theta=10$. I'm getting extremely strange results such that halfway through the distribution it levels of at $.36366$ until the end. The density function is summing to $1.15...$. After checking the formula that I input, I know it's right, but is there something I should know about modeling this in excel that maybe the program is picky about? I'm not sure if this is enough information, if not comment on what you need to know and I'll get back to you.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-10-02T23:20:53.033" Id="71752" LastActivityDate="2013-10-03T01:47:26.723" OwnerUserId="16557" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;excel&gt;&lt;pareto-distribution&gt;" Title="Why am I getting this result in modeling a Pareto Type II distribution in Excel?" ViewCount="151" />
  
  
  
  
  <row Body="&lt;p&gt;The z-table contains the area to the left of the z-number under a normal distribution with $\mu = 0$ and $\sigma = 1$. Your p-value will generally represent one minus that area (for a one tailed test). Transforming this p-value into it's corresponding z-score should not be that hard under these circumstances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your normal distribution with $\mu = 0$ and $\sigma = 1$ is defined by the function&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Your p-value is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p = \int_z^\infty{\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;All you have to do is plug in your numbers and solve for $z$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note how the integral starts at $z$ and not at $-\infty$ because we're calculating $p$ which is one minus the area to the left of $z$ (This is true for one-tailed tests. For two-tailed tests divide $p$ by two before plugging it in the above formula). &lt;/p&gt;&#10;&#10;&lt;p&gt;Solving for $z$ yields:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{1}{2} \text{erfc}\left(\frac{z}{\sqrt{2}}\right) = p&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\text{erfc}\left(\frac{z}{\sqrt{2}}\right) = 2p&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{z}{\sqrt{2}} = \text{erfc}^{-1}(2p)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;z = \sqrt{2} \cdot \text{erfc}^{-1}(2p)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Don't know much java but after reading these &lt;a href=&quot;http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/special/Erf.html&quot; rel=&quot;nofollow&quot;&gt;docs&lt;/a&gt; I suspect the code would be as easy as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;double pToZ(double p) {&#10;    double z = Math.sqrt(2) * Erf.erfcInv(2*p);&#10;    return(z);&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note $\text{erfc()}$ stands for $1 - \text{erf()}$ where $\text{erf}$ is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Error_function&quot; rel=&quot;nofollow&quot;&gt;error function&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-03T12:49:20.553" Id="71792" LastActivityDate="2013-10-03T15:00:49.560" LastEditDate="2013-10-03T15:00:49.560" LastEditorUserId="27435" OwnerUserId="27435" ParentId="71788" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I'm not sure whether there's a better way but perhaps you can run one-way anovas on factor 1 for each category group of factor 2 separately? &lt;/p&gt;&#10;&#10;&lt;p&gt;You would use &lt;code&gt;split files ...&lt;/code&gt; for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Try it and see whether that duplicates Andy's results.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-03T15:15:38.927" Id="71808" LastActivityDate="2013-10-03T15:15:38.927" OwnerUserId="17197" ParentId="71800" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a large dataset I am trying to do cluster analysis using SOM. The dataset is huge (~ billions of records) and I am not sure what the number of neurons should be or the SOM grid size to start with. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers to some material that talks about estimating the number of neurons and grid size would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-03T15:40:16.843" FavoriteCount="1" Id="71812" LastActivityDate="2013-12-29T01:17:48.367" LastEditDate="2013-10-03T16:18:06.123" LastEditorUserId="7290" OwnerUserId="31037" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;references&gt;&lt;neural-networks&gt;&lt;self-organizing-maps&gt;" Title="Kohonen self organizing maps: determining the number of neurons and grid size" ViewCount="322" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have applied &lt;code&gt;libsvm&lt;/code&gt; with a linear kernel to a set of instances and I have obtained a 68 % success:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;instance1 : f11, f12, f13, f14&#10;instance2 : f21, f22, f23, f24&#10;instance3 : f31, f32, f33, f34&#10;instance4 : f41, f42, f43, f44&#10;..............................&#10;instanceN : fN1, fN2, fN3, fN4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Taking the same set of instances but multiplying each value (&lt;code&gt;f11*1000 ... fN4*1000&lt;/code&gt;) I have obtained a 90% of success.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I the multiplied by 1000 is normalized the percentatge of success turn to 68%.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems something related with normalization but I don't know which it is the reason.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-03T16:23:27.357" Id="71822" LastActivityDate="2013-12-04T19:55:19.057" LastEditDate="2013-10-03T22:42:13.633" LastEditorUserId="21599" OwnerUserId="31036" PostTypeId="1" Score="1" Tags="&lt;svm&gt;&lt;normalization&gt;&lt;libsvm&gt;&lt;weka&gt;" Title="Normalization in SVM" ViewCount="134" />
  <row Body="&lt;p&gt;Kohonen's &lt;em&gt;Self Organizing Maps&lt;/em&gt; (1995) says that the SOM is an approximation of some density function, p(x) and the dimensions for the array should correspond to this distribution.  &quot;&lt;em&gt;Therefore visual inspection of the rough form of p(x), e.g. by &lt;a href=&quot;https://en.wikipedia.org/wiki/Sammon_mapping&quot; rel=&quot;nofollow&quot;&gt;Sammon's mapping&lt;/a&gt; ought to be done first.&lt;/em&gt;&quot;  p.112&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-03T19:05:21.017" Id="71841" LastActivityDate="2013-12-10T04:40:46.530" LastEditDate="2013-12-10T04:40:46.530" LastEditorUserId="9007" OwnerUserId="6301" ParentId="71812" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;The Bayesian information criterion is defined as $BIC = -2 \text{ln}(L) + k\text{ln}(n)$, where $L$ is the maximized likelihood of the data, and where $n$ is the sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case of a huge sample size, BIC tend to $\infty$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any transformation that needs to be done in order to compute the BIC for large samples?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-03T20:14:37.743" Id="71847" LastActivityDate="2013-10-03T21:15:59.870" OwnerUserId="29183" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;bayesian&gt;&lt;econometrics&gt;&lt;bic&gt;" Title="Bayesian Information Criterion (BIC) for large samples" ViewCount="281" />
  <row Body="&lt;p&gt;I was on the same hunt before and I think &lt;a href=&quot;http://vizsage.com/other/leastsquaresexcel/&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; may be a useful place to start. The excel macro function gives linear fit terms and their uncertainties based on tabular points and uncertainty for each point in both ordinates. Maybe look up the paper it is based on to decide if you want to implement it in a different environment, modify, etc. (There is some legwork done for Mathematica.) It seems to have good walk-through documentation on the surface but haven't opened up the macro to see how well annotated it is.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-03T21:44:51.157" Id="71852" LastActivityDate="2014-03-06T14:14:59.360" LastEditDate="2014-03-06T14:14:59.360" LastEditorUserId="7290" OwnerUserId="31049" ParentId="70629" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose that we have an $M/GI/\infty$ queue, that is, we have infinitely many servers, a Poisson arrival process with rate $\lambda$ (i.e., random arrival times $0=t_0 &amp;lt; t_1 &amp;lt; t_2 &amp;lt; \dots &amp;lt; t_n &amp;lt; \dots$, with inter-arrival times $\theta_n = t_n - t_{n-1}$ being independent exponentially distributed with mean $1/\lambda$ ), and i.i.d delays (independent of arrivals) with finite mean $\tau$. We know (e.g., from the book of Takacs[1962]) that there exists a stationary distribution such that the size $q(t)$ of queue  at time $t$ (in the steady-state) follows a Poisson distribution with mean $\lambda \alpha$, that is,&lt;/p&gt;&#10;&#10;&lt;p&gt;$ P(q(t) = k ) = \dfrac{e^{-\lambda \tau}(\lambda \tau)^k}{k!}, \qquad $ for all $t \ge 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose that we want the distribution of the size of queue at the time of the $n$ arrival, assuming that it is in the steady state. That is, we want $p_k = P(q_n = k)$, where $q_n = \lim_{t \to t_n^-} q(t)$. How can we obtain $p_k$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-04T04:19:29.040" Id="71881" LastActivityDate="2013-10-04T21:28:52.040" LastEditDate="2013-10-04T21:28:52.040" LastEditorUserId="-1" OwnerUserId="31059" PostTypeId="1" Score="1" Tags="&lt;poisson-process&gt;&lt;queueing&gt;" Title="M/GI/inf queue in stationary distribution, how to get queue size distribution at the arrival times?" ViewCount="60" />
  <row Body="&lt;p&gt;Such type of question is not really suitable for this site, since there can be no one correct answer to such a question. Here are my observations.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Your response variable is censored, so linear model may be not the best model, since clearly it will not produce the exact zeroes. I would look into some sort of censored regression, like &lt;a href=&quot;http://en.wikipedia.org/wiki/Tobit_model&quot; rel=&quot;nofollow&quot;&gt;tobit regression&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Since you want a regression model, forecasting the future values of the response variable involves forecasting the future values of the predictor variables. So when evaluating forecasting performance you should use the forecasts of predictor variables, not their out-of-sample values to get more reliable forecasting performance estimate. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2013-10-04T08:00:21.127" Id="71893" LastActivityDate="2013-10-04T08:00:21.127" OwnerUserId="2116" ParentId="71802" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have 4 treatment groups:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 control (placebo)&#10;1 with X treatment&#10;1 with Y treatment &#10;1 combination-treatment-group XY&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and I have monitored the individuals (n=10) in each group by measurement of their tumor volume at baseline, day 1, 3, 7, 10 and 14 to see what treatment is best and how early effect can be seen. What statistics should I use? &lt;/p&gt;&#10;&#10;&lt;p&gt;The repetitive measurement at day 1, 3, 7, 10 and 14 is some sort of paired comparisons I guess and the comparisons between groups are unpaired. Since there are more than two groups I guess it is ANOVA I should use but will I then have to Bonferroni correct for the fact that I have &quot;looked for the same difference&quot; 5 times?--even though I would expect the difference to show a trend--I would expect the difference between an effective drug and placebo to become greater for each treatment day. I would also like to test if the combination treatment is better than single X and single Y on its own. Since I only have 10 individuals in each group I can not &quot;effort&quot; much Bonferroni correction.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have done a lot of unpaired and paired t-tests but that might not be correct without any post hoc testing. I am a bit confused about the fact that I lose power because I test more things at once or compare the differences at more than one day. Will it be fair to leave all p-values uncorrected (no Bonferroni) and state that it is a hypothesis generating study testing when a predefined difference can be seen?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-04T15:03:36.147" FavoriteCount="2" Id="71917" LastActivityDate="2014-11-02T17:07:10.913" LastEditDate="2013-10-04T19:12:37.737" LastEditorUserId="930" OwnerUserId="31079" PostTypeId="1" Score="3" Tags="&lt;repeated-measures&gt;&lt;bonferroni&gt;" Title="How to compare several small groups with follow-up data?" ViewCount="74" />
  <row Body="&lt;p&gt;Robust PCA is a very active research area, and identifying and removing outliers in a sound way is quite delicate. (I've written two papers in this field, so I do know a bit about it.) While I don't know SPSS, you may be able to implement the relatively simple Algorithm (1) &lt;a href=&quot;http://users.ece.utexas.edu/~cmcaram/pubs/HRPCA_Journal.final.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This algorithm (not mine) has rigorous guarantees but requires only some basic computations and a &quot;while&quot; loop. Assuming you are searching for $d$ principal components, the basic procedure is&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Compute PCA on your data,&lt;/li&gt;&#10;&lt;li&gt;Project your data on to the top $d$ principal components,&lt;/li&gt;&#10;&lt;li&gt;Throw away &quot;at random&quot; one of the data points whose projection is &quot;too large&quot;, and&lt;/li&gt;&#10;&lt;li&gt;Repeat this &quot;a few&quot; times.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Everything in quotation marks is a heuristic; you can find the details in the paper.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The idea behind this procedure is that vectors whose projection after PCA is large &lt;em&gt;may&lt;/em&gt; have effected the estimate too much, and so you &lt;em&gt;may&lt;/em&gt; want to throw them away.  It turns out that choosing the ones to throw away &quot;at random&quot; is actually a reasonable thing to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone actually wants to take the time to write the SPSS code for this, I'm sure @cathy would appreciate it.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-04T19:30:28.923" Id="71928" LastActivityDate="2013-10-04T19:30:28.923" OwnerUserId="30577" ParentId="71899" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="71938" AnswerCount="1" Body="&lt;p&gt;I am running an experiment looking at brain volume changes in a rare disorder. We have a small number of patients (n = 8) but a large control group (n = 100). Some colleagues have suggested that a balanced group should be used, ie. selecting 8 control brains, but I find this counterintuitive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Doesn't it make more sense to use the entire control group to obtain a better estimate of population brain volume?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-04T19:57:56.043" Id="71933" LastActivityDate="2014-01-25T00:08:04.770" OwnerUserId="3855" PostTypeId="1" Score="4" Tags="&lt;sample-size&gt;&lt;unbalanced-classes&gt;" Title="Is a large control sample better than a balanced sample size when the treatment group is small?" ViewCount="577" />
  
  <row AcceptedAnswerId="71968" AnswerCount="1" Body="&lt;p&gt;Consider three samples, i.e. three different lists of numbers, A, B and C. The sample sizes of the respective samples might be different. How can I calculate the probability that a draw from sample A exceeds a draw from sample B given that a draw from sample A exceeds sample C?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for an elegant an resource saving method as proposed &lt;a href=&quot;http://stats.stackexchange.com/a/71611/17818&quot;&gt;here&lt;/a&gt;. With the reply to this question, which was also posted by myself, I hoped to figure out the rest by myself, but I was wrong in that. Any helpful comments are appreciated!    &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-05T00:37:06.103" Id="71956" LastActivityDate="2013-10-07T21:03:16.280" LastEditDate="2013-10-05T11:49:39.470" LastEditorUserId="88" OwnerUserId="17818" PostTypeId="1" Score="3" Tags="&lt;conditional-probability&gt;&lt;sample&gt;&lt;ranking&gt;&lt;combinatorics&gt;" Title="Rank-sum test and conditional probability" ViewCount="105" />
  <row AnswerCount="1" Body="&lt;p&gt;This question has no practical importance for me, but anyway: I am interested whether we can predict somehow one missing value in a correlation matrix. Obviously, classical regression modeling is not appropriate here. Is there any other way, how we can say something about missing correlation? In addition, does the answer depend on the type of correlation (Pearson, Spearman, Kendall, MIC)? Here is an example (real data, Pearson correlation).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6I7Cu.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;matrix &amp;lt;- structure(c(1, NA, 0.126, -0.163, 0.203, 0.172, -0.101, 0.068, &#10;-0.125, 0.066, 0.195, NA, 1, 0.019, -0.169, 0.211, 0.265, -0.023, &#10;0.088, 0.016, 0.14, 0.275, 0.126, 0.019, 1, 0.218, 0.094, -0.053, &#10;0.189, 0.179, -0.07, -0.021, 0.172, -0.163, -0.169, 0.218, 1, &#10;-0.066, -0.182, 0.302, 0.193, -0.116, -0.08, -0.151, 0.203, 0.211, &#10;0.094, -0.066, 1, 0.251, 0.064, 0.122, 0.093, 0.214, 0.378, 0.172, &#10;0.265, -0.053, -0.182, 0.251, 1, -0.101, 0.02, 0.192, 0.353, &#10;0.304, -0.101, -0.023, 0.189, 0.302, 0.064, -0.101, 1, 0.652, &#10;0.126, 0.023, -0.016, 0.068, 0.088, 0.179, 0.193, 0.122, 0.02, &#10;0.652, 1, 0.127, 0.01, 0.083, -0.125, 0.016, -0.07, -0.116, 0.093, &#10;0.192, 0.126, 0.127, 1, 0.295, 0.023, 0.066, 0.14, -0.021, -0.08, &#10;0.214, 0.353, 0.023, 0.01, 0.295, 1, 0.299, 0.195, 0.275, 0.172, &#10;-0.151, 0.378, 0.304, -0.016, 0.083, 0.023, 0.299, 1), .Dim = c(11L, &#10;11L), .Dimnames = list(NULL, NULL))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2013-10-05T09:22:53.023" Id="71970" LastActivityDate="2013-10-05T10:13:52.380" LastEditDate="2013-10-05T09:29:05.400" LastEditorUserId="3277" OwnerUserId="14730" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;predictive-models&gt;&lt;prediction&gt;&lt;matrix&gt;" Title="Can we estimate value of missing correlation in a given correlation matrix?" ViewCount="112" />
  <row Body="&lt;p&gt;The principle is that you want to impute no special relation between the two variables other than that which is implied by their known relations to the other variables. This implies that you should plug in the value that zeros the partial correlation of the two variables with the $p-2$ other variables held constant. This is the same as maximizing the determinant of the new matrix; the new inverse will have zeros in the corresponding positions. (This works for any symmetric positive definite matrix, not just a Pearson correlation matrix.) In the sample matrix the imputed value is .119695.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-05T10:13:52.380" Id="71971" LastActivityDate="2013-10-05T10:13:52.380" OwnerUserId="20776" ParentId="71970" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'm guessing its the same as &quot;weakly stationary&quot;. That means that $(x_k,\dots,x_{k-l})$ all (for all $k$, and any $l)$ have the same expectation and covariance matrix but not necessarily the same distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-05T11:18:57.117" Id="71975" LastActivityDate="2013-10-05T11:50:44.047" LastEditDate="2013-10-05T11:50:44.047" LastEditorUserId="930" OwnerUserId="31106" ParentId="65353" PostTypeId="2" Score="-1" />
  <row AcceptedAnswerId="72031" AnswerCount="1" Body="&lt;p&gt;How do I take the partial derivative of bivariate normal cdf and bivariate normal pdf with its arguments (i.e. $x_{1}$ ,$x_{2}$ , and $\rho$  in the following equations)?&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;y=\Phi(x_{1},x_{2},\rho)&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;z=\phi(x_{1},x_{2},\rho)&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x_{1}$ is normally distributed with mean 0 and variance 1 and $x_{2}$ is normally distributed with mean 0 and variance 1. $\rho$ is the correlation between $x_{1}$ and $x_{2}$. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-05T12:00:46.770" Id="71976" LastActivityDate="2013-10-06T02:13:29.733" OwnerUserId="29825" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;" Title="Partial derivative of bivariate normal cdf and pdf" ViewCount="757" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am working with two highly skewed Bernoulli distributions where 96-99+% of the samples are in the &quot;false&quot; category, and the rest are in the &quot;true&quot; category (sort of speak). I am looking for a two-sided test of difference of proportions between the two samples. I can often achieve 500+ &quot;trues&quot; and tens or hundreds of thousands of &quot;falses&quot; in a reasonable time but I'm not sure if approximation to the normal distribution can withstand this extreme skewness.&lt;/p&gt;&#10;&#10;&lt;p&gt;I initially thought I might need something non-parametric, but here, I actually know the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been using a student's t-test, while paying attention to sample size estimation, but past experience has led me to be skeptical of its results. Thanks for your help.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-02T21:14:55.710" Id="72002" LastActivityDate="2013-10-12T22:59:22.727" LastEditDate="2013-10-12T22:59:22.727" LastEditorUserId="805" OwnerDisplayName="Ben" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;normal-distribution&gt;&lt;bernoulli-distribution&gt;" Title="Significance test for highly skewed Bernoulli distribution" ViewCount="189" />
  
  <row Body="&lt;p&gt;It depends on many factors. First - are you taking the bet &lt;strong&gt;once&lt;/strong&gt; or as long as you want?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;If you take it just once then the answer is &lt;strong&gt;no&lt;/strong&gt;, as there is much higher probability of losing money then winning(even though the expected value is positive)&lt;/li&gt;&#10;&lt;li&gt;If you can take this bet for as long as you want, it is a good  bet, as with expected value of income equal to 2, we could expect that if we play long enough - we will gain more and more money&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In general the problem here is weighting the expected outcome (expected value) and the risk (variance) of the process. In &quot;real life&quot;, where all resources are limited, the risk factor may be much bigger then the one coming from expected outcome (which is just a theoretical entity). As such there is no good answer for &quot;real world&quot; bet, only for theoretical betting games, where there exists infinity &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-06T06:39:16.417" Id="72038" LastActivityDate="2013-10-07T06:33:11.420" LastEditDate="2013-10-07T06:33:11.420" LastEditorUserId="28903" OwnerUserId="28903" ParentId="72037" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The term balanced usually refers to having a fairly equal representation of data classes in each sample. Imbalance refers to one or more classes having very low relative frequencies compared to the other classes. In the context of cross-validation, one ideally would like a balanced representation of classes in each 'fold' of data.  Because of sparse class examples in the training data, a model might be biased towards predicting the majority class most of the time.  Ways to deal with this problem range from changing the probability thresholds (and hence specificity and sensitivity) to focus more on the underrepresented class, attempting to re-balance test data via selective or up/down sampling, stratification of folds, and re-weighting to focus more on classes with errors (as in boosting) .&lt;/p&gt;&#10;&#10;&lt;p&gt;There was a related prior question &lt;a href=&quot;http://stats.stackexchange.com/questions/42999/training-approaches-for-highly-imbalanced-data-set&quot;&gt;here&lt;/a&gt; that included a link to a related paper, &lt;a href=&quot;http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf&quot; rel=&quot;nofollow&quot;&gt;Haibo He, Edwardo A. Garcia, &quot;Learning from Imbalanced Data,&quot; IEEE Transactions on Knowledge and Data Engineering, pp. 1263-1284, September, 2009&lt;/a&gt;. Another related paper is, &lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=6&amp;amp;ved=0CFIQFjAF&amp;amp;url=http://www.cse.chalmers.se/~feldt/publications/afzal_2012_resampling_methods.pdf&amp;amp;ei=-CFRUryzHKOUiQKKioGwAQ&amp;amp;usg=AFQjCNHRMKEXPiOMFkdLSyeTi8XLdbWxvw&amp;amp;bvm=bv.53537100,d.cGE&quot; rel=&quot;nofollow&quot;&gt;Resampling Methods in Software Quality Classification, Wasif Afzal, et al, May, 2012&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-06T08:37:38.743" Id="72044" LastActivityDate="2013-10-06T08:56:56.060" LastEditDate="2013-10-06T08:56:56.060" LastEditorUserId="13519" OwnerUserId="13519" ParentId="72024" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="72222" AnswerCount="1" Body="&lt;p&gt;In the following code I perform a logistic regression on grouped data using glm and &quot;by hand&quot; using mle2. Why does the logLik function in R give me a log likelihood logLik(fit.glm)=-2.336 that is different than the one logLik(fit.ml)=-5.514 I get by hand? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(bbmle)&#10;&#10;#successes in first column, failures in second&#10;Y &amp;lt;- matrix(c(1,2,4,3,2,0),3,2)&#10;&#10;#predictor&#10;X &amp;lt;- c(0,1,2)&#10;&#10;#use glm&#10;fit.glm &amp;lt;- glm(Y ~ X,family=binomial (link=logit))&#10;summary(fit.glm)&#10;&#10;#use mle2&#10;invlogit &amp;lt;- function(x) { exp(x) / (1+exp(x))}&#10;nloglike &amp;lt;- function(a,b) {&#10;  L &amp;lt;- 0&#10;  for (i in 1:n){&#10;     L &amp;lt;- L + sum(y[i,1]*log(invlogit(a+b*x[i])) + &#10;               y[i,2]*log(1-invlogit(a+b*x[i])))&#10;  }&#10; return(-L) &#10;}  &#10;&#10;fit.ml &amp;lt;- mle2(nloglike,&#10;           start=list(&#10;             a=-1.5,&#10;             b=2),&#10;           data=list(&#10;             x=X,&#10;             y=Y,&#10;             n=length(X)),&#10;           method=&quot;Nelder-Mead&quot;,&#10;           skip.hessian=FALSE)&#10;summary(fit.ml)&#10;&#10;#log likelihoods&#10;logLik(fit.glm)&#10;logLik(fit.ml)&#10;&#10;&#10;y &amp;lt;- Y&#10;x &amp;lt;- X&#10;n &amp;lt;- length(x)&#10;nloglike(coef(fit.glm)[1],coef(fit.glm)[2])&#10;nloglike(coef(fit.ml)[1],coef(fit.ml)[2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2013-10-06T13:07:31.790" Id="72060" LastActivityDate="2013-10-08T10:28:53.007" OwnerUserId="17124" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;self-study&gt;&lt;generalized-linear-model&gt;" Title="Log Likelihood for GLM" ViewCount="821" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $X$ &amp;amp; $Y$ be random vectors. Let $Z$=$f(X,Y)$ be a random variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then if $X$ &amp;amp; $Y$ are independent, the following is a well known result:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B)$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;I need some help to 'formalize' what appears to be an intuitive generalization when $X$ &amp;amp; $Y$ are not assumed to be independent, namely that&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B\mid Y=y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;without assuming $Y$ is discrete. &lt;/p&gt;&#10;&#10;&lt;p&gt;One 'immediate' problem for example is that, I am not sure that the RHS of the above equation is well defined. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Breiman's text 'Probability'&lt;/em&gt;, defines the conditional probability $P(C\mid Y=y)$ (&lt;em&gt;Definition 4.7&lt;/em&gt; ) as a measurable function in $y$ satisfying&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(C, Y\in A )$=$\int_{A} P( C\mid Y=y) \, P_{Y}(dy)$ where $C$ is a measurable subset of the sample space and $A$ is an arbitrary Borel subset in the state space of $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Using the concept of Regular Conditional Probability, I think the statement above can be formalized as follows.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X$ &amp;amp; $Y$ be random vectors. Let $Z$=$f(X,Y)$ be a random variable, and $P_{Y}$ be the marginal distribution of $Y$. Let $Q_{X\mid Y}(\cdot\mid y)$ be a regular conditional distribution for $X$ given $Y=y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B\mid Y=y)$, is I believe an &lt;strong&gt;&lt;em&gt;intuitive interpretation&lt;/em&gt;&lt;/strong&gt; of the following 'unproven' statement&lt;/p&gt;&#10;&#10;&lt;p&gt;$Q_{Z\mid Y}( B\mid y)$= $Q_{X\mid Y}(B_y \mid y)$  &lt;/p&gt;&#10;&#10;&lt;p&gt;where $B$ is any arbitrary Borel subset of $R$, and  $B_y$ = $[x \mid f(x,y) \in B]$&lt;/p&gt;&#10;&#10;&lt;p&gt;and $Q_{Z\mid Y}(\cdot\mid y)$ the regular conditional distribution for $Z=f(X,Y)$ given $Y=y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Written alternatively we need to prove that&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(Z\in B, Y\in A )$=$\int_{A} Q_{Z\mid Y}( B\mid y) \, P_{Y}(dy)$=$\int_{A} Q_{X\mid Y}( B_y\mid y) \, P_{Y}(dy)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;So to conclude my question is how to prove the above statement about Regular Conditional Probabilities, and if the generalization to non independent vectors is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;FYI I posted a similar question on MathStackExchange and have not got a response.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-06T17:16:29.520" Id="72069" LastActivityDate="2013-10-06T18:21:50.583" LastEditDate="2013-10-06T18:21:50.583" LastEditorUserId="88" OwnerUserId="31144" PostTypeId="1" Score="1" Tags="&lt;conditional-probability&gt;" Title="Extension of conditional probability result for non independent random vectors" ViewCount="32" />
  
  
  <row AcceptedAnswerId="72124" AnswerCount="1" Body="&lt;p&gt;I have the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Formulate the likelihood function, the log-likelihood function, and the maximum-likelihood estimate as well as the Fisher information and the observed Fisher information for each of the following problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;c)&#10;$X_i,\dots,X_n \overset{\mathrm{i.i.d}}{\sim} \mathcal{N}(\theta,\theta^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;d)&#10;$X_i,\dots,X_n \overset{\mathrm{i.i.d}}{\sim} \mathcal{N}(\theta,\theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following so far:&#10;&lt;img src=&quot;http://i.stack.imgur.com/O66Jj.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;First, did I do the right things so far?&#10;And more important: How can I calculate the maximum likelihood estimate? Or is this even possible?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After CoolSerdash's hint I got the following:&#10;&lt;img src=&quot;http://i.stack.imgur.com/RlKZQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that correct? But I have some doubts to formulate the observed Fisher information. Wouldn't this term get a beast inserting the maximum-likelihood estimate?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Similar question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to do the same for a Gamma distribution:&#10;$X_i,\dots,X_n \overset{\mathrm{i.i.d}}{\sim} \Gamma(\alpha,\beta)$ where $\alpha$ is known.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following so far:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5Jatl.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this make sense?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-06T21:09:11.787" FavoriteCount="1" Id="72081" LastActivityDate="2013-10-07T17:17:21.160" LastEditDate="2013-10-07T14:28:16.853" LastEditorUserId="21054" OwnerUserId="30623" PostTypeId="1" Score="5" Tags="&lt;self-study&gt;&lt;estimation&gt;&lt;maximum-likelihood&gt;" Title="Maximum likelihood estimate: Is this possible to solve?" ViewCount="289" />
  <row Body="&lt;p&gt;We wish to show that $\hat \theta_n \xrightarrow{P} \theta_0$. By definition,&#10;this states that given $\epsilon &amp;gt; 0$&#10;$$&#10;P(|\hat \theta_n - \theta_0| &amp;gt; \epsilon) \rightarrow 0 \text{ as } n \rightarrow \infty.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given $X_1,..., X_n$ are iid with finite $n$, we see that if we were to&#10;reorder $\{X_i\}$ in ascending order, the definition of&#10;$\hat \theta_n$ gives&#10;$\hat \theta_n = X_{\lceil n/2 \rceil}$.&#10;From this we have that&#10;$$&#10;\hat F_n(\hat \theta_n) = \left \{&#10;\begin{array}{ll}&#10;1/2 &amp;amp; \text{ if n is even} \\&#10;\frac{1}{2} + \frac{1}{2n} &amp;amp; \text{ if n is odd.}&#10;\end{array}&#10;\right.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using this, we can calculate&#10;\begin{align}&#10;P(|\hat \theta_n - \theta_0| &amp;gt; \epsilon) &#10;    &amp;amp;\leq P(|\hat \theta_n - \theta_0| \geq \epsilon) \\&#10;    &amp;amp;= P(\hat \theta_n \leq \theta_0 - \epsilon) + P(\theta_n + &#10;    \epsilon \leq \hat \theta_n) \\&#10;    &amp;amp;= P(\hat F_n(\hat \theta_n) \leq \hat F_n(\theta_0 - \epsilon)) + &#10;    P(\hat F_n(\theta_n + \epsilon) \leq \hat F_n(\hat \theta_n)) &#10;\end{align}&#10;where the last equality is due to the fact that $\hat F_n(x)$ is non-decreasing.&#10;Notice that because $\hat F_n(x)$ is non-decreasing and not strictly&#10;increasing, it can only preserve weak inequalities.&lt;/p&gt;&#10;&#10;&lt;p&gt;We continue by analyzing the terms individually. First, consider the term&#10;$$&#10;P(\hat F_n(\hat \theta_n) \leq \hat F_n(\theta_0 - \epsilon)).&#10;$$&#10;When $n$ is even,&#10;\begin{align*}&#10;P(1/2 \leq \hat F_n(\theta_0 - \epsilon))&#10;    &amp;amp;= P(1/2 - F(\theta_0 - \epsilon)) \leq \hat F_n(\theta_0 - \epsilon) - &#10;    F(\theta_0 - \epsilon))\\&#10;    &amp;amp;\rightarrow 0 \text{ as } n \rightarrow \infty&#10;\end{align*}&#10;where this last fact follows from the fact that &#10;$1/2 - F(\theta_0 - \epsilon) &amp;gt; 0 $ (we can see this from the defintion of the median) and that &#10;$\hat F_n(x) \xrightarrow{p} F(x)$, as we demonstrated previously.&#10;Similarly, when $n$ is odd,&#10;\begin{align*}&#10;P(1/2 + \frac{1}{2n} \leq \hat F_n(\theta_0 - \epsilon))&#10;    &amp;amp;= P(1/2 + \frac{1}{2n} - F(\theta_0 - \epsilon)) \leq \hat F_n(\theta_0 - \epsilon) - &#10;    F(\theta_0 - \epsilon))\\&#10;    &amp;amp;\rightarrow 0 \text{ as } n \rightarrow \infty.&#10;\end{align*}&#10;Now consider the term&#10;$$&#10;P(\hat F_n(\theta_n + \epsilon) \leq \hat F_n(\hat \theta_n)).&#10;$$&#10;Similar to before, we have&#10;\begin{align*}&#10;P(\hat F_n(\theta_n + \epsilon) \leq \hat F_n(\hat \theta_n)) &#10;    &amp;amp;= P(-\hat F_n(\theta_n + \epsilon) \geq -\hat F_n(\hat \theta_n)) \\&#10;    &amp;amp;= P(F(\theta_n + \epsilon) -\hat F_n(\theta_n + \epsilon) \geq &#10;    F(\theta_n + \epsilon) -\hat F_n(\hat \theta_n)) \\&#10;    &amp;amp;\rightarrow 0 \text{ as } n \rightarrow \infty.&#10;\end{align*}&#10;This last fact comes again from the fact that that $\hat F_n(x) \xrightarrow{p} F(x)$ and that $F(\theta_n + \epsilon) -\hat F_n(\hat \theta_n) &amp;gt; 0$, which we have because the uniqueness of the median gives that for $\epsilon &amp;gt; 0$ we have $F(\theta_0 + \epsilon) &amp;gt; .5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;These two results, applied back give&#10;\begin{align*}&#10;P(|\hat \theta_n - \theta_0| &amp;gt; \epsilon) &#10;    &amp;amp;\leq P(|\hat \theta_n - \theta_0| \geq \epsilon) \nonumber \\&#10;    &amp;amp;\rightarrow 0 \text{ as } n \rightarrow \infty&#10;\end{align*}&#10;and thus&#10;$$&#10;\hat \theta_n \xrightarrow{p} \theta_0.&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-07T00:19:34.110" Id="72089" LastActivityDate="2013-10-07T00:19:34.110" OwnerUserId="25325" ParentId="72023" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="72785" AnswerCount="2" Body="&lt;p&gt;I have results from 5 surveys each 2 years apart and let us assume that no subjects are selected in more than one survey.&lt;/p&gt;&#10;&#10;&lt;p&gt;The sampling method used in these surveys are biased and I have sampling weights calculated(with respect to the population) for each data point in each study.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, how would I be able to combine the 5 datasets and have the weights recalculated so as to obtain one giant dataset for analysis on this population?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, what should I do if subjects appear in more than one survey? &lt;/p&gt;&#10;&#10;&lt;h1&gt;Updates/Further Elaboration:&lt;/h1&gt;&#10;&#10;&lt;p&gt;thank you @user30523, here are some more infomation that might be useful:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I wish to find out the estimated distribution of height across the population using these 5 datasets. &lt;/p&gt;&#10;&#10;&lt;p&gt;In some data, younger people are oversampled because of the location where the survey are conducted. Let's assume the weights are calculated with respect to their age. &lt;/p&gt;&#10;&#10;&lt;p&gt;Eg. assuming 2% of the population are 15 years old, and the location of the survey is at a mall where 15-year-olds made up 5% of all shoppers, then sampling weight for an subject aged 15 in that survey would be calculated as 0.02 / 0.05 = 0.4. For simplicity, each person in the mall has equal chance of being surveyed and all participants complied when asked.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that 5 surveys are conducted in 5 different malls and each has their set of weights calculated in the same way, how would I then be able to combine all 5 datasets and recalculate the sampling weights?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S: I'm new to the topic on sampling weights so do correct me if I have made errors in the way I have calculated the weights.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-07T02:44:46.310" Id="72094" LastActivityDate="2013-10-17T01:14:08.513" LastEditDate="2013-10-07T07:09:40.857" LastEditorUserId="31157" OwnerUserId="31157" PostTypeId="1" Score="4" Tags="&lt;sampling&gt;&lt;survey&gt;&lt;meta-analysis&gt;&lt;population&gt;&lt;weighted-sampling&gt;" Title="How to combine data from 5 surveys from the same population spanning 10 years" ViewCount="571" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Why does the Okun's law, the one that measures the relationship between unemployment rate and GDP, regress the percentage change in GDP on change in unemployment rate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Why cannot we just simply regress GDP on unemployment rate?&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the statistical difference between the two approaches?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-07T07:43:41.730" Id="72113" LastActivityDate="2013-10-07T08:11:10.247" LastEditDate="2013-10-07T08:11:10.247" LastEditorUserId="2116" OwnerDisplayName="user31167" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;econometrics&gt;&lt;macroeconomics&gt;" Title="Why does Okun's law regress change in GDP on change in unemployment rate?" ViewCount="128" />
  <row Body="&lt;p&gt;The second problem (d), where the mean is equal to the variance is discussed on pp. 53 of &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387759700&quot; rel=&quot;nofollow&quot;&gt;Asymptotic Theory of Statistics and Probability&lt;/a&gt; by Anirban DasGupta (2008). The $\mathcal{N}(\theta, \theta)$ distribution, the normal distribution with an equal mean and variance can be seen as a continuous analog of the Poisson distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will try to outline a path to the solutions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The log-likelihood function of a $\mathcal{N}(\mu, \sigma^{2})$ is &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_distribution#Estimation_of_parameters&quot; rel=&quot;nofollow&quot;&gt;given by&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\ell(\mu, \sigma^2)=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}.&#10;$$&#10;Setting $\mu=\sigma^{2}=\theta$ yields&#10;$$&#10;\ell(\theta)=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\theta)-\frac{1}{2\theta}\sum_{i=1}^{n}(x_{i}-\theta)^{2}.&#10;$$&#10;Expanding the term under the sum leads to &#10;$$&#10;\begin{align}&#10;\ell(\theta) &amp;amp;=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\theta)-\frac{1}{2\theta}\left(\sum_{i=1}^{n}x_{i}^{2}-2\theta\sum_{i=1}^{n}x_{i}+n\theta^{2}\right) \\&#10;&amp;amp;=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\theta)-\frac{s}{2\theta}+t-\frac{n\theta}{2} \\&#10;\end{align}&#10;$$&#10;where $s=\sum_{i=1}^{n}x_{i}^{2}$ and $t=\sum_{i=1}^{n}x_{i}$. Taking the first derivative wrt $\theta$ gives&#10;$$&#10;S(\theta)=\frac{d}{d\theta}\ell(\theta)=\frac{s}{2\theta^{2}}-\frac{n}{2\theta}-\frac{n}{2}.&#10;$$&#10;So $s$ is the minimal sufficient statistic. The maximum likelihood estimator $\hat{\theta}$ can be found by setting $S(\theta)=0$ and solving for $\theta$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Applying the same procedure to $\mathcal{N}(\mu=\theta, \sigma^{2}=\theta^{2})$, the log-likelihood function is&#10;$$&#10;\ell(\theta)=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\theta^{2})-\frac{1}{2\theta^{2}}\sum_{i=1}^{n}(x_{i}-\theta)^{2}.&#10;$$&#10;This leads to the following score function (again, with $s=\sum_{i=1}^{n}x_{i}^{2}$ and $t=\sum_{i=1}^{n}x_{i}$):&#10;$$&#10;S(\theta)=\frac{s}{\theta^{3}}-\frac{t}{\theta^2}-\frac{n}{\theta}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Fisher information&lt;/h2&gt;&#10;&#10;&lt;p&gt;The Fisher information is defined as the negative second derivative of the log-likelihood function:&#10;$$&#10;I(\theta)=-\frac{d^{2}\,\ell(\theta)}{d\,\theta^{2}}=-\frac{d\,S(\theta)}{d\,\theta}.&#10;$$&#10;The &lt;em&gt;observed&lt;/em&gt; Fisher information is $I(\hat{\theta})$, the Fisher information evaluated at the maximum likelihood estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the second question (d), we have:&#10;$$&#10;I(\theta)=-\frac{d}{d\,\theta}\left(\frac{s}{2\theta^{2}}-\frac{n}{2\theta}-\frac{n}{2} \right) = \frac{s}{\theta^{3}}-\frac{n}{2\theta^{2}}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And for the first question (c), we have:&#10;$$&#10;I(\theta)=-\frac{d}{d\,\theta}\left(\frac{s}{\theta^{3}}-\frac{t}{\theta^2}-\frac{n}{\theta}\right) = \frac{3s}{\theta^{4}}-\frac{2t}{\theta^{3}}-\frac{n}{\theta^{2}}.&#10;$$&#10;To get the &lt;em&gt;observed&lt;/em&gt; Fisher information, plug in the maximum likelihood estimates.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Gamma distribution&lt;/h2&gt;&#10;&#10;&lt;p&gt;It looks right to me but you don't need the sums in the expressions of the Fisher information.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-07T11:04:13.890" Id="72124" LastActivityDate="2013-10-07T17:17:21.160" LastEditDate="2013-10-07T17:17:21.160" LastEditorUserId="21054" OwnerUserId="21054" ParentId="72081" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;This is an expansion of Scortchi and Manoel's answers, but since you seem to use R I thought I'd supply some code. :)&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe the easiest and most straightforward solution to your problem is to use a Bayesian analysis with non-informative prior assumptions as proposed by Gelman et al (2008). As Scortchi mentions, Gelman recommends to put a Cauchy prior with mean 0.0 and scale 2.5 on each coefficient (normalized to have mean 0.0 and a SD of 0.5). This will regularize the coefficients and pull them just slightly towards zero. In this case it is exactly what you want. Due to having very wide tails the Cauchy still allows for large coefficients (as opposed to the short tailed Normal), from Gelman:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5wjpb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How to run this analysis? Use the &lt;code&gt;bayesglm&lt;/code&gt; function in &lt;a href=&quot;http://cran.r-project.org/web/packages/arm/&quot;&gt;arm package&lt;/a&gt; that implements this analysis!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(arm)&#10;&#10;set.seed(123456)&#10;# Faking some data where x1 is unrelated to y&#10;# while x2 perfectly separates y.&#10;d &amp;lt;- data.frame(y  =  c(0,0,0,0, 0, 1,1,1,1,1),&#10;                x1 = rnorm(10),&#10;                x2 = sort(rnorm(10)))&#10;&#10;fit &amp;lt;- glm(y ~ x1 + x2, data=d, family=&quot;binomial&quot;)&#10;&#10;## Warning message:&#10;## glm.fit: fitted probabilities numerically 0 or 1 occurred &#10;&#10;summary(fit)&#10;## Call:&#10;## glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;, data = d)&#10;##&#10;## Deviance Residuals: &#10;##       Min          1Q      Median          3Q         Max  &#10;## -1.114e-05  -2.110e-08   0.000e+00   2.110e-08   1.325e-05  &#10;## &#10;## Coefficients:&#10;##               Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;## (Intercept)    -18.528  75938.934       0        1&#10;## x1              -4.837  76469.100       0        1&#10;## x2              81.689 165617.221       0        1&#10;## &#10;## (Dispersion parameter for binomial family taken to be 1)&#10;## &#10;##     Null deviance: 1.3863e+01  on 9  degrees of freedom&#10;## Residual deviance: 3.3646e-10  on 7  degrees of freedom&#10;## AIC: 6&#10;## &#10;## Number of Fisher Scoring iterations: 25&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does not work that well... Now the Bayesian version:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit &amp;lt;- bayesglm(y ~ x1 + x2, data=d, family=&quot;binomial&quot;)&#10;display(fit)&#10;## bayesglm(formula = y ~ x1 + x2, family = &quot;binomial&quot;, data = d)&#10;##             coef.est coef.se&#10;## (Intercept) -1.10     1.37  &#10;## x1          -0.05     0.79  &#10;## x2           3.75     1.85  &#10;## ---&#10;## n = 10, k = 3&#10;## residual deviance = 2.2, null deviance = 3.3 (difference = 1.1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Super-simple, no?&lt;/p&gt;&#10;&#10;&lt;h2&gt;References&lt;/h2&gt;&#10;&#10;&lt;p&gt;Gelman et al (2008), &quot;A weakly informative default prior distribution for logistic &amp;amp; other regression models&quot;, Ann. Appl. Stat., 2, 4&#10;&lt;a href=&quot;http://projecteuclid.org/euclid.aoas/1231424214&quot;&gt;http://projecteuclid.org/euclid.aoas/1231424214&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-07T12:37:13.007" Id="72128" LastActivityDate="2014-02-28T09:54:58.877" LastEditDate="2014-02-28T09:54:58.877" LastEditorUserId="6920" OwnerUserId="6920" ParentId="11109" PostTypeId="2" Score="11" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Lets say we extract all the ngrams (unigrams, bigrams, trigrams) along with their frequency from a text. Now if we want to select say some Top K ngrams, is there a way to estimate a good value of K using Zipf's law ?&#10;e.g. if Ngrams are -&gt; w1:10, w2:9, w3:9, w4:8, w5:5, w6:3, w7:1, w8:1, w9:1, w10:1.&#10;Here are Ngrams are w1,w2,w3 etc and numbers after them are their frequencies.&#10;Ideally we would like to have K here as 5 or 6.&#10;Is there a way to estimate K using Zipf's law ?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-10-08T09:51:19.353" Id="72219" LastActivityDate="2013-10-08T09:51:19.353" OwnerUserId="13104" PostTypeId="1" Score="1" Tags="&lt;text-mining&gt;&lt;zipf&gt;" Title="Using Zipf's law to select top K ngrams?" ViewCount="126" />
  
  
  <row Body="&lt;p&gt;There are two issues, though discussion of them are somewhat related:&lt;/p&gt;&#10;&#10;&lt;p&gt;i) What's a meaningful measure of treatment effect in the population?&lt;/p&gt;&#10;&#10;&lt;p&gt;ii) What's a good way to measure/test it in samples? &lt;/p&gt;&#10;&#10;&lt;p&gt;If you're especially interested in the difference in population means, then you should be looking to base your inference on that. On the other hand, often people really have a concept of just comparing the size &quot;I want to know which one tends to be typically smaller&quot;. Indeed, a ratio of typical values may be just as meaningful to them as a difference, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the difference of population means is not especially more meaningful than say a ratio of means or a difference in medians, then you might consider a nonparametric test, such as a Wilcoxon-Mann-Whitney test (often called a Wilcoxon Rank-Sum test or a Mann-Whitney U test). This is good at pickup up differences and is not very badly affected by large outliers. In addition, it's equally adept at picking up that a ratio of typical values differs from 1 (this would be the same as looking at the difference of logs, and it will take the same value on the log scale as it would on the original scale). On the other hand it's also sensitive to other kinds of location measure than the mean, and can be conceived in terms of the probability that a random value from one population exceeds a random value from the other (and you can even write your hypotheses that way if you choose).&lt;/p&gt;&#10;&#10;&lt;p&gt;If the difference in population means is central, then you look at that, but as you note, in &lt;em&gt;samples&lt;/em&gt; the sample means are sensitive to a few large values. This can certainly affect the usual t-test, but it's possible to use, for example, a permutation test or a bootstrap test (the two are quite similar).&lt;/p&gt;&#10;&#10;&lt;p&gt;A Kolmogorov-Smirnov test is sensitive to much more than a tendency for one group to be larger or smaller than the other. For example, if the treatment makes the observations more variable without making them larger, the KS test will still tend to reject the null - it will tell you they're different, because it's a test for equality of distributions. By having power against such alternatives, it has less power against alternatives that measure tendency to be larger or smaller.&lt;/p&gt;&#10;&#10;&lt;p&gt;The fact that you'd consider a Kolmogorov-Smirnov test makes me lean toward suggesting the Wilcoxon-Mann-Whitney as very likely a substantially better option.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you haven't considered it, you might want to look into the possibility of &lt;em&gt;matching&lt;/em&gt; or otherwise adjusting for use level. If you have some way of measuring/identifying the heavy users you could either match on it (and do a paired test), or you could use it as a covariate (or perhaps as an exposure measure) in a GLM. This will tend to give you better power to identify a treatment-difference.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-08T19:56:57.653" Id="72269" LastActivityDate="2013-10-08T19:56:57.653" OwnerUserId="805" ParentId="72265" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm using external validation to measure the success of a clustering algorithm. I don't consider my categories to be definite, so I'm looking for a measure that is forgiving to the following extent:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If two clusters are merged into one, then this shouldn't be unduly penalised as it is still a good match (but still to be penalised to some extent to prevent the algorithm being pushed towards generating huge clusters)&lt;/li&gt;&#10;&lt;li&gt;If one cluster is split, into two, then this shouldn't be unduly penalised&lt;/li&gt;&#10;&lt;li&gt;Suppose there are two clusters, A and B. Suppose half of A is put into A2, half of B into B2 and the other half of A and B are combined into C. This kind of alternative categorisation should be penalised, more so than the first two occurrences, but not unduly as it could quite possibly represent another possible valid classification&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What is a good measure for this kind of cluster validation?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is related to my &lt;a href=&quot;http://stats.stackexchange.com/questions/72205/comparing-two-sets-of-clusters&quot;&gt;previous question&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-09T01:40:10.640" Id="72291" LastActivityDate="2013-10-09T09:02:41.520" OwnerUserId="31208" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;" Title="Forgiving measure for external cluster validation" ViewCount="49" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Member of parliament can vote or can be absent during voting. Let assume:&lt;/p&gt;&#10;&#10;&lt;p&gt;If, for a given MP, there is a pattern present-absent-present (&lt;code&gt;010&lt;/code&gt;) in consecutive voting, this absence is strategic - MP does not want to reveal his preferences.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each MPs I have a data in the format:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    00011001000101_00001000_10_011_00000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where &lt;code&gt;0&lt;/code&gt; means presence of MP during voting, &lt;code&gt;1&lt;/code&gt; - absence. &lt;code&gt;_&lt;/code&gt; means there was a  interruption (ie. coffee break, night etc.) between voting.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can count probability of absence of MP - it is number of all absences divided by a number of all votings.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming absences are independent, and probability for one absence is known (computed as above), how can I count expected number of &lt;code&gt;010&lt;/code&gt; patterns in data?&lt;/p&gt;&#10;&#10;&lt;p&gt;For above data, I want to do it this way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    p=9/(23+9)=0.28125 - probability of absence&#10;    (1-p)*p*(1-p) = 0.1453 - probability of 010 pattern&#10;    number of all possible length 3 patterns in given data:&#10;    (14-2)+(8-2)+(2-2)+(3-2)+(5-2) = 22&#10;    so the expected number of 010 pattern is 22*0.1453 == 3.2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is my reasoning OK? &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I test - based on expected and real numbers of &lt;code&gt;010&lt;/code&gt; patterns -  if this is only by chance or not?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-09T12:03:09.410" Id="72340" LastActivityDate="2013-10-09T12:03:09.410" OwnerUserId="7642" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;probability&gt;" Title="How to count expected number of paterns?" ViewCount="23" />
  <row Body="&lt;p&gt;If you don't know the distribution of individual changes over time, you cannot approximate it with distribution of between-patient differences.  For example, if you have 10 patient with respective iron levels (510,520,...,600) before treatment and (520,530,...,610) after treatment, the Kruskal-Wallis ANOVA (or any other similar algorithm) would claim that there is no significant change of iron levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;IMHO, without the control group, the best you can do is to count how many patients increased their iron level and how many decreased it, and test the significance of this.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, if the KW ANOVA tells you that there is a significant iron level, it is (no false positives).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-09T13:24:26.213" Id="72347" LastActivityDate="2013-10-09T13:38:44.607" LastEditDate="2013-10-09T13:38:44.607" LastEditorUserId="31264" OwnerUserId="31264" ParentId="71533" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Does it makes sense to standardize (z-transformation) all ordinal variables (I only have Likert-scale questions) before running a PCA? Or is the z-transformation only used when having variables with different scales?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-10-09T14:02:40.587" Id="72353" LastActivityDate="2013-10-09T14:16:21.313" LastEditDate="2013-10-09T14:16:21.313" LastEditorUserId="22047" OwnerUserId="30877" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;likert&gt;" Title="z-transformation of Likert-scale questions before running a pca" ViewCount="261" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a problem similar to what was discussed &lt;a href=&quot;http://mathematica.stackexchange.com/questions/13054/estimate-error-on-slope-of-linear-regression-given-data-with-associated-uncertai&quot;&gt;here&lt;/a&gt;: I have a dataset (x_i,y_i) with y-error which is nonuniform over the interval of interest. I want to fit a line to this and extract a value of uncertainty of the slope of the fit which takes into account the y errors. I think it's safe to assume the errors are normal with mean zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am using Origin for my fits, and as far as I can tell it is only calculating the basic least squares standard error and ignoring my error bars.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately I am not familiar with Mathematica so I am unable to decipher the codes given in the linked question. They seem to do what I want but I can't make sense of them. Can someone suggest a different route?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-09T19:52:02.577" Id="72382" LastActivityDate="2013-10-09T19:52:02.577" OwnerUserId="31295" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;curve-fitting&gt;" Title="Obtaining an estimate of slope error on a linear fit to data with y error" ViewCount="72" />
  <row AnswerCount="2" Body="&lt;p&gt;I have the following relationships&lt;/p&gt;&#10;&#10;&lt;p&gt;logY ~ logX1 + logX2 + logX3 + logX4 + logX5 &lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;X1 ~ Z1 + Z2 + Z3 + Z4 + Z5&lt;/p&gt;&#10;&#10;&lt;p&gt;X2 ~ Z1 + Z2 + Z3 + Z4 + Z5&lt;/p&gt;&#10;&#10;&lt;p&gt;X3 ~ Z1 + Z2 + Z3 + Z4 + Z5&lt;/p&gt;&#10;&#10;&lt;p&gt;where Y and Z1, Z2, Z3, Z4, Z5 are endogenous (Say while the Z's play a role in determining Y, the values of Z's are fixed depending upont he values of Y - Kind of like advertising expense has an impact on sales revenue but at the same time managers determine the advertisement expense on the expected sales revenue). So all the variable are changing simultaneously. Can anyone help me on how I can estimate this relationship? I also have instruments for each of the Z's (lagged variables have been treated as instruments and I have the previous year data for the problem as well. Thank you for all your help and suggestions.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-10-09T20:44:49.813" Id="72387" LastActivityDate="2013-10-13T19:43:04.407" LastEditDate="2013-10-13T19:43:04.407" LastEditorUserId="30582" OwnerUserId="30582" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;econometrics&gt;" Title="simultaneous equations" ViewCount="117" />
  <row Body="&lt;p&gt;Perhaps it makes sense to do 4 linear regressions to determine how the $Z$s determine the $X$s i.e. &lt;/p&gt;&#10;&#10;&lt;p&gt;$X_1 = \beta'_0 + \beta_1' Z_1 + \beta_2'Z_2 + \beta_3'Z_3 + \beta_4' Z_4$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_2 = \beta''_0 + \beta_1'' Z_1 + \beta_2''Z_2 + \beta_3''Z_3 + \beta_4'' Z_4$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_3 = \beta'''_0 + \beta_1''' Z_1 + \beta_2'''Z_2 + \beta_3'''Z_3 + \beta_4''' Z_4$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_4 = \beta''''_0 + \beta_1'''' Z_1 + \beta_2''''Z_2 + \beta_3''''Z_3 + \beta_4'''' Z_4$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $Y = (\beta_0' + \beta_0'' + \beta_0''' + \beta_0'''') + (\beta_1'+\beta_2''+\beta_3'''+\beta_4'''')Z_1 + \ldots$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-09T21:53:16.440" Id="72393" LastActivityDate="2013-10-09T21:53:16.440" OwnerUserId="17661" ParentId="72387" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This is an area of hypothesis testing that has always fascinated me. Specifically because one day someone decided on some arbitrary number that dichotomized the testing procedure and since then people rarely question it. &lt;/p&gt;&#10;&#10;&lt;p&gt;I remember having a lecturer tell us not to put too much faith in the the Staiger and Stock test of instrumental variables (where the F-stat should be above 10 in the first stage regression to avoid weak instrument problems) because the number 10 was a completely arbitrary choice. I remember saying &quot;But is that not what we do with regular hypothesis testing?????&quot;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-09T23:07:50.870" Id="72403" LastActivityDate="2013-10-09T23:07:50.870" OwnerUserId="31188" ParentId="55691" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;There is some confusion with respect to the measurement error. What is the definition in statistics and definition in psychometry ?  The statistics does not seem to recognize the measurement error popularly called construct bias in psychometry.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-10T00:46:42.743" Id="72407" LastActivityDate="2013-10-13T09:32:59.457" LastEditDate="2013-10-13T09:32:59.457" LastEditorUserId="10619" OwnerUserId="10619" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;&lt;bias&gt;&lt;measurement-error&gt;" Title="What is the difference between the concept and treatment of measurement error in psychometry and in statistics?" ViewCount="103" />
  <row AnswerCount="0" Body="&lt;p&gt;I am confuse to either use a t-statistic or white-robust-t test as t-statistic is for homoskedastic and white-robust-t is for heteroskedastic error. Which test should i use after data with heteroskedastic has been corrected using e-views?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the data before correcting the errors for heteroskedasticity&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Dependent Variable: LOG_WAGE        &#10;Method: Least Squares           &#10;Sample: 1 514           &#10;Included observations: 514      &#10;&#10;Variable    Coefficient Std. Error  t-Statistic Prob.  &#10;&#10;&#10;C           1.758689    0.099402    17.69275    0.0000&#10;EDUC        0.089089    0.007015    12.69990    0.0000&#10;EXPER       0.041822    0.004884    8.562720    0.0000&#10;EXPER^2    -0.000737    0.000109    -6.784465   0.0000&#10;&#10;R-squared   0.330308        Mean dependent var  3.237487&#10;Adjusted R-squared  0.326369        S.D. dependent var  0.505323&#10;S.E. of regression  0.414744        Akaike info criterion   1.085441&#10;Sum squared resid   87.72638        Schwarz criterion   1.118454&#10;Log likelihood  -274.9583       Hannan-Quinn criter.    1.098380&#10;F-statistic 83.84807        Durbin-Watson stat  1.812307&#10;Prob(F-statistic)   0.000000            &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is the data after correcting&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Dependent Variable: LOG_WAGE        &#10;Method: Least Squares           &#10;Sample: 1 514           &#10;Included observations: 514      &#10;White heteroskedasticity-consistent standard errors &amp;amp; covariance                &#10;&#10;Variable    Coefficient Std. Error  t-Statistic Prob.  &#10;&#10;&#10;C           1.758689    0.102406    17.17369    0.0000&#10;EDUC        0.089089    0.007513    11.85735    0.0000&#10;EXPER       0.041822    0.004687    8.923171    0.0000&#10;EXPER^2    -0.000737    0.000104    -7.101447   0.0000&#10;&#10;R-squared   0.330308        Mean dependent var  3.237487&#10;Adjusted R-squared  0.326369        S.D. dependent var  0.505323&#10;S.E. of regression  0.414744        Akaike info criterion   1.085441&#10;Sum squared resid   87.72638        Schwarz criterion   1.118454&#10;Log likelihood  -274.9583       Hannan-Quinn criter.    1.098380&#10;F-statistic 83.84807        Durbin-Watson stat  1.812307&#10;Prob(F-statistic)   0.000000            &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I need to do a a hypothesis testing is there any non-linear relationship between the variables. I am just confused which test i should use. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-10-10T01:22:13.523" Id="72408" LastActivityDate="2013-10-10T01:49:51.680" LastEditDate="2013-10-10T01:49:51.680" LastEditorUserId="31225" OwnerUserId="31225" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;t-test&gt;&lt;heteroscedasticity&gt;" Title="if standard error that is heteroskedastic is corrected using E-views, does it become homoskedastic?" ViewCount="101" />
  <row AnswerCount="1" Body="&lt;p&gt;Let $X$ be a random variable having expected value $\mu$ and variance $\sigma^2$. Find the Expected Value and Variance of $Y = \frac{X−\mu}{\sigma}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to show some progress I've made so far, but honestly I've been thinking about this problem for the past few days but just have no idea where to start. Any hint or insight on a starting point would be much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-10T04:00:27.487" Id="72415" LastActivityDate="2013-10-10T04:06:18.590" LastEditDate="2013-10-10T04:04:07.093" LastEditorUserId="805" OwnerUserId="31312" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;variance&gt;&lt;expected-value&gt;&lt;theory&gt;" Title="Theoretical expected value and variance" ViewCount="231" />
  <row AnswerCount="3" Body="&lt;p&gt;I want to know &lt;strong&gt;how to check a data set for normality in Excel, just to verify that the requirements for using a t-test are being met&lt;/strong&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the right tail, is it appropriate to just calculate a mean and standard deviation, add 1, 2 &amp;amp; 3 standard deviations from the mean to create a range then compare that to the normal 68/95/99.7 for the standard normal distribution after using the norm.dist function in excel to test each standard deviation value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or is there a better way to test for normality?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-10T04:41:49.360" FavoriteCount="4" Id="72418" LastActivityDate="2014-02-04T22:23:25.687" LastEditDate="2013-10-10T06:05:50.533" LastEditorUserId="183" OwnerUserId="30677" PostTypeId="1" Score="7" Tags="&lt;normal-distribution&gt;&lt;excel&gt;" Title="How to check for normal distribution using Excel for performing a t-test?" ViewCount="42810" />
  <row Body="&lt;p&gt;A simple approach would be the following: &lt;/p&gt;&#10;&#10;&lt;p&gt;1) Take all observations sampled at random under condition A and obtain the &lt;em&gt;relevant one sided&lt;/em&gt; p-value from Wilcoxon's rank sum test.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Do the same for the observations sampled under condition B.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) If the smaller of the two p-values is below the level $\alpha/2$ and the other p-value is below $\alpha$, then your claim holds at the $\alpha$ level. (This would be the Bonferroni-Holm correction for multiple testing.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the sample sizes are extremely low, you will get a &quot;significant&quot; result only if the signal is very strong.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-10T08:18:09.707" Id="72428" LastActivityDate="2013-10-10T08:18:09.707" OwnerUserId="30351" ParentId="72179" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This may be a problem of interpretation, a misunderstanding of what a so-called &quot;direct effect&quot; coefficient really is.&lt;/p&gt;&#10;&#10;&lt;p&gt;In regression models with continuous predictor variables and no interaction terms -- that is, with no terms that are constructed as the product of other terms -- each variable's coefficient is the slope of the regression surface in the direction of that variable. It is constant, regardless of the values of the variables, and is obviously a measure of the effect of that variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;In models with interactions -- that is, with terms that are constructed as the products of other terms -- that interpretation can be made without further qualification only for variables that are &lt;strong&gt;not&lt;/strong&gt; involved in any interactions. The coefficient of a variable that &lt;strong&gt;is&lt;/strong&gt; involved in interactions is the slope of the regression surface in the direction of that variable &lt;strong&gt;when the values of all the variables that interact with the variable in question are zero&lt;/strong&gt;, and the significance test of the coefficient refers to the slope of the regression surface &lt;strong&gt;only in that region of the predictor space&lt;/strong&gt;. Since there is no requirement that there actually be data in that region of the space, the apparent direct effect coefficient may bear little resemblance to the slope of the regression surface in the region of the predictor space where data were actually observed. There is no true &quot;direct effect&quot; in such cases; the best substitute is probably the &quot;average effect&quot;: the slope of the regression surface in the direction of the variable in question, taken at each data point and averaged over all data points. For more on this, see &lt;a href=&quot;http://stats.stackexchange.com/questions/65898/answer/65917&quot;&gt;Why could centering independent variables change the main effects with moderation?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-10T08:31:00.453" Id="72429" LastActivityDate="2013-10-10T09:11:52.483" LastEditDate="2013-10-10T09:11:52.483" LastEditorUserId="17230" OwnerUserId="20776" ParentId="5450" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I have some problem with this code in Openbugs. The model is sintatically correct and data are loaded, but when I compile, software output is &quot;multiple definitions of node fn[1]&quot;. I don't know how to resolve the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model&#10;{&#10;&#10;for(i in 1:N)&#10;{&#10;x[i] ~ dgev(n[i],t[i],s[i])&#10;&#10;n[i] &amp;lt;- fn[i]+sn[i]&#10;t[i] &amp;lt;- ft[i]+st[i]&#10;s[i] &amp;lt;- fs[i]+ss[i]&#10;&#10;for(j in 1:N)&#10;{&#10;&#10;fn[i] &amp;lt;- beta0n+beta1n*lon[i]+beta2n*lat[i]&#10;ft[i] &amp;lt;- beta0t+beta1t*lon[i]+beta2t*lat[i]&#10;fs[i] &amp;lt;- beta0s&#10;&#10;sn[i] ~ dmnorm(snmean[],tsn[,])&#10;st[i] ~ dmnorm(snmean[],tst[,])&#10;ss[i] ~ dmnorm(snmean[],tss[,])&#10;&#10;tsn[i,j] &amp;lt;- inverse(covn[,])&#10;tst[i,j] &amp;lt;- inverse(covt[,])&#10;tss[i,j]&amp;lt;- inverse(covs[,])&#10;&#10;snmean[i]&amp;lt;-0&#10;&#10;covn[i,j] &amp;lt;- alfan*exp(-pow(sqrt(pow(lat[i]-lat[j],2)+pow(lon[i]-lon[j],2))/lambda, 0.5))&#10;covt[i,j] &amp;lt;- alfat*exp(-pow(sqrt(pow(lat[i]-lat[j],2)+pow(lon[i]-lon[j],2))/lambda, 0.5))&#10;covs[i,j] &amp;lt;- alfas*exp(-pow(sqrt(pow(lat[i]-lat[j],2)+pow(lon[i]-lon[j],2))/lambda, 0.5))&#10;}&#10;}&#10;&#10;alfan &amp;lt;- 1/alfangamma&#10;alfangamma ~ dgamma(1,12)&#10;beta0n ~ dnorm(0,50)&#10;beta1n ~ dnorm(0,60)&#10;beta2n ~ dnorm(0,65)&#10;lambda ~ dgamma(5,0.33)&#10;alfat &amp;lt;- 1/alfatgamma&#10;alfatgamma ~ dgamma(1,1)&#10;beta0t ~ dnorm(0,33)&#10;beta1t ~ dnorm(0,42)&#10;beta2t ~ dnorm(0,53)&#10;alfas &amp;lt;- 1/alfasgamma&#10;alfasgamma ~ dgamma(1,0.04)&#10;beta0s ~ dnorm(0,55)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These are data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;list(x=c(86.7,67.2,116.8,75.8,121.5,112.7,70,93.3,92,74.4,&#10;64.1,82.9,63.5,96,104.8,85.4,78.7,86.2,85.5,115.3,&#10;92.2,72,79.1,128.6,74,96.7,88.8,75.8,122.6,98.6,&#10;75.3,104.5,97.7,89.5,70.9,70.4,68.7,83.4,61.7,69.1,&#10;92,112.6,100.1,86.9,96.8,87.8,103.7,130,105.5,96.8,&#10;76.3,99.7,125.2,99.8,75.8,75,75.6,77.9,84.4,98.2,&#10;110.7,106,96,101.4,136,94.9,105.5,91.2,100.1,160.5,&#10;117.4,82.7,106.8,82.8,201.5,97.5,64,94.2,59.4),&#10;lon=c(661130,719070,758530,741785,693810,747735,749840,709325,&#10;657250,659975,735320,701000,682010,733050,696730,672870,&#10;715450,726370,695350,699790,708077,715095,733140,709480,&#10;700550,676530,737730,703795,720330,712090,673790,709070,&#10;686700,676325,707420,646900,692150,684590,665540,687800,&#10;667290,700860,730875,671520,700540,721600,718900,755440,&#10;690700,688698,678225,719940,744200,656680,657010,755720,&#10;710800,686310,747861,731250,737700,715220,731025,747810,&#10;654100,651280,738680,694760,676900,725245,725430,766485,&#10;680340,677050,746130,699230,693849,685117,682720),&#10;lat=c(233825,265660,250325,268332,273070,244475,263150,247050,&#10;228850,267750,262270,285890,263780,248330,283130,251875,227440,&#10;237220,255060,220375,278255,258260,252930,270170,237300,283550,250830,240680,275760,218325,269310,274350,240570,215325,227220,&#10;267700,289940,226110,209848,290270,235375,272510,219535,257080,246940,236075,278900,233270,214450,282796,288730,225580,234920,&#10;242110,220940,220430,225520,230590,254586,242140,227600,280130,267175,250410,259350,240080,241230,245160,246180,221680,268950,&#10;248850,273225,280200,229570,259430,230708,248061,259340),N=79)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks to everyone that will answer.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Now my code doesn't compile! I understand that the problem is to do inverse of matrix. How can I resolve this problem? Maybe is matrix too large (79*79)? Is there another method to calculate an inverse of matrix?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" ClosedDate="2014-03-19T14:21:11.657" CommentCount="1" CreationDate="2013-10-10T08:58:21.843" Id="72430" LastActivityDate="2014-03-19T14:20:37.917" LastEditDate="2014-03-19T14:20:37.917" LastEditorUserId="919" OwnerUserId="31322" PostTypeId="1" Score="-1" Tags="&lt;code&gt;" Title="Multiple definitions of node openbugs" ViewCount="109" />
  
  
  
  
  
  
  <row AcceptedAnswerId="72692" AnswerCount="1" Body="&lt;p&gt;The data contain category and sub-category distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;The categories are topics in a quiz such as: Music, Sports, Business. &lt;/p&gt;&#10;&#10;&lt;p&gt;Each category has three levels to choose from: Basic, Standard and Advanced.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: A user might take a quiz on Music across different levels. Say the number of questions attempted is 100. The user would have answered them across levels. 40 for basic, 40 for standard and 20 for advanced. The data consist of counts of the questions attempted within each category for each user.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best way to represent these data on a graph? Each graph would contain up to 5 main categories. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-10-10T15:25:25.017" Id="72460" LastActivityDate="2013-10-13T22:36:21.843" LastEditDate="2013-10-10T17:49:32.410" LastEditorUserId="919" OwnerUserId="31340" PostTypeId="1" Score="0" Tags="&lt;data-visualization&gt;&lt;categorical-data&gt;&lt;barplot&gt;" Title="Represent data across multiple categories and sub categories" ViewCount="151" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I put this question because while reading the benefits of  standardizing explanatory variables or not, I read &lt;em&gt;good but contrasting&lt;/em&gt; opinions about standardizing when there are interaction in the model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some talk about how problems of collinearity are removed when standardizing (e.g. &lt;a href=&quot;http://stats.stackexchange.com/questions/60476/collinearity-diagnostics-problematic-only-when-the-interaction-term-is-included#61022&quot;&gt;Collinearity diagnostics problematic only when the interaction term is included&lt;/a&gt;), which is basically the case of my GLMM. However, others claim that standard errors and p-values of interactions of standardized models are not reliable... (e.g.&lt;a href=&quot;http://stats.stackexchange.com/questions/19216/variables-are-often-adjusted-e-g-standardised-before-making-a-model-when-is&quot;&gt;Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?&lt;/a&gt; or &lt;a href=&quot;http://quantpsy.org/interact/interactions.htm&quot; rel=&quot;nofollow&quot;&gt;http://quantpsy.org/interact/interactions.htm&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;So, any ideas on what is the right thing to do?&lt;/p&gt;&#10;" ClosedDate="2013-10-17T07:08:33.993" CommentCount="2" CreationDate="2013-10-10T19:59:51.410" Id="72482" LastActivityDate="2013-10-11T10:28:02.303" LastEditDate="2013-10-11T10:28:02.303" LastEditorUserId="-1" OwnerUserId="31351" PostTypeId="1" Score="2" Tags="&lt;standardization&gt;" Title="What are the pros and cons of standardizing variable in presence of an interaction?" ViewCount="62" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm looking at a dataset that has pre-post test measurements on users' stress, depression and anxiety levels collected from a website's online health assessment. On average, the healthier participants at baseline got worse over time, and the sicker participants at baseline got much better, and the middle group gets a little better. There's definitely a regression effect going on here, but also a treatment effect too. &lt;/p&gt;&#10;&#10;&lt;p&gt;As this data was collected based on website usage, there isn't really a control group (all of the &quot;post&quot; measurements come from people that have used the online program). There are probably ways that I could synthesize a control group using the people who I can guess didn't make much use out of the treatment (based on number of logins or length of time between logins), but is there a way to separate out the treatment effect from the regression effect when you can't use difference-in-difference techniques using a control group or anything like that? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-10T23:01:51.663" Id="72495" LastActivityDate="2013-10-10T23:01:51.663" OwnerUserId="31358" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="How to separate out the regression effect vs treatment effect without a control group?" ViewCount="49" />
  
  
  <row Body="&lt;p&gt;Write your system explicitly for time $t$ as (&quot;$L$&quot; for &quot;loss&quot;, as a positive quantity, and &quot;$G$&quot; for &quot;gain&quot;)&#10;$$ A_t - A_{t-1} = - L^A_{t} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ B_t - B_{t-1} = - L^B_{t} + G_{t-1}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ C_t - C_{t-1} = - L^C_{t} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The following three relations hold exactly:&#10;$$  L^A_{t} = G_{t}^{A\rightarrow B} +  G_{t}^{A\rightarrow C} $$&#10;$$  L^B_{t} = G_{t}^{B\rightarrow A} +  G_{t}^{B\rightarrow C} $$&#10;$$  L^C_{t} = G_{t}^{C\rightarrow A} +  G_{t}^{C\rightarrow B} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you substitute in the first three you obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ A_t - A_{t-1} = - G_{t}^{A\rightarrow B} -  G_{t}^{A\rightarrow C} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ B_t - B_{t-1} = - G_{t}^{B\rightarrow A} -  G_{t}^{B\rightarrow C} + G_{t}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ C_t - C_{t-1} = - G_{t}^{C\rightarrow A} -  G_{t}^{C\rightarrow B} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You have $6$ unknown quantities to estimate &lt;em&gt;per time period&lt;/em&gt;. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate &lt;em&gt;something&lt;/em&gt;. What? Let's say you assume that there is a relatively stable &quot;churn&quot; from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of &quot;hidden transfers of market share&quot;). Write $G_{t}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).&#10;Your equations will become&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We have turned a set of mathematical identities into a &lt;em&gt;model&lt;/em&gt;. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \left[ \begin{matrix}&#10;A_t  \\&#10;B_t  \\&#10;C_t  \\&#10;\end{matrix} \right] = \left [\begin{matrix}&#10;1-a_b-a_c  &amp;amp; b_a &amp;amp; c_a \\&#10;a_b &amp;amp; 1-b_a-b_c &amp;amp; c_b \\&#10;a_c &amp;amp; b_c &amp;amp; 1-c_a-c_b \\&#10;\end{matrix} \right]  \left[ \begin{matrix}&#10;A_{t-1}  \\&#10;B_{t-1}  \\&#10;C_{t-1}  \\ &#10;\end{matrix} \right]+ \left[ \begin{matrix}&#10;u^A_{t}  \\&#10;u^B_{t}  \\&#10;u^C_{t}  \\&#10;\end{matrix} \right] $$&lt;/p&gt;&#10;&#10;&lt;p&gt;or, to homogenize notation,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \left[ \begin{matrix}&#10;A_t  \\&#10;B_t  \\&#10;C_t  \\&#10;\end{matrix} \right] = \left [\begin{matrix}&#10;\gamma_{11}  &amp;amp; \gamma_{12} &amp;amp; \gamma_{13} \\&#10;\gamma_{21} &amp;amp; \gamma_{22} &amp;amp; \gamma_{23} \\&#10;\gamma_{31} &amp;amp; \gamma_{32} &amp;amp; \gamma_{33} \\&#10;\end{matrix} \right]  \left[ \begin{matrix}&#10;A_{t-1}  \\&#10;B_{t-1}  \\&#10;C_{t-1}  \\ &#10;\end{matrix} \right]+ \left[ \begin{matrix}&#10;u^A_{t}  \\&#10;u^B_{t}  \\&#10;u^C_{t}  \\&#10;\end{matrix} \right] $$&lt;/p&gt;&#10;&#10;&lt;p&gt;subject to the equality restrictions&#10;$$  \begin{matrix}&#10;\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\&#10;\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\&#10;\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\&#10;\end{matrix} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company).&lt;br&gt;&#10;Note that these restrictions &lt;em&gt;imply&lt;/em&gt; the &quot;add up to unity&quot; restriction $A_t+B_t+C_t =1$ for each $t$, so this last one does not impose any additional structure on the unknown coefficients -but it does imply a relation between the error terms, namely that $u^A_{t} + u^B_{t}  +u^C_{t} =0$. Any additional assumptions on the three error terms should either come from knowledge of the specific real world phenomenon under study, and/or through a statistical specification search.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, an estimation for a hidden transfer of market share will be, for example&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat G_{t}^{A\rightarrow B} = \hat \gamma_{21}A_{t-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - &quot;success&quot; is never guaranteed. Then you should try to come up with a different model.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-11T01:09:15.643" Id="72507" LastActivityDate="2013-10-12T14:29:18.637" LastEditDate="2013-10-12T14:29:18.637" LastEditorUserId="28746" OwnerUserId="28746" ParentId="71849" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;I hope to use one-class SVM of LIBSVM to train a training samples so as to get a model. Then, I will use the model to predict whether the new test data and the training data is same type or not. In the training process, I have some questions as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Should the training samples all be positive examples or not?&lt;/li&gt;&#10;&lt;li&gt;Which kernel function can get better result, &lt;strong&gt;linear&lt;/strong&gt; kernel or &lt;strong&gt;RBF&lt;/strong&gt; kernel?&lt;/li&gt;&#10;&lt;li&gt;What is the effect of nu's values to the model?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-10-11T02:49:49.953" Id="72515" LastActivityDate="2013-10-31T11:19:08.333" LastEditDate="2013-10-11T03:27:59.190" LastEditorUserId="7290" OwnerUserId="31373" PostTypeId="1" Score="3" Tags="&lt;svm&gt;&lt;data-mining&gt;&lt;outliers&gt;&lt;libsvm&gt;" Title="Training one class SVM using LibSVM" ViewCount="1537" />
  <row Body="&lt;p&gt;You're asking two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is there a generic test for unimodality?&lt;/li&gt;&#10;&lt;li&gt;Are there tests to test whether a sample is derived from a given distribution, say, a normal distribution?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Ad 1): Yes, the Hartigan-Hartigan dip test, &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1176346577&quot; rel=&quot;nofollow&quot;&gt;Ann. Statist. 13(1):70-84&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ad 2): There exists a number of special tests, but the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov-Smirnov&lt;/a&gt; test is a general-purpose nonparametric test, although with low statistical power. &lt;/p&gt;&#10;&#10;&lt;p&gt;Best,&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-11T08:43:06.977" Id="72526" LastActivityDate="2013-10-11T10:57:12.170" LastEditDate="2013-10-11T10:57:12.170" LastEditorUserId="11878" OwnerUserId="11878" ParentId="72525" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Yep, if had you bothered to read the manual of &lt;code&gt;quantile&lt;/code&gt;, you would have found the function &lt;code&gt;ecdf&lt;/code&gt; in the &quot;See Also&quot; section.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- 0.1&#10;vec &amp;lt;- rnorm( 100 )&#10;ecdf( vec )( x )&#10;# or&#10;my.ecdf &amp;lt;- ecdf( vec )&#10;my.ecdf( x )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;ecdf&lt;/code&gt; is a function returning another function -- that in turn is the experimental distribution function of your distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-11T12:03:59.843" Id="72542" LastActivityDate="2013-10-11T12:03:59.843" OwnerUserId="14803" ParentId="72539" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;A data driven (and thus probably not so very good) approach&lt;/p&gt;&#10;&#10;&lt;p&gt;Calculate four correlation matrices: One for each layer and one for the pooled data (three lines per sample). If they all look quite similar, run a PCA based on the correlation matrix of the pooled sample and go on with the first few PCs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of comparing the four correlation matrices, you could also consider the four loading matrices of the corresponding PCAs and compare the loadings of the first few PCs. This is much easier if you have lots of variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-11T12:05:40.297" Id="72543" LastActivityDate="2013-10-11T12:05:40.297" OwnerUserId="30351" ParentId="43529" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="72549" AnswerCount="1" Body="&lt;p&gt;I have one group of respondents which answer on a scale of 1-5 once before and once after an experiment. I want to see if the experiment made a difference to their responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was told not to use a t-test because of the Likert scale (ordinal data does not seem to fit a t-test) and because my data are not nearly normally distributed (answers to the questions lean heavily to the 1 of the scale (which is not a mistake in the design)).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if the Wilcoxon signed-rank test works, because it seems to be designed for differences in groups (as in &quot;Do men respond differently from women?&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions on what could actually be used here?&lt;/p&gt;&#10;&#10;&lt;p&gt;(The answer &lt;a href=&quot;http://stats.stackexchange.com/questions/20245/using-t-test-for-comparing-likert-responses-before-and-after-intervention&quot;&gt;here&lt;/a&gt; refers to a &quot;special paired t-test&quot;, but does not explain which one)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-11T13:10:18.780" Id="72548" LastActivityDate="2013-10-11T13:51:35.963" OwnerUserId="27427" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;t-test&gt;&lt;likert&gt;" Title="Before and after data: Which test for average comparison of Likert scale data?" ViewCount="184" />
  
  <row Body="&lt;p&gt;Microsoft research have an online tool &lt;a href=&quot;http://research.microsoft.com/en-us/um/redmond/projects/mscompbio/fisherexacttest&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. You can also download an Excel add-in from &lt;a href=&quot;http://www.real-statistics.com/free-download/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your result according to the Microsoft tool is 6.511E-017.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-11T16:12:37.340" Id="72562" LastActivityDate="2013-10-11T16:12:37.340" OwnerUserId="29364" ParentId="72557" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;There is a very nice post here that gives a neat solution to the problem of finding the ridge parameter when the degrees of freedom are known:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/8309/how-to-calculate-regularization-parameter-in-ridge-regression-given-degrees-of-f&quot;&gt;How to calculate regularization parameter in ridge regression given degrees of freedom and input matrix?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is how can you know what the degrees of freedom are before knowing the ridge parameter value is? ( I have added a comment to the other thread but its quite old so thought it best to create a new topic).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-11T16:59:04.847" Id="72566" LastActivityDate="2013-10-11T16:59:04.847" OwnerUserId="25973" PostTypeId="1" Score="1" Tags="&lt;ridge-regression&gt;" Title="Degrees of Freedom for Ridge regression without knowing the Ridge Parameter?" ViewCount="54" />
  <row AnswerCount="1" Body="&lt;p&gt;I think I understand the concept of p-value but unfortunately I still have to exert a lot of brain cycles to get my arms around it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to get an explanation of the p-value that is rigorous enough for a sophisticated layman - something that would be intuitive.&lt;/p&gt;&#10;" ClosedDate="2013-10-11T20:05:32.083" CommentCount="0" CreationDate="2013-10-11T19:42:47.813" Id="72579" LastActivityDate="2013-10-13T21:27:49.040" LastEditDate="2013-10-11T20:28:53.333" LastEditorUserId="2970" OwnerUserId="14163" PostTypeId="1" Score="0" Tags="&lt;p-value&gt;&lt;intuition&gt;" Title="Explaining p-value to a sophisticated layman" ViewCount="1093" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm using Weka to perform classification, clustering, and some regression on a few large data sets. I'm currently trying out all the classifiers (decision tree, SVM, naive bayes, etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an automated way (in Weka or other machine learning toolkit) to sweep through all the available classifier algorithms to find the one that produces the best cross-validated accuracy or other metric? I'm not talking about boosting; rather, I'm looking to just choose the best classifier using a given data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to find the best clustering algorithm, too, for my other clustering problem; perhaps finding the lowest sum-of-squared-error?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-11T20:36:25.720" Id="72587" LastActivityDate="2013-10-13T07:01:11.173" OwnerUserId="29072" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;clustering&gt;&lt;data-mining&gt;&lt;weka&gt;" Title="Sweeping across multiple classifiers and choosing the best?" ViewCount="137" />
  
  
  <row AcceptedAnswerId="72615" AnswerCount="1" Body="&lt;p&gt;I was reading a paper related to Auto encoders for my project work. It is required to input images as vectors to the neural network. I couldn't understand a certain sentence due to lack of knowledge of statistics (I guess). I Googled, but the problem is I don't know what it is exactly and searching the same phrase returns the same kind of documents but not their explanation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Source: &lt;a href=&quot;http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;We train on 1.6 million 32*32 color images that have been preprocessed&#10;  by subtracting from each pixel its mean value over all images and then&#10;  dividing by the standard deviation of all pixels over all images.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What does it mean by &quot;subtracting from each pixel its mean value over all images and then&#10;dividing by the standard deviation of all pixels over all images&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My interpretation is: &quot;Subtracting from each pixel its mean value over all images&quot;&#10;   It means, for a pixel position in an image, subtract the average of values of that pixel position over all images and subtract from the current pixel value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;It is somewhat ambiguous to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please explain in some math terms.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-12T10:43:14.610" Id="72612" LastActivityDate="2013-10-12T12:16:10.913" LastEditDate="2013-10-12T11:51:10.477" LastEditorUserId="31411" OwnerUserId="31411" PostTypeId="1" Score="1" Tags="&lt;standard-deviation&gt;&lt;neural-networks&gt;&lt;mean&gt;&lt;image-processing&gt;" Title="What does &quot;Mean of each pixel over all images&quot; mean?" ViewCount="818" />
  
  <row Body="&lt;p&gt;Let’s remove the two categories with probability $0$ in both distributions. Your example is $P = (0.9, 0.1)$ and $Q = (1,0)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The KL divergence is $KL(P||Q) = \sum_i p_i \log\left( {p_i \over q_i }\right)$. It is not &#10;$$ 0.9 \times \log\, 0.9 + 0 $$&#10;but &#10;$$ 0.9 \times \log\, 0.9 + 0.1 \times ( +\infty ) = + \infty.$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-12T12:19:43.223" Id="72616" LastActivityDate="2013-10-12T12:19:43.223" OwnerUserId="8076" ParentId="72611" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;When two samples are related, or dependent, but the observations are not matched, are there any tests that will determine if the samples (means or otherwise) are different? I've searched extensively and have only found tests for matched samples, which is not what I need.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-12T18:47:43.087" FavoriteCount="1" Id="72635" LastActivityDate="2013-10-13T00:13:24.583" LastEditDate="2013-10-13T00:13:24.583" LastEditorUserId="805" OwnerUserId="31424" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;" Title="Test for differences between (among) related, but not matched, samples" ViewCount="49" />
  
  <row Body="&lt;p&gt;A possible formulation of this model is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;The purpose of the optimization problem is to obtain the % of each coal type to mix in order to minimize the cost of the mix without violating any operational constraint.&lt;/p&gt;&#10;&#10;&lt;p&gt;$i = $ index for coal type (1 = A, 2 = B, 3 = B, 4 = D)&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_{i} =$ % of coal type $i$ to be included in the mix&lt;/p&gt;&#10;&#10;&lt;p&gt;$c_{i} =$ cost per pound of coal of type $i$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$b_{i} =$ BTUs per pound of coal of type $i$&lt;/p&gt;&#10;&#10;&lt;p&gt;$a_{i} =$ % of ashes of coal of type $i$&lt;/p&gt;&#10;&#10;&lt;p&gt;$m_{i} =$ % of moisture of coal of type $i$&lt;/p&gt;&#10;&#10;&lt;p&gt;Objective Function: Minimize the cost of a pound of the mix&lt;/p&gt;&#10;&#10;&lt;p&gt;Min $Z = \sum_{i=1}^{4} c_{i} \cdot x_{i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Subject to the following contraints:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$BTU/lb$ of the mix must be equal to 11,900:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^4 b_{i} \cdot x_{i} = 11900$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Content of ashes of the mix must be less than 12.2%:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^4 a_{i} \cdot x{i} \leqslant 12.2\%$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The percent of moisture of the mix must be less than 9.4%:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^4 m_{i} \cdot x{i} \leqslant 9.4\%$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The percent of each coal in the mix must add up to 100%:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^4 x{i} = 100\%$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Non-negativity constraint:&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_{i} \geqslant 0, \forall i$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can implement the model in R using the Rglpk package or using the Excel Solver Add-in in MS Excel.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-13T13:26:01.097" Id="72665" LastActivityDate="2013-10-13T13:26:01.097" OwnerUserId="17045" ParentId="72650" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm after model that would spit out weights where the weights &lt;code&gt;sum to 1&lt;/code&gt; rather than parameters themselves.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I have done:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have fitted 3 logit models each with 3 independent variables (and &lt;code&gt;y&lt;/code&gt; dependent)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logit(y~x+o+z)&#10;logit(y~r+q+a)&#10;logit(y~b+n+m)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have fitted 3 models since fitting 1 model with all the parameters would deem some as insignificant (due to dependencies-correlation of the respond/dependent variable). Please note that the dependent variable (explanatory) y is ~ (0,1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, when I fit each model to my new data for prediction, I would like to do something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;predict(w1*fitted(logit1)+w2*fitted(logit2)+w3*fitted(logit3))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where w1, w2, w3 represent weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have fitted again logit model as: logit(y~logit1+logit2+logit3) and I calculated the weights as &lt;code&gt;param1/sum(param1+param2+param3)&lt;/code&gt;. This way the model didn't provide reasonable fit. (This is very likely wrong approach anyway).&lt;/p&gt;&#10;&#10;&lt;p&gt;The second approach I have tried:&lt;/p&gt;&#10;&#10;&lt;p&gt;I sampled possible weights from 0.05 to 1 (23 combinations). Multiplied the corresponding weights with the 3 models (fitted in-sample) and calculated AUC (area under the curve). THe AUC however was reasonable good for nearly all weights. Slightly higher results recorded at two combinations of weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;What model would fit to this approach? Or how to approach this modelling?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-13T14:14:50.187" Id="72668" LastActivityDate="2013-10-14T07:53:08.240" LastEditDate="2013-10-14T07:53:08.240" LastEditorUserId="23732" OwnerUserId="23732" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;generalized-linear-model&gt;&lt;logit&gt;" Title="Parameters as sort of weights" ViewCount="30" />
  
  <row Body="&lt;p&gt;TraMineR, and more generally sequence analysis, treats each sequence as a whole. Ideally, weights should therefore correct for selection bias of the sample of sequences you consider, and longitudinal weights should be used.&#10;The choice of the weights depends on which sequences you retain for your analysis:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If you use only complete sequences until the last wave, then the choice should be the longitudinal weights associated to the last wave, which accounts attrition until the last wave).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you chose to retain all sequences complete until say the 6th wave and admit missing states for the last four waves, then you should chose the weights associated to wave 6.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you retain all sequences, even for those for which you have valid data for the first wave only, then you should use weights of the first wave.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In case you select only sequences with no more than say 4 missing states, the choice may be more ambiguous. If most of the four missing states occur in the last four positions, then you could adopt the solution 2 as a good approximation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hope this helps.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-14T05:56:44.773" Id="72710" LastActivityDate="2013-10-14T05:56:44.773" OwnerUserId="11805" ParentId="72706" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I would stick with $p$ only, as $q$ does not add any information on top of $p$. I would add interaction terms between $X_1$ and $p$ and $X_2$ and $p$ and then include the main effects of both $X_1$, $X_2$ and $p$. So:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y =\beta_0 + \underbrace{\beta_1 X_1 + \beta_2 X_2 + \beta_3 p}_{\textrm{main effects}} + \underbrace{\beta_4 X_1 p + \beta_5 X_2 p}_{\textrm{interactions}} + \varepsilon$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-14T08:46:34.843" Id="72722" LastActivityDate="2013-10-14T08:46:34.843" OwnerUserId="23853" ParentId="72721" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As you say, the data is not stationary, we can find the stationary transformed data by differencing, and checked by the unit root test (e.g Augmented Dickey-Fuller test, Elliott-Rothenberg-Stock test, KPSS test, Phillips-Perron test, Schmidt-Phillips test, Zivot-Andrews test...) We can talk about ARMA model only after confirming the stationarity.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is a classical way to identify the ARMA(p, q) by the ACF plot and PACF plot. ARMA(0,1) and ARMA(0,0) can be told here. Another method to identify p, q is about the EACF, but it is not widely used for univariate time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;Empirical studies show that AIC usually tends to overfitting. The advantage of using AIC is for automatic algorithm to find the best model, but it is not usually recommended in traditional time series textbook.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-14T14:41:48.740" Id="72736" LastActivityDate="2013-10-14T14:41:48.740" OwnerUserId="29187" ParentId="72709" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;Consider a continuous response $Y$ and design matrix vector $\mathbf{X}$. These are related through some function $f(X) = Y$. Suppose that I am interested in estimating the probability that $Y \leq 0.1$ conditional on observing $\mathbf{X}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to use quantile regression to do this - &lt;strong&gt;can I confirm that this is a legitimate methodology&lt;/strong&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;We have quantiles $\tau \in [0,1]$ and after estimating our quantile regression for each $\tau$ we have our quantile estimates $\mathbf{q} := \{\hat{Q}(\tau) : \tau \in \{0.01,0.02,...,0.99\}\}$. I want to select the $\tau$ such that $\hat{Q}(\tau) \approx 0.1$. When I find such a $\hat{Q}(\tau)$ it seems to then follow naturally that $P(Y \leq 0.1) = \tau$. The reason is that my model has estimated the $\tau$-th quantile to be $0.1$, which is point on the x-axis in $Y$'s pdf that I need to find to be able to determine $P(Y \leq 0.1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice this may not work since an estimated quantile can be lower for higher $\tau$ under some $\mathbf{X}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not looking for logistic regression with a discretized response as a solution (since I already know about this).&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-10-14T15:45:47.440" Id="72743" LastActivityDate="2013-10-15T13:11:00.480" LastEditDate="2013-10-15T13:11:00.480" LastEditorUserId="30951" OwnerUserId="30951" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;conditional-probability&gt;&lt;quantiles&gt;&lt;quantile-regression&gt;&lt;conditioning&gt;" Title="Using quantile regression to predict probability of surpassing threshold" ViewCount="182" />
  
  <row Body="&lt;p&gt;It doesn't appear that $Y$ is binary.  Ordinal regression is a good choice here.  With any of the ordinal models (proportional odds, proportional hazards, probit, etc.) you can compute the probability that $Y \geq y$ for all $y$.  That probability will change at the unique values of $y$.  The R &lt;code&gt;rms&lt;/code&gt; package &lt;code&gt;orm&lt;/code&gt; function implements this efficiently and has a function generator for exceedance probabilities.  If you were extremely fortunate and really have Gaussian residuals you can use the maximum likelihood estimator of the exceedance probabilities, which is a simple function of $\hat{\mu}$ and $\hat{\sigma}$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-14T16:42:40.467" Id="72747" LastActivityDate="2013-10-14T16:42:40.467" OwnerUserId="4253" ParentId="72743" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="72755" AnswerCount="2" Body="&lt;p&gt;I am trying to implement the ideas in this paper: &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0925231212003396&quot; rel=&quot;nofollow&quot;&gt;http://www.sciencedirect.com/science/article/pii/S0925231212003396&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;This requires me to be able to remove individual trees from the forest and reclassify my training data for each removal. I've been using the randomForest package in R and had a comb through the manual but couldn't find any way of running the forest with a subset of trees, or even with an individual tree. There is a getTree function but that only gives a matrix of the node structure of the tree.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any way to do this, either in randomForest (preferably) or via another random forest implementation (e.g. scikit-learn)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-14T18:27:13.640" FavoriteCount="1" Id="72754" LastActivityDate="2013-10-15T23:13:06.160" LastEditDate="2013-10-14T19:13:29.730" LastEditorUserId="31481" OwnerUserId="31481" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;random-forest&gt;&lt;classification-tree&gt;&lt;max-margin&gt;" Title="Is there a way to remove individual trees from a forest in the randomForest package in R?" ViewCount="143" />
  
  <row Body="&lt;p&gt;I am thinking of the following two points:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;You are observing the true labels and their associated predictors, a.k.a the pair $y_i,x_i$ only when the algorithm is predicting a label of $1$. The algorithm is updated regardless of whether it made an error or not. This means that there is no feedback on mistakes (like in online learning). We get new data irrespective of our prediction performance.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The question we need to ask is then: &lt;em&gt;Does the algorithm's output influence the data source?&lt;/em&gt; If the algorithm is not influencing the source, then this aspect where we 'conditionally observe new data' will not bias the algorithm by itself (everything else held constant).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-10-14T21:24:50.677" Id="72766" LastActivityDate="2013-10-14T21:24:50.677" OwnerUserId="30815" ParentId="72756" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="72779" AnswerCount="2" Body="&lt;p&gt;I understand that in a linear regression model like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i = b_0 + b_1  x_i  + \epsilon_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;I can have a null and an alternative hypothesis:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0: b_1 = 0$ and $H_1: b_1 \neq 0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;And then I can reject $H_0$ or fail to reject $H_0$. But what if I want to accept that $b_1 = 0$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-14T21:58:30.230" Id="72768" LastActivityDate="2013-10-14T23:15:43.797" LastEditDate="2013-10-14T22:50:41.347" LastEditorUserId="23848" OwnerUserId="23848" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;equivalence&gt;" Title="How to test (and accept) that a coefficient in a linear regression model equals zero" ViewCount="78" />
  <row Body="&lt;p&gt;You cannot. &quot;Accept that $b_1=0$&quot; is the same as &quot;reject that $b_1\ne 0$&quot;.  But on what basis you could do this?  No matter how many observations you have, you cannot distinguish between 0 and sufficiently small value of $b_1$.  You can only accept that $|b_1|&amp;lt;\epsilon$  (the smaller $\epsilon$ the more observations you need).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-14T22:07:54.383" Id="72770" LastActivityDate="2013-10-14T22:07:54.383" OwnerUserId="31264" ParentId="72768" PostTypeId="2" Score="3" />
  <row Body="&lt;h3&gt;Additional missing data after log transformation&lt;/h3&gt;&#10;&#10;&lt;p&gt;If you have additional missing data after log transformation, it is likely that you have data that is less than or equal to zero. (i.e., log(0), log(-1), etc. is not defined). So if you want to use a log transformation on data with negative numbers, you need to add a constant to the raw variable so that the minimum of the resulting variable is greater than zero. So your transformation could be&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log(x + c)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x$ is your untransformed variable and $c = 1 - \textrm{min}(x)$.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Transformation flips the skewness&lt;/h3&gt;&#10;&#10;&lt;p&gt;There is plenty of discussion on this site about when and whether transformations are useful. You might also like this &lt;a href=&quot;http://pareonline.net/getvn.asp?v=8&amp;amp;n=6&quot; rel=&quot;nofollow&quot;&gt;discussion of issues surrounding transformations&lt;/a&gt;. In general, if a log transformation is flipping the direction of your skewness, then there is a good chance that you did not have very much skewness to begin with. To test whether the transformation makes a substantive difference with the context of multiple regression, examine your correlations, R-squares, and standardised betas before and after transformation, and see what changes you observed. In many cases you will see that it makes little difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another point, is that the assumption pertains to the residuals of a multiple regression and not the dependent variable itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really care about optimising the transformation to make the variable approximate a normal distribution, then you can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Power_transform&quot; rel=&quot;nofollow&quot;&gt;Box-Cox transformation&lt;/a&gt;. Or a simpler approach is just to try a range of transformations. A common set of transformations from greater to less change is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;-1/x^2&#10;-1/x&#10;log(x)&#10;sqrt(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So if &lt;code&gt;log(x)&lt;/code&gt; is transforming too much, you could try &lt;code&gt;sqrt(x)&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-14T22:29:32.767" Id="72773" LastActivityDate="2013-10-14T22:48:09.820" LastEditDate="2013-10-14T22:48:09.820" LastEditorUserId="22047" OwnerUserId="183" ParentId="72769" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Any statistical distribution is described in terms of shape, scale and location parameters. But what do these parameters mean, geometrically, statistically and for a layman with minimum statistical knowledge?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have explored wikipedia and still, this doubt continues to exist.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-15T06:36:38.530" FavoriteCount="1" Id="72806" LastActivityDate="2013-10-15T12:52:10.830" LastEditDate="2013-10-15T08:18:12.107" LastEditorUserId="22047" OwnerUserId="31505" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;" Title="Parameters of a Statistical Distribution" ViewCount="168" />
  <row Body="&lt;p&gt;Well, I would suggest you to go through a book on R by Maria L Rizzo. One of the chapters contain the use of EM algorithm with a numerical example. I remember going through the code for better understanding. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, try to view it from a clustering point of view in the beginning. Work out by hand, a clustering problem where 10 observations are taken from two different normal densities. This should help.Take help from R :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-15T07:03:26.383" Id="72808" LastActivityDate="2013-10-15T07:03:26.383" OwnerUserId="31505" ParentId="72774" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;In many cases a distribution can be described as a result of some idealized experiment. For example if we flip a fair coin $n$ times the number of heads will follow a binomial distribution with parameters $n$ and .5. These idealized experiments are often used as models; they are used as simplified representation of how the data came to be. There are obviously many such models, and as a consequence many distributions. If you want the logic behind all distributions, then that will require a book of many volumes, e.g.:&lt;/p&gt;&#10;&#10;&lt;p&gt;N. L. Johnson, S. Kotz and N. Balakrishnan (2000). &lt;em&gt;Continuous Multivariate Distributions&lt;/em&gt;, Vol. 1 (second edition), New York: Wiley &amp;amp; Sons.&lt;/p&gt;&#10;&#10;&lt;p&gt;N. L. Johnson, S. Kotz and N. Balakrishnan (1997). &lt;em&gt;Discrete Multivariate Distributions&lt;/em&gt;. New York: John Wiley &amp;amp; Sons.&lt;/p&gt;&#10;&#10;&lt;p&gt;N. L. Johnson, S. Kotz and N. Balakrishnan (1995). &lt;em&gt;Continuous Univariate Distributions&lt;/em&gt;, Vol. 2 (second edition), New York: John Wiley &amp;amp; Sons.&lt;/p&gt;&#10;&#10;&lt;p&gt;N. L. Johnson, S. Kotz and N. Balakrishnan (1994). &lt;em&gt;Continuous Univariate Distributions&lt;/em&gt;, Vol. 1 (second edition), New York: John Wiley &amp;amp; Sons.&lt;/p&gt;&#10;&#10;&lt;p&gt;N. L. Johnson, A. W. Kemp and S. Kotz (1992). &lt;em&gt;Univariate Discrete Distributions&lt;/em&gt; (second edition), New York: John Wiley &amp;amp; Sons.&lt;/p&gt;&#10;&#10;&lt;p&gt;A shorter list of distributions that is more suitable/affordable for owning yourself is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Forbes, C., Evans, M., Hastings, N., &amp;amp; Peacock, B. (2011). Statistical distributions. Wiley&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-15T08:28:49.833" Id="72820" LastActivityDate="2013-10-15T08:28:49.833" OwnerUserId="23853" ParentId="72807" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;For regression tasks you should be using a linear neuron in the output. Your logit output likely looks sigmoidal when plotted against the response variable. Plus the loss function you're using, probably makes little sense in this context.&lt;/p&gt;&#10;&#10;&lt;p&gt;Input features should always be de-meaned and divided by standard deviation. That has nothing to do with the unit types, and everything to do with training by backprop. Gradient descent will be cradling around a minima if you don't normalize properly because the error surface will be a thin ellipsoid.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, consider rectified linear units in the hidden because they train much faster than logit or tanh.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-15T11:39:25.197" Id="72832" LastActivityDate="2013-10-15T12:24:08.840" LastEditDate="2013-10-15T12:24:08.840" LastEditorUserId="9568" OwnerUserId="9568" ParentId="72600" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider the problem of the choice of estimator of $\sigma^2$ based on a random sample of size $n$  from a $N(\mu,\sigma^2)$ distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;In undergraduate, we were always taught to use the sample variance&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{s}^2 = \dfrac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;instead of the maximum likelihood estimator&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\sigma}^2 = \dfrac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is because we learned that $\hat{s}^2$ is an &lt;strong&gt;unbiased estimator&lt;/strong&gt; and that $\hat{\sigma}^2$ is &lt;strong&gt;biased&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However now I'm studying for a PhD and I've read that we choose estimators based on minimizing mean square error (=bias$^2$ + var).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://mcs.une.edu.au/~stat354/notes/node63.html&quot; rel=&quot;nofollow&quot;&gt;It can be shown&lt;/a&gt; that $$mse(\hat{\sigma}^2) &amp;lt; mse(\hat{s}^2 ).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, why do most people use $\hat{s}^2$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-15T13:24:29.923" Id="72841" LastActivityDate="2013-10-15T13:24:29.923" OwnerUserId="30494" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;unbiased-estimator&gt;" Title="Choice of variance estimator" ViewCount="47" />
  <row Body="&lt;p&gt;It may not be a good idea to consider trend &amp;amp; Seasonality as deterministic components of your dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Un-observed component model approach is an ideal way to handle such ambiguities. It estimates the trend, seasonality &amp;amp; other exogenous variables as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ideas.repec.org/h/eee/ecofch/1-07.html&quot; rel=&quot;nofollow&quot;&gt;http://ideas.repec.org/h/eee/ecofch/1-07.html&lt;/a&gt; is your starting point.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-15T13:28:09.540" Id="72842" LastActivityDate="2013-10-15T13:28:09.540" OwnerUserId="31507" ParentId="72830" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You could order the nob values from low to high and then use a search procedure to identify when and if local mean(s) changed significantly via Intervention Detection (trial and error). ID is esseentially a single dimension(characteristic) cluster analysis. Alternatively you could pre-specify the number of groups (classes) that you wished to have(n) and then find the n-1 breakpoints which optimally classifies the nob values. I have not ever done this but it might be worth a try.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-10-15T15:11:16.427" Id="72860" LastActivityDate="2013-10-15T15:11:16.427" OwnerUserId="3382" ParentId="72858" PostTypeId="2" Score="-1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to collect a two group sample for a comparison analysis (perhaps using logistic regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;The population that I need to extract a sample from is all firms from country A with activities in country B. The firms are classified into two categories: having a subsidiary in country B (S), or not having a subsidiary in country B (NS). I expect the share of S firms to be small relative to NS firms (but I have no way of knowing for sure).&lt;/p&gt;&#10;&#10;&lt;p&gt;I already hold the entire population of S firms (because this data was available to me). However data on NS firms is not readily available and I have to collect that, and I will probably not get access to identify and collect all NB firms. &lt;/p&gt;&#10;&#10;&lt;p&gt;So my situation is I have the entire population of S firms, and need to collect enough NS firms for subsequent analysis to be significant. Most likely my final sample will consist of all S firms and some share of the population of NS firms. Without much experience in doing these kinds of studies, I can't help to think that there is some kind is bias/reliability issue when sampling this way (one group: entire group population, other group: some part of group population). I have learned that &lt;em&gt;if&lt;/em&gt; it so happens that the population of NS firms is indeed much larger than S firms (again there is no way to know without data for the entire population of firms), and I e.g. end up with similar-sized samples of each group, there will be a case of oversampling the minority group. However I cannot find any remarks anywhere that consider this a problem for a comparison study, as a correct sample representation of the entire population is less important in this manner.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my concern justified? Or is it fine to do it that way for e.g. logistic regression? If not, how can I get around the issue?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-15T17:19:39.267" Id="72872" LastActivityDate="2013-10-15T20:00:07.840" LastEditDate="2013-10-15T20:00:07.840" LastEditorUserId="17230" OwnerUserId="31525" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;sampling&gt;&lt;bias&gt;&lt;methodology&gt;&lt;oversampling&gt;" Title="Proper Sampling - can I collect a two-group sample this way without issues?" ViewCount="59" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have this partition function&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ve5sd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if I take the derivative of log(Z(x)) wrt $\lambda_k$&lt;/p&gt;&#10;&#10;&lt;p&gt;the result is &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6Qikg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I didn't get how this was derived. This is the &lt;a href=&quot;http://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-15T18:36:50.983" Id="72883" LastActivityDate="2013-10-16T01:03:06.297" OwnerUserId="12329" PostTypeId="1" Score="1" Tags="&lt;optimization&gt;&lt;graphical-model&gt;" Title="Confusion relative to derivative of partition function" ViewCount="26" />
  
  <row AnswerCount="0" Body="&lt;p&gt;My prof claims that raising a symmetric r.v., like N(0,1), to an odd power gives a distribution with expectation 0.  What's the best way to see this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-15T19:27:55.210" Id="72891" LastActivityDate="2013-10-15T19:27:55.210" OwnerUserId="31534" PostTypeId="1" Score="1" Tags="&lt;mean&gt;" Title="symmetric r.v. raised to an odd power" ViewCount="51" />
  
  <row AcceptedAnswerId="72916" AnswerCount="1" Body="&lt;p&gt;In multiple regression, if you have just an ANOVA table, and nothing else, no specific data, how can you do a partial F test on X1, given X2 is already in the model?&lt;/p&gt;&#10;&#10;&lt;p&gt;So, you have the ANOVA table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;source        df   SS    MS     F&#10;-----------------------------------&#10;regression    2    1.44  0.72   9.72&#10;error         3    0.22  0.07&#10;total         5    1.66&#10;-----------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All values are filled in. With only this information, how can you do the partial F test where: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;F = MSR(X1|X2) / MSE(X1, X2)&lt;/li&gt;&#10;&lt;li&gt;MSR(X1|X2) = SSR(X1, X2) - SS(X2) = 1.44 - ????&lt;/li&gt;&#10;&lt;li&gt;MSE (X1, X2) = MSE = 0.07&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;SSR(X1, X2) can be obtained from the table (SS regression)&#10;MSE(X1, X2) can also be obtained from the table (just MSE)&#10;but I cannot get SS(X2) from the table, as far as I know......&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as I know, you need specific X and Y values to do this. Any other way from just the table?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-16T01:00:58.310" Id="72913" LastActivityDate="2013-10-16T19:03:49.020" LastEditDate="2013-10-16T19:03:49.020" LastEditorUserId="88" OwnerUserId="21686" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;anova&gt;&lt;multiple-regression&gt;" Title="Partial F ratio from ANOVA table" ViewCount="293" />
  <row AnswerCount="1" Body="&lt;p&gt;I have 1 short string of text (let's say it's a tweet, max 140 characters):&lt;br&gt;&#10;&lt;code&gt;&quot;A review of my beloved Roku 3 media player&quot;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I also have a larger body of text (like a blog article, hundreds of words) which I know is related to the tweet:&lt;br&gt;&#10;&lt;code&gt;&quot;The Roku 3 media player is a great way to watch your favorite ....&quot;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The tweet and blog article are both about the &lt;strong&gt;Roku 3 media player&lt;/strong&gt; specifically. Same author, and they share many of the same phrases, words, collocations, etc. It's likely that the string &quot;Roku 3&quot; appears in the text, along with variations like &quot;&quot;Roku 3 streaming media player&quot;, &quot;Roku 3 player&quot; etc&lt;/p&gt;&#10;&#10;&lt;p&gt;I then have 10 other tweets, some which are related to the &quot;Roku 3 media player&quot;, some of which are not (but very similar):  &lt;/p&gt;&#10;&#10;&lt;p&gt;RELATED &quot;A good review of the &lt;strong&gt;Roku 3 media player&lt;/strong&gt;&quot;&lt;br&gt;&#10;UNRELATED &quot;The Roku 2 media player review&quot;&lt;br&gt;&#10;RELATED &quot;&lt;strong&gt;Roku 3&lt;/strong&gt; is amazing&quot;&lt;br&gt;&#10;RELATED &quot;The &lt;strong&gt;Roku 3&lt;/strong&gt; is better than the Roku 2 by far&quot;&lt;br&gt;&#10;RELATED &quot;The &lt;strong&gt;Roku&lt;/strong&gt; version &lt;strong&gt;3&lt;/strong&gt; streaming media player, fully reviewed&quot;&lt;br&gt;&#10;UNRELATED &quot;A comparison review of the top &lt;strong&gt;3&lt;/strong&gt; media player boxes. &lt;strong&gt;Roku&lt;/strong&gt;, Android, Toshiba&quot;&lt;br&gt;&#10;RELATED &quot;&lt;strong&gt;Roku 3 streaming media player&lt;/strong&gt; reviewed&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Those are some examples, and I would have 10 in total. All of the tweets contain &quot;Roku&quot;, some are about &quot;Roku 2&quot; and are unrelated, one uses &quot;Roku version 3&quot; and is related etc. Obviously, this is a very small data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best method to classify each of the 10 tweets as relevant or not, in relation to the first tweet and blog article? What sort of features would be useful?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-16T05:39:24.313" FavoriteCount="1" Id="72924" LastActivityDate="2013-10-16T06:02:26.797" OwnerUserId="31554" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" Title="How to determine if short strings of text are closely related to a larger text?" ViewCount="102" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a retrospective dataset of patients treated with a certain drug (treatment, $n=46$) or with placebo (control $n=96$). The stored variables are age, sex, stage of disease. I want to assess the effect of treatment on overall survival with propensity score. Here are the steps I followed:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I calculated propensity score with a binary logistic regression&#10;model using treatment as dependent variable and age,   sex, stage as&#10;covariates.&lt;/li&gt;&#10;&lt;li&gt;I used fuzzy matching to create a 1:1 matching with 0.05 tolerance.&lt;/li&gt;&#10;&lt;li&gt;I deleted the unmatched cases and obtained a dataset of 46*2 cases&#10;(46 treated, 46 controls).&lt;/li&gt;&#10;&lt;li&gt;I used a Cox proportional regression model using propensity score&#10;and treatment as covariates.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is my procedure correct? I'm using SPSSv19.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-16T12:59:52.663" FavoriteCount="1" Id="72947" LastActivityDate="2014-04-28T00:12:13.577" LastEditDate="2014-04-28T00:12:13.577" LastEditorUserId="26338" OwnerUserId="28724" PostTypeId="1" Score="2" Tags="&lt;cox-model&gt;&lt;propensity-scores&gt;&lt;fuzzy&gt;" Title="Propensity score and Cox regression" ViewCount="334" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to learn some statistics using the book, Biometry by Sokal and Rohlf (3e). This is an exercise in the 5th chapter which covers probability, the binomial distribution, and Poisson distribution. &#10;&lt;img src=&quot;http://i.stack.imgur.com/T0Tth.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I realize there is a formula to produce an answer to this question:&#10;$$&#10;n = \frac 4 {( \sqrt{p} - \sqrt{q} )^2}&#10;$$&#10;However, this equation is not in this text. I'd like to know how to calculate sample size knowing only the probability, the desired level of confidence, and the binomial distribution. Are there any resources covering this topic that I can be pointed to? I've tried Google, but what I've seen so far requires information I don't have access to in this problem.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-16T14:27:26.077" FavoriteCount="1" Id="72955" LastActivityDate="2013-10-17T16:36:36.770" LastEditDate="2013-10-17T03:18:40.653" LastEditorUserId="805" OwnerUserId="31569" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;binomial&gt;&lt;proportion&gt;&lt;power-analysis&gt;&lt;type-i-errors&gt;" Title="Determining sample size with a proportion and binomial distribution" ViewCount="493" />
  
  <row Body="&lt;p&gt;Point is, that &lt;strong&gt;your dataset is too small&lt;/strong&gt; (4 groups 5 values each). The means obtained from such data are not very accurate representative values for each group - and therefore &lt;strong&gt;you should not run ANOVA&lt;/strong&gt; to make inference about differences among group.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;One thing is to be understandable to the audience but more important is to be scientifically accurate.&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I suggest to solve this issue by &lt;strong&gt;Kruskal-Wallis&lt;/strong&gt; followed by multiple comparisons.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Boxplots&lt;/strong&gt; (with &lt;strong&gt;&lt;em&gt;medians&lt;/em&gt;&lt;/strong&gt;) is probably the most used graphical representation of multiple comparisons of groups. To display differences you either make &lt;strong&gt;brackets&lt;/strong&gt; above pairs which are statistically different and add (&lt;code&gt;***&lt;/code&gt;-symbols or &lt;code&gt;N.S.&lt;/code&gt;) This looks good if you have &lt;em&gt;small number of groups&lt;/em&gt;. Or can make &lt;strong&gt;notches&lt;/strong&gt; on each boxplot (very helpful in &lt;em&gt;large number of groups&lt;/em&gt;) by which anyone will found desired comparison be eye.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;You may created boxplots for example in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data&amp;lt;-data.frame(value=c(rnorm(60),rnorm(20)+3), &#10;group=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), each=20))&#10;&#10;          value group&#10;1  -1.206926025     A&#10;2  -0.311125313     A&#10;3   1.336579675     A&#10;......&#10;21  1.543827796     B&#10;22 -1.874257866     B&#10;......&#10;80  4.383037868     D&#10;etc.&#10;&#10;&#10;boxplot(data$value ~ data$group, notch=TRUE,&#10;col = &quot;red&quot;, xlab=&quot;group&quot;, ylab=&quot;value&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dVzGb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Boxplots shows &lt;strong&gt;median&lt;/strong&gt; values instead of mean.&#10;I strongly suggest to &lt;strong&gt;not display ONLY mean values&lt;/strong&gt; for each group. Raw data are the last possibility.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-16T15:28:50.510" Id="72964" LastActivityDate="2013-10-16T17:01:48.550" LastEditDate="2013-10-16T17:01:48.550" LastEditorUserId="28218" OwnerUserId="28218" ParentId="72956" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Based on your question and follow-up comments, I'd start with a dot-plot. They're quick and easy (even in Excel).  Here's s sample with your data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vHFq3.png&quot; alt=&quot;DotPlot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This chart type scales well, handles large numbers of data points well and is very easy to understand-even to a non-tech audience.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-16T15:42:10.740" Id="72966" LastActivityDate="2013-10-16T15:42:10.740" OwnerUserId="10217" ParentId="72956" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have corpora of classified text. From these I create vectors. Each vector corresponds to one document.  Vector components are word weights in this document computed as TFIDF values. Next I build a model in which every class is presented by a single vector. Model has as many vectors as there classes in the corpora. Component of a model vector is computed as mean of all component values taken from vectors in this class. &#10;For unclassified vectors I determine similarity with a model vector by computing cosine between these vectors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question: Can I use Euclidean Distance between unclassified and model vector to compute their similarity? If not - why?&lt;br&gt;&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-16T17:33:04.630" Id="72978" LastActivityDate="2013-10-16T19:01:28.013" LastEditDate="2013-10-16T19:01:28.013" LastEditorUserId="88" OwnerUserId="24199" PostTypeId="1" Score="1" Tags="&lt;distance&gt;" Title="Vector space model: cosine similarity vs euclidean distance" ViewCount="471" />
  
  <row Body="&lt;p&gt;Cross-validation article in Encyclopedia of Database Systems says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Stratification is the process of rearranging the data as to ensure&#10;  each fold is a good representative of the whole. For example in a&#10;  binary classification problem where each class comprises 50% of the&#10;  data, it is best to arrange the data such that in every fold, each&#10;  class comprises around half the instances.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;About the importance of the stratification, Kohavi (A study of cross-validation and bootstrap for accuracy estimation and model selection) concludes that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;stratication is generally a better scheme, both in terms of bias and&#10;  variance, when compared to regular cross-validation.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2013-10-16T20:09:48.570" Id="72993" LastActivityDate="2013-10-16T20:09:48.570" OwnerUserId="31536" ParentId="49540" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="73016" AnswerCount="2" Body="&lt;p&gt;I am trying to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&quot; rel=&quot;nofollow&quot;&gt;bfgs&lt;/a&gt; algorithm in order to fit a set of $\{(x,y),f(x,y)\}$ to a function in the form of let's say $a\cdot cos(x)+b \cdot y=f(x,y)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I try to understand how to use bfgs algorithm with &lt;a href=&quot;http://www.chokkan.org/software/liblbfgs/group__liblbfgs__api.html&quot; rel=&quot;nofollow&quot;&gt;liblbfgs&lt;/a&gt;, but I don't understand the &lt;a href=&quot;http://www.chokkan.org/software/liblbfgs/&quot; rel=&quot;nofollow&quot;&gt;example&lt;/a&gt;  and it is not clear what function the author tried to fit.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-16T20:55:33.907" Id="73005" LastActivityDate="2015-03-02T14:05:20.883" LastEditDate="2013-10-16T21:34:49.873" LastEditorUserId="21743" OwnerUserId="21743" PostTypeId="1" Score="0" Tags="&lt;optimization&gt;" Title="How to use liblbfgs for fitting?" ViewCount="181" />
  
  <row Body="&lt;p&gt;Because the $N$ (independent) coin flips occur with probability proportional to $p^k(1-p)^{N-k}$, the likelihood induced on the coin's bias is $\textrm{Beta}(k + 1, N-k + 1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could have picked any parametrization of the bias.  You chose to represent it as a probability $0 \le p \le 1$, but it could have been an &quot;odds&quot; $0\le o$, or a log-odds $\ell$.  Since this choice is arbitrary, your prior should be independent of this choice. Jeffreys found the only prior that satisfies this &quot;indifference&quot; to the choice of parametrization: the Jeffreys prior, $\textrm{Beta}(\frac12, \frac12)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Pointwise product of densities gives the posterior $\textrm{Beta}(k+\frac12, N-k+\frac12)$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-16T22:01:59.733" Id="73014" LastActivityDate="2013-10-17T19:05:34.923" LastEditDate="2013-10-17T19:05:34.923" LastEditorUserId="858" OwnerUserId="858" ParentId="51107" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The example is doing a 100-dimensional (see #define N 100 in the code) optimization. The author is only printing the first two dimensions of x = (x[0],x[1],...,x[N]) as shown below for iteration 1.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Iteration 1:&#10;fx = 254.065298, x[0] = -1.069065, x[1] = 1.053443&#10;xnorm = 10.612828, gnorm = 325.365479, step = 0.000607&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now f(x) is defined in the function &lt;em&gt;evaluate&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for (i = 0;i &amp;lt; n;i += 2) {&#10;    lbfgsfloatval_t t1 = 1.0 - x[i];&#10;    lbfgsfloatval_t t2 = 10.0 * (x[i+1] - x[i] * x[i]);&#10;    g[i+1] = 20.0 * t2;&#10;    g[i] = -2.0 * (x[i] * g[i+1] + t1);&#10;    fx += t1 * t1 + t2 * t2;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;fx is for the function value at $x$ and $g(x)$ is a $N\times 1$ (or $100\times 1$) dimensional gradient.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;f(x) = \sum_{i=0,2,4,...,N-2}(1-x_i)^2 + \left(10(x_{i+1} - x_i^2)\right)^2&#10;\end{align}&#10;The gradient at odd components ($i=1,3,5,...$) is&#10;$$200(x_{i+1} - x_i^2) $$&#10;The gradient component at even coordinates ($i=0,2,4,6,...$) is &#10;$$-2(1-x_i) - 400*x_i*(x_{i+1} - x_i^2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;indexing starts from 0&lt;/li&gt;&#10;&lt;li&gt;I believe the gradients in the code are incorrect. When I change the appropriate line g[i+1] = 20.0 * t2; to g[i+1] = 200.0 * t2; I am getting a different answer. Potentially I may be making a mistake here. Nonetheless, hopefully I have answered your question.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Our fitting problem&lt;/strong&gt;&#10;In our case, we have a two dimensional problem. Rename our $f(x,y)$ to $z$. Then, we have an $m\times 3$ dimensional matrix of values with each row being a tuple $(x_j,y_j,z_j), j=1,...,m$ which are fixed. We could now minimize the function $h(a,b)$&#10;\begin{align}&#10;h(a,b) = \sum_{j=1}^{m}(a\cos(x_j) +b y_j - z_j)^2&#10;\end{align}&#10;With&#10;\begin{align}&#10;\frac{\partial h(a,b)}{\partial a} = -2\sum_{j=1}^{m}\left((a\cos(x_j) +b y_j - z_j)\sin(x_j)\right)\\&#10;\frac{\partial h(a,b)}{\partial b} = 2\sum_{j=1}^{m}\left((a\cos(x_j) +b y_j - z_j)y_j\right) &#10;\end{align}&#10;as the gradient functions.&#10;All that you need to do is encode these in place of the for loop above, change #define N 100 to 2 and initialize some initial value of $a,b$ to be passed into the lbfgs function.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-16T22:32:05.073" Id="73016" LastActivityDate="2013-10-17T02:30:27.043" LastEditDate="2013-10-17T02:30:27.043" LastEditorUserId="30815" OwnerUserId="30815" ParentId="73005" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="73024" AnswerCount="1" Body="&lt;p&gt;I was wondering  why it is necessarily true that if a test statistic exceeds the critical value of t, then it will also be true that the p-value will not exceed the level of significance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-17T00:23:22.743" FavoriteCount="1" Id="73023" LastActivityDate="2013-10-17T00:31:51.443" OwnerUserId="31597" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;t-test&gt;&lt;p-value&gt;" Title="Relationship between test stat and p-value in relation to t-test" ViewCount="115" />
  <row Body="&lt;p&gt;Do you mean equation (2)? I think he's not using Bayes' theorem at all -- he's just using the definition of conditional probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Recall that if $A$, $B$ are events, then&#10;$$&#10;P(A | B) = \frac{P(A \cap B)}{P(B)}.&#10;$$&#10;If you want to know more about conditional probability, I think the &lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_probability&quot; rel=&quot;nofollow&quot;&gt;Wiki article&lt;/a&gt; is pretty good.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-17T01:22:03.280" Id="73027" LastActivityDate="2013-10-17T01:22:03.280" OwnerUserId="6497" ParentId="72988" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="73156" AnswerCount="2" Body="&lt;p&gt;When using support vector machine, are there any guidelines on choosing linear kernel vs. nonlinear kernel, like RBF? I once heard that non-linear kernel tends not to perform well once the number of features is large. Are there any references on this issue?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-17T02:21:02.553" FavoriteCount="3" Id="73032" LastActivityDate="2014-07-08T14:02:17.630" LastEditDate="2013-10-17T02:30:41.000" LastEditorUserId="22468" OwnerUserId="3269" PostTypeId="1" Score="11" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;references&gt;&lt;kernel&gt;" Title="Linear kernel and non-linear kernel for support vector machine?" ViewCount="2251" />
  <row AnswerCount="1" Body="&lt;p&gt;I have to do an empirical analysis for a statistics paper. For this I want to show the differences of dependence structure for a specific data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I selected 2 stock prices, transformed them into the returns and started to measure the dependency with R. So far it is no problem, I have a result for Bravais-Pearson, Kendall and Spearman. Additionally I plotted the regression model for this two values.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have read in many papers, according to Sklar's Theorem, that it is easy to get the copula function out of the distribution function, just by use the inverse.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, if there is a possibility with R to plot the copula function (and density) just by having this data set (2 returns) or if I must first estimate the parameters to be able to plot this function.   &lt;/p&gt;&#10;&#10;&lt;p&gt;And how can I do this with R? I tried to search the answer in the handbook of the package &quot;copula&quot;, but my search wasn't really helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-17T05:20:18.950" FavoriteCount="1" Id="73041" LastActivityDate="2013-10-21T15:15:26.320" LastEditDate="2013-10-17T06:11:38.180" LastEditorUserId="805" OwnerUserId="31602" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;correlation&gt;&lt;data-visualization&gt;&lt;data-transformation&gt;&lt;copula&gt;" Title="Plot of copula (based on data set) - R" ViewCount="510" />
  
  <row Body="&lt;p&gt;You have almost performed what is usually called a &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_power&quot;&gt;power analysis&lt;/a&gt;. I say almost, because what you usually measure in a power calculation is not the mean p-value, but rather the probability that, given the sample size and the hypothesised mean difference, you would get a p-value lower than say 0.05. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can make small changes to your calculations in order to get this probability, however. The following script is a modification of your script that calculates the power for sample sizes from 2 to 50:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ctrl.mean &amp;lt;- 1&#10;ctrl.sd &amp;lt;- 0.1&#10;treated.mean &amp;lt;- 1.1&#10;treated.sd &amp;lt;- 0.22&#10;&#10;n_range &amp;lt;- 2:50&#10;max_samples &amp;lt;- 50&#10;power &amp;lt;- NULL&#10;p.theshold &amp;lt;- 0.05&#10;rpt &amp;lt;- 1000&#10;&#10;for(n in n_range) {&#10;  pvals &amp;lt;- replicate(rpt, {&#10;    t.test(rnorm(n,ctrl.mean, ctrl.sd), y = rnorm(n, treated.mean, treated.sd))$p.value&#10;  })&#10;  power &amp;lt;- rbind(power, mean(pvals &amp;lt; p.theshold) )&#10;}&#10;&#10;plot(n_range, power, type=&quot;l&quot;, ylim=c(0, 1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XyrnP.png&quot; alt=&quot;power analysis&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The way I would read this graph goes like: &quot;Given my assumptions of the two groups, the probability that I would find a significant effect at n = 30 is roughly 50%&quot;. Often an 80% chance of finding an actual effect is considered a high level of power. By the way, power analysis is generally considered &lt;em&gt;a good thing&lt;/em&gt;. :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-17T08:27:11.787" Id="73048" LastActivityDate="2013-10-17T08:41:04.680" LastEditDate="2013-10-17T08:41:04.680" LastEditorUserId="6920" OwnerUserId="6920" ParentId="73045" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;When you have &lt;strong&gt;multiple variable&lt;/strong&gt; and you are looking for variable(s) &lt;strong&gt;which is the best for discriminating between groups&lt;/strong&gt; (&quot;yes&quot; and &quot;no&quot; samples in this case) a tool for this is &lt;strong&gt;MANOVA&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Suppose we have a data.frame with 7 variables and one group:&#10;my.data&amp;lt;-data.frame(v1=rnorm(100),v2=rnorm(100),v3=rnorm(100),&#10;v4=rnorm(100),v5=rnorm(100),v6=rnorm(100), v7=c(rnorm(50),&#10;rnorm(50)+20),response=rep(c(&quot;yes&quot;,&quot;no&quot;), each=50))&#10;&#10;# run MANOVA&#10;my.mnv&amp;lt;-manova(cbind(v1,v2,v3,v4,v5,v6,v7) ~ response, data=my.data)&#10;&#10;# and look on p-values (if p-value &amp;lt; 0.05 then it is able to &#10;# significantly discriminate between &quot;yes&quot; and &quot;no&quot;)&#10;summary.aov(my.mnv)&#10;&#10;# plot&#10;pairs(my.data[c(&quot;v1&quot;,&quot;v2&quot;,&quot;v3&quot;,&quot;v4&quot;,&quot;v5&quot;,&quot;v6&quot;,&quot;v7&quot;)], pch=22,&#10;bg=c(&quot;red&quot;, &quot;yellow&quot;)[unclass(my.data$response)])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;It's not good to make conclusions about statistical significance based on looking on the plot&lt;/strong&gt; (although it is necessary to look on it). In you case of 107 variables the &lt;code&gt;pairs()&lt;/code&gt; plot will be very chaotic.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-17T08:31:52.307" Id="73049" LastActivityDate="2013-10-17T08:38:21.307" LastEditDate="2013-10-17T08:38:21.307" LastEditorUserId="28218" OwnerUserId="28218" ParentId="72981" PostTypeId="2" Score="6" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose we have two variables $x_1$ and $x_2$ and an interaction term $x_1 \cdot x_2$. Suppose we set the family-wide error rate to $\alpha = 0.05$. For the Bonferonni correction, would be look at $\alpha/2$ or $\alpha/3$?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-17T14:17:45.533" FavoriteCount="0" Id="73067" LastActivityDate="2013-10-17T14:17:45.533" OwnerUserId="31615" PostTypeId="1" Score="2" Tags="&lt;multiple-comparisons&gt;" Title="Bonferonni Correction interaction" ViewCount="57" />
  
  
  <row Body="&lt;p&gt;From a mathematical standpoint, there's nothing wrong with doing a Sobel test with survey data (by the way, and slightly off-topic -- you should consider using a bootstrapping method to test your indirect effects instead of a Sobel test; bootstrapping methods &lt;a href=&quot;http://www.public.asu.edu/~davidpm/classes/publications/2007PsychologicalScience.pdf&quot; rel=&quot;nofollow&quot;&gt;are uniformly more powerful&lt;/a&gt; than Sobel tests).  The real question is what conclusions you would be able to draw from your Sobel test.&lt;/p&gt;&#10;&#10;&lt;p&gt;To get a clear sense of the problem, consider a simple study in which the researcher measures people's scores on a self-report measure of trait empathy and the amount of money these people donated to charitable causes within the last year.  Assuming that the researchers observed a relationship between trait empathy and donations, few people would make the mistake of concluding that trait empathy causes donations (i.e., empathy -&gt; donations), since the people in the study were not randomized to their values of trait empathy.  Thus, it is possible that people who thought about their levels of donations reported higher levels of trait empathy (i.e., donations -&gt; empathy) or that a third variable caused the observed values of both donations and empathy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's now consider a study in which the researchers measured trait empathy, charitable donations, and positive emotions.  The researcher wishes to show that the experience of positive emotions mediates the link between trait empathy and charitable donations (i.e., that empathy -&gt; positive emotions -&gt; donations).  In order to convincingly establish mediation, we must show both that empathy -&gt; positive emotions and that empathy -&gt; donations.  However, because people were not randomized to their values of trait empathy, we cannot conclude that empathy caused either positive emotions or donations.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, even if we had randomized people to their empathy scores, we would still not necessarily be able to conclude that positive emotions were a mediator for the empathy -&gt; donations effect because, after people's assignment to their empathy scores, people were not randomized to their values of positive emotions.  Thus, even if we established a non-zero indirect effect, it is possible that, for example, an unobserved candidate mediator causes both positive emotions and donations, and it is this unobserved candidate mediator that creates the observed empathy -&gt; positive emotions -&gt; donations indirect effect (for more information about this problem, see some of the references added below).&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, there is nothing wrong with doing a Sobel test or any other test of mediation with survey data.  However, just as when you examine simple bivariate relationships with survey data, such a test probably will not reveal much about causal mechanisms because the assumptions required to draw these conclusions are implausible at best.&lt;/p&gt;&#10;&#10;&lt;p&gt;I recommend reading some of the references below for more information about assumptions in mediation models.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927874/&quot; rel=&quot;nofollow&quot;&gt;Jo, B. (2008).  Causal inference in randomized experiments with mediational processes. Psychological Methods, 13, 314–336.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1011.1079.pdf&quot; rel=&quot;nofollow&quot;&gt;Imai, K., Keele, L., &amp;amp; Yamamoto, T. (2010). Identiﬁcation, inference and sensitivity&#10;analysis for causal mediation eﬀects. Statistical Science, 25, 51-71.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://imai.princeton.edu/research/files/mediationP.pdf&quot; rel=&quot;nofollow&quot;&gt;Imai, K., Keele, L., Tingley, D., &amp;amp; Yamamoto T. (2011). Unpacking the black box of causality: Learning about causal mechanisms from experimental and observational studies. American Political Science Review, 105, 765-789.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-17T18:12:15.970" Id="73086" LastActivityDate="2013-10-18T16:38:49.657" LastEditDate="2013-10-18T16:38:49.657" LastEditorUserId="11091" OwnerUserId="11091" ParentId="73037" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;So I'm using R's &lt;code&gt;pairwise.t.test&lt;/code&gt; on 4 populations with a Hochberg p-value correction and an alternative hypothesis that the population on the row has a higher mean than the population on the column. I get the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pairwise.t.test(data, labels, alt='greater', p.adjust.method='hochberg')&#10;&#10;   A       B      C    &#10;B 0.00323  -      -     &#10;C 0.00019  0.4864 -     &#10;D 0.00135  0.9955 0.9970&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Clearly we can reject the null hypothesis and accept the alternative hypothesis for pairs &lt;code&gt;(B, A)&lt;/code&gt;, &lt;code&gt;(C, A)&lt;/code&gt;, and &lt;code&gt;(D, A)&lt;/code&gt;. We can also not reject the null hypothesis for pairs &lt;code&gt;(D, B)&lt;/code&gt; and &lt;code&gt;(D, C)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now about &lt;code&gt;(B, D)&lt;/code&gt;: If we were using no p-value correction, I would be confident in rejecting the alternative hypothesis that B has the same mean as D and accepting the null hypothesis that B has a high mean than D. However, the p-value correction causes me to be suspicious of this.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is of the &quot;give me a fish and then teach me to fish&quot; variety: Does the same symmetry exist with the p-value correction as without? Also, can anyone point me toward a resource that would deepen my understanding of p-value correction to the point where the answer to my first question is obvious.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-17T20:03:22.360" Id="73097" LastActivityDate="2013-10-17T20:03:22.360" OwnerUserId="22027" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="Symmetry of alternative hypotheses of pairwise t-tests with p-value correction?" ViewCount="45" />
  
  <row Body="&lt;p&gt;You may want to look at &lt;strong&gt;Multidimensional Scaling&lt;/strong&gt;. This will try to find the 2d projection that best preserves your distances. For obvious reasons, There is no lossless 4d to 2d projection.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-18T07:46:48.313" Id="73124" LastActivityDate="2013-10-18T07:46:48.313" OwnerUserId="7828" ParentId="73039" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;If I understand, you are looking for &lt;a href=&quot;http://en.wikipedia.org/wiki/K-mer&quot; rel=&quot;nofollow&quot;&gt;k-mers&lt;/a&gt; which are patterns of size k found in sequences.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is an R package for analyzing sequence data called &lt;a href=&quot;http://mephisto.unige.ch/traminer/&quot; rel=&quot;nofollow&quot;&gt;TraMineR&lt;/a&gt; which includes functions for plotting the sequences, finding the variance of state durations, compute within sequence entropy, extract frequent event subsequences, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also compare two sequences to see how they align in time by using &lt;a href=&quot;http://en.wikipedia.org/wiki/Dynamic_time_warping&quot; rel=&quot;nofollow&quot;&gt;Dynamic Time Warping&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-18T17:29:17.817" Id="73166" LastActivityDate="2013-10-18T17:29:17.817" OwnerUserId="3287" ParentId="73084" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Bayesian: &quot;Hello, Machine Learner!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Frequentist:  &quot;Hello, Machine Learner!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Machine Learning: &quot;I hear you guys are good at stuff.  Here's some data.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Yes, let's write down a model and then calculate the MLE.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Hey, F, that's not what you told me yesterday!  I had some univariate data and I wanted to estimate the variance, and I calculated the MLE.  Then you pounced on me and told me to &lt;a href=&quot;http://en.wikipedia.org/wiki/Bessel%27s_correction&quot;&gt;divide by $n-1$ instead of by $n$&lt;/a&gt;.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Ah yes, thanks for reminding me.  I often think that I'm supposed to use the MLE for everything, but I'm interested in &lt;a href=&quot;http://en.wikipedia.org/wiki/Biased_estimator&quot;&gt;unbiased estimators&lt;/a&gt; and so on.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;Eh, what's this philosophizing about?  Will it help me?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot; OK, an &lt;em&gt;estimator&lt;/em&gt; is a black box, you put data in and it gives you some numbers out.  We frequentists don't care about how the box was constructed, about what principles were used to design it.  For example, I don't know how to derive the $\div(n-1)$ rule.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot; So, what do you care about?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Evaluation.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;I like the sound of that.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;A black box is a black box.  If somebody claims a particular estimator is an unbiased estimator for $\theta$, then we try many values of $\theta$ in turn, generate many samples from each based on some assumed model, push them through the estimator, and find the average &lt;em&gt;estimated $\theta$&lt;/em&gt;.  If we can prove that the expected estimate equals the true value, for all values, then we say it's unbiased.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;Sounds great!  It sounds like frequentists are pragmatic people.  You judge each black box by its results.  Evaluation is key.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Indeed!  I understand you guys take a similar approach.  Cross-validation, or something?  But that sounds messy to me.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;Messy?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;The idea of testing your estimator on real data seems dangerous to me.  The empirical data you use might have all sorts of problems with it, and might not behave according the model we agreed upon for evaluation.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;What?  I thought you said you'd proved some results?  That your estimator would always be unbiased, for all $\theta$.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Yes.  While your method might have worked on one dataset (the dataset with train and test data) that you used in your evaluation, I can prove that mine will always work.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;For all datasets?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;No.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;So my method has been cross-validated on one dataset.  You haven't test yours on any real dataset?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;That's right.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;That puts me in the lead then!  My method is better than yours.  It predicts cancer 90% of the time.  Your 'proof' is only valid if the entire dataset behaves according to the model you assumed.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Emm, yeah, I suppose.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;And that interval has 95% &lt;em&gt;coverage&lt;/em&gt;.  But I shouldn't be surprised if it only contains the correct value of $\theta$ 20% of the time?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;That's right.  Unless the data is truly i.i.d Normal (or whatever), my proof is useless.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;So my evaluation is more trustworthy and comprehensive? It only works on the datasets I've tried so far, but at least they're real datasets, warts and all.  There you were, trying to claim you were more 'conservative' and 'thorough' and that you were interested in model-checking and stuff.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: (interjects) &quot;Hey guys, Sorry to interrupt.  I'd love to step in and balance things up, perhaps demonstrating some other issues, but I really love watching my frequentist colleague squirm.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Woah!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;OK, children.  It was all about evaluation.  An estimator is a black box.  Data goes in, data comes out.  We approve, or disapprove, of an estimator based on how it performs under evaluation.  We don't care about the 'recipe' or 'design principles' that are used.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Yes.  But we have very different ideas about which evaluations are important.  ML will do train-and-test on real data.  Whereas I will do an evaluation that is more general (because it involves a broadly-applicable proof) and also more limited (because I don't know if your dataset is actually drawn from the modelling assumptions I use while designing my evaluation.)&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;What evaluation do you use, B?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: (interjects) &quot;Hey. Don't make me laugh. He doesn't evaluate anything.  He just uses his subjective beliefs and runs with it.  Or something.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;That's the common interpretation.  But it's also possible to define Bayesianism by the evaluations preferred.  Then we can use the idea that none of us care what's in the black box, we care only about different ways to evaluate.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B continues:   &quot;Classic example:  Medical test.  The result of the blood test is either Positive or Negative.  A frequentist will be interested in, of the Healthy people, what proportion get a Negative result.  And similarly, what proportion of Sick people will get a Positive. The frequentist will calculate these for each blood testing method that's under consideration and then recommend that we use the test that got the best pair of scores.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Exactly. What more could you want?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;What about those individuals that got a Positive test result?  They will want to know 'of those that get a Positive result, how many will get Sick?' and 'of those that get a Negative result, how many are Healthy?' &quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;Ah yes, that seems like a better pair of questions to ask.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;HERESY!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Here we go again.  He doesn't like where this is going.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;This is about 'priors', isn't it?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;EVIL&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Anyway, yes, you're right ML.  In order to calculate the proportion of Positive-result people that are Sick you must do one of two things.  One option is to run the tests on lots of people and just observe the relevant proportions. How many of those people go on to die of the disease, for example.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;That sounds like what I do.  Use train-and-test.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;But you can calculate these numbers in advance, if you are willing to make an assumption about the rate of Sickness in the population.  The frequentist also makes his calcuations in advance, but without using this population-level Sickness rate.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F:  &quot;MORE UNFOUNDED ASSUMPTIONS.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Oh shut up.  Earlier, you were found out.  ML discovered that you are just as fond of unfounded assumptions as anyone.  Your 'proven' coverage probabilities won't stack up in the real world unless all your assumptions stand up.  Why is my prior assumption so diffent?  You call me crazy, yet you pretend your assumptions are the work of a conservative, solid, assumption-free analysis.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B (continues): &quot;Anyway, ML, as I was saying. Bayesians like a different kind of evaluation.  We are more interested in conditioning on the observed data, and calculating the accuracy of our estimator accordingly.  We cannot perform this &lt;em&gt;evaluation&lt;/em&gt; without using a prior.  But the interesting thing is that, once we decide on this form of evaluation, and once we choose our prior, we have an automatic 'recipe' to create an appropriate estimator.  The frequentist has no such recipe.  If he wants an unbiased estimator for a complex model, he doesn't have any automated way to build a suitable estimator.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;And you do? You can automatically build an estimator?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Yes.  I don't have an automatic way to create an unbiased estimator, because I think bias is a bad way to evaluate an estimator.  But given the conditional-on-data estimation that I like, and the prior, I can connect the prior and the likelihood to give me the estimator.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;So anyway, let's recap.  We all have different ways to evaluate our methods, and we'll probably never agree on which methods are best.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Well, that's not fair.  We could mix and match them.  If any of us have good labelled training data, we should probably test against it.  And generally we all should test as many assumptions as we can.  And some 'frequentist' proofs might be fun too, predicting the performance under some presumed model of data generation.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Yeah guys.  Let's be pragmatic about evaluation.  And actually, I'll stop obsessing over infinite-sample properties.  I've been asking the scientists to give me an infinite sample, but they still haven't done so.  It's time for me to focus again on finite samples.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;So, we just have one last question.  We've argued a lot about how to &lt;em&gt;evaluate&lt;/em&gt; our methods, but how do we &lt;em&gt;create&lt;/em&gt; our methods.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Ah. As I was getting at earlier, we Bayesians have the more powerful general method.  It might be complicated, but we can always write some sort of algorithm (maybe a naive form of MCMC) that will sample from our posterior.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F(interjects): &quot;But it might have bias.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;So might your methods.  Need I remind you that the MLE is often biased?  Sometimes, you have great difficulty finding unbiased estimators, and even when you do you have a stupid estimator (for some really complex model) that will say the variance is negative.  And you call that unbiased.  Unbiased, yes.  But useful, no!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;OK guys. You're ranting again.  Let me ask you a question, F.  Have you ever compared the bias of your method with the bias of B's method, when you've both worked on the same problem?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;Yes.  In fact, I hate to admit it, but B's approach sometimes has lower bias and &lt;a href=&quot;http://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;MSE&lt;/a&gt; than my estimator!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;The lesson here is that, while we disagree a little on evaluation, none of us has a monopoly on how to create estimator that have properties we want.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B: &quot;Yes, we should read each other's work a bit more.  We can give each other inspiration for estimators.  We might find that other's estimators work great, out-of-the-box, on our own problems.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;F: &quot;And I should stop obsessing about bias.  An unbiased estimator might have ridiculous variance.  I suppose all of us have to 'take responsibility' for the choices we make in how we evaluate and the properties we wish to see in our estimators.  We can't hind behind a philosophy.  Try all the evaluations you can.  And I will keep sneaking a look at the Bayesian literature to get new ideas for estimators!&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;B:&quot;In fact, a lot of people don't really know what their own philosophy is.  I'm not even sure myself.  If I use a Bayesian recipe, and then proof some nice theoretical result, doesn't that mean I'm a frequentist?  A frequentist cares about above proofs about performance, he doesn't care about recipes.  And if I do some train-and-test instead (or as well), does that mean I'm a machine-learner?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;ML: &quot;It seems we're all pretty similar then.&quot;&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2013-10-18T19:17:41.230" CreationDate="2013-10-18T19:17:41.230" Id="73180" LastActivityDate="2013-10-18T19:35:16.013" LastEditDate="2013-10-18T19:35:16.013" LastEditorUserId="7817" OwnerUserId="7817" ParentId="6" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I once read the following inequality &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Wx8rr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any specific name for this inequality? And, how to prove it?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-18T19:34:57.900" Id="73182" LastActivityDate="2013-10-18T19:34:57.900" OwnerUserId="3125" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;random-variable&gt;" Title="An inequality for the non-negative random variable" ViewCount="40" />
  
  <row Body="&lt;p&gt;It's very difficult to add to the constellation of stars that are already listed, but for interest purposes I will throw in the &lt;em&gt;improbable&lt;/em&gt; polymath &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Maynard_Keynes&quot; rel=&quot;nofollow&quot;&gt;John Maynard Keynes&lt;/a&gt; who many would not realize published &lt;em&gt;A Treatise on Probability&lt;/em&gt; (1921) that can be downloaded &lt;a href=&quot;http://www.gutenberg.org/ebooks/32625&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;; and whose work was quoted frequently by Harold Jeffreys (1939).&lt;/p&gt;&#10;&#10;&lt;p&gt;Keynes by all accounts helped to bring forward Bayesian statistics and in his treatise considered the most important principle to be the &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_indifference&quot; rel=&quot;nofollow&quot;&gt;Principle of Indifference&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to Wikipedia, &lt;em&gt;The &quot;Principle of insufficient reason&quot; was renamed the &quot;Principle of Indifference&quot; by the economist John Maynard Keynes (1921), who was careful to note that it applies only when there is no knowledge indicating unequal probabilities.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-10-18T20:20:29.640" CreationDate="2013-10-18T20:20:29.640" Id="73187" LastActivityDate="2013-10-18T20:42:01.120" LastEditDate="2013-10-18T20:42:01.120" LastEditorUserId="31323" OwnerUserId="31323" ParentId="5115" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="73245" AnswerCount="1" Body="&lt;p&gt;I'm simply trying to recalculate with dnorm() the log-likelihood provided by the logLik function from a lm model (in R).&lt;/p&gt;&#10;&#10;&lt;p&gt;It works (almost perfectly) for high number of data (eg n=1000)  : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; n &amp;lt;- 1000&#10;&amp;gt; x &amp;lt;- 1:n&#10;&amp;gt; set.seed(1)&#10;&amp;gt; y &amp;lt;- 10 + 2*x + rnorm(n, 0, 2)&#10;&amp;gt; mod &amp;lt;- glm(y ~ x, family = gaussian)&#10;&amp;gt; logLik(mod)&#10;'log Lik.' -2145.562 (df=3)&#10;&amp;gt; sigma &amp;lt;- sqrt(summary(mod)$dispersion)&#10;&amp;gt; sum(log(dnorm(x = y, mean = predict(mod), sd = sigma)))&#10;[1] -2145.563&#10;&amp;gt; sum(log(dnorm(x = resid(mod), mean = 0, sd = sigma)))&#10;[1] -2145.563&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but for small datasets there are clear differences : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; n &amp;lt;- 5&#10;&amp;gt; x &amp;lt;- 1:n&#10;&amp;gt; set.seed(1)&#10;&amp;gt; y &amp;lt;- 10 + 2*x + rnorm(n, 0, 2)&#10;&amp;gt; &#10;&amp;gt; mod &amp;lt;- glm(y ~ x, family = gaussian)&#10;&amp;gt; logLik(mod)&#10;'log Lik.' -8.915768 (df=3)&#10;&amp;gt; sigma &amp;lt;- sqrt(summary(mod)$dispersion)&#10;&amp;gt; sum(log(dnorm(x = y, mean = predict(mod), sd = sigma)))&#10;[1] -9.192832&#10;&amp;gt; sum(log(dnorm(x = resid(mod), mean = 0, sd = sigma)))&#10;[1] -9.192832&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Because of small dataset effect I thought it could be due to the differences in residual variance estimates between lm and glm but using lm provides the same result as glm : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; modlm &amp;lt;- lm(y ~ x)&#10;&amp;gt; logLik(modlm)&#10;'log Lik.' -8.915768 (df=3)&#10;&amp;gt; &#10;&amp;gt; sigma &amp;lt;- summary(modlm)$sigma&#10;&amp;gt; sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma)))&#10;[1] -9.192832&#10;&amp;gt; sum(log(dnorm(x = resid(modlm), mean = 0, sd = sigma)))&#10;[1] -9.192832&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where am I wrong ? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-18T22:27:08.737" FavoriteCount="1" Id="73196" LastActivityDate="2013-10-19T18:45:53.120" LastEditDate="2013-10-19T09:54:25.490" LastEditorUserId="88" OwnerUserId="31679" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;likelihood&gt;&lt;lm&gt;" Title="Recalculate log-likelihood from a simple R lm model" ViewCount="718" />
  <row AnswerCount="0" Body="&lt;p&gt;I've just implemented a naive Bayesian classifier and found out about the Fisher method (&lt;a href=&quot;http://stats.stackexchange.com/questions/31366/linear-discriminant-analysis-and-bayes-rule&quot;&gt;Linear discriminant analysis and Bayes rule&lt;/a&gt;) while looking for ways to improve it. I'm very new to this field.&lt;/p&gt;&#10;&#10;&lt;p&gt;My raw data model is like the following (for sentiment analysis):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{&quot;I like the movie a lot&quot;, &quot;positive&quot;, 1000}&#10;{&quot;I hate the movie a lot&quot;, &quot;negative&quot;, 100}&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you see, I have only two classes, possibly more, and the third value is weight values to count when getting probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;So when given this kind of data,  and to get the PDF like below,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$PDF(x|k) = \frac {e^{-d/2}} {(2\pi)^{p/2}\sqrt{|S|})}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have no idea how to approach this. What should I set as discriminants? Where should I start to matrix-ify my data to get the covariance? Once I model the data, the next step seems to be relatively easier with just calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, what should I do to get the values of $p$ discriminants from the data &lt;code&gt;{&quot;I like the movie a lot&quot;, &quot;positive&quot;, 1000}&lt;/code&gt;, with the first value as training text, send as class value, third as weight value.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-19T02:19:52.437" Id="73208" LastActivityDate="2013-10-19T02:53:07.130" LastEditDate="2013-10-19T02:53:07.130" LastEditorUserId="7290" OwnerUserId="31663" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;naive-bayes&gt;&lt;discriminant&gt;" Title="Bayesian, Fisher method: model very simple data to get discriminants" ViewCount="71" />
  
  
  
  
  <row AcceptedAnswerId="73220" AnswerCount="2" Body="&lt;p&gt;I was wondering if I could get some opinions on an issue. I'm analyzing my data using mixed-effects modeling in R (lme4 package). My model has by-subject and by-item intercepts and slopes, and random correlation parameters between them. Since the current version of lmer() does not have MCMC sampling implemented, I cannot get a pvalue for the coefficients in the model. Therefore, I would like to report the &lt;strong&gt;t-value&lt;/strong&gt; instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have often seen papers in my field (psycholinguistics) just say something like &quot;In all models presented, |t| &gt; 2 and |z| &gt; 2 correspond to a significant effect at a significance level of .05&quot;. I was wondering whether there is some reference I can provide from this type of sentences? I understand that this tends to be the case, but I wonder whether this is something that has been shown (and I should give references) or whether it is ok to just state it and assume everyone will be ok with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suggestions welcome! &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-19T03:15:22.560" FavoriteCount="3" Id="73214" LastActivityDate="2013-10-19T15:58:17.873" LastEditDate="2013-10-19T03:35:55.697" LastEditorUserId="27615" OwnerUserId="27615" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;references&gt;&lt;p-value&gt;&lt;lmer&gt;" Title="Avoiding p-values and reporting t-values instead. References?" ViewCount="225" />
  <row AcceptedAnswerId="73222" AnswerCount="1" Body="&lt;p&gt;My notes define a loss function as the 'cost' incurred when the true value of $\theta$ is estimated by $\hat\theta$. What kind of cost is it talking about? monetary cost? or is it something related to errors?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-19T11:19:33.860" FavoriteCount="1" Id="73221" LastActivityDate="2013-10-19T14:51:39.313" LastEditDate="2013-10-19T14:51:39.313" LastEditorUserId="930" OwnerUserId="31104" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;decision-theory&gt;" Title="What is a loss function in decision theory?" ViewCount="108" />
  <row AnswerCount="0" Body="&lt;p&gt;Local linear regression is a popular tool. How can one choose the number of grid points for which to estimate the unknown function on?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-19T15:56:12.137" Id="73233" LastActivityDate="2013-10-19T15:56:12.137" OwnerUserId="30494" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;nonparametric&gt;" Title="Local linear regression: number of grid points?" ViewCount="46" />
  <row AnswerCount="1" Body="&lt;p&gt;I have this function $P = f(\alpha)$. $\alpha$ is a function $\alpha = f(\theta, x)=\theta x$. Now I have&lt;/p&gt;&#10;&#10;&lt;p&gt;$P = \alpha(x_1+x_2 + ...x_n)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I need to calculate the partial derivative of P wrt $\theta$. Then&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{\partial P}{\partial \theta} = \frac{\partial P}{\partial \alpha} * \frac{\partial \alpha}{\partial \theta} = (x_1+x_2+x_3)*\frac{\partial \alpha}{\partial \theta}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now what I am trying to do is use gradient descent to maximize P wrt $\theta$. So what I was expecting was the partial derivative wrt $\theta$ to be a single value. However, Here $\frac{\partial \alpha}{\partial \theta} = \frac{\partial \theta x}{\partial \theta}$ changes wrt to x. So I am a bit confused how to get a single value out of it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-19T16:25:33.063" Id="73234" LastActivityDate="2013-10-19T18:27:56.793" OwnerUserId="12329" PostTypeId="1" Score="-1" Tags="&lt;gradient-descent&gt;" Title="Confusion related to calculation of partial derivative" ViewCount="38" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have read this vector manipulation in standard books:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[XX^T] =E[\mathrm{trace}(XX^T)]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $X^T$ is the transpose of $X$, $X$ has a normal distribution and has dimension $n\times 1$, $XX^T$ has dimension $n\times n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can they introduce trace into expectation?&lt;/p&gt;&#10;" ClosedDate="2013-10-19T19:37:48.163" CommentCount="2" CreationDate="2013-10-19T17:44:55.213" Id="73242" LastActivityDate="2013-10-19T20:42:42.320" LastEditDate="2013-10-19T20:42:42.320" LastEditorUserId="805" OwnerUserId="31696" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;linear-algebra&gt;" Title="Trace(AB)=Trace(BA)? even if A, B are vectors?" ViewCount="82" />
  <row Body="&lt;p&gt;The &lt;code&gt;logLik()&lt;/code&gt; function provides the evaluation of the log-likelihood by &lt;em&gt;substituting the ML estimates of the parameters&lt;/em&gt; for the values of the unknown parameters. Now, the maximum likelihood estimates of the regression parameters (the $\beta_j$'s in $X{\boldsymbol \beta}$) coincide with the least-squares estimates, but the ML estimate of $\sigma$ is $\frac{\sum \hat\epsilon_i^2}{n}$, whereas you are using $\hat\sigma = \sqrt{\frac{\sum \hat\epsilon_i^2}{n-2}}$, that is the square root of the unbiased estimate of $\sigma^2$.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;  n &amp;lt;- 5&#10;&amp;gt;  x &amp;lt;- 1:n&#10;&amp;gt;  set.seed(1)&#10;&amp;gt;  y &amp;lt;- 10 + 2*x + rnorm(n, 0, 2)&#10;&amp;gt;  modlm &amp;lt;- lm(y ~ x)&#10;&amp;gt;  sigma &amp;lt;- summary(modlm)$sigma&#10;&amp;gt; &#10;&amp;gt;  # value of the likelihood with the &quot;classical&quot; sigma hat&#10;&amp;gt;  sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma)))&#10;[1] -9.192832&#10;&amp;gt; &#10;&amp;gt;  # value of the likelihood with the ML sigma hat&#10;&amp;gt;  sigma.ML &amp;lt;- sigma*sqrt((n-dim(model.matrix(modlm))[2])/n) &#10;&amp;gt;  sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma.ML)))&#10;[1] -8.915768&#10;&amp;gt;  logLik(modlm)&#10;'log Lik.' -8.915768 (df=3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2013-10-19T18:45:53.120" Id="73245" LastActivityDate="2013-10-19T18:45:53.120" OwnerUserId="8402" ParentId="73196" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I developed an index value (vulnerability score scale of 0 to 1) using a series of variables. I would like to regress these variables with the index value to determine the relative predictive power of each variable. Can I do this? &lt;/p&gt;&#10;&#10;&lt;p&gt;I ran the regression and came up with standardized B coefficients. I then interpreted those as relative contribution of the variable towards predicting the index value (vulnerability score). &lt;/p&gt;&#10;&#10;&lt;p&gt;I know one cannot regress a variable against itself but I am essentially doing this but primarily am just looking at determining to what degree each variable predicts the indexed value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any insights would be helpful if this is a proper use of regression or if there is an alternative method to assess this. Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-19T20:29:44.800" Id="73251" LastActivityDate="2014-04-19T19:19:20.203" OwnerUserId="31698" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="Can I regress an index value with variables used to create the index?" ViewCount="285" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Wikipedia says now, here in the introduction:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Student%27s_t-distribution&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;... then the t-distribution (for n-1) can be defined as the distribution of the location of the true mean, relative to the sample mean and divided by the sample standard deviation... In this way the t-distribution can be used to estimate how likely it is that the true mean lies in any given range.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this right?  It seems not right to me.  How can we have a distribution on the true mean after obtaining a sample, without some sort of Bayesian prior?  I understand we can get a confidence interval for the true mean.  But a distribution?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-20T03:31:09.140" FavoriteCount="1" Id="73268" LastActivityDate="2013-10-20T06:18:16.167" OwnerUserId="31703" PostTypeId="1" Score="3" Tags="&lt;t-distribution&gt;" Title="Can the t-distribution be defined as the distribution on the true mean of a sampled normal?" ViewCount="65" />
  
  <row AcceptedAnswerId="73289" AnswerCount="1" Body="&lt;p&gt;I just ran my first ever PCA, so please excuse any naivety on my part.&lt;/p&gt;&#10;&#10;&lt;p&gt;As input, I used five years worth of the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;    &lt;li&gt;&lt;a href=&quot;http://au.spindices.com/indices/equity/sp-asx-200-a-reit-sector&quot; rel=&quot;nofollow&quot;&gt;S&amp;P/ASX 200 A-REIT&lt;/a&gt;&lt;/li&gt;&#10;    &lt;li&gt;&lt;a href=&quot;http://au.spindices.com/indices/equity/sp-asx-200-consumer-discretionary-sector&quot; rel=&quot;nofollow&quot;&gt;S&amp;P/ASX 200 Consumer Discretionary&lt;/a&gt;&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Consumer Staples&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Energy&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Financial-x-A-REIT&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Health Care&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Industrials&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Information Technology&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Materials&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Resources&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Telecommunication Services&lt;/li&gt;&#10;    &lt;li&gt;S&amp;P/ASX 200 Utilities&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Using R, I simply ran the following commands:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;arc.pca1 &amp;lt;- princomp(sp_sector_data, scores=TRUE, cor=TRUE)&#10;summary(arc.pca1)&#10;plot(arc.pca1)&#10;biplot(arc.pca1)&#10;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Summary&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&#10;Importance of components:&#10;                         Comp.1     Comp.2     Comp.3&#10;Standard deviation     2.603067 1.05203261 0.88394057&#10;Proportion of Variance 0.564663 0.09223105 0.06511258&#10;Cumulative Proportion  0.564663 0.65689405 0.72200662&#10;&#10;                           Comp.4     Comp.5     Comp.6&#10;Standard deviation     0.84122312 0.76978259 0.73901015&#10;Proportion of Variance 0.05897136 0.04938044 0.04551133&#10;Cumulative Proportion  0.78097798 0.83035842 0.87586975&#10;&#10;                           Comp.7     Comp.8     Comp.9&#10;Standard deviation     0.66409102 0.62338449 0.52003850&#10;Proportion of Variance 0.03675141 0.03238402 0.02253667&#10;Cumulative Proportion  0.91262116 0.94500518 0.96754185&#10;&#10;                          Comp.10    Comp.11      Comp.12&#10;Standard deviation     0.45637805 0.42371864 0.0409804189&#10;Proportion of Variance 0.01735674 0.01496146 0.0001399496&#10;Cumulative Proportion  0.98489859 0.99986005 1.0000000000&#10;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Loadings&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&#10;Loadings:&#10;        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7&#10;RE      -0.235         0.520 -0.533 -0.438  0.355 -0.150&#10;disc    -0.332               -0.125                0.294&#10;staples -0.295  0.226                      -0.211  0.554&#10;energy  -0.332 -0.251         0.172  0.176        -0.130&#10;fin_RE  -0.323               -0.118 -0.130         0.384&#10;health  -0.224  0.465 -0.124 -0.193  0.603  0.537 -0.112&#10;ind     -0.337                                          &#10;IT      -0.224  0.145 -0.757        -0.461        -0.312&#10;mat     -0.329 -0.351         0.295         0.126 -0.116&#10;res     -0.335 -0.350         0.297         0.123 -0.133&#10;telco   -0.161  0.609  0.327  0.609 -0.311        -0.113&#10;util    -0.270  0.160  0.146 -0.256  0.234 -0.694 -0.509&#10;&#10;        Comp.8 Comp.9 Comp.10 Comp.11 Comp.12&#10;RE      -0.217                               &#10;disc     0.309  0.567  0.596                 &#10;staples -0.688        -0.141                 &#10;energy         -0.215  0.240  -0.783  -0.165 &#10;fin_RE   0.374 -0.724          0.207         &#10;health                                       &#10;ind      0.398  0.311 -0.743  -0.221         &#10;IT      -0.183                               &#10;mat     -0.127                 0.461  -0.638 &#10;res     -0.123                 0.226   0.752 &#10;telco    0.116                               &#10;util                           0.115         &#10;&#10;               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6&#10;SS loadings     1.000  1.000  1.000  1.000  1.000  1.000&#10;Proportion Var  0.083  0.083  0.083  0.083  0.083  0.083&#10;Cumulative Var  0.083  0.167  0.250  0.333  0.417  0.500&#10;&#10;               Comp.7 Comp.8 Comp.9 Comp.10 Comp.11&#10;SS loadings     1.000  1.000  1.000   1.000   1.000&#10;Proportion Var  0.083  0.083  0.083   0.083   0.083&#10;Cumulative Var  0.583  0.667  0.750   0.833   0.917&#10;&#10;               Comp.12&#10;SS loadings      1.000&#10;Proportion Var   0.083&#10;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Scree Plot&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6edCJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Biplot&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QrCyS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Is this useful?&lt;/h3&gt;&#10;&#10;&lt;p&gt;Am I right in assuming that these indices are correlated with each other?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does the biplot show some sort of clustering?&lt;/p&gt;&#10;&#10;&lt;p&gt;What if anything, does any of this mean?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-10-20T13:28:51.057" FavoriteCount="1" Id="73286" LastActivityDate="2013-10-20T14:15:13.610" LastEditDate="2013-10-20T14:15:13.610" LastEditorUserId="22468" OwnerUserId="31710" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;clustering&gt;&lt;pca&gt;&lt;biplot&gt;" Title="Interpretation of PCA biplot?" ViewCount="1024" />
  
  
  
  <row AcceptedAnswerId="73331" AnswerCount="2" Body="&lt;p&gt;I am looking at my statistics book and an article online for linear regression and was wondering if anyone can verify that these two equations are entirely different. Consider the equation $\hat{y} = ax + b$&lt;/p&gt;&#10;&#10;&lt;p&gt;In my book, a and b are :&lt;/p&gt;&#10;&#10;&lt;p&gt;$a = \frac{r \cdot S_{y}}{S_{x}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$b = \bar{y} - a\bar{x}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$r = \sum \frac{(x_{i} - \bar{x})(y_{i} -\bar{y})}{S_{x}S_{y}(n-1)}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$\displaystyle S_{y} = \sqrt{ \frac{\sum (y_i - \bar{y})^{2}}{(n-1)} }$&lt;/p&gt;&#10;&#10;&lt;p&gt;From one online article, a and b are:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\displaystyle a = \frac{n \sum x_{i}y_{i} - \sum x_{i} \sum y_{i}}{n \sum x^2_{i} - (\sum x_{i})^2}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$b = \bar{y} - a\bar{x}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The a from the online article vaguely looks like covariance in the numerator and the denominator looks like variance but for only one random variable, not two. Can someone explain the discrepancy (if there are any) and construct an argument for my book's choice? I can understand the second formulation mainly because it comes from setting partial derivatives to zero to minimize an objective function and then finding the coefficients a and b. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-20T22:32:47.917" FavoriteCount="1" Id="73322" LastActivityDate="2013-10-22T12:48:53.930" LastEditDate="2013-10-20T22:46:13.933" LastEditorUserId="31672" OwnerUserId="31672" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;mathematical-statistics&gt;" Title="Explain Statistics: Matching formulas for linear regression" ViewCount="141" />
  <row AnswerCount="1" Body="&lt;p&gt;6 light bulbs are chosen at random from 17 bulbs of which 6 are defective. &lt;/p&gt;&#10;&#10;&lt;p&gt;(a) What is the probability that exactly 2 are defective? &lt;/p&gt;&#10;&#10;&lt;p&gt;(b) What is the probability that at most 1 is defective?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-21T00:38:08.623" Id="73332" LastActivityDate="2013-10-21T01:52:08.647" LastEditDate="2013-10-21T01:09:48.807" LastEditorUserId="31733" OwnerUserId="31733" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Probability that items chosen randomly are defective?" ViewCount="38" />
  <row Body="&lt;p&gt;As Steffen pointed out, the example matrix encodes the number of times a word appears in a text. The position of the encoding into the matrix is given by the word (column position on the matrix) and by the text (row position on the matrix). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, The hashing trick works the same way, though you don't have to initially define the dictionary containing the column position for each word. &lt;/p&gt;&#10;&#10;&lt;p&gt;In fact it is the hashing function that will give you the range of possible column positions (the hashing function will give you a minimum and maximum value possible) and the exact position of the word you want to encode into the matrix. So for example, let's imagine that the word &quot;likes&quot; is hashed by our hashing function into the number 5674, then the column 5674 will contain the encodings relative to the word &quot;likes&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In such a fashion you won't need to build a dictionary before analyzing the text. If you will use a sparse matrix as your text matrix you won't even have to define exactly what the matrix size will have to be. Just by scanning the text, on the fly, you will convert words into column positions by the hashing function and your text matrix will be populated of data (frequencies, i.e.) accordingly to what document you are progressively analyzing (row position).  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-21T11:02:59.820" Id="73354" LastActivityDate="2013-10-21T17:21:41.650" LastEditDate="2013-10-21T17:21:41.650" LastEditorUserId="20370" OwnerUserId="20370" ParentId="73325" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;There's &lt;a href=&quot;http://books.google.co.uk/books?id=F78QmEmce_gC&quot; rel=&quot;nofollow&quot;&gt;Barnett (2009), &lt;em&gt;Comparative Statistical Inference&lt;/em&gt;&lt;/a&gt;, which does a good job of contrasting different methodologies, with only as much maths as necessary. Nothing on Machine Learning though&amp;mdash;some of the references &lt;a href=&quot;http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning&quot;&gt;here&lt;/a&gt; may be useful (&amp;amp; indeed the answers).&lt;/p&gt;&#10;&#10;&lt;p&gt;A survey of different areas of application (psychometrics, econometrics, &amp;amp;c.) would be interesting, &amp;amp; I hope someone can suggest one.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-01-19T17:08:43.427" CreationDate="2013-10-21T13:48:28.970" Id="73370" LastActivityDate="2013-10-21T13:48:28.970" OwnerUserId="17230" ParentId="73281" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="73401" AnswerCount="1" Body="&lt;p&gt;When there are two change points in  a piecewise constant hazard model then the density function becomes some triangle exponential distribution. In this situation I can't generate the survival time from the CDF using probability integral transformation. Can any one help me to generate the survival time from this model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-21T14:57:14.690" Id="73373" LastActivityDate="2014-04-07T18:05:40.923" LastEditDate="2013-10-21T16:18:07.280" LastEditorUserId="7290" OwnerUserId="31753" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;distributions&gt;&lt;survival&gt;&lt;random-generation&gt;" Title="Generating survival times for a piecewise constant hazard model with two change points" ViewCount="667" />
  <row AcceptedAnswerId="73388" AnswerCount="2" Body="&lt;p&gt;I came across a result in a time series textbook the other day and have not been able to understand why it is true (the authors don't give a proof but just state it as true).  I want to show that the eigenvalues of the matrix $\mathbf{G}$ given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$G=&#10;\begin{pmatrix}&#10;\phi_1&amp;amp;\phi_2 &amp;amp;\phi_3 &amp;amp;...&amp;amp;\phi_{p-1} &amp;amp; \phi_p\\&#10;1 &amp;amp; 0 &amp;amp;0 &amp;amp;...&amp;amp; 0 &amp;amp;0\\&#10;0 &amp;amp; 1 &amp;amp; 0 &amp;amp;... &amp;amp;0 &amp;amp;0\\&#10;\vdots &amp;amp; &amp;amp; &amp;amp; \ddots&amp;amp;0&amp;amp;0\\&#10;0 &amp;amp; 0 &amp;amp;...&amp;amp;...&amp;amp;1 &amp;amp;0&#10;\end{pmatrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;correspond to the reciprocal roots of the $AR(p)$ characteristic polynomial&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Phi(u)=1-\phi_1u-\phi_2u^2-...-\phi_pu^p$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The one thing i was able to deduce is that the eigenvalues of $\mathbf{G}$ must satisfy&#10;$$\lambda^p-\phi_1\lambda^{p-1}-\phi_2\lambda^{p-2}-...-\phi_{p-1}-\phi_p=0$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-21T15:39:32.187" Id="73378" LastActivityDate="2013-10-21T19:22:36.117" OwnerUserId="30490" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;self-study&gt;" Title="Reciprocal roots and eigenvalues relationship in time series" ViewCount="116" />
  <row Body="&lt;p&gt;Regular linear regression (e.g. the &lt;code&gt;lm&lt;/code&gt; or &lt;code&gt;glm&lt;/code&gt; functions in R) handles negative values just fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;One model you could try would be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model1 &amp;lt;- lm(points ~ away + opponent_fact_1 + opponent_fact_2, data=my_data_frame)&#10;summary(model1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you've got a lot of data (and several rows per player and per opponent), you could also try this model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model2 &amp;lt;- lm(points ~ away + factor(player_id) + factor(opponent_team_id), data=my_data_frame)&#10;summary(model2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This will give you a model that includes a coefficient for each player, and for each opponent_team_id.  These coefficients will represent the average points expected for a player, as well as the average points expected against a given opponent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Have you every run a regression model before?  What's the goal of this analysis?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-21T16:16:55.103" Id="73381" LastActivityDate="2013-10-21T16:16:55.103" OwnerUserId="2817" ParentId="73371" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The standard deviation of the mean is usually unknown. We would write it as $$  \sigma_{\bar x } ={\sigma \over \sqrt n} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard error of the mean is an estimate of the standard deviation of the mean. $$ \hat \sigma_{\bar x}  = {s \over \sqrt n}.  $$ &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-21T17:57:03.800" Id="73394" LastActivityDate="2013-10-21T17:57:03.800" OwnerUserId="24073" ParentId="73390" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;What is the preferred model to analyze firms' R&amp;amp;D decision and intensity?&lt;br&gt;&#10;Normally the questionaires include two steps:   &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;firms are asked whether they engage in R&amp;amp;D (binary variable (yes/no))&lt;br&gt;&#10;(around 70 % answer no)   &lt;/li&gt;&#10;&lt;li&gt;If yes, firms report how much they invest.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hence, 70% of all firms  report 0 investment, while 30% &gt; 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen several papers using different models. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Simple OLS &lt;/li&gt;&#10;&lt;li&gt;Heckman selection model (normally without an exclusion restriction)&lt;/li&gt;&#10;&lt;li&gt;Zero-inflated count model (Possion or negative binominal)&lt;/li&gt;&#10;&lt;li&gt;Hurdle model&lt;/li&gt;&#10;&lt;li&gt;?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What is the best model for this kind of data?&lt;br&gt;&#10;How do I know that this particular model is the most appropriate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-21T18:17:03.087" Id="73396" LastActivityDate="2013-10-21T18:17:03.087" OwnerUserId="31761" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;model-selection&gt;" Title="Model R&amp;D decision and intensity" ViewCount="25" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I am interested in sampling many pairs $(\mathbf X, Y)$ from some distribution $f(\mathbf x, y)$ where $\mathbf x \in \mathbb R^p$, $p$ large ; I am interested in both exact and approximate simulations. $f(\mathbf x)$ is easy to sample from, but $f(y \mid \mathbf x)$ is not. &lt;/p&gt;&#10;&#10;&lt;p&gt;For motivation, I could do Gibbs sampling if the distributions $f(\mathbf x\mid y)$  and $f(y \mid \mathbf x)$ were both easy to simulate from by initializing $(\mathbf X_0, Y_0)$ and drawing $\mathbf X_t \sim f(x \mid Y_{t-1})$ and $Y_t \sim f(y \mid \mathbf X_t)$. If $f(\mathbf x \mid y)$ is easy to sample from, then I am in really good shape because, worst case scenario, I can replace $f(y \mid \mathbf x)$ with any update that leaves this distribution invariant. &lt;/p&gt;&#10;&#10;&lt;p&gt;In my situation, $f(\mathbf x \mid y)$ is difficult to sample from, and substantial work would have to go into constructing a suitable transition kernel given the dimension of $\mathbf x$. However, I can draw from the marginal $f(\mathbf x)$ exactly! It seems like this should buy me something like it does with the Gibbs sampler, however if I do something like the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Draw $\mathbf X_t \sim f(\mathbf x)$;&lt;/li&gt;&#10;&lt;li&gt;Draw $Y_t \sim K(y \mid \mathbf X_t, Y_{t-1})$ where $K(\cdot \mid \mathbf x, y)$ leaves $f(y \mid \mathbf x)$ invariant,&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I believe I will not get the correct stationary distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;If it helps, I might be willing to evaluate the density $f(y \mid \mathbf x)$. I know $f(y \mid \mathbf x)$ up-to a normalizing constant, but I can afford to do one numerical integration to get the constant (I would really prefer not to, though). However, $f(y \mid \mathbf x)$ is expensive to compute.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thought I've had is to generate $f(\mathbf x)$ and then do a small number of MH random walk steps, but I would very much prefer getting an answer that will at least converge to the correct thing as $t \to \infty$ (this one will always have some error if I do a small number of steps for each $Y_t$). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Maybe this all seems obvious - just use generic sampling techniques for each $f(y \mid \mathbf x)$ and, in general, I shouldn't be able to do better. I guess what I have in mind is that I should somehow be able to get the MCMC to work across $t$, and &lt;em&gt;not&lt;/em&gt; have to do my approximate sampling within $t$. That is, I want my approximation to get better and better as $t \to \infty$; I don't want my approximation to have the same amount of error within $t$. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, though, $f(y \mid \mathbf x)$ might be very different for different values of $\mathbf x$ so I shouldn't hope for a solution that will always work. I just want something that has a good chance of working and addresses the above issues. It is strange to me that $f(\mathbf x)$ would be less useful than $f(\mathbf x \mid y)$. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-21T21:23:55.850" Id="73413" LastActivityDate="2013-10-21T21:30:30.100" LastEditDate="2013-10-21T21:30:30.100" LastEditorUserId="5339" OwnerUserId="5339" PostTypeId="1" Score="1" Tags="&lt;simulation&gt;&lt;monte-carlo&gt;" Title="Approximately sampling $(X, Y)$ when sampling $X$ is easy" ViewCount="29" />
  
  <row AnswerCount="1" Body="&lt;p&gt;In R, I can fit a Probability Density Function to some empirical data using the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;energy &amp;lt;- rnorm(30) * 20&#10;dens &amp;lt;- density(energy)&#10;sum(dens$y)*diff(dens$x[1:2])&#10;hist(energy,probability=TRUE)&#10;lines(density(energy),col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This produces the following graph of the Probability Density Function (PDF):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/roJ99.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Howevever, I would like to fit a student-t distribution to this data instead. I'm wondering if its possible to do this and if its possible to plot the result like in the diagram above?&lt;/p&gt;&#10;" ClosedDate="2013-10-21T23:30:00.013" CommentCount="3" CreationDate="2013-10-21T22:48:43.223" Id="73416" LastActivityDate="2013-10-21T23:20:11.733" LastEditDate="2013-10-21T23:10:10.283" LastEditorUserId="20666" OwnerUserId="20666" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;pdf&gt;&lt;kernel-density-estimate&gt;&lt;ggplot&gt;" Title="In R, how do I fit a student-t distribution to a set of empirical data?" ViewCount="1172" />
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I am interested in deepening my Asymptotic Theory understanding. My current knowledge is that of a typical PhD student (from a decent University), say at the level of Green's textbook. Are there any good book(s) that you would recommend?&lt;/p&gt;&#10;&#10;&lt;p&gt;Much appreciated. &lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2013-10-22T11:56:10.263" CreationDate="2013-10-22T10:29:40.320" FavoriteCount="2" Id="73455" LastActivityDate="2013-10-22T15:12:12.233" LastEditDate="2013-10-22T14:39:30.687" LastEditorUserId="5739" OwnerUserId="31743" PostTypeId="1" Score="5" Tags="&lt;econometrics&gt;&lt;asymptotics&gt;" Title="Asymptotic Theory in Economics" ViewCount="473" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Currently I am using rfe function in the &quot;caret&quot; package to do feature selection. There are 380 variables as input candidates. I have done many trials and I noticed that something weird always happens, as the rfe tends to output all candidate variables as the best subset while I have set the parameter size to be 5 to 10. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering whether anyone else has come across this problem. Any help will be appreciated. Thanks in advance for any help. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-22T13:13:29.150" Id="73469" LastActivityDate="2013-10-22T13:13:29.150" OwnerUserId="31788" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;feature-selection&gt;&lt;caret&gt;" Title="Recursive Feature Elimination Fails to Output as Expected" ViewCount="33" />
  <row AnswerCount="0" Body="&lt;p&gt;I have one sample and three sub-samples and I would like to calculate the p-value for the hypothesis that the proportion in the sub-sample is not different from the proportion in another sub-sample or not different from the proportion in the sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;The sample consists of counting how often there is agreement between one individual and another (1 if they agree and 0 otherwise). The first sub-sample removes some observations from the sample. The second sub-sample removes some observations from the first sub-sample. The third sub-sample removes other observations from the first sub-sample. The sample sizes of the sample and sub-samples are 400, 340, 260, 170. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have computed the 95 % confidence interval and find that these largely overlap for the first three, but the fourth confidence interval overlaps little if at all with the other three. I have also computed the p-values for the hypothesis that the proportions are the same under the assumption that the samples are independent. The results here are of course the same. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I would like to know is how large the effect is when the assumption of independence is violated, does anyone know by how much it can skew the p-values? Otherwise I'd like to know if there is a better way to compute the p-values for this problem? Should I use the McNemar test (I encountered it in some of the answers around the web, though I am not sure if it applies)? Preferably I'd also like to know how to compute the power of such a test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers,&lt;/p&gt;&#10;&#10;&lt;p&gt;Martin&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-22T14:36:50.403" Id="73476" LastActivityDate="2013-10-22T14:36:50.403" OwnerUserId="31790" PostTypeId="1" Score="1" Tags="&lt;proportion&gt;&lt;sample&gt;&lt;mcnemar-test&gt;&lt;subset&gt;" Title="Comparing the proportion of a subsample to the proportion of sample or another" ViewCount="155" />
  <row Body="" CommentCount="0" CreationDate="2013-10-22T15:20:37.460" Id="73481" LastActivityDate="2013-10-22T15:20:37.460" LastEditDate="2013-10-22T15:20:37.460" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;Restricted Boltzmann Machines are Markov random fields that usually have complete connectivity between the hidden and visible variables.  If we drop the &quot;restricted&quot; part and look at &lt;a href=&quot;http://www.scholarpedia.org/article/Boltzmann_machine&quot; rel=&quot;nofollow&quot;&gt;Boltzmann Machines&lt;/a&gt; more generally, there can also be complete connectivity among the visible variables (pixels).&lt;/p&gt;&#10;&#10;&lt;p&gt;So yes, they're possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't read the whole paper you linked to, but here is my assessment of what they mean.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;If $p(x)$ is an MRF, and all the elements of $y$ are conditionally independent of one another given $x$, then $p(x, y)$ is also an MRF.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If the $y$ variables are &lt;em&gt;not&lt;/em&gt; conditionally independent of one another, then this will not &lt;em&gt;necessarily&lt;/em&gt; be true (although it still could be).  For instance, if there were directed connections among the $y$ variables, then you wouldn't have an MRF anymore.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;However, the case you seem to be thinking about, where there are undirected connections between the variables (like the Boltzmann Machines I linked to) are still considered MRFs because all their pieces are MRFs.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-22T16:34:44.577" Id="73491" LastActivityDate="2013-10-22T16:34:44.577" OwnerUserId="4862" ParentId="73484" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You may want to consider other metrics such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2&quot; rel=&quot;nofollow&quot;&gt;adjusted R square&lt;/a&gt;, or &lt;a href=&quot;http://en.wikipedia.org/wiki/Mallows%27s_Cp&quot; rel=&quot;nofollow&quot;&gt;Mallow's Cp&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-22T18:10:17.457" Id="73503" LastActivityDate="2013-10-22T18:33:26.177" LastEditDate="2013-10-22T18:33:26.177" LastEditorUserId="7290" OwnerUserId="31799" ParentId="73451" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;How does one randomly sample from a T-distribution in R. From what I've found, the function &lt;code&gt;rt&lt;/code&gt; in R doesn't let you specify the mean and standard deviation. For a normal distribution it is simply &lt;code&gt;rnorm(x,mu,sd)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;For the t-distribution there is a &lt;em&gt;central&lt;/em&gt; and a &lt;em&gt;non-central&lt;/em&gt; version. I want to know the difference between the two.  In addition, if I want to specify the mean and standard deviation, does that automatically mean I am dealing with the non-central version of the t-distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;I chose to use the t-distribution because the data I am using are rather fat tailed and a t-distribution with a low degrees of freedom seems like a good idea. What other distributions are there for handling fat tailed data? Also it would be great if you can specify the function in R too.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-10-22T20:10:35.257" Id="73513" LastActivityDate="2013-10-23T08:22:09.563" LastEditDate="2013-10-22T21:45:41.263" LastEditorUserId="7290" OwnerUserId="11048" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;distributions&gt;&lt;random-generation&gt;&lt;t-distribution&gt;&lt;non-central&gt;" Title="Generate data from a t-distribution with specified mean and standard deviation" ViewCount="472" />
  <row Body="&lt;p&gt;While it's possible to do it recursively for fixed degrees of freedom (write the cdf for a given d.f. in terms of the cdf for lower degrees of freedom, and the integrals foir the two lowest-integer df may be done directly), I've never seen anyone try to implement it that way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some algorithms for the cdf of the t are based on the incomplete beta function (which is a commonly used function in various parts of mathematics or physics). &lt;/p&gt;&#10;&#10;&lt;p&gt;There are some for the inverse cdf (quantile function) based on ratios of polynomials.&lt;/p&gt;&#10;&#10;&lt;p&gt;Plain googling on &lt;em&gt;algorithm cdf|&quot;distribution function&quot; student t&lt;/em&gt; turns up plenty of references within the pages linked (e.g. &lt;a href=&quot;http://devdoc.madlib.net/v0.2beta/student_8cpp.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;), such as Abramowitz and Stegun's &lt;em&gt;Handbook of Mathematical Functions&lt;/em&gt; (which gives some small-d.f.-exact and approximate calculations), and various other books and papers. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want the noncentral t (e.g. for power calculations) a standard reference is Lenth, R. V. 1989. &quot;Algorithm AS 243: Cumulative distribution function of the noncentral t distribution&quot;. &lt;em&gt;Applied Statistics&lt;/em&gt;, &lt;strong&gt;38&lt;/strong&gt;, 185-189.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-22T20:23:11.970" Id="73515" LastActivityDate="2013-10-22T20:23:11.970" OwnerUserId="805" ParentId="72764" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="73548" AnswerCount="1" Body="&lt;p&gt;The following appeared on an assignment of mine (already turned in). I contend that not enough information is given to provide an answer.... it seems pretty cut and clear to me. However, instructor insisted it's solvable in minitab. Can you help me figure out what I'm not understanding?  &lt;/p&gt;&#10;&#10;&lt;p&gt;How do you solve this without a model of distribution of weekly demand, or at least an average value to use as constant approximation. I must be missing something simple.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem:  &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a service company.  &lt;/p&gt;&#10;&#10;&lt;p&gt;10% of the weekly demand is for a service category named &quot;X&quot; [Assume service categories are mutually exclusive].  &lt;/p&gt;&#10;&#10;&lt;p&gt;The company must revise their resource plan iff there are too few customer orders(less than one/week) or too many customer orders (more than five/week) of service category &quot;X&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the next 12 weeks, what is the probability that the company will &lt;strong&gt;not&lt;/strong&gt; need to revise the resource plan?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-23T05:34:47.950" Id="73545" LastActivityDate="2013-10-23T06:26:43.607" OwnerUserId="21972" PostTypeId="1" Score="1" Tags="&lt;binomial&gt;&lt;poisson&gt;&lt;proportion&gt;" Title="Basic binomial question" ViewCount="62" />
  <row Body="&lt;p&gt;This depends on the total number of customer orders; consider the situation if you have just one order per week. Then you are almost certainly have less than one in any given week. OTH if you have 1000 customers, you will have about 100 ordering &quot;X&quot; each week which is too much.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's also not clear if the 10% is an average or a fixed number; the same is the case for the missing number of orders per week. The most likely way to interpret this question is to assume that each customer order has a chance of 10% of belonging to category &quot;X&quot; - but then we will still need the number of customer orders. If the number of orders is fixed then X the number of orders of &quot;X&quot; per week would be binomially distributed and the question would be solvable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think it is really questionable to claim that it is &quot;solvable in Minitab&quot; and give no theoretical background. There may be a button in Minisab that takes these numbers and gives an answer, but is it the answer to the question as it is stated here?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Short version, I agree with you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-23T06:26:43.607" Id="73548" LastActivityDate="2013-10-23T06:26:43.607" OwnerUserId="10524" ParentId="73545" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="73612" AnswerCount="2" Body="&lt;p&gt;My question seems to be very basic one but my search has not given any similar question. I have small dataset of 8 $(x,y)$ values with uncertainties for $y$ (dependent variable) and the theory predicts quadratic dependence $y=a x^2 + b x + c$. I wish to fit this dataset to the quadratic equation and to calculate confidence intervals for the parameters $a$, $b$, $c$ based both on the residuals (they are very small, i.e. the quadratic model describes the experimental data with very small residuals) and on declared uncertainty for every measurement (the uncertainties are much bigger than the residuals). What is the most correct way to do this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-23T16:37:54.700" Id="73594" LastActivityDate="2013-10-24T17:00:17.517" LastEditDate="2013-10-24T07:29:39.930" LastEditorUserId="25283" OwnerUserId="25283" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;fitting&gt;&lt;uncertainty&gt;" Title="Parameter confidence intervals which include errors in data" ViewCount="89" />
  <row Body="&lt;p&gt;An interesting and topical issue in risk modelling. In my experience, risk models for credit which are developed on shorter periods of data tend to produce unstable coefficients - cross validation and/or out-of-time testing for model performance have usually shown this to be the case.&lt;/p&gt;&#10;&#10;&lt;p&gt;For stress testing of credit risk models, we are concerned with estimating probability of default (PD, probability of customer not repaying a loan) and loss given default (LGD, proportion of loan lost in event of default) during downturn macroeconomic conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of regulatory perspectives for stress testing, the &lt;a href=&quot;http://www.bis.org/bcbs/about.htm&quot; rel=&quot;nofollow&quot;&gt;Basel Committee on Banking Supervision (BCBS)&lt;/a&gt;, which can be considered the central bank of central banks, indicates a minimum of 5 to 7 years of data (dependent on model and portfolio type) for model development, unless strong evidence is shown that more recent data is more predictive. Banking regulators typically adhere to these time periods as part of Basel II and III standards for calculating regulatory capital.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, the time period of 2008 to 2009 would seem to be somewhat short as the financial crisis has persisted for a longer period in some countries, e.g. UK where macro economic conditions worsened in 2007. It could be argued that the crisis is still ongoing in some parts of the world.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-23T16:46:43.820" Id="73595" LastActivityDate="2013-10-23T16:46:43.820" OwnerUserId="30121" ParentId="71990" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="73630" AnswerCount="1" Body="&lt;p&gt;My overall question is: why use &lt;code&gt;bayesglm&lt;/code&gt; instead of other classification methods? &lt;/p&gt;&#10;&#10;&lt;p&gt;Note:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I'm only interested in prediction.&lt;/li&gt;&#10;&lt;li&gt;I have a decent amount of data (~ 100,000 obs). &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I feel like the sample size is large enough the parameters of a regular logistic regression are going to be normally distributed (CLT). What would I gain by specifying priors? My hunch is that it will only matter for a small dataset, but I don't have any theoretical or applied evidence.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-23T17:25:23.237" Id="73602" LastActivityDate="2013-10-23T20:34:04.577" OwnerUserId="10500" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;generalized-linear-model&gt;" Title="Why use bayesglm?" ViewCount="305" />
  <row Body="&lt;p&gt;For (a), a simple way to look at is that you've reduced your probability space to only the combinations that have at least one boy:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;BB = 1/3&#10;BG = 1/3&#10;GB = 1/3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;GG is no longer a possibility based on the fact that your neighbor said he had at least one boy.  Of the possibilities remaining, you're left with a 2/3 probability that he has a girl.  The information he gave you reduced the probability of him having a girl from 3/4 to 2/3. Formally, this can be shown as follows:  $$P(At\ least\ one\ girl|At\ least\ one\ boy) = \frac{P(At\ least\ one\ girl\ \cap At\ least\ one\ boy)}{P(At\ least\ one\ boy)}  $$&#10;From your original box, we can see the probability of having at least one boy and at least one girl is BG + GB = 0.25 + 0.25 = 0.5, but we need to divide by the probability of at least one boy, which is BB + BG + GB = 0.25 + 0.25 + 0.25 = 0.75, so we get $\frac{\tfrac{1}{2}}{\tfrac{3}{4}} = \frac{2}{3}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For (b), now that we've seen a boy, the only uncertainty remaining is the gender of the other child, and given no other information, the probability of the other child being female is 1/2, which is the answer.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-23T17:41:54.690" Id="73607" LastActivityDate="2013-10-23T17:41:54.690" OwnerUserId="27342" ParentId="73593" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Sheldon, Sheldon. How comes that you have to ask a question about math to people like &lt;em&gt;us&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;In survival analysis, your setting is called &quot;competing risk&quot;. The joint distribution of the earliest failure time and the type of failure is fully described by the so called &quot;cumulative incidence function&quot; (it even allows for censoring, i.e. no failure until end of time horizon). I am quite sure that you will find relevant information in the literature stated in&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/13935/assumptions-and-pitfalls-in-competing-risks-model&quot;&gt;Assumptions and pitfalls in competing risks model&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-23T19:04:02.220" Id="73622" LastActivityDate="2013-10-23T19:04:02.220" OwnerUserId="30351" ParentId="73410" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I will try to reply using empirical evidence. Let's assume you are measuring the heights in a men sample. In this case, the outlier will be represented by a very tall men (a giant). It is very likely this men will represent an outlier also for other variables like for instance shoe size or arms lengths and so on.&#10;Other case, you are measuring financial performance of US Public company. An outlier will be a very successful company with a sales growth twice the industry average. Very likely the same company will be an outlier in respect of any measure of profitability or stock price appreciation. In a nut shell, I am incline to think something behaving exceptionally out of the norm will tend to conserve this property across different manifestations. Is there a theorem that disprove this theory? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-10-23T19:56:26.190" Id="73625" LastActivityDate="2013-10-23T19:56:26.190" OwnerUserId="31824" ParentId="73613" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In my industry it is common to test a sample of 20-30 and then use that data to draw conclusions about the reliability of the product with a certain confidence.  We have tables for such things but it appears that for the case of 0 failures in the sample, the &quot;Success Run Theorem&quot; is used.  In my references this appears as:&#10;$$R_c = (1-C)^{\frac{1}{(n+1)}}$$&#10;where&#10;$C$ = confidence level,&#10;$R$ = reliability at confidence level $C$, and&#10;$n$ = sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I cannot find an explanation of how to get to the above equation from Bayes' theorem: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KH5f9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Every attempt to talk myself through Bayes theorem to arrive at the Success Run Theorem gets me confused.  Even more confusing is when I try to extend my understanding to cases where some failures are observed in the sampling.  Then I know to use this formula: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/22Fvx.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/9rogV.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But again I don't understand where it comes from (binomial?) or how it relates to the above two other formulas, if at all.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My specific question would be how you go from Bayes Theorem (written as probabilities) to the Success Run Theorem (written as confidence, reliability, sample size)?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for helping a poor engineer lost in the world of stats.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-23T23:21:54.180" Id="73645" LastActivityDate="2014-02-25T13:00:01.130" LastEditDate="2014-02-25T13:00:01.130" LastEditorUserId="-1" OwnerUserId="31189" PostTypeId="1" Score="1" Tags="&lt;reliability&gt;&lt;bayes&gt;&lt;confidence&gt;" Title="How do you derive the Success-Run Theorem from the traditional form of Bayes Theorem?" ViewCount="894" />
  <row AcceptedAnswerId="73651" AnswerCount="4" Body="&lt;p&gt;Suppose I have a sample $(X_n,Y_n), n=1..N$ from the joint distribution of $X$ and $Y$. How do I test the hypothesis that $X$ and $Y$ are &lt;strong&gt;independent&lt;/strong&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;No assumption is made on the joint or marginal distribution laws of $X$ and $Y$ (least of all joint normality, since in that case independence is identical to correlation being $0$).&lt;/p&gt;&#10;&#10;&lt;p&gt;No assumption is made on the nature of a possible relationship between $X$ and $Y$; it may be non-linear, so the variables are &lt;em&gt;uncorrelated&lt;/em&gt; ($r=0$) but &lt;em&gt;highly co-dependent&lt;/em&gt; ($I=H$).&lt;/p&gt;&#10;&#10;&lt;p&gt;I can see two approaches:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Bin both variables and use &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot;&gt;Fisher's exact test&lt;/a&gt; or &lt;a href=&quot;http://en.wikipedia.org/wiki/G-test&quot;&gt;G-test&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Pro: use well-established statistical tests&lt;/li&gt;&#10;&lt;li&gt;Con: depends on binning&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Estimate the &lt;em&gt;dependency&lt;/em&gt; of $X$ and $Y$: &lt;a href=&quot;http://en.wikipedia.org/wiki/Mutual_information&quot;&gt;$\frac{I(X;Y)}{H(X,Y)}$&lt;/a&gt; (this is $0$ for independent $X$ and $Y$ and $1$ when they completely determine each other).&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Pro: produces a number with a clear theoretical meaning&lt;/li&gt;&#10;&lt;li&gt;Con: depends on the approximate entropy computation (i.e., binning again)&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Do these approaches make sense?&lt;/p&gt;&#10;&#10;&lt;p&gt;What other methods people use?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-23T23:54:16.613" FavoriteCount="10" Id="73646" LastActivityDate="2014-06-22T16:58:45.130" LastEditDate="2014-06-22T16:58:45.130" LastEditorUserId="13538" OwnerUserId="13538" PostTypeId="1" Score="19" Tags="&lt;hypothesis-testing&gt;&lt;references&gt;&lt;independence&gt;" Title="How do I test that two continuous variables are independent?" ViewCount="4227" />
  <row Body="&lt;p&gt;Define an accuracy metric that reasonably models how you want your algorithm to perform. &lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have a metric in hand you can cross-validate this question and see if it is an improves the performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some common accuracy metrics that model the problem different: Normalized mutual information, Gini on the labels argsorted by the probabilities, Precision, Recall, AUC.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the classes are extremely unbalanced and FN are crucial, you'll see an improvement.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-24T04:00:00.463" Id="73653" LastActivityDate="2013-10-24T04:00:00.463" OwnerUserId="9568" ParentId="73647" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a variable called &quot;obs&quot; and from this variable I generated a new variable called &quot;obs_sub&quot; by excluding all observations for which a dummy variable is equal to one. Now what I want to know is if I remove these observations whether the mean of the sub-sample is equal to the mean of the sample. This is my null hypothesis. &lt;/p&gt;&#10;&#10;&lt;p&gt;I could run a regression of $E[obs|d] = \alpha + \beta \cdot d$, but the regression with the dummy $(d)$ will only tell me whether the coefficient on the dummy is statistically significant from zero. It will not tell me whether the data are unlikely given $E[obs|d=1] = E[obs]$ or is the null hypothesis $\beta=0$ the same as the null hypothesis as $E[obs|d=1] = E[obs]$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers,&lt;/p&gt;&#10;&#10;&lt;p&gt;Martin&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-24T08:43:06.080" Id="73667" LastActivityDate="2013-10-24T14:29:22.767" LastEditDate="2013-10-24T14:29:22.767" LastEditorUserId="31790" OwnerUserId="31790" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;categorical-data&gt;&lt;mean&gt;&lt;sample&gt;" Title="How to compute whether the mean of a non-random sub-sample is statistically different from the mean of the sample?" ViewCount="139" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I imagine this is a somewhat common situation in practice. I am thinking mainly in terms of preclinical drug trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) During the course of the study a new technique is learned or some time/money is freed up to be able to use a new technique as a secondary outcome. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) Some key piece of equipment breaks, the lab member with the skill to perform a technique leaves, or a planned technique is looking like it is yielding inconsistent results for unknown reasons so that outcome is dropped for financial reasons. &lt;/p&gt;&#10;&#10;&lt;p&gt;3) When time comes to analyze data it comes to the knowledge of the researcher that the planned methods are inadequate/inappropriate and they wish to use a different approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Experiment 1 (e.g. test drug A) does not look like it will pan out, so a financial decision is made to stop that experiment and instead test drug B. Alternatively some other part of the experiment may be modified such as cell culture or animal strain.&lt;/p&gt;&#10;&#10;&lt;p&gt;5) Study is stopped prematurely or additional funds are freed up to increase sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can think of more, but you get the idea. Are p values of any use under these circumstances? Should corrections for multiple comparisons be made in the case of #4? At what point does deviation from the plan invalidate the hypothesis testing procedure?&lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to me would be to simply plot the data and describe it in the hopes it may be useful for someone. However this behaviour is discouraged in favor of performing significance/hypothesis tests and the data may not be published if these are not included.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&#10;I was thinking about it and the actual best thing to do is to take the data and come up with a model to explain it that makes a precise prediction that can then be tested. It seems like this should always be the best thing to do, though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-24T19:26:11.043" FavoriteCount="1" Id="73706" LastActivityDate="2013-11-04T01:13:44.870" LastEditDate="2013-10-24T21:53:14.830" LastEditorUserId="31334" OwnerUserId="31334" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;multiple-comparisons&gt;&lt;experiment-design&gt;&lt;p-value&gt;" Title="What is the best way to analyze data if your experimental design changes while running the experiment?" ViewCount="301" />
  <row AcceptedAnswerId="73745" AnswerCount="2" Body="&lt;p&gt;A random variable X={X1..Xm} and Var(X) is the mXm variance-covariance matrix.  Is there an accepted 1-d statistic like variance that may be extracted from the sample variance of data representing a 2-d random variable?  &lt;/p&gt;&#10;&#10;&lt;p&gt;(explanation of the 2d equivalent of a sample variance)&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/49521/how-to-find-variance-between-multidimensional-points/73717#73717&quot;&gt;How to find variance between multidimensional points?&lt;/a&gt;&lt;/p&gt;&#10;" ClosedDate="2013-10-25T16:03:18.167" CommentCount="9" CreationDate="2013-10-24T21:38:34.937" Id="73724" LastActivityDate="2013-10-25T16:27:34.883" LastEditDate="2013-10-25T16:27:34.883" LastEditorUserId="31895" OwnerUserId="31895" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;" Title="1-d extension of 2-d variance?" ViewCount="70" />
  
  <row Body="&lt;p&gt;I am not sure if I really get what is troubling you, but perhaps a couple of hints will help you:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How can I conceptualize the prior of a deterministic variable in Bayesian data analysis? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Formally, deterministic variables don't have a probability distribution (yes you guessed it, since they are not random variables!). No random variable =&gt; no prior.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Now, as far as I understand the first is a deterministic variable and&#10;the second is stochastic.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If by first you mean $\lambda_C$, since it is a function of r.v. $\sigma$, IT IS a random variable.&#10;(just a side note: usually you reserve the term &quot;stochastic&quot; for random quantities involving time, such as a stochastic process)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;However, I have problems conceptualizing the prior of the deterministic variable.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I think I answered this in the first comment. But let me elaborate. In Bayesian statistics there are multiple ways how you can treat a parameter, depending on your previous knowledge. In general you assume the parameter to have an uncertain value, and consider it random. Depending on your beliefs and prior knowledge about its values you can either, use a uniform distribution, or a distribution that favors a certain value, e.g. if you know the precision is most likely centered around a certain value, you can use a Gamma($\alpha$,$\beta$) distributed prior with appropriate values $\alpha$ and $\beta$. Now here is the point that might be tripping you up. Priors are there since sometimes you have a strong belief, but usually you will not be a 100% certain about the value of a certain parameter. &#10;Technically, that case can be considered a random variable, with a point mass. But that is completely beyond the point. Priors help model uncertainty of parameters.&#10;Deterministic parameters in the model are something completely different. They are unknowns, that you decide to model as single values without any uncertainty.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How would you, for example, plot the graph of the prior?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;By plotting its probability distribution.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How would I even calculate the single values for the prior of $\lambda_C$?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Not sure what you mean here. I hope by now you can see this question makes no sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-25T10:03:45.003" Id="73746" LastActivityDate="2013-10-25T10:03:45.003" OwnerUserId="12436" ParentId="73740" PostTypeId="2" Score="7" />
  <row AnswerCount="4" Body="&lt;p&gt;If we have data set, X and Y variable. Say, we do correlation analysis and get some correlation coefficient. Besides, we find an important fact after observing their relationship: That is, the scatter plot of X and Y has a triangular shape. Which means that (for example) when X values are increasing Y values are increasing for all X values, whereas vice versa is not true; when the Y value is increasing X values are anything for all Y values.&lt;/p&gt;&#10;&#10;&lt;p&gt;What kind of analysis should I do to investigate this?  &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;(&lt;em&gt;Update in response to&lt;/em&gt; @Penguin_Knight)&lt;/p&gt;&#10;&#10;&lt;p&gt;Your graph is exactly what I explained. Just take away the negative values with an imaginary y=0 line. As you can see there are many data points the X of which is either 0 or very small, and Y has pretty high value. However there is no data point that have y=0 and x is quite high value. And as you can see it makes the data scatter shape a right angle triangle. Thus we could say that the high X values necessitates high Y values but high Y values does not always have high X values. I find it very interesting. In practice for example I measure Complexity and Size of given entities. And my analysis show that all complex entities have big size but not all big-size entities are complex. Then I conclude that the certain amount of complexity requires defined amount of size. You cannot put more complexity in a given size. This is a bit abstract but you see my point? It is very interesting and I would like to get some help on how this kind of relationships are discussed or described in statistics formally.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-25T10:55:34.690" Id="73750" LastActivityDate="2014-03-27T16:56:36.483" LastEditDate="2013-10-26T16:41:23.380" LastEditorUserId="7290" OwnerUserId="31909" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;" Title="Are there statistical techniques that investigate such relationships ...?" ViewCount="207" />
  <row AnswerCount="0" Body="&lt;p&gt;What if have N audio signals of different length and for each we know corresponding sentence.&lt;/p&gt;&#10;&#10;&lt;p&gt;How we can train classifer for audio signals-&gt;sentence ? And maybe Hidden Markov Model can be used here?&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally my question is about, is there some technique exist that can be trained with audio signal not separated to letters or phonemes by hand but just by providing sentence that contains in it,the same with handwriten strings or it can be analogy that we give classifer image and information what object present on image but not specify rect positions of them&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-10-25T11:13:12.140" FavoriteCount="1" Id="73752" LastActivityDate="2013-10-25T13:49:50.297" LastEditDate="2013-10-25T13:49:50.297" LastEditorUserId="16843" OwnerUserId="16843" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;hidden-markov-model&gt;" Title="Hidden Markov Model speech recognition" ViewCount="62" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am new to g*power and have a question about which test I have to choose and how to interpret the given sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 2 measurements (pre / post), one control-group and one intervention-group. In a reference study, I was able to find an effect size of .7 for the most important parameter. In g*power I chose F-test -&gt; ANOVA RM within-between interaction, a power of 0.8 and alpha .05.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are my questions:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Did I choose the correct test?&lt;/li&gt;&#10;&lt;li&gt;Is the resulting sample size referring to each group or total?&lt;/li&gt;&#10;&lt;li&gt;Regarding the effect size of .7 I found in a reference study, am I allowed to fill it in g*power -&gt; effect size f, or should I leave it as it was at .25?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-10-25T15:56:13.647" Id="73777" LastActivityDate="2014-10-06T20:12:50.127" LastEditDate="2013-10-25T16:30:46.350" LastEditorUserId="7290" OwnerUserId="31919" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;effect-size&gt;&lt;power-analysis&gt;" Title="Sample size and correct choice of test in g*power" ViewCount="124" />
  
  <row Body="&lt;p&gt;@AsymLabs In R, you can just use the command lm(c(32,40,46) ~ c(1,2,3), weights=1/c(6,8,40)). To get slope = 7.4359 and intercept = 24.7179. That gives the fit whuber described. It's a regression weighted according to the inverse of the variance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-25T20:13:45.023" Id="73800" LastActivityDate="2013-10-25T20:13:45.023" OwnerUserId="31923" ParentId="73793" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="73990" AnswerCount="1" Body="&lt;p&gt;Here is a section of &lt;em&gt;&lt;a href=&quot;http://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf&quot; rel=&quot;nofollow&quot;&gt;Machine Learning: a Probabilistic Perspective&lt;/a&gt;&lt;/em&gt; by Kevin Patrick Murphy&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jatxV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't understand in (19.18) why there is a negative sign. For me, $\log \tilde{p}(\mathbf{y})=\sum_{s\sim t}\log\psi_{st}(y_s,y_t)$ holds. When $y_s$ and $y_t$ agree, $\log\psi_{st}(y_s,y_t)=w_{st}$, otherwise $\log\psi_{st}(y_s,y_t)=-w_{st}$. So shouldn't it be $\log \tilde{p}(\mathbf{y})=\sum_{s\sim t} y_sw_{st}y_t$? Also, when all entries of $\mathbf{y}$ agree, $\sum_{s\sim t} y_sw_{st}y_t$ is maximized because all summands are positive. So where is the problem? Do I miss something? Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-26T02:02:38.987" Id="73808" LastActivityDate="2013-10-28T22:22:22.797" LastEditDate="2013-10-26T16:55:44.637" LastEditorUserId="4864" OwnerUserId="4864" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;graphical-model&gt;" Title="Sign of the unnormalized log likelihood in Ising model" ViewCount="82" />
  <row Body="&lt;p&gt;Let $B,C_1,C_2$ be independent $\mathrm{N}(0,1)$ random variables. Define $A_1=B+C_1$ and $A_2=2B+C_2$. Since we are conditioning on the same information, and $C_1$ and $C_2$ have the same distribution, by symmetry we have&#10;$$&#10;  \mathrm{E}[C_1\mid A_1,A_2] = \mathrm{E}[C_2\mid A_1,A_2]&#10;$$&#10;almost surely (we haven't used the independence assumption yet). Hence,&#10;$$&#10;  \mathrm{E}[B\mid A_1,A_2] = \mathrm{E}[B\mid A_1,A_2] + \mathrm{E}[C_2\mid A_1,A_2] - \mathrm{E}[C_1\mid A_1,A_2]&#10;$$&#10;$$&#10;  = \mathrm{E}[B+C_2-C_1\mid A_1,A_2] = \mathrm{E}[A_2-A_1\mid A_1,A_2]&#10;$$&#10;$$&#10;  = \mathrm{E}[A_2\mid A_1,A_2] - \mathrm{E}[A_1\mid A_1,A_2] = A_2 - A_1 = B+C_2-C_1&#10;$$&#10;almost surely. Therefore (Why? Remember the independence assumption and use &lt;a href=&quot;http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;. What is the distribution of $-C_1$?),&#10;$$&#10;  \mathrm{E}[B\mid A_1,A_2]\sim \mathrm{N}(0,3) \, .&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you have any doubts about $\mathrm{E}[B\mid A_1,A_2]$ being a random variable, &lt;a href=&quot;http://stats.stackexchange.com/questions/38700/conditional-expectation-of-multivariate-distributions/38707#38707&quot;&gt;check this answer&lt;/a&gt;.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-26T02:32:12.797" Id="73809" LastActivityDate="2013-10-27T17:26:08.913" LastEditDate="2013-10-27T17:26:08.913" LastEditorUserId="9394" OwnerUserId="9394" ParentId="73798" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AcceptedAnswerId="73850" AnswerCount="5" Body="&lt;p&gt;I need a numerical example to illustrate cases where $Cov(X_1, X_2) = 0$. Can you think of examples involving functions or matrices? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-26T21:55:27.983" FavoriteCount="1" Id="73845" LastActivityDate="2013-10-27T17:22:52.433" LastEditDate="2013-10-27T12:03:12.770" LastEditorUserId="686" OwnerUserId="31672" PostTypeId="1" Score="3" Tags="&lt;variance&gt;&lt;covariance&gt;" Title="What is a numerical example of $Var(X_1 + X_2) = Var(X_1) + Var(X_2)$" ViewCount="188" />
  
  
  <row Body="&lt;p&gt;Unless you're using terms differently that what I understand you to mean, you're mistaken when you assert that &quot;formulas don't refer to any specific variables in a dataset&quot;. They certainly do refer to specific variables, explicitly by name.&lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://stackoverflow.com/a/16313110/330679&quot;&gt;this stackoverflow answer&lt;/a&gt; for some background information and where R formulas originate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Formulas are used for many purposes in R, and a specific component of a formula (such as a variable name or an operator) may have a somewhat different meaning in a different context. &lt;/p&gt;&#10;&#10;&lt;p&gt;The meaning of the formula in &lt;code&gt;plot(y ~ x1 + x2, data=mydata)&lt;/code&gt; and in &lt;code&gt;lm(y ~ x1 + x2, data=mydata)&lt;/code&gt; and in &lt;code&gt;glm(y ~ x1 + x2, family=binomial, data=mydata)&lt;/code&gt; are all somewhat different ... and as you go further afield, meanings can change even more, even between packages doing very similar things.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what that formula might mean &lt;em&gt;in R&lt;/em&gt; is very context dependent -- and we don't have sufficient context.&lt;/p&gt;&#10;&#10;&lt;p&gt;(You don't even mention whether you're using a package in R or building something yourself.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Given this is a naive Bayes classifier, your interpretation certainly makes sense (think in terms of logs, for example), and likely that's what I'd have anticipated it to mean, but that's not really anything to do with R unless you're using some particular package... whose specific interpretation of formulas we might then be able to explain.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-26T23:04:56.650" Id="73854" LastActivityDate="2013-10-27T06:53:15.480" LastEditDate="2013-10-27T06:53:15.480" LastEditorUserId="805" OwnerUserId="805" ParentId="73832" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;You are just confusing $f(x)$ and $F(X)$. The density function $f(x)$ can be greater 1. It just integrates to 1. It is $F(X) \in [0, 1]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-27T20:30:28.140" Id="73888" LastActivityDate="2013-10-27T20:30:28.140" OwnerUserId="12436" ParentId="73881" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="73893" AnswerCount="2" Body="&lt;p&gt;For a pdf $f(x)$ (i.e. &lt;strong&gt;continuous&lt;/strong&gt; distribution), Entropy (differential entropy) &lt;a href=&quot;http://en.wikipedia.org/wiki/Differential_entropy&quot; rel=&quot;nofollow&quot;&gt;is defined&lt;/a&gt; as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_C(X) = -\int_\mathbb{X} f(x)\log f(x)\,dx.$&lt;/p&gt;&#10;&#10;&lt;p&gt;For a &lt;strong&gt;discrete&lt;/strong&gt; distribution with p.m.f $F(x)$, Entropy is defined as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_D(X) = -\sum_{i=1}^n {F(x_i) \log F(x_i)}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;The definitions look analogous to each other. However, entropy increases with dispersion for continuous but not for discrete distributions. Why?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-10-27T21:15:30.070" Id="73891" LastActivityDate="2013-10-28T00:04:20.103" LastEditDate="2013-10-27T22:55:20.047" LastEditorUserId="27838" OwnerUserId="27838" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;entropy&gt;&lt;intuition&gt;" Title="Why does entropy increase with dispersion for continuous but not for discrete distributions?" ViewCount="183" />
  
  
  <row Body="&lt;p&gt;Just to make sure I understand you, I think you're plotting the index value by the year value, and there are several index values per year value. And you're in Graph Builder using the Smoother element. Here's a quick mock-up:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/h1mTS.png&quot; alt=&quot;JMP Graph Builder smoother with duplicate X values&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually in my mock-up, I also have the Points element turned on to emphasize the multiple Y values by X.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the &lt;a href=&quot;http://www.jmp.com/support/help/Additional_Examples_Using_Graph_Builder.shtml&quot; rel=&quot;nofollow&quot;&gt;JMP 11 doc&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The smoother is a cubic spline with a default lambda of 0.05 and&#10;  standardized X values. You can change the value of lambda using the&#10;  slider. You can obtain the same spline in the Bivariate platform...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Cubic splines are technically only defined for data sets with unique X values. In case of duplicate Xs, JMP first takes the weighted mean of the corresponding Y values. Use the Freq drop zone in Graph Builder if you want to control the weighting, otherwise each Y is weighted equally.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-27T23:50:31.847" Id="73903" LastActivityDate="2013-10-27T23:50:31.847" OwnerUserId="1191" ParentId="73887" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="73917" AnswerCount="1" Body="&lt;p&gt;I´d like to analyse the effect of a treatment (&lt;code&gt;treatment    : Factor w/ 2 levels &quot;ambient&quot;,&quot;elevated&quot;&lt;/code&gt;) in tree diameter increment. Tree diameter is influenced by tree size. To do so, I performed the following lm:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = BAI2013 ~ diameterJul12 * treatment, data = bandNA)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-16.6493  -3.1740  -0.3767   3.3631  22.7267 &#10;&#10;Coefficients:&#10;                                 Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)                     -20.49357    2.12883  -9.627  &amp;lt; 2e-16 ***&#10;diameterJul12                     1.24194    0.08876  13.992  &amp;lt; 2e-16 ***&#10;treatmentelevated                10.72336    3.45783   3.101 0.002295 ** &#10;diameterJul12:treatmentelevated  -0.54953    0.14795  -3.714 0.000285 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 6.461 on 153 degrees of freedom&#10;Multiple R-squared:  0.6035,    Adjusted R-squared:  0.5958 &#10;F-statistic: 77.63 on 3 and 153 DF,  p-value: &amp;lt; 2.2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I interpret the results? I need to figure out whether the slope of BAI~diameter is steeper for elevated trees than for ambient.&#10;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-10-28T03:04:51.660" Id="73914" LastActivityDate="2013-10-28T05:07:51.823" OwnerUserId="31979" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;lm&gt;" Title="R- Analysis of homogeneity of slopes" ViewCount="113" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a very particular question, I have seen a similar one &lt;a href=&quot;http://stats.stackexchange.com/questions/46597/dependent-bernoulli-trials&quot;&gt;here&lt;/a&gt;, but my knowledge is too limited to make use of it. I will try to explain myself as clearly as possible... Wish me luck!&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a sequence of (&lt;em&gt;a priori&lt;/em&gt;) probabilities of a binary variable, we could say $X_i \sim Ber(p_i)$, i.e. $P(X_i = 1) = 1 - P(X_i = 0)=p_i$. I know that $X_i$ are not independent, but I don't know exactly $P(X_i|X_{j\neq i})$ nor $P(X_i|X_{j_1\neq i},...,X_{j_n\neq i})$ (I don't know anything about their dependence), and I want to know $$P(\sum_{j=-K}^{j=K} X_{i+j} =0)$$&#10;What kind of knowledge/hypothesis do you think I need in order to approximate this more efficiently? Assuming that they are independent does not work well enough. What would you do if you found a similar situation? I am not an expert at all in this matters, and the more I learn about statistics the less I know! &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I can see that the probabilities form small &quot;triangle&quot; shapes, so maybe something like $P(X_i = 1 | X_{i-1} = 1, X_{i-2} = 0) = P(X_i = 0 | X_{i-1} = 0, X_{i-2} = 1) $ can help? If so, is there any way to use this? As I said, I have no idea...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much for your help!&lt;/p&gt;&#10;&#10;&lt;p&gt;edit: I think the title is not very good, but I don't know how to explain it better... One more question, can I say $Y_i = X_i - X_{i-1}$ and try to see $P(X_i|Y_{i-1})$?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-10-28T10:00:02.867" FavoriteCount="1" Id="73931" LastActivityDate="2015-03-02T06:33:34.697" LastEditDate="2013-10-28T10:05:39.867" LastEditorUserId="28322" OwnerUserId="28322" PostTypeId="1" Score="2" Tags="&lt;cross-correlation&gt;&lt;bernoulli-distribution&gt;" Title="Using cross correlation to infer dependence, can it be done?" ViewCount="103" />
</parent>
